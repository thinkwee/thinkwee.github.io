<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Note for Hierarchical Latent Dirichlet Allocation">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Hierarchical Latent Dirichlet Allocation">
<meta property="og:url" content="https://thinkwee.top/2019/11/15/hlda/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Note for Hierarchical Latent Dirichlet Allocation">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg">
<meta property="article:published_time" content="2019-11-15T02:53:34.000Z">
<meta property="article:modified_time" content="2025-01-30T02:10:45.361Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="math">
<meta property="article:tag" content="topic model">
<meta property="article:tag" content="lda">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg">


<link rel="canonical" href="https://thinkwee.top/2019/11/15/hlda/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/11/15/hlda/","path":"2019/11/15/hlda/","title":"Note for Hierarchical Latent Dirichlet Allocation"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Note for Hierarchical Latent Dirichlet Allocation | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">52</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#improvements-of-hlda"><span class="nav-number">1.</span> <span class="nav-text">Improvements of hLDA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dp"><span class="nav-number">2.</span> <span class="nav-text">DP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#stick-breaking-process"><span class="nav-number">3.</span> <span class="nav-text">Stick-Breaking Process</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dp2crp"><span class="nav-number">4.</span> <span class="nav-text">DP2CRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#crp"><span class="nav-number">5.</span> <span class="nav-text">CRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ncrp"><span class="nav-number">6.</span> <span class="nav-text">nCRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hlda"><span class="nav-number">7.</span> <span class="nav-text">hLDA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gibbs-sampling-in-hlda"><span class="nav-number">8.</span> <span class="nav-text">Gibbs Sampling in hLDA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hlda%E6%94%B9%E8%BF%9B%E4%BA%86%E4%BB%80%E4%B9%88"><span class="nav-number">9.</span> <span class="nav-text">hLDA改进了什么</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dp"><span class="nav-number">10.</span> <span class="nav-text">DP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8A%98%E6%A3%92%E8%BF%87%E7%A8%8B"><span class="nav-number">11.</span> <span class="nav-text">折棒过程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dp2crp"><span class="nav-number">12.</span> <span class="nav-text">DP2CRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#crp"><span class="nav-number">13.</span> <span class="nav-text">CRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ncrp"><span class="nav-number">14.</span> <span class="nav-text">nCRP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hlda"><span class="nav-number">15.</span> <span class="nav-text">hLDA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gibbs-sampling-in-hlda"><span class="nav-number">16.</span> <span class="nav-text">Gibbs Sampling in hLDA</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="sidebar-inner sidebar-post-related">
          <div class="animated">
              <div class="links-of-blogroll-title"><i class="fa fa-signs-post fa-fw"></i>
    Related Posts
  </div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2020/02/13/svm/" rel="bookmark">
        <time class="popular-posts-time">2020-02-13</time>
        <br>
      SVM
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2018/07/23/lda/" rel="bookmark">
        <time class="popular-posts-time">2018-07-23</time>
        <br>
      Note for Latent Dirichlet Allocation
      </a>
    </li>
    <li class="popular-posts-item">
      <a class="popular-posts-link" href="/2019/07/29/CorEx/" rel="bookmark">
        <time class="popular-posts-time">2019-07-29</time>
        <br>
      Study Notes for Correlation Explaination
      </a>
    </li>
  </ul>

          </div>
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/11/15/hlda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Note for Hierarchical Latent Dirichlet Allocation | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Note for Hierarchical Latent Dirichlet Allocation
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-11-15 10:53:34" itemprop="dateCreated datePublished" datetime="2019-11-15T10:53:34+08:00">2019-11-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-30 10:10:45" itemprop="dateModified" datetime="2025-01-30T10:10:45+08:00">2025-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2019/11/15/hlda/" class="post-meta-item leancloud_visitors" data-flag-title="Note for Hierarchical Latent Dirichlet Allocation" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>24k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>21 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Note for Hierarchical Latent Dirichlet Allocation</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li>Still mainly referred to Prof. Yida Xu’s
tutorials<sup class="refplus-num"><a href="#ref-yidaxu">[1]</a></sup>.</li>
</ul>
<h1 id="improvements-of-hlda">Improvements of hLDA</h1>
<ul>
<li><p>Improved two points</p>
<ul>
<li><p>Introduced Dirichlet Process</p></li>
<li><p>Introduced Hierarchical Structure</p></li>
</ul></li>
</ul>
<h1 id="dp">DP</h1>
<ul>
<li><p>The Dirichlet Process extends the concept of Dirichlet
Distribution to a random process. Typically, sampling by probability
yields a sample, a value, while sampling by random process yields a
function, a distribution. Given the DP's hyperparameter <span class="math inline">\(\alpha\)</span>, a metric space <span class="math inline">\(\theta\)</span>, and a measure <span class="math inline">\(H\)</span> on this metric space (called base
distribution), sampling from <span class="math inline">\(DP(\alpha,H)\)</span> generates an
infinite-dimensional discrete distribution <span class="math inline">\(G\)</span> on <span class="math inline">\(\theta\)</span>. For any partition <span class="math inline">\(A_1,...,A_n\)</span> of <span class="math inline">\(\theta\)</span>, the partitioned <span class="math inline">\(G\)</span> still follows the Dirichlet
Distribution corresponding to the hyperparameters:</p>
<p><span class="math display">\[
(G(A_1,...,A_n))  \sim  Dir(\alpha H(A_1),...,\alpha H(A_n))
\]</span></p>
<p><span class="math inline">\(G\)</span> is defined as a sample
path/function/realization of the Dirichlet Process, i.e., <span class="math inline">\(G=DP(t,w_0) \sim \ DP(\alpha,H)\)</span>. A
realization of the Dirichlet Process is a probability measure, a
function defined on the metric space <span class="math inline">\(\theta\)</span>, with its output being a
probability. Note that due to its infinite-dimensionality, <span class="math inline">\(\alpha\)</span> cannot be preset to a specific
dimension, but only set to be the same <span class="math inline">\(\alpha\)</span>. Compared to LDA, we can see that
DP's hyperparameter <span class="math inline">\(\alpha\)</span> is a
concentration parameter that can only control the certainty of G
distribution trending towards uniformity, while the specific
distribution trend is determined by the partition <span class="math inline">\(A\)</span>.</p></li>
<li><p>Here we can see the difference from LDA's use of Dir
Distribution: DP directly samples to generate a probability measure,
which can further generate a discrete probability distribution; while in
LDA, sampling from Dir Distribution only yields a sample, which serves
as a parameter for the multinomial distribution, determining a discrete
distribution.</p></li>
<li><p>DP can be used to describe mixture models in scenarios with an
uncertain number of components. In a GMM scenario, if there are n
samples but we don't know how many GMs generated these n samples, for
sample i, we assign it to a certain GM, and let the parameters of this
GM for sample i be <span class="math inline">\(\theta _i\)</span>. This
<span class="math inline">\(\theta\)</span> follows a base distribution
<span class="math inline">\(H(\theta)\)</span>. If <span class="math inline">\(H\)</span> is a continuous distribution, the
probability of two samples taking the same <span class="math inline">\(\theta\)</span> approaches zero, equivalent to n
samples corresponding to n GMs. We can discretize this <span class="math inline">\(H\)</span> into G, with the discretization method
being <span class="math inline">\(G \sim DP(\alpha,H)\)</span>. The
smaller <span class="math inline">\(\alpha\)</span> is, the more
discrete it becomes; the larger <span class="math inline">\(\alpha\)</span> is, the closer G is to H. Note
that <span class="math inline">\(H\)</span> can also be
discrete.</p></li>
<li><p>The two parameters of DP, <span class="math inline">\(H\)</span>
and <span class="math inline">\(\alpha\)</span>, where the former
determines the location of each discrete point of <span class="math inline">\(G\)</span>, i.e., the specific value of <span class="math inline">\(\theta _i\)</span>; the latter determines the
degree of discreteness, or how dispersed <span class="math inline">\(\theta\)</span> is, whether the probability
distribution is concentrated or dispersed, consistent with the <span class="math inline">\(\alpha\)</span> in Dirichlet
Distribution.</p></li>
<li><p>Since G satisfies Dirichlet Distribution, it has many good
properties, including conjugacy to the multinomial distribution,
collapsing and splitting, and renormalization.</p>
<ul>
<li><p><span class="math inline">\(E[G(A_i)]=H(A_i)\)</span></p></li>
<li><p><span class="math inline">\(Var[G(A_i)]=\frac
{H(A_i)[1-H(A_i)]}{\alpha + 1}\)</span></p></li>
<li><p>We can see that when <span class="math inline">\(\alpha\)</span>
takes extreme values, the variance degenerates to 0 or the variance of a
Bernoulli distribution, corresponding to the two extreme cases of G
discretizing H that we mentioned earlier.</p></li>
</ul></li>
<li><p>So what do we want to do with DP? Create a generative model: we
want to obtain a probability measure <span class="math inline">\(G \sim
\ DP(H,\alpha)\)</span>, obtain the group parameter for each sample
point i based on <span class="math inline">\(G\)</span>: <span class="math inline">\(x_i \sim \  G\)</span>, and then generate the
sample point i based on this parameter and function <span class="math inline">\(F\)</span>: <span class="math inline">\(p_i \sim \
F(x_i)\)</span></p></li>
<li><p>Next, we can use the Chinese Restaurant Process (CRP), Stick
Breaking Process, and Polya Urn Model to refine this <span class="math inline">\(x_i\)</span>, that is, split the group parameter
corresponding to the sample point i into the group assignment and group
parameters, written as <span class="math inline">\(x_i=\phi
_{g_i}\)</span>, where <span class="math inline">\(g\)</span> is the
group assignment of the sample point, and <span class="math inline">\(\phi\)</span> is the group parameter.</p></li>
<li><p>Next, using <a target="_blank" rel="noopener" href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process">echen's
description</a> to describe how three models refine <span class="math inline">\(x_i\)</span>:</p></li>
<li><p>In the Chinese Restaurant Process:</p>
<ul>
<li><p>We generate table assignments <span class="math inline">\(g_1,
\ldots, g_n \sim CRP(\alpha)\)</span> according to a Chinese Restaurant
Process. (<span class="math inline">\(g_i\)</span> is the table assigned
to datapoint <span class="math inline">\(i\)</span>.)</p></li>
<li><p>We generate table parameters <span class="math inline">\(\phi_1,
\ldots, \phi_m \sim G_0\)</span> according to the base distribution
<span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>Given table assignments and table parameters, we generate each
datapoint <span class="math inline">\(p_i \sim F(\phi_{g_i})\)</span>
from a distribution <span class="math inline">\(F\)</span> with the
specified table parameters. (For example, <span class="math inline">\(F\)</span> could be a Gaussian, and <span class="math inline">\(\phi_i\)</span> could be a parameter vector
specifying the mean and standard deviation).</p></li>
</ul></li>
<li><p>In the Polya Urn Model:</p>
<ul>
<li><p>We generate colors <span class="math inline">\(\phi_1, \ldots,
\phi_n \sim Polya(G_0, \alpha)\)</span> according to a Polya Urn Model.
(<span class="math inline">\(\phi_i\)</span> is the color of the ith
ball.)</p></li>
<li><p>Given ball colors, we generate each datapoint <span class="math inline">\(p_i \sim F(\phi_i)\)</span>.</p></li>
</ul></li>
<li><p>In the Stick-Breaking Process:</p>
<ul>
<li><p>We generate group probabilities (stick lengths) <span class="math inline">\(w_1, \ldots, w_{\infty} \sim
Stick(\alpha)\)</span> according to a Stick-Breaking process.</p></li>
<li><p>We generate group parameters <span class="math inline">\(\phi_1,
\ldots, \phi_{\infty} \sim G_0\)</span> from <span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>We generate group assignments <span class="math inline">\(g_1,
\ldots, g_n \sim Multinomial(w_1, \ldots, w_{\infty})\)</span> for each
datapoint.</p></li>
<li><p>Given group assignments and group parameters, we generate each
datapoint <span class="math inline">\(p_i \sim
F(\phi_{g_i})\)</span>.</p></li>
</ul></li>
<li><p>In the Dirichlet Process:</p>
<ul>
<li><p>We generate a distribution <span class="math inline">\(G \sim
DP(G_0, \alpha)\)</span> from a Dirichlet Process with base distribution
<span class="math inline">\(G_0\)</span> and dispersion parameter <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>We generate group-level parameters <span class="math inline">\(x_i \sim G\)</span> from <span class="math inline">\(G\)</span>, where <span class="math inline">\(x_i\)</span> is the group parameter for the ith
datapoint. (Note: this is not the same as <span class="math inline">\(\phi_i\)</span>. <span class="math inline">\(x_i\)</span> is the parameter associated to the
group that the ith datapoint belongs to, whereas <span class="math inline">\(\phi_k\)</span> is the parameter of the kth
distinct group.)</p></li>
<li><p>Given group-level parameters <span class="math inline">\(x_i\)</span>, we generate each datapoint <span class="math inline">\(p_i \sim F(x_i)\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="stick-breaking-process">Stick-Breaking Process</h1>
<ul>
<li><p>The Stick-Breaking Process provides an infinite division on <span class="math inline">\(\theta\)</span>. Let the DP parameters be <span class="math inline">\(\alpha\)</span>, and the process is as
follows:</p>
<ul>
<li><p><span class="math inline">\(\beta _1 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_1 = \beta _1\)</span></p></li>
<li><p><span class="math inline">\(\beta _2 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_2 = (1-\pi _1) * \beta
_2\)</span></p></li>
</ul></li>
<li><p>This way, each time a division on [0,1] is obtained from the Beta
distribution, cutting the entire <span class="math inline">\(\theta\)</span> into two parts. The first part is
taken as the first division on <span class="math inline">\(\theta\)</span>, and the remaining part is seen as
the whole for the next stick-breaking. Then, cut it into two parts
again, with the first part taken as the second division on <span class="math inline">\(\theta\)</span>. It's like a stick being
continuously broken, each time breaking from the remaining part, and the
final segments are the divisions.</p></li>
</ul>
<h1 id="dp2crp">DP2CRP</h1>
<ul>
<li><p>Introduce an indicator function. If two sample points i and j are
assigned to the same component, their indicator function <span class="math inline">\(z\)</span> is the same, which represents which
component each sample belongs to, <span class="math inline">\(x_i \sim
Component(\theta _{z_i})\)</span></p></li>
<li><p>For a mixture distribution, such as GMM, we want to obtain the
predictive distribution, that is, given the component assignment of
known data, for a new unknown data point, we want to know which
component it belongs to:</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i})
\]</span></p></li>
<li><p>From the definition, we know this probability should be
independent of <span class="math inline">\(H\)</span>, because we don't
care about the specific value of <span class="math inline">\(\theta\)</span>, we only care which <span class="math inline">\(\theta\)</span> it is, so the predictive
distribution is closely related to <span class="math inline">\(\alpha\)</span>. Expanding it:</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i}) = \frac {p(z_i=m,z_{not \ i})}{p(z_{not \ i})} \\
\]</span></p></li>
<li><p>Since in DP, the number of categories can be infinite, we first
assume k categories, and then let k approach infinity</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \
i}|p_1...p_k)p(p_1...p_k)}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)p(p_1...p_k)}
\]</span></p></li>
<li><p>The probabilities of these k categories follow a Dirichlet
Distribution. Assuming the Base Distribution is uniform, then</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \ i}|p_1...p_k)Dir(\frac
{\alpha}{k} ... \frac {\alpha}{k})}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)Dir(\frac {\alpha}{k} ... \frac{\alpha}{k})}
\]</span></p></li>
<li><p>In both numerator and denominator, the integral is essentially a
multinomial distribution multiplied by a Dirichlet distribution. Due to
conjugacy, the posterior should still be a Dirichlet distribution. We
derive the integral of the multinomial distribution multiplied by the
Dirichlet distribution:</p>
<p><span class="math display">\[
\int _{p_1...p_k} p(n_1...n_k|p_1...p_k) p(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} Mul(n_1...n_k|p_1...p_k) Dir(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} (\frac {n!}{n_1!...n_k!} \prod _{i=1}^k p_i ^{n_i})
\frac {\Gamma(\sum \alpha _i)}{\prod \Gamma (\alpha _i)} \prod _{i=1}^k
p_i^{\alpha _i -1} \\
\]</span></p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \int _{p_1...p_k} \prod _{i=1}^k  p_i^{n_i+\alpha _i -1} \\
\]</span></p></li>
<li><p>The integral term is actually a Dirichlet Distribution <span class="math inline">\(Dir(\alpha _1 + n_1 ... \alpha _k + n_k)\)</span>
excluding the constant part, so the integral result is 1/constant,
i.e.:</p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \frac { \prod \Gamma (\alpha _i + n_i)}{\Gamma (n + \sum
\alpha _i)}
\]</span></p></li>
<li><p>This expression includes three parts. The first part with n's is
introduced by the multinomial distribution, representing that we only
look at the size of each set after division, not the specific content of
each set, which is different from our requirements, so we don't need
this constant. The second part is generated by the Dir distribution
prior, and in the predictive distribution, the distribution priors are
all the same, so they cancel out. We mainly focus on the third part,
substituting it back into the predictive distribution fraction.</p></li>
<li><p>First, define an auxiliary variable <span class="math inline">\(n_{l , not \ i} = Count(z_{not \ i} ==
l)\)</span>, then:</p>
<p><span class="math display">\[
n_1 = n_{1,not \ i} \\
\]</span></p>
<p><span class="math display">\[
... \\
\]</span></p>
<p><span class="math display">\[
n_k = n_{k,not \ i} \\
\]</span></p></li>
<li><p>Because we are seeking <span class="math inline">\(p(z_i=m,
z_{not \ i})\)</span>, the number of other categories is already
determined by samples other than the ith sample. What about the mth
category?</p>
<p><span class="math display">\[
n_m = n_{m,not \ i} + 1
\]</span></p></li>
<li><p>This completes the transformation from indicator function
representation to multinomial distribution. Substituting the third part
of the previous derivation into the numerator gives:</p>
<p><span class="math display">\[
\frac {\Gamma(n_{m,not \ i} + \frac {\alpha}{k} + 1) \prod _{l=1,l \neq
m}^k Gamma(n_{l,not \ i})}{\Gamma (\alpha + n)}
\]</span></p></li>
<li><p>Similarly calculating the numerator, the numerator doesn't need
to consider the ith sample assigned to the mth category, so the form is
simpler:</p>
<p><span class="math display">\[
\frac {\prod _{l=1}^k \Gamma(n_{l,not \ i})}{\Gamma(\alpha +n -1)}
\]</span></p></li>
<li><p>Dividing the two expressions and using the property of the Gamma
function <span class="math inline">\(\Gamma(x) = (x-1) \Gamma
(x-1)\)</span> to simplify, we get:</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i} + \frac {\alpha}{k}}{n + \alpha - 1}
\]</span></p></li>
<li><p>Letting k approach infinity, we get:</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i}}{n + \alpha - 1}
\]</span></p></li>
<li><p>However, the sum of this expression for all categories from 1 to
m is not 1, but <span class="math inline">\(\frac {n-1}{n + \alpha
-1}\)</span>. The remaining probability is set as the probability of
taking a new category, thus completing the predictive distribution.
Interestingly, this probability corresponds exactly to the Chinese
Restaurant Process.</p></li>
</ul>
<h1 id="crp">CRP</h1>
<ul>
<li><p>The classic description of the Chinese Restaurant Process is to
distribute n people to an uncertain number of tables, creating a
partition on an integer set. Assuming each element in the set is a
customer, when the nth customer enters a restaurant, they choose a table
according to the following probabilities:</p>
<p><span class="math display">\[
\begin{aligned} p(\text { occupied table } i | \text { previous
customers }) &amp;=\frac{n_{i}}{\alpha +n-1} \\ p(\text { next
unoccupied table } | \text { previous customers }) &amp;=\frac{\alpha
}{\alpha +n-1} \end{aligned}
\]</span></p></li>
<li><p>Where <span class="math inline">\(n_i\)</span> is the number of
people already at table i, and <span class="math inline">\(\alpha\)</span> is the hyperparameter. This way,
the assignment of people to tables corresponds to a partition on the
integer set.</p></li>
<li><p>Analyzing this, if choosing an occupied table, customers tend to
choose tables with more people; if torn between occupied tables and a
new table, it depends on the hyperparameter <span class="math inline">\(\alpha\)</span></p></li>
<li><p>According to the previous derivation, this <span class="math inline">\(\alpha\)</span> is actually the hyperparameter of
the Dirichlet Distribution, and the effect completely matches. Since we
choose a uniform base distribution in CRP, the corresponding Dirichlet
Distribution chooses symmetric hyperparameters with the same <span class="math inline">\(alpha _i\)</span>. The larger <span class="math inline">\(\alpha\)</span> is, the more likely it is to
obtain an equal probability for each item in the Dirichlet Distribution
as a prior for the multinomial distribution. In the Chinese Restaurant
Process, this corresponds to each customer wanting to choose a new
table, so each table has only one person and is equally distributed.
Conversely, the smaller <span class="math inline">\(\alpha\)</span> is,
the less certain, and in the Chinese Restaurant Process, the table
assignments are also less certain.</p></li>
<li><p>We can obtain that the expected number of tables after the mth
person chooses is <span class="math inline">\(E(K_m|\alpha ) = O(\alpha
\log m)\)</span>, specifically <span class="math inline">\(E(K_m|\alpha
) = \alpha (\Psi (\alpha + n) - \Psi (\alpha )) \approx \alpha \log (1 +
\frac{n}{\alpha })\)</span>, which means the increase in the number of
clusters is linearly related to the logarithm of the sample size. We can
estimate the hyperparameter <span class="math inline">\(\alpha\)</span>
based on the amount of data and the desired number of clusters.</p></li>
</ul>
<h1 id="ncrp">nCRP</h1>
<ul>
<li><p>The above only completes an uncertain number of clustering using
DP. We can consider each table in the restaurant as a topic, people as
words, and the topic model as assigning words to topics, or people to
tables, but this is the same as LDA, with no correlation between topics.
To establish a hierarchical relationship between topics, Blei proposed
the Nested Chinese Restaurant Process.</p></li>
<li><p>In the Nested Chinese Restaurant Process, we unify the concepts
of restaurants and tables: restaurants are tables, tables are
restaurants! Why do we say this? First, we set a root restaurant
(obviously, we're building a tree), then choose a table in the root
restaurant according to the Chinese Restaurant Process. Each table in
the restaurant has a note indicating which restaurant the customer
should go to the next day. So the next day, the customer arrives at this
restaurant and chooses a table according to CRP, while also knowing
which restaurant to go to on the third day. Thus, tables correspond to
restaurants, and the tables of the parent restaurant correspond to child
restaurants. Each day is a layer of the tree, establishing a
hierarchical structure of the Chinese Restaurant Process.</p></li>
</ul>
<h1 id="hlda">hLDA</h1>
<ul>
<li><p>Now we can describe hLDA in the framework of nCRP</p></li>
<li><p>Define symbols</p>
<ul>
<li><p><span class="math inline">\(z\)</span>: topics, assuming <span class="math inline">\(K\)</span> topics</p></li>
<li><p><span class="math inline">\(\beta\)</span>: parameters from
topics to word distribution, Dir prior parameters</p></li>
<li><p><span class="math inline">\(w\)</span>: words</p></li>
<li><p><span class="math inline">\(\theta\)</span>: document-to-topic
distribution</p></li>
<li><p><span class="math inline">\(\alpha\)</span>: parameters of
document-to-topic distribution, Dir prior parameters</p></li>
</ul></li>
<li><p>We can simply define LDA:</p>
<p><span class="math display">\[
p(w | \beta) \sim Dir(\beta) \\
p(\theta | \alpha) \sim Dir(\alpha) \\
\theta \sim p(\theta | \alpha) \\
w \sim p(w | \theta , \beta) = \sum _{i=1}^K \theta _i p(w|z=i, \beta
_i) \\
\]</span></p></li>
<li><p>hLDA process:</p>
<ul>
<li><p>Obtain a path from root to leaf of length <span class="math inline">\(L\)</span> according to nCRP</p></li>
<li><p>Sample a topic distribution on the path from a <span class="math inline">\(L\)</span>-dimensional Dirichlet</p></li>
<li><p>Generate a word by mixing these L topics</p></li>
</ul></li>
<li><p>Detailed description: <a target="_blank" rel="noopener" href="https://imgchr.com/i/Mwfu34"><img data-src="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg" alt="Mwfu34.md.jpg"></a></p></li>
<li><p>Probability graph, where <span class="math inline">\(c\)</span>
is the restaurant, nCRP is separately drawn out here. Actually, <span class="math inline">\(c\)</span> determines the topic <span class="math inline">\(z\)</span>, and <span class="math inline">\(\gamma\)</span> is the concentration parameter of
CRP corresponding to DP: <a target="_blank" rel="noopener" href="https://imgchr.com/i/Mwf3Hx"><img data-src="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg" alt="Mwf3Hx.md.jpg"></a></p></li>
</ul>
<h1 id="gibbs-sampling-in-hlda">Gibbs Sampling in hLDA</h1>
<ul>
<li><p>Define variables:</p>
<ul>
<li><p><span class="math inline">\(w_{m,n}\)</span>: the nth word in the
mth document</p></li>
<li><p><span class="math inline">\(c_{m,l}\)</span>: the restaurant
corresponding to the topic at the lth layer in the path of the mth
document, needs to be sampled and calculated</p></li>
<li><p><span class="math inline">\(z_{m,n}\)</span>: the topic assigned
to the nth word in the mth document, needs to be sampled and
calculated</p></li>
</ul></li>
<li><p>The sampling formula from the posterior distribution is divided
into two parts. The first part is obtaining the path, which will use the
previous predictive distribution; the second part is known the path,
which is similar to ordinary LDA. The final sampling formula is:</p>
<p><span class="math display">\[
p\left(\mathbf{w}_{m} | \mathbf{c}, \mathbf{w}_{-m},
\mathbf{z}\right)=\prod_{\ell=1}^{L}\left(\frac{\Gamma\left(n_{c_{m,
\ell},-m}^{(\cdot)}+W \eta\right)}{\prod_{w} \Gamma\left(n_{c_{m,
e},-m}^{(w)}+\eta\right)} \frac{\prod_{w} \Gamma\left(n_{c_{m,
\ell},-m}^{(w)}+n_{c_{m, \ell},
m}^{(w)}+\eta\right)}{\Gamma\left(n_{c_{m, \ell},-m}^{(\cdot)}+n_{c_{m,
\ell}, m}^{(\cdot)}+W \eta\right)}\right)
\]</span></p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li>依然主要参考了徐亦达老师的教程<sup class="refplus-num"><a href="#ref-yidaxu">[1]</a></sup>。</li>
</ul>
<h1 id="hlda改进了什么">hLDA改进了什么</h1>
<ul>
<li><p>改进了两点</p>
<ul>
<li><p>引入了Dirichlet Process</p></li>
<li><p>引入了层次结构</p></li>
</ul></li>
</ul>
<h1 id="dp">DP</h1>
<ul>
<li><p>Dirichlet Process将Dirichlet
Distribution的概念扩展到随机过程，一般依概率采样会得到一个样本，一个值，而依据随机过程采样得到的是一个函数，是一个分布。给定DP的超参<span class="math inline">\(\alpha\)</span>，给定度量空间<span class="math inline">\(\theta\)</span>，以及该度量空间上的一个测度<span class="math inline">\(H\)</span>（称为基分布, Base Distribution)，<span class="math inline">\(DP(\alpha,H)\)</span>中采样得到的就是一个在<span class="math inline">\(\theta\)</span>上的无限维离散分布<span class="math inline">\(G\)</span>，假如对这个无限维（无限个离散点）做<span class="math inline">\(\theta\)</span>上的任意一种划分<span class="math inline">\(A_1,...,A_n\)</span>，那么划分之后的<span class="math inline">\(G\)</span>分布依然满足对应Dirichlet
Distribution在超参上的划分：</p>
<p><span class="math display">\[
(G(A_1,...,A_n))  \sim  Dir(\alpha H(A_1),...,\alpha H(A_n))
\]</span></p>
<p><span class="math inline">\(G\)</span>定义为Dirichlet
Process的一个sample path/function/realization,即<span class="math inline">\(G=DP(t,w_0) \sim \
DP(\alpha,H)\)</span>。Dirichelt
Process的一个realization是一个概率测度，是一个函数，定义域在度量空间<span class="math inline">\(\theta\)</span>上，函数输出即概率。注意因为是无限维，因此不能预先设置<span class="math inline">\(\alpha\)</span>的维数，只能设置为一样的<span class="math inline">\(\alpha\)</span>，对比LDA，可以看到DP的超参<span class="math inline">\(\alpha\)</span>是一个concentration
parameter，只能控制G分布趋于均匀分布的确定性，而不能控制G分布趋于怎样的分布，趋于怎样的分布由划分<span class="math inline">\(A\)</span>决定。</p></li>
<li><p>这里可以看到和LDA使用Dir
Distribution的区别：DP是直接采样生成了一个概率测度，可以进而生成离散的概率分布；而LDA中对Dir
Distribution采样只能得到一个样本，但是这个样本作为了多项式分布的参数，确定了一个多项式分布（也是离散的）。</p></li>
<li><p>DP可以用于描述混合模型，在混合组件数量不确定的情况下，通过DP来构造一个组件分配。放在GMM的场景里，假如有n个样本，但我不知道有几个GM来生成这n个样本，那么对样本i，我将其分配给某一个GM，称这个样本i所在GM的参数为<span class="math inline">\(\theta _i\)</span>，那么这个<span class="math inline">\(\theta\)</span>服从一个基分布<span class="math inline">\(H(\theta)\)</span>，假如<span class="math inline">\(H\)</span>是连续分布，那么两个样本取到相同的<span class="math inline">\(\theta\)</span>的概率趋于零，相当于n个样本对应n个GM，那么我们可以把这个<span class="math inline">\(H\)</span>离散化为G，离散的方式为<span class="math inline">\(G \sim DP(\alpha,H)\)</span>，<span class="math inline">\(\alpha\)</span>越小越离散，越大则<span class="math inline">\(G\)</span>越趋近于<span class="math inline">\(H\)</span>。注意<span class="math inline">\(H\)</span>也可以是离散的。</p></li>
<li><p>DP的两个参数，<span class="math inline">\(H\)</span>和<span class="math inline">\(\alpha\)</span>，前者决定了<span class="math inline">\(G\)</span>的每一个离散点的位置，即<span class="math inline">\(\theta
_i\)</span>具体的值；后者决定了离散程度，或者理解为<span class="math inline">\(\theta\)</span>有多分散，有多不重复，即概率分布是集中的还是分散的，这个Dirichlet
Distribution里的<span class="math inline">\(\alpha\)</span>是一致的。</p></li>
<li><p>由于G满足Dirichlet
Distribution,因此有很多好的性质，包括对于多项式分布的conjugate，collapsing和splitting，以及renormalization。</p>
<ul>
<li><p><span class="math inline">\(E[G(A_i)]=H(A_i)\)</span></p></li>
<li><p><span class="math inline">\(Var[G(A_i)]=\frac
{H(A_i)[1-H(A_i)]}{\alpha + 1}\)</span></p></li>
<li><p>可以看到<span class="math inline">\(\alpha\)</span>取极端时，方差分别退化为0或者伯努利分布的方差，对应着之前我们说的G去离散化H的两种极端情况。</p></li>
</ul></li>
<li><p>那么我们想用DP做什么，做一个生成式模型：我们想得到一个概率测度<span class="math inline">\(G \sim \ DP(H,\alpha)\)</span>，根据<span class="math inline">\(G\)</span>得到每一个样本点i所属的组对应的参数(Group
Parameter)<span class="math inline">\(x_i \sim
\  G\)</span>，之后根据这个参数和函数<span class="math inline">\(F\)</span>生成样本点i：<span class="math inline">\(p_i \sim \ F(x_i)\)</span></p></li>
<li><p>接下来可以用中国餐馆过程(CRP)、折棒过程(Stick Breaking)和Polya
Urm模型来细化这个<span class="math inline">\(x_i\)</span>，即将和样本点i对应组的参数拆成样本点i对应的组和每组的参数，写成<span class="math inline">\(x_i=\phi _{g_i}\)</span>，其中<span class="math inline">\(g\)</span>是样本点的组分配，<span class="math inline">\(\phi\)</span>是组参数。</p></li>
<li><p>接下来套用<a target="_blank" rel="noopener" href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process">echen大佬的描述</a>来描述三个模型如何细化<span class="math inline">\(x_i\)</span>的：</p></li>
<li><p>In the Chinese Restaurant Process:</p>
<ul>
<li><p>We generate table assignments <span class="math inline">\(g_1,
\ldots, g_n \sim CRP(\alpha)\)</span> according to a Chinese Restaurant
Process. (<span class="math inline">\(g_i\)</span> is the table assigned
to datapoint <span class="math inline">\(i\)</span>.)</p></li>
<li><p>We generate table parameters <span class="math inline">\(\phi_1,
\ldots, \phi_m \sim G_0\)</span> according to the base distribution
<span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>Given table assignments and table parameters, we generate each
datapoint <span class="math inline">\(p_i \sim F(\phi_{g_i})\)</span>
from a distribution <span class="math inline">\(F\)</span> with the
specified table parameters. (For example, <span class="math inline">\(F\)</span> could be a Gaussian, and <span class="math inline">\(\phi_i\)</span> could be a parameter vector
specifying the mean and standard deviation).</p></li>
</ul></li>
<li><p>In the Polya Urn Model:</p>
<ul>
<li><p>We generate colors <span class="math inline">\(\phi_1, \ldots,
\phi_n \sim Polya(G_0, \alpha)\)</span> according to a Polya Urn Model.
(<span class="math inline">\(\phi_i\)</span> is the color of the ith
ball.)</p></li>
<li><p>Given ball colors, we generate each datapoint <span class="math inline">\(p_i \sim F(\phi_i)\)</span>.</p></li>
</ul></li>
<li><p>In the Stick-Breaking Process:</p>
<ul>
<li><p>We generate group probabilities (stick lengths) <span class="math inline">\(w_1, \ldots, w_{\infty} \sim
Stick(\alpha)\)</span> according to a Stick-Breaking process.</p></li>
<li><p>We generate group parameters <span class="math inline">\(\phi_1,
\ldots, \phi_{\infty} \sim G_0\)</span> from <span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>We generate group assignments <span class="math inline">\(g_1,
\ldots, g_n \sim Multinomial(w_1, \ldots, w_{\infty})\)</span> for each
datapoint.</p></li>
<li><p>Given group assignments and group parameters, we generate each
datapoint <span class="math inline">\(p_i \sim
F(\phi_{g_i})\)</span>.</p></li>
</ul></li>
<li><p>In the Dirichlet Process:</p>
<ul>
<li><p>We generate a distribution <span class="math inline">\(G \sim
DP(G_0, \alpha)\)</span> from a Dirichlet Process with base distribution
<span class="math inline">\(G_0\)</span> and dispersion parameter <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>We generate group-level parameters <span class="math inline">\(x_i \sim G\)</span> from <span class="math inline">\(G\)</span>, where <span class="math inline">\(x_i\)</span> is the group parameter for the ith
datapoint. (Note: this is not the same as <span class="math inline">\(\phi_i\)</span>. <span class="math inline">\(x_i\)</span> is the parameter associated to the
group that the ith datapoint belongs to, whereas <span class="math inline">\(\phi_k\)</span> is the parameter of the kth
distinct group.)</p></li>
<li><p>Given group-level parameters <span class="math inline">\(x_i\)</span>, we generate each datapoint <span class="math inline">\(p_i \sim F(x_i)\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="折棒过程">折棒过程</h1>
<ul>
<li><p>折棒过程提供了一种在<span class="math inline">\(\theta\)</span>上的无限划分，依然令DP的参数为<span class="math inline">\(\alpha\)</span>，折棒过程如下：</p>
<ul>
<li><p><span class="math inline">\(\beta _1 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_1 = \beta _1\)</span></p></li>
<li><p><span class="math inline">\(\beta _2 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_2 = (1-\pi _1) * \beta
_2\)</span></p></li>
</ul></li>
<li><p>这样每次从Beta分布中得到[0,1]上的一个划分，将整个<span class="math inline">\(\theta\)</span>切成两部分，第一部分作为<span class="math inline">\(\theta\)</span>上的第一个划分，剩下的部分看成下一次折棒的整体，接着从上面切两部分，第一部分作为<span class="math inline">\(\theta\)</span>上的第二个划分，像一个棒不断被折断，每次从剩下的部分里折，最后折成的分段就是划分。</p></li>
</ul>
<h1 id="dp2crp">DP2CRP</h1>
<ul>
<li><p>引入一个示性函数，假如两个样本点i,j他们被分配的组件相同，则他们的示性函数<span class="math inline">\(z\)</span>相同，也就是表征每一个样本属于哪一个组件，<span class="math inline">\(x_i \sim Component(\theta
_{z_i})\)</span></p></li>
<li><p>那么对于混合分布，比如GMM，我们希望得到的是predictive
distribution，即已知数据的组件分配情况下，新来了一个未知数据，我想知道他属于哪个组件：</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i})
\]</span></p></li>
<li><p>结合定义可以知道这个概率应该是和<span class="math inline">\(H\)</span>无关的，因为我不在乎<span class="math inline">\(\theta\)</span>具体的值，我只在乎是哪一个<span class="math inline">\(\theta\)</span>，所以predictive
distribution与<span class="math inline">\(\alpha\)</span>密切相关。将其展开：</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i}) = \frac {p(z_i=m,z_{not \ i})}{p(z_{not \ i})} \\
\]</span></p></li>
<li><p>由于在DP里是划分的类别数可以到无穷多个，因此这里采用了一个小技巧，我们先假设有k类，之后在把k趋于无穷</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \
i}|p_1...p_k)p(p_1...p_k)}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)p(p_1...p_k)}
\]</span></p></li>
<li><p>这里的k个类的概率是符合Dirichlet Distribution的，假设这里的Base
Distribution是均匀分布，则</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \ i}|p_1...p_k)Dir(\frac
{\alpha}{k} ... \frac {\alpha}{k})}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)Dir(\frac {\alpha}{k} ... \frac{\alpha}{k})}
\]</span></p></li>
<li><p>上面无论分子分母，积分内其实都是一个多项式分布乘以一个Dirichlet分布，根据共轭我们知道后验应该还是一个Dirichlet分布，我们推导一下多项式分布与Dirichlet分布相乘的积分：</p>
<p><span class="math display">\[
\int _{p_1...p_k} p(n_1...n_k|p_1...p_k) p(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} Mul(n_1...n_k|p_1...p_k) Dir(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} (\frac {n!}{n_1!...n_k!} \prod _{i=1}^k p_i ^{n_i})
\frac {\Gamma(\sum \alpha _i)}{\prod \Gamma (\alpha _i)} \prod _{i=1}^k
p_i^{\alpha _i -1} \\
\]</span></p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \int _{p_1...p_k} \prod _{i=1}^k  p_i^{n_i+\alpha _i -1} \\
\]</span></p></li>
<li><p>其中积分式内实际上是一个Dirichelt Distribution<span class="math inline">\(Dir(\alpha _1 + n_1 ... \alpha _k +
n_k)\)</span>排除了常数部分，因此积分的结果就是1/常数，即：</p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \frac { \prod \Gamma (\alpha _i + n_i)}{\Gamma (n + \sum
\alpha _i)}
\]</span></p></li>
<li><p>上式包括了三个部分，第一部分的一堆n，它是由多项式分布引入的，代表我们只看划分后每个集合的大小，而不看划分之后每个集合具体的内容，这和我们的需求是不一样的，因此不需要这个常数；第二个部分，是由Dir分布先验产生的，而在predictive
distribution中，分布先验都相同，因此抵消了，所以我们主要关注第三部分，回代入predictive
distribution那个分式当中。</p></li>
<li><p>首先定义一个辅助变量<span class="math inline">\(n_{l , not \ i} =
Count(z_{not \ i} == l)\)</span>，那么：</p>
<p><span class="math display">\[
n_1 = n_{1,not \ i} \\
\]</span></p>
<p><span class="math display">\[
... \\
\]</span></p>
<p><span class="math display">\[
n_k = n_{k,not \ i} \\
\]</span></p></li>
<li><p>因为我们是是在求<span class="math inline">\(p(z_i=m, z_{not \
i})\)</span>，那么肯定除了第m类，其余类的数量早已由除了第i个样本以外的样本确定，那么第m类呢？</p>
<p><span class="math display">\[
n_m = n_{m,not \ i} + 1
\]</span></p></li>
<li><p>这样我们就完成了从指示函数表示的概率到多项式分布的转换，分子部分代入之前得到的第三部分有：</p>
<p><span class="math display">\[
\frac {\Gamma(n_{m,not \ i} + \frac {\alpha}{k} + 1) \prod _{l=1,l \neq
m}^k Gamma(n_{l,not \ i})}{\Gamma (\alpha + n)}
\]</span></p></li>
<li><p>同理计算分子，分子不用考虑第i个样本分给第m类，因此不用在累乘里单独拎出来第m项，形式要简单一些：</p>
<p><span class="math display">\[
\frac {\prod _{l=1}^k \Gamma(n_{l,not \ i})}{\Gamma(\alpha +n -1)}
\]</span></p></li>
<li><p>将上面两式相除，再利用Gamma函数<span class="math inline">\(\Gamma(x) = (x-1) \Gamma
(x-1)\)</span>的性质简化，得到：</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i} + \frac {\alpha}{k}}{n + \alpha - 1}
\]</span></p></li>
<li><p>再令k趋于无穷，得到：</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i}}{n + \alpha - 1}
\]</span></p></li>
<li><p>但是上面这个式子对所有的类别从1到m求和并不为1，而是<span class="math inline">\(\frac {n-1}{n + \alpha
-1}\)</span>，剩下一部分概率就设为取一个新类别的概率，这样我们的predictive
distribution就算完成了，而且可以发现，这个概率，实际上就对应着中国餐馆过程。</p></li>
</ul>
<h1 id="crp">CRP</h1>
<ul>
<li><p>中国餐馆过程的经典描述就是把n个人，一个一个人来，分到不确定张数目的桌子上，做一个整数集合上的划分。假设集合每个元素是一位顾客，第n位顾客走进了一家参观，则他按照以下概率去选择某一张已经有人的桌子坐下，或者找一张没人的新桌子坐下：</p>
<p><span class="math display">\[
\begin{aligned} p(\text { occupied table } i | \text { previous
customers }) &amp;=\frac{n_{i}}{\alpha +n-1} \\ p(\text { next
unoccupied table } | \text { previous customers }) &amp;=\frac{\alpha
}{\alpha +n-1} \end{aligned}
\]</span></p></li>
<li><p>其中<span class="math inline">\(n_i\)</span>是第i张桌子上已经有的人数，$$是超参数。这样人到桌子的分配就对应了整数集合上的划分。</p></li>
<li><p>分析一下，若是选择已经有人的桌子，则顾客倾向于选择人多的桌子；若是在有人的桌子与新桌子之间纠结，则依赖于超参$$</p></li>
<li><p>那根据之前的推导，这个<span class="math inline">\(\alpha\)</span>其实就是Dirichlet
Distribution的超参数，且效果完全吻合。由于在CRP中我们base
distribution选的是均匀分布，那对应的Dirichlet
Distribution选择对称超参，各个<span class="math inline">\(alpha
_i\)</span>相同。那么<span class="math inline">\(\alpha\)</span>越大，以Dirichlet
Distritbuion为参数先验的多项式分布里，取得各个项等概率的可能就越大，在中国餐馆过程中对应着每个顾客进来都想选择一张新桌子，因此每个桌子都只有一个人，等量分配；反之<span class="math inline">\(\alpha\)</span>越小则越不确定，在中国餐馆过程中桌子的分配也不确定</p></li>
<li><p>可以得到第m个人选择之后，桌子数量的期望是<span class="math inline">\(E(K_m|\alpha ) = O(\alpha \log
m)\)</span>，具体而言是<span class="math inline">\(E(K_m|\alpha ) =
\alpha (\Psi (\alpha + n) - \Psi (\alpha )) \approx \alpha \log (1 +
\frac{n}{\alpha })\)</span>，
也就是聚类数的增加与样本数的对数成线性关系。我们可以根据数据量和想要聚类的数量来反估计超参<span class="math inline">\(\alpha\)</span>的设置。</p></li>
</ul>
<h1 id="ncrp">nCRP</h1>
<ul>
<li><p>以上仅仅完成了一个利用了DP的不确定数目聚类，我们可以认为餐馆里每个桌子是一个主题，人就是单词，主题模型就是把词分配到主题，把人分配到桌子，但是这样的话和LDA一样，主题之间没有关联。为了建立主题之间的层次关系，Blei提出了嵌套餐馆过程。</p></li>
<li><p>在嵌套餐馆过程中，我们统一了餐馆和桌子的概念，餐馆就是桌子，桌子就是餐馆！为什么这么说？首先我们设置一个餐馆作为root餐馆（显然我们要建立一棵树了），然后根据中国餐馆过程选择root餐馆里的一个桌子，餐馆里的每个桌子上都有一张纸条指示顾客第二天去某一个餐馆，因此第二天顾客来到这个餐馆，接着根据CRP选个桌子，同时知晓了自己第三天该去哪个参观。因此桌子对应着餐馆，父节点餐馆的桌子对应着子节点餐馆，每一天就是树的每一层，这样就建立了一个层次结构的中国餐馆过程。</p></li>
</ul>
<h1 id="hlda">hLDA</h1>
<ul>
<li><p>接下来我们可以在nCRP的框架上描述hLDA</p></li>
<li><p>定义符号</p>
<ul>
<li><p><span class="math inline">\(z\)</span>：主题，假设有<span class="math inline">\(K\)</span>个</p></li>
<li><p><span class="math inline">\(\beta\)</span>：主题到词分布的参数，Dir先验参数</p></li>
<li><p><span class="math inline">\(w\)</span>：词</p></li>
<li><p><span class="math inline">\(\theta\)</span>：文档到主题的分布</p></li>
<li><p><span class="math inline">\(\alpha\)</span>：文档到主题分布的参数，Dir先验参数</p></li>
</ul></li>
<li><p>那么可以简单定义LDA：</p>
<p><span class="math display">\[
p(w | \beta) \sim Dir(\beta) \\
p(\theta | \alpha) \sim Dir(\alpha) \\
\theta \sim p(\theta | \alpha) \\
w \sim p(w | \theta , \beta) = \sum _{i=1}^K \theta _i p(w|z=i, \beta
_i) \\
\]</span></p></li>
<li><p>hLDA流程如下：</p>
<ul>
<li><p>根据nCRP获得一条从root到leaf的长为<span class="math inline">\(L\)</span>的路径</p></li>
<li><p>根据一个<span class="math inline">\(L\)</span>维的Dirichlet采样一个在路径上的主题分布</p></li>
<li><p>根据这L个主题混合生成一个词</p></li>
</ul></li>
<li><p>详细描述如下： <a target="_blank" rel="noopener" href="https://imgchr.com/i/Mwfu34"><img data-src="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg" alt="Mwfu34.md.jpg"></a></p></li>
<li><p>概率图如下，其中<span class="math inline">\(c\)</span>是餐馆，这里把nCRP单独拎出来了，实际上<span class="math inline">\(c\)</span>决定了主题<span class="math inline">\(z\)</span>，另外<span class="math inline">\(\gamma\)</span>是nCRP中CRP对应DP的concentration
paramter： <a target="_blank" rel="noopener" href="https://imgchr.com/i/Mwf3Hx"><img data-src="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg" alt="Mwf3Hx.md.jpg"></a></p></li>
</ul>
<h1 id="gibbs-sampling-in-hlda">Gibbs Sampling in hLDA</h1>
<ul>
<li><p>定义变量：</p>
<ul>
<li><p><span class="math inline">\(w_{m,n}\)</span>：第m篇文档里的第n个词</p></li>
<li><p><span class="math inline">\(c_{m,l}\)</span>：第m篇文档里路径上第l层选择的主题对应的餐馆，需要采样计算</p></li>
<li><p><span class="math inline">\(z_{m,n}\)</span>：第m篇文档里第n个词分配的主题，需要采样计算</p></li>
</ul></li>
<li><p>从后验分布中采样的公式分为两部分，第一部分是得到路径，这一部分就会利用到之前的predictive
distribution；第二部分是已知路径，剩下的部分就是普通的LDA，最终采样公式为：</p>
<p><span class="math display">\[
p\left(\mathbf{w}_{m} | \mathbf{c}, \mathbf{w}_{-m},
\mathbf{z}\right)=\prod_{\ell=1}^{L}\left(\frac{\Gamma\left(n_{c_{m,
\ell},-m}^{(\cdot)}+W \eta\right)}{\prod_{w} \Gamma\left(n_{c_{m,
e},-m}^{(w)}+\eta\right)} \frac{\prod_{w} \Gamma\left(n_{c_{m,
\ell},-m}^{(w)}+n_{c_{m, \ell},
m}^{(w)}+\eta\right)}{\Gamma\left(n_{c_{m, \ell},-m}^{(\cdot)}+n_{c_{m,
\ell}, m}^{(\cdot)}+W \eta\right)}\right)
\]</span></p></li>
</ul>
</div>
<ul id="refplus"><li id="ref-yidaxu" data-num="1">[1]  徐亦达机器学习课程 Variational Inference for LDA https://www.youtube.com/watch?v=e1wr0xHbfYk</li></ul>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/topic-model/" rel="tag"># topic model</a>
              <a href="/tags/lda/" rel="tag"># lda</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/11/13/kg/" rel="prev" title="Paper reading on Knowledge Graphs">
                  <i class="fa fa-angle-left"></i> Paper reading on Knowledge Graphs
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/12/16/201912/" rel="next" title="Paper Reading 4">
                  Paper Reading 4 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:52</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/11/15/hlda/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
