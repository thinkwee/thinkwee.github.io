<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Knowledge Graph Special Collection  Entity Alignment in Cross-lingual Knowledge Graphs Knowledge Graph Language Model Dynamic Knowledge Graph Dialogue Generation Graph2Seq Graph Matching Network Dyna">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper reading on Knowledge Graphs">
<meta property="og:url" content="https://thinkwee.top/2019/11/13/kg/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Knowledge Graph Special Collection  Entity Alignment in Cross-lingual Knowledge Graphs Knowledge Graph Language Model Dynamic Knowledge Graph Dialogue Generation Graph2Seq Graph Matching Network Dyna">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/14/MY7je0.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/14/MY7je0.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png">
<meta property="article:published_time" content="2019-11-13T01:36:19.000Z">
<meta property="article:modified_time" content="2025-01-30T02:10:45.361Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="natural language processing">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="knowledge graph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/11/14/MY7je0.jpg">


<link rel="canonical" href="https://thinkwee.top/2019/11/13/kg/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/11/13/kg/","path":"2019/11/13/kg/","title":"Paper reading on Knowledge Graphs"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper reading on Knowledge Graphs | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">52</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network"><span class="nav-number">1.</span> <span class="nav-text">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling"><span class="nav-number">2.</span> <span class="nav-text">Barack&#39;s
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs"><span class="nav-number">3.</span> <span class="nav-text">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks"><span class="nav-number">4.</span> <span class="nav-text">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects"><span class="nav-number">5.</span> <span class="nav-text">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-embedding-model"><span class="nav-number">5.1.</span> <span class="nav-text">Graph Embedding Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-matching-networks"><span class="nav-number">5.2.</span> <span class="nav-text">Graph Matching Networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-to-update-knowledge-graphs-by-reading-news"><span class="nav-number">6.</span> <span class="nav-text">Learning to
Update Knowledge Graphs by Reading News</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-number">6.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-number">6.2.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#results"><span class="nav-number">6.3.</span> <span class="nav-text">Results</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs"><span class="nav-number">7.</span> <span class="nav-text">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network"><span class="nav-number">8.</span> <span class="nav-text">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling"><span class="nav-number">9.</span> <span class="nav-text">Barack’s
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs"><span class="nav-number">10.</span> <span class="nav-text">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks"><span class="nav-number">11.</span> <span class="nav-text">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects"><span class="nav-number">12.</span> <span class="nav-text">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-embedding-model"><span class="nav-number">12.1.</span> <span class="nav-text">Graph Embedding Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-matching-networks"><span class="nav-number">12.2.</span> <span class="nav-text">Graph Matching Networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-to-update-knowledge-graphs-by-reading-news"><span class="nav-number">13.</span> <span class="nav-text">Learning to
Update Knowledge Graphs by Reading News</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-number">13.1.</span> <span class="nav-text">encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-number">13.2.</span> <span class="nav-text">decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#result"><span class="nav-number">13.3.</span> <span class="nav-text">result</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs"><span class="nav-number">14.</span> <span class="nav-text">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/11/13/kg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper reading on Knowledge Graphs | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper reading on Knowledge Graphs
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-11-13 09:36:19" itemprop="dateCreated datePublished" datetime="2019-11-13T09:36:19+08:00">2019-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-30 10:10:45" itemprop="dateModified" datetime="2025-01-30T10:10:45+08:00">2025-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/2019/11/13/kg/" class="post-meta-item leancloud_visitors" data-flag-title="Paper reading on Knowledge Graphs" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>35k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>32 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><ul>
<li>Knowledge Graph Special Collection
<ul>
<li>Entity Alignment in Cross-lingual Knowledge Graphs</li>
<li>Knowledge Graph Language Model</li>
<li>Dynamic Knowledge Graph Dialogue Generation</li>
<li>Graph2Seq</li>
<li>Graph Matching Network</li>
<li>Dynamic Knowledge Graph Update</li>
<li>Attention-based Embeddings for Relation Prediction</li>
</ul></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</h1>
<ul>
<li><p>Research: Entity Alignment in Cross-lingual Knowledge
Graphs</p></li>
<li><p>Generally, the approach involves projecting entities into
low-dimensional vectors in their respective language knowledge graphs
and then learning a similarity calculation function</p></li>
<li><p>The problem is that this approach relies on the assumption that
the neighborhood structures of the same entities are identical in
different language knowledge graphs, but this assumption is not always
true. Therefore, traditional methods are not friendly to entities with
few aligned neighborhood nodes or few neighborhood nodes</p></li>
<li><p>The authors propose a topic entity graph to encode the context
information of entity nodes, transforming the node embedding match into
a graph match between topic entity graphs</p></li>
<li><p>Here, the topic entity refers to the entities that need to be
aligned. The topic entity graph is a subgraph constructed by the one-hop
neighborhood entities and the entity itself. If these two entities are
not directly connected in the original knowledge graph, an edge is added
to the knowledge graph</p></li>
<li><p>After obtaining the topic entity graphs, a four-layer network
calculates the similarity between two topic entity graphs:</p>
<ul>
<li><p>Input representation layer: Learn embeddings for each entity in
the topic entity graph. First, use word-level LSTM to learn initial
embeddings, then because it's a directed graph, distinguish between
input and output neighbors, perform aggregation separately (FFN + mean
pooling), concatenate with the previous entity embedding and update
(FFN), iterate K times</p></li>
<li><p>Node-level local matching layer: Mutually match all entity nodes
between the two graphs using attention-based matching. First, calculate
the cosine distance between a node i in topic entity graph 1 and all
nodes in topic entity graph 2 as attention weights, use these weights to
weight all nodes in topic entity graph 2 to obtain graph 2's graph
embedding, then calculate a multi-perspective cosine distance between
this graph embedding and graph 1's query entity embedding. The
multi-perspective means l perspectives represented by l weighted vectors
(d-dimensional, same as embedding dimension). The cosine distance of one
perspective is calculated by element-wise multiplication of a weighted
vector and then computing the cosine distance. The l perspectives
together form a matrix <span class="math inline">\(W \in
R^{l*d}\)</span>, as follows:</p>
<p><span class="math display">\[
score_{perspective_k} = cosine(W_k \cdot embedding_1, W_k \cdot
embedding_2)
\]</span></p></li>
<li><p>Global matching layer: The local matching layer still has the
previously mentioned problem of not being friendly to nodes with few
co-occurring neighbors. Here, a global matching is needed. Specifically,
use a GCN to propagate the local match embedding (the vector obtained
from the previous layer's multi-perspective cosine score) to the entire
topic entity graph, then perform an FFN + max/mean pooling on the entire
graph to obtain the graph matching vector for the two graphs</p></li>
<li><p>Prediction layer: Concatenate the graph matching vectors of the
two topic entity graphs and send them to a softmax for
prediction</p></li>
</ul></li>
<li><p>During training, 20 negative samples were heuristically generated
for each positive sample, 10 in each direction. Here, word-level average
embedding was directly used as the feature vector for entities, matching
the 10 most similar entities.</p></li>
</ul>
<h1
id="baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling">Barack's
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</h1>
<ul>
<li><p>The authors want to maintain a local knowledge graph in the
language model to manage detected facts and use this graph to query
unknown facts for text generation, called KGLM (Knowledge Graph Language
Model)</p></li>
<li><p>Assuming the entity set is <span
class="math inline">\(\xi\)</span>, KGLM predicts:</p>
<p><span class="math display">\[
p(x_t,\xi _t|x_{1,t-1},\xi_ {1,t-1})
\]</span></p></li>
<li><p>The process of generating the next word can be split as
follows:</p>
<ul>
<li><p>Next word is not an entity: Calculate probability in the normal
vocabulary range</p></li>
<li><p>Next word is a completely new entity: Calculate probability
across both normal vocabulary and all entities</p></li>
<li><p>Next word is an entity related to an already seen entity: First
select a previously seen entity as the parent node, then select a child
node, and then calculate probability across the normal vocabulary and
all aliases of the child node</p></li>
</ul></li>
<li><p>The authors use LSTM as the base model for the language model.
All selections - selecting new entities, selecting parent nodes,
selecting child nodes - are based on the LSTM's hidden state (divided
into three parts), with pre-trained embeddings of entities and relations
as input dependencies, followed by probability calculation via
softmax</p></li>
<li><p>To implement such a model, the dataset should provide entity
information. The authors proposed the Linked WikiText2 dataset, with the
following construction process:</p>
<ul>
<li><p>Create entity links based on Wikipedia links, use neural-el to
identify additional links in the Wikidata database, and use Stanford
CoreNLP for co-reference resolution</p></li>
<li><p>Construct local knowledge graph: Next, build parent relations
between entities. For each entity a, add all related entities {b} in
Wikidata as matching candidates. If a related entity b appears in
subsequent paragraphs, set entity a as the parent node of entity
b</p></li>
<li><p>The above method only creates an initial set and needs continuous
expansion. The authors also created alias tables for dates, quantifiers,
etc.</p></li>
<li><p>Below is a representation of a sentence in Linked WikiText2.
Compared to the API query method of WikiText, Linked WikiText2 directly
operates on the original HTML, preserving more link information: <img data-src="https://s2.ax1x.com/2019/11/14/MY7je0.jpg"
alt="MY7je0.jpg" /></p></li>
</ul></li>
<li><p>Train and Inference: First, use TransE algorithm for pre-training
entities and relations. Given a triple (p,r,e), the objective is to
minimize the distance:</p>
<p><span class="math display">\[
\delta\left(\mathbf{v}_{p}, \mathbf{v}_{r},
\mathbf{v}_{e}\right)=\left\|\mathbf{v}_{p}+\mathbf{v}_{r}-\mathbf{v}_{e}\right\|^{2}
\]</span></p>
<p>Use Hinge Loss to ensure the score difference between positive and
negative samples does not exceed <span
class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}=\max \left(0, \gamma+\delta\left(\mathbf{v}_{p},
\mathbf{v}_{r},
\mathbf{v}_{e}\right)-\delta\left(\mathbf{v}_{p}^{\prime},
\mathbf{v}_{r}, \mathbf{v}_{e}^{\prime}\right)\right)
\]</span></p></li>
<li><p>Although the entire process is generative, all variables are
visible, so it can be trained end-to-end. For entity nodes with multiple
parent nodes, probability needs to be normalized</p></li>
<li><p>During inference, we don't have annotation information. We want
to calculate the marginal probability of word <span
class="math inline">\(x\)</span> summed over entities <span
class="math inline">\(\xi\)</span>, not the joint probability (we only
want to get words, with entity information marginalized), but there are
too many entities to calculate joint probabilities and sum them.
Therefore, the authors use importance sampling:</p>
<p><span class="math display">\[
\begin{aligned} p(\mathbf{x}) &amp;=\sum_{\mathcal{E}} p(\mathbf{x},
\mathcal{E})=\sum_{\mathcal{E}} \frac{p(\mathbf{x},
\mathcal{E})}{q(\mathcal{E} | \mathbf{x})} q(\mathcal{E} | \mathbf{x})
\\ &amp; \approx \frac{1}{N} \sum_{\mathcal{E} \sim q}
\frac{p(\mathbf{x}, \mathcal{E})}{q(\mathcal{E} | \mathbf{x})}
\end{aligned}
\]</span></p>
<p>Where the proposed distribution is obtained by the discriminative
KGLM, i.e., training another KGLM to judge the annotation of the current
token</p></li>
<li><p>The results are impressive. KGLM, using only LSTM with a small
number of parameters, shows a significant advantage in entity word
prediction compared to the large-scale GPT-2 model.</p></li>
</ul>
<h1
id="dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</h1>
<ul>
<li><p>The authors propose a new task of dialogue generation based on
dynamic knowledge graphs, aiming to capture relationships and generalize
knowledge graph-based dialogue generation to zero-shot
scenarios</p></li>
<li><p>The task description is divided into two steps:</p>
<ul>
<li><p>For each dialogue turn t, given input x and graph K, generate the
correct answer y that includes the correct knowledge graph
entities</p></li>
<li><p>When the knowledge graph is updated (only relations and
recipients can be updated), the answer y should be correspondingly
modified</p></li>
</ul></li>
<li><p>To effectively measure the quality of dynamic knowledge graph
dialogue, the authors propose two types of metrics:</p>
<ul>
<li><p>Knowledge Entity Modeling: including accuracy of predicting known
entities, entity word hit rate; true positive/false negative/false
positive rates for distinguishing predicted entities from generic words;
true positive/false negative/false positive rates for all entities in
the knowledge graph</p></li>
<li><p>Graph Adaptability: The authors propose three methods of changing
the graph, including shuffling and randomly replacing entities,
observing whether the generated sequence is replaced and replaced
correctly</p></li>
</ul></li>
<li><p>The authors create a parallel corpus containing dialogue data
from two TV series in Chinese and English, with detailed
processing</p></li>
<li><p>The proposed Qadpt model modifies the seq2seq approach. First,
the decoder's current state <span class="math inline">\(d_t\)</span>
generates a controller <span class="math inline">\(c_t\)</span> to
decide whether to select entities from the KG or generate general
vocabulary. Similar to the copy mechanism, this selection is not hard
but calculates probabilities separately, then concatenates two
vocabulary lists and selects based on probability:</p>
<p><span class="math display">\[
\begin{aligned} P\left(\{K B, \mathcal{W}\} | y_{1} y_{2} \ldots
y_{t-1}, \mathbf{e}(x)\right)
\\=\operatorname{softmax}\left(\phi\left(\mathbf{d}_{t}\right)\right) \\
\mathbf{w}_{t}=P\left(\mathcal{W} | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ c_{t}=P\left(K B | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ \mathbf{o}_{t}=\left\{c_{t} \mathbf{k}_{t} ;
\mathbf{w}_{t}\right\} \end{aligned}
\]</span></p></li>
<li><p>Regarding how to generate entity candidate lists, they perform
reasoning on the knowledge graph. Unlike typical attention-based graph
embedding approaches, the authors use multi-hop reasoning</p>
<ul>
<li>First, combine path matrix and adjacency matrix into a transition
matrix, where the path matrix represents the probability of selecting
each relation for each entity learned from <span
class="math inline">\(d_t\)</span>, then select recipient nodes based on
probability:</li>
</ul>
<p><span class="math display">\[
\begin{aligned} \mathbf{R}_{t}
&amp;=\operatorname{softmax}\left(\theta\left(\mathbf{d}_{t}\right)\right)
\\ \mathbf{A}_{i, j, \gamma} &amp;=\left\{\begin{array}{ll}{1,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \in \mathcal{K}} \\ {0,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \notin
\mathcal{K}}\end{array}\right.\\ \mathbf{T}_{t}=\mathbf{R}_{t}
\mathbf{A} \end{aligned}
\]</span></p>
<ul>
<li>Then take an initial vector <span class="math inline">\(s\)</span>
(uniformly distributed?), transform n times using the transition matrix
to obtain the probability of each entity appearing, and provide it to
the controller for calculation. Here, cross-entropy is calculated using
one-hot ground truth as an auxiliary loss</li>
</ul></li>
</ul>
<h1
id="graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</h1>
<ul>
<li><p>As the name suggests, the input is graph-structured data,
generating a sequence</p></li>
<li><p>Previous approaches encoded graphs into fixed-length sequences
for Seq2Seq, which the authors believe leads to information loss.
Encoding graph to encoder sequence introduces an additional layer of
information loss</p></li>
<li><p>A more natural approach would be for the decoder to perform
attention on the encoded graph nodes, directly utilizing graph
information</p></li>
<li><p>First, the graph encoder, referencing GraphSage's approach.
Notably, the authors handle directed graphs by distinguishing neighbor
nodes in two directions, performing Aggregate and Update operations, and
concatenating after k hops</p></li>
<li><p>The authors tried Mean, LSTM, and Pooling methods. Since
neighbors are unordered, LSTM has no temporal effect, so the authors
randomly arrange neighbors and use LSTM Aggregate</p></li>
<li><p>The authors believe that in addition to node embedding, graph
embedding should be passed to the decoder. They adopted two methods to
obtain graph embedding</p>
<ul>
<li><p>Pooling-based: First pass all node embeddings through a fully
connected layer, then perform element-wise max, min, average pooling.
The authors found the three methods performed similarly and used max
pooling as the default pooling method</p></li>
<li><p>Node-based: Add a super node to the graph, connected to all other
nodes, using the embedding of this node after graph encoding as the
graph embedding</p></li>
</ul></li>
<li><p>Attention-based decoder: Graph embedding is input as the initial
state of the decoder, and at each step, the decoder generates attention
on all node embeddings and uses this as the decoder's hidden state for
that time step</p></li>
<li><p>Focusing on NLG tasks, the authors tested SQL2Text task, first
building a graph from SQL Query, then using Graph2Seq. The effect was
significantly better than Seq2seq from SQL Query to Text</p></li>
<li><p>In the Aggregate comparison experiment, they found Mean Pooling
performed best, and for Graph Embedding, Pooling-based significantly
outperformed Node-based</p></li>
</ul>
<h1
id="graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</h1>
<ul>
<li>Google-produced, experimental and visualization results are as rich
as ever</li>
<li>Two contributions:
<ul>
<li>Proved that GNN can generate graph embeddings for similarity
calculation</li>
<li>Proposed attention-based Graph Matching Networks, surpassing
baselines</li>
</ul></li>
</ul>
<h2 id="graph-embedding-model">Graph Embedding Model</h2>
<ul>
<li><p>Baseline: Graph Embedding Model, a simple
encode-propagation-aggregate model</p></li>
<li><p>Encode: Encode point and edge features through MLP to get
embeddings</p></li>
<li><p>Propagation: Transmit center point, adjacent point, and adjacent
edge embeddings to the next layer's center point embedding</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right) \\ \mathbf{h}_{i}^{(t+1)} &amp;=f_{\text {node
}}\left(\mathbf{h}_{i}^{(t)}, \sum_{j:(j, i) \in E} \mathbf{m}_{j
\rightarrow i}\right) \end{aligned}
\]</span></p></li>
<li><p>Aggregate: The author uses a gating method to weight and sum the
embeddings of each node to obtain the final graph embedding</p>
<p><span class="math display">\[
\mathbf{h}_{G}=\operatorname{MLP}_{G}\left(\sum_{i \in V}
\sigma\left(\operatorname{MLP}_{\operatorname{gate}}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\odot \operatorname{MLP}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\]</span></p></li>
</ul>
<h2 id="graph-matching-networks">Graph Matching Networks</h2>
<ul>
<li><p>GMN does not separately generate embeddings for two graphs and
then match them, but directly accepts two graphs as input to output a
similarity score</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right), \forall(i, j) \in E_{1} \cup E_{2} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=f_{\text {match }}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right) \\ \forall i \in V_{1}, j &amp; \in V_{2},
\text { or } i \in V_{2}, j \in V_{1} \\ \mathbf{h}_{i}^{(t+1)}
&amp;=f_{\text {node }}\left(\mathbf{h}_{i}^{(t)}, \sum_{j}
\mathbf{m}_{j \rightarrow i}, \sum_{j^{\prime}} \mu_{j^{\prime}
\rightarrow i}\right) \\ \mathbf{h}_{G_{1}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{1}}\right) \\ \mathbf{h}_{G_{2}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{2}}\right) \\ s &amp;=f_{s}\left(\mathbf{h}_{G_{1}},
\mathbf{h}_{G_{2}}\right) \end{aligned}
\]</span></p></li>
<li><p>From the above formula, GMN made two modifications in the
propagation phase</p>
<ul>
<li><p>Since a pair of graphs is input at once, the first step of
neighborhood nodes is found from the range of two graphs. However,
generally, there are no node connections between two graphs, unless the
same nodes in both graphs share neighborhoods?</p></li>
<li><p>In addition to neighborhood information transmission, the authors
also calculate the match between two graphs, using a simple attention
mechanism, weighting the difference between two node embeddings by their
distance:</p>
<p><span class="math display">\[
\begin{aligned} a_{j \rightarrow i} &amp;=\frac{\exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right)\right)}{\sum_{j^{\prime}} \exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j^{\prime}}^{(t)}\right)\right)} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right) \end{aligned}
\]</span></p></li>
<li><p>When updating to the next layer of node embeddings, the match
part actually calculates the weighted distance of a graph's node with
all nodes of b graph:</p>
<p><span class="math display">\[
\sum_{j} \boldsymbol{\mu}_{j \rightarrow i}=\sum_{j} a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right)=\mathbf{h}_{i}^{(t)}-\sum_{j}
a_{j \rightarrow i} \mathbf{h}_{j}^{(t)}
\]</span></p></li>
<li><p>This calculation increases complexity to <span
class="math inline">\(O(V(G_1)V(G_2))\)</span>, but this point-by-point
comparison can distinguish subtle changes and is more interpretable. So
the algorithm's use case should be small graphs with high precision
requirements</p></li>
</ul></li>
<li><p>For such matching problems, pair or triplet loss can be used,
with the former comparing similarity and dissimilarity, and the latter
comparing which is more similar to two candidates. The authors provide
margin loss in both forms:</p>
<p><span class="math display">\[
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2}, t\right)}\left[\max
\left\{0, \gamma-t\left(1-d\left(G_{1},
G_{2}\right)\right)\right\}\right] \\
L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\max \left\{0, d\left(G_{1},
G_{2}\right)-d\left(G_{1}, G_{3}\right)+\gamma\right\}\right] \\
\]</span></p></li>
<li><p>The authors specifically mention that to accelerate computation,
graph embeddings can be binarized, using Hamming distance instead,
sacrificing some Euclidean space. The specific method is to pass the
entire vector through tanh and use average inner product as the graph
similarity during training, designing a loss that pushes the Hamming
distance of positive sample pairs towards 1 and negative sample pairs
towards -1. This loss design is more stable than margin loss when used
for retrieval:</p>
<p><span class="math display">\[
s\left(G_{1}, G_{2}\right)=\frac{1}{H} \sum_{i=1}^{H} \tanh
\left(h_{G_{1} i}\right) \cdot \tanh \left(h_{G_{2} i}\right) \\
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2},
t\right)}\left[\left(t-s\left(G_{1}, G_{2}\right)\right)^{2}\right] / 4
\\
\begin{aligned} L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\left(s\left(G_{1},
G_{2}\right)-1\right)^{2}+\right.\\\left.\left(s\left(G_{1},
G_{3}\right)+1\right)^{2}\right] / 8 \end{aligned} \\
\]</span></p>
<p>Dividing by 4 or 8 is to constrain the loss range to the [0,1]
interval.</p></li>
</ul>
<h1 id="learning-to-update-knowledge-graphs-by-reading-news">Learning to
Update Knowledge Graphs by Reading News</h1>
<ul>
<li>An EMNLP 2019 work, the author is definitely a basketball fan, using
a very appropriate NBA player transfer example to illustrate the problem
this paper aims to solve: knowledge graph updating</li>
<li>For example, when a player transfers clubs, the player graphs of the
two related clubs will change. The authors highlight two key points, as
shown in Figure 1:
<ul>
<li>Knowledge graph updates only occur in the text subgraph, not the
1-hop subgraph</li>
<li>Traditional methods cannot extract hidden knowledge graph update
information from text, such as a player's teammates changing after a
transfer, which is not explicitly mentioned in the text but can be
inferred</li>
</ul></li>
<li>The overall structure is an encoder based on R-GCN and GAT, and a
decoder based on DistMult, essentially modifying RGCN to an
attention-based approach, with the decoder remaining unchanged,
performing link prediction tasks</li>
</ul>
<h2 id="encoder">Encoder</h2>
<ul>
<li><p><strong>Encoder: RGCN+GAT = RGAT.</strong> In RGCN, the forward
propagation process is as follows:</p>
<p><span class="math display">\[
\mathbf{H}^{l+1} = \sigma\left(\sum_{r \in \mathbf{R}}
\hat{\mathbf{A}}_{r}^{l} \mathbf{H}^{l} \mathbf{W}_{r}^{l}\right)
\]</span></p></li>
<li><p>This means assigning a parameter matrix to each type of
heterogeneous edge, calculating independently, summing the results, and
then applying an activation function. By replacing the adjacency matrix
with an attention matrix, the attention calculation becomes:</p>
<p><span class="math display">\[
a_{ij}^{lr} = \left\{
\begin{array}{ll}
\frac{\exp \left(\text{att}^{lr}\left(\mathbf{h}_i^l,
\mathbf{h}_j^l\right)\right)}{\sum_{k \in \mathcal{N}_i^r} \exp
\left(\text{att}^{lr}\left(\mathbf{h}_i^l,
\mathbf{h}_k^l\right)\right)}, &amp; j \in \mathcal{N}_i^r \\
0, &amp; \text{otherwise}
\end{array}
\right.
\]</span></p></li>
<li><p>The attention function <span
class="math inline">\(\text{attn}\)</span> is computed based on the
text:</p>
<ul>
<li><p>First, a bidirectional GRU encodes the sequence as <span
class="math inline">\(u\)</span>.</p></li>
<li><p>Sequence attention is then used to obtain the contextual
representation:</p>
<p><span class="math display">\[
b_t^{lr} = \frac{\exp \left(\mathbf{u}_t^T
\mathbf{g}_{\text{text}}^{lr}\right)}{\sum_{k=1}^{|S|} \exp
\left(\mathbf{u}_k^T \mathbf{g}_{\text{text}}^{lr}\right)} \\
\mathbf{c}^{lr} = \sum_{t=1}^{|S|} b_t^{lr} \mathbf{u}_t
\]</span></p></li>
<li><p>When applying attention, a trainable guidance vector <span
class="math inline">\(g\)</span> incorporates this contextual
representation through simple linear interpolation:</p>
<p><span class="math display">\[
\mathbf{g}_{\text{fin}}^{lr} = \alpha^{lr}
\mathbf{g}_{\text{graph}}^{lr} + \left(1-\alpha^{lr}\right)
\mathbf{U}^{lr} \mathbf{c}^{lr} \\
\text{att}^{lr}(\mathbf{h}_i^l, \mathbf{h}_j^l) =
\mathbf{g}_{\text{fin}}^{lr}[\mathbf{h}_i^l \| \mathbf{h}_j^l]
\]</span></p></li>
</ul></li>
<li><p>In practical applications, aimed at the task of KG updates, the
authors incorporated several techniques:</p>
<ul>
<li>The number of parameters in RGCN/RGAT increases linearly with the
number of edge (relation) types. To reduce parameters, the authors used
<strong>basis decomposition</strong>, where <span
class="math inline">\(k\)</span> relation types share <span
class="math inline">\(b\)</span> parameter sets (<span
class="math inline">\(b &lt; k\)</span>), and these <span
class="math inline">\(k\)</span> parameters are linear combinations of
the <span class="math inline">\(b\)</span> sets.</li>
<li>In sparse relational datasets, messages cannot aggregate effectively
in one or two layers of RGAT. Thus, the authors added an artificial
"SHORTCUT" relation between all entities in the graph. Using an
information extraction tool, the "SHORTCUT" relation was refined into
<strong>add</strong>, <strong>delete</strong>, and
<strong>other</strong> categories to preliminarily capture transfer
relationships (e.g., an individual leaving one club and joining
another).</li>
</ul></li>
</ul>
<h2 id="decoder">Decoder</h2>
<ul>
<li><p>The paper <em>Embedding Entities and Relations for Learning and
Inference in Knowledge Bases</em> summarized the task of learning
relation embeddings in KGs. The method involves combining different
linear/bilinear parameter matrices with various scoring functions to
compute the <strong>margin triplet loss</strong>:</p>
<figure>
<img data-src="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg"
alt="Knowledge Graph Embedding" />
<figcaption aria-hidden="true">Knowledge Graph Embedding</figcaption>
</figure></li>
<li><p><strong>DistMult</strong> is the simplest approach, where
bilinear parameter matrices are replaced with diagonal matrices. The
final score is obtained by element-wise multiplication of two entity
embeddings, weighted by a relation-specific parameter. The formula used
is:</p>
<p><span class="math display">\[
P(y) = \operatorname{sigmoid}\left(\mathbf{h}_i^T \left(\mathbf{r}_k
\circ \mathbf{h}_j\right)\right)
\]</span></p></li>
</ul>
<h2 id="results">Results</h2>
<ul>
<li>Comparing several baselines (RGCN, PCNN), the authors used a
GRU-based network to extract semantic similarities, which is
data-intensive. While the dataset size was small, the final results were
impressive. Notably, RGAT doubled the accuracy of RGCN in small-sample
classes like <strong>add</strong> and <strong>delete</strong>.</li>
<li>A highlight of the paper was how it framed the link prediction
problem, emphasizing continual learning and updates. By simplifying the
task to link prediction, the model achieved high performance without
extensive modifications.</li>
</ul>
<h1
id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</h1>
<ul>
<li><p>This study also focused on <strong>link prediction</strong>,
again using GAT.</p></li>
<li><p>The authors emphasized the importance of relations in KGs, but
direct feature assignment to edges is challenging. To address this, they
incorporated edge features into node features indirectly.</p></li>
<li><p>A diagram explains this approach effectively:</p>
<figure>
<img data-src="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png"
alt="Attention-based Embeddings" />
<figcaption aria-hidden="true">Attention-based Embeddings</figcaption>
</figure></li>
<li><p>From left to right:</p>
<ul>
<li><p><strong>Input:</strong> Node features are input into GAT. Each
node feature is derived via self-attention over triplet features, which
are created by concatenating node and relation features. Green indicates
relation features, and yellow indicates node features:</p>
<p><span class="math display">\[
c_{ijk} = \mathbf{W}_1\left[\vec{h}_i \| \vec{h}_j \| \vec{g}_k\right]
\\
\begin{aligned}
\alpha_{ijk} &amp;= \operatorname{softmax}_{jk}\left(b_{ijk}\right) \\
&amp;= \frac{\exp \left(b_{ijk}\right)}{\sum_{n \in \mathcal{N}_i}
\sum_{r \in \mathcal{R}_{in}} \exp \left(b_{inr}\right)}
\end{aligned} \\
\overrightarrow{h_i&#39;} = \|_{m=1}^M \sigma\left(\sum_{j \in
\mathcal{N}_i} \alpha_{ijk}^m c_{ijk}^m\right)
\]</span></p></li>
<li><p><strong>Intermediate Layer:</strong> GAT computes multi-head
attention, concatenating the results. Features (6-dimensional gray) are
transformed and concatenated with relation features (green) to form
triplet representations for each node.</p></li>
<li><p><strong>Final Layer:</strong> Average pooling is applied instead
of concatenation. Input node features are incorporated again,
concatenated with relation features, and loss is calculated based on the
triplet distance: subject + predicate - object. Negative sampling
involves randomly replacing the subject or object.</p></li>
</ul></li>
<li><p><strong>Decoder:</strong> The decoder uses
<strong>ConvKB</strong>:</p>
<p><span class="math display">\[
f\left(t_{ij}^k\right) = \left(\prod_{m=1}^\Omega
\operatorname{ReLU}\left(\left[\vec{h}_i, \vec{g}_k, \vec{h}_j\right] *
\omega^m\right)\right) \mathbf{. W}
\]</span></p></li>
<li><p><strong>Loss:</strong> Soft-margin loss is used (although the
values for 1 and -1 seem reversed):</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L} &amp;= \sum_{t_{ij}^k \in \{S \cup S&#39;\}} \log \left(1 +
\exp \left(l_{t_{ij}^k} * f\left(t_{ij}^k\right)\right)\right) +
\frac{\lambda}{2}\|\mathbf{W}\|_2^2 \\
\text{where } l_{t_{ij}^k} &amp;=
\begin{cases}
1 &amp; \text{for } t_{ij}^k \in S \\
-1 &amp; \text{for } t_{ij}^k \in S&#39;
\end{cases}
\end{aligned}
\]</span></p></li>
<li><p>Additional improvements included adding edges between 2-hop
nodes.</p></li>
<li><p>The results were excellent, achieving SOTA performance on
FB15K-237 and WN18RR. The authors avoided integrating edge features
directly into GAT's message passing, focusing instead on encoding these
features effectively and ensuring that initial features influenced every
model layer for robust gradient propagation.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</h1>
<ul>
<li><p>研究：跨语言知识图谱中实体的对齐</p></li>
<li><p>一般的做法，在各自语言的知识图谱中投影到低维向量，然后学习到一个相似度计算函数</p></li>
<li><p>问题在于，上面的做法依赖于相同实体在不同语言知识图谱下的邻域结构相同这一假设，但这一假设并不总是成立，因此传统的做法对少对齐邻域节点/少邻域节点的实体不友好</p></li>
<li><p>作者提出主题实体图来编码实体节点的上下文信息，将节点embedding之间的Match转换为主题实体图之间的图Match</p></li>
<li><p>这里的主题实体指的是需要对齐的实体，主题实体图即主题实体周围一跳邻域实体与本体一同构建的子图，假如在原知识图谱里这两个实体没有直接相连的话就在知识图谱里添加一条边</p></li>
<li><p>得到主题实体图之后，经过四层网络计算出两个主题实体图之间的相似度：</p>
<ul>
<li><p>输入表示层：对主题实体图中的每一个实体学习到embedding。首先用word-level
lstm学习到初始embedding，然后因为是有向图，因此需要将邻域节点区分为输入和输出邻居，分别做聚合（FFN+mean
pooling），拼接到上一次得到的实体embedding并更新（FFN），迭代K次</p></li>
<li><p>节点级别的局部匹配层：两个图互相匹配所有实体节点，这里用了基于注意力的Matching，即先用cosine距离得到主题实体图1里某个节点i和主题实体图2所有节点之间的相似度，用这个相似度作为注意力权重去加权主题实体图2所有节点得到图2的graph
embedding，然后用这个graph embedding和图1的query entity
embedding之间计算一个多角度cosine距离作为local match
score，这个多角度是指l个角度用l个加权向量(d维，同embedding维度）表示。一个角度下的cosine距离就用一个加权向量（逐元素相乘）加权两个embedding再计算cosine距离，l个角度合在一起就是一个<span
class="math inline">\(W \in R^{l*d}\)</span>的矩阵，如下：</p>
<p><span class="math display">\[
score_{perspective_k} = cosine(W_k \cdot embedding_1, W_k \cdot
embedding_2)
\]</span></p></li>
<li><p>全局匹配层：局部匹配层存在着之前提到的对少共现邻居节点不友好的问题，这里就需要全局匹配。具体做法是，再用一个GCN将局部的match
embedding（上一层多角度cosine
score得到的向量）传递到整个主题实体图，之后在整个图上做一个FFN +
max/mean pooling，得到两个图的graph matching vector</p></li>
<li><p>预测层:将两个主题实体图的graph matching
vector拼接送入一个softmax预测</p></li>
</ul></li>
<li><p>训练时，对每一个正样本启发式的生成了20个负样本，两个方向各十个，这里直接基于word-level
average embedding作为实体的特征向量，匹配十个最相似的实体</p></li>
</ul>
<h1
id="baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling">Barack’s
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</h1>
<ul>
<li><p>作者希望在语言模型当中维持一个local knowledge
graph来管理已经探测到的事实，并利用该图谱来查询未知的事实用于文本生成，称之为KGLM(Knowledge
Graph Language Model)</p></li>
<li><p>假设实体集合为<span
class="math inline">\(\xi\)</span>，则KGLM要预测的是</p>
<p><span class="math display">\[
p(x_t,\xi _t|x_{1,t-1},\xi_ {1,t-1})
\]</span></p></li>
<li><p>生成下一个词的流程可以拆分如下：</p>
<ul>
<li><p>下一个词不是实体：那就正常词典范围上计算概率</p></li>
<li><p>下一个词是全新实体：在正常词典以及所有实体范围上计算概率</p></li>
<li><p>下一个词是与已经看见的实体相关的实体：先挑出一个已经看见的实体作为父节点，再挑出一个子节点，之后在正常词典以及该子节点的所有别名上计算概率</p></li>
</ul></li>
<li><p>作者使用LSTM作为LM的基础模型，所有的挑选：挑新实体、挑父节点、挑子节点，均利用LSTM的隐状态（切分为三部分），并加上实体和关系的预训练embedding作为输入依赖，之后通过softmax计算概率</p></li>
<li><p>为了实现这样的模型数据集应该提供实体信息，作者提出了Linked
WikiText2数据集，该数据集的构建流程如下：</p>
<ul>
<li><p>根据维基百科上的链接创造实体之间的链接，借助neural-el来识别wikidata数据库中额外的链接，使用stanford
corenlp来做共指消解。</p></li>
<li><p>构造local knowledge graph：接下来需要建立实体之间的parents
relation，每遇到一个实体a，将其在wikidata中相关联的所有实体{b}加入matching的候选，加入相关联的某一实体b在之后的文段中出现了，则将实体a作为实体b的父节点</p></li>
<li><p>以上的做法只是构建了初始集合，需要不断扩展，作者还对日期、量词等做了alias
table</p></li>
<li><p>以下是一句话在Linked WikiText2中的表示，相比WikiText使用api
query构造的方法，Linked
WikiText2直接对原始html操作，保留了更多链接信息： <img data-src="https://s2.ax1x.com/2019/11/14/MY7je0.jpg"
alt="MY7je0.jpg" /></p></li>
</ul></li>
<li><p>Train and
Inference：首先对实体和关系使用TransE算法做一个Pretraining，给定三元组(p,r,e)，目标是最小化距离：</p>
<p><span class="math display">\[
\delta\left(\mathbf{v}_{p}, \mathbf{v}_{r},
\mathbf{v}_{e}\right)=\left\|\mathbf{v}_{p}+\mathbf{v}_{r}-\mathbf{v}_{e}\right\|^{2}
\]</span></p>
<p>采用Hinge Loss，使得正负样本得分之差不超过<span
class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}=\max \left(0, \gamma+\delta\left(\mathbf{v}_{p},
\mathbf{v}_{r},
\mathbf{v}_{e}\right)-\delta\left(\mathbf{v}_{p}^{\prime},
\mathbf{v}_{r}, \mathbf{v}_{e}^{\prime}\right)\right)
\]</span></p></li>
<li><p>虽然整个过程是生成式的，但是所有变量均可见，因此可以端到端的直接训练，对于有多个父节点的实体节点，需要对概率做归一化</p></li>
<li><p>在推理过程中，我们没有标注信息，我们希望计算的也是单词<span
class="math inline">\(x\)</span>在实体<span
class="math inline">\(\xi\)</span>求和得到的边缘概率，而不是联合概率（我们只希望得到词，词的实体信息被marginalize了），然而实体太多，不可能对所有实体计算联合概率再求和，因此作者采用了重要性采样:</p>
<p><span class="math display">\[
\begin{aligned} p(\mathbf{x}) &amp;=\sum_{\mathcal{E}} p(\mathbf{x},
\mathcal{E})=\sum_{\mathcal{E}} \frac{p(\mathbf{x},
\mathcal{E})}{q(\mathcal{E} | \mathbf{x})} q(\mathcal{E} | \mathbf{x})
\\ &amp; \approx \frac{1}{N} \sum_{\mathcal{E} \sim q}
\frac{p(\mathbf{x}, \mathcal{E})}{q(\mathcal{E} | \mathbf{x})}
\end{aligned}
\]</span></p>
<p>其中proposed
distribution使用判别式的KGLM得到，即另外训练一个KGLM判断当前token的annotation</p></li>
<li><p>结果非常漂亮，KGLM仅仅用了LSTM，参数量也不大，和超大规模的GPT-2模型相比，在实体词的预测上有着明显优势。</p></li>
</ul>
<h1
id="dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</h1>
<ul>
<li><p>本文作者提出了一个新的任务，动态知识图谱对话生成，也就是希望抓住图谱中的关系，来将基于知识图谱的对话生成推广到zero-shot</p></li>
<li><p>任务的详细描述分为两步：</p>
<ul>
<li><p>每轮对话t，给定输入x和图谱K，希望生成正确的回答y，而且包含正确的知识图谱实体</p></li>
<li><p>当知识图谱更新之后（这里只可能更新关系和受体），回答y能够相应更改回答。</p></li>
</ul></li>
<li><p>为了有效衡量动态知识图谱对话的质量，作者提出了两类指标：</p>
<ul>
<li><p>知识实体建模：包括已知要预测实体，实体词命中的准确率；判别要预测实体还是通用词的TP;整个知识图谱所有实体的TP</p></li>
<li><p>图自适应：作者提出了三种改变图的方法，包括shuffle和随即替换实体，观察生成的序列是否替换且替换正确</p></li>
</ul></li>
<li><p>作者提出了一个平行语料库，包含中英两个电视剧的语料，并做了详细的处理</p></li>
<li><p>作者提出的模型Qadpt在seq2seq的基础上修改，首先将decoder的当前状态<span
class="math inline">\(d_t\)</span>生成一个controller<span
class="math inline">\(c_t\)</span>来决定是从KG里挑实体还是从generic
vocab里生成一般词汇。和copy
mechanism一样这个选择不是hard，而是分别计算概率，最后将两部分词表拼到一起，最后依概率选择：</p>
<p><span class="math display">\[
\begin{aligned} P\left(\{K B, \mathcal{W}\} | y_{1} y_{2} \ldots
y_{t-1}, \mathbf{e}(x)\right)
\\=\operatorname{softmax}\left(\phi\left(\mathbf{d}_{t}\right)\right) \\
\mathbf{w}_{t}=P\left(\mathcal{W} | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ c_{t}=P\left(K B | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ \mathbf{o}_{t}=\left\{c_{t} \mathbf{k}_{t} ;
\mathbf{w}_{t}\right\} \end{aligned}
\]</span></p></li>
<li><p>至于如何产生实体候选列表，就是在知识图谱上做reasoning，不同于一般的attention
based graph embedding的做法，作者采用了multi-hop reasoning</p>
<ul>
<li>首先将path matrix和adjacency matrix合成transition matrix，其中的path
matrix是指用<span
class="math inline">\(d_t\)</span>学习到的每个实体选择每一种关系的概率，之后依概率选择受体节点：</li>
</ul>
<p><span class="math display">\[
\begin{aligned} \mathbf{R}_{t}
&amp;=\operatorname{softmax}\left(\theta\left(\mathbf{d}_{t}\right)\right)
\\ \mathbf{A}_{i, j, \gamma} &amp;=\left\{\begin{array}{ll}{1,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \in \mathcal{K}} \\ {0,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \notin
\mathcal{K}}\end{array}\right.\\ \mathbf{T}_{t}=\mathbf{R}_{t}
\mathbf{A} \end{aligned}
\]</span></p>
<ul>
<li>之后取一个初始向量<span
class="math inline">\(s\)</span>（均匀分布？），用transition
matrix做n次transform，得到每个实体出现的概率并提供给controller计算，这里会使用one
hot ground truth计算一个交叉熵作为辅助损失</li>
</ul></li>
</ul>
<h1
id="graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</h1>
<ul>
<li><p>顾名思义，输入为图结构组织的数据，生成的是序列</p></li>
<li><p>以往的做法，将图编码成固定长度的序列，再用Seq2Seq，作者认为这样存在信息丢失，本身Enc
Seq 2 Dec Seq就存在信息丢失，现在Graph 2 Enc
Seq会再丢失一层信息</p></li>
<li><p>因此比较自然的做法应该是，解码器在编码的图节点上做attention，直接利用图的信息</p></li>
<li><p>首先是图编码器，参考了GraphSage的做法，值得注意的是作者处理的是有向图，因此将邻居节点按两个方向做了区分，分别做Aggregate和Update的操作，做了k跳之后再拼接回来</p></li>
<li><p>作者试了Mean、LSTM、Pooling三种，由于邻居是无序的，因此LSTM没有时序上的效果，作者直接随机排列邻居用LSTM
Aggregate</p></li>
<li><p>作者认为传给解码器的不只node embedding还需要graph
embedding。作者采用了两种方法获取graph embedding</p>
<ul>
<li><p>Pooling-based：先将所有的node
embedding经过一个全连接层，然后逐元素做max、min、average
pooling，作者发现三种方法的实际效果相差不大，就使用max
pooling作为默认的池化方法</p></li>
<li><p>Node-based：在图中加入一个超级节点，该节点与图中其他所有节点相连，用该节点经过图编码之后的embedding作为Graph
embedding</p></li>
</ul></li>
<li><p>基于注意力的解码器：graph
embedding作为解码器的初始状态输入，之后decoder每一步生成在所有node
embedding上的attention并加权作为该时间步decoder的隐状态</p></li>
<li><p>重点关注NLG task，作者测试了SQL2Text任务，首先将SQL
Query建图，然后使用Graph2Seq。效果显著好于SQL
Query到Text的Seq2seq</p></li>
<li><p>另外在Aggregate的比对实验中发现，Mean
Pooling的效果最好，对于Graph Embedding，Pooling based的效果显著好于Node
based</p></li>
</ul>
<h1
id="graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</h1>
<ul>
<li>google出品，实验和可视化结果一如既往的丰富</li>
<li>两点贡献：
<ul>
<li>证明了GNN可以产生用于相似度计算的graph embedding</li>
<li>提出了attention based的Graph Matching
Networks，并超越了baseline</li>
</ul></li>
</ul>
<h2 id="graph-embedding-model">Graph Embedding Model</h2>
<ul>
<li><p>baseline:Graph Embedding
Model，一个简单的encode-propagation-aggregate模型</p></li>
<li><p>encode：将点和边的特征通过MLP编码得到embedding</p></li>
<li><p>proprgation：将中心点，邻接点，邻接边的embedding传递到下一层的中心点embedding</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right) \\ \mathbf{h}_{i}^{(t+1)} &amp;=f_{\text {node
}}\left(\mathbf{h}_{i}^{(t)}, \sum_{j:(j, i) \in E} \mathbf{m}_{j
\rightarrow i}\right) \end{aligned}
\]</span></p></li>
<li><p>aggregate：作者用门控的方式将各个节点的embedding加权求和得到最后的graph
embedding</p>
<p><span class="math display">\[
\mathbf{h}_{G}=\operatorname{MLP}_{G}\left(\sum_{i \in V}
\sigma\left(\operatorname{MLP}_{\operatorname{gate}}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\odot \operatorname{MLP}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\]</span></p></li>
</ul>
<h2 id="graph-matching-networks">Graph Matching Networks</h2>
<ul>
<li><p>GMN并不像Baseline一样分别对两个图先生成embedding再match，而是接受两个图作为输入直接输出similarity
score。</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right), \forall(i, j) \in E_{1} \cup E_{2} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=f_{\text {match }}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right) \\ \forall i \in V_{1}, j &amp; \in V_{2},
\text { or } i \in V_{2}, j \in V_{1} \\ \mathbf{h}_{i}^{(t+1)}
&amp;=f_{\text {node }}\left(\mathbf{h}_{i}^{(t)}, \sum_{j}
\mathbf{m}_{j \rightarrow i}, \sum_{j^{\prime}} \mu_{j^{\prime}
\rightarrow i}\right) \\ \mathbf{h}_{G_{1}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{1}}\right) \\ \mathbf{h}_{G_{2}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{2}}\right) \\ s &amp;=f_{s}\left(\mathbf{h}_{G_{1}},
\mathbf{h}_{G_{2}}\right) \end{aligned}
\]</span></p></li>
<li><p>从上面的公式可以看到，在propagation阶段，GMN做出了两点改动</p>
<ul>
<li><p>因为一次性输入一对图，因此第一步的邻域节点是从两张图的范围内找。但是一般而言两张图之间是没有节点连接的，除非两张图里的相同节点共享邻域?</p></li>
<li><p>除了邻域信息的传递之外，作者还计算了两张图之间的match，这里用了一个最简单的attention机制，用待匹配两个节点embedding的距离加权两个节点embedding之间的差：</p>
<p><span class="math display">\[
\begin{aligned} a_{j \rightarrow i} &amp;=\frac{\exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right)\right)}{\sum_{j^{\prime}} \exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j^{\prime}}^{(t)}\right)\right)} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right) \end{aligned}
\]</span></p></li>
<li><p>这样在update到下一层节点embedding时，match的那部分实际上计算了a图某一结点与b图所有节点的加权距离：</p>
<p><span class="math display">\[
\sum_{j} \boldsymbol{\mu}_{j \rightarrow i}=\sum_{j} a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right)=\mathbf{h}_{i}^{(t)}-\sum_{j}
a_{j \rightarrow i} \mathbf{h}_{j}^{(t)}
\]</span></p></li>
<li><p>这样计算的复杂度就升到了<span
class="math inline">\(O(V(G_1)V(G_2))\)</span>，但正是这逐点的比较能够区分那些细微的变化，而且可视化更加具有可解释性。所以该算法的使用场景应该是小图且对区分精度要求高</p></li>
</ul></li>
<li><p>对于这样的匹配问题可以用pair 或者triplet
loss，前者比较相似不相似，后者比较和两个候选相比跟哪个更相似，作者分别给出了两种形式下的margin
loss：</p>
<p><span class="math display">\[
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2}, t\right)}\left[\max
\left\{0, \gamma-t\left(1-d\left(G_{1},
G_{2}\right)\right)\right\}\right] \\
L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\max \left\{0, d\left(G_{1},
G_{2}\right)-d\left(G_{1}, G_{3}\right)+\gamma\right\}\right] \\
\]</span></p></li>
<li><p>作者还特意提到，为了加速运算，可以对Graph
Embedding做二值化处理，这样在衡量距离的时候就是用汉明距离，牺牲掉了一些欧式空间的部分，具体做法是将整个向量过tanh并作平均内积用作训练时的图相似度，并设计损失将正样本对的汉明距离推向1，负样本对的汉明距离推向-1，假如在推断检索下使用汉明距离进行检索，这样的损失设计比margin
loss更加稳定：</p>
<p><span class="math display">\[
s\left(G_{1}, G_{2}\right)=\frac{1}{H} \sum_{i=1}^{H} \tanh
\left(h_{G_{1} i}\right) \cdot \tanh \left(h_{G_{2} i}\right) \\
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2},
t\right)}\left[\left(t-s\left(G_{1}, G_{2}\right)\right)^{2}\right] / 4
\\
\begin{aligned} L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\left(s\left(G_{1},
G_{2}\right)-1\right)^{2}+\right.\\\left.\left(s\left(G_{1},
G_{3}\right)+1\right)^{2}\right] / 8 \end{aligned} \\
\]</span></p>
<p>其中除以4或者除以8是为了约束损失的范围在[0,1]区间内。</p></li>
</ul>
<h1 id="learning-to-update-knowledge-graphs-by-reading-news">Learning to
Update Knowledge Graphs by Reading News</h1>
<ul>
<li>EMNLP2019的一项工作，作者肯定是个篮球迷，举了一个很恰当的NBA转会的例子来说明本文要解决的问题：知识图谱更新</li>
<li>比如发生了球员转俱乐部，则相关的两个俱乐部的球员图谱就会发生变化，作者提出了两个重点，如文中图1所示：
<ul>
<li>知识图谱的更新只发生在text subgraph而不是1-hop subgraph</li>
<li>传统方法不能从文本中获取隐藏的图谱更新信息，例如球员转会之后，这个球员的队友就会发生变化，这是文中没提但是可以推断出来的</li>
</ul></li>
<li>整体的结构是一个基于R-GCN和GAT的encoder和一个基于DistMult的decoder，基本上就是把RGCN改成了attention
based，decoder依然不变，做链接预测任务</li>
</ul>
<h2 id="encoder">encoder</h2>
<ul>
<li><p>encoder:RGCN+GAT=RGAT，在RGCN中前向过程为：</p>
<p><span class="math display">\[
\mathbf{H}^{l+1}=\sigma\left(\sum_{r \in \mathbf{R}}
\hat{\mathbf{A}}_{r}^{l} \mathbf{H}^{l} \mathbf{W}_{r}^{l}\right)
\]</span></p></li>
<li><p>即对异构的边分给予一个参数矩阵，独立的计算之后求和再激活。
将邻接矩阵改为注意力矩阵，注意力计算为：</p>
<p><span class="math display">\[
a_{i j}^{l r}=\left\{\begin{array}{ll}{\frac{\exp \left(a t t^{l
r}\left(\mathbf{h}_{i}^{l}, \mathbf{h}_{j}^{l}\right)\right)}{\sum_{k
\in \mathcal{N}_{i}^{r}} \exp \left(a t t^{l} r\left(\mathbf{h}_{i}^{l},
\mathbf{h}_{k}^{l}\right)\right)}} &amp; {, j \in \mathcal{N}_{i}^{r}}
\\ {0} &amp; {, \text { otherwise }}\end{array}\right.
\]</span></p></li>
<li><p>其中注意力函数<span
class="math inline">\(attn\)</span>基于文本计算</p>
<ul>
<li><p>首先用双向GRU对序列编码<span
class="math inline">\(u\)</span></p></li>
<li><p>再利用序列注意力得到上下文表示</p>
<p><span class="math display">\[
b_{t}^{l r}=\frac{\exp \left(\mathbf{u}_{t}^{T} \mathbf{g}_{t e x t}^{l
r}\right)}{\sum_{k=1}^{|S|} \exp \left(\mathbf{u}_{k}^{T} \mathbf{g}_{t
e x t}^{l r}\right)} \\
\mathbf{c}^{l r}=\sum_{t=1}^{|S|} b_{t}^{l r} \mathbf{u}_{t} \\
\]</span></p></li>
<li><p>之后利用注意力的时候，trainable guidance vector<span
class="math inline">\(g\)</span>就利用了这个上下文表示，利用一个简单的线性插值引入</p>
<p><span class="math display">\[
\mathbf{g}_{f i n}^{l r}=\alpha^{l r} \mathbf{g}_{g r a p h}^{l
r}+\left(1-\alpha^{l r}\right) \mathbf{U}^{l r} \mathbf{c}^{l r} \\
a t t^{l r}(\mathbf{h}_{i}^{l}, \mathbf{h}_{j}^{l}) =\mathbf{g}_{f i
n}^{l r}[\mathbf{h}_{i}^{l} | \mathbf{h}_{j}^{l}]  \\
\]</span></p></li>
</ul></li>
<li><p>在实际应用到作者想要完成的kg
update任务中，作者还引入了几个小技巧</p>
<ul>
<li>RGCN/RGAT中的参数量随着边(关系)的类别数量成线性增长，为了减少参数量，作者利用了basis-decomposition，也就是k类关系，存在k套参数，这k套参数用b套参数线性组合而成，而b小于k，这样来减少参数</li>
<li>实际数据集里实体之间的关系很稀疏，一两层的RGAT聚合不到消息，因此在构造数据集时首先对图中所有实体之间人为添加一个叫SHORTCUT的关系，并使用现成的信息抽取工具将SHORTCUT细化为add,delete和other，用来初步的判定人员的转会（从一个俱乐部delete，add到另一个俱乐部）关系</li>
</ul></li>
</ul>
<h2 id="decoder">decoder</h2>
<ul>
<li><p>在EMBEDDING ENTITIES AND RELATIONS FOR LEARNING AND INFERENCE IN
KNOWLEDGE
BASES一文中总结了知识图谱中的关系embedding学习问题，可以归结为不同的线性/双线性参数矩阵搭配不同的打分函数，计算margin
triplet loss： <a target="_blank" rel="noopener" href="https://imgchr.com/i/lzxTht"><img data-src="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg"
alt="lzxTht.md.jpg" /></a></p></li>
<li><p>DistMult即最简单的，将双线性参数矩阵换成对角阵，即最后的分数是两个实体embedding逐元素相乘并加权求和得到，权重与关系相关，在本文中的具体实现为：</p>
<p><span class="math display">\[
P(y)=\operatorname{sigmoid}\left(\mathbf{h}_{i}^{T}\left(\mathbf{r}_{k}
\circ \mathbf{h}_{j}\right)\right)
\]</span></p></li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>对比几个Baseline：RGCN,PCNN，感觉作者使用了GRU这样data-hungry的网络提取语义计算相似度，数据集偏小，当然最后结果还是很好看，可以看到数据集明显不平衡，但是在add和delete这些小样本类上RGAT比RGCN提升了一倍的准确率。</li>
<li>值得称赞的是论文将链接预测问题包装的很好，一个update突出了持续学习持续更新的想法，最后简化问题为链接预测，模型没有太多改进，但是效果达到了。</li>
</ul>
<h1
id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</h1>
<ul>
<li><p>依然是做链接预测，依然是基于GAT</p></li>
<li><p>作者认为在KG当中关系非常重要，但是又不好给边加特征，因此就曲线救国，将边的特征融入到节点的特征当中</p></li>
<li><p>一图胜千言 <a target="_blank" rel="noopener" href="https://imgchr.com/i/1SLmi6"><img data-src="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png"
alt="1SLmi6.md.png" /></a></p></li>
<li><p>从左到右</p>
<ul>
<li><p>输入，依然是节点特征输入GAT，只不过每个节点特征是与其相关的三元组特征做self
attention得到，而三元组特征由节点和关系特征拼接得到，绿色为输入关系特征，黄色为输入节点特征：</p>
<p><span class="math display">\[
c_{i j k}=\mathbf{W}_{1}\left[\vec{h}_{i}\left\|\vec{h}_{j}\right\|
\vec{g}_{k}\right] \\
\begin{aligned} \alpha_{i j k} &amp;=\operatorname{softmax}_{j
k}\left(b_{i j k}\right) \\ &amp;=\frac{\exp \left(b_{i j
k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}}
\exp \left(b_{i n r}\right)} \end{aligned} \\
\overrightarrow{h_{i}^{\prime}}=\|_{m=1}^{M} \sigma\left(\sum_{j \in
\mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right) \\
\]</span></p></li>
<li><p>之后经过GAT，得到灰色的中间层节点特征，两个3维灰色拼接是指GAT里multi-head
attention的拼接，之后两个6维灰色与绿色做变换之后拼接是指依然用三元组表示每个节点</p></li>
<li><p>最后一层，不做拼接了，做average
pooling，并且加入了输入节点特征，再拼接上关系特征，计算损失</p></li>
<li><p>损失依然用三元组距离，即subject+predicate-object，margin triplet
loss，负采样时随机替换subject或者object</p></li>
</ul></li>
<li><p>以上是encoder部分，decoder用ConvKB</p>
<p><span class="math display">\[
f\left(t_{i j}^{k}\right)=\left(\prod_{m=1}^{\Omega}
\operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k},
\vec{h}_{j}\right] * \omega^{m}\right)\right) \mathbf{. W} \\
\]</span></p></li>
<li><p>损失为soft-margin loss（好像1和-1写反了？）</p>
<p><span class="math display">\[
\begin{array}{l}{\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup
S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k}} *
f\left(t_{i
j}^{k}\right)\right)\right)+\frac{\lambda}{2}\|\mathbf{W}\|_{2}^{2}} \\
{\text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}{1} &amp;
{\text { for } t_{i j}^{k} \in S} \\ {-1} &amp; {\text { for } t_{i
j}^{k} \in S^{\prime}}\end{array}\right.}\end{array}
\]</span></p></li>
<li><p>另外作者还为2跳距离的节点之间加入了边</p></li>
<li><p>结果非常好，在FB15K-237和WN18RR上取得了SOTA。作者并没有试图将边的特征直接整合进GAT的message
passing，而是就把特征当成待训练的输入，用encoder专注于训练特征，并且在模型的每一层都直接输入的初始特征来保证梯度能够传递到原始输入。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/knowledge-graph/" rel="tag"># knowledge graph</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/10/30/heterogeneous/" rel="prev" title="Note for Heterogeneous Information Network">
                  <i class="fa fa-angle-left"></i> Note for Heterogeneous Information Network
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/11/15/hlda/" rel="next" title="Note for Hierarchical Latent Dirichlet Allocation">
                  Note for Hierarchical Latent Dirichlet Allocation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:51</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":null,"security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/11/13/kg/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
