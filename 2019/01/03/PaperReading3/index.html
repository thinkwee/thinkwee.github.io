<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Convolutional Sequence to Sequence Robust Unsupervised Cross-Lingual Word Embedding Mapping">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Reading 3">
<meta property="og:url" content="https://thinkwee.top/2019/01/03/PaperReading3/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Convolutional Sequence to Sequence Robust Unsupervised Cross-Lingual Word Embedding Mapping">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/03/11/APCWNT.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/03/13/Ak0cYd.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/03/11/APCWNT.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/03/13/Ak0cYd.png">
<meta property="article:published_time" content="2019-01-03T08:21:42.000Z">
<meta property="article:modified_time" content="2025-07-15T20:35:27.656Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="math">
<meta property="article:tag" content="abstractive summarization">
<meta property="article:tag" content="theory">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">


<link rel="canonical" href="https://thinkwee.top/2019/01/03/PaperReading3/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/01/03/PaperReading3/","path":"2019/01/03/PaperReading3/","title":"Paper Reading 3"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper Reading 3 | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">53</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#convolutional-sequence-to-sequence-learning"><span class="nav-number">1.</span> <span class="nav-text">Convolutional
Sequence to Sequence Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings"><span class="nav-number">2.</span> <span class="nav-text">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#model"><span class="nav-number">2.1.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#preprocessing"><span class="nav-number">2.2.</span> <span class="nav-text">Preprocessing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#completely-unsupervised-initialization"><span class="nav-number">2.3.</span> <span class="nav-text">Completely unsupervised
initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#robust-self-learning-process"><span class="nav-number">2.4.</span> <span class="nav-text">Robust self-learning process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#further-improving-results-through-reweighting-with-symmetric-weights"><span class="nav-number">2.5.</span> <span class="nav-text">Further
improving results through reweighting with symmetric weights</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#convolutional-sequence-to-sequence-learning"><span class="nav-number">3.</span> <span class="nav-text">Convolutional
Sequence to Sequence Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings"><span class="nav-number">4.</span> <span class="nav-text">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">4.2.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">4.3.</span> <span class="nav-text">完全无监督的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%B2%81%E6%A3%92%E7%9A%84%E8%87%AA%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-number">4.4.</span> <span class="nav-text">鲁棒的自学习过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E5%AF%B9%E7%A7%B0%E6%9D%83%E9%87%8D%E9%87%8D%E5%88%86%E9%85%8D%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E5%96%84%E7%BB%93%E6%9E%9C"><span class="nav-number">4.5.</span> <span class="nav-text">通过对称权重重分配进一步改善结果</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/01/03/PaperReading3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper Reading 3 | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Reading 3
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-01-03 16:21:42" itemprop="dateCreated datePublished" datetime="2019-01-03T16:21:42+08:00">2019-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 04:35:27" itemprop="dateModified" datetime="2025-07-16T04:35:27+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2019/01/03/PaperReading3/" class="post-meta-item leancloud_visitors" data-flag-title="Paper Reading 3" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>12 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li><p>Convolutional Sequence to Sequence</p></li>
<li><p>Robust Unsupervised Cross-Lingual Word Embedding Mapping</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="convolutional-sequence-to-sequence-learning">Convolutional
Sequence to Sequence Learning</h1>
<ul>
<li>Extremely straightforward, both the encoder and decoder use
sequence-to-sequence learning with convolutional neural networks</li>
<li>Whether using a transformer or a CNN as an encoder, it is necessary
to capture the semantic information of the entire sentence. Given the
current situation where both are significantly ahead of RNNs, tree
structures are more suitable as prior structures for natural language
data.</li>
<li>transformer directly models the so-called self-attention in the
first layer, personally, I believe this self-attention is modeling the
parse relationships of sentences, modeling all parse pairs (a word to
all other words in the sentence), all dimensions (it may be syntactic
parse, entity relationship, coreference resolution, or dependency
parse), and using the mechanism of attention to filter out unnecessary
relationships, and then reorganizes them through a layer of fully
connected layers. By iterating this parse + reorganization block
multiple layers, abstracting features step by step, and adding
structures commonly used in deep network design such as batch
normalization and residual, the transformer is formed. Therefore, the
transformer constructs the global parse relationships all at once and
then gradually reorganizes, abstracts, and filters.</li>
<li>The structure of CNN conforms more to the conventional 套路 of
syntactic parsing, still modeling across various dimensions, but it does
not construct the global relationships all at once. Instead, it first
analyzes local relationships (n-gram of kernel size) at the bottom
level, and then summarizes and abstracts these local relationships
through stacked layers.</li>
<li>Facebook uses a CNN block with a standard one-dimensional
convolution in this paper, but employs a gated linear unit, i.e.,
multi-convolves to double the channel as the input for the gate
structure, utilizing the gate structure to filter information and
construct non-linear relationships, similar to the gating design of
LSTM, and also achieving a similar effect of self-attention, while
giving up the pooling design. On the decoder side, CNN is also used (I
feel it's not very necessary), and the decoder still follows a process
of generating from left to right in word order. To ensure this order
relationship, the input to the decoder is masked, but I still haven't
understood how the specific code implements it... Doing the masking and
generating step by step in this way does not fully utilize the
acceleration of CNN.</li>
<li>In this paper, attention is also introduced, which is the
traditional encoder-decoder attention; the difference lies in
<ul>
<li>Employed multi-layer attention; although the key remains the output
of the encoder's last layer, attention is individually introduced to
each layer of the decoder. However, the authors themselves also say that
the decoder does not require many layers, two are sufficient, so this
multi-layer attention may not be fully utilized. Moreover, multi-layers
represent more context needed to decode each word, it seems that CNN as
a decoder does not need much context, or has not fully utilized the
longer context.</li>
<li>The value of attention is not the same as the key; instead, it is
the output of the last layer of the encoder plus the embedding of the
encoder input. The authors believe that this approach can
comprehensively consider both specific and abstract representations, and
the actual effect is indeed better.</li>
</ul></li>
<li>The author mentioned bytenet as a reference, but for some reason,
did not adopt the dilation convolution design from bytenet.</li>
</ul>
<h1
id="a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</h1>
<ul>
<li>Completely unsupervised cross-lingual word embedding mapping</li>
<li>Cross-lingual word embeddings, which refer to the use of the same
word embedding matrix across multiple languages, allowing for
cross-lingual model transfer of large-scale pre-trained word embeddings
and/or language models</li>
<li>The general approach is to use word embedding matrices of two
languages, map them to the same cross-lingual word embedding space, and
establish word correspondence between the two languages</li>
<li>This type of research has been popular recently, and the most
well-known downstream application it has spawned should be Facebook's
unsupervised machine translation from 2018</li>
<li>Previous methods are divided into three categories:
<ul>
<li>Supervised, using bilingual dictionaries, constructing thousands of
supervised word pairs, treating learning mapping as a regression
problem: modeling with the minimum mean square objective function, which
subsequently gave rise to various methods: canonical correlation
analysis; orthogonal methods; maximum margin methods. These methods can
all be categorized as linear transformation mappings of word embedding
matrices of the two languages into the same space.</li>
<li>Semi-supervised, achieved through seed dictionary and bootstrap,
such methods depend on good seeds and are prone to falling into local
optima</li>
<li>Another category is unsupervised generative methods, but the
existing methods are too dependent on specific tasks, have poor
generalization ability, and it is difficult to achieve good results for
two different languages of the language system.</li>
</ul></li>
<li>The text provided does not contain any source text to translate.
Please provide the source text you wish to have translated into
English.</li>
<li><img data-src="https://s2.ax1x.com/2019/03/11/APCWNT.png" title="fig:"
alt="APCWNT.png" /></li>
</ul>
<h2 id="model">Model</h2>
<ul>
<li>Let <span class="math inline">\(X\)</span> and <span
class="math inline">\(Z\)</span> be the word embedding matrices of two
languages, the goal is to learn linear transformation matrices <span
class="math inline">\(W_x\)</span> and <span
class="math inline">\(W_z\)</span> such that the mapped matrices of the
two languages are in the same cross-lingual space, forming a new
cross-lingual word embedding matrix</li>
<li>The iterative update of the model depends on an alignment matrix
<span class="math inline">\(D\)</span> , <span
class="math inline">\(D_{ij}=1\)</span> , which is aligned when and only
when the <span class="math inline">\(i\)</span> word of Language A
corresponds to the <span class="math inline">\(j\)</span> word of
Language B. The alignment relationship reflected by this matrix is
unidirectional</li>
<li>Model is divided into four steps: preprocessing, fully unsupervised
initialization, a robust self-learning process, and further improvement
of results through symmetric weight reallocation</li>
</ul>
<h2 id="preprocessing">Preprocessing</h2>
<ul>
<li>Normalize the length of word embeddings</li>
<li>Perform mean removal for each dimension again</li>
<li>The first two preprocessing steps mentioned in the author's previous
paper "Learning principled bilingual mappings of word embeddings while
preserving monolingual invariance" aim to simplify the problem to
seeking cosine similarity and maximum covariance. This paper discusses
supervised methods and is to be read.</li>
<li>Perform another length normalization to ensure that each word
embedding has a unit length, making the inner product of two word
embeddings equivalent to the cosine distance</li>
</ul>
<h2 id="completely-unsupervised-initialization">Completely unsupervised
initialization</h2>
<ul>
<li><p>Initialization is difficult to perform because the word embedding
matrices of the two languages are not aligned in two dimensions (each
word, each dimension of the embedding)</p></li>
<li><p>The approach in this paper is to first construct two matrices
<span class="math inline">\(X^{&#39;}\)</span> and <span
class="math inline">\(Z^{&#39;}\)</span> , with the word embeddings in
each dimension of these matrices aligned</p></li>
<li><p><span class="math inline">\(X^{&#39;}\)</span> and <span
class="math inline">\(Z^{&#39;}\)</span> are obtained by calculating the
square root of the similarity matrix of the original word embedding
matrix, i.e., <span class="math inline">\(X^{&#39;} = \sqrt
sorted{XX^T}\)</span> and <span class="math inline">\(Z^{&#39;} = \sqrt
sorted{ZZ^T}\)</span> . The product of a matrix with its transpose
results in a similarity matrix under the same language (because
preprocessing was done previously). Based on previous observations, two
words representing the same meaning in two languages should have a
similar single-language similarity distribution. Therefore, we sort each
row of the similarity matrices of the two languages separately, from
large to small. If two words have the same meaning, the corresponding
rows in their sorted similarity matrices within their own language
should have a similar distribution.</p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/03/13/Ak0cYd.png" alt="Ak0cYd.png" />
<figcaption aria-hidden="true">Ak0cYd.png</figcaption>
</figure></li>
<li><p>This skips the direct alignment of each dimension of word
embeddings, converting it to alignment based on dictionary similarity.
Afterward, only word alignment is needed, i.e., sorting each row of the
similarity matrix individually and establishing the correspondence
between words with similar row distributions.</p></li>
<li><p>Established word alignment, i.e., established the initial <span
class="math inline">\(D\)</span> matrix</p></li>
</ul>
<h2 id="robust-self-learning-process">Robust self-learning process</h2>
<ul>
<li><p>Compute orthogonal mappings to maximize the similarity of the
current <span class="math inline">\(D\)</span> matrix</p>
<p><span class="math display">\[
argmax_{W_x,W_z} \sum _i \sum _j D_{ij}((X_{i^*}W_X) \cdot (Z_{j^*}W_Z))
\\
\]</span></p>
<p>The optimal solution can be directly calculated as: <span
class="math inline">\(W_X=U,W_Z=V\)</span> , where <span
class="math inline">\(U,V\)</span> comes from <span
class="math inline">\(USV^T\)</span> , which is the singular value
decomposition of <span class="math inline">\(X^TDZ\)</span></p></li>
<li><p>After mapping the word embeddings of the two languages into a
cross-lingual word embedding space (still two word embedding matrices,
but within the same cross-lingual space), for each word in Language A,
find its nearest word in Language B within the cross-lingual word
embedding space, establish a mapping relationship, and update the <span
class="math inline">\(D\)</span> matrix.</p>
<p><span class="math display">\[
D_{ij} = 1 \ \ \ if  \ \ j = argmax _k (X_{i^*}W_X) \cdot (Z_{j^*}W_Z)
\\
else \ \ D_{ij} = 0 \\
\]</span></p></li>
<li><p>Repetitive iteration, <span class="math inline">\(W_X,W_Z
\rightarrow D \rightarrow W_X,W_Z \rightarrow D \rightarrow W_X,W_Z
\rightarrow D \rightarrow W_X,W_Z\)</span></p></li>
<li><p>Using a completely unsupervised initialization <span
class="math inline">\(D\)</span> matrix results in better performance
than random initialization, but it still falls into local optima.
Therefore, the authors proposed several small tricks for the second step
of the iteration, i.e., updating the <span
class="math inline">\(D\)</span> matrix, to make the learning more
robust</p>
<ul>
<li>Random dictionary induction: In each iteration, set elements of the
D matrix model to 0 with a certain probability, forcing the model to
explore more possibilities</li>
<li>Based on word frequency dictionary truncation: Only update the top k
most frequent words during each dictionary induction, to avoid noise
from low-frequency words, with a truncation limit of 20,000</li>
<li>CSLS retrieval: Previous methods find the nearest j for each i by
mapping it to the cross-lingual word embedding space, updating <span
class="math inline">\(D_{ij}\)</span> to 1. This nearest neighbor method
is affected by the dimensionality disaster and does not perform well
(the specific phenomenon caused is called hubs, where words cluster
together, and hubs words are the nearest neighbors of many words with
little difference). CSLS, which stands for cross-domain similarity local
scaling, penalizes these hub words.</li>
<li>Bidirectional dictionary induction, not only for finding j from i,
but also for finding i from j</li>
<li>These tricks differ in the initialization of the constructed
matrices, do not perform random inductive, and the dictionary truncation
limit is set to 4000</li>
</ul></li>
</ul>
<h2
id="further-improving-results-through-reweighting-with-symmetric-weights">Further
improving results through reweighting with symmetric weights</h2>
<ul>
<li><p>After the iterative process is completed</p>
<p><span class="math display">\[
W_X = US^{\frac 12} \\
W_Z = UV^{\frac 12} \\
\]</span></p></li>
<li><p>Compared to previous papers, this method encourages the model to
explore a wider search space by performing whitening and de-whitening
before and after each iteration, and it is insensitive to
direction</p></li>
<li><p>The reasons for reallocating weights were mentioned in the
previous paper, to be read</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="convolutional-sequence-to-sequence-learning">Convolutional
Sequence to Sequence Learning</h1>
<ul>
<li>非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习</li>
<li>无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。</li>
<li>transformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch
normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。</li>
<li>CNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal
size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。</li>
<li>Facebook在本论文中采用的CNN
block采用了普通的一维卷积，但是使用了gated linear
unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的......这样做mask然后一步一步生成其实并没有充分利用CNN的加速。</li>
<li>在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于
<ul>
<li>采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。</li>
<li>注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。</li>
</ul></li>
<li>作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation
convolution设计。</li>
</ul>
<h1
id="a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</h1>
<ul>
<li>完全无监督的跨语言词嵌入映射</li>
<li>跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移</li>
<li>一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系</li>
<li>这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译</li>
<li>之前的方法分三种：
<ul>
<li>有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical
correlation
analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。</li>
<li>半监督的，通过seed
dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；</li>
<li>另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。</li>
</ul></li>
<li>本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed
dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。</li>
<li><img data-src="https://s2.ax1x.com/2019/03/11/APCWNT.png" title="fig:"
alt="APCWNT.png" /></li>
</ul>
<h2 id="模型">模型</h2>
<ul>
<li>令<span class="math inline">\(X\)</span>和<span
class="math inline">\(Z\)</span>分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵<span
class="math inline">\(W_x\)</span>和<span
class="math inline">\(W_z\)</span>，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵</li>
<li>模型的迭代更新依赖一个对齐矩阵<span
class="math inline">\(D\)</span>，<span
class="math inline">\(D_{ij}=1\)</span>当且仅当A语言的第<span
class="math inline">\(i\)</span>个词对应着B语言的第<span
class="math inline">\(j\)</span>个词，该矩阵反映的对齐关系是单向的</li>
<li>模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果</li>
</ul>
<h2 id="预处理">预处理</h2>
<ul>
<li>对词嵌入做长度归一化</li>
<li>再针对每一维做去均值</li>
<li>前两个预处理在作者之前的论文Learning principled bilingual mappings
of word embeddings while preserving monolingual
invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读</li>
<li>再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离</li>
</ul>
<h2 id="完全无监督的初始化">完全无监督的初始化</h2>
<ul>
<li><p>初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐</p></li>
<li><p>本文的做法是，先构造两个矩阵<span
class="math inline">\(X^{&#39;}\)</span>和<span
class="math inline">\(Z^{&#39;}\)</span>，这两个矩阵的词嵌入每一维是对齐的</p></li>
<li><p><span class="math inline">\(X^{&#39;}\)</span>和<span
class="math inline">\(Z^{&#39;}\)</span>分别通过计算原词嵌入矩阵的相似矩阵开方得到，即<span
class="math inline">\(X^{&#39;} = \sqrt sorted{XX^T}\)</span>，<span
class="math inline">\(Z^{&#39;} = \sqrt
sorted{ZZ^T}\)</span>。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布</p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/03/13/Ak0cYd.png" alt="Ak0cYd.png" />
<figcaption aria-hidden="true">Ak0cYd.png</figcaption>
</figure></li>
<li><p>这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。</p></li>
<li><p>建立了词语的对齐，即建立了初始化的<span
class="math inline">\(D\)</span>矩阵</p></li>
</ul>
<h2 id="鲁棒的自学习过程">鲁棒的自学习过程</h2>
<ul>
<li><p>计算正交映射以最大化当前<span
class="math inline">\(D\)</span>矩阵的相似度</p>
<p><span class="math display">\[
argmax_{W_x,W_z} \sum _i \sum _j D_{ij}((X_{i^*}W_X) \cdot (Z_{j^*}W_Z))
\\
\]</span></p>
<p>最优解可以直接计算得到：<span
class="math inline">\(W_X=U,W_Z=V\)</span>，其中<span
class="math inline">\(U,V\)</span>来自<span
class="math inline">\(USV^T\)</span>，是<span
class="math inline">\(X^TDZ\)</span>的奇异值分解</p></li>
<li><p>将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新<span
class="math inline">\(D\)</span>矩阵。</p>
<p><span class="math display">\[
D_{ij} = 1 \ \ \ if  \ \ j = argmax _k (X_{i^*}W_X) \cdot (Z_{j^*}W_Z)
\\
else \ \ D_{ij} = 0 \\
\]</span></p></li>
<li><p>反复迭代，<span class="math inline">\(W_X,W_Z \rightarrow D
\rightarrow W_X,W_Z \rightarrow D \rightarrow W_X,W_Z \rightarrow D
\rightarrow W_X,W_Z\)</span></p></li>
<li><p>使用完全无监督的初始化<span
class="math inline">\(D\)</span>矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新<span
class="math inline">\(D\)</span>矩阵时提了几个小trick使得学习更为鲁棒</p>
<ul>
<li>随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能</li>
<li>基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000</li>
<li>CSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新<span
class="math inline">\(D_{ij}\)</span>为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语</li>
<li>双向词典归纳，不仅针对i找j，也针对j找i</li>
<li>这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000</li>
</ul></li>
</ul>
<h2
id="通过对称权重重分配进一步改善结果">通过对称权重重分配进一步改善结果</h2>
<ul>
<li><p>即迭代完成之后计算</p>
<p><span class="math display">\[
W_X = US^{\frac 12} \\
W_Z = UV^{\frac 12} \\
\]</span></p></li>
<li><p>比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感</p></li>
<li><p>重新分配权重的原因在之前的论文中提到，待阅读</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/abstractive-summarization/" rel="tag"># abstractive summarization</a>
              <a href="/tags/theory/" rel="tag"># theory</a>
              <a href="/tags/nlp/" rel="tag"># nlp</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/11/16/coling/" rel="prev" title="Notes for Computational Linguistics">
                  <i class="fa fa-angle-left"></i> Notes for Computational Linguistics
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/01/13/glove/" rel="next" title="Glove Embedding - Mathematical Derivation">
                  Glove Embedding - Mathematical Derivation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">17:08</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/01/03/PaperReading3/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
