<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Graph-based Automatic Summary Related Paper Selection Reading  AMR Generative Summary AMR Multi-document Summarization Two Papers pagerank in encoder attention Build a graph based on thematic modelin">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Graph-based Summarization">
<meta property="og:url" content="https://thinkwee.top/2019/10/03/graph-summarization/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Graph-based Automatic Summary Related Paper Selection Reading  AMR Generative Summary AMR Multi-document Summarization Two Papers pagerank in encoder attention Build a graph based on thematic modelin">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png">
<meta property="article:published_time" content="2019-10-03T09:18:15.000Z">
<meta property="article:modified_time" content="2025-07-16T10:15:04.327Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="natural language processing">
<meta property="article:tag" content="graph neural network">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="summarization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png">


<link rel="canonical" href="https://thinkwee.top/2019/10/03/graph-summarization/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/10/03/graph-summarization/","path":"2019/10/03/graph-summarization/","title":"Note for Graph-based Summarization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Note for Graph-based Summarization | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">51</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#toward-abstractive-summarization-using-semantic-representations"><span class="nav-number">1.</span> <span class="nav-text">Toward
Abstractive Summarization Using Semantic Representations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#source-graph-construction"><span class="nav-number">1.1.</span> <span class="nav-text">Source Graph Construction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subgraph-prediction"><span class="nav-number">1.2.</span> <span class="nav-text">Subgraph Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generation"><span class="nav-number">1.3.</span> <span class="nav-text">Generation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#abstract-meaning-representation-for-multi-document-summarization"><span class="nav-number">2.</span> <span class="nav-text">Abstract
Meaning Representation for Multi-Document Summarization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#source-sentence-selection"><span class="nav-number">2.1.</span> <span class="nav-text">Source Sentence Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#content-planning"><span class="nav-number">2.2.</span> <span class="nav-text">Content Planning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#surface-realization"><span class="nav-number">2.3.</span> <span class="nav-text">Surface Realization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#towards-a-neural-network-approach-to-abstractive-multi-document-summarization"><span class="nav-number">3.</span> <span class="nav-text">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#abstractive-document-summarization-with-a-graph-based-attentional-neural-model"><span class="nav-number">4.</span> <span class="nav-text">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#topical-coherence-for-graph-based-extractive-summarization"><span class="nav-number">5.</span> <span class="nav-text">Topical
Coherence for Graph-based Extractive Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph-based-neural-multi-document-summarization"><span class="nav-number">6.</span> <span class="nav-text">Graph-based
Neural Multi-Document Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#toward-abstractive-summarization-using-semantic-representations"><span class="nav-number">7.</span> <span class="nav-text">Toward
Abstractive Summarization Using Semantic Representations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#source-graph-construction"><span class="nav-number">7.1.</span> <span class="nav-text">Source Graph Construction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subgraph-prediction"><span class="nav-number">7.2.</span> <span class="nav-text">Subgraph Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generation"><span class="nav-number">7.3.</span> <span class="nav-text">Generation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#abstract-meaning-representation-for-multi-document-summarization"><span class="nav-number">8.</span> <span class="nav-text">Abstract
Meaning Representation for Multi-Document Summarization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#source-sentence-selection"><span class="nav-number">8.1.</span> <span class="nav-text">Source Sentence Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#content-planning"><span class="nav-number">8.2.</span> <span class="nav-text">Content Planning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#surface-realization"><span class="nav-number">8.3.</span> <span class="nav-text">Surface Realization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#towards-a-neural-network-approach-to-abstractive-multi-document-summarization"><span class="nav-number">9.</span> <span class="nav-text">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#abstractive-document-summarization-with-a-graph-based-attentional-neural-model"><span class="nav-number">10.</span> <span class="nav-text">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#topical-coherence-for-graph-based-extractive-summarization"><span class="nav-number">11.</span> <span class="nav-text">Topical
Coherence for Graph-based Extractive Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#graph-based-neural-multi-document-summarization"><span class="nav-number">12.</span> <span class="nav-text">Graph-based
Neural Multi-Document Summarization</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/10/03/graph-summarization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Note for Graph-based Summarization | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Note for Graph-based Summarization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-10-03 17:18:15" itemprop="dateCreated datePublished" datetime="2019-10-03T17:18:15+08:00">2019-10-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 18:15:04" itemprop="dateModified" datetime="2025-07-16T18:15:04+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/2019/10/03/graph-summarization/" class="post-meta-item leancloud_visitors" data-flag-title="Note for Graph-based Summarization" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>23k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>20 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>Graph-based Automatic Summary Related Paper Selection Reading</p>
<ul>
<li>AMR Generative Summary</li>
<li>AMR Multi-document Summarization Two Papers</li>
<li>pagerank in encoder attention</li>
<li>Build a graph based on thematic modeling, use ILP for extractive
summarization</li>
<li>Multi-document Extractive Summary Based on GCN</li>
<li>STRUCTURED NEURAL SUMMARIZATION</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="toward-abstractive-summarization-using-semantic-representations">Toward
Abstractive Summarization Using Semantic Representations</h1>
<ul>
<li>Explored how to construct an AMR graph for summarization from the
original AMR graph, i.e., graph summarization</li>
<li>Three-step approach, source graph construction, subgraph prediction,
text generation</li>
</ul>
<h2 id="source-graph-construction">Source Graph Construction</h2>
<ul>
<li>This step involves merging multiple graphs into a source graph, and
some concept nodes need to be combined</li>
<li>Firstly, AMR does not repeat the modeling of the mentioned concept,
but supplements the frequency mentioned as a feature into the node
embedding</li>
<li>Node merging consists of two steps: merging the subtrees of some
nodes, followed by merging nodes with the same concepts</li>
<li>The node names after subtree merging contain information from all
subtree nodes, and only nodes that are completely identical can be
merged thereafter. Therefore, nodes that have undergone one subtree
merge are difficult to merge again (difficult to be completely
identical), and some coreference resolution work needs to be done
(future work)</li>
<li>Some nodes will have multiple edges, take the two edges with the
most occurrences, merge them, and discard the other edges</li>
<li>Directly merge the same concept nodes</li>
<li>Add a total root node to connect the root nodes of each
sentence</li>
<li>The source graph connected in this way has a low edge coverage for
the gold summary graph, so further post-processing is done on the source
graph, adding null edges between all nodes to improve coverage</li>
</ul>
<h2 id="subgraph-prediction">Subgraph Prediction</h2>
<ul>
<li><p>Subgraph prediction problem is a structured prediction
problem</p></li>
<li><p>The author constructs the subgraph scoring function for the model
parameters as a linear weighted combination of edge and node
features:</p>
<p><span class="math display">\[
\operatorname{score}\left(V^{\prime}, E^{\prime} ; \boldsymbol{\theta},
\boldsymbol{\psi}\right)=\sum_{v \in V^{\prime}}
\boldsymbol{\theta}^{\top} \mathbf{f}(v)+\sum_{e \in E^{\prime}}
\boldsymbol{\psi}^{\top} \mathbf{g}(e)
\]</span></p></li>
<li><p>decoding: Selects the subgraph with the highest score based on
ILP. The constraint is that the selected subgraph must be legal and
connected, which can be described by the indicator function v, e, and
the flow f. <span class="math inline">\(v\_i=1\)</span> means that the
i-th node is selected, <span class="math inline">\(e\_{i,j}=1\)</span>
means that the edge between nodes i and j is selected, and <span
class="math inline">\(f\_{i,j}\)</span> represents the flow from i to j.
Then, the legal condition is:</p>
<p><span class="math display">\[
v_{i}-e_{i, j} \geq 0, \quad v_{j}-e_{i, j} \geq 0, \quad \forall i, j
\leq N
\]</span></p></li>
<li><p>Union: The flow that originates from the root reaches each
selected conceptual node, each conceptual node consumes a flow, and the
flow can only pass through when the edges are selected. These three
constraints are described mathematically as:</p>
<p><span class="math display">\[
\begin{array}{r}{\sum_{i} f_{0, i}-\sum_{i} v_{i}=0} \\ {\sum_{i} f_{i,
j}-\sum_{k} f_{j, k}-v_{j}=0, \quad \forall j \leq N} \\ {N \cdot e_{i,
j}-f_{i, j} \geq 0, \quad \forall i, j \leq N}\end{array}
\]</span></p></li>
<li><p>The author only assumes that each concept has only one parent
node, i.e., constructed in the form of a tree</p>
<p><span class="math display">\[
\sum _j e_{i,j} \leq 1, \quad \forall i, j \leq N
\]</span></p></li>
<li><p>This form of ILP has appeared in sentence compression and
dependency parsing, and the author has completed optimization using
Gurobi's ILP algorithm</p></li>
<li><p>A constraint can be added to limit the length of the summary, for
example, the total number of selected edges not exceeding L</p></li>
<li><p>The above is decoding, i.e., selecting subgraphs, but the
selection of graphs is based on scores, and the scores are weighted by
parameters, thus also including an optimization of parameters. We need a
loss function to measure the gap between the decoded graph and the gold
summary graph, however, the gold summary graph may not be in the source
graph. The authors refer to ramp loss from machine translation,
comparing the perceptron loss used by the perceptron, the hinge loss in
structured SVM, and the ramp loss, where <span
class="math inline">\(G\)</span> is the source graph, and <span
class="math inline">\(G^{\*}\)</span> is the gold summary graph:</p>
<p><span class="math display">\[
\begin{array}{ll}{\text {perceptron loss: }} &amp; {-\text { score
}\left(G^{*}\right)+\max _{G} \text { score }(G)} \\ {\text {hinge loss:
}} &amp; {-\text { score(G^{*} ) }+\max _{G}\left(\text
{score}(G)+\operatorname{cost}\left(G ; G^{*}\right)\right)} \\ {\text
{ramp loss: }} &amp; {-\max _{G}\left(\text
{score}(G)-\operatorname{cost}\left(G ; G^{*}\right)\right)+\max
_{G}\left(\text {score}(G)+\operatorname{cost}\left(G ;
G^{*}\right)\right)}\end{array}
\]</span></p></li>
<li><p>cost penalty for redundant edges</p></li>
<li><p>Perceptron loss is very simple, it is to minimize the score gap
between the gold graph and the decoded graph</p></li>
<li><p>hinge loss is added to the ILP with a penalty for redundant
edges, making the score of the decoded graph as large as possible, not
just close to the gold graph, here the score of the decoded graph will
be slightly lower than the graph obtained by direct score
calculation</p></li>
<li><p>ramp loss compared to hinge loss is that an inverse penalty is
added to the former, while the actual ramp loss still narrows the score
gap between the two images, with one image having a slightly higher
score than the best decoded graph and the other slightly lower, relaxing
the conditions</p></li>
</ul>
<h2 id="generation">Generation</h2>
<ul>
<li>Authors currently only counted the text span corresponding to the
concept nodes in the decoded graph, and did not generate a readable
summary, therefore only ROUGE-1 was calculated</li>
</ul>
<h1
id="abstract-meaning-representation-for-multi-document-summarization">Abstract
Meaning Representation for Multi-Document Summarization</h1>
<ul>
<li>This is an extension of the previous one</li>
<li>Using AMR to construct a rooted directed acyclic graph, where nodes
are concepts and edges are semantic relationships:
<ul>
<li>Node: May be a frameset (frame) in PropBank, an ordinary English
word, a special category word, or a string,</li>
<li>Edge: It can be a predicate relationship from PropBank or a modified
relationship</li>
</ul></li>
<li>The entire system consists of three parts
<ul>
<li>source sentence selection: input a series of articles and then pick
out sentences from different aspects of a certain topic</li>
<li>Content planning: Input a series of sentences, output an abstract
diagram</li>
<li>Surface realization: Convert diagrams into readable summary
sentences</li>
</ul></li>
<li>Three components can be optimized with domain-specific small corpora
separately</li>
</ul>
<h2 id="source-sentence-selection">Source Sentence Selection</h2>
<ul>
<li>Because it is a multi-document summary, spectral clustering is
performed for each input example (multiple documents), and several
sentences are then selected from each cluster</li>
<li>There are multiple sentence groups, and just like the modified input
summary model, it is necessary to reconstruct training pairs. Here, it
is to construct the training pairs to be provided for content planning,
that is, the sentence groups and their corresponding gold summary AMR
graphs. For each sentence in the gold summary, calculate an average
similarity with the sentence group, and select the one with the highest
similarity as the sentence group in the training pair. The average
similarity includes:
<ul>
<li>LCS</li>
<li>VSM</li>
<li>Smatch method, referring to the paper Smatch: an evaluation metric
for semantic feature structures</li>
<li>Concept Coverage, i.e., the concept in the maximum coverage gold
summary AMR graph</li>
</ul></li>
<li>Four similarity measures have also been ablated</li>
</ul>
<h2 id="content-planning">Content Planning</h2>
<ul>
<li><p>Training is for the AMR graph of sentence pairs and summaries,
naturally this part is about learning this transformation
process</p></li>
<li><p>Firstly, the summary in the sentence group needs to be converted
into an AMR graph; the author tried two AMR Parsers, JAMR and
CAMR</p></li>
<li><p>After that, convert each sentence in the sentence group into an
AMR graph and then merge them (this part of the paper is not described
clearly)</p>
<ul>
<li>Are same concept nodes merged?</li>
<li>Perform coreference resolution, merge nodes with the same reference
concepts</li>
<li>Some special nodes require integrating subtrees into the node
information, called mega-nodes, which is essentially canceling
unnecessary expansion and directly writing the specific expansion
information into the node, for example, date entity: year 2002: month 1:
day 5. These mega-nodes can only be merged if they are completely
identical.</li>
<li>Generate a root node finally, and connect the root nodes of various
subgraphs</li>
<li>Through the last operation, does the merging of seemingly identical
concept nodes represent the merging of the same nodes within the same
subgraph?</li>
</ul></li>
<li><p>Next, design an algorithm to identify the summarized AMR graph
from the source AMR graph, which includes two parts</p>
<ul>
<li><p>Graph Decoding: Identifies an optimal summary graph through
Integer Linear Programming (ILP): First, construct a parameterized graph
scoring function, where each node feature and edge feature is weighted
by parameters and summed to obtain a score; here, the features are
manually constructed, referring to Fei Liu's series of AMR summarization
papers; next, perform an ILP to find a subgraph that maximizes the
score, with the constraint of L nodes and the subgraph being
connected.</p></li>
<li><p>Parameter update: Minimize the gap between the abstract graph
decoded by the system and the gold summary graph. This step optimizes
the feature weighting parameters in the scoring function from the
previous step. Construct a loss function to measure the gap between the
decoded graph and the gold graph. Sometimes, the gold graph cannot be
decoded from the source graph, in which case structured ramp loss is
used, considering not only the score but also the cost, i.e., the degree
of agreement between the gold graph and the decoded graph on whether to
include a certain node or edge in the summary.</p>
<p><span class="math display">\[
L_{ramp}(\theta, \phi) = max_G (score(G)+cost(G;G_{gold})) -
max_G(score(G) - cost(G;G_{gold}))
\]</span></p></li>
</ul></li>
</ul>
<h2 id="surface-realization">Surface Realization</h2>
<ul>
<li>Convert image to sentence</li>
<li>AMR graphs do not convert into sentences because the graphs do not
contain grammatical information; a graph may generate multiple sentences
that are not grammatically correct. The author takes a two-step
approach, first converting the AMR graph into PENMAN form, and then
using the existing AMR-to-text to convert PENMAN into sentences</li>
</ul>
<h1
id="towards-a-neural-network-approach-to-abstractive-multi-document-summarization">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</h1>
<ul>
<li><p>This paper is an extension of the previous paper, expanding from
single-document summarization to multi-document summarization, mainly
focusing on how to transfer pre-trained models on large-scale
single-document summarization datasets to the multi-document
summarization task</p></li>
<li><p>Compared to the single-document model, an additional
document-level encoding layer is added on the encoding side. There is no
dependency or sequential relationship between documents, so there is no
need to use RNN. The authors directly use linear weighting. It is worth
noting that the weights of this weighting should not be fixed or
directly learned, but should be determined based on the document itself.
Therefore, the authors add a dependency relationship learned from the
document itself and the relationship between the document set:</p>
<p><span class="math display">\[
w_{m}=\frac{\mathbf{q}^{T}\left[\mathbf{d}_{m} ;
\mathbf{d}_{\Sigma}\right]}{\sum_{m} \mathbf{q}^{T}\left[\mathbf{d}_{m}
; \mathbf{d}_{\Sigma}\right]}
\]</span></p></li>
<li><p>The mechanism of attention remains essentially unchanged, with
the decoder's initial state transitioning from single-document encoding
to multi-document encoding, and the attention weighting shifting from
the number of sentences in a single document to the number of sentences
in multiple documents. One issue that arises here is that the number of
sentences in multi-documents is too large, with many attentions being
distributed very evenly, resulting in an excessive amount of information
after weighting. Therefore, the authors truncate the global soft
attention, allowing only the top k sentences to be weighted, with the
rest of the sentences being discarded directly during encoding.</p></li>
<li><p>The migration from single-document to multi-document actually is
not the focus of the paper. The author trains the single-document model
part on CNN/DM and then trains the multi-document part on a small DUC
dataset, but these two datasets are quite consistent. Many works trained
on CNNDM and tested on DUC can achieve good results.</p></li>
<li><p>The paper's ablation is very detailed, comparing the effects
under various functional graph model methods, including Textrank,
Lexrank, Centroid</p></li>
<li><p>It is noteworthy that the author uses edit distance to measure
the abstractness of the abstract</p></li>
</ul>
<h1
id="abstractive-document-summarization-with-a-graph-based-attentional-neural-model">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</h1>
<ul>
<li><p>A paper by the team of Teacher Wan, with very good ideas, the
important parts are in two points:</p>
<ul>
<li>hierarchical encoder and decoder: Since encoding and decoding at the
sentence level are required to adapt to the graph scoring operation, a
hierarchical seq2seq is adopted, with both encoding and decoding at the
word-level and sentence-level</li>
<li>graph-attention: The graph used here is actually a fully connected
graph from pagerank, where similarity is directly measured by the inner
product of the hidden vectors of enc-dec, and then the topic-aware
pagerank is used to recalculate the sentence-level attention
weights.</li>
</ul></li>
<li><p>In the encoding-decoding stage, we use hidden layers to calculate
similarity, which is the same as the original attention, but the
original attention adds a parameter matrix (modern attention doesn't
even bother to add a parameter matrix), so this similarity can reflect
the attention weight (score). Then, graph-attention directly calculates
the Markov chain iteration of pagerank on this similarity, considering
the stable distribution <span class="math inline">\(f\)</span> of the
Markov chain to be the sentence score after re-ranking. There is
something the paper doesn't mention; the author makes an assumption that
the state obtained during encoding-decoding is already in a stable
state, rather than starting from scratch, so we can let <span
class="math inline">\(f(t+1)=f(t)=f\)</span> and directly calculate the
stable distribution:</p>
<p><span class="math display">\[
\mathbf{f}(t+1)=\lambda W D^{-1} \mathbf{f}(t)+(1-\lambda) \mathbf{y} \\
\mathbf{f}=(1-\lambda)\left(I-\lambda W D^{-1}\right)^{-1} \mathbf{y} \\
\]</span></p></li>
<li><p>The basic form is consistent with pagerank, part of which is
based on salience allocation from a similarity matrix, and the other
part supplements a uniform distribution y to ensure the convergence of
the Markov chain (here it seems to be abbreviated, writing the uniform
transition matrix multiplied by f directly as a uniform distribution).
It is noteworthy that this calculation is done on the encoding and
decoding hidden layer states at the sentence level, therefore it is the
graph attention score of various encoding sentences given a certain
decoding sentence. How to reflect this certain decoding sentence? That
is to use topic-aware pagerank, treat the decoding sentence as a topic,
add this topic sentence to the pagerank graph, and change y from a
uniform distribution to a one-hot distribution, which ensures the
influence of the decoding sentence in the graph and thereby influences
other sentences.</p></li>
<li><p>Afterwards, the distraction attention mechanism was adopted to
prevent repeated attention:</p>
<p><span class="math display">\[
\alpha_{i}^{j}=\frac{\max \left(f_{i}^{j}-f_{i}^{j-1},
0\right)}{\sum_{l}\left(\max \left(f_{l}^{j}-f_{l}^{j-1},
0\right)\right)}
\]</span></p></li>
<li><p>Some minor techniques have also been applied at the decoding end,
including:</p>
<ul>
<li>Handling OOV, use <span class="citation"
data-cites="entity+word">@entity+word</span> length as a label to
replace all entities that are prone to become OOV, and attempt to
restore the entity labels generated in the decoded sentence, searching
in the original text according to word length</li>
<li>hierarchical beam search: word-level beam search scoring considers
the original sentence of "attend to" and the bigram overlap of the
currently generated part, hoping for a larger overlap; sentence-level
beam search hopes that the original sentence attended to is different
for each generated sentence, this description is not very clear, it
should be that N different original sentences are attended to when
generating each sentence, producing N different decoded sentences</li>
</ul></li>
<li><p>The hierarchical decoding in this article actually plays a very
crucial role; the author did not use word-level attention all at once
but rather reordered based on the sentence relationship component
diagram and also fully utilized two levels of information in beam
search</p></li>
</ul>
<h1
id="topical-coherence-for-graph-based-extractive-summarization">Topical
Coherence for Graph-based Extractive Summarization</h1>
<ul>
<li>Build a graph based on thematic modeling, use ILP for extractive
summarization</li>
<li>The author used a bipartite graph, with one side being sentence
nodes and the other side being topic nodes, connected by edges. The
weight of the edges is the sum of the logarithms of the probabilities of
all words in a sentence under a certain topic, normalized by the length
of the sentence</li>
<li>Using HITS algorithm to calculate the importance of sentences on a
bipartite graph</li>
</ul>
<h1 id="graph-based-neural-multi-document-summarization">Graph-based
Neural Multi-Document Summarization</h1>
<ul>
<li>Using GCN for extractive summarization, here GCN plays a role of
feature supplementation. The original approach is a two-level GRU, where
documents are clustered to create embeddings, with each sentence having
an embedding. Then, similar to IR, a comparison is made between sentence
embeddings and document embeddings to calculate salience scores.
Afterward, a greedy method is used to extract sentences based on the
scores, with the overall framework still being the scoring-extraction
approach</li>
<li>GCN added two layers between the GRUs, i.e., the embedding of
sentences under a sentence relationship graph was performed with three
layers of GCN, followed by the generation of document embeddings by the
GRU at the document level</li>
<li>Here are two points to focus on: how to construct the sentence
relationship diagram</li>
<li>The author of the sentence relationship diagram tried three methods:
<ul>
<li>The most naive, TF-IDF cosine distance</li>
<li>ADG in the paper "Towards Coherent Multi-Document
Summarization"</li>
<li>Author's Improved PDG on ADG</li>
</ul></li>
<li>After that, simply apply GCN propagation</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="toward-abstractive-summarization-using-semantic-representations">Toward
Abstractive Summarization Using Semantic Representations</h1>
<ul>
<li>探讨了如何从原文的AMR图构建摘要的AMR图，即graph summarization</li>
<li>三步走，source graph construction, subgraph prediction, text
generation</li>
</ul>
<h2 id="source-graph-construction">Source Graph Construction</h2>
<ul>
<li>这一步是将多句graph合并为source graph，一些concept node需要合并</li>
<li>首先AMR不会重复建模提到的concept，而是将提到的频次作为特征补充进node
embedding当中</li>
<li>节点合并包含两步：把一些节点的子树合并，接着把相同的概念节点合并</li>
<li>子树合并之后的节点名称里包含了所有子树节点的信息，之后只有完全相同的节点才能合并，因此经过一次子树合并之后的节点很难再次合并（很难完全相同），这里需要做一些共指消解的工作(future
work)</li>
<li>一些节点之间会有多条边，取出现次数最多的两个边的label，合并，抛弃其他边</li>
<li>相同的概念节点直接合并</li>
<li>加一个总的root节点连接各个句子的root节点</li>
<li>这样连接出来的source graph对于gold summary
graph的边覆盖度不高，因此对于source
graph还后处理一下，将所有节点之间加入null 边，提高覆盖率</li>
</ul>
<h2 id="subgraph-prediction">Subgraph Prediction</h2>
<ul>
<li><p>子图预测问题是一个structured prediction problem</p></li>
<li><p>作者构建子图打分函数为模型参数线性加权边和节点的特征：</p>
<p><span class="math display">\[
\operatorname{score}\left(V^{\prime}, E^{\prime} ; \boldsymbol{\theta},
\boldsymbol{\psi}\right)=\sum_{v \in V^{\prime}}
\boldsymbol{\theta}^{\top} \mathbf{f}(v)+\sum_{e \in E^{\prime}}
\boldsymbol{\psi}^{\top} \mathbf{g}(e)
\]</span></p></li>
<li><p>decoding：基于ILP选出得分最大的子图。这里的约束条件是选出的子图必须合法且是联通的，可以通过指示函数v,e和流量f来描述。<span
class="math inline">\(v_i=1\)</span>即第i个节点被选中，<span
class="math inline">\(e_{i,j}=1\)</span>即i,j两个节点之间的边被选中，<span
class="math inline">\(f_{i,j}\)</span>代表从i流向j的流量，那么合法即：</p>
<p><span class="math display">\[
v_{i}-e_{i, j} \geq 0, \quad v_{j}-e_{i, j} \geq 0, \quad \forall i, j
\leq N
\]</span></p></li>
<li><p>联通即：从根流出的流量到达选中的每一个概念节点，每一个概念节点消耗一个流量，只有边被选中时流量才可能通过，这三个约束用数学描述为：</p>
<p><span class="math display">\[
\begin{array}{r}{\sum_{i} f_{0, i}-\sum_{i} v_{i}=0} \\ {\sum_{i} f_{i,
j}-\sum_{k} f_{j, k}-v_{j}=0, \quad \forall j \leq N} \\ {N \cdot e_{i,
j}-f_{i, j} \geq 0, \quad \forall i, j \leq N}\end{array}
\]</span></p></li>
<li><p>另外作者只假设了每一个概念只有一个父节点，即构建为树的形式</p>
<p><span class="math display">\[
\sum _j e_{i,j} \leq 1, \quad \forall i, j \leq N
\]</span></p></li>
<li><p>这种形式的ILP在sentence compression和dependency
parsing中都出现过，作者使用gurobi的ILP算法完成最优化</p></li>
<li><p>可以附加一个约束来限制摘要的长度，例如选中的边总数不大于L</p></li>
<li><p>以上是decoding，即选子图，但选图基于分数，而分数由参数加权，因此还包含了一个参数的优化。我们需要一个损失函数来衡量decoded
graph和gold summary graph之间的差距，然而gold summary
graph可能不在source graph当中，作者借鉴了机器翻译中的ramp
loss，作者对比了感知机所用的perceptron loss, structured SVM中的hinge
loss以及ramp loss，其中<span class="math inline">\(G\)</span>是source
graph，<span class="math inline">\(G^{*}\)</span> 是gold summary
graph：</p>
<p><span class="math display">\[
\begin{array}{ll}{\text {perceptron loss: }} &amp; {-\text { score
}\left(G^{*}\right)+\max _{G} \text { score }(G)} \\ {\text {hinge loss:
}} &amp; {-\text { score(G^{*} ) }+\max _{G}\left(\text
{score}(G)+\operatorname{cost}\left(G ; G^{*}\right)\right)} \\ {\text
{ramp loss: }} &amp; {-\max _{G}\left(\text
{score}(G)-\operatorname{cost}\left(G ; G^{*}\right)\right)+\max
_{G}\left(\text {score}(G)+\operatorname{cost}\left(G ;
G^{*}\right)\right)}\end{array}
\]</span></p></li>
<li><p>cost对多余的边惩罚</p></li>
<li><p>perceptron loss很简单，就是希望缩小gold graph与decoded
graph之间的分数差距</p></li>
<li><p>hinge loss在ILP中加入对多余边的惩罚，使得decoded
graph的分数尽可能大，而不仅仅是和gold
graph接近，这里decoded的graph会比直接计算分数得到的graph分值上差一点</p></li>
<li><p>ramp loss相比hinge loss就是在前面一项加了一个反向的惩罚，实际ramp
loss依然是在缩小两个图的分数差距，只不过一个图比best decoded
graph分值高一点，另一个比best decoded graph低一点，放宽松了条件</p></li>
</ul>
<h2 id="generation">Generation</h2>
<ul>
<li>作者目前只统计了decoded graph中概念节点对应的text
span，并没有生成可读的摘要，因此只计算了ROUGE-1</li>
</ul>
<h1
id="abstract-meaning-representation-for-multi-document-summarization">Abstract
Meaning Representation for Multi-Document Summarization</h1>
<ul>
<li>这是上一篇的扩展</li>
<li>用AMR构建有根有向无环图，节点是概念，边是语义关系:
<ul>
<li>节点：可能是PropBank里的一个frameset（命题），一个普通英语单词，一个特殊类别词，一个字符串，</li>
<li>边：可以是PropBank里的命题关系，或者魔改之后的关系</li>
</ul></li>
<li>整个系统三个部分
<ul>
<li>source sentence
selection：输入一系列文章，然后挑出关于某一主题不同方面的句子</li>
<li>content planning：输入一系列句子，输出摘要图</li>
<li>surface realization：将图转换为可读的摘要句</li>
</ul></li>
<li>三个组件可分别用领域内小语料优化</li>
</ul>
<h2 id="source-sentence-selection">Source Sentence Selection</h2>
<ul>
<li>因为是多文档摘要，因此对每一个输入样例（多篇文档），做谱聚类，每个簇再挑若干句子</li>
<li>这样就有多个句子组，之后和更改了输入的摘要模型一样，需要重新构造训练对，这里是要构造接下来提供给content
planning的训练对，即句子组和对应的gold summary的AMR graph。就对gold
summary里的每一句，和句子组算一个平均相似度，选相似度大的作为训练对里的句子组。平均相似度有：
<ul>
<li>LCS</li>
<li>VSM</li>
<li>Smatch方法，参考了论文Smatch: an evaluation metric for semantic
feature structures</li>
<li>Concept Coverage，即最大覆盖gold summary AMR graph里的concept</li>
</ul></li>
<li>四种相似度也做了ablation</li>
</ul>
<h2 id="content-planning">Content Planning</h2>
<ul>
<li>训练对是句子组和summary的AMR
graph，自然这个部分就是学习这个转换过程</li>
<li>首先要把句子组里的summary转成AMR graph，作者试用了两种AMR
Parser，JAMR和CAMR</li>
<li>之后把句子组里的每一句也转成AMR
graph，并且做合并（这一部分论文描述并不清楚）
<ul>
<li>相同概念节点合并？</li>
<li>做共指消解，把相同指代概念节点合并</li>
<li>一些特殊节点需要把子树整合进节点信息里，叫mega-node，其实就是取消不必要的展开，将展开的具体信息直接写进节点里，例如date
entity :year 2002 :month 1 :day
5。这些mega-node只有完全相同时才能合并</li>
<li>最后生成一个root节点，把各个子图的root节点连起来</li>
<li>通过最后一个操作貌似相同概念节点合并是同一子图内相同节点合并？</li>
</ul></li>
<li>接下来设计算法，从源AMR graph中识别出摘要的AMR graph，包含两部分
<ul>
<li><p>graph
decoding：通过整数线性规划(ILP)识别出一个最优摘要图：首先构造一个参数化的图打分函数，将每一个节点特征和边特征通过参数加权并累加得到分数，这里的特征是手工构造，参考Fei
Liu他的一系列AMR
summarization的论文；接下来做一个ILP，要求找一个子图，使得得分最大，限制为L个节点而且子图是连接的。</p></li>
<li><p>parameter update：最小化系统解码出的摘要图和gold
summary图之间的差距。这一步优化的是上一步打分函数中的特征加权参数。构造损失函数来衡量decoded
graph和gold graph之间的差距。有时gold graph不能从source
graph中解码出来，这时就采用structed ramp
loss，不仅仅考虑score，还考虑cost，即gold graph和decoded
graph就是否将某个节点或者边加入摘要达成一致的程度</p>
<p><span class="math display">\[
L_{ramp}(\theta, \phi) = max_G (score(G)+cost(G;G_{gold})) -
max_G(score(G) - cost(G;G_{gold}))
\]</span></p></li>
</ul></li>
</ul>
<h2 id="surface-realization">Surface Realization</h2>
<ul>
<li>将图转成句子</li>
<li>AMR图并不好转成句子，因为图并不包含语法信息，一个图可能生成多句不合法的句子，作者两步走，先将AMR图转成PENMAN形式，然后用现有的AMR-to-text来将PENMAN转成句子</li>
</ul>
<h1
id="towards-a-neural-network-approach-to-abstractive-multi-document-summarization">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</h1>
<ul>
<li><p>这篇论文是上篇论文的扩展，从单文档摘要扩展到多文档摘要，主要是如何将大规模单文档摘要数据集上预训练好的模型迁移到多文档摘要任务上</p></li>
<li><p>相比单文档模型，编码端又加了一层文档级别的编码，文档之间并没有依存或者顺序关系，因此没必要用RNN，作者直接用了线性加权,值得注意的是这个加权的权重不应该是固定或者直接学习出来的，而应该根据文档本身决定，因此作者给权重加了一个依赖关系学习出来，依赖文档本身和文档集的关系：</p>
<p><span class="math display">\[
w_{m}=\frac{\mathbf{q}^{T}\left[\mathbf{d}_{m} ;
\mathbf{d}_{\Sigma}\right]}{\sum_{m} \mathbf{q}^{T}\left[\mathbf{d}_{m}
; \mathbf{d}_{\Sigma}\right]}
\]</span></p></li>
<li><p>注意力的机制基本不变，decoder的初始状态从单文档变成多文档编码，注意力加权从单篇文档句子数量到多篇文档句子数量。这里带来的一个问题是多文档的句子数量太大了，很多注意力被分散的很均匀，加权之后包含的信息量太大。因此作者将global
soft attention给截断了一下，只有top
k个句子可以用权重加权，其余的句子直接在编码中被抛弃</p></li>
<li><p>单文档到多文档的迁移其实并不是论文的重点，作者在CNN/DM上训练单文档的模型部分，之后在少量DUC数据集上训练多文档的部分，但是这两个数据集挺一致的，很多工作在CNNDM上训练在DUC上测试也能取得不错的效果。</p></li>
<li><p>论文的ablation做的非常详细，对比了多种功能图模型方法下的效果，包括Textrank,Lexrank,Centroid</p></li>
<li><p>值得注意的是作者使用了编辑距离来衡量文摘的抽象程度</p></li>
</ul>
<h1
id="abstractive-document-summarization-with-a-graph-based-attentional-neural-model">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</h1>
<ul>
<li><p>万老师团队的一篇论文，想法非常的好，重要的部分在两点：</p>
<ul>
<li>hierarchical encoder and
decoder：由于需要在句子级别上做编解码以适应图打分的操作，所以采用了分层的seq2seq，无论编码解码都是word-level加sentence-level</li>
<li>graph-attention：这里用的图是其实是pagerank里的全连接图，相似度直接用enc-dec的隐层向量内积来衡量，然后利用topic-aware
pagerank来重新计算句子级别注意力权重。</li>
</ul></li>
<li><p>在编解码阶段，我们利用隐层来计算相似度，这和原始的attention是一样的，只不过原始的attention加了一个参数矩阵（现代的attention连参数矩阵都懒得加了）使得这个相似度能够体现出注意力权重（分数），那么graph-attention就是在这个相似度上直接计算pagerank的markov链迭代，认为马氏链的稳定分布<span
class="math inline">\(f\)</span>就是重新rank之后的句子分数，这里有一点论文里没讲，作者做了一个假设，即编解码时拿到的已经是稳定状态，而不是从头迭代，因此可以令<span
class="math inline">\(f(t+1)=f(t)=f\)</span>，直接算出稳定分布：</p>
<p><span class="math display">\[
\mathbf{f}(t+1)=\lambda W D^{-1} \mathbf{f}(t)+(1-\lambda) \mathbf{y} \\
\mathbf{f}=(1-\lambda)\left(I-\lambda W D^{-1}\right)^{-1} \mathbf{y} \\
\]</span></p></li>
<li><p>基本形式与pagerank一致，一部分是基于相似矩阵的salience分配，另一部分补上一个均匀分布<span
class="math inline">\(y\)</span>保证马氏链收敛(这里感觉应该是简略了了，把均匀转移矩阵乘以f直接写成了均匀分布)，值得注意的是这是在sentence-level的编解码隐层状态做的计算，因此是计算给定某解码句下，各个编码句的graph
attention score，如何体现这个给定某解码句？那就是用topic-aware
pagerank，将解码句看成topic，把这个topic句加入pagerank的图里，并且y从均匀分布改成one-hot分布，即保证了解码句在graph中的影响力，并借此影响其他句子。</p></li>
<li><p>之后借鉴了distraction attention使得注意力不重复：</p>
<p><span class="math display">\[
\alpha_{i}^{j}=\frac{\max \left(f_{i}^{j}-f_{i}^{j-1},
0\right)}{\sum_{l}\left(\max \left(f_{l}^{j}-f_{l}^{j-1},
0\right)\right)}
\]</span></p></li>
<li><p>在解码端也做了一些小技巧，包括：</p>
<ul>
<li>OOV的处理，用@entity+单词长度来作为标签替换所有容易成为OOV的实体，并尝试把解码句中生成的实体标签还原，根据单词长度在原文中查找</li>
<li>hierarchical beam search：word-level的beam search打分考虑了attend
to的原文句子和当前生成部分的bigram
overlap，希望这个overlap越大越好；sentence-level的beam
search则希望生成每一句时attend
to的原文句子不相同，这一段描述不是很清楚，应该是生成每一句时会attend
N个不同的原文句产生N个不同的decoded sentence</li>
</ul></li>
<li><p>本文的层次编解码其实起到了很关键的作用，作者并没有一股脑用单词级别的注意力，还是根据句子关系构件图并重排序，在beam
search也充分利用了两个层次的信息</p></li>
<li><p>从ablation来看，graph attention和sentence
beam的效果其实不大，影响ROUGE分数最大的是考虑了bigram
overlap的word-level beam
search，这也暴露了ROUGE的问题，即我们之前工作中提到的OTR问题</p></li>
</ul>
<h1
id="topical-coherence-for-graph-based-extractive-summarization">Topical
Coherence for Graph-based Extractive Summarization</h1>
<ul>
<li>基于主题建模构建图，使用ILP做抽取式摘要</li>
<li>作者使用了二分图，一边是句子节点，一边是主题节点，两组节点之间用边连接，边的权值是句子中所有单词在某一主题下概率的对数和，除以句子长度做归一化</li>
<li>使用HITS算法在二分图上计算句子的重要程度</li>
</ul>
<h1 id="graph-based-neural-multi-document-summarization">Graph-based
Neural Multi-Document Summarization</h1>
<ul>
<li>用GCN做抽取式摘要，在这里GCN起到了一个特征补充的作用，原始的做法就是一个two-level
GRU，documents
cluster做一个embedding，其中每一个sentence有一个embedding，然后类似IR，拿sentence
embedding和documents embedding做一个比较算出salience
score，之后再用一个贪心的方法根据分数抽句子，大框架依然是打分-抽取的思路</li>
<li>GCN加进了两层GRU之间，即句子的embedding在一个句子关系图下做了三层GCN，之后再由documents层次的GRU生成documents
embedding</li>
<li>这里就关注两点：句子关系图如何构建</li>
<li>句子关系图作者试了三种：
<ul>
<li>最naive的，tfidf的cosine距离</li>
<li>Towards Coherent Multi-Document Summarization一文中的ADG</li>
<li>作者在ADG上改进的PDG</li>
</ul></li>
<li>之后直接套GCN传播就行了</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
              <a href="/tags/graph-neural-network/" rel="tag"># graph neural network</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/summarization/" rel="tag"># summarization</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/09/23/easyrl/" rel="prev" title="Easy Reinforcement Learning Notes">
                  <i class="fa fa-angle-left"></i> Easy Reinforcement Learning Notes
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/10/30/heterogeneous/" rel="next" title="Note for Heterogeneous Information Network">
                  Note for Heterogeneous Information Network <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:34</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/10/03/graph-summarization/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
