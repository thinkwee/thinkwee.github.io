<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Note for CorEx(Correlation Explaination).">
<meta property="og:type" content="article">
<meta property="og:title" content="Study Notes for Correlation Explaination">
<meta property="og:url" content="https://thinkwee.top/2019/07/29/CorEx/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Note for CorEx(Correlation Explaination).">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.ax1x.com/2019/07/31/etsOld.gif">
<meta property="og:image" content="https://s2.ax1x.com/2019/07/31/eYYvRK.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/07/31/eYYvRK.png">
<meta property="article:published_time" content="2019-07-29T03:17:11.000Z">
<meta property="article:modified_time" content="2025-01-30T02:10:45.357Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="corex">
<meta property="article:tag" content="math">
<meta property="article:tag" content="topic model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/07/31/etsOld.gif">


<link rel="canonical" href="https://thinkwee.top/2019/07/29/CorEx/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/07/29/CorEx/","path":"2019/07/29/CorEx/","title":"Study Notes for Correlation Explaination"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Study Notes for Correlation Explaination | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">52</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#discovering-structure-in-high-dimensional-data-through-correlation-explanation"><span class="nav-number">2.</span> <span class="nav-text">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#define-total-correlation"><span class="nav-number">2.1.</span> <span class="nav-text">Define Total Correlation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iteration"><span class="nav-number">2.2.</span> <span class="nav-text">Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pseudo-algorithm"><span class="nav-number">2.3.</span> <span class="nav-text">Pseudo-algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#maximally-informative-hierarchical-representations-of-high-dimensional-data"><span class="nav-number">3.</span> <span class="nav-text">Maximally
Informative Hierarchical Representations of High-Dimensional Data</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#upper-and-lower-bounds"><span class="nav-number">3.1.</span> <span class="nav-text">Upper and lower bounds</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#analysis"><span class="nav-number">3.2.</span> <span class="nav-text">Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization"><span class="nav-number">3.3.</span> <span class="nav-text">Optimization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge"><span class="nav-number">4.</span> <span class="nav-text">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract-1"><span class="nav-number">4.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#semi-supervised"><span class="nav-number">4.2.</span> <span class="nav-text">Semi-supervised</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">5.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#discovering-structure-in-high-dimensional-data-through-correlation-explanation"><span class="nav-number">6.</span> <span class="nav-text">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89total-correlation"><span class="nav-number">6.1.</span> <span class="nav-text">定义Total Correlation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3"><span class="nav-number">6.2.</span> <span class="nav-text">迭代</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%AA%E7%AE%97%E6%B3%95"><span class="nav-number">6.3.</span> <span class="nav-text">伪算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#maximally-informative-hierarchical-representations-of-high-dimensional-data"><span class="nav-number">7.</span> <span class="nav-text">Maximally
Informative Hierarchical Representations of High-Dimensional Data</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8A%E7%95%8C%E5%92%8C%E4%B8%8B%E7%95%8C"><span class="nav-number">7.1.</span> <span class="nav-text">上界和下界</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">7.2.</span> <span class="nav-text">分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-number">7.3.</span> <span class="nav-text">优化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge"><span class="nav-number">8.</span> <span class="nav-text">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="nav-number">8.1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3"><span class="nav-number">8.2.</span> <span class="nav-text">半监督</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/07/29/CorEx/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Study Notes for Correlation Explaination | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Study Notes for Correlation Explaination
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-07-29 11:17:11" itemprop="dateCreated datePublished" datetime="2019-07-29T11:17:11+08:00">2019-07-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-30 10:10:45" itemprop="dateModified" datetime="2025-01-30T10:10:45+08:00">2025-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2019/07/29/CorEx/" class="post-meta-item leancloud_visitors" data-flag-title="Study Notes for Correlation Explaination" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>23k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>21 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><hr />
<p>Note for CorEx(Correlation Explaination).</p>
<span id="more"></span>
<p><img data-src="https://s2.ax1x.com/2019/07/31/etsOld.gif" /></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="abstract">Abstract</h1>
<ul>
<li>Correlation Explaination is a type of learning method that can be
used in topic models, yielding results similar to LDA but with a
completely different processing process. Correlation Explaination does
not make any structural prior assumptions about the generation of data,
but, similar to information gain, uses the difference in Total
Correlation to find the topic that best explains the data. One rapid
calculation method is abbreviated as CorEx.</li>
<li>For convenience, the following text uses concepts from LDA to
analogize to concepts in CorEx, including that the background is
document topic modeling, a topic is a discrete random variable, and a
document may contain multiple topics, etc.</li>
</ul>
<h1
id="discovering-structure-in-high-dimensional-data-through-correlation-explanation">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</h1>
<h2 id="define-total-correlation">Define Total Correlation</h2>
<ul>
<li><p>The entropy of the discrete random variable <span
class="math inline">\(X\)</span> is defined as</p>
<p><span class="math display">\[
H(X) \equiv \mathbb{E}_{X}[-\log p(x)]
\]</span></p></li>
<li><p>The mutual information between two random variables is defined
as</p>
<p><span class="math display">\[
I(X_1 : X_2) = H\left(X_{1}\right)+H\left(X_{2}\right)-H\left(X_{1},
X_{2}\right)
\]</span></p></li>
<li><p>We define Total Correlation (or Multivariate Mutual Information)
as</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=\sum_{i \in G}
H\left(X_{i}\right)-H\left(X_{G}\right)
\]</span></p></li>
<li><p>Among them, <span class="math inline">\(G\)</span> is a subset of
<span class="math inline">\(X\)</span> . Intuitively, it is the sum of
the entropy of each random variable in the subset minus the joint
entropy of the subset. When there are only two variables in G, TC is
equivalent to the mutual information between the two variables.</p></li>
<li><p>To facilitate understanding, TC can also be expressed in the form
of KL divergence</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=D_{K L}\left(p\left(x_{G}\right) \| \prod_{i \in
G} p\left(x_{i}\right)\right)
\]</span></p></li>
<li><p>The KL divergence between the joint distribution and the product
of the marginal distributions can be seen as TC, so when TC is 0, the KL
divergence is 0, the joint distribution equals the product of the
marginal distributions, which means the internal correlation of the data
is 0, the variables are mutually independent, and the joint distribution
can be factorized into the product of the marginal
distributions.</p></li>
<li><p>Next, we define conditional TC</p>
<p><span class="math display">\[
T C(X | Y)=\sum_{i} H\left(X_{i} | Y\right)-H(X | Y)
\]</span></p></li>
<li><p>Then we can use the difference between TC and conditional TC to
measure the contribution of a certain condition (variable) to the
correlation of the data, the original text states: measure the extent to
which <span class="math inline">\(Y\)</span> explains the correlations
in <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
T C(X ; Y) \equiv T C(X)-T C(X | Y)=\sum_{i \in \mathbb{N}_{n}}
I\left(X_{i} : Y\right)-I(X : Y)
\]</span></p></li>
<li><p>When <span class="math inline">\(T C(X ; Y)\)</span> is
maximized, <span class="math inline">\(T C(X | Y)\)</span> is 0, which
means that the joint distribution of <span
class="math inline">\(X\)</span> can be decomposed given <span
class="math inline">\(Y\)</span> . This implies that <span
class="math inline">\(Y\)</span> explains all the correlation in <span
class="math inline">\(X\)</span> . We believe that a good topic should
be a representation of the document, which explains the document's Total
Correlation to the maximum extent.</p></li>
<li><p>Now we can treat <span class="math inline">\(Y\)</span> as a
latent variable that explains <span class="math inline">\(X\)</span> ,
that is, the topic. Next, we need to determine the topic. In LDA, the
topic is explicitly defined as a word probability distribution, whereas
in CorEx, we define the topic through <span
class="math inline">\(p(Y|X)\)</span> , meaning it is defined as a
discrete random variable that can affect <span
class="math inline">\(X\)</span> , with <span
class="math inline">\(k\)</span> possible values, unlike LDA which
defines <span class="math inline">\(|V|\)</span> possible
values.</p></li>
<li><p>LDA iteratively updates the topic assignment for each word,
thereby indirectly obtaining the document's topic distribution and the
distribution of words over topics. CorEx, however, is different; it
calculates a topic distribution for both documents and words. CorEx
continuously updates the probability of each topic <span
class="math inline">\(p(y_j)\)</span> , the topic distribution of each
word <span class="math inline">\(p(y_j|x_i)\)</span> , the allocation
matrix from words to topic subsets <span
class="math inline">\(\alpha\)</span> , and the topic distribution of
each document <span class="math inline">\(p(y_j|x)\)</span> .</p></li>
<li><p>At initialization, we randomly set <span
class="math inline">\(\alpha\)</span> and the document's topic
distribution <span class="math inline">\(p(y|x)\)</span></p></li>
<li><p>LDA is a generative model, while CorEX is a discriminative
model.</p></li>
</ul>
<h2 id="iteration">Iteration</h2>
<ul>
<li><p>The topic we need to find is</p>
<p><span class="math display">\[
\max _{p(y | x)} T C(X ; Y) \quad \text { s.t. } \quad|Y|=k
\]</span></p></li>
<li><p>We can find m topics and divide <span
class="math inline">\(X\)</span> into m disjoint subsets for
modeling</p>
<p><span class="math display">\[
\max _{G_{j}, p\left(y_{j} | x_{C_{j}}\right)} \sum_{j=1}^{m} T
C\left(X_{G_{j}} ; Y_{j}\right) \quad \text { s.t. }
\quad\left|Y_{j}\right|=k, G_{j} \cap G_{j^{\prime} \neq j}=\emptyset
\]</span></p></li>
<li><p>Rewrite the above equation in terms of mutual information</p>
<p><span class="math display">\[
\max _{G, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i \in G_{j}}
I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X_{G_{j}}\right)
\]</span></p></li>
<li><p>We further simplify this expression using an indicator function,
removing the subscripts of the subset <span
class="math inline">\(G\)</span> , and uniformly representing the
partition results of the subset with a single <span
class="math inline">\(\alpha\)</span> connectivity matrix</p>
<p><span class="math display">\[
\alpha_{i, j}=\mathbb{I}\left[X_{i} \in G_{j}\right] \in\{0,1\}  \\
\max _{\alpha, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X\right) \\
\]</span></p></li>
<li><p>We must also add a constraint to ensure that the subsets do not
intersect</p>
<p><span class="math display">\[
\sum_{\overline{j}} \alpha_{i, \overline{j}}=1
\]</span></p></li>
<li><p>This is an optimization problem with constraints, which can be
solved using the Lagrange multiplier method</p>
<p><span class="math display">\[
\begin{aligned} p\left(y_{j} | x\right) &amp;=\frac{1}{Z_{j}(x)}
p\left(y_{j}\right) \prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
p\left(y_{j} | x_{i}\right) &amp;=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \end{aligned} \\
\]</span></p></li>
<li><p>Note that this is the optimal theme solution obtained under the
confirmation of matrix <span class="math inline">\(\alpha\)</span> , by
relaxing the conditions for the optimal solution, we can obtain the
iterative formula for <span class="math inline">\(\alpha\)</span> after
the theme update</p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\alpha_{i, j}^{* *}=\exp \left(\gamma\left(I\left(X_{i} :
Y_{j}\right)-\max _{\overline{j}} I\left(X_{i} :
Y_{\overline{j}}\right)\right)\right) \\
\]</span></p></li>
</ul>
<h2 id="pseudo-algorithm">Pseudo-algorithm</h2>
<ul>
<li><p>Pseudo-algorithm description as follows</p>
<p> <span class="math display">\[
\text { input : A matrix of size } n_{s} \times n \text { representing }
n_{s} \text { samples of } n \text { discrete random variables } \\
\]</span> </p>
<p> <span class="math display">\[
\text { set } : \text { Set } m, \text { the number of latent variables,
} Y_{j}, \text { and } k, \text { so that }\left|Y_{j}\right|=k  \\
\]</span></p>
<p> <span class="math display">\[
\text { output: Parameters } \alpha_{i, j}, p\left(y_{j} | x_{i}\right),
p\left(y_{j}\right), p\left(y | x^{(l)}\right) \\
\]</span></p>
<p> <span class="math display">\[
\text { for } i \in \mathbb{N}_{n}, j \in \mathbb{N}_{m}, l \in
\mathbb{N}_{n_{s}}, y \in \mathbb{N}_{k}, x_{i} \in \mathcal{X}_{i} \\
\]</span></p>
<p> <span class="math display">\[
\text { Randomly initialize } \alpha_{i, j}, p\left(y | x^{(l)}\right)
\\
\]</span></p>
<p><span class="math display">\[
\text {repeat} \\
\]</span></p>
<p> <span class="math display">\[
\text { Estimate marginals, } p\left(y_{j}\right), p\left(y_{j} |
x_{i}\right) \text { using  } \\
\]</span> </p>
<p><span class="math display">\[
p\left(y_{j} | x_{i}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \\
\]</span></p>
<p> <span class="math display">\[
\text { Calculate } I\left(X_{i} : Y_{j}\right) \text { from marginals;
} \\
\]</span> </p>
<p> <span class="math display">\[
\text { Update } \alpha \text { using  } \\
\]</span> </p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\]</span></p>
<p> <span class="math display">\[
\text { Calculate } p\left(y | x^{(l)}\right), l=1, \ldots, n_{s} \text
{ using } \\
\]</span> </p>
<p><span class="math display">\[
p\left(y_{j} | x\right)=\frac{1}{Z_{j}(x)} p\left(y_{j}\right)
\prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
\]</span></p>
<p> <span class="math display">\[
\text { until convergence; }
\]</span> </p></li>
</ul>
<h1
id="maximally-informative-hierarchical-representations-of-high-dimensional-data">Maximally
Informative Hierarchical Representations of High-Dimensional Data</h1>
<ul>
<li>This paper analyzes the upper and lower bounds of TC, which helps to
further understand the meaning of TC and proposes an optimization method
for a hierarchical high-dimensional data representation that maximizes
information content. The CorEx mentioned earlier can be considered a
special case of this optimization method.</li>
</ul>
<h2 id="upper-and-lower-bounds">Upper and lower bounds</h2>
<ul>
<li><p>Most definitions are similar to the previous ones, more general
in nature. We extend documents and topics to data <span
class="math inline">\(X\)</span> and representation <span
class="math inline">\(Y\)</span> . When the joint probability can be
decomposed, we call <span class="math inline">\(Y\)</span> a
representation of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
p(x, y)=\prod_{j=1}^{m} p\left(y_{j} | x\right) p(x) \\
\]</span></p></li>
<li><p>Thus, the representation of a data set is completely determined
by the representation of the variable domain and the conditional
probability <span class="math inline">\(p(y_j|x)\)</span> .</p></li>
<li><p>Hierarchical representations can be stacked in layers; we define
hierarchical representation as:</p>
<p><span class="math display">\[
Y^{1 : r} \equiv Y^{1}, \ldots, Y^{r}
\]</span></p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/07/31/eYYvRK.png" alt="eYYvRK.png" />
<figcaption aria-hidden="true">eYYvRK.png</figcaption>
</figure></li>
<li><p>The <span class="math inline">\(Y^k\)</span> represents <span
class="math inline">\(Y^{k-1}\)</span> . We mainly focus on the upper
and lower bounds of the informativeness of hierarchical representations
for data. This type of hierarchical representation is a general
representation, including RBM and autoencoders, etc.</p></li>
<li><p>Definition:</p>
<p><span class="math display">\[
T C_{L}(X ; Y) \equiv \sum_{i=1}^{n} I\left(Y :
X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} : X\right) \\
\]</span></p></li>
<li><p>There exist the following boundaries and decompositions:</p>
<p><span class="math display">\[
T C(X) \geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)
\]</span></p></li>
<li><p>Then you get a lower bound of <span
class="math inline">\(Y\)</span> relative to <span
class="math inline">\(X\)</span> TC value:</p>
<p><span class="math display">\[
T C(X ; Y) \geq T C_{L}(X ; Y)
\]</span></p></li>
<li><p>When <span class="math inline">\(TC(Y)\)</span> is 0, the lower
bound is obtained, at which point <span class="math inline">\(Y\)</span>
are mutually independent and do not contain any information about <span
class="math inline">\(X\)</span> . Extending the inequality above to the
hierarchical representation, we can obtain</p>
<p><span class="math display">\[
T C(X) \geq \sum_{k=1}^{r} T C_{L}\left(Y^{k-1} ; Y^{k}\right)
\]</span></p></li>
<li><p>Attention here is that we define the 0th layer as <span
class="math inline">\(X\)</span> , and we can also find the upper
bound</p>
<p><span class="math display">\[
T C(X) \leq \sum_{k=1}^{r}\left(T C_{L}\left(Y^{k-1} ;
Y^{k}\right)+\sum_{i=1}^{m_{k-1}} H\left(Y_{i}^{k-1} |
Y^{k}\right)\right)
\]</span></p></li>
<li><p>The difference between the upper and lower bounds is a pile of
accumulated conditional entropy.</p></li>
<li><p>The lower and upper bounds of TC can help measure the extent of
interpretation for the data</p>
<h2 id="analysis">Analysis</h2></li>
<li><p>Consider the simplest case first, where the first layer
represents a single variable <span class="math inline">\(Y^{1} \equiv
Y_{1}^{1}\)</span></p>
<p><span class="math display">\[
TC(Y)+TC_L(X;Y)=TC(X;Y) \leq TC(X) \leq TC_L(X;Y)+\sum _{i=1}^{m_0}
H(X_i|Y)
\]</span></p></li>
<li><p>To be supplemented</p>
<h2 id="optimization">Optimization</h2></li>
<li><p>We can optimize layer by layer, so that each layer maximally
explains the correlations in the layer below, which can be achieved by
optimizing the lower bound, taking the first layer as an example</p>
<p><span class="math display">\[
\max _{\forall j, p\left(y_{j}^{1} | x\right)} T C_{L}\left(X ;
Y^{1}\right)
\]</span></p></li>
<li><p>Define the ancestor information as</p>
<p><span class="math display">\[
A I_{\alpha}(X ; Y) \equiv \sum_{i=1}^{n} \alpha_{i} I\left(Y :
X_{i}\right)-I(Y : X) \\
\alpha_{i} \in[0,1] \\
\]</span></p></li>
<li><p>If given a certain <span class="math inline">\(\alpha\)</span> ,
whose <span class="math inline">\(AI_{\alpha}\)</span> is positive, it
implies the existence of common ancestors for some ( <span
class="math inline">\(\alpha\)</span> -dependent) set of <span
class="math inline">\(X_i\)</span> s in any DAG that describes <span
class="math inline">\(X\)</span> , here it is not quite understood, but
it can be seen as a generalization of the aforementioned connected
matrix <span class="math inline">\(\alpha\)</span> , generalizing from
binarization to the 01 interval. The optimization problem can be
represented by <span class="math inline">\(AI_{\alpha}\)</span> and can
be written as</p>
<p><span class="math display">\[
\max _{p(y | x)} \sum_{i=1}^{n} \alpha_{i} I\left(Y : X_{i}\right)-I(Y :
X)
\]</span></p></li>
<li><p>The form has been transformed into the same as the previous
paragraph, and the subsequent solution is also the same</p>
<p><span class="math display">\[
p(y | x)=\frac{1}{Z(x)} p(y) \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}
\]</span></p></li>
<li><p>Taking the logarithmic expectation of the normalized denominator
<span class="math inline">\(Z(x)\)</span> yields the free energy, which
is precisely our optimization objective</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}[\log Z(x)] &amp;=\mathbb{E}\left[\log
\frac{p(y)}{p(y | x)} \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}\right] \\ &amp;=\sum_{i=1}^{n}
\alpha_{i} I\left(Y : X_{i}\right)-I(Y : X) \end{aligned}
\]</span></p></li>
<li><p>For multiple latent variables, the author reconstructed the lower
bound and similarly extended <span class="math inline">\(\alpha\)</span>
to continuous values in the 01 interval. The specific process is
relatively complex, and the final optimization objective changed from
maximizing the <span class="math inline">\(TC_L(X;Y)\)</span> of all
latent units to optimizing the lower bounds of <span
class="math inline">\(p(y_j|x)\)</span> and <span
class="math inline">\(\alpha\)</span> .</p>
<p><span class="math display">\[
\max _{\alpha_{i, j}, p\left(y_{j} | x\right) \atop c_{i,
j}\left(\alpha_{i, j}\right)=0}^{m} \sum_{j=1}^m \left(\sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-I\left(Y_{j} : X\right)\right)
\]</span></p></li>
<li><p>Defined the relationship between <span
class="math inline">\(X_i\)</span> and <span
class="math inline">\(Y_j\)</span> , i.e., the structure. As for
optimizing the structure, the ideal situation is</p>
<p><span class="math display">\[
\alpha _{i,j} = \mathbb{I} [j = argmax _{j} I(X_i : Y_j)]
\]</span></p></li>
<li><p>This structure is rigidly connected, with each node only
connected to a specific hidden layer node in the next layer. Based on
<span class="math inline">\(I(Y_j : X_i | Y_{1:j-1}) \geq \alpha _{i,j}
I(Y_j : X_i)\)</span> , the authors propose a heuristic algorithm to
estimate <span class="math inline">\(\alpha\)</span> . We verify whether
<span class="math inline">\(X_i\)</span> correctly estimates <span
class="math inline">\(Y_j\)</span> .</p>
<p><span class="math display">\[
d_{i,j}^l \equiv \mathbb{I} [argmax_{y_j} \log p(Y_j = y_j|x^{(l)}) =
argmax_{y_j} \log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]
\]</span></p></li>
<li><p>Afterward, we summed up over all the samples, counted the number
of correct estimates, and set the <span
class="math inline">\(\alpha\)</span> value according to the
proportion.</p></li>
</ul>
<h1
id="anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</h1>
<h2 id="abstract-1">Abstract</h2>
<ul>
<li><p>This paper formally applies CorEx to topic modeling, emphasizing
the advantages compared to LDA.</p>
<ul>
<li>No structural assumptions need to be made for the data, and compared
to LDA, CorEX has fewer hyperparameters</li>
<li>Different from LDA, it can be generalized to hierarchical models and
semi-supervised models without any structural modifications to the
model</li>
</ul></li>
<li><p>The iterative process of the model still follows these steps:</p>
<p><span class="math display">\[
p_t(y_j) = \sum _{\overline{x}} p_t(y_j | \overline{x})p(\overline{x})
\\
\]</span></p>
<p><span class="math display">\[
p_t(x_i | y_j) = \sum _{\overline{x}}
p_t(y_j|\overline{x})p(\overline{x}) \mathbb{I} [\overline{x}_i =
x_i]/p_t(y_j) \\
\]</span></p>
<p><span class="math display">\[
\log p_{t+1} (y_j | x^l) = \log p_t(y_j) + \sum _{i=1}^n \alpha _{i,j}^t
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \log \mathbb{Z} _j (x^l) \\
\]</span></p></li>
<li><p>Due to the use of bag-of-words information and the processing of
sparse matrices, the calculation of edge probabilities and conditional
probabilities is very fast. The slowest step in the iteration is the
third formula, which is to calculate the topic distribution of all
documents. We rewrite the summation of logarithmic terms in this
formula:</p>
<p><span class="math display">\[
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \log
\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \log
(\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})
\]</span></p></li>
<li><p>The cumulative calculation is performed for each document,
computing the likelihood over the entire dictionary, however, only a
small portion of the words in the dictionary appear in each document.
When a word in the dictionary does not appear in the document, only the
first term in the above formula is not zero; when the word does appear,
the term <span class="math inline">\(\log
P(X_i^l=1|y_j)/p(X_i^l=1)\)</span> is zero, with the remaining terms
retained, thus the author prioritizes the assumption that the word is
not present in the document and then updates and supplements the
probability terms for those words that do appear. After such
optimization, the calculation speed of CorEx is similar to that of
LDA.</p></li>
<li><p>The greatest benefit of this optimization is that the
computational complexity is only linearly related to the number of
documents and the number of topics, thus making it possible to compute
over large-scale documents with large-scale topics.</p>
<h2 id="semi-supervised">Semi-supervised</h2></li>
<li><p>Some value in the weight matrix can be fixed. The normal <span
class="math inline">\(\alpha\)</span> is in the 01 interval, and the
anchor of the i-th word in the j-th topic can be set to <span
class="math inline">\(\beta _{i,j}\)</span>, where <span
class="math inline">\(\beta\)</span> is the strength of the
anchor.</p></li>
<li><p>This approach can assign an anchor word to each topic, with one
or more words as an anchor, offering great flexibility.</p></li>
<li><p>In business terms, the advantages of CorEx lie in:</p>
<ul>
<li>Extremely fast in training with a very large number of topics.</li>
<li>It is convenient to anchor words to adapt to the field.</li>
<li>The themes in CorEx are non-overlapping; there will be no repeated
themes</li>
</ul></li>
<li><p>Iterative hierarchical topics are based on the hierarchical
method of the previous paper, hierarchical topics can be used to
aggregate concepts and divide subtopics.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="概述">概述</h1>
<ul>
<li>Correlation
Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation
Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total
Correlation之差来找出最能explain数据的Correlation的主题。
其中一种快速计算方法就简写为CorEx。</li>
<li>为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。</li>
</ul>
<h1
id="discovering-structure-in-high-dimensional-data-through-correlation-explanation">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</h1>
<h2 id="定义total-correlation">定义Total Correlation</h2>
<ul>
<li><p>定义<span
class="math inline">\(X\)</span>为一离散随机变量，则其熵为</p>
<p><span class="math display">\[
H(X) \equiv \mathbb{E}_{X}[-\log p(x)]
\]</span></p></li>
<li><p>两个随机变量之间的互信息定义为</p>
<p><span class="math display">\[
I(X_1 : X_2) = H\left(X_{1}\right)+H\left(X_{2}\right)-H\left(X_{1},
X_{2}\right)
\]</span></p></li>
<li><p>我们定义Total Correlation（或者叫多元互信息multivariate mutual
information）为</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=\sum_{i \in G}
H\left(X_{i}\right)-H\left(X_{G}\right)
\]</span></p></li>
<li><p>其中<span class="math inline">\(G\)</span>是<span
class="math inline">\(X\)</span>的一个子集。直观来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。</p></li>
<li><p>为了更方便理解，TC还可以写成KL散度的形式</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=D_{K L}\left(p\left(x_{G}\right) \| \prod_{i \in
G} p\left(x_{i}\right)\right)
\]</span></p></li>
<li><p>也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。</p></li>
<li><p>接着我们定义conditional TC</p>
<p><span class="math display">\[
T C(X | Y)=\sum_{i} H\left(X_{i} | Y\right)-H(X | Y)
\]</span></p></li>
<li><p>那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure
the extent to which <span class="math inline">\(Y\)</span> explains the
correlations in <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
T C(X ; Y) \equiv T C(X)-T C(X | Y)=\sum_{i \in \mathbb{N}_{n}}
I\left(X_{i} : Y\right)-I(X : Y)
\]</span></p></li>
<li><p><span class="math inline">\(T C(X ; Y)\)</span>最大时，<span
class="math inline">\(T C(X | Y)\)</span>为0，也就是已知<span
class="math inline">\(Y\)</span>时<span
class="math inline">\(X\)</span>的联合分布可分解，也就说明<span
class="math inline">\(Y\)</span> explains all the correlation in <span
class="math inline">\(X\)</span>。我们认为好的主题应当是文档的一种表示，其解释的文档Total
Correlation应该最大。</p></li>
<li><p>现在我们就可以把<span
class="math inline">\(Y\)</span>看成时解释<span
class="math inline">\(X\)</span>的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过<span
class="math inline">\(p(Y|X)\)</span>来定义主题，也就是说只将其定义为一个能够影响<span
class="math inline">\(X\)</span>的离散随机变量，取值范围有<span
class="math inline">\(k\)</span>种可能，而不像LDA定义为<span
class="math inline">\(|V|\)</span>种取值可能。</p></li>
<li><p>LDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率<span
class="math inline">\(p(y_j)\)</span>，每个词的主题分布<span
class="math inline">\(p(y_j|x_i)\)</span>，词到主题子集合的分配矩阵<span
class="math inline">\(\alpha\)</span>，以及每篇文档的主题分布<span
class="math inline">\(p(y_j|x)\)</span><br />
</p></li>
<li><p>初始化时，我们随机设定<span
class="math inline">\(\alpha\)</span>以及文档的主题分布<span
class="math inline">\(p(y|x)\)</span></p></li>
<li><p>LDA是生成式模型，而CorEX是判别式模型。</p></li>
</ul>
<h2 id="迭代">迭代</h2>
<ul>
<li><p>我们要找到的主题是</p>
<p><span class="math display">\[
\max _{p(y | x)} T C(X ; Y) \quad \text { s.t. } \quad|Y|=k
\]</span></p></li>
<li><p>我们可以找m个主题，并将<span
class="math inline">\(X\)</span>分为m个不相交的子集来建模</p>
<p><span class="math display">\[
\max _{G_{j}, p\left(y_{j} | x_{C_{j}}\right)} \sum_{j=1}^{m} T
C\left(X_{G_{j}} ; Y_{j}\right) \quad \text { s.t. }
\quad\left|Y_{j}\right|=k, G_{j} \cap G_{j^{\prime} \neq j}=\emptyset
\]</span></p></li>
<li><p>将上式用互信息改写为</p>
<p><span class="math display">\[
\max _{G, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i \in G_{j}}
I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X_{G_{j}}\right)
\]</span></p></li>
<li><p>我们用指示函数进一步简化这个式子，去掉子集<span
class="math inline">\(G\)</span>的下标，统一用一个<span
class="math inline">\(\alpha\)</span>连通矩阵来代表子集的划分结果</p>
<p><span class="math display">\[
\alpha_{i, j}=\mathbb{I}\left[X_{i} \in G_{j}\right] \in\{0,1\}  \\
\max _{\alpha, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X\right) \\
\]</span></p></li>
<li><p>同时我们要加一个限制项保证子集不相交</p>
<p><span class="math display">\[
\sum_{\overline{j}} \alpha_{i, \overline{j}}=1
\]</span></p></li>
<li><p>这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出</p>
<p><span class="math display">\[
\begin{aligned} p\left(y_{j} | x\right) &amp;=\frac{1}{Z_{j}(x)}
p\left(y_{j}\right) \prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
p\left(y_{j} | x_{i}\right) &amp;=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \end{aligned} \\
\]</span></p></li>
<li><p>注意，这是在<span
class="math inline">\(\alpha\)</span>矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后<span
class="math inline">\(\alpha\)</span>的迭代公式</p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\alpha_{i, j}^{* *}=\exp \left(\gamma\left(I\left(X_{i} :
Y_{j}\right)-\max _{\overline{j}} I\left(X_{i} :
Y_{\overline{j}}\right)\right)\right) \\
\]</span></p></li>
</ul>
<h2 id="伪算法">伪算法</h2>
<ul>
<li><p>伪算法描述如下</p>
<p><span class="math display">\[
\text { input : A matrix of size } n_{s} \times n \text { representing }
n_{s} \text { samples of } n \text { discrete random variables } \\
\]</span></p>
<p><span class="math display">\[
\text { set } : \text { Set } m, \text { the number of latent variables,
} Y_{j}, \text { and } k, \text { so that }\left|Y_{j}\right|=k  \\
\]</span></p>
<p><span class="math display">\[
\text { output: Parameters } \alpha_{i, j}, p\left(y_{j} | x_{i}\right),
p\left(y_{j}\right), p\left(y | x^{(l)}\right) \\
\]</span></p>
<p><span class="math display">\[
\text { for } i \in \mathbb{N}_{n}, j \in \mathbb{N}_{m}, l \in
\mathbb{N}_{n_{s}}, y \in \mathbb{N}_{k}, x_{i} \in \mathcal{X}_{i} \\
\]</span></p>
<p><span class="math display">\[
\text { Randomly initialize } \alpha_{i, j}, p\left(y | x^{(l)}\right)
\\
\]</span></p>
<p><span class="math display">\[
\text {repeat} \\
\]</span></p>
<p><span class="math display">\[
\text { Estimate marginals, } p\left(y_{j}\right), p\left(y_{j} |
x_{i}\right) \text { using  } \\
\]</span></p>
<p><span class="math display">\[
p\left(y_{j} | x_{i}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \\
\]</span></p>
<p><span class="math display">\[
\text { Calculate } I\left(X_{i} : Y_{j}\right) \text { from marginals;
} \\
\]</span></p>
<p><span class="math display">\[
\text { Update } \alpha \text { using  } \\
\]</span></p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\]</span></p>
<p><span class="math display">\[
\text { Calculate } p\left(y | x^{(l)}\right), l=1, \ldots, n_{s} \text
{ using } \\
\]</span></p>
<p><span class="math display">\[
p\left(y_{j} | x\right)=\frac{1}{Z_{j}(x)} p\left(y_{j}\right)
\prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
\]</span></p>
<p><span class="math display">\[
\text { until convergence; }
\]</span></p></li>
</ul>
<h1
id="maximally-informative-hierarchical-representations-of-high-dimensional-data">Maximally
Informative Hierarchical Representations of High-Dimensional Data</h1>
<ul>
<li>本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。</li>
</ul>
<h2 id="上界和下界">上界和下界</h2>
<ul>
<li><p>大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据<span
class="math inline">\(X\)</span>和表示<span
class="math inline">\(Y\)</span>，当联合概率可以分解时，我们称<span
class="math inline">\(Y\)</span>是<span
class="math inline">\(X\)</span>的一种表示</p>
<p><span class="math display">\[
p(x, y)=\prod_{j=1}^{m} p\left(y_{j} | x\right) p(x) \\
\]</span></p></li>
<li><p>这样，一种数据的表示完全由表示变量域和条件概率<span
class="math inline">\(p(y_j|x)\)</span>决定。</p></li>
<li><p>表示可以层次性堆叠，我们定义层次表示为：</p>
<p><span class="math display">\[
Y^{1 : r} \equiv Y^{1}, \ldots, Y^{r}
\]</span></p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/07/31/eYYvRK.png" alt="eYYvRK.png" />
<figcaption aria-hidden="true">eYYvRK.png</figcaption>
</figure></li>
<li><p>其中<span class="math inline">\(Y^k\)</span>是<span
class="math inline">\(Y^{k-1}\)</span>的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。</p></li>
<li><p>定义：</p>
<p><span class="math display">\[
T C_{L}(X ; Y) \equiv \sum_{i=1}^{n} I\left(Y :
X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} : X\right) \\
\]</span></p></li>
<li><p>则存在以下的边界和分解：</p>
<p><span class="math display">\[
T C(X) \geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)
\]</span></p></li>
<li><p>同时得到<span class="math inline">\(Y\)</span>关于<span
class="math inline">\(X\)</span>的TC值的一个下界：</p>
<p><span class="math display">\[
T C(X ; Y) \geq T C_{L}(X ; Y)
\]</span></p></li>
<li><p>当<span
class="math inline">\(TC(Y)\)</span>为0时取到下界，这时<span
class="math inline">\(Y\)</span>之间相互独立，不包含关于<span
class="math inline">\(X\)</span>的信息。将上面<span
class="math inline">\(TC(X)\)</span>的不等式扩展到层次表示，则可以得到</p>
<p><span class="math display">\[
T C(X) \geq \sum_{k=1}^{r} T C_{L}\left(Y^{k-1} ; Y^{k}\right)
\]</span></p></li>
<li><p>注意在这里我们定义第0层表示就是<span
class="math inline">\(X\)</span>，我们还能找到上界</p>
<p><span class="math display">\[
T C(X) \leq \sum_{k=1}^{r}\left(T C_{L}\left(Y^{k-1} ;
Y^{k}\right)+\sum_{i=1}^{m_{k-1}} H\left(Y_{i}^{k-1} |
Y^{k}\right)\right)
\]</span></p></li>
<li><p>可以看到上界与下界之间就差了一堆累加的条件熵。</p></li>
<li><p>TC的上下界可以帮助衡量表示对于数据的解释程度，</p>
<h2 id="分析">分析</h2></li>
<li><p>先考虑最简单的情况，即第一层表示只有一个变量<span
class="math inline">\(Y^{1} \equiv Y_{1}^{1}\)</span>，这时</p>
<p><span class="math display">\[
TC(Y)+TC_L(X;Y)=TC(X;Y) \leq TC(X) \leq TC_L(X;Y)+\sum _{i=1}^{m_0}
H(X_i|Y)
\]</span></p></li>
<li><p>待补充</p>
<h2 id="优化">优化</h2></li>
<li><p>我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly
explain the correlations in the layer
below)，这可以通过优化下界得到，以第一层为例</p>
<p><span class="math display">\[
\max _{\forall j, p\left(y_{j}^{1} | x\right)} T C_{L}\left(X ;
Y^{1}\right)
\]</span></p></li>
<li><p>定义<span class="math inline">\(\alpha\)</span>祖先信息为</p>
<p><span class="math display">\[
A I_{\alpha}(X ; Y) \equiv \sum_{i=1}^{n} \alpha_{i} I\left(Y :
X_{i}\right)-I(Y : X) \\
\alpha_{i} \in[0,1] \\
\]</span></p></li>
<li><p>假如给定某个<span class="math inline">\(\alpha\)</span>，其<span
class="math inline">\(AI_{\alpha}\)</span>为正，则 it implies the
existence of common ancestors for some (<span
class="math inline">\(\alpha\)</span>-dependent) set of <span
class="math inline">\(X_i\)</span> ’s in any DAG that describes <span
class="math inline">\(X\)</span>，这里不太懂，但可以看成是上文联通矩阵<span
class="math inline">\(\alpha\)</span>的泛化版本，从binarize泛化到01区间。最优化问题用<span
class="math inline">\(AI_{\alpha}\)</span>表示可以写成</p>
<p><span class="math display">\[
\max _{p(y | x)} \sum_{i=1}^{n} \alpha_{i} I\left(Y : X_{i}\right)-I(Y :
X)
\]</span></p></li>
<li><p>化成了和上文一样的形式，之后的解法也一样</p>
<p><span class="math display">\[
p(y | x)=\frac{1}{Z(x)} p(y) \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}
\]</span></p></li>
<li><p>对归一化分母<span
class="math inline">\(Z(x)\)</span>取对数期望，可以得到自由能量，这正是我们的优化目标</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}[\log Z(x)] &amp;=\mathbb{E}\left[\log
\frac{p(y)}{p(y | x)} \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}\right] \\ &amp;=\sum_{i=1}^{n}
\alpha_{i} I\left(Y : X_{i}\right)-I(Y : X) \end{aligned}
\]</span></p></li>
<li><p>对于多个隐变量，作者重构了下界，同样将<span
class="math inline">\(\alpha\)</span>扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的<span
class="math inline">\(TC_L(X;Y)\)</span>变为优化<span
class="math inline">\(p(y_j|x)\)</span>和<span
class="math inline">\(\alpha\)</span>的下界：</p>
<p><span class="math display">\[
\max _{\alpha_{i, j}, p\left(y_{j} | x\right) \atop c_{i,
j}\left(\alpha_{i, j}\right)=0}^{m} \sum_{j=1}^m \left(\sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-I\left(Y_{j} : X\right)\right)
\]</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span>定义了<span
class="math inline">\(X_i\)</span>和<span
class="math inline">\(Y_j\)</span>之间的关系，即结构。至于优化结构，理想的情况是</p>
<p><span class="math display">\[
\alpha _{i,j} = \mathbb{I} [j = argmax _{j} I(X_i : Y_j)]
\]</span></p></li>
<li><p>这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于<span
class="math inline">\(I(Y_j : X_i | Y_{1:j-1}) \geq \alpha _{i,j} I(Y_j
: X_i)\)</span>,作者提出了一种启发式的算法来估计<span
class="math inline">\(\alpha\)</span>。我们检查<span
class="math inline">\(X_i\)</span>是否正确估计<span
class="math inline">\(Y_j\)</span></p>
<p><span class="math display">\[
d_{i,j}^l \equiv \mathbb{I} [argmax_{y_j} \log p(Y_j = y_j|x^{(l)}) =
argmax_{y_j} \log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]
\]</span></p></li>
<li><p>之后我们在所有样本上累加，统计正确估计数目，并根据比例设置<span
class="math inline">\(\alpha\)</span>值。</p></li>
</ul>
<h1
id="anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</h1>
<h2 id="概述-1">概述</h2>
<ul>
<li><p>本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：</p>
<ul>
<li>不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数</li>
<li>不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型</li>
</ul></li>
<li><p>模型的迭代依然是这几步：</p>
<p><span class="math display">\[
p_t(y_j) = \sum _{\overline{x}} p_t(y_j | \overline{x})p(\overline{x})
\\
\]</span></p>
<p><span class="math display">\[
p_t(x_i | y_j) = \sum _{\overline{x}}
p_t(y_j|\overline{x})p(\overline{x}) \mathbb{I} [\overline{x}_i =
x_i]/p_t(y_j) \\
\]</span></p>
<p><span class="math display">\[
\log p_{t+1} (y_j | x^l) = \log p_t(y_j) + \sum _{i=1}^n \alpha _{i,j}^t
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \log \mathbb{Z} _j (x^l) \\
\]</span></p></li>
<li><p>由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：</p>
<p><span class="math display">\[
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \log
\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \log
(\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})
\]</span></p></li>
<li><p>这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中<span
class="math inline">\(\log
P(X_i^l=1|y_j)/p(X_i^l=1)\)</span>为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。</p></li>
<li><p>这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。</p>
<h2 id="半监督">半监督</h2></li>
<li><p>半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的<span
class="math inline">\(\alpha\)</span>在01区间之间，而将第i号词anchor在第j号主题可以将<span
class="math inline">\(\alpha_{i,j} = \beta _{i,j}\)</span>，其中<span
class="math inline">\(\beta\)</span>是anchor的强度。</p></li>
<li><p>这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。</p></li>
<li><p>在业务上来说，CorEx的优势在于：</p>
<ul>
<li>在训练超大规模主题数时非常快。</li>
<li>可以方便的anchor词以适应领域。</li>
<li>CorEx的主题之间词是不相交的，不会出现重复主题</li>
</ul></li>
<li><p>层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/corex/" rel="tag"># corex</a>
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/topic-model/" rel="tag"># topic model</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/07/28/acl2019/" rel="prev" title="Outstanding Papers Reading (ACL 2019)">
                  <i class="fa fa-angle-left"></i> Outstanding Papers Reading (ACL 2019)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/08/13/CogGraph/" rel="next" title="Study Notes for Cognitive Graph">
                  Study Notes for Cognitive Graph <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:51</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":null,"security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/07/29/CorEx/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
