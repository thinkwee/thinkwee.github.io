<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Selected Reading of ACL&#x2F;NAACL 2019 Automatic Summarization Papers  DPPs Similarity Measurement Improvement STRASS: Backpropagation for Extractive Summarization Translate first, then generate the abst">
<meta property="og:type" content="article">
<meta property="og:title" content="Summarization-Related Papers Reading (ACL&#x2F;NAACL 2019)">
<meta property="og:url" content="https://thinkwee.top/2019/08/15/acl2019-summarization/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Selected Reading of ACL&#x2F;NAACL 2019 Automatic Summarization Papers  DPPs Similarity Measurement Improvement STRASS: Backpropagation for Extractive Summarization Translate first, then generate the abst">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/12/ezCge0.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/14/mkkJHO.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/21/mtX0eS.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/21/mtXczq.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/12/ezCge0.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/14/mkkJHO.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/21/mtX0eS.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/08/21/mtXczq.png">
<meta property="article:published_time" content="2019-08-15T02:51:37.000Z">
<meta property="article:modified_time" content="2025-07-16T10:15:04.327Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="summarization">
<meta property="article:tag" content="natural language processing">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png">


<link rel="canonical" href="https://thinkwee.top/2019/08/15/acl2019-summarization/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/08/15/acl2019-summarization/","path":"2019/08/15/acl2019-summarization/","title":"Summarization-Related Papers Reading (ACL/NAACL 2019)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Summarization-Related Papers Reading (ACL/NAACL 2019) | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Thinking</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">54</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li><li class="menu-item menu-item-ddrbench"><a href="https://thinkwee.notion.site/ddrbench" rel="section" target="_blank"><i class="fa-solid fa-user-secret fa-fw"></i>DDRBench</a></li><li class="menu-item menu-item-nover"><a href="https://huggingface.co/spaces/thinkwee/NOVER" rel="section" target="_blank"><i class="fa fa-wind fa-fw"></i>NOVER</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization"><span class="nav-number">1.</span> <span class="nav-text">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings"><span class="nav-number">2.</span> <span class="nav-text">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-robust-abstractive-system-for-cross-lingual-summarization"><span class="nav-number">3.</span> <span class="nav-text">A
Robust Abstractive System for Cross-Lingual Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction"><span class="nav-number">4.</span> <span class="nav-text">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization"><span class="nav-number">5.</span> <span class="nav-text">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieve"><span class="nav-number">5.1.</span> <span class="nav-text">Retrieve</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rerank"><span class="nav-number">5.2.</span> <span class="nav-text">Rerank</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rewrite"><span class="nav-number">5.3.</span> <span class="nav-text">Rewrite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jointly-learning"><span class="nav-number">5.4.</span> <span class="nav-text">Jointly Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#result"><span class="nav-number">5.5.</span> <span class="nav-text">result</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#biset"><span class="nav-number">5.6.</span> <span class="nav-text">biset</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization"><span class="nav-number">6.</span> <span class="nav-text">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings"><span class="nav-number">7.</span> <span class="nav-text">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-robust-abstractive-system-for-cross-lingual-summarization"><span class="nav-number">8.</span> <span class="nav-text">A
Robust Abstractive System for Cross-Lingual Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction"><span class="nav-number">9.</span> <span class="nav-text">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization"><span class="nav-number">10.</span> <span class="nav-text">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#retrieve"><span class="nav-number">10.1.</span> <span class="nav-text">Retrieve</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rerank"><span class="nav-number">10.2.</span> <span class="nav-text">Rerank</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rewrite"><span class="nav-number">10.3.</span> <span class="nav-text">Rewrite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#jointly-learning"><span class="nav-number">10.4.</span> <span class="nav-text">Jointly Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#result"><span class="nav-number">10.5.</span> <span class="nav-text">result</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#biset"><span class="nav-number">10.6.</span> <span class="nav-text">biset</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">62</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/08/15/acl2019-summarization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Summarization-Related Papers Reading (ACL/NAACL 2019) | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Summarization-Related Papers Reading (ACL/NAACL 2019)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-08-15 10:51:37" itemprop="dateCreated datePublished" datetime="2019-08-15T10:51:37+08:00">2019-08-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 18:15:04" itemprop="dateModified" datetime="2025-07-16T18:15:04+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/2019/08/15/acl2019-summarization/" class="post-meta-item leancloud_visitors" data-flag-title="Summarization-Related Papers Reading (ACL/NAACL 2019)" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>23k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>21 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>Selected Reading of ACL/NAACL 2019 Automatic Summarization Papers</p>
<ul>
<li><p>DPPs Similarity Measurement Improvement</p></li>
<li><p>STRASS: Backpropagation for Extractive Summarization</p></li>
<li><p>Translate first, then generate the abstract</p></li>
<li><p>Reading Comprehension + Automatic Abstract</p></li>
<li><p>BiSET: Retrieve + Fast Rerank + Selective Encoding + Template
Based</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</h1>
<ul>
<li>This is very similar to what the others in our group have done,
using DPPs to process extractive abstractive summaries</li>
<li>In the abstract indicator paper mentioned in the preceding text, it
is also mentioned that creating an abstract, especially a key sentence
abstract, is all about three words: qd, qd, and still qd!
<ul>
<li>q: quality, which sentence is important and needs to be extracted as
an abstract. This step is feature construction.</li>
<li>d: diversity, the sentences extracted should not be redundant, and
should not repeatedly use the same sentences. If there are too many
important sentences that are the same, they become less significant.
This step is the construction of the sampling method.</li>
</ul></li>
<li>Determinantal Point Processes (DPPs) are a sampling method that
ensures the extracted sentences are important (based on precomputed
importance values) and non-repetitive. The authors of this paper have a
very clear line of thought: I want to improve the DPPs in extractive
summarization, how to improve them? DPPs rely on the similarity between
sentences to avoid extracting duplicate sentences, so I will directly
improve the calculation of similarity, thus the problem is shifted to a
very mature field: semantic similarity computation.</li>
<li>Next, just use the web to do semantic similarity calculation. The
author is quite innovative, using capsule networks, which were
originally proposed to solve the problem of relative changes in object
positions in computer vision. The author believes that it can be
generalized to extract spatial and directional information of low-level
semantic features. Here, I am not very familiar with capsule networks
and their applications in NLP, but based on the comparative experiments
provided by the author, the improvement is actually just one point, and
the entire DPP is only 2 points better than the best system before
(2009), which seems a bit forced.</li>
<li>The network provided by the author is truly complex, not in terms of
principle, but due to the use of many components, including:
<ul>
<li>CNN with three to seven different sizes of convolutional kernels for
extracting low-level features</li>
<li>Capsule networks extract high-level features, utilizing recent
techniques such as parameter sharing and routing</li>
<li>One-hot vectors were still used, i.e., whether a word exists in a
certain sentence</li>
<li>Fusion of various features, including inner product, absolute
difference, and concatenation with all independent features, to predict
the similarity between two sentences</li>
<li>And the similarity is only part of the goal; the authors also used
LSTM to reconstruct two sentences, incorporating the reconstruction loss
into the final total loss <img data-src="https://s2.ax1x.com/2019/08/12/ezCge0.png" alt="ezCge0.png" /></li>
</ul></li>
<li>Absolutely, at first glance, one would assume this is a CV's
work</li>
<li>The author at least used the latest available techniques, creating
an integrated network, which may not be as concise and elegant in an
academic sense, but in terms of industry, many such network integration
operations are very effective</li>
<li>Another point is that although it is a sampled abstract, the
author's work is fully supervised, so a dataset still needs to be
constructed
<ul>
<li>Constructing Supervised Extractive Summary Datasets from Generative
Summary Datasets</li>
<li>Constructing a supervised sentence similarity calculation dataset
from generative abstract data sets</li>
<li>This structure also limits its generalization ability to some
extent</li>
</ul></li>
<li>Authors' starting point is actually very good: because traditional
similarity is at the word level, without delving into semantic features,
the direction of constructing a network to extract features is correct,
albeit somewhat complex. Moreover, since only the sentence feature
extraction part of the similarity calculation in the extraction-based
abstracting method has been improved, the overall impact is not
particularly significant. The final result may have outperformed many
traditional methods, but it has not improved much compared to the
traditional best method, and it is only about 1 point better than pure
DPPs.</li>
</ul>
<h1
id="strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/14/mkkJHO.png" alt="mkkJHO.png" />
<figcaption aria-hidden="true">mkkJHO.png</figcaption>
</figure>
<ul>
<li><p>Another paper using supervised methods for extractive
summarization, the content can be roughly guessed from the title, based
on embedding, and aiming to be light and effective, the simplest goal is
to keep the embedding of the summary consistent with the gold
embedding.</p></li>
<li><p>The difficulty lies in the fact that it is extractive and
discrete, thus requiring a process to unify the three parts of
extraction, embedding, and comparison scoring, softening it to be
differentiable, enabling end-to-end training. The authors propose four
steps:</p>
<ul>
<li>Mapping document embeddings to a comparison space</li>
<li>Extract sentences to form an abstract</li>
<li>Extraction-based abstract embedding</li>
<li>Comparison with gold summary embedding</li>
</ul></li>
<li><p>First, given a document, directly obtain the doc embedding and
the sentence embedding for each sentence in the document using
sent2vec</p></li>
<li><p>After that, only one fully connected layer is used as the mapping
function f(d), where the author proposes the first hypothesis: the
extracted abstract sentences should have similarity to the document:</p>
<p><span class="math display">\[
sel(s,d,S,t) = sigmoid (ncos^{+}(s,f(d),S)-t)
\]</span></p></li>
<li><p>s is the sentence embedding, S represents the set of sentences, t
is the threshold. sel represents select, i.e., the confidence of
selecting this sentence to form the summary. This formula indicates that
the similarity between the selected sentence embedding and the document
embedding should be greater than the threshold t, and sigmoid is used
for softening, converting {0,1} to [0,1].</p></li>
<li><p>Afterward, further softening is applied; the author does not
select sentences to form the abstract based on scores, but directly
approximates the abstract's embedding based on scores</p>
<p><span class="math display">\[
app(d,S,t) = \sum _{s \in S} s * nb_w(s) * sel(s,d,S,t)
\]</span></p></li>
<li><p>nb_w is the number of words, i.e., the sum of the embedding of
all sentences weighted by the number of words in each sentence and the
select score to obtain the embedding of the generated summary</p></li>
<li><p>The final step involves comparing the embedding similarity
calculation loss with the gold summary, where the authors introduce a
regularization term to aim for a higher compression ratio of the
extracted summary. I feel that this is a compensation brought about by a
series of softening operations in the previous step, as no sentences are
selected; instead, all sentences are weighted, thus necessitating
regularization to force the model to discard some sentences:</p>
<p><span class="math display">\[
loss = \lambda * \frac{nb_w(gen_sum)}{nb_w(d)} + (1-\lambda) *
cos_{sim}(app(d,S,t),ref_{sum})
\]</span></p></li>
<li><p>What is the method for obtaining the embedding of the gold
summary?</p></li>
<li><p>The authors also normalized the results of the cosine similarity
calculation to ensure that the same threshold could be applied to all
documents</p></li>
<li><p>The results actually show that ROUGE is not as good as generative
methods, of course, one reason is that the dataset is inherently
generative, but it is strong in simplicity, speed, and when using
supervised methods for extraction, there is no need to consider the
issue of redundancy.</p></li>
</ul>
<h1 id="a-robust-abstractive-system-for-cross-lingual-summarization">A
Robust Abstractive System for Cross-Lingual Summarization</h1>
<ul>
<li>In fact, one sentence can summarize this paper: Generate
multilingual abstracts by first translating, while others first abstract
and then translate</li>
<li>All are implemented using existing frameworks
<ul>
<li>Marian: Fast Neural Machine Translation in C++</li>
<li>Abstract: Pointer-generator</li>
</ul></li>
<li>The author actually has sufficient supervisory data; it was
previously thought that the abstracts were multilingual, with small
amounts of corpus, or were summaries not relying on translation, which
could extract common abstract features across multiple languages</li>
<li>However, this paper indeed achieved robustness: generally, to
achieve robustness, one introduces noise, and this paper exactly used
back-translation to introduce noise: first, English is translated into a
minor language, then translated back, and trained a generative abstract
model on this bad English document, making the model more robust to
noise. The final results were also significantly improved, and it also
achieved good effects on Arabic that had not been trained, indicating
that different people's translations have their own correctness, while
the errors in machine translation are always similar.</li>
</ul>
<h1
id="answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/21/mtX0eS.png" alt="mtX0eS.png" />
<figcaption aria-hidden="true">mtX0eS.png</figcaption>
</figure>
<ul>
<li><p>This paper comes from the Smart Lab of NTT, the so-called largest
telecommunications company in the world, and proposes a multi-task
model: Reading Comprehension + Automatic Summary</p></li>
<li><p>The paper conducted many experiments and analyses, and provided a
detailed analysis of the conditions under which their module works, and
also utilized many techniques proposed in recent years for the abstract
section, rather than simply patching together.</p></li>
<li><p>This paper is also based on the HotpotQA dataset, similar to the
one in CogQA, but that one used the full wiki setting, which means there
was no gold evidence. This paper, however, requires gold evidence, so it
used the HotpotQA distractor setting.</p></li>
<li><p>For the distractor setting of HotpotQA, the supervisory signal
consists of two parts: answer and evidence, with the input also having
two parts: query and context, where the evidence is a sentence within
the context. The authors adopt the baseline from the HotpotQA paper:
Simple and effective multi-paragraph reading comprehension, and all
parts except the Query-Focused Extractor shown in the above figure. The
basic idea is to combine the query and context, add a lot of fully
connected (FC), attention, and BiRNN to extract features, and finally
output a classification of answer type and a sequence labeling of answer
span in the answer part, while directly applying the output of the BiRNN
to each sentence for binary classification. <img data-src="https://s2.ax1x.com/2019/08/21/mtXczq.png"
alt="mtXczq.png" /></p></li>
<li><p>The author refines the supervision task of evidence into a
query-based summarization, adding a module called Query-Focused
Extractor (QFE) after the BiRNN, emphasizing that the evidence should be
a summary extracted from the context under the query conditions,
satisfying:</p>
<ul>
<li>Sentences within the summary should not be redundant</li>
<li>sentences within the summary should have different attention based
on the query</li>
</ul></li>
<li><p>For the first point, the author designed an RNN within the QFE,
which allows attention to be paid to previously extracted sentences
during the generation of attention and even the extraction of summaries.
The time step of the RNN is defined as each time a sentence is
extracted, with the input being the vector of the sentence extracted at
that time step</p></li>
<li><p>In response to the second point, the author added an attention
mechanism for the query within the QFE, with the weighted query vector
referred to as glimpse. Note that this is the attention from the QA
context to the QA query; both the key and value in the attention are the
QA query, while the query in the attention does not directly take the
entire QA context but rather the output of the RNN, i.e., the context
encoded by the RNN after extracting a set of sentences. Such a design is
also intuitive.</p></li>
<li><p>After the RNN encodes the extracted sentences and forms glimpse
vectors with attention-weighted queries, QFE receives these two vectors,
combines them with the vectors of unextracted sentences for each
context, to output the probability of each sentence being extracted, and
then selects the sentence with the highest probability to add to the set
of extracted sentences. Subsequently, the system continues to cyclically
calculate the RNN and glimpse. The dependency relationships of the
entire system are clearly shown in the figure above.</p></li>
<li><p>Due to the variable number of sentences in gold evidence, the
author employs the method of adding a dummy sentence with an EOE to
dynamically extract, and when an EOE is extracted, the model no longer
continues to extract sentences.</p></li>
<li><p>During training, the loss function for evidence is:</p>
<p><span class="math display">\[
L_E = - \sum _{t=1}^{|E|} \log (max _{i \in E / E^{t-1}} Pr(i;E^{t-1}))
+ \sum _i min(c_i^t, \alpha _i^t)
\]</span></p>
<p>Here, <span class="math inline">\(E\)</span> is the set of sentences
of gold evidence, <span class="math inline">\(E^t\)</span> is the set of
sentences extracted by QFE, <span class="math inline">\(\alpha
_i^t\)</span> is the attention of the i-th word in the query at time
step t, where the time step is consistent with the previous text, being
the time step for extracting sentences. <span class="math inline">\(c^t
= \sum _{i=1}^{t-1} \alpha ^i\)</span> is the coverage vector. The first
half of the loss refers to the negative log-likelihood loss of the gold
evidence, finding the gold sentence with the highest QFE prediction
probability in the extracted sentence set, calculating the loss, and
then excluding this sentence to find the next highest, until all gold
sentences are found or no gold sentence can be found in the extracted
sentence set. The second half is a regularization application of the
coverage mechanism to ensure that the sentences selected for loss
calculation do not have overly repetitive (concentrated) attention on
the query.</p></li>
<li><p>Authors achieved results on the HotpotQA and textual entailment
dataset FEVER, with the evidence part of the indicators far superior to
the baseline, while the answer part also saw a significant improvement,
though not as pronounced as the evidence part, and slightly inferior to
the BERT model. On the full wiki test set, it was also comprehensively
surpassed by CogQA. Here, the authors state that there is a dataset
shift problem. However, at least this paper achieved an 8-point
improvement on the answer part by simply adding a small module to the
baseline, demonstrating that a well-designed summarization part indeed
helps in the selection of answers in multi-task learning.</p></li>
</ul>
<h1
id="biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</h1>
<ul>
<li>Another model pieced together from various components, the title
actually spells it all out: Bi-directional, selective encoding,
template, together forming the BiSET module, and the other two preceding
processes: Retrieve and Fast Rerank also follow the architecture from
the paper "Retrieve, Rerank and Rewrite: Soft Template Based Neural
Summarization." It should be based on soft template summarization, with
the mechanism of selective encoding added, so these two papers are put
together to discuss template-based generative summarization and its
improvements.</li>
<li>The idea behind the soft template approach is not to let the model
generate sentences entirely, but rather for humans to provide the
template and for the model to only fill in the words. However, if the
template is completely designed manually, it would regress to the
methods of several decades ago. The author's approach is to
automatically extract templates from existing gold summaries.</li>
<li>Generally divided into three steps:
<ul>
<li>Retrieve: Extract candidates from the training corpus</li>
<li>Rerank: Learning Template Saliency Measurement for seq2seq
Models</li>
<li>Rewriting: Let the seq2seq model learn to generate the final
summary</li>
</ul></li>
<li>This method should be more suitable for long sentence compression,
or for single-sentence generative summarization, where the long
sentences to be compressed can be used as queries for retrieval</li>
</ul>
<h2 id="retrieve">Retrieve</h2>
<ul>
<li>Utilizing the existing Lucene search engine, given a long sentence
to be compressed as a query, search the document collection to identify
the summaries of the top 30 documents as candidate templates</li>
</ul>
<h2 id="rerank">Rerank</h2>
<ul>
<li><p>The abstracts (soft template) retrieved through the search are
sorted by relevance, but we require sorting by similarity. Therefore, we
use the ROUGE score to measure the similarity between the soft template
and the gold summary. Here, reranking is not about sorting out the
results but rather considering the rank of each template comprehensively
during the generation of the summary, and the loss can be observed in
the parts that are omitted.</p></li>
<li><p>Specifically, first use a BiLSTM encoder to encode the input x
and a certain candidate template r; here, the hidden layer states are
encoded separately, but the same encoder is used, and then input the two
hidden layer states into a Bilinear network to predict the ROUGE value
between the gold summary y corresponding to the input x and r, which is
equivalent to a network that makes a saliency prediction for r given
x:</p>
<p><span class="math display">\[
h_x = BiLSTM(x) \\
h_r = BiLSTM(r) \\
ROUGE(r,y) = sigmoid(h_r W_s h_x^T + b_s) \\
\]</span></p></li>
<li><p>This completes the supervised part of reranking</p></li>
</ul>
<h2 id="rewrite">Rewrite</h2>
<ul>
<li>This part is a standard seq2seq, still using the previously encoded
<span class="math inline">\(h_x, h_r\)</span> to concatenate it and feed
it into an attentional RNN decoder to generate an abstract, and
calculate the loss</li>
</ul>
<h2 id="jointly-learning">Jointly Learning</h2>
<ul>
<li>The model's loss is divided into two parts. The Rerank part ensures
that the encoded template and the input, after passing through bilinear
processing, can correctly predict the ROUGE value. The Rewrite part
ensures the generation of a correct summary. This is equivalent to, in
addition to the ordinary seq2seq summary generation, I also candidate
some other gold summaries as input. This candidate is initially filtered
through retrieval. When used, the Rerank part guarantees that the
encoded part is the template component within the summary, i.e., the
part that can be taken out and compared with the gold summary, thereby
assisting the decoder in the Rewrite part's generation.</li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>We know that in summarization, the decoder is actually very
dependent on the encoder's input, which includes both the template and
the original input. The authors provide several ideal examples, where
the output summary is basically in the format of the template, but the
key entities are extracted from the original input and filled into the
template summary.</li>
<li>Although a somewhat esoteric rerank loss method was used for
extracting the soft template, the role of the template is indeed
evident. The model actually finds a summary that is very close to the
gold summary as input, and makes slight modifications (rewrites) on this
basis, which is much more efficient than end-to-end seq2seq. The authors
also tried removing the retrieve step and directly finding the ROUGE
score highest summary from the entire corpus as the template, with the
final model's results reaching 50 ROUGE-1 and 48 ROUGE-L</li>
<li>This operation of taking the output as input is actually a
compensation for the insufficient abstract ability of the decoder, and
it is an empirical method derived from the observation of the dataset,
which can effectively solve the problem</li>
</ul>
<h2 id="biset">biset</h2>
<ul>
<li>Replaced the rerank part with CNN+GLU to encode documents and
queries, and then computed the sim matrix using the encoded vectors</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</h1>
<ul>
<li>这和我们组其他人做的很像，用DPPs处理抽取式文摘</li>
<li>在上文的文摘指标论文中也提到了，搞文摘，尤其是抽取式文摘，就是三个词，qd，qd，还是qd!
<ul>
<li>q:quality，哪个句子重要，需要被抽出来作为文摘。这一步是特征的构造</li>
<li>d:diversity,抽取出来的句子之间不能冗余，不能老抽一样的句子。重要的一样的句子多了，也就变得不重要了。这一步是抽样方法的构造</li>
</ul></li>
<li>DPPs(Determinantal Point
Processes)就是一种抽样方法，保证抽出来的句子重要（根据特征计算好的重要性数值），而且不重复。本文作者就是思路非常清晰：我要改进抽取式文摘中的DPPs，怎么改进？DPPs依赖句子之间的相似度来避免抽重复的句子，那我就改进不管DPPs怎么改，直接改相似度的计算，所以问题就换到了一个非常成熟的领域：语义相似度计算。</li>
<li>接下来就随便套网络来做语义相似度计算就行了。作者比较新潮，用了胶囊网络，这个网络本来是提出用于解决计算机视觉中物体的位置相对变化问题的，作者认为可以将其泛化到提取底层语义特征的空间与方位信息，这里我对胶囊网络及其在NLP的应用不太了解，但是就作者给出的对比实验来说，改进其实也就1个点，而整个DPP相比之前最好系统（2009）也就好2个点，感觉还是有点刻意为之。</li>
<li>另外作者给出的网络是真的复杂，不是原理复杂，而是用了很多组件，包括：
<ul>
<li>三四五六七大小卷积核的CNN用于提取底层特征</li>
<li>胶囊网络提取高层特征，用到了近年来的参数共享和路由等技巧</li>
<li>还是用了one-hot向量，即一个词是否存在在某一个句子里</li>
<li>各种特征的融合，包括内积，绝对差，再和所有独立的特征全部拼接到一起，预测两个句子的相似度</li>
<li>而且相似度还只是一部分目标，另外作者还用了LSTM来重构两个句子，将重构损失加入最终的总损失
<img data-src="https://s2.ax1x.com/2019/08/12/ezCge0.png"
alt="ezCge0.png" /></li>
</ul></li>
<li>粗看图片绝对以为这是一篇CV的work</li>
<li>作者好歹是把最近能用的技巧都用上了，做了一个集大成的网络，可能学术上看没那么简洁优美，但是就工业上来说很多这样的网络集成操作就是很work</li>
<li>另外虽然是抽取式摘要，但作者的工作是完全监督，因此还需要构造数据集
<ul>
<li>从生成式摘要数据集中构造有监督抽取式摘要数据集</li>
<li>从生成式摘要数据集中构造有监督句子相似度计算据集</li>
<li>这种构造也一定程度上限制了其泛化能力</li>
</ul></li>
<li>作者的出发点其实非常好：因为传统的相似度停留在词的粒度，没有深入到语义特征，因此构造网络提取特征的方向没错，只不过稍显复杂，而且由于仅仅改进了抽取式文摘中某一种抽样方法的相似度计算中的句子特征提取部分，对整体的影响并没有特别大，最后的结果虽然吊打了很多传统方法，但是相比传统最佳方法并没有提高多少，相比纯DPPs更是只有1个点左右的提高。</li>
</ul>
<h1
id="strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/14/mkkJHO.png" alt="mkkJHO.png" />
<figcaption aria-hidden="true">mkkJHO.png</figcaption>
</figure>
<ul>
<li><p>又是一篇用监督方法做抽取式文摘的，从标题就可以大致猜出来内容，基于embedding，还要light
and effective，那最简单的目标就是把摘要的embedding和gold
embedding保持一致。</p></li>
<li><p>困难的地方在于这是抽取式的，是离散的，因此需要一个流程把抽取、embedding、比较打分三个部分统一起来，软化使其可导，可以端到端训练，作者给出的是四个步骤：</p>
<ul>
<li>将文档embedding映射到比较空间</li>
<li>抽句子组成摘要</li>
<li>去近似抽出的摘要的embedding</li>
<li>和gold summary的embedding比较</li>
</ul></li>
<li><p>首先给定一篇文档，直接用sent2vec获得doc
embedding和文档里每一句的sentence embedding</p></li>
<li><p>之后只用一层全连接作为映射函数f(d)，这里作者给出了第一个假设：抽取的摘要句子应该和文档具有相似度：</p>
<p><span class="math display">\[
sel(s,d,S,t) = sigmoid (ncos^{+}(s,f(d),S)-t)
\]</span></p></li>
<li><p>其中s是句子embedding，S代表句子集，t为阈值。
sel代表select，即选择这个句子组成摘要的置信度，这个式子说明选出句子的embedding和文档embedding之间的相似度应该大于阈值t，且使用sigmoid做了软化，将{0,1}软化为[0,1]</p></li>
<li><p>之后进一步软化，作者并不根据分数选出句子组成文摘，而是根据分数直接近似文摘的embedding</p>
<p><span class="math display">\[
app(d,S,t) = \sum _{s \in S} s * nb_w(s) * sel(s,d,S,t)
\]</span></p></li>
<li><p>其中nb_w是number of words，即使用每个句子的字数和select
score对所有的句子embedding加权求和得到generated
summary的embedding</p></li>
<li><p>最后一步，和gold
summary比较embedding相似度计算损失，这里作者加入了一个正则项，希望提出来的摘要压缩比越高越好，这里我感觉是上一步一系列软化操作带来的补偿，因为没有选择句子，而是对所有句子加权，因此需要正则强迫模型放弃一些句子：</p>
<p><span class="math display">\[
loss = \lambda * \frac{nb_w(gen_sum)}{nb_w(d)} + (1-\lambda) *
cos_{sim}(app(d,S,t),ref_{sum})
\]</span></p></li>
<li><p>这里有一个问题，gold summary的embedding是怎么得到的？</p></li>
<li><p>另外为了保证能够对所有文档使用同一个阈值，作者还对cosine相似度计算的结果做了归一化</p></li>
<li><p>从结果来看其实ROUGE还不如生成式的方法好，当然一方面原因是因为数据集本来就是生成式的，但是强在简单，快，而且用监督的方法做抽取式也不用考虑redundency的问题。</p></li>
</ul>
<h1 id="a-robust-abstractive-system-for-cross-lingual-summarization">A
Robust Abstractive System for Cross-Lingual Summarization</h1>
<ul>
<li>其实一句话就能概括这篇论文：做多语言生成式文摘，别人是先摘要再翻译，这篇文章是先翻译再生成摘要</li>
<li>均是用已有框架实现
<ul>
<li>翻译：Marian: Fast Neural Machine Translation in C++</li>
<li>摘要：pointer-generator</li>
</ul></li>
<li>作者其实有充足的监督数据，之前以为是多语言、小语料的摘要，或者是不借助于翻译的摘要，能够挖掘多语种共有的摘要特征</li>
<li>但是这篇论文确实实现了robust：一般要做robust就是引入噪声，本文正好用了回译引入噪声：先把英语翻译成小语种，翻译回来，在这样的bad
english
document上训练生成式摘要模型，使得模型对噪声更加鲁棒，最后的结果也是提高了许多，并且对没有训练过的阿拉伯语也取得了较好的效果，说明不同的人翻译各有各的正确，而机器翻译的错误总是相似的。</li>
</ul>
<h1
id="answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/21/mtX0eS.png" alt="mtX0eS.png" />
<figcaption aria-hidden="true">mtX0eS.png</figcaption>
</figure>
<ul>
<li><p>这篇论文来自号称全球最大电信公司NTT的智能实验室，提出了一个多任务模型：阅读理解+自动摘要</p></li>
<li><p>论文做了很多实验和分析，并且详细分析了在何种情况下他们的module
works，对于摘要部分也利用了许多近年来提出的技巧，而不是简单的拼凑。</p></li>
<li><p>这篇论文同样也是在HotpotQA数据集上做，和CogQA那一篇一样，但那一篇用的是full
wiki setting，即没有gold evidence，而这篇需要gold
evidence因此用了HotpotQA 的distractor setting。</p></li>
<li><p>对于HotpotQA的distractor
setting，监督信号有两部分：answer和evidence，输入有两部分:query和context，其中evidence是context当中的句子。作者沿用了HotpotQA论文里的baseline:Simple
and effective multi-paragraph reading
comprehension，及上图中Query-Focused
Extractor以外的部分。基本思想就是将query和context结合，加上一堆FC,attention，BiRNN提取特征，最终在answer部分输出一个answer
type的分类和answer span的sequence
labelling，而在evidence部分直接接BiRNN输出的结果对每个句子做二分类。
<img data-src="https://s2.ax1x.com/2019/08/21/mtXczq.png"
alt="mtXczq.png" /></p></li>
<li><p>作者将evidence这边的监督任务细化为一个query
based的summarization，就在BiRNN后面加了一个模块，称之为Query-Focused
Extractor(QFE)，相比原始的简单二分类，QFE强调了evidence应该是在query条件下从context中抽取出来的summary，因满足：</p>
<ul>
<li>summary内的句子之间应该不能冗余</li>
<li>summary内不同句子应该有着query上不同的注意力</li>
</ul></li>
<li><p>针对第一点，作者在QFE内设计了一个RNN，使得在生成注意力乃至抽取摘要时都能注意到之前已经抽取出来的句子,其中RNN的时间步定义为每一次抽取一个句子，输入即某一时间步抽取出的句子的vector</p></li>
<li><p>针对第二点，作者在QFE内增加了一个针对query的注意力，加权之后的query向量称为glimpse，注意这里是QA
context对QA
query的注意力，attention里的key和value都是QA的query，而attention里的query不是直接拿整个QA
context，而是RNN的输出，即已经抽取出的句子集经过RNN编码的context，这样的设计也是符合直觉的。</p></li>
<li><p>在RNN编码已抽取句子、注意力加权query形成glimpse向量之后，QFE拿到这两部分向量，结合每一个context未抽取句子的向量来输出每一个句子被抽取的概率，并选择最大概率的句子加入已抽取句子集合，然后接着循环计算RNN和glimpse。整个系统的依赖关系在上图中展示的很清晰。</p></li>
<li><p>由于gold evidence的句子数目不固定，作者采用添加一个EOE的dummy
sentence的方法来动态抽取，当抽取到EOE时，模型就不再接着抽取句子。</p></li>
<li><p>在训练时，evidence这边的损失函数为：</p>
<p><span class="math display">\[
L_E = - \sum _{t=1}^{|E|} \log (max _{i \in E / E^{t-1}} Pr(i;E^{t-1}))
+ \sum _i min(c_i^t, \alpha _i^t)
\]</span></p>
<p>这里<span class="math inline">\(E\)</span>是gold
evidence的句子集合，<span
class="math inline">\(E^t\)</span>是QFE抽取出来的句子集合，<span
class="math inline">\(\alpha
_i^t\)</span>是t时间步query里第i个词的注意力，这里的时间步和前文一致，是抽句子的时间步。而<span
class="math inline">\(c^t = \sum _{i=1}^{t-1} \alpha
^i\)</span>是coverage向量。 损失的前半部分指的是gold
evidence的负对数似然损失，依次在抽取句子集合里找拥有最大QFE预测概率的gold
sentence，算损失，然后排除这个句子接着找下一个最大的，直到gold
sentence找完或者抽取句子集合里找不到gold
sentence，后半部分是coverage机制的一个正则化应用，保证挑出来计算损失的句子不会在query上拥有过于重复（集中）的注意力。</p></li>
<li><p>作者在HotpotQA和文本蕴含数据集FEVER上做了结果，evidence部分的指标远好于baseline，而answer部分的指标也有较大提升，但不如evidence部分明显，且与BERT模型部分相比还差一点，在full
wiki setting的测试集上也被CogQA全面超过，这里作者说存在dataset
shift问题。但至少本文仅仅在baseline上添加了一个小模块，就获得了answer部分的8个点的提升，说明精心设计的summarization部分在多任务学习中确实帮助到了answer的选取。</p></li>
</ul>
<h1
id="biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</h1>
<ul>
<li>又是一篇将各个组件拼拼凑凑出来的一个模型，标题其实已经全写出来了：Bi-directional，
selective encoding，
template，共同组成了BiSET模块，另外两个前置过程：Retrieve和Fast
Rerank也是沿用Retrieve, Rerank and Rewrite: Soft Template Based Neural
Summarization这篇论文里的架构。应该大体是基于soft
template的summarization，加上了selective
encoding的机制，因此就把这两篇论文放在一起，讨论基于模板的生成式摘要及其改进。</li>
<li>基于软模板的思想是，不要完全让模型来生成句子，而是人给出模板，模型只负责填词。然而完全人工设计模板那就退化到几十年前的方式了，作者的思路是，从已有的gold
summary中自动提取模板。</li>
<li>大体分为三步：
<ul>
<li>Retrieve：从训练语料中检索出候选软模板</li>
<li>Rerank：让seq2seq模型学习到template saliency measurement</li>
<li>Rewrite：让seq2seq模型学习到final summary generation</li>
</ul></li>
<li>这种方法应该比较适用于长句压缩，或者说单句生成式摘要，这样待压缩的长句可以作为query进行retrieve</li>
</ul>
<h2 id="retrieve">Retrieve</h2>
<ul>
<li>使用现成的Lucene搜索引擎，给定要压缩的一个长句作为query，从文档集中搜索出top
30篇文档的summary作为候选模板</li>
</ul>
<h2 id="rerank">Rerank</h2>
<ul>
<li><p>经过搜索搜出来的摘要（soft
template)是按照搜索相关度排序的，但我们需要的是按照摘要相似度排序，因此我们使用ROUGE值衡量soft
template和gold
summary之间的相似程度，这里的rerank并不是要真的排序出来，而是在生成摘要时综合考虑每个template的rank程度，之后在损失部分可以看出来。</p></li>
<li><p>具体而言，先用一个BiLSTM编码器编码输入x和某一个候选模板r，这里是分别编码隐层状态，但是共用编码器，之后将两个隐层状态输入一个Bilinear网络预测出输入x对应的gold
summary y和r之间的ROUGE值，相当于这是一个给定x，给r做出saliency
prediction的网络：</p>
<p><span class="math display">\[
h_x = BiLSTM(x) \\
h_r = BiLSTM(r) \\
ROUGE(r,y) = sigmoid(h_r W_s h_x^T + b_s) \\
\]</span></p></li>
<li><p>这就完成了rerank的监督部分</p></li>
</ul>
<h2 id="rewrite">Rewrite</h2>
<ul>
<li>这部分就是普通的seq2seq，依然是利用之前编码好的<span
class="math inline">\(h_x, h_r\)</span>，将其拼接起来送入一个attentional
RNN decoder生成摘要，计算损失</li>
</ul>
<h2 id="jointly-learning">Jointly Learning</h2>
<ul>
<li>模型的损失分为两部分，Rerank部分要保证编码出来的template和输入再经过bilinear之后能正确预测ROUGE值，Rewrite部分要保证生成正确的摘要，相当于在普通的seq2seq生成摘要之外，我还候选了一些其他的gold
summary作为输入，这个候选是通过retrieve的方式粗筛选的，具体使用时通过Rerank部分保证encode出来的部分是summary里的template成分，即可以拿出来和gold
summary比对的部分，从而辅助rewrite部分decoder的生成。</li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>我们知道做summarization，decoder通过注意力其实是很依赖encoder的输入的，这里的encoder输入既包含template，又包含原始输入，作者给出了几个比较理想的例子，即输出的summary基本上按照template的格式，但是在关键实体部分从原始输入中提取实体填到template
summary当中。</li>
<li>虽然如何提取soft template这方面使用了一个比较玄学的rerank
loss的方式，但是template的作用确实很明显，模型实际上是找到和gold
summary很接近的一个summary作为输入，在此基础上稍加更改(rewrite)，效率远比端到端的seq2seq好，作者还尝试了去掉retrieve，直接从整个语料中找ROUGE最高的summary作为template，最后模型出来的结果高达50的ROUGE-1，48的ROUGE-L</li>
<li>这种找输出作为输入的操作，其实是对decoder抽象能力不足的一种补偿，是对数据集观察得出的经验方法，能很实际的解决问题</li>
</ul>
<h2 id="biset">biset</h2>
<ul>
<li>将rerank部分换成了CNN+GLU来编码文档和查询，编码后的向量计算sim
matrix，</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/summarization/" rel="tag"># summarization</a>
              <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/08/13/CogGraph/" rel="prev" title="Study Notes for Cognitive Graph">
                  <i class="fa fa-angle-left"></i> Study Notes for Cognitive Graph
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/09/23/easyrl/" rel="next" title="Easy Reinforcement Learning Notes">
                  Easy Reinforcement Learning Notes <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">17:13</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/08/15/acl2019-summarization/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
