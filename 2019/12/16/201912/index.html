<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Paper reading on  GNN Pooling Discourse-Aware Summarization Siamese BERT Large Chatbot">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Reading 4">
<meta property="og:url" content="https://thinkwee.top/2019/12/16/201912/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Paper reading on  GNN Pooling Discourse-Aware Summarization Siamese BERT Large Chatbot">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/29/1MfDOI.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/29/1MHPr4.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/29/1MfDOI.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/29/1MHPr4.png">
<meta property="article:published_time" content="2019-12-16T08:07:25.000Z">
<meta property="article:modified_time" content="2025-07-15T20:39:53.339Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="graph neural network">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="summarization">
<meta property="article:tag" content="natural language processing">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">


<link rel="canonical" href="https://thinkwee.top/2019/12/16/201912/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2019/12/16/201912/","path":"2019/12/16/201912/","title":"Paper Reading 4"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper Reading 4 | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">50</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#edge-contraction-pooling-for-graph-neural-networks"><span class="nav-number">1.</span> <span class="nav-text">Edge
Contraction Pooling for Graph Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization"><span class="nav-number">2.</span> <span class="nav-text">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents"><span class="nav-number">3.</span> <span class="nav-text">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sentence-bert-sentence-embeddings-using-siamese-bert-networks"><span class="nav-number">4.</span> <span class="nav-text">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#towards-a-human-like-open-domain-chatbot"><span class="nav-number">5.</span> <span class="nav-text">Towards a Human-like
Open-Domain Chatbot</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#edge-contraction-pooling-for-graph-neural-networks"><span class="nav-number">6.</span> <span class="nav-text">Edge
Contraction Pooling for Graph Neural Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization"><span class="nav-number">7.</span> <span class="nav-text">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents"><span class="nav-number">8.</span> <span class="nav-text">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sentence-bert-sentence-embeddings-using-siamese-bert-networks"><span class="nav-number">9.</span> <span class="nav-text">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#towards-a-human-like-open-domain-chatbot"><span class="nav-number">10.</span> <span class="nav-text">Towards a Human-like
Open-Domain Chatbot</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2019/12/16/201912/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper Reading 4 | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Reading 4
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-12-16 16:07:25" itemprop="dateCreated datePublished" datetime="2019-12-16T16:07:25+08:00">2019-12-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 04:39:53" itemprop="dateModified" datetime="2025-07-16T04:39:53+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/2019/12/16/201912/" class="post-meta-item leancloud_visitors" data-flag-title="Paper Reading 4" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>12 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<p>Paper reading on</p>
<ul>
<li>GNN Pooling</li>
<li>Discourse-Aware Summarization</li>
<li>Siamese BERT</li>
<li>Large Chatbot</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="edge-contraction-pooling-for-graph-neural-networks">Edge
Contraction Pooling for Graph Neural Networks</h1>
<ul>
<li><p>A new GNN pooling method that considers edges</p></li>
<li><p>Significance of pooling in GNNs:</p>
<ul>
<li>Identify clusters based on features or structure</li>
<li>Reduce computational complexity</li>
</ul></li>
<li><p>The authors' edgepool method can improve graph classification and
node classification performance.</p></li>
<li><p>There are two types of pooling: fixed and learned. The authors
briefly introduce three learned pooling methods:</p>
<ul>
<li><p>DiffPool: DiffPool learns a probability allocation, using one GNN
to learn embedding and another to learn cluster assignment, treating the
cluster assignment as a soft assign matrix <span
class="math inline">\(S\)</span>. Nodes are assigned to clusters based
on node features, with a predetermined number of clusters. Each layer
pools both embedding and adjacency matrix simultaneously, as
follows:</p>
<p><span class="math display">\[
\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1}
\times d}} \\
{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times
n_{l+1}}}\end{array} \\
\]</span></p>
<p>Problems include: fixed cluster number; assignment based solely on
node features without considering node distances; cluster assignment
matrix linearly related to node count, difficult to scale; challenging
to train</p></li>
<li><p>TopKPool: A straightforward approach that learns a projection
vector, projecting each node's features to a single weighted value,
retaining the top-k nodes. Issues include inability to modify the graph
(add nodes) and potential information loss due to hard
assignment</p></li>
<li><p>SAGPool: An improvement on TopK, using attention-weighted
neighborhood nodes before projection, but still maintaining a hard topk
assignment</p></li>
</ul></li>
<li><p>The edge pooling concept reduces sampling through edge
contraction. Given an edge e with nodes <span
class="math inline">\(v_i\)</span> and <span
class="math inline">\(v_j\)</span>, edge contraction means connecting
all adjacent nodes of i and j to a new node <span
class="math inline">\(v_e\)</span>. This operation can be repeated
multiple times, similar to expanding receptive field in CNNs.</p></li>
<li><p>How to select edges?</p>
<ul>
<li><p>First, calculate edge scores by concatenating and linearly
transforming the embeddings of connected nodes</p>
<p><span class="math display">\[
r(e_{ij}) = W (n_i || n_j) + b
\]</span></p></li>
<li><p>Then normalize all scores using softmax, with the author adding
0.5 to ensure a mean of 1, explained as improving numerical stability
and gradient propagation</p>
<p><span class="math display">\[
s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
\]</span></p></li>
<li><p>Begin contracting edges based on scores, avoiding contraction of
already contracted edge nodes. This reduces nodes by half each
time.</p></li>
</ul></li>
<li><p>The new node score is directly obtained by weighted averaging of
the two endpoint node features:</p>
<p><span class="math display">\[
\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
\]</span></p></li>
</ul>
<h1
id="discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</h1>
<ul>
<li><p>Using a hierarchical LSTM encoder + LSTM decoder for extractive
summarization as a baseline, the authors added a three-layer attention
to incorporate discourse information. Specifically, discourse
information refers to sentence-level elaborate relationships, where one
sentence provides detailed explanation or supplementary information
about another. The authors argue that document summarization, as a
discourse-level task, naturally requires discourse information.</p></li>
<li><p>The authors use attention to learn directed elaborate edges
between sentences, as shown in the following diagram: <img data-src="https://s2.ax1x.com/2020/01/29/1MfDOI.png"
alt="1MfDOI.png" /></p></li>
<li><p>Three components:</p>
<ul>
<li><p>Parent Attention: Use hierarchical encoder to obtain sentence
representations, then use attention to represent the probability of
sentence k being the parent node of sentence i, with elaborate edges
pointing from k to i (without using self-attention)</p>
<p><span class="math display">\[
\begin{aligned} p(k | i, \mathbf{H}) &amp;=\operatorname{softmax}(g(k,
i)) \\ g(k, i) &amp;=v_{a}^{\mathrm{T}} \tanh \left(U_{a} \cdot
H_{k}+W_{a} H_{i}\right) \end{aligned}
\]</span></p></li>
<li><p>Recursive Attention: Calculate multi-hop parent nodes, obtaining
the probability of k being the d-hop parent node of i. This can be
simply achieved by powering the attention matrix, with special handling
for the root sentence (virtual node) which has no parent nodes:</p>
<p><span class="math display">\[
\alpha_{d, k, i}=\left\{\begin{array}{ll}{p(k | i, \mathbf{H})} &amp;
{(d=1)} \\ {\sum_{l=0}^{N} \alpha_{d-1, k, l} \times \alpha_{1, l, i}}
&amp; {(d&gt;1)}\end{array}\right.
\]</span></p></li>
<li><p>Selective Attention: Combine attention information by first
weighted summing parent node information for sentence i at each hop:</p>
<p><span class="math display">\[
\gamma_{d, i}=\sum_{k=0}^{N} \alpha_{d, k, i} H_{k}
\]</span></p>
<p>Then calculate hop weights using selective attention, depending on
sentence i's encoder and decoder states <span
class="math inline">\(H,s\)</span>, and encoder states of all parent
nodes:</p>
<p><span class="math display">\[
\beta_{d, i}=\operatorname{softmax}\left(\mathbf{W}_{\beta}\left[H_{i} ;
s_{i} ; K\right]\right)
\]</span></p>
<p>Obtain weighted information from all hops and append to decoder
input</p>
<p><span class="math display">\[
\Omega_{i}=\sum_{d} \beta_{d, i} \gamma_{d, i} \\
p\left(y_{i} | \mathbf{x},
\theta\right)=\operatorname{softmax}\left(\mathbf{W}_{o} \tanh
\left(\mathbf{W}_{c^{\prime}}\left[H_{i} ; s_{t} ; K ;
\Omega_{i}\right]\right)\right) \\
\]</span></p></li>
</ul></li>
<li><p>The authors mention that Rhetorical Structure Analysis (RST)
currently lacks good off-the-shelf tools with high accuracy. They
propose a joint learning framework, which turns out to mean using
existing RST Parsers to obtain elaborate edges during training to guide
Parent Attention, with no parser needed during testing. The parser's
errors still significantly impact the model. The objective function
is:</p>
<p><span class="math display">\[
-\log p(\mathbf{y} | \mathbf{x})-\lambda \cdot \sum_{k=1}^{N}
\sum_{i=1}^{N} E_{k, i} \log \alpha_{1, k, i}
\]</span></p>
<p>The second term guides attention using parser-obtained edges</p></li>
<li><p>The authors first use HILDA parser to obtain RST discourse
annotations, then convert them to dependency format using a method from
"Single-document summarization as a tree knapsack problem"</p></li>
<li><p>Although still dependent on parser during training, the authors
created two baselines: one without parser using the previous sentence as
the elaborate parent, another letting attention learn independently.
Results showed the parser-informed attention model outperformed
baselines. The model showed more significant advantages on short texts
(75 words) compared to long texts (275 words) in the Daily Mail dataset,
partly due to ROUGE metric's preference for longer texts, indicating
discourse information indeed helps in extracting the most important
information within word count constraints.</p></li>
<li><p>This paper can be seen as an attention model (self-attention +
multi-blocks) injecting prior information to achieve better results in
single-document extractive summarization.</p></li>
</ul>
<h1
id="a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</h1>
<ul>
<li>A NAACL 2018 paper considering discourse information for abstractive
summarization on research paper datasets</li>
<li>Here, discourse is narrowly defined as sections in research papers,
essentially a hierarchical attention model built upon the
pointer-generator, with the following structure: <img data-src="https://s2.ax1x.com/2020/01/29/1MHPr4.png" alt="1MHPr4.png" /></li>
<li>Praiseworthy is the authors' provision of two large-scale
long-document research paper summary datasets, PubMed and arXiv, both
reaching tens of thousands in scale, with average source document
lengths over 3000 and 4900 words, and average summary lengths exceeding
100 words - valuable ultra-long single-document summary datasets.</li>
</ul>
<h1
id="sentence-bert-sentence-embeddings-using-siamese-bert-networks">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</h1>
<ul>
<li>Highlight: Sentence pair regression tasks with BERT are
time-consuming. The authors propose a Siamese BERT network, improving
inference speed by 1,123,200 times</li>
<li>Evidently, this speedup claim is ambiguous. Naive BERT is slow in
semantic matching tasks because each pair requires sending two sentences
through BERT to calculate scores. The authors slightly modify BERT to
use embeddings as sentence feature vectors, directly using cosine
distance for matching</li>
<li>Next, they prove that original BERT embeddings are not good semantic
matching features. SBERT adds regression or classification layers after
BERT and introduces triplet loss, significantly improving performance
over original BERT. It can be seen as a fine-tuning of BERT for semantic
matching tasks.</li>
</ul>
<h1 id="towards-a-human-like-open-domain-chatbot">Towards a Human-like
Open-Domain Chatbot</h1>
<ul>
<li>Highlight: Google-produced, large. Researching detailed design
aspects</li>
<li>2.6 billion parameters. 40 billion token corpus. To capture
multi-turn dialogue quality, the authors propose Sensibleness and
Specificity Average (SSA) as a metric, finding that models optimized for
perplexity achieve the best SSA</li>
<li>The authors use evolved transformer, training a seq2seq model with
multi-turn dialogue input, vocabulary size of 8k (using BPE), achieving
a test set perplexity of only 10.2. The model outperforms other dialogue
systems supplemented with rules, systems, and knowledge, again proving
that deep neural networks can achieve miracles with sufficient data and
training</li>
<li>SSA measures two aspects: sensible and specific. It's a
human-evaluated metric where testers first judge if the response is
sensible, and if so, then judge its specificity, as systems often
scoring well on automatic metrics tend to give vague "I don't know"
responses. The authors found SSA correlates with human assessments of
system human-likeness</li>
<li>SSA has two testing environments: a specified test set of 1,477
multi-turn dialogues, and direct chatting with the system for 14-28
turns</li>
<li>The authors provide numerous training and testing details,
essentially highlighting the model's scale: trained on one TPU-v3 Pod
for 30 days, 164 epochs, observing 10T tokens in total</li>
<li>This powerful yet simple model doesn't require complex decoding to
ensure high-quality, diverse responses. The authors used sample and
rank: dividing logits by temperature T, then softmax, randomly sampling
multiple sequences based on probability and selecting the
highest-probability sequence. Higher temperatures reduce logits'
differences, facilitating context-related rare word generation. Sample
and rank surprisingly outperforms beam search, provided the model
achieves low perplexity. The authors set temperature to 0.88, sampling
20 sentences</li>
<li>Statistical tests revealed a correlation coefficient exceeding 0.9
between perplexity and SSA</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="edge-contraction-pooling-for-graph-neural-networks">Edge
Contraction Pooling for Graph Neural Networks</h1>
<ul>
<li><p>一种新的GNN池化方式，考虑了边</p></li>
<li><p>池化在GNN中的意义：</p>
<ul>
<li>识别基于特征或者基于结构的聚类</li>
<li>减少计算量</li>
</ul></li>
<li><p>作者提出的edgepool能够提高图分类和节点分类的性能。</p></li>
<li><p>pooling有两种，fixed和learned，作者简单介绍了三种learned pooling
method</p>
<ul>
<li><p>DiffPool：DiffPool学习到一种概率分配，用一个GNN学习embedding，用一个GNN学习聚类分配，将聚类分配视为一个soft
assign matrix<span
class="math inline">\(S\)</span>，基于节点特征将每个节点分配给一个聚类，聚类数量事先固定，每一层同时对embedding和邻接矩阵进行pooling，如下：</p>
<p><span class="math display">\[
\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1}
\times d}} \\
{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times
n_{l+1}}}\end{array} \\
\]</span></p>
<p>问题在于：聚类数量不可变；基于节点特征分配而不考虑节点之间距离；聚类分配矩阵与节点数目成线性关系，难以scale；难以训练</p></li>
<li><p>TopKPool：简单粗暴，学习到一个投影向量，将每个节点的特征投影加权为一个单值，取topk个节点保留作为Pooling，问题在于不能改变图（加节点），以及这种hard
assignment容易丢失信息</p></li>
<li><p>SAGPool：对TopK的改进，对邻域节点使用了注意力加权，再投影，不过依然是topk的hard
assignment。</p></li>
</ul></li>
<li><p>edge pooling的思想是通过边的收缩(edge
contraction)来降采样，给定一条边e，两边节点<span
class="math inline">\(v_i\)</span>和<span
class="math inline">\(v_j\)</span>，边收缩指的是将i和j的所有邻接节点全部接到一个新节点<span
class="math inline">\(v_e\)</span>，这个操作显然是可以叠加多次，类似于CNN的不断扩大感受野。</p></li>
<li><p>如何选边？</p>
<ul>
<li><p>先对边计算分数，这里简单的将边连接的两个节点的embedding拼接再线性变换</p>
<p><span class="math display">\[
r(e_{ij}) = W (n_i || n_j) + b
\]</span></p></li>
<li><p>之后对所有的分数做softmax归一化，注意这里作者加了0.5使得均值为1，作者给出的解释是数值计算更稳定且梯度传导更好</p>
<p><span class="math display">\[
s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
\]</span></p></li>
<li><p>按照分数开始收缩边，假如边连接了已经收缩的边节点那就不再收缩了。这样每次都能减少一半的节点。</p></li>
</ul></li>
<li><p>新的节点分数直接用边分数加权两端节点特征和得到：</p>
<p><span class="math display">\[
\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
\]</span></p></li>
</ul>
<h1
id="discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</h1>
<ul>
<li><p>以hierarchical lstm encoder+lstm
decoder的抽取式摘要作为baseline，添加了一个三层attention用来加入篇章信息，这里的篇章信息具体指的是句子级别的elaborate关系，即某一句详细阐述或者补充说明了另一句，作者认为document
summarization这种篇章级别的任务当然需要篇章信息。</p></li>
<li><p>作者使用了attention来学习句子之间的elaborate有向边，具体如下图：
<img data-src="https://s2.ax1x.com/2020/01/29/1MfDOI.png"
alt="1MfDOI.png" /></p></li>
<li><p>三个组件</p>
<ul>
<li><p>Parent Attention：使用hierarchical
encoder得到每个句子的表示，之后用attention表示句子k是句子i父节点的概率，即elaborate的边由k指向i（作者没有用self
attention）</p>
<p><span class="math display">\[
\begin{aligned} p(k | i, \mathbf{H}) &amp;=\operatorname{softmax}(g(k,
i)) \\ g(k, i) &amp;=v_{a}^{\mathrm{T}} \tanh \left(U_{a} \cdot
H_{k}+W_{a} H_{i}\right) \end{aligned}
\]</span></p></li>
<li><p>Recursive
Attention：即计算多跳父节点，得到k是i的d跳父节点概率，这里简单的用注意力矩阵幂应该就可以得到，注意要对root句子（虚节点）做特殊处理，root没有父节点：</p>
<p><span class="math display">\[
\alpha_{d, k, i}=\left\{\begin{array}{ll}{p(k | i, \mathbf{H})} &amp;
{(d=1)} \\ {\sum_{l=0}^{N} \alpha_{d-1, k, l} \times \alpha_{1, l, i}}
&amp; {(d&gt;1)}\end{array}\right.
\]</span></p></li>
<li><p>Selective
Attention：综合得到的attention信息，首先将句子i某一跳所有父节点的信息加权求和：</p>
<p><span class="math display">\[
\gamma_{d, i}=\sum_{k=0}^{N} \alpha_{d, k, i} H_{k}
\]</span></p>
<p>之后再用selective
attention计算该跳的权重，依赖于句子i的encoder和decoder state<span
class="math inline">\(H,s\)</span>，以及所有父节点的encoder state：</p>
<p><span class="math display">\[
\beta_{d, i}=\operatorname{softmax}\left(\mathbf{W}_{\beta}\left[H_{i} ;
s_{i} ; K\right]\right)
\]</span></p>
<p>得到权重加权所有跳的信息，并补充进decoder input当中（拼接）</p>
<p><span class="math display">\[
\Omega_{i}=\sum_{d} \beta_{d, i} \gamma_{d, i} \\
p\left(y_{i} | \mathbf{x},
\theta\right)=\operatorname{softmax}\left(\mathbf{W}_{o} \tanh
\left(\mathbf{W}_{c^{\prime}}\left[H_{i} ; s_{t} ; K ;
\Omega_{i}\right]\right)\right) \\
\]</span></p></li>
</ul></li>
<li><p>这里，作者说提到了修辞结构分析（RST）目前没有很好的off-the-shelf
tools，误差大，这是硬伤，因此提出了一个联合学习的框架，后来发现联合学习是指训练集上依然用已有的RST
Parser得到elaborate edges，用以指导Parent
Attention，之后测试集就不需要了，这样的话Parser当中的误差对模型的影响依然很大。目标函数为：</p>
<p><span class="math display">\[
-\log p(\mathbf{y} | \mathbf{x})-\lambda \cdot \sum_{k=1}^{N}
\sum_{i=1}^{N} E_{k, i} \log \alpha_{1, k, i}
\]</span></p>
<p>其中第二项就是用parser得到的边指导attention</p></li>
<li><p>作者先用HILDA
parser得到RST格式的篇章标注信息，然后用Single-document summarization as
a tree knapsack problem一文中的方法转换为dependency的格式</p></li>
<li><p>虽然依然依赖于parser进行训练，但是作者做了两个Baseline，一个是不用parser，直接将前一句作为下一句的elaborate
parent，另一个也不用parser，让attention自己学习，结果发现baseline都不如注入了parser信息的attention模型。让attention自己学习最差，其次是学一个固定的前句父节点。作者提出的模型相比baseline在daily
mail数据集上抽短文本（75)比抽长文本(275)优势更大，这里有ROUGE指标偏爱长文本的原因，也说明在字数限制下，抽最重要的信息方面，discourse的信息确实可以起到帮助。</p></li>
<li><p>这篇文章可以看成一个attention模型(self attention +
multi-blocks)，注入了一些先验信息来帮助在单文档抽取式摘要获得更好的结果。</p></li>
</ul>
<h1
id="a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</h1>
<ul>
<li>NAACL
2018的一篇论文，依然是考虑了篇章信息，不过是在科研论文数据集上做生成式摘要。</li>
<li>这里的discourse有些狭义了，指的是科研论文里的每一个section，其实还是一个hierarchical
attention，作者也直接在pointer-generator上改了，结构如下： <img data-src="https://s2.ax1x.com/2020/01/29/1MHPr4.png" alt="1MHPr4.png" /></li>
<li>值得称赞的是作者提供了两个大规模长文档的科研论文摘要数据集，pubmed以及arxiv，均达到十万规模即便，平均原文长度达到3000+和4900+，平均摘要长度也过百，是很有价值的超长单文档摘要数据集。</li>
</ul>
<h1
id="sentence-bert-sentence-embeddings-using-siamese-bert-networks">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</h1>
<ul>
<li>亮点：用BERT做句子对回归任务很耗时，作者提出孪生BERT网络，将推理速度提高了1123200倍</li>
<li>显然提高这么多倍的说法是有歧义的，naive
bert在语义匹配任务上耗时，是因为每匹配一对就要将一对句子送进BERT计算出分数，而作者将BERT稍作修改，用BERT得到的embedding作为句子的特征向量，直接用向量的之间的cosine距离来做匹配，当然要快</li>
<li>接下来就是证明原始BERT得到的embedding并不能很好的作为语义匹配的特征向量，SBERT也就是在BERT之后加了回归层或者分类层，引入triplet
loss，得到的效果就比原始BERT好很多。可以看成是BERT在语义匹配任务上的一种微调吧。</li>
</ul>
<h1 id="towards-a-human-like-open-domain-chatbot">Towards a Human-like
Open-Domain Chatbot</h1>
<ul>
<li>亮点：谷歌出品，大。研究一些细节设计。</li>
<li>26亿参数量。400亿token的语料。为了很好的捕捉多轮对话的质量，作者提出了Sensibleness
and Specificity
Average(SSA)作为指标，并且发现最优化perplexity的模型能够达到最好的SSA。</li>
<li>作者使用evolved
transformer，多轮对话作为输入，训练了一个seq2seq，词标大小8k（用了BPE），最后测试集的困惑度只有10.2，且实际表现比其他的补充了规则、系统、知识的复杂的对话系统表现要好，再次证明了深度神经网络，只要数据够多，训得够好，就是可以大力出奇迹。</li>
<li>SSA衡量两个方面：合理且具体。这是一个人工衡量指标，首先问测试人员回答是否合理，假如合理，再问回答是否具体，因为很多时候回答不具体（总是回答i
don't
know）的系统反而在自动指标上取得比较好的成绩。作者也实验发现SSA和人工检测系统是否human-like一致，SSA高的系统表现更加像人类。</li>
<li>SSA有两种测试环境，一种是指定测试集，作者收集了1477个多轮对话作为测试数据集；另一个就是让测试人员直接和系统闲聊，至少14轮，至多28轮</li>
<li>作者给出了很多训练细节和测试细节，具体可见论文，反正就是大，在一块TPU-v3
Pod上训练了30天，164个epoch，模型总共观察了10T个token。</li>
<li>这么强大而简单的模型，在decoding时不需要复杂的处理来保证生成高质量且多样化的回答。作者采用了sample
and
rank：生成Logits之后先除以温度T，再过softmax，按概率随机采样生成多个序列之后取概率最大的那一句作为输出。作者发现温度越高，即logits输出的差异性越小，容易生成与上下文相关的罕见词。作者比对发现sample
and rank虽然简单但是比beam search表现更好，前提是能够训练到low
perplexity。作者将温度设为0.88，采样20句。</li>
<li>统计测试发现perplexity和SSA的相关系数高达0.9以上。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/graph-neural-network/" rel="tag"># graph neural network</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/summarization/" rel="tag"># summarization</a>
              <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/11/15/hlda/" rel="prev" title="Note for Hierarchical Latent Dirichlet Allocation">
                  <i class="fa fa-angle-left"></i> Note for Hierarchical Latent Dirichlet Allocation
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/02/07/reformer/" rel="next" title="Reformer - Paper Reading">
                  Reformer - Paper Reading <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:25</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2019/12/16/201912/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
