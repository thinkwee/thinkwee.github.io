<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="A simple note on the RL used in single-agent and multi-agent.">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-agent Reinforcement Learning Notes">
<meta property="og:url" content="https://thinkwee.top/2023/07/20/marl/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="A simple note on the RL used in single-agent and multi-agent.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/598fac8fa369cf85729973005cc0b37a.png">
<meta property="og:image" content="https://thinkwee.top/img/marl1.png">
<meta property="og:image" content="https://thinkwee.top/img/marl2.png">
<meta property="og:image" content="https://thinkwee.top/img/marl3.png">
<meta property="og:image" content="https://thinkwee.top/img/marl4.png">
<meta property="og:image" content="https://thinkwee.top/img/marl1.png">
<meta property="og:image" content="https://thinkwee.top/img/marl2.png">
<meta property="og:image" content="https://thinkwee.top/img/marl3.png">
<meta property="og:image" content="https://thinkwee.top/img/marl4.png">
<meta property="article:published_time" content="2023-07-20T03:38:14.000Z">
<meta property="article:modified_time" content="2025-07-15T17:14:42.904Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="reinforcement learning">
<meta property="article:tag" content="multi-agent">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/598fac8fa369cf85729973005cc0b37a.png">


<link rel="canonical" href="https://thinkwee.top/2023/07/20/marl/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2023/07/20/marl/","path":"2023/07/20/marl/","title":"Multi-agent Reinforcement Learning Notes"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Multi-agent Reinforcement Learning Notes | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">53</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#sequential-decision-making"><span class="nav-number">1.</span> <span class="nav-text">Sequential Decision Making</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bellman-equation-u-and-q-functions"><span class="nav-number">1.1.</span> <span class="nav-text">Bellman Equation, U and Q
Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reward-shaping"><span class="nav-number">1.2.</span> <span class="nav-text">Reward Shaping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#solving-mdps"><span class="nav-number">1.3.</span> <span class="nav-text">Solving MDPs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#value-iteration"><span class="nav-number">1.3.1.</span> <span class="nav-text">Value Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-iteration"><span class="nav-number">1.3.2.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-programming"><span class="nav-number">1.3.3.</span> <span class="nav-text">Linear Programming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#online-algorithms"><span class="nav-number">1.3.4.</span> <span class="nav-text">Online Algorithms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#slot-machine-problem"><span class="nav-number">1.4.</span> <span class="nav-text">Slot Machine Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pomdp"><span class="nav-number">1.5.</span> <span class="nav-text">POMDP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#single-agent-rl"><span class="nav-number">2.</span> <span class="nav-text">Single Agent RL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#passive-reinforcement-learning"><span class="nav-number">2.1.</span> <span class="nav-text">Passive Reinforcement
Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#direct-value-estimation"><span class="nav-number">2.1.1.</span> <span class="nav-text">Direct Value Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaptive-dynamic-programming-adp"><span class="nav-number">2.1.2.</span> <span class="nav-text">Adaptive Dynamic Programming
(ADP)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-difference-learning"><span class="nav-number">2.1.3.</span> <span class="nav-text">Temporal Difference Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#active-reinforcement-learning"><span class="nav-number">2.2.</span> <span class="nav-text">Active Reinforcement
Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#introducing-exploration"><span class="nav-number">2.2.1.</span> <span class="nav-text">Introducing Exploration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#td-q-learning"><span class="nav-number">2.2.2.</span> <span class="nav-text">TD Q-learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sarsa"><span class="nav-number">2.2.3.</span> <span class="nav-text">Sarsa</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generalization-in-reinforcement-learning"><span class="nav-number">2.3.</span> <span class="nav-text">Generalization in
Reinforcement Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#policy-search"><span class="nav-number">2.4.</span> <span class="nav-text">Policy Search</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#marl-multi-agent-rl"><span class="nav-number">3.</span> <span class="nav-text">MARL (Multi-Agent Rl)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="nav-number">4.</span> <span class="nav-text">序列决策</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bellman%E6%96%B9%E7%A8%8Bu%E5%92%8Cq%E5%87%BD%E6%95%B0"><span class="nav-number">4.1.</span> <span class="nav-text">bellman方程，U和Q函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reward-shaping"><span class="nav-number">4.2.</span> <span class="nav-text">reward shaping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3mdp"><span class="nav-number">4.3.</span> <span class="nav-text">求解MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">4.3.1.</span> <span class="nav-text">价值迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">4.3.2.</span> <span class="nav-text">策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92"><span class="nav-number">4.3.3.</span> <span class="nav-text">线性规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E7%BA%BF%E7%AE%97%E6%B3%95"><span class="nav-number">4.3.4.</span> <span class="nav-text">在线算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98"><span class="nav-number">4.4.</span> <span class="nav-text">老虎机问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pomdp"><span class="nav-number">4.5.</span> <span class="nav-text">POMDP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#single-agent-rl"><span class="nav-number">5.</span> <span class="nav-text">Single Agent RL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A2%AB%E5%8A%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.1.</span> <span class="nav-text">被动强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E4%BB%B7%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.1.1.</span> <span class="nav-text">直接价值估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92adp"><span class="nav-number">5.1.2.</span> <span class="nav-text">自适应动态规划(ADP)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.1.3.</span> <span class="nav-text">时序差分学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E5%8A%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.2.</span> <span class="nav-text">主动强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E5%85%A5explore"><span class="nav-number">5.2.1.</span> <span class="nav-text">引入explore</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#td-q-learning"><span class="nav-number">5.2.2.</span> <span class="nav-text">TD Q-learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sarsa"><span class="nav-number">5.2.3.</span> <span class="nav-text">Sarsa</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%B3%9B%E5%8C%96"><span class="nav-number">5.3.</span> <span class="nav-text">强化学习中的泛化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%90%9C%E7%B4%A2"><span class="nav-number">5.4.</span> <span class="nav-text">策略搜索</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#marl-multi-agent-rl"><span class="nav-number">6.</span> <span class="nav-text">MARL (Multi-Agent Rl)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2023/07/20/marl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Multi-agent Reinforcement Learning Notes | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Multi-agent Reinforcement Learning Notes
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-20 11:38:14" itemprop="dateCreated datePublished" datetime="2023-07-20T11:38:14+08:00">2023-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 01:14:42" itemprop="dateModified" datetime="2025-07-16T01:14:42+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL/" itemprop="url" rel="index"><span itemprop="name">RL</span></a>
        </span>
    </span>

  
    <span id="/2023/07/20/marl/" class="post-meta-item leancloud_visitors" data-flag-title="Multi-agent Reinforcement Learning Notes" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>21k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>20 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/598fac8fa369cf85729973005cc0b37a.png" width="500"/></p>
<p>A simple note on the RL used in single-agent and multi-agent.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="sequential-decision-making">Sequential Decision Making</h1>
<ul>
<li>Modeling agents in sequential decision making problems, rather than
just single-step or one-shot decision problems.</li>
<li>Discuss a sequential decision problem in:
<ul>
<li>A fully observable stochastic environment</li>
<li>With a Markov transition model</li>
<li>Additive rewards Typically includes a set of states, a set of
actions, a transition model, and a reward function. The solution to the
problem is the policy.</li>
</ul></li>
<li>If the sequence has no time limit, and the optimal policy only
depends on the state, independent of time, it is called a stationary
policy.</li>
</ul>
<h2 id="bellman-equation-u-and-q-functions">Bellman Equation, U and Q
Functions</h2>
<ul>
<li><p>The value function represents the cumulative reward of a
state/action sequence, such as U(s0,a0,s1,a0....), starting from the
current state and action s0,a0.</p></li>
<li><p>The value is defined by an additive discount, where future
rewards are discounted by gamma:</p>
<p><span class="math display">\[
U\left(\,\left[s_{0},\,s_{1},\,s_{2},\,\cdots\,\right]\,\right)=R(s_{0})+\gamma
R(s_{1})+\gamma ^2 R(s_{2})+\,\cdots
\]</span></p>
<ul>
<li>Because recent rewards are more important.</li>
<li>If rewards can be invested, earlier rewards have higher value.</li>
<li>This is equivalent to each transition having a <span
class="math inline">\(1-\gamma\)</span> chance of unexpected
termination.</li>
<li>Satisfies stationarity, meaning the best action at time t+1 is the
same as the best action at time t in the future.</li>
<li>Prevents infinite sequence transitions.</li>
</ul></li>
<li><p>Based on the value function, the best action can be selected at
each state, i.e., the action that maximizes the current value
(instantaneous reward + expected discounted future value):</p>
<ul>
<li><p>There is an expectation here, as each action can lead to
different future states (with probability), so the value of an action is
the sum of its expected value over all possible future states.</p></li>
<li><p>The value function is only a function of the state, as it has
already accumulated the expectation over all actions:</p>
<p><span class="math display">\[
\pi^*(s) = \underset{a \in A(s)}{\text{argmax}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p></li>
</ul></li>
<li><p>Similarly, the value function is only a function of the state,
and has already accumulated the expectation over all actions. This is
essentially the same as the previous formula, but without the argmax to
select actions; instead, it's a max, as the agent assumes the best
action is taken. The interpretation is: if the agent selects the best
action, the state value is the expected reward of the next transition
plus the discounted value of the next state:</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p>
<ul>
<li>Note that these two formulas are essentially the same; both involve
selecting actions and summing over all possible states, but this is
based on an estimate of the future expectation. The actual execution is
choosing an action and transitioning to a state.</li>
<li>This equation is the <strong>Bellman Equation</strong>.</li>
</ul></li>
<li><p>Introduce the Q-function, which is a function of both action and
state, while U is just a function of state. The relationship between
them is as follows:</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} Q(s,a)
\]</span></p></li>
<li><p>Similarly, the Bellman equation can be written for the
Q-function:</p>
<p><span class="math display">\[
Q(s,a) = \sum_{s^{&#39;}} P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;})]
\]</span></p></li>
<li><p>Note that the above discussion focuses on the optimal value
function and optimal Q-function, both using max, meaning they compute
the return under the optimal policy, as opposed to on-policy value
functions and Q-functions, which calculate the expected return.</p></li>
</ul>
<h2 id="reward-shaping">Reward Shaping</h2>
<ul>
<li><p>The reward function R can be modified (without changing the
optimal policy) to stabilize the reinforcement learning process.</p>
<ul>
<li>Constraints: Avoid agent exploitation of reward.</li>
<li>Exploration: Encourage exploration.</li>
<li>Acceleration: Improve sparse reward situations by breaking tasks
down into smaller sub-tasks, making it easier for the agent to
learn.</li>
</ul></li>
<li><p>A common modification is the introduction of a potential
function.</p></li>
<li><p>A potential function is a state-only function <span
class="math inline">\(\Phi(s)\)</span> (unlike the value function, it is
independent of action/state sequences and does not result from removing
actions).</p></li>
<li><p>The potential function encodes the objective environmental
factors that influence the rewards.</p></li>
<li><p>It can be proven that a potential function can be any arbitrary
function of state s, and when added to the immediate reward, the optimal
policy derived from the Bellman equation remains unchanged.
Specifically, when the reward function is modified as:</p>
<p><span class="math display">\[
R^{&#39;}(s,a,s^{&#39;}) = R(s,a,s^{&#39;}) + \gamma \Phi(s^{&#39;}) -
\Phi(s)
\]</span></p>
<p>The optimal policy remains unchanged, <span
class="math inline">\(Q(s,a)=Q^{&#39;}(s,a)\)</span>.</p></li>
</ul>
<h2 id="solving-mdps">Solving MDPs</h2>
<h3 id="value-iteration">Value Iteration</h3>
<ul>
<li><p>With n states, there are n equations and n unknowns in the
Bellman equation. The analytical solution of nonlinear equations is
difficult, but an iterative method can be used. Starting with random
initial values, each state's value is updated based on neighboring
states' values until equilibrium is reached.</p></li>
<li><p>Introduce an iteration timestep i, the Bellman update
(<strong>Bellman Update</strong>) is as follows:</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>It can be proven that infinite iterations will guarantee
convergence to the optimal solution (assuming the immediate rewards are
correct).</p></li>
</ul>
<h3 id="policy-iteration">Policy Iteration</h3>
<ul>
<li><p>Sometimes we do not need to compute the exact value function,
just the action that yields the maximum value. This leads to the idea of
directly iterating and optimizing the policy.</p></li>
<li><p>Starting with an initial policy, the following two steps are
alternated:</p>
<ul>
<li>Policy Evaluation: Given a policy, compute the value of each state
under the policy at a particular timestep.</li>
<li>Policy Improvement: Calculate a new policy based on the value
function (using the Bellman equation) for all states.</li>
</ul></li>
<li><p>The process continues until policy improvement does not result in
a significant change in the value function.</p></li>
<li><p>Policy evaluation is also based on the Bellman equation, but we
do not need to traverse actions since they are determined by the policy.
By fixing the current policy <span class="math inline">\(\pi_i\)</span>
at timestep i, we obtain n equations, which can be solved:</p>
<p><span class="math display">\[
U_{i}(s) = \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>When the state space is large, solving it exactly becomes
difficult. In this case, a modified policy iteration can be used for
policy evaluation, where the value function for the next timestep is
computed directly from the current policy and iterated repeatedly:</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>The above method is synchronous, meaning all states are updated
at each iteration. In fact, we can update only a subset of states, which
is called asynchronous policy iteration.</p>
<ul>
<li>The advantage is that we can focus on updating strategies for
certain effective states, as some states may not reach an optimal
solution regardless of the action taken.</li>
</ul></li>
</ul>
<h3 id="linear-programming">Linear Programming</h3>
<ul>
<li>TBD</li>
</ul>
<h3 id="online-algorithms">Online Algorithms</h3>
<ul>
<li>Value iteration and policy iteration are offline methods: given all
conditions/rewards, the optimal solution is computed, and the agent
executes it.</li>
<li>Online algorithms: The agent does not receive an offline solution
and execute it. Instead, it computes decisions in real-time at each
decision point.</li>
</ul>
<h2 id="slot-machine-problem">Slot Machine Problem</h2>
<ul>
<li>TBD</li>
</ul>
<h2 id="pomdp">POMDP</h2>
<ul>
<li>Partially Observable Markov Decision Process</li>
<li>Since the agent is uncertain about its current state (the definition
of partial observability), a belief state is introduced, and the agent’s
decision-making cycle has an additional step:
<ul>
<li>Act based on the belief state.</li>
<li>Perceive observations (evidence).</li>
<li>Update the belief state based on the perception, action, and
previous belief state through some updating mechanism.</li>
</ul></li>
<li>In physical state space, solving POMDP can be simplified to solving
the MDP in the belief state space.</li>
<li>Value iteration for POMDPs.</li>
</ul>
<h1 id="single-agent-rl">Single Agent RL</h1>
<ul>
<li>The agent is in an MDP but does not know the transition model or
reward function and needs to take actions to learn more
information.</li>
<li>The sequential decision-making problem above assumes a known
environment and optimal policy derivation. However, in general
reinforcement learning, the environment is unknown, and the agent learns
the optimal policy through interaction with the environment.</li>
<li>Model-based Methods:
<ul>
<li>The environment provides a transition model or initially an unknown
model that needs to be learned.</li>
<li>Typically, a value function is learned, defined as the total reward
accumulated from state s.</li>
<li>The sequential decision-making problems discussed above</li>
</ul></li>
</ul>
<p>are often solved through value iteration or policy iteration in a
model-based manner. - Model-free Methods: - The environment is not known
beforehand and needs to be learned. Instead of computing and using a
model, the agent computes and learns the value function or policy. -
Model-free approaches are often simpler to implement than model-based
methods but may require more interactions with the environment.
Examples: - Q-learning - SARSA - Monte Carlo methods.</p>
<h2 id="passive-reinforcement-learning">Passive Reinforcement
Learning</h2>
<ul>
<li>The policy is fixed, and the value function is learned.</li>
<li>The policy is fixed, for example, a greedy approach that selects the
action with the maximum value. In this case, the Q-function only needs
to be learned, and the optimal action under the fixed policy will
emerge.</li>
<li>This is similar to policy evaluation (where, given a policy, the
value of each state at a particular time step is computed), but the
agent doesn't know the transition probabilities between states or the
immediate rewards after taking an action.</li>
</ul>
<h3 id="direct-value-estimation">Direct Value Estimation</h3>
<ul>
<li>The value of a state is defined as the expected total reward
(reward-to-go) from that state.</li>
<li>Each trial will leave a sample of the value for the states it passes
through (multiple visits to the same state will provide multiple
samples).</li>
<li>This way, samples are collected, and supervised learning can be used
to map states to values.</li>
<li>However, this method ignores an important constraint: the value of a
state should satisfy the Bellman equation for the fixed policy, i.e.,
the value of the state is related to the reward and expected value of
the successor states, not just its own value.</li>
<li>Ignoring this will lead to a larger search space and slow
convergence.</li>
</ul>
<h3 id="adaptive-dynamic-programming-adp">Adaptive Dynamic Programming
(ADP)</h3>
<ul>
<li>The agent learns the transition model between states and uses
dynamic programming to solve the MDP.</li>
<li>In a fully observable or deterministic environment, the agent
continually runs trials to gather data, then trains a supervised model
that takes the current state and action as inputs and outputs the
transition probabilities (the transition model).</li>
<li>After obtaining the transition model, the agent can solve the MDP
using sequence decision methods, correcting the policy iteratively.</li>
<li>ADP requires the agent to trial continuously, gather historical data
with reward signals, and then learn the environment's transition model,
which transforms the problem into a known sequence decision
problem.</li>
<li>ADP can be seen as an extension of policy iteration in the passive
reinforcement learning setting.</li>
</ul>
<h3 id="temporal-difference-learning">Temporal Difference Learning</h3>
<ul>
<li><p>In the passive reinforcement learning setting, where a policy
<span class="math inline">\(\pi\)</span> is given, if the agent takes
action <span class="math inline">\(\pi(s)\)</span> from state <span
class="math inline">\(s\)</span> and transitions to state <span
class="math inline">\(s^{&#39;}\)</span>, the value function is updated
using the temporal difference equation:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow U^{\pi}(s) + \alpha (R(s,\pi(s),s^{&#39;}) +
\gamma U^{\pi}(s^{&#39;}) - U^{\pi}(s))
\]</span></p></li>
<li><p>Here, <span class="math inline">\(\alpha\)</span> is the learning
rate. Compared to Bellman, temporal difference updates the value based
on the observed difference between the value of state <span
class="math inline">\(s\)</span> and the reward plus discounted future
value of state <span class="math inline">\(s^{&#39;}\)</span>:</p>
<ul>
<li><p>The difference term provides error information, and the update
reduces this error.</p></li>
<li><p>The modified formula shows that the value of the state is updated
using interpolation between the current value and the reward + future
discounted value:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow (1-\alpha)U^{\pi}(s) + \alpha
(R(s,\pi(s),s^{&#39;}) + \gamma U^{\pi}(s^{&#39;}))
\]</span></p></li>
</ul></li>
<li><p>Connection and difference with adaptive dynamic programming:</p>
<ul>
<li>Both adjust the current value based on future estimates, but ADP
uses a weighted sum over all possible successor states, while temporal
difference only uses the observed successor state.</li>
<li>ADP aims for as many adjustments as possible to ensure consistency
between the value estimate and the transition model, while TD makes a
single adjustment based on the observed transition.</li>
<li>TD can be seen as an approximation of ADP:
<ul>
<li>TD can use the transition model to generate multiple
pseudo-experiences, rather than relying only on the actual observed
transition, leading to value estimates that are closer to ADP.</li>
<li>Prioritized sweeping updates states that are highly probable and
recently had large adjustments in their successor states.</li>
<li>One advantage of TD as an approximation of ADP is that early on, the
transition model may not be accurate, so learning an exact value
function to match the transition model is less meaningful.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="active-reinforcement-learning">Active Reinforcement
Learning</h2>
<ul>
<li>The policy needs to be learned.</li>
<li>A complete transition model needs to be learned, taking into account
all possible actions since the policy is not fixed (unknown).</li>
<li>Consider whether, after learning the optimal policy, simply
executing it is always the right action.</li>
</ul>
<h3 id="introducing-exploration">Introducing Exploration</h3>
<ul>
<li>Adaptive dynamic programming is greedy, so exploration needs to be
introduced.</li>
<li>A broad design would introduce an exploration function <span
class="math inline">\(f(u, n)\)</span>, where a higher value <span
class="math inline">\(u\)</span> encourages greediness, and fewer trials
<span class="math inline">\(n\)</span> encourage exploration.</li>
</ul>
<h3 id="td-q-learning">TD Q-learning</h3>
<ul>
<li><p>A temporal difference method for active reinforcement
learning.</p></li>
<li><p>No model of the transition probabilities is required (model-free
method).</p></li>
<li><p>The agent learns the action-value function to avoid needing the
transition model itself:</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
\max_{a^{&#39;}}Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>No transition model <span
class="math inline">\(P(s&#39;|s,a)\)</span> is needed.</p></li>
<li><p>Since no policy is provided, we need to take the max over all
possible actions.</p></li>
<li><p>Learning is difficult when rewards are sparse.</p></li>
</ul>
<h3 id="sarsa">Sarsa</h3>
<ul>
<li><p>Sarsa stands for state, action, reward, state, action, and
represents the update rule for this five-tuple:</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>Compared to TD Q-learning, it does not take the max over all
possible actions, but instead updates based on the action actually
taken.</p></li>
<li><p>If the agent is greedy and always selects the action with the
highest Q-value, then Sarsa and Q-learning are equivalent. If not
greedy, Sarsa penalizes actions that encounter negative rewards during
exploration.</p></li>
<li><p>On/Off Policy:</p>
<ul>
<li>Sarsa is on-policy: "If I stick to my policy, what is the value of
this action at this state?"</li>
<li>Q-learning is off-policy: "If I stop following my current policy and
instead use an estimated optimal policy, what is the value of this
action at this state?"</li>
</ul></li>
</ul>
<h2 id="generalization-in-reinforcement-learning">Generalization in
Reinforcement Learning</h2>
<ul>
<li>Both the value function and the Q-function are stored in table form,
and the state space is large.</li>
<li>If they can be parameterized, the number of parameters to be learned
can be greatly reduced.</li>
<li>In passive reinforcement learning, supervised learning can be used
to learn the value function based on trials, and functions or neural
networks can be used to parameterize this.</li>
<li>In temporal difference learning, the difference term can be
parameterized and learned through gradient descent.</li>
<li>There are several issues with parameterizing and approximating the
value or Q-function:
<ul>
<li>Difficulty in convergence.</li>
<li>Catastrophic forgetting: This can be mitigated by experience replay,
where trajectories are saved and replayed to ensure that value functions
for states not currently visited remain accurate.</li>
</ul></li>
<li>Reward function design, how to address sparse rewards?
<ul>
<li>Issue: credit assignment, which action should be credited for the
final positive or negative reward?</li>
<li>Reward shaping can help by providing intermediate rewards; potential
functions are one example, reflecting progress towards partial goals or
measurable distances from the final desired state.</li>
</ul></li>
<li>Another approach is hierarchical reinforcement learning, TBD.</li>
</ul>
<h2 id="policy-search">Policy Search</h2>
<ul>
<li><p>Adjust the policy as long as there is improvement in
performance.</p></li>
<li><p>A policy is a function mapping states to actions.</p></li>
<li><p>If the policy is parameterized, it can be optimized. However,
optimizing the Q-function doesn't necessarily lead to the optimal value
estimate or Q-function because policy search only cares whether the
policy is optimal.</p></li>
<li><p>Directly learning Q-values and then using argmax to derive the
policy can lead to discrete, non-differentiable issues. In this case,
Q-values are treated as logits, and softmax is used to represent action
probabilities, with techniques like Gumbel-Softmax ensuring the policy
is continuously differentiable.</p></li>
<li><p>If the expected reward from executing the policy can be written
as a parameterized expression, policy gradient methods can be used for
direct optimization. Otherwise, the expression can be computed by
observing accumulated rewards during policy execution and optimized
using experience gradients.</p></li>
<li><p>For the simplest case where only one action is taken, the policy
gradient can be written as:</p>
<p><span class="math display">\[
\triangledown_{\theta}\sum_a R(s_0,a,s_0)\pi_{\theta}(s_0,a)
\]</span></p></li>
<li><p>This sum can be approximated using samples generated from the
policy’s probability distribution, and extended to sequential states,
resulting in the REINFORCE algorithm. Here, the policy probability
weighted sum is approximated over N trials, and the single-step reward
is extended to a value function, with states extended to the entire
state space of the environment:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{j=1}^N \frac{u_j(s)
\triangledown_{\theta}\pi_{\theta}(s,a_j)}{\pi_{\theta}(s,a_j)}
\]</span></p></li>
</ul>
<h1 id="marl-multi-agent-rl">MARL (Multi-Agent Rl)</h1>
<ul>
<li>TBD</li>
<li><img data-src="/img/marl1.png" width="1200"></li>
<li><img data-src="/img/marl2.png" width="1200"></li>
<li><img data-src="/img/marl3.png" width="1200"></li>
<li><img data-src="/img/marl4.png" width="1200"></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="序列决策">序列决策</h1>
<ul>
<li>建模序列决策下的智能体，而不仅仅是单一回合或者一次性决策问题</li>
<li>讨论一个
<ul>
<li>完全可观测的随机环境</li>
<li>具有马尔科夫转移模型</li>
<li>加性奖励
的序列决策问题，通常包含状态集合、动作集合、转移模型、奖励函数。问题的解即策略。</li>
</ul></li>
<li>假如序列没有时间限制，最优策略只与状态有关，与时间无关，则称最优策略是平稳的。</li>
</ul>
<h2 id="bellman方程u和q函数">bellman方程，U和Q函数</h2>
<ul>
<li><p>价值函数（价值函数），代表某一状态/行为序列的奖励综合，U(s0,a0,s1,a0....)，从当前状态和动作s0,a0开始</p></li>
<li><p>用加性折扣定义价值，未来的奖励乘gamma递减:</p>
<p><span class="math display">\[
U\left(\,\left[s_{0},\,s_{1},\,s_{2},\,\cdots\,\right]\,\right)=R(s_{0})+\gamma
R(s_{1})+\gamma ^2 R(s_{2})+\,\cdots
\]</span></p>
<ul>
<li>因为看重近期奖励</li>
<li>如果奖励可以投资，则越早的奖励价值越大</li>
<li>等价于每次转移有<span
class="math inline">\(1-\gamma\)</span>的意外终止</li>
<li>满足平稳性，t+1的最佳选择未来也是t的最佳选择未来</li>
<li>避免无穷的序列转移</li>
</ul></li>
<li><p>基于价值函数，可以选出当前最佳动作，即在当前状态下，使得当前价值最大的动作（转移的即时奖励+后续的期望折扣价值）</p>
<ul>
<li><p>这里存在一个期望，因为每个动作都可能到每个状态（概率），因此一个动作的价值是在所有可能的未来状态下累加。</p></li>
<li><p>价值函数只是状态的函数，已经对所有动作累加求期望</p>
<p><span class="math display">\[
\pi^*(s) = \underset{a \in A(s)}{\text{argmax}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p></li>
</ul></li>
<li><p>同理，价值函数只是状态的函数，已经对所有动作累加求期望，其实就是上式，只不过不是argmax选动作，而是max，这里的解释是：假设agent选择了最佳动作，状态价值是下一次转移的期望奖励加上下一个状态的折扣价值</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p>
<ul>
<li>注意两个式子本质是一样的，都是挑动作，都是在所有可能的状态下累加，但这是基于对未来期望的估计，实际执行就是选择一个动作，转移到一个状态。</li>
<li>该式即<strong>bellman方程</strong></li>
</ul></li>
<li><p>引入Q函数，Q是动作和状态的函数，U仅仅是状态的函数，两者的转换关系如下</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} Q(s,a)
\]</span></p></li>
<li><p>同理也可以写成bellman方程的形式</p>
<p><span class="math display">\[
Q(s,a) = \sum_{s^{&#39;}} P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;})]
\]</span></p></li>
<li><p>注意以上讨论的都是最优价值函数和最优q函数，都是取max，即计算最优策略下的return，区别于on-policy价值函数和q函数，计算的是期望</p></li>
</ul>
<h2 id="reward-shaping">reward shaping</h2>
<ul>
<li><p>可以通过修改奖励函数R（而不改变最优策略）来使强化学习过程更加稳定</p>
<ul>
<li>约束：避免一些agent套路reward的情况</li>
<li>探索：鼓励explore</li>
<li>加速：改善奖励稀疏的情况，将任务分解成更小的子任务，从而使得智能体更容易学习</li>
</ul></li>
<li><p>一种常见的修改方式是引入势函数</p></li>
<li><p>势函数是一个仅与状态相关的函数<span
class="math inline">\(\Phi(s)\)</span>（不同于价值函数，与动作状态序列无关，不是消掉动作得到的）</p></li>
<li><p>势函数编码了环境本身客观存在的因素，影响了奖励</p></li>
<li><p>可以证明，势函数可以为状态s的任意函数，且加入及时奖励时，bellman方程得到的最优策略不变，即当奖励函数改成</p>
<p><span class="math display">\[
R^{&#39;}(s,a,s^{&#39;}) = R(s,a,s^{&#39;}) + \gamma \Phi(s^{&#39;}) -
\Phi(s)
\]</span></p>
<p>时，最优策略不变，<span
class="math inline">\(Q(s,a)=Q^{&#39;}(s,a)\)</span></p></li>
</ul>
<h2 id="求解mdp">求解MDP</h2>
<h3 id="价值迭代">价值迭代</h3>
<ul>
<li><p>n个状态，bellman方程就有n个方程n个未知数，非线性方程的解析解很难得到，可以通过迭代的方法，随机初始值，再根据邻居的价值更新每个状态的价值，重复直至平衡</p></li>
<li><p>引入迭代的timestep i，bellman更新(<strong>Bellman
Update</strong>)如下</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>可以证明：无限次迭代可以保证达到平衡，得到最优策略(前提是即时奖励是正确的)。</p></li>
</ul>
<h3 id="策略迭代">策略迭代</h3>
<ul>
<li>有些时候我们并不需要得到精确的价值函数，只要知道哪个动作带来的价值最大即可，这就引出了直接对策略进行迭代优化的思想
.</li>
<li>从某个初始策略开始，交替进行以下两个步骤
<ul>
<li>策略评估：给定策略，计算执行策略后某一时间步每个状态的价值</li>
<li>策略改进：基于所有状态价值的一步前瞻（即价值函数bellman方程）来计算新的策略</li>
</ul></li>
<li>直到策略改进不对价值产生（足够大）改变</li>
<li>策略评估也是基于bellman方程，只不过不用遍历动作，因为动作已经由策略决定，然后固定当前时间步i策略<span
class="math inline">\(\pi_i\)</span>，我们可以得到n个方程，求解即可</li>
</ul>
<p><span class="math display">\[
U_{i}(s) = \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p>
<ul>
<li>在状态空间比较大的时候，精确求解比较困难，这时候可以使用修正策略迭代来进行策略评估，即下一时间步的价值函数直接由当前策略计算出，然后反复迭代</li>
</ul>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p>
<ul>
<li>以上都是同步的形式，即每次迭代更新所有状态。事实上可以只更新部分状态，即异步策略迭代
<ul>
<li>好处是可以只专注为某些有效的状态更新策略，有些状态可能无论什么动作都达不到最优解</li>
</ul></li>
</ul>
<h3 id="线性规划">线性规划</h3>
<ul>
<li>TBD</li>
</ul>
<h3 id="在线算法">在线算法</h3>
<ul>
<li>价值迭代和策略迭代都是离线的：给定了所有条件/奖励，先生成最优解，然后agent执行</li>
<li>在线算法：agent不是拿到离线解之后再执行，而是在每个决策点即时计算决策。</li>
</ul>
<h2 id="老虎机问题">老虎机问题</h2>
<ul>
<li>TBD</li>
</ul>
<h2 id="pomdp">POMDP</h2>
<ul>
<li>部分可观测环境的马尔科夫决策过程</li>
<li>因为agent对自己所处的状态不确定（这是部分可观测的定义），所以需要引入一个信念状态，然后agent的决策周期增加了一个环节
<ul>
<li>根据信念状态，执行动作</li>
<li>观测感知（证据）</li>
<li>基于感知、动作、之前的信念状态，通过某种更新机制得到新的信念</li>
</ul></li>
<li>在物理空间状态上求解POMDP可以简化为在信念状态空间上求解MDP</li>
<li>POMDP的价值迭代</li>
</ul>
<h1 id="single-agent-rl">Single Agent RL</h1>
<ul>
<li>Agent处在MDP当中，不知道转移模型和奖励函数，需要通过采取行动了解更多信息</li>
<li>上文的序列决策是在已知环境下，如何得到一个最优策略，其实不需要agent的互动。一般而言的强化学习是指环境未知，需要agent在与环境的交互中来得到数据，从而确定最优策略。</li>
<li>基于模型的方法
<ul>
<li>环境提供了转移模型，或者一开始未知环境模型，但是需要去学习</li>
<li>通常会学习一个价值函数，定义为状态s之后的奖励总和</li>
<li>上文的序列决策都是在基于模型的前提下阐述的</li>
</ul></li>
<li>无模型的方法
<ul>
<li>不知道环境的转移模型，而且也不会学习它</li>
<li>agent直接学习策略，一般通过两种方式来在无模型的前提下学习策略
<ul>
<li>学习Q函数，即学习处于状态s下采取动作a得到的奖励</li>
<li>直接学习策略<span
class="math inline">\(\pi\)</span>，即学习状态到动作的映射</li>
</ul></li>
</ul></li>
</ul>
<h2 id="被动强化学习">被动强化学习</h2>
<ul>
<li>策略固定，学习价值函数</li>
<li>策略固定，比如说贪心的取价值最大的动作，这时候只需要将Q函数学好，策略固定的情况下具体的最优动作也就出来了。</li>
<li>类似于策略评估（给定策略，计算执行策略后某一时间步每个状态的价值），但agent不知道采取动作后到各个状态的转移概率，也不知道即时奖励</li>
</ul>
<h3 id="直接价值估计">直接价值估计</h3>
<ul>
<li>一个状态的价值定义为从该状态出发的期望总奖励（reward-to-go）</li>
<li>每次trial都会在其经过的状态上留下一个价值的数值样本(多次经过一个状态就提供多个样本)</li>
<li>这样就收集了样本，可以使用监督学习学到状态到价值的映射</li>
<li>但是该方法忽略了一个重要约束：状态价值应满足固定策略的bellman方程，即状态的价值和后继状态的奖励和期望价值相关，而不是只取决于自己</li>
<li>这种忽略将导致搜索空间变大，收敛缓慢</li>
</ul>
<h3 id="自适应动态规划adp">自适应动态规划(ADP)</h3>
<ul>
<li>agent学习状态之间转移模型，并用dp解决MDP</li>
<li>在环境确定/可观测的情况下，不断的trial，得到数据，训练一个监督模型，输入当前状态和动作，输出后续状态概率，即转移模型</li>
<li>得到转移模型后，之后按照序列决策的方法，通过修正策略迭代求解MDP</li>
<li>可以看到ADP是需要agent先不断的trial，在环境中得到一系列包含奖励信号的历史数据，然后用这些数据学习到环境的转移模型，将其转化为环境已知的序列决策问题。</li>
<li>自适应动态规划可以看成是策略迭代在被动强化学习设置下的扩展</li>
</ul>
<h3 id="时序差分学习">时序差分学习</h3>
<ul>
<li><p>在被动强化学习的setting下，即给定策略<span
class="math inline">\(\pi\)</span>，假如在状态s下采取动作<span
class="math inline">\(\pi(s)\)</span>转移到了状态<span
class="math inline">\(s^{&#39;}\)</span>，则通过时序差分方程更新价值函数:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow U^{\pi}(s) + \alpha (R(s,\pi(s),s^{&#39;}) +
\gamma U^{\pi}(s^{&#39;}) - U^{\pi}(s))
\]</span></p></li>
<li><p>其中<span
class="math inline">\(\alpha\)</span>是学习率。对比bellman，时序差分是在观测到在状态s下采取动作a到达了状态s'，就根据这个相继状态之间价值的差分更新价值:</p>
<ul>
<li><p>差分项是关于误差的信息，更新是为了减少这个误差</p></li>
<li><p>公式变化后可以看出来是当前价值和奖励+未来折扣价值之间做插值:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow (1-\alpha)U^{\pi}(s) + \alpha
(R(s,\pi(s),s^{&#39;}) + \gamma U^{\pi}(s^{&#39;}))
\]</span></p></li>
</ul></li>
<li><p>与自适应动态规划的联系与区别：</p>
<ul>
<li>都是根据未来调整当前价值，自适应的未来是在所有可能后继状态上加权求和，而时间差分的未来是观测到的后继</li>
<li>自适应尽可能进行多的调整，以保证价值估计和转移模型的一致性；差分对观测到的转移只做单次调整</li>
<li>TD可以看成一种近似ADP
<ul>
<li>可以用转移模型生成多个pseudo
experience，而不是仅仅只看真实发生的一次转移，这样TD的价值估计会接近ADP</li>
<li>prioritized sweeping，对哪些高概率
后继状态刚刚经过大调整的状态进行更新</li>
<li>TD作为近似ADP的一个优点是，训练刚开始时，转移模型往往学不正确，因此像ADP一样学习一个精确的价值函数来匹配这个转移模型意义不大。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="主动强化学习">主动强化学习</h2>
<ul>
<li>需要学习策略</li>
<li>需要学习一个完整的转移模型，需要考虑所有的动作，因为策略不固定（未知）</li>
<li>需要考虑，得到最优策略后，简单的执行这个策略就是正确的吗？</li>
</ul>
<h3 id="引入explore">引入explore</h3>
<ul>
<li>自适应动态规划是greedy的，需要引入exploration</li>
<li>一个宏观的设计，是引入探索函数f(u,n)，选择价值u较高的即贪心，选择尝试次数n少的即探索</li>
</ul>
<h3 id="td-q-learning">TD Q-learning</h3>
<ul>
<li><p>一种主动强化学习下的时序差分方法</p></li>
<li><p>无需一个学习转移概率的模型，无模型的方法</p></li>
<li><p>通过学习动作-价值函数来避免对转移模型本身的需求</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
max_{a^{&#39;}}Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>不需要转移模型P(s'|s,a)</p></li>
<li><p>注意因为没有给定策略，这里需要对所有可能动作取max</p></li>
<li><p>奖励稀疏时难以学习</p></li>
</ul>
<h3 id="sarsa">Sarsa</h3>
<ul>
<li><p>即state,action,reward,state,action，sarsa的缩写代表了更新的五元组</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>相比TD
Q-learning，不是对所有可能动作取max，而是先执行动作，再根据这个动作更新</p></li>
<li><p>如果agent是贪心的，总是执行q-value最大的动作，则sarse和q-learning等价；如果不是贪心，sarsa会惩罚探索时遇到的负面奖励动作</p></li>
<li><p>on/off policy</p>
<ul>
<li>sarsa是on-policy：“假设我坚持我自己的策略，那么这个动作在该状态下的价值是多少？”</li>
<li>q-learning是off-policy：“假设我停止使用我正在使用的任何策略，并依据估计选择最佳动作的策略开始行动，那么这个动作在改状态下的价值是多少？”</li>
</ul></li>
</ul>
<h2 id="强化学习中的泛化">强化学习中的泛化</h2>
<ul>
<li>价值函数和Q函数都用表格的形式记录，状态空间巨大</li>
<li>要是能参数化，需要学习的参数值可以减少很多</li>
<li>对于被动强化学习，需要根据trials使用监督学习价值函数，这里可以用函数或者NN来参数化。</li>
<li>对于时序差分学习，可以将差分项参数化，通过梯度下降学习</li>
<li>参数化来近似学习价值或者q函数存在几个问题
<ul>
<li>难以收敛</li>
<li>灾难性遗忘：可以通过experience
replay，保存轨迹进行回放，确保agent不再访问的那部分状态空间上的价值函数仍然准确</li>
</ul></li>
<li>奖励函数设计，如何解决稀疏奖励？
<ul>
<li>问题：credit
assignment，最后的正面或者负面奖励应该归因到哪次动作上</li>
<li>可以通过修改奖励函数（reward
shaping）来提供一些中间奖励，势函数就是一个例子，势反映了我们所希望的部分状态（某个子目标的实现、离最终希望的终止状态的某种可度量的距离）</li>
</ul></li>
<li>另一种方案是分层强化学习，TBD</li>
</ul>
<h2 id="策略搜索">策略搜索</h2>
<ul>
<li><p>只要策略的表现有所改进，就继续调整策略</p></li>
<li><p>策略是一个状态到动作的映射函数</p></li>
<li><p>将策略参数化表达，尽管可以通过优化q函数得到，但并不一定得到最优的q函数或者价值估计，因为策略搜索只在乎策略是否最优</p></li>
<li><p>直接学习Q值，然后argmax
Q值得到策略会存在离散不可导问题，这时将Q值作为logits，用softmax表示动作概率，用类似gumbel-softmax使得策略连续可导</p></li>
<li><p>假如执行策略所得到的期望奖励可以写成关于参数的表达式，则可以使用策略梯度直接优化；否则可以通过执行策略观测累计的奖励来计算表达式，通过经验梯度优化</p></li>
<li><p>考虑最简单的只有一次动作的情况，策略梯度可以写成下式，即对各个动作的奖励按其策略概率加权求和，并对策略参数求导。</p>
<p><span class="math display">\[
\triangledown_{\theta}\sum_aR(s_0,a,s_0)\pi_{\theta}(s_0,a)
\]</span></p></li>
<li><p>将这个求和用策略所定义的概率分布生成的样本来近似，并且扩展到时序状态，就得到了REINFORCE算法，这里用N次trial近似策略概率加权求和，并且将单步奖励扩展到价值函数，状态也扩展到整个环境的状态集合：</p>
<p><span class="math display">\[
\frac1N
\sum_{j=1}^N\frac{u_j(s)\triangledown_{\theta}\pi_{\theta}(s,a_j)}{\pi_{\theta}(s,a_j)}
\]</span></p></li>
</ul>
<h1 id="marl-multi-agent-rl">MARL (Multi-Agent Rl)</h1>
<ul>
<li>先挖坑</li>
<li><img data-src="/img/marl1.png" width="1200"></li>
<li><img data-src="/img/marl2.png" width="1200"></li>
<li><img data-src="/img/marl3.png" width="1200"></li>
<li><img data-src="/img/marl4.png" width="1200"></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a>
              <a href="/tags/multi-agent/" rel="tag"># multi-agent</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/05/gpt-debate/" rel="prev" title="Debates between GPTs">
                  <i class="fa fa-angle-left"></i> Debates between GPTs
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/23/next-on-llm/" rel="next" title="[Some Questions asking Myself 2024.4]">
                  [Some Questions asking Myself 2024.4] <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">17:08</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2023/07/20/marl/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
