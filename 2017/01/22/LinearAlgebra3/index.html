<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Lecture 17: Determinants and Their PropertiesDeterminant The determinant of matrix A is a number associated with the matrix, denoted as $detA或者|A|$  Properties of determinants  $detI&#x3D;1$  The sign of">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Linear Algebra 3">
<meta property="og:url" content="https://thinkwee.top/2017/01/22/LinearAlgebra3/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Lecture 17: Determinants and Their PropertiesDeterminant The determinant of matrix A is a number associated with the matrix, denoted as $detA或者|A|$  Properties of determinants  $detI&#x3D;1$  The sign of">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png">
<meta property="article:published_time" content="2017-01-22T11:21:02.000Z">
<meta property="article:modified_time" content="2025-07-15T20:44:22.191Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="linearalgebra">
<meta property="article:tag" content="math">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png">


<link rel="canonical" href="https://thinkwee.top/2017/01/22/LinearAlgebra3/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2017/01/22/LinearAlgebra3/","path":"2017/01/22/LinearAlgebra3/","title":"Note for Linear Algebra 3"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Note for Linear Algebra 3 | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">51</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-17-Determinants-and-Their-Properties"><span class="nav-number">1.</span> <span class="nav-text">Lecture 17: Determinants and Their Properties</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Determinant"><span class="nav-number">1.1.</span> <span class="nav-text">Determinant</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Triangular-matrix-determinant"><span class="nav-number">1.2.</span> <span class="nav-text">Triangular matrix determinant</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-little-more"><span class="nav-number">1.3.</span> <span class="nav-text">A little more</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Eighteenth-Lecture-Determinant-Formulas-and-Algebraic-Cofactors"><span class="nav-number">2.</span> <span class="nav-text">Eighteenth Lecture: Determinant Formulas and Algebraic Cofactors</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Determinant-formula"><span class="nav-number">2.1.</span> <span class="nav-text">Determinant formula</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algebraic-cofactor"><span class="nav-number">2.2.</span> <span class="nav-text">Algebraic cofactor</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#19th-Lecture-Cramer%E2%80%99s-Rule-Inverse-Matrix-Volume"><span class="nav-number">3.</span> <span class="nav-text">19th Lecture: Cramer’s Rule, Inverse Matrix, Volume</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Invertible-matrix"><span class="nav-number">3.1.</span> <span class="nav-text">Invertible matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kramer%E2%80%99s-Rule"><span class="nav-number">3.2.</span> <span class="nav-text">Kramer’s Rule</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Volume"><span class="nav-number">3.3.</span> <span class="nav-text">Volume</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-20-Eigenvalues-and-Eigenvectors"><span class="nav-number">4.</span> <span class="nav-text">Lecture 20: Eigenvalues and Eigenvectors</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-vector"><span class="nav-number">4.1.</span> <span class="nav-text">Feature vector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Several-examples"><span class="nav-number">4.2.</span> <span class="nav-text">Several examples</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#21st-Lecture-Diagonalization-and-Powers-of-A"><span class="nav-number">5.</span> <span class="nav-text">21st Lecture: Diagonalization and Powers of A</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Diagonalization"><span class="nav-number">5.1.</span> <span class="nav-text">Diagonalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A%E2%80%99s-power"><span class="nav-number">5.2.</span> <span class="nav-text">A’s power</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">5.3.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-22-Differential-Equations-and-exp-At"><span class="nav-number">6.</span> <span class="nav-text">Lecture 22: Differential Equations and exp(At)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Differential-Equation"><span class="nav-number">6.1.</span> <span class="nav-text">Differential Equation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#exp-At"><span class="nav-number">6.2.</span> <span class="nav-text">exp(At)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#21st-Lecture-Markov-Matrix-Fourier-Series"><span class="nav-number">7.</span> <span class="nav-text">21st Lecture: Markov Matrix; Fourier Series</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-matrix"><span class="nav-number">7.1.</span> <span class="nav-text">Markov matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fourier-Series"><span class="nav-number">7.2.</span> <span class="nav-text">Fourier Series</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-22-Symmetric-Matrices-and-Positive-Definiteness"><span class="nav-number">8.</span> <span class="nav-text">Lecture 22: Symmetric Matrices and Positive Definiteness</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Symmetric-matrix"><span class="nav-number">8.1.</span> <span class="nav-text">Symmetric matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Positivity"><span class="nav-number">8.2.</span> <span class="nav-text">Positivity</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E8%A7%92%E9%98%B5%E8%A1%8C%E5%88%97%E5%BC%8F"><span class="nav-number">8.3.</span> <span class="nav-text">三角阵行列式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-little-more"><span class="nav-number">8.4.</span> <span class="nav-text">A little more</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AB%E8%AE%B2%EF%BC%9A%E8%A1%8C%E5%88%97%E5%BC%8F%E5%85%AC%E5%BC%8F%E5%92%8C%E4%BB%A3%E6%95%B0%E4%BD%99%E5%AD%90%E5%BC%8F"><span class="nav-number">9.</span> <span class="nav-text">第十八讲：行列式公式和代数余子式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%8C%E5%88%97%E5%BC%8F%E5%85%AC%E5%BC%8F"><span class="nav-number">9.1.</span> <span class="nav-text">行列式公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E6%95%B0%E4%BD%99%E5%AD%90%E5%BC%8F"><span class="nav-number">9.2.</span> <span class="nav-text">代数余子式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B9%9D%E8%AE%B2%EF%BC%9A%E5%85%8B%E6%8B%89%E9%BB%98%E6%B3%95%E5%88%99%EF%BC%8C%E9%80%86%E7%9F%A9%E9%98%B5%EF%BC%8C%E4%BD%93%E7%A7%AF"><span class="nav-number">10.</span> <span class="nav-text">第十九讲：克拉默法则，逆矩阵，体积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%86%E7%9F%A9%E9%98%B5"><span class="nav-number">10.1.</span> <span class="nav-text">逆矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%8B%E6%8B%89%E9%BB%98%E6%B3%95%E5%88%99"><span class="nav-number">10.2.</span> <span class="nav-text">克拉默法则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%93%E7%A7%AF"><span class="nav-number">10.3.</span> <span class="nav-text">体积</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E8%AE%B2%EF%BC%9A%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="nav-number">11.</span> <span class="nav-text">第二十讲：特征值和特征向量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="nav-number">11.1.</span> <span class="nav-text">特征向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E4%BE%8B%E5%AD%90"><span class="nav-number">11.2.</span> <span class="nav-text">几个例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%80%E8%AE%B2%EF%BC%9A%E5%AF%B9%E8%A7%92%E5%8C%96%E5%92%8CA%E7%9A%84%E5%B9%82"><span class="nav-number">12.</span> <span class="nav-text">第二十一讲：对角化和A的幂</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E8%A7%92%E5%8C%96"><span class="nav-number">12.1.</span> <span class="nav-text">对角化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A%E7%9A%84%E5%B9%82"><span class="nav-number">12.2.</span> <span class="nav-text">A的幂</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">12.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E8%AE%B2%EF%BC%9A%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E5%92%8Cexp-At"><span class="nav-number">13.</span> <span class="nav-text">第二十二讲：微分方程和exp(At)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B"><span class="nav-number">13.1.</span> <span class="nav-text">微分方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#exp-At"><span class="nav-number">13.2.</span> <span class="nav-text">exp(At)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%80%E8%AE%B2%EF%BC%9A%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E7%9F%A9%E9%98%B5-%E5%82%85%E7%AB%8B%E5%8F%B6%E7%BA%A7%E6%95%B0"><span class="nav-number">14.</span> <span class="nav-text">第二十一讲：马尔科夫矩阵;傅立叶级数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E7%9F%A9%E9%98%B5"><span class="nav-number">14.1.</span> <span class="nav-text">马尔科夫矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0"><span class="nav-number">14.2.</span> <span class="nav-text">傅里叶级数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E8%AE%B2%EF%BC%9A%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E6%AD%A3%E5%AE%9A%E6%80%A7"><span class="nav-number">15.</span> <span class="nav-text">第二十二讲：对称矩阵及其正定性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5"><span class="nav-number">15.1.</span> <span class="nav-text">对称矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%AE%9A%E6%80%A7"><span class="nav-number">15.2.</span> <span class="nav-text">正定性</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2017/01/22/LinearAlgebra3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Note for Linear Algebra 3 | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Note for Linear Algebra 3
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-01-22 19:21:02" itemprop="dateCreated datePublished" datetime="2017-01-22T19:21:02+08:00">2017-01-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 04:44:22" itemprop="dateModified" datetime="2025-07-16T04:44:22+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a>
        </span>
    </span>

  
    <span id="/2017/01/22/LinearAlgebra3/" class="post-meta-item leancloud_visitors" data-flag-title="Note for Linear Algebra 3" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png" width="500"/></p>
<h1 id="Lecture-17-Determinants-and-Their-Properties"><a href="#Lecture-17-Determinants-and-Their-Properties" class="headerlink" title="Lecture 17: Determinants and Their Properties"></a>Lecture 17: Determinants and Their Properties</h1><h2 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h2><ul>
<li><p>The determinant of matrix A is a number associated with the matrix, denoted as $detA或者|A|$</p>
</li>
<li><p>Properties of determinants</p>
<ul>
<li><p>$detI=1$</p>
</li>
<li><p>The sign of the determinant value will be reversed when rows are exchanged</p>
</li>
<li><p>The determinant of a permutation matrix is 1 or -1, depending on the parity of the number of rows exchanged</p>
</li>
<li><p>Two rows being equal makes the determinant equal to 0 (which can be directly deduced from property two)</p>
</li>
<li><p>Matrix elimination does not change its determinant (proof is below)</p>
</li>
<li><p>A certain row is 0, the determinant is 0 (multiplying by 0 is equivalent to a certain row being 0, resulting in 0)</p>
</li>
<li><p>When and only when A is a singular matrix</p>
</li>
<li><p>$det(A+B) \neq detA+detB \ detAB=(detA)(detB)$</p>
</li>
<li><p>$detA^{-1}detA=1$</p>
</li>
<li><p>$detA^2=(detA)^2$</p>
</li>
<li><p>$det2A=2^n detA$</p>
</li>
<li><p>$detA^T=detA$ (Proof see below)</p>
</li>
</ul>
</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li><p>The determinant is linear by row, but the determinant itself is not linear</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
1 & 0 \\
0 & 1 \\
\end{vmatrix}=1 \\
\begin{vmatrix}
0 & 1 \\
1 & 0 \\
\end{vmatrix}=-1 \\
\begin{vmatrix}
ta & tb \\
c & d \\
\end{vmatrix}=
t\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix} \\
\begin{vmatrix}
t+a & t+b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}+
\begin{vmatrix}
t & t \\
c & d \\
\end{vmatrix}</script></li>
<li><p>Proof that elimination does not change the determinant</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & b \\
c-la & d-lb \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}-l
\begin{vmatrix}
a & b \\
a & b \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}</script></li>
<li><p>Proof that the transpose does not change the determinant</p>
<script type="math/tex; mode=display">
A=LU \\</script></li>
<li><p>Translation: $|U^TL^T|=|LU|$ </p>
<script type="math/tex; mode=display">
|U^T||L^T|=|L||U|</script></li>
<li><p>The above four matrices are all triangular matrices, the determinant equals the product of the diagonal elements, the transpose has no effect, so they are equal</p>
</li>
</ul>
<h2 id="Triangular-matrix-determinant"><a href="#Triangular-matrix-determinant" class="headerlink" title="Triangular matrix determinant"></a>Triangular matrix determinant</h2><ul>
<li>The determinant of the triangular matrix U is the product of the elements on the diagonal (the pivot product)</li>
<li>Why do the other elements of the triangular matrix not work? Because by elimination we can obtain a matrix with only diagonal elements, and elimination does not change the determinant</li>
<li>Why is it the product of the diagonal elements? Because after elimination, the diagonal elements can be successively extracted, yielding $d_1d_2d_3…d_nI$ , where the determinant of the unit matrix is 1</li>
<li>The determinant of a singular matrix is 0, and it has rows of all zeros; the determinant of an invertible matrix is not 0, and it can be reduced to a triangular matrix, with the determinant being the product of the diagonal elements of the triangular matrix</li>
</ul>
<h2 id="A-little-more"><a href="#A-little-more" class="headerlink" title="A little more"></a>A little more</h2><ul>
<li>The determinant obtained from odd-numbered permutations and even-numbered permutations is definitely different (signs differ), which means the matrices after odd-numbered and even-numbered permutations will not be the same, i.e., permutations strictly distinguish between odd and even</li>
</ul>
<h1 id="Eighteenth-Lecture-Determinant-Formulas-and-Algebraic-Cofactors"><a href="#Eighteenth-Lecture-Determinant-Formulas-and-Algebraic-Cofactors" class="headerlink" title="Eighteenth Lecture: Determinant Formulas and Algebraic Cofactors"></a>Eighteenth Lecture: Determinant Formulas and Algebraic Cofactors</h1><h2 id="Determinant-formula"><a href="#Determinant-formula" class="headerlink" title="Determinant formula"></a>Determinant formula</h2><ul>
<li><p>Derive the 2x2 determinant</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & 0 \\
c & d \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & 0 \\
c & 0 \\
\end{vmatrix}+
\begin{vmatrix}
a & 0 \\
0 & d \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
c & 0 \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
0 & d \\
\end{vmatrix} \\
=0+ad-bc+0</script><p>We can find that this method involves taking one row at a time, decomposing this row (determinants are linear by rows), extracting factors, obtaining the unit matrix through row exchanges, and then obtaining the answer through properties one and two</p>
</li>
<li><p>If expanded to a 3x3 matrix, the first row is decomposed into three parts, each of which is further decomposed into three parts for the second row, resulting in a total of 27 parts. The parts that are not zero are those matrices where there are elements in each row and column.</p>
</li>
<li><p>For example</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & 0 & 0\\
0 & 0 & b\\
0 & c & 0\\
\end{vmatrix}</script><p>Extract the factors to obtain $abc$ , swap the second and third rows to get the identity matrix, so the answer is $abc*detI=abc$ , and since a row swap was performed, the answer is negative, $-abc$</p>
</li>
<li><p>A matrix of size n*n can be divided into $n!$ parts, because the first row is divided into n parts, the second row cannot be repeated, and n-1 rows are chosen, each with one repetition, thus obtaining $n!$ parts</p>
</li>
<li><p>The determinant formula is the sum of these $n!$ parts</p>
</li>
</ul>
<h2 id="Algebraic-cofactor"><a href="#Algebraic-cofactor" class="headerlink" title="Algebraic cofactor"></a>Algebraic cofactor</h2><ul>
<li>$det=a<em>{11}(a</em>{22}a<em>{33}-a</em>{23}a<em>{32})+a</em>{12}(….)+a_{13}(….)$</li>
<li>Extract a factor, the remainder formed by the remaining factors, i.e., the content within the parentheses, is the minor determinant</li>
<li>From the matrix perspective, selecting an element, its algebraic cofactor is the determinant of the matrix obtained by excluding the row and column of this element</li>
<li>The algebraic cofactor of $a<em>{ij}$ is denoted as $c</em>{ij}$</li>
<li>Pay attention to the sign of the algebraic cofactor, which is related to the parity of $i+j$ . Even numbers take the positive sign, and odd numbers take the negative sign. Here, the symbol refers to the sign in front of the determinant after the normal calculation of the submatrix corresponding to the algebraic cofactor</li>
<li>$detA=a<em>{11}C</em>{11}+a<em>{12}C</em>{12}+….+a<em>{1n}C</em>{1n}$</li>
</ul>
<h1 id="19th-Lecture-Cramer’s-Rule-Inverse-Matrix-Volume"><a href="#19th-Lecture-Cramer’s-Rule-Inverse-Matrix-Volume" class="headerlink" title="19th Lecture: Cramer’s Rule, Inverse Matrix, Volume"></a>19th Lecture: Cramer’s Rule, Inverse Matrix, Volume</h1><h2 id="Invertible-matrix"><a href="#Invertible-matrix" class="headerlink" title="Invertible matrix"></a>Invertible matrix</h2><ul>
<li><p>Only when the determinant is not zero is the matrix invertible</p>
</li>
<li><p>Invertible matrix formula</p>
<script type="math/tex; mode=display">
A^{-1}=\frac{1}{detA}C^T</script><p>The algebraic cofactor of $C<em>{ij}$ is $A</em>{ij}$</p>
</li>
<li><p>Proof: i.e., prove $AC^T=(detA)I$</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
a_{11} & ... & a_{1n} \\
a_{n1} & ... & a_{nn} \\
\end{bmatrix}
\begin{bmatrix}
c_{11} & ... & c_{n1} \\
c_{1n} & ... & c_{nn} \\
\end{bmatrix}=
\begin{bmatrix}
detA & 0 & 0 \\
0 & detA & 0 \\
0 & 0 & detA \\
\end{bmatrix}</script><p>On the diagonal are determinants, because $det=a<em>{11}(a</em>{22}a<em>{33}-a</em>{23}a<em>{32})+a</em>{12}(….)+a_{13}(….)$ other positions are all 0, because the algebraic cofactor of row a multiplied by row b is equivalent to calculating the determinant of a matrix where row a and row b are equal, and the determinant is 0</p>
</li>
</ul>
<h2 id="Kramer’s-Rule"><a href="#Kramer’s-Rule" class="headerlink" title="Kramer’s Rule"></a>Kramer’s Rule</h2><ul>
<li><p>Ax=b</p>
<script type="math/tex; mode=display">
Ax=b \\
x=A^{-1}b \\
x=\frac{1}{detA}C^Tb \\
\\
x_1=\frac{detB_1}{detA} \\
x_3=\frac{detB_2}{detA} \\
... \\</script></li>
<li><p>Kramer’s rule states that the determinant of matrix $B_i$ is obtained by replacing the ith column of matrix $A$ with b, while keeping the rest unchanged</p>
</li>
</ul>
<h2 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h2><ul>
<li><p>The determinant of A can represent a volume, for example, the determinant of a 3x3 matrix represents a volume within a three-dimensional space</p>
</li>
<li><p>Each row of the matrix represents one edge of a box (originating from the same vertex), and the determinant is the volume of the box; the sign of the determinant represents the left-hand or right-hand system.</p>
</li>
<li><p>(1) The unit matrix corresponds to the unit cube, with a volume of 1</p>
</li>
<li><p>For the orthogonal matrix Q,</p>
<script type="math/tex; mode=display">
QQ^T=I \\
|QQ^T|=|I| \\
|Q||Q^T|=1 \\
{|Q|}^2=1 \\
|Q|=1 \\</script><p>The box corresponding to Q is the unit cube corresponding to the unit matrix rotated by an angle in space</p>
</li>
<li><p>(3a) If a row of a matrix is doubled, i.e., one set of edges of the box is doubled, the volume is also doubled. From the perspective of determinants, the factor can be factored out, so the determinant is also doubled</p>
</li>
<li><p>(2) Swapping two rows of a permutation matrix does not change the volume of the box</p>
</li>
<li><p>(3b) A row of the matrix is split, and the box is also divided into two parts accordingly</p>
</li>
<li><p>The above, the three properties of determinants (1, 2, 3a, 3b) can all be verified in terms of volume</p>
</li>
</ul>
<h1 id="Lecture-20-Eigenvalues-and-Eigenvectors"><a href="#Lecture-20-Eigenvalues-and-Eigenvectors" class="headerlink" title="Lecture 20: Eigenvalues and Eigenvectors"></a>Lecture 20: Eigenvalues and Eigenvectors</h1><h2 id="Feature-vector"><a href="#Feature-vector" class="headerlink" title="Feature vector"></a>Feature vector</h2><ul>
<li>Given matrix A, matrix A can be regarded as a function acting on a vector x, resulting in the vector Ax</li>
<li>When \( \mathbf{A} \) is parallel to \( \mathbf{x} \), i.e., \( \frac{\partial}{\partial x} \), we call \( \mathbf{v} \) the eigenvector and \( \lambda \) the eigenvalue</li>
<li>If A is a singular matrix, $\lambda = 0$ is an eigenvalue</li>
</ul>
<h2 id="Several-examples"><a href="#Several-examples" class="headerlink" title="Several examples"></a>Several examples</h2><ul>
<li><p>If A is a projection matrix, it can be observed that its eigenvectors are any vectors on the projection plane, because $Ax$ represents the projection onto the plane, and all vectors on the plane remain unchanged after projection, thus being parallel. At the same time, the eigenvalues are 1. If a vector is perpendicular to the plane, $Ax=0$ , the eigenvalue is 0. Therefore, the eigenvectors of the projection matrix A fall into two cases, with eigenvalues of 1 or 0.</p>
</li>
<li><p>Another example</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix} \\
\lambda =1, x=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix} \\
\lambda =-1, x=
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix} \\</script></li>
<li><p>An n*n matrix has n eigenvalues</p>
</li>
<li><p>The sum of the eigenvalues equals the sum of the diagonal elements, this sum being called the trace</p>
</li>
<li><p>How to solve $Ax=\lambda x$</p>
<script type="math/tex; mode=display">
(A-\lambda I)x=0 \\</script></li>
<li><p>The visible equation has non-zero solutions, $(A-\lambda I)$ must be singular, i.e.:</p>
<script type="math/tex; mode=display">
det(A-\lambda I)=0 \\</script></li>
<li><script type="math/tex; mode=display">
If \qquad Ax=\lambda x \\
Then \qquad (A+3I)x=(\lambda +3)x \\</script></li>
<li><p>Because the unit matrix is added, the eigenvector remains unchanged as x, and the eigenvalue is increased by the coefficient of the unit matrix, i.e., $(\lambda +3)$</p>
</li>
<li><p>The eigenvalues of A+B are not necessarily the sum of the eigenvalues of A and B, because their eigenvectors may not be the same. Similarly, the eigenvalues of AB are not necessarily the product of their eigenvalues.</p>
</li>
<li><p>For another example, consider the rotation matrix Q</p>
<script type="math/tex; mode=display">
Q=
\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix} \\
trace=0=\lambda _1 +\lambda _2 \\
det=1=\lambda _1 \lambda _2 \\</script></li>
<li><p>However, it can be seen that $\lambda _1，\lambda _2$ has no real solutions</p>
</li>
<li><p>Consider an even worse case (the matrix is more asymmetric, and it is even harder to obtain real eigenvalues)</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
3 & 1 \\
0 & 3 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
3-\lambda & 1 \\
0 & 3-\lambda \\
\end{vmatrix}
==(3-\lambda )^2=0 \\
\lambda _1=\lambda _2=3 \\
x_1=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}</script></li>
</ul>
<h1 id="21st-Lecture-Diagonalization-and-Powers-of-A"><a href="#21st-Lecture-Diagonalization-and-Powers-of-A" class="headerlink" title="21st Lecture: Diagonalization and Powers of A"></a>21st Lecture: Diagonalization and Powers of A</h1><h2 id="Diagonalization"><a href="#Diagonalization" class="headerlink" title="Diagonalization"></a>Diagonalization</h2><ul>
<li><p>Assuming A has n linearly independent eigenvectors, arranged as columns to form the matrix S, i.e., the eigenvector matrix</p>
</li>
<li><p>All discussions about matrix diagonalization presented here are under the premise that S is invertible, i.e., the n eigenvectors are linearly independent</p>
</li>
<li><script type="math/tex; mode=display">
AS=A[x_1,x_2...x_n]=[\lambda _1 x_1,....\lambda _n x_n] \\
=[x_1,x_2,...x_n]
\begin{bmatrix}
\lambda _1 & 0 & ... & 0 \\
0 & \lambda _2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0  & 0 & \lambda _n \\
\end{bmatrix} \\
=S \Lambda \\</script></li>
<li><p>Assuming S is invertible, i.e., the n eigenvectors are linearly independent, we can obtain</p>
<script type="math/tex; mode=display">
S^{-1}AS=\Lambda \\
A=S\Lambda S^{-1} \\</script></li>
<li><p>$\Lambda$ is a diagonal matrix, here we obtain a matrix decomposition other than $A=LU$ and $A=QR$</p>
</li>
<li><script type="math/tex; mode=display">
if \qquad Ax=\lambda x \\
A^2 x=\lambda AX=\lambda ^2 x \\
A^2=S\Lambda S^{-1} S \Lambda S^{-1}=S \Lambda ^2 S^{-1} \\</script></li>
<li><p>The two equations above regarding $A^2$ indicate that the squared eigen vectors remain unchanged, the eigenvalues are squared, and similarly for the K-th power</p>
</li>
<li><p>Eigenvalues and eigenvectors help us understand matrix powers. When calculating matrix powers, we can decompose the matrix into the form of a matrix of eigenvectors multiplied by a diagonal matrix, where K multiplications can cancel each other out, as shown in the above formula</p>
</li>
<li><p>What kind of matrix’s power tends to 0 (stable)</p>
<script type="math/tex; mode=display">
A^K \rightarrow 0 \quad as \quad K \rightarrow \infty \\
if \quad all |\lambda _i|<1 \\</script></li>
<li><p>Which matrices can be diagonalized? If all eigenvalues are different, then A can be diagonalized</p>
</li>
<li><p>If matrix A is already diagonal, then $\Lambda$ is the same as A</p>
</li>
<li><p>The number of times an eigenvalue repeats is called the algebraic multiplicity, for triangular matrices, such as</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
2 & 1 \\
0 & 2 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
2-\lambda & 1 \\
0 & 2-\lambda \\
\end{vmatrix}=0 \\
\lambda =2 \\
A-\lambda I=
\begin{bmatrix}
0 & 1 \\
0 & 0 \\
\end{bmatrix} \\</script></li>
<li><p>For $A-\lambda I$ , the geometric multiplicity is 1, while the algebraic multiplicity of the eigenvalue is 2</p>
</li>
<li><p>The eigenvector is only (1,0), therefore, for a triangular matrix, it cannot be diagonalized, and there do not exist two linearly independent eigenvectors.</p>
</li>
</ul>
<h2 id="A’s-power"><a href="#A’s-power" class="headerlink" title="A’s power"></a>A’s power</h2><ul>
<li><p>Most matrices have a set of linearly independent eigenvalues that can be diagonalized. If diagonalization is possible, we need to focus on how to solve for the powers of A.</p>
</li>
<li><script type="math/tex; mode=display">
give \quad u_0 \\
u_{k+1}=Au_k \\
u_k=A^ku_0 \\
how \quad to \quad solve \quad u_k \\
u_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\
Au_0=c_1 \lambda _1 x_1 + c_2 \lambda _2 x_2 +...+c_n \lambda _n x_n \\
A^{100}u_0=c_1 \lambda _1^{100} x_1 + c_2 \lambda _2^{100} x_2 +...+c_n \lambda _n^{100} x_n \\
=S\Lambda ^{100} C \\
=u_{100} \\</script></li>
<li><p>Because the n feature vectors are mutually linearly independent, they can serve as a set of bases to cover the entire n-dimensional space, and naturally, $u_0$ can be represented as a linear combination of the feature vectors, with C being the linear coefficient vector. The above formula has derived the method for solving matrix powers, and the next example will be given using the Fibonacci sequence.</p>
<script type="math/tex; mode=display">
F_0=0 \\
F_1=1 \\
F_2=1 \\
F_3=2 \\
F_4=3 \\
F_5=5 \\
..... \\
F_{100}=? \\</script></li>
<li><p>The growth rate of the Fibonacci sequence is how fast? Determined by the eigenvalues, we attempt to construct vectors to find the matrix relationship of the iterative Fibonacci sequence</p>
<script type="math/tex; mode=display">
F_{k+2}=F_{k+1}+F_k \\
F_{k+1}=F_{k+1} \\</script></li>
<li><p>Define vector</p>
<script type="math/tex; mode=display">
u_k=
\begin{bmatrix}
F_{k+1} \\
F_k \\
\end{bmatrix} \\</script></li>
<li><p>Using this vector, the first two equations can be written in matrix form</p>
<script type="math/tex; mode=display">
u_{k+1}=
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
\end{bmatrix}
u_k \\
A=
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
\end{bmatrix} \\
\lambda =\frac {1 \pm \sqrt 5}2 \\</script></li>
<li><p>Obtaining two eigenvalues, it is easy to obtain the corresponding eigenvectors</p>
</li>
<li><p>Returning to the Fibonacci sequence, the growth rate of the Fibonacci sequence is determined by the eigenvalues of the “sequence update matrix” we construct, and as can be seen from $A^{100}u<em>0=c_1 \lambda _1^100 x_1 + c_2 \lambda _2^100 x_2 +…+c_n \lambda _n^100 x_n$ , the growth rate is mainly determined by the larger eigenvalues, therefore $F</em>{100}$ can be written in the following form</p>
<script type="math/tex; mode=display">
F_{100} \approx c_1 {\frac {1 + \sqrt 5}2}^{100} \\</script></li>
<li><p>There are initial values</p>
<script type="math/tex; mode=display">
u_0=
\begin{bmatrix}
F_1 \\
F_0 \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
=c_1x_1+c_2x_2</script></li>
<li><p>Among them, $x<em>1,x_2$ are two feature vectors, whose linear coefficients can be calculated, and by substituting them into the formula, an approximate value of $F</em>{100}$ can be obtained</p>
</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>We find that under the condition of A being invertible, A can be decomposed into the form $S\Lambda S^{-1}$</li>
<li>This form has a characteristic that facilitates the calculation of the power of A, as it can be observed that the unit matrix of the eigenvalues of A’s power is the power of the unit matrix of A’s eigenvalues</li>
<li>We attempt to apply this feature in solving the Fibonacci sequence, first converting the sequence update into a matrix form</li>
<li>Determine the eigenvalues and eigenvectors of the matrix</li>
<li>From the expansion of the power series of A, it can be seen that the power of A is mainly determined by the larger eigenvalues, therefore $F<em>{100}$ can be written in the form of $F</em>{100} \approx c_1 {(\frac {1 + \sqrt 5}2)}^{100}$</li>
<li>By the initial value $F<em>0$ , calculate the linear coefficients, substitute them into the above formula, and obtain the approximate value of $F</em>{100}$</li>
<li>This is an example of a difference equation; the next section will discuss differential equations</li>
</ul>
<h1 id="Lecture-22-Differential-Equations-and-exp-At"><a href="#Lecture-22-Differential-Equations-and-exp-At" class="headerlink" title="Lecture 22: Differential Equations and exp(At)"></a>Lecture 22: Differential Equations and exp(At)</h1><h2 id="Differential-Equation"><a href="#Differential-Equation" class="headerlink" title="Differential Equation"></a>Differential Equation</h2><ul>
<li><p>The solutions to linear equations with constant coefficients are in exponential form; if the solution to the differential equation is in exponential form, one can find the solution by using linear algebra to determine the exponents and coefficients</p>
</li>
<li><p>For example</p>
<script type="math/tex; mode=display">
\frac{du_1}{dt}=-u_1+2u_2 \\
\frac{du_2}{dt}=u_1-2u_2 \\
u(0)=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix} \\</script></li>
<li><p>First, we list the coefficient matrix and find the eigenvalues and eigenvectors of the matrix</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
-1 & 2 \\
1 & -2 \\
\end{bmatrix}</script></li>
<li><p>$\lambda=0$ is a solution of this singular matrix, and from the trace it can be seen that the second eigenvalue is $\lambda=-3$ , and two eigenvectors are obtained</p>
<script type="math/tex; mode=display">
x_1=
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix} \\
x_2=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix}</script></li>
<li><p>The general solution form of the differential equation will be</p>
<script type="math/tex; mode=display">
u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2</script></li>
<li><p>Why?</p>
<script type="math/tex; mode=display">
\frac{du}{dt} \\
=c_1 \lambda _1 e^{\lambda _1 t}x_1 \\
=A c_1 e^{\lambda _1 t}x_1 \\
because \quad A x_1=\lambda _1 x_1 \\</script></li>
<li><p>In the differential equation $u_{k+1}=Au_k$ , the form of the solution is $c_1\lambda _1 ^k x_1+c_2 \lambda _2 ^k x_2$</p>
</li>
<li><p>In the differential equation $\frac {du}{dt}=Au$ , the form of the solution is $u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2$</p>
</li>
<li><p>Solved from the initial values, i.e., the coefficient matrix C multiplied by the eigenvector matrix S yields the initial values</p>
</li>
<li><p>It can be seen that as t approaches infinity, the solution of the example equation is reduced to only the steady-state part, i.e., $(\frac 23,\frac 13)$</p>
</li>
<li><p>When does the solution tend towards 0? There exist negative eigenvalues because $e^{\lambda t}$ needs to tend towards 0</p>
</li>
<li><p>If the eigenvalues are complex? The magnitude of the imaginary part is 1, so if the real part of the complex number is negative, the solution still tends towards 0</p>
</li>
<li><p>When does a steady state exist? Only 0 and negative eigenvalues exist, as in the example above</p>
</li>
<li><p>When does the solution fail to converge? Any eigenvalue has a real part greater than 0</p>
</li>
<li><p>The sign of the coefficient matrix changes, the eigenvalues also change sign, the steady-state solution remains steady-state, and the convergent solution will become divergent</p>
</li>
<li><p>How to directly determine if the solution converges from a matrix? That is, do all the real parts of the eigenvalues have a value less than 0?</p>
</li>
<li><p>The trace of the matrix should be less than 0, but the sum of the diagonal elements being 0 does not necessarily converge, as</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
-2 & 0 \\
0 & 1 \\
\end{bmatrix}</script></li>
<li><p>Therefore, another condition is required: the value of the determinant is the product of the eigenvalues, so the value of the determinant should be greater than 0</p>
</li>
</ul>
<h2 id="exp-At"><a href="#exp-At" class="headerlink" title="exp(At)"></a>exp(At)</h2><ul>
<li>Can the solution be expressed in the form of $S,\Lambda$</li>
<li>Matrix A represents $u_1,u_2$ coupling, first we need to diagonalize u to decouple</li>
<li><script type="math/tex; mode=display">
\frac{du}{dt} = Au \\
set \quad u=Sv \\
S \frac{dv}{dt} = ASv \\
\frac{dv}{dt}=S^{-1}ASv=\Lambda v \\
v(t)=e^{\Lambda t}v(0) \\
u(t)=Se^{\Lambda t}S^{-1}u(0) \\</script></li>
</ul>
<h1 id="21st-Lecture-Markov-Matrix-Fourier-Series"><a href="#21st-Lecture-Markov-Matrix-Fourier-Series" class="headerlink" title="21st Lecture: Markov Matrix; Fourier Series"></a>21st Lecture: Markov Matrix; Fourier Series</h1><h2 id="Markov-matrix"><a href="#Markov-matrix" class="headerlink" title="Markov matrix"></a>Markov matrix</h2><ul>
<li><p>A typical Markov matrix</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
0.1 & 0.01 & 0.3 \\
0.2 & 0.99 & 0.3 \\
0.7 & 0 & 0.4 \\
\end{bmatrix}</script></li>
<li><p>Each element is greater than or equal to 0, the sum of each column is 1, and the powers of the Markov matrix are all Markov matrices</p>
</li>
<li><p>$\lambda=1$ is an eigenvalue, and the absolute values of the other eigenvalues are all less than 1</p>
</li>
<li><p>In the previous lecture, we discussed that the power of a matrix can be decomposed into</p>
<script type="math/tex; mode=display">
u_k=A^ku_0=c_1\lambda _1 ^kx_1+c_2\lambda _2 ^kx_2+.....</script></li>
<li><p>When A is a Markov matrix, there is only one eigenvalue of 1, and the other eigenvalues are less than 1. As k increases, the terms with eigenvalues less than 1 tend to approach 0, retaining only the term with the eigenvalue of 1, and the elements of the corresponding eigenvector are all greater than 0</p>
</li>
<li><p>When the sum of each column is 1, there necessarily exists an eigenvalue $\lambda =1$</p>
</li>
<li><p>Proof:</p>
<script type="math/tex; mode=display">
A-I=
\begin{bmatrix}
-0.9 & 0.01 & 0.3 \\
0.2 & -0.01 & 0.3 \\
0.7 & 0 & -0.6 \\
\end{bmatrix}</script></li>
<li><p>If 1 is an eigenvalue, then $A-I$ should be singular. It can be seen that the sum of each column of $A-I$ is 0, indicating that the row vectors are linearly dependent, i.e., the matrix is singular, and the all-ones vector lies in the left null space.</p>
</li>
<li><p>For the Markov matrix A, we study $u_{k+1}=Au_k$</p>
</li>
<li><p>An example, u is the population in Massachusetts and California, A is the population mobility matrix</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k+1}
=
\begin{bmatrix}
0.9 & 0.2 \\
0.1 & 0.8 \\
\end{bmatrix}
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k}</script></li>
<li><p>It can be seen that each year (k), 80% of people stay in Massachusetts, 20% move to California, and 10% from California also relocate to Massachusetts</p>
</li>
<li><p>On the Markov matrix A</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
0.9 & 0.2 \\
0.1 & 0.8 \\
\end{bmatrix} \\
\lambda _1 =1 \\
\lambda _2 =0.7 \\</script></li>
<li><p>For the eigenvalue of 1, the eigenvector is easily found as $(2,1)$ , and for the eigenvalue of 0.7, the eigenvector is (-1,1).</p>
</li>
<li><p>Obtain the formula we are to study</p>
<script type="math/tex; mode=display">
u_k=c_1*1^k*
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix}
+c_2*(0.7)^k*
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}</script></li>
<li><p>Assuming there were initially 0 people in California and 1000 people in Massachusetts, i.e., $u_0$ , substituting this into the formula yields $c_1,c_2$ . It can be seen that after many years, the populations of California and Massachusetts will stabilize, each accounting for one-third and two-thirds of the total 1000 people, respectively.</p>
</li>
<li><p>The vector with a sum of 1 is another way to define a Markov matrix</p>
</li>
</ul>
<h2 id="Fourier-Series"><a href="#Fourier-Series" class="headerlink" title="Fourier Series"></a>Fourier Series</h2><ul>
<li><p>Discussion of projection problems with standard orthogonal bases</p>
</li>
<li><p>If $q_1….q_n$ is a set of standard orthogonal bases, any vector $v$ is a linear combination of this set of bases</p>
</li>
<li><p>We now need to determine the linear combination coefficients $x_1….x_n$ , $v=x_1q_1+x_2q_2+…x_nq_n$ . One method is to take the inner product of $v$ and $q_i$ , and calculate the coefficients one by one</p>
<script type="math/tex; mode=display">
q_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\</script></li>
<li><p>Written in matrix form</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
q_1 & q_2 & ... & q_n \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
... \\
x_n \\
\end{bmatrix}=
v \\
Qx=v \\
x=Q^{-1}v=Q^Tv \\</script></li>
<li><p>Now discussing Fourier series</p>
</li>
<li><p>We hope to decompose the function</p>
<script type="math/tex; mode=display">
f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......</script></li>
<li><p>The key is that in this decomposition, $coskx,sinkx$ constitutes an infinite orthogonal basis for a set of function spaces, i.e., the inner products of these functions are 0 (the inner product of vectors is a discrete value summation, while the inner product of functions is a continuous value integration).</p>
</li>
<li><p>How to calculate the Fourier coefficients?</p>
</li>
<li><p>Using the previous vector example to calculate</p>
</li>
<li><p>Sequentially compute the inner product of $f(x)$ with each element of the orthogonal basis, obtaining the corresponding coefficient multiplied by $\pi$ , for example</p>
<script type="math/tex; mode=display">
\int _0 ^{2\pi} f(x)cosx dx=0+ a_1 \int _0^{2\pi}(cosx)^2dx+0+0...+0=\pi a_1 \\</script></li>
</ul>
<h1 id="Lecture-22-Symmetric-Matrices-and-Positive-Definiteness"><a href="#Lecture-22-Symmetric-Matrices-and-Positive-Definiteness" class="headerlink" title="Lecture 22: Symmetric Matrices and Positive Definiteness"></a>Lecture 22: Symmetric Matrices and Positive Definiteness</h1><h2 id="Symmetric-matrix"><a href="#Symmetric-matrix" class="headerlink" title="Symmetric matrix"></a>Symmetric matrix</h2><ul>
<li><p>The eigenvalues of a symmetric matrix are real numbers, and the eigenvectors corresponding to distinct eigenvalues are mutually orthogonal</p>
</li>
<li><p>For a general matrix $A=S\Lambda S^{-1}$ , S is the matrix of eigenvectors</p>
</li>
<li><p>For the symmetric matrix $A=Q\Lambda Q^{-1}=Q\Lambda Q^T$ , Q is the matrix of standard orthogonal eigenvectors</p>
</li>
<li><p>Why are all eigenvalues real numbers?</p>
</li>
<li><p>Conjugate both left and right, as we are now only considering the real matrix A, $Ax^{<em>}=\lambda ^{</em>} x^{*}$</p>
</li>
<li><p>$\lambda$ and its conjugate are eigenvalues; now take the transpose of both sides of the equation, $x^{<em> T}A^T=x^{</em> T} \lambda ^{* T}$</p>
</li>
<li><p>In the above formula, $A=A^T$ , and both sides are multiplied by $x$ , comparing with $x^{<em> T}A\lambda x^{</em> T}x$ yields $\lambda ^{*}=\lambda$ , i.e., the eigenvalues are real numbers</p>
</li>
<li><p>It is evident that for multiple matrices, the condition $A=A^{* T}$ is required to satisfy symmetry</p>
</li>
<li><p>For symmetric matrices</p>
<script type="math/tex; mode=display">
A=Q\Lambda Q^{-1}=Q\Lambda Q^T \\
=\lambda _1 q_1 q_1^T+\lambda _2 q_2 q_2^T+.... \\</script></li>
<li><p>So every symmetric matrix is a combination of some mutually orthogonal projection matrices</p>
</li>
<li><p>For symmetric matrices, the number of positive principal minors is equal to the number of positive eigenvalues, and the product of the principal minors equals the product of the eigenvalues, which equals the determinant of the matrix</p>
</li>
</ul>
<h2 id="Positivity"><a href="#Positivity" class="headerlink" title="Positivity"></a>Positivity</h2><ul>
<li>Positive definite matrices are symmetric matrices, a subclass of symmetric matrices, whose all eigenvalues are positive, all leading principal minors are positive, and all subdeterminants are positive</li>
<li>The sign of eigenvalues is related to stability</li>
<li>The eigenvalue, determinant, and main element are unified as one in linear algebra</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li><p>行列式按行是线性的，但行列式本身不是线性的</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
1 & 0 \\
0 & 1 \\
\end{vmatrix}=1 \\
\begin{vmatrix}
0 & 1 \\
1 & 0 \\
\end{vmatrix}=-1 \\
\begin{vmatrix}
ta & tb \\
c & d \\
\end{vmatrix}=
t\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix} \\
\begin{vmatrix}
t+a & t+b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}+
\begin{vmatrix}
t & t \\
c & d \\
\end{vmatrix}</script></li>
<li><p>证明消元不改变行列式</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & b \\
c-la & d-lb \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}-l
\begin{vmatrix}
a & b \\
a & b \\
\end{vmatrix}=
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}</script></li>
<li><p>证明转置不改变行列式</p>
<script type="math/tex; mode=display">
A=LU \\</script></li>
<li>即证 $|U^TL^T|=|LU|$<script type="math/tex; mode=display">
|U^T||L^T|=|L||U|</script></li>
<li>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 </li>
</ul>
<h2 id="三角阵行列式"><a href="#三角阵行列式" class="headerlink" title="三角阵行列式"></a>三角阵行列式</h2><ul>
<li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li>
<li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li>
<li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3…d_nI$，其中单位矩阵的行列式为1</li>
<li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li>
</ul>
<h2 id="A-little-more"><a href="#A-little-more" class="headerlink" title="A little more"></a>A little more</h2><ul>
<li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li>
</ul>
<h1 id="第十八讲：行列式公式和代数余子式"><a href="#第十八讲：行列式公式和代数余子式" class="headerlink" title="第十八讲：行列式公式和代数余子式"></a>第十八讲：行列式公式和代数余子式</h1><h2 id="行列式公式"><a href="#行列式公式" class="headerlink" title="行列式公式"></a>行列式公式</h2><ul>
<li><p>推导2*2行列式</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & 0 \\
c & d \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
c & d \\
\end{vmatrix}=
\begin{vmatrix}
a & 0 \\
c & 0 \\
\end{vmatrix}+
\begin{vmatrix}
a & 0 \\
0 & d \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
c & 0 \\
\end{vmatrix}+
\begin{vmatrix}
0 & b \\
0 & d \\
\end{vmatrix} \\
=0+ad-bc+0</script><p>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</p>
</li>
<li>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</li>
<li><p>例如</p>
<script type="math/tex; mode=display">
\begin{vmatrix}
a & 0 & 0\\
0 & 0 & b\\
0 & c & 0\\
\end{vmatrix}</script><p>先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$</p>
</li>
<li>n*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分</li>
<li>行列式公式就是这$n!$个部分加起来</li>
</ul>
<h2 id="代数余子式"><a href="#代数余子式" class="headerlink" title="代数余子式"></a>代数余子式</h2><ul>
<li>$det=a<em>{11}(a</em>{22}a<em>{33}-a</em>{23}a<em>{32})+a</em>{12}(….)+a_{13}(….)$</li>
<li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li>
<li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li>
<li>$a<em>{ij}$的代数余子式记作$c</em>{ij}$</li>
<li>注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li>
<li>$detA=a<em>{11}C</em>{11}+a<em>{12}C</em>{12}+….+a<em>{1n}C</em>{1n}$    </li>
</ul>
<h1 id="第十九讲：克拉默法则，逆矩阵，体积"><a href="#第十九讲：克拉默法则，逆矩阵，体积" class="headerlink" title="第十九讲：克拉默法则，逆矩阵，体积"></a>第十九讲：克拉默法则，逆矩阵，体积</h1><h2 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h2><ul>
<li>只有行列式不为0时，矩阵才是可逆的</li>
<li><p>逆矩阵公式</p>
<script type="math/tex; mode=display">
A^{-1}=\frac{1}{detA}C^T</script><p>其中$C<em>{ij}$是$A</em>{ij}$的代数余子式</p>
</li>
<li><p>证明：即证$AC^T=(detA)I$</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
a_{11} & ... & a_{1n} \\
a_{n1} & ... & a_{nn} \\
\end{bmatrix}
\begin{bmatrix}
c_{11} & ... & c_{n1} \\
c_{1n} & ... & c_{nn} \\
\end{bmatrix}=
\begin{bmatrix}
detA & 0 & 0 \\
0 & detA & 0 \\
0 & 0 & detA \\
\end{bmatrix}</script><p>对角线上都是行列式，因为$det=a<em>{11}(a</em>{22}a<em>{33}-a</em>{23}a<em>{32})+a</em>{12}(….)+a_{13}(….)$<br>其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</p>
</li>
</ul>
<h2 id="克拉默法则"><a href="#克拉默法则" class="headerlink" title="克拉默法则"></a>克拉默法则</h2><ul>
<li><p>解Ax=b</p>
<script type="math/tex; mode=display">
Ax=b \\
x=A^{-1}b \\
x=\frac{1}{detA}C^Tb \\
\\
x_1=\frac{detB_1}{detA} \\
x_3=\frac{detB_2}{detA} \\
... \\</script></li>
<li>克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变</li>
</ul>
<h2 id="体积"><a href="#体积" class="headerlink" title="体积"></a>体积</h2><ul>
<li>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</li>
<li>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</li>
<li>(1)单位矩阵对应单位立方体，体积为1</li>
<li><p>对正交矩阵Q,</p>
<script type="math/tex; mode=display">
QQ^T=I \\
|QQ^T|=|I| \\
|Q||Q^T|=1 \\
{|Q|}^2=1 \\
|Q|=1 \\</script><p>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</p>
</li>
<li>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</li>
<li>(2)交换矩阵两行，盒子的体积不变</li>
<li>(3b)矩阵某一行拆分，盒子也相应切分为两部分</li>
<li>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</li>
</ul>
<h1 id="第二十讲：特征值和特征向量"><a href="#第二十讲：特征值和特征向量" class="headerlink" title="第二十讲：特征值和特征向量"></a>第二十讲：特征值和特征向量</h1><h2 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h2><ul>
<li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li>
<li>当Ax平行于x时，即$Ax=\lambda x$，我们称$x$为特征向量，$\lambda$为特征值</li>
<li>如果A是奇异矩阵，$\lambda = 0$是一个特征值</li>
</ul>
<h2 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h2><ul>
<li>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</li>
<li><p>再举一例</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix} \\
\lambda =1, x=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix} \\
\lambda =-1, x=
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix} \\</script></li>
<li>n*n矩阵有n个特征值</li>
<li>特征值的和等于对角线元素和，这个和称为迹(trace)，</li>
<li><p>如何求解$Ax=\lambda x$</p>
<script type="math/tex; mode=display">
(A-\lambda I)x=0 \\</script></li>
<li><p>可见方程有非零解，$(A-\lambda I)$必须是奇异的<br>即: </p>
<script type="math/tex; mode=display">
det(A-\lambda I)=0 \\</script></li>
<li><script type="math/tex; mode=display">
If \qquad Ax=\lambda x \\
Then \qquad (A+3I)x=(\lambda +3)x \\</script></li>
<li>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\lambda +3)$</li>
<li>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</li>
<li><p>再举一例，对旋转矩阵Q</p>
<script type="math/tex; mode=display">
Q=
\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix} \\
trace=0=\lambda _1 +\lambda _2 \\
det=1=\lambda _1 \lambda _2 \\</script></li>
<li>但是可以看出 $\lambda _1，\lambda _2$无实数解 </li>
<li><p>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
3 & 1 \\
0 & 3 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
3-\lambda & 1 \\
0 & 3-\lambda \\
\end{vmatrix}
==(3-\lambda )^2=0 \\
\lambda _1=\lambda _2=3 \\
x_1=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}</script></li>
</ul>
<h1 id="第二十一讲：对角化和A的幂"><a href="#第二十一讲：对角化和A的幂" class="headerlink" title="第二十一讲：对角化和A的幂"></a>第二十一讲：对角化和A的幂</h1><h2 id="对角化"><a href="#对角化" class="headerlink" title="对角化"></a>对角化</h2><ul>
<li>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</li>
<li>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</li>
<li><script type="math/tex; mode=display">
AS=A[x_1,x_2...x_n]=[\lambda _1 x_1,....\lambda _n x_n] \\
=[x_1,x_2,...x_n]
\begin{bmatrix}
\lambda _1 & 0 & ... & 0 \\
0 & \lambda _2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0  & 0 & \lambda _n \\
\end{bmatrix} \\
=S \Lambda \\</script></li>
<li><p>假设S可逆，即n个特征向量无关，此时可以得到</p>
<script type="math/tex; mode=display">
S^{-1}AS=\Lambda \\
A=S\Lambda S^{-1} \\</script></li>
<li>$\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解</li>
<li><script type="math/tex; mode=display">
if \qquad Ax=\lambda x \\
A^2 x=\lambda AX=\lambda ^2 x \\
A^2=S\Lambda S^{-1} S \Lambda S^{-1}=S \Lambda ^2 S^{-1} \\</script></li>
<li>上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理</li>
<li>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</li>
<li><p>什么样的矩阵的幂趋向于0(稳定)</p>
<script type="math/tex; mode=display">
A^K \rightarrow 0 \quad as \quad K \rightarrow \infty \\
if \quad all |\lambda _i|<1 \\</script></li>
<li>哪些矩阵可以对角化？<br>如果所有特征值不同，则A可以对角化</li>
<li>如果矩阵A已经是对角阵，则$\Lambda$与A相同</li>
<li><p>特征值重复的次数称为代数重度，对三角阵，如</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
2 & 1 \\
0 & 2 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
2-\lambda & 1 \\
0 & 2-\lambda \\
\end{vmatrix}=0 \\
\lambda =2 \\
A-\lambda I=
\begin{bmatrix}
0 & 1 \\
0 & 0 \\
\end{bmatrix} \\</script></li>
<li>对$A-\lambda I$，几何重数是1，而特征值的代数重度是2</li>
<li>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</li>
</ul>
<h2 id="A的幂"><a href="#A的幂" class="headerlink" title="A的幂"></a>A的幂</h2><ul>
<li>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</li>
<li><script type="math/tex; mode=display">
give \quad u_0 \\
u_{k+1}=Au_k \\
u_k=A^ku_0 \\
how \quad to \quad solve \quad u_k \\
u_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\
Au_0=c_1 \lambda _1 x_1 + c_2 \lambda _2 x_2 +...+c_n \lambda _n x_n \\
A^{100}u_0=c_1 \lambda _1^{100} x_1 + c_2 \lambda _2^{100} x_2 +...+c_n \lambda _n^{100} x_n \\
=S\Lambda ^{100} C \\
=u_{100} \\</script></li>
<li><p>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例</p>
<script type="math/tex; mode=display">
F_0=0 \\
F_1=1 \\
F_2=1 \\
F_3=2 \\
F_4=3 \\
F_5=5 \\
..... \\
F_{100}=? \\</script></li>
<li><p>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系</p>
<script type="math/tex; mode=display">
F_{k+2}=F_{k+1}+F_k \\
F_{k+1}=F_{k+1} \\</script></li>
<li><p>定义向量</p>
<script type="math/tex; mode=display">
u_k=
\begin{bmatrix}
F_{k+1} \\
F_k \\
\end{bmatrix} \\</script></li>
<li><p>利用这个向量可以将前两个等式写成矩阵形式 </p>
<script type="math/tex; mode=display">
u_{k+1}=
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
\end{bmatrix}
u_k \\
A=
\begin{bmatrix}
1 & 1 \\
1 & 0 \\
\end{bmatrix} \\
\lambda =\frac {1 \pm \sqrt 5}2 \\</script></li>
<li>得到两个特征值，我们很容易得到特征向量</li>
<li><p>回到斐波那契数列，斐波那契数列的增长速率由我们构造的”数列更新矩阵”的特征值决定，而且由$A^{100}u<em>0=c_1 \lambda _1^100 x_1 + c_2 \lambda _2^100 x_2 +…+c_n \lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F</em>{100}$可以写成如下形式</p>
<script type="math/tex; mode=display">
F_{100} \approx c_1 {\frac {1 + \sqrt 5}2}^{100} \\</script></li>
<li><p>再有初始值有</p>
<script type="math/tex; mode=display">
u_0=
\begin{bmatrix}
F_1 \\
F_0 \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
=c_1x_1+c_2x_2</script></li>
<li>其中$x<em>1,x_2$是两个特征向量，线性系数可求，代入公式可求$F</em>{100}$的近似值</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>我们发现在A可逆的情况下，A可以分解成$S\Lambda S^{-1}$的形式</li>
<li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li>
<li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li>
<li>求出矩阵的特征值，特征向量</li>
<li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F<em>{100}$可以写成$F</em>{100} \approx c_1 {(\frac {1 + \sqrt 5}2)}^{100}$的形式</li>
<li>由初始值$F<em>0$求出线性系数，代入上式，得到$F</em>{100}$的近似值</li>
<li>以上是差分方程的一个例子，下一节将讨论微分方程</li>
</ul>
<h1 id="第二十二讲：微分方程和exp-At"><a href="#第二十二讲：微分方程和exp-At" class="headerlink" title="第二十二讲：微分方程和exp(At)"></a>第二十二讲：微分方程和exp(At)</h1><h2 id="微分方程"><a href="#微分方程" class="headerlink" title="微分方程"></a>微分方程</h2><ul>
<li>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</li>
<li><p>举个例子</p>
<script type="math/tex; mode=display">
\frac{du_1}{dt}=-u_1+2u_2 \\
\frac{du_2}{dt}=u_1-2u_2 \\
u(0)=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix} \\</script></li>
<li><p>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量</p>
<script type="math/tex; mode=display">
A=
\begin{bmatrix}
-1 & 2 \\
1 & -2 \\
\end{bmatrix}</script></li>
<li><p>易得$\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\lambda=-3$，并得到两个特征向量</p>
<script type="math/tex; mode=display">
x_1=
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix} \\
x_2=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix}</script></li>
<li><p>微分方程解的通解形式将是</p>
<script type="math/tex; mode=display">
u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2</script></li>
<li><p>为什么？</p>
<script type="math/tex; mode=display">
\frac{du}{dt} \\
=c_1 \lambda _1 e^{\lambda _1 t}x_1 \\
=A c_1 e^{\lambda _1 t}x_1 \\
because \quad A x_1=\lambda _1 x_1 \\</script></li>
<li>在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\lambda _1 ^k x_1+c_2 \lambda _2 ^k x_2$</li>
<li>在微分方程$\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2$</li>
<li>$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</li>
<li>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\frac 23,\frac 13)$</li>
<li>什么时候解趋向于0？存在负数特征值，因为$e^{\lambda t}$需要趋向于0</li>
<li>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</li>
<li>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</li>
<li>什么时候解无法收敛？任何特征值的实数部分大于0</li>
<li>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</li>
<li>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</li>
<li><p>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
-2 & 0 \\
0 & 1 \\
\end{bmatrix}</script></li>
<li>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</li>
</ul>
<h2 id="exp-At"><a href="#exp-At" class="headerlink" title="exp(At)"></a>exp(At)</h2><ul>
<li>是否可以把解表示成$S,\Lambda$的形式</li>
<li>矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦</li>
<li><script type="math/tex; mode=display">
\frac{du}{dt} = Au \\
set \quad u=Sv \\
S \frac{dv}{dt} = ASv \\
\frac{dv}{dt}=S^{-1}ASv=\Lambda v \\
v(t)=e^{\Lambda t}v(0) \\
u(t)=Se^{\Lambda t}S^{-1}u(0) \\</script></li>
</ul>
<h1 id="第二十一讲：马尔科夫矩阵-傅立叶级数"><a href="#第二十一讲：马尔科夫矩阵-傅立叶级数" class="headerlink" title="第二十一讲：马尔科夫矩阵;傅立叶级数"></a>第二十一讲：马尔科夫矩阵;傅立叶级数</h1><h2 id="马尔科夫矩阵"><a href="#马尔科夫矩阵" class="headerlink" title="马尔科夫矩阵"></a>马尔科夫矩阵</h2><ul>
<li><p>一个典型的马尔科夫矩阵</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
0.1 & 0.01 & 0.3 \\
0.2 & 0.99 & 0.3 \\
0.7 & 0 & 0.4 \\
\end{bmatrix}</script></li>
<li>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</li>
<li><p>$\lambda=1$是一个特征值，其余的特征值的绝对值都小于1</p>
</li>
<li><p>在上一讲中我们谈到矩阵的幂可以分解为</p>
<script type="math/tex; mode=display">
u_k=A^ku_0=c_1\lambda _1 ^kx_1+c_2\lambda _2 ^kx_2+.....</script></li>
<li>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</li>
<li>当每一列和为1时，必然存在一个特征值$\lambda =1$</li>
<li><p>证明：</p>
<script type="math/tex; mode=display">
A-I=
\begin{bmatrix}
-0.9 & 0.01 & 0.3 \\
0.2 & -0.01 & 0.3 \\
0.7 & 0 & -0.6 \\
\end{bmatrix}</script></li>
<li>若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</li>
<li>对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$</li>
<li><p>一个例子，u是麻省和加州的人数，A是人口流动矩阵</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k+1}
=
\begin{bmatrix}
0.9 & 0.2 \\
0.1 & 0.8 \\
\end{bmatrix}
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k}</script></li>
<li>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</li>
<li><p>对马尔科夫矩阵A</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
0.9 & 0.2 \\
0.1 & 0.8 \\
\end{bmatrix} \\
\lambda _1 =1 \\
\lambda _2 =0.7 \\</script></li>
<li>对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)</li>
<li><p>得到我们要研究的公式</p>
<script type="math/tex; mode=display">
u_k=c_1*1^k*
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix}
+c_2*(0.7)^k*
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}</script></li>
<li>假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</li>
<li>行向量为和为1是另外一种定义马尔科夫矩阵的方式</li>
</ul>
<h2 id="傅里叶级数"><a href="#傅里叶级数" class="headerlink" title="傅里叶级数"></a>傅里叶级数</h2><ul>
<li>先讨论带有标准正交基的投影问题</li>
<li>假设$q_1….q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合</li>
<li><p>现在我们要求出线性组合系数$x_1….x_n$<br>$v=x_1q_1+x_2q_2+…x_nq_n$<br>一种方法是将$v$与$q_i$做内积，逐一求出系数</p>
<script type="math/tex; mode=display">
q_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\</script></li>
<li><p>写成矩阵形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
q_1 & q_2 & ... & q_n \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
... \\
x_n \\
\end{bmatrix}=
v \\
Qx=v \\
x=Q^{-1}v=Q^Tv \\</script></li>
<li>现在讨论傅里叶级数</li>
<li><p>我们希望将函数分解</p>
<script type="math/tex; mode=display">
f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......</script></li>
<li>关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</li>
<li>如何求出傅里叶系数？</li>
<li>利用之前的向量例子来求</li>
<li><p>将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\pi$，例如</p>
<script type="math/tex; mode=display">
\int _0 ^{2\pi} f(x)cosx dx=0+ a_1 \int _0^{2\pi}(cosx)^2dx+0+0...+0=\pi a_1 \\</script></li>
</ul>
<h1 id="第二十二讲：对称矩阵及其正定性"><a href="#第二十二讲：对称矩阵及其正定性" class="headerlink" title="第二十二讲：对称矩阵及其正定性"></a>第二十二讲：对称矩阵及其正定性</h1><h2 id="对称矩阵"><a href="#对称矩阵" class="headerlink" title="对称矩阵"></a>对称矩阵</h2><ul>
<li>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</li>
<li>对一般矩阵$A=S\Lambda S^{-1}$，S为特征向量矩阵</li>
<li>对对称矩阵$A=Q\Lambda Q^{-1}=Q\Lambda Q^T$，Q为标准正交的特征向量矩阵</li>
<li>为什么特征值都是实数？</li>
<li>$Ax=\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{<em>}=\lambda ^{</em>} x^{*}$</li>
<li>即$\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{<em> T}A^T=x^{</em> T} \lambda ^{* T} $</li>
<li>上式中$A=A^T$，且两边同乘以$x$，与$x^{<em> T}A\lambda x^{</em> T}x$对比可得$\lambda ^{*}=\lambda$，即特征值是实数</li>
<li>可见，对于复数矩阵，需要$A=A^{* T}$才满足对称</li>
<li><p>对于对称矩阵</p>
<script type="math/tex; mode=display">
A=Q\Lambda Q^{-1}=Q\Lambda Q^T \\
=\lambda _1 q_1 q_1^T+\lambda _2 q_2 q_2^T+.... \\</script></li>
<li>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</li>
<li>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</li>
</ul>
<h2 id="正定性"><a href="#正定性" class="headerlink" title="正定性"></a>正定性</h2><ul>
<li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li>
<li>特征值的符号与稳定性有关</li>
<li>主元、行列式、特征值三位一体，线性代数将其统一</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/linearalgebra/" rel="tag"># linearalgebra</a>
              <a href="/tags/math/" rel="tag"># math</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/01/21/LinearAlgebra2/" rel="prev" title="Note for Linear Algebra 2">
                  <i class="fa fa-angle-left"></i> Note for Linear Algebra 2
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2017/02/12/MachineLearningNote/" rel="next" title="Notes for ML">
                  Notes for ML <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">15:36</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2017/01/22/LinearAlgebra3/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
