<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Notes on some concepts and algorithms in machine learning, sourced from:  Elective Course on Pattern Recognition (An elective course for third-year students at Beijing University of Posts and Telecom">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes for ML">
<meta property="og:url" content="https://thinkwee.top/2017/02/12/MachineLearningNote/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Notes on some concepts and algorithms in machine learning, sourced from:  Elective Course on Pattern Recognition (An elective course for third-year students at Beijing University of Posts and Telecom">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/33690ecfca113fa9961fb6a6a328039c.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0H2cV.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0HRXT.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0opQO.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0o9yD.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0HRXT.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0opQO.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0o9yD.png">
<meta property="article:published_time" content="2017-02-12T14:40:38.000Z">
<meta property="article:modified_time" content="2025-07-16T10:43:15.335Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="code">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/33690ecfca113fa9961fb6a6a328039c.png">


<link rel="canonical" href="https://thinkwee.top/2017/02/12/MachineLearningNote/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2017/02/12/MachineLearningNote/","path":"2017/02/12/MachineLearningNote/","title":"Notes for ML"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Notes for ML | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">52</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction-to-statistical-learning-methods"><span class="nav-number">1.</span> <span class="nav-text">Introduction to
Statistical Learning Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#statistical-learning-supervised-learning-three-elements"><span class="nav-number">1.1.</span> <span class="nav-text">Statistical
Learning, Supervised Learning, Three Elements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-evaluation-model-selection"><span class="nav-number">1.2.</span> <span class="nav-text">Model evaluation, model
selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regularization-cross-validation"><span class="nav-number">1.3.</span> <span class="nav-text">Regularization,
cross-validation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generalization-ability"><span class="nav-number">1.4.</span> <span class="nav-text">Generalization ability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generative-model-discriminative-model"><span class="nav-number">1.5.</span> <span class="nav-text">Generative model,
discriminative model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#categorization-annotation-regression"><span class="nav-number">1.6.</span> <span class="nav-text">Categorization,
annotation, regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#k-nearest-neighbors-method"><span class="nav-number">2.</span> <span class="nav-text">k-Nearest Neighbors method</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k-nearest-neighbors-method-1"><span class="nav-number">2.1.</span> <span class="nav-text">k-Nearest Neighbors method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-nearest-neighbors-algorithm"><span class="nav-number">2.2.</span> <span class="nav-text">k-Nearest Neighbors
algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-nearest-neighbors-model"><span class="nav-number">2.3.</span> <span class="nav-text">k-Nearest Neighbors model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-d-tree"><span class="nav-number">2.4.</span> <span class="nav-text">k-d tree</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#support-vector-machine"><span class="nav-number">3.</span> <span class="nav-text">Support Vector Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#linearly-separable-support-vector-machine-and-hard-margin-maximization"><span class="nav-number">3.1.</span> <span class="nav-text">Linearly
separable support vector machine and hard margin maximization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#linear-algebra-basics"><span class="nav-number">4.</span> <span class="nav-text">Linear Algebra Basics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#moore-penrose"><span class="nav-number">4.1.</span> <span class="nav-text">Moore-penrose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%B9"><span class="nav-number">4.2.</span> <span class="nav-text">迹</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pca-explanation"><span class="nav-number">4.3.</span> <span class="nav-text">PCA Explanation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#probability-theory-and-information-theory"><span class="nav-number">5.</span> <span class="nav-text">Probability Theory
and Information Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-sigmoid"><span class="nav-number">5.1.</span> <span class="nav-text">Logistic Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kl-divergence-and-cross-entropy"><span class="nav-number">5.2.</span> <span class="nav-text">KL divergence and
cross-entropy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cross-entropy-and-maximum-log-likelihood-relationship"><span class="nav-number">5.3.</span> <span class="nav-text">Cross-entropy
and maximum log-likelihood relationship</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#computational-method"><span class="nav-number">6.</span> <span class="nav-text">Computational Method</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#gradient-descent"><span class="nav-number">6.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#newtons-method"><span class="nav-number">6.2.</span> <span class="nav-text">Newton&#39;s Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#constraint-optimization"><span class="nav-number">6.3.</span> <span class="nav-text">Constraint Optimization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#modify-algorithm"><span class="nav-number">7.</span> <span class="nav-text">Modify algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#modify-the-hypothesis-space"><span class="nav-number">7.1.</span> <span class="nav-text">Modify the hypothesis space</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regularization"><span class="nav-number">7.2.</span> <span class="nav-text">Regularization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA"><span class="nav-number">8.</span> <span class="nav-text">统计学习方法概论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="nav-number">8.1.</span> <span class="nav-text">统计学习，监督学习，三要素</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">8.2.</span> <span class="nav-text">模型评估，模型选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">8.3.</span> <span class="nav-text">正则化，交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="nav-number">8.4.</span> <span class="nav-text">泛化能力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.5.</span> <span class="nav-text">生成模型，判别模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%A0%87%E6%B3%A8%E5%9B%9E%E5%BD%92"><span class="nav-number">8.6.</span> <span class="nav-text">分类，标注，回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#k%E8%BF%91%E9%82%BB%E6%B3%95"><span class="nav-number">9.</span> <span class="nav-text">k近邻法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E8%BF%91%E9%82%BB%E6%B3%95-1"><span class="nav-number">9.1.</span> <span class="nav-text">k近邻法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="nav-number">9.2.</span> <span class="nav-text">k近邻算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E8%BF%91%E9%82%BB%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.3.</span> <span class="nav-text">k近邻模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kd%E6%A0%91"><span class="nav-number">9.4.</span> <span class="nav-text">kd树</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">10.</span> <span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-number">10.1.</span> <span class="nav-text">线性可分支持向量机与硬间隔最大化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E4%BB%A3%E5%9F%BA%E7%A1%80"><span class="nav-number">11.</span> <span class="nav-text">线代基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#moore-penrose"><span class="nav-number">11.1.</span> <span class="nav-text">Moore-penrose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%B9"><span class="nav-number">11.2.</span> <span class="nav-text">迹</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pca%E8%A7%A3%E9%87%8A"><span class="nav-number">11.3.</span> <span class="nav-text">PCA解释</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-number">12.</span> <span class="nav-text">概率论信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-sigmoid"><span class="nav-number">12.1.</span> <span class="nav-text">Logistic Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kl%E6%95%A3%E5%BA%A6%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="nav-number">12.2.</span> <span class="nav-text">KL散度和交叉熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8E%E6%9C%80%E5%A4%A7%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%85%B3%E7%B3%BB"><span class="nav-number">12.3.</span> <span class="nav-text">交叉熵与最大对数似然关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-number">13.</span> <span class="nav-text">计算方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">13.1.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-number">13.2.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96"><span class="nav-number">13.3.</span> <span class="nav-text">约束优化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E7%AE%97%E6%B3%95"><span class="nav-number">14.</span> <span class="nav-text">修改算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4"><span class="nav-number">14.1.</span> <span class="nav-text">修改假设空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">14.2.</span> <span class="nav-text">正则化</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2017/02/12/MachineLearningNote/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Notes for ML | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Notes for ML
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-02-12 22:40:38" itemprop="dateCreated datePublished" datetime="2017-02-12T22:40:38+08:00">2017-02-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 18:43:15" itemprop="dateModified" datetime="2025-07-16T18:43:15+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2017/02/12/MachineLearningNote/" class="post-meta-item leancloud_visitors" data-flag-title="Notes for ML" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>28k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>26 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/33690ecfca113fa9961fb6a6a328039c.png" width="500"/></p>
<p>Notes on some concepts and algorithms in machine learning, sourced
from:</p>
<ul>
<li>Elective Course on Pattern Recognition (An elective course for
third-year students at Beijing University of Posts and
Telecommunications, Pattern Recognition, textbook is "Pattern
Recognition" compiled by Zhang Xuegong, published by Tsinghua University
Press)</li>
<li>Watermelon Book</li>
<li>Statistical Learning Methods</li>
<li>Deep Learning (Translated in Chinese: <a
target="_blank" rel="noopener" href="https://github.com/exacity/deeplearningbook-chinese">exacity/deeplearningbook-chinese</a>)</li>
</ul>
<p>Update:</p>
<ul>
<li><p>2017-02-12 Overview Update</p></li>
<li><p>2017-03-01 Update k-Nearest Neighbors</p></li>
<li><p>2017-03-08 Update SVM</p></li>
<li><p>2018-01-04 Update of fundamental knowledge of machine learning
and mathematical knowledge in the book "Deep Learning"</p></li>
<li><p>2018-08-09 The content of Statistical Learning Methods has been
posted in another article titled "Handwritten Notes on Statistical
Learning Methods," and it is estimated that it will not be updated
anymore. Later, some remaining contents in "Deep Learning" may be
updated</p></li>
</ul>
<p><span id="more"></span></p>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0H2cV.png" alt="i0H2cV.png" />
<figcaption aria-hidden="true">i0H2cV.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="introduction-to-statistical-learning-methods">Introduction to
Statistical Learning Methods</h1>
<h2
id="statistical-learning-supervised-learning-three-elements">Statistical
Learning, Supervised Learning, Three Elements</h2>
<ul>
<li><p>If a system can improve its performance by executing a certain
process, that is learning</p></li>
<li><p>The methods of statistical learning are based on constructing
statistical models from data for the purpose of prediction and
analysis</p></li>
<li><p>Obtain the training dataset; determine the hypothesis space
containing all possible models; establish the criteria for model
selection; implement the algorithm for solving the optimal model; select
the optimal model through learning methods; use the optimal model for
predicting or analyzing new data</p></li>
<li><p>The task of supervised learning is to learn a model that can make
a good prediction for the corresponding output of any given
input</p></li>
<li><p>Each specific input is an instance, typically represented by a
feature vector. It constitutes a feature space, with each dimension
corresponding to a feature.</p></li>
<li><p>Supervised learning learns a model from a training dataset, which
consists of input-output pairs (samples)</p></li>
<li><p>Supervised learning learns a model from a training set,
represented as a conditional probability distribution or a decision
function</p></li>
<li><p>Statistical Learning Three Elements: Method = Model + Strategy +
Algorithm. The model includes probabilistic models (conditional
probability) and non-probabilistic models (decision functions); the
strategy refers to the method of selecting the optimal model,
introducing the concepts of loss function (cost function) and risk
function, and realizing the optimization of empirical risk or structural
risk; the algorithm refers to the specific computational method for
learning the model.</p></li>
<li><p>Loss function is used to measure the degree of error between the
predicted values and the true values, common ones include: 0-1 loss
function, squared loss function, absolute loss function, logarithmic
loss function, denoted as <span
class="math inline">\(L(Y,P(Y|X))\)</span> , risk function (expected
loss) is the average loss below the joint distribution of the model:
<span class="math display">\[
R_{exp}(f)=E_p[L(Y,f(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy
\]</span> , empirical risk (empirical loss) is the average loss of the
model about the training set: <span class="math display">\[
R_{emp}(f)=\frac 1N \sum_{i=1}^NL(y_i,f(x_i))
\]</span></p></li>
<li><p>Ideally, expected risk can be estimated using empirical risk,
however, when the sample size is small, minimizing empirical risk is
prone to overfitting, thus the concept of structural risk
(regularization) is proposed: <span class="math display">\[
R_{srm}(f)=\frac1N \sum_{i=1}^NL(y_i,f(x_i))+ \lambda J(f)
\]</span> , where J(f) represents the complexity of the model, and the
coefficient <span class="math inline">\(\lambda\)</span> is used to
weigh the empirical risk and model complexity. ML belongs to empirical
risk minimization, while MAP belongs to structural risk
minimization.</p></li>
</ul>
<h2 id="model-evaluation-model-selection">Model evaluation, model
selection</h2>
<ul>
<li>The error of the model on the training set and the test set is
respectively called training error and test error. The loss function
used may not be consistent, and making them consistent is ideal</li>
<li>Overfitting: Learning too much, the complexity of the model is
higher than the real model, performing well on the learned data but poor
in predicting unknown data. To avoid overfitting, the correct number of
features and the correct feature vectors are required.</li>
<li>Two methods for model selection: regularization and
cross-validation</li>
</ul>
<h2 id="regularization-cross-validation">Regularization,
cross-validation</h2>
<ul>
<li>Add a regularization term (penalty) to the empirical risk, with
higher penalties for more complex models</li>
<li>The general method is to randomly divide the dataset into three
parts: the training set, the validation set, and the test set, which are
used respectively for training data, selecting models, and finally
evaluating the learning method. Cross-validation involves repeatedly
randomly splitting the data into training and test sets, learning
multiple times, and conducting testing and model selection.</li>
<li>Cross-validation types: Simple cross-validation; S-fold
cross-validation; Leave-one-out cross-validation</li>
</ul>
<h2 id="generalization-ability">Generalization ability</h2>
<ul>
<li>Generalization error: Error in predicting unknown data</li>
<li>Generalization error upper bound is generally a function of the
sample size; as the sample size increases, the upper bound of
generalization tends to 0. This implies that the larger the hypothesis
space capacity, the harder the model is to learn, and the larger the
upper bound of the generalization error</li>
<li>For binary classification problems, the upper bound of
generalization error: where d is the capacity of the function set, for
any function, at least with probability <span
class="math inline">\(1-\delta\)</span></li>
</ul>
<p><span class="math display">\[
R_{exp}(f)\leq R_{emp}(f)+\varepsilon (d,N,\delta ) \\
\varepsilon (d,N,\delta )=\sqrt{\frac 1{2N}(log d+log \frac 1 \delta)}
\\
\]</span></p>
<h2 id="generative-model-discriminative-model">Generative model,
discriminative model</h2>
<ul>
<li>The generation method is based on learning the joint probability
distribution of data, and then calculating the conditional probability
distribution as the predictive model, i.e., the generative model, such
as the Naive Bayes method and the Hidden Markov Model</li>
<li>Discriminant methods learn decision functions or conditional
probability distributions directly from data as predictive models, i.e.,
discriminant models, such as k-nearest neighbors, perceptrons, decision
trees, logistic regression models, maximum entropy models, support
vector machines, boosting methods, and conditional random fields,
etc</li>
<li>The generation method can recover the joint probability
distribution, has a fast learning convergence speed, and is suitable for
cases with latent variables</li>
<li>High accuracy of discrimination methods, capable of abstracting
data, and simplifying learning problems</li>
</ul>
<h2 id="categorization-annotation-regression">Categorization,
annotation, regression</h2>
<ul>
<li><p>Categorization, which takes discrete finite values as output, is
also known as a classification decision function or classifier</p></li>
<li><p>For binary classification, the total number of four cases:
correctly predicted as correct TP; correctly predicted as incorrect FN;
incorrectly predicted as correct FP; incorrectly predicted as incorrect
TN</p>
<p><span class="math display">\[
Precision:P=\frac{TP}{TP+FP} \\
Recall:R=\frac{TP}{TP+FN} \\
F1 value:\frac {2}{F_1}=\frac1P+\frac1R \\
\]</span></p></li>
<li><p>Annotation: Input an observation sequence, output a marked
sequence</p></li>
<li><p>Regression: Function fitting, the commonly used loss function is
the squared loss function, which is fitted using the least squares
method</p></li>
</ul>
<h1 id="k-nearest-neighbors-method">k-Nearest Neighbors method</h1>
<h2 id="k-nearest-neighbors-method-1">k-Nearest Neighbors method</h2>
<ul>
<li>k-Nearest Neighbor method assumes that a training dataset is given,
with instances of predetermined categories. In classification, for new
instances, predictions are made based on the categories of the k nearest
training instances, through majority voting or other methods.</li>
<li>k-value selection, distance measurement, and classification decision
rules are the three elements of the k-nearest neighbor method.</li>
<li>k-Nearest Neighbor is a lazy learning method; it does not train the
samples.</li>
</ul>
<h2 id="k-nearest-neighbors-algorithm">k-Nearest Neighbors
algorithm</h2>
<ul>
<li><p>For new input instances, find the k nearest instances to the
instance in the training dataset, and if the majority of these k
instances belong to a certain class, classify the instance into that
class. That is:</p>
<p><span class="math display">\[
y=arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j), \  i=1,2,...,N; \
j=1,2,...,K
\]</span></p>
<p>When <span class="math inline">\(y_i=c_i\)</span> is <span
class="math inline">\(I=1\)</span> , the neighborhood <span
class="math inline">\(N_k(x)\)</span> consists of k nearest neighbor
points covering x.</p></li>
<li><p>When k=1, it is called the nearest neighbor algorithm</p></li>
<li><p>k-nearest neighbor algorithm does not have an explicit learning
process</p></li>
</ul>
<h2 id="k-nearest-neighbors-model">k-Nearest Neighbors model</h2>
<ul>
<li><p>k-nearest neighbor model refers to the partitioning of the
feature space.</p></li>
<li><p>In the feature space, for each training instance point, all
points closer to this point than others form a region called a unit.
Each training instance point has a unit, and the units of all training
instance points constitute a partition of the feature space, with the
class of each instance being the class label of all points within its
unit.</p></li>
<li><p>Distance metrics include: Euclidean distance, <span
class="math inline">\(L_p\)</span> distance, Minkowski
distance.</p></li>
<li><p>The Euclidean distance is a special case of the <span
class="math inline">\(L_p\)</span> distance (p=2), and when p=1, it
becomes the Manhattan distance, defined as:</p>
<p><span class="math display">\[
L_p(x_i,x_j)=(\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac1p}
\]</span></p></li>
<li><p>k value is small, approximation error is small, estimation error
is large, the overall model is complex, prone to overfitting. k value is
large, estimation error is small, approximation error is large, the
model is simple</p></li>
<li><p>Generally, the cross-validation method is used to determine the
value of k</p></li>
<li><p>The majority voting rule is equivalent to empirical risk
minimization</p></li>
</ul>
<h2 id="k-d-tree">k-d tree</h2>
<ul>
<li>kd-tree is a special structure used to store training data, which
improves the efficiency of k-nearest neighbor search, which is
essentially a binary search tree</li>
<li>Each node of the k-d tree corresponds to a k-dimensional
hyperrectangle region</li>
<li>Method for constructing a balanced kd-tree: Divide by taking the
median of each dimension sequentially, for example, in three dimensions,
first draw a line at the median of the x-axis, divide it into two parts,
then draw a line at the median of the y-axis for each part, and then
along the z-axis, and then cycle through x, y, z.</li>
<li>After the construction of the kd-tree is completed, it can be used
for k-nearest neighbor searches. The following uses the nearest neighbor
search as an example, where k=1:
<ul>
<li>Starting from the root node, recursively search downwards to the
area where the target point is located, until reaching the leaf
nodes</li>
<li>Taking this leaf node as the current nearest point, the distance
from the current nearest point to the target point is the current
nearest distance, and our goal is to search the tree to find a suitable
node to update the current nearest point and the current nearest
distance</li>
<li>Recursive upward rollback, perform the following operation on each
node
<ul>
<li>If the instance point saved by the node is closer to the target
point than the current nearest point, update the instance point to the
current nearest point, and update the distance from the instance point
to the target point to the current nearest distance</li>
<li>The child nodes of this node are divided into two, one of which
contains our target point. This part we have come from the target point
all the way up recursively, and it has been updated. Therefore, we need
to find the other child node of this node to see if it can be updated:
Check if the corresponding area of the other child node intersects with
the hypersphere centered at the target point and with the current
shortest distance as the radius. If they intersect, move to this child
node and continue the upward search; if they do not intersect, do not
move and continue the upward search.</li>
<li>Until the root node is searched, the current nearest point at this
time is the nearest neighbor of the target point.</li>
</ul></li>
</ul></li>
<li>The computational complexity of k-d tree search is <span
class="math inline">\(O(logN)\)</span> , suitable for k-nearest neighbor
search when the number of training instances is much larger than the
number of spatial dimensions</li>
</ul>
<h1 id="support-vector-machine">Support Vector Machine</h1>
<h2
id="linearly-separable-support-vector-machine-and-hard-margin-maximization">Linearly
separable support vector machine and hard margin maximization</h2>
<ul>
<li><p>The goal of learning is to find a separating hyperplane in the
feature space that can distribute instances into different classes
(binary classification). The separating hyperplane corresponds to the
equation <span class="math inline">\(wx+b=0\)</span> , determined by the
normal vector w and the intercept b, and can be represented by (w,
b)</p></li>
<li><p>Here, x is the feature vector <span
class="math inline">\((x_1,x_2,...)\)</span> , and y is the label of the
feature vector</p></li>
<li><p>Given a linearly separable training dataset, the separating
hyperplane learned by maximizing the margin or solving the equivalent
convex quadratic programming problem, denoted as <span
class="math inline">\(wx+b=0\)</span> , and the corresponding
classification decision function denoted as <span
class="math inline">\(f(x)=sign(wx+b)\)</span> , is called a linearly
separable support vector machine</p></li>
<li><p>Under the determination of the hyperplane <span
class="math inline">\(wx+b=0\)</span> , the distance from the point
(x,y) to the hyperplane can be</p>
<p><span class="math display">\[
\gamma _i=\frac{w}{||w||}x_i+\frac{b}{||w||}
\]</span></p></li>
<li><p>Whether the symbol of <span class="math inline">\(wx+b\)</span>
is consistent with the symbol of class label y indicates whether the
classification is correct, <span class="math inline">\(y=\pm 1\)</span>
, thus obtaining the geometric distance</p>
<p><span class="math display">\[
\gamma _i=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \\
Define the geometric interval of the hyperplane (w,b) with respect to
the training data set T as the minimum geometric interval of all sample
points \\
\gamma=min\gamma _i \\
\]</span></p></li>
<li><p>Simultaneously define the relative distance as the function
interval</p>
<p><span class="math display">\[
\gamma _i=y_i(wx_i+b) \\
\gamma =min\gamma _i \\
\]</span></p></li>
<li><p>Hard margin maximization is for linearly separable hyperplanes,
while soft margin maximization is for approximately linearly separable
cases</p></li>
<li><p>Finding a hyperplane with the maximum geometric margin, which can
be represented as the following constrained optimization problem</p>
<p><span class="math display">\[
max_{(w,b)} \gamma \\
s.t. \quad y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq \gamma
,i=1,2,...,N \\
\]</span></p></li>
<li><p>We can convert geometric intervals into functional intervals,
which has no effect on optimization. Moreover, if we fix the relative
interval as a constant (1), the maximization of the geometric interval
can be transformed into the minimization of <span
class="math inline">\(||w||\)</span> , thus the constrained optimization
problem can be rewritten as</p>
<p><span class="math display">\[
min_{(w,b)} \frac12 {||w||}^2 \\
s.t. \quad y_i(wx_i+b)-1 \geq 0 \\
\]</span></p></li>
<li><p>The above equation is the basic form of SVM, and when the above
optimization problem is solved, we obtain a separating hyperplane with
the maximum margin, which is the maximum margin method</p></li>
<li><p>The maximum margin separating hyperplane exists and is unique,
proof omitted</p></li>
<li><p>In the linearly separable case, the instances of the nearest
sample points to the separating hyperplane in the training dataset are
called support vectors</p></li>
<li><p>For the positive example points of <span
class="math inline">\(y_i=1\)</span> , the support vectors lie on the
plane <span class="math inline">\(wx+b=1\)</span> , similarly, the
negative example points lie on the plane <span
class="math inline">\(wx+b=-1\)</span> . These two planes are parallel
and there are no training data points between them. The distance between
the two planes is the margin, which depends on the normal vector w of
the separating hyperplane, as <span class="math inline">\(\frac 2
{||w||}\)</span></p></li>
<li><p>Thus, it can be seen that support vector machines are determined
by a very small number of important training samples (support
vectors)</p></li>
<li><p>To solve the optimization problem, we introduce the dual
algorithm and also introduce kernel functions to generalize to nonlinear
classification problems</p></li>
<li><p>To be supplemented</p></li>
</ul>
<h1 id="linear-algebra-basics">Linear Algebra Basics</h1>
<h2 id="moore-penrose">Moore-penrose</h2>
<ul>
<li><p>For non-square matrices, their inverse matrix is undefined,
therefore we specially define the pseudoinverse of non-square matrices:
Moore-Penrose pseudoinverse <span class="math display">\[
A^+=lim_{\alpha \rightarrow 0}(A^TA+\alpha I)^{-1}A^T
\]</span></p></li>
<li><p>The actual algorithms for computing the pseudo-inverse do not
base themselves on this definition, but rather use the following
formula:</p>
<p><span class="math display">\[
A^+=VD^+U^T
\]</span></p>
<p>U, D, and V are the matrices obtained from the singular value
decomposition of matrix A, which are diagonal matrices. The
pseudo-inverse <span class="math inline">\(D^+\)</span> of the diagonal
matrix D is obtained by taking the reciprocal of its non-zero elements
and then transposing.</p></li>
<li><p>When the number of columns of matrix A exceeds the number of
rows, using the pseudoinverse to solve the linear equation is one of
many possible methods. Particularly, <span class="math inline">\(x =
A^+y\)</span> is the one with the smallest Euclidean norm among all
feasible solutions to the equation.</p></li>
<li><p>When the number of rows of matrix A exceeds the number of
columns, there may be no solution. In this case, the pseudo-inverse
<span class="math inline">\(x\)</span> obtained minimizes the Euclidean
distance between <span class="math inline">\(Ax\)</span> and <span
class="math inline">\(y\)</span> .</p></li>
<li><p>To be supplemented</p></li>
</ul>
<h2 id="迹">迹</h2>
<ul>
<li><p>Trace operation returns the sum of the diagonal elements of the
matrix.</p></li>
<li><p>Using trace operations, the matrix Frobenius norm can be
described as:</p>
<p><span class="math display">\[
||A_F||=\sqrt{Tr(AA^T)}
\]</span></p></li>
<li><p>Trace has transposition invariance and permutation
invariance</p></li>
<li><p>The trace of a scalar is its own</p></li>
</ul>
<h2 id="pca-explanation">PCA Explanation</h2>
<ul>
<li>To be supplemented</li>
</ul>
<h1 id="probability-theory-and-information-theory">Probability Theory
and Information Theory</h1>
<h2 id="logistic-sigmoid">Logistic Sigmoid</h2>
<ul>
<li><p>Logistic and sigmoid are often used interchangeably, this
function is used to compress real numbers into the interval (0,1),
representing binary classification probabilities:</p>
<p><span class="math display">\[
\sigma (x) = \frac{1}{1+exp(-x)}
\]</span></p></li>
<li><p>Softmax is an extension of the sigmoid, being a smoothed version
of the argmax function (argmax returns a one-hot vector while softmax
returns probabilities for various possible outcomes), extending binary
classification to the multi-class (disjoint) case:</p>
<p><span class="math display">\[
\sigma (z)_j = \frac{e^z j}{\sum _{k=1}^K e^z k}
\]</span></p></li>
<li><p>Both exhibit saturation phenomena when the input is too large or
too small, but when the two functions are introduced as nonlinear
activation units in a neural network, because the cost function takes
the negative logarithm, this saturation phenomenon can be
eliminated.</p></li>
<li><p>Softmax function, due to the inclusion of exponential functions,
also has issues with underflow and overflow. When the input is uniformly
distributed and the number of input samples is large, the denominator
exponential values approach 0, and the summation may also approach 0,
leading to underflow in the denominator. Overflow can also occur when
the parameters of the exponential function are large. The solution is to
process the input x as z = x - max(xi), that is, subtracting the maximum
component from each component of the vector. Adding or subtracting a
scalar from the input vector does not change the softmax function values
(redundancy of the softmax function), but at this point, the maximum
value of the processed input is 0, excluding overflow. After the
exponential function, at least one term in the summation of the
denominator exists as 1, excluding underflow.</p></li>
<li><p>Utilizing the redundancy of the softmax function can also deduce
that sigmoid is a special case of softmax: <img data-src="https://s1.ax1x.com/2018/10/20/i0HRXT.png"
alt="i0HRXT.png" /></p></li>
</ul>
<h2 id="kl-divergence-and-cross-entropy">KL divergence and
cross-entropy</h2>
<ul>
<li><p>KL divergence: A measure of the difference between two
distributions, PQ, non-negative and asymmetric:</p>
<p><span class="math display">\[
D_{KL}(P||Q) = E_{x \sim P} [log \frac{P(x)}{Q(x)}] = E_{x \sim P} [log
P(x) - log Q(x)]
\]</span></p></li>
<li><p>Cross-entropy:</p>
<p><span class="math display">\[
H(P,Q) = -E_{x \sim P} log Q(x)
\]</span></p></li>
<li><p>The cross-entropy form is simple, and the minimization of KL
divergence with respect to Q (actual output) is unrelated to the first
term in the divergence formula, therefore, minimizing KL divergence can
actually be seen as minimizing cross-entropy. Moreover, since KL
divergence represents the difference between PQ (actual output and
correct output), it can be regarded as a loss function</p></li>
<li><p>In dealing with binary classification problems using logistic
regression, q(x) refers to the logistic function, and p(x) refers to the
actual distribution of the data (either 0 or 1)</p></li>
<li><p>The expected self-information of q with respect to p, i.e., the
binary cross-entropy (Logistic cost function):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m y^{(i)} log h_{\theta} (x^{(i)}) +
(1-y^{(i)}) log (1-h_{\theta}(x^{(i)}))]
\]</span></p></li>
<li><p>Similarly, we can obtain the multi-class cross-entropy (Softmax
cost function):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m \sum _{j=1}^k 1\{ y^{(i)}=j \} log
\frac {e^{\theta _j ^T x^{(i)}}} {\sum _{l=1}^k e^{\theta _j ^T
x^{(i)}}}]
\]</span></p></li>
</ul>
<h2
id="cross-entropy-and-maximum-log-likelihood-relationship">Cross-entropy
and maximum log-likelihood relationship</h2>
<ul>
<li><p>Given a sample dataset X with distribution <span
class="math inline">\(P_{data}(x)\)</span> , we aim to obtain a model
<span class="math inline">\(P_{model}(x,\theta)\)</span> whose
distribution is as close as possible to <span
class="math inline">\(P_{data}(x)\)</span> . <span
class="math inline">\(P_{model}(x,\theta)\)</span> maps any x to a real
number to estimate the true probability <span
class="math inline">\(P_{data}(x)\)</span> . In <span
class="math inline">\(P_{model}(x,\theta)\)</span> , the maximum
likelihood estimate of <span class="math inline">\(\theta\)</span> is
the <span class="math inline">\(\theta\)</span> that maximizes the
product of probabilities obtained by the model for the sample data:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} p_{model} (X;\theta)
\]</span></p></li>
<li><p>Because taking the logarithm and scaling transformation does not
change argmax, taking the logarithm transforms it into a summation and
then dividing by the sample size to average it yields:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} E_{x \sim p_{data}} log
p_{model}(x;\theta)
\]</span></p></li>
<li><p>It can be observed that the above expression is the negative of
the cross-entropy, and its value is maximized when Pdata(x) =
Pmodel(x,θ), so:</p></li>
<li><p>Maximum likelihood = minimum negative log-likelihood = minimizing
cross-entropy = minimizing KL divergence = minimizing the gap between
data and model = minimizing the cost function</p></li>
<li><p>Maximum likelihood estimation can be extended to maximum
conditional likelihood estimation, which constitutes the foundation of
most supervised learning: Formula:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} \sum_{i=1}^m log
P(y^{(i)} | x^{(i)} ; \theta)
\]</span></p></li>
<li><p>Maximum likelihood estimation is consistent.</p></li>
</ul>
<h1 id="computational-method">Computational Method</h1>
<h2 id="gradient-descent">Gradient Descent</h2>
<ul>
<li>How to transform parameters (inputs) to make the function smaller
(minimize the cost function)?</li>
<li>The principle is that moving the input in the opposite direction of
the derivative by a small step can reduce the function's output.</li>
<li>Extend the input to vector-form parameters, treat the function as a
cost function, and thus obtain a gradient-based optimization
algorithm.</li>
<li>First-order optimization algorithm: including gradient descent,
using the Jacobian matrix (including the relationship between partial
derivatives of vectors), and updating the suggested parameter updates
through gradient descent: <img data-src="https://s1.ax1x.com/2018/10/20/i0opQO.png" alt="i0opQO.png" /></li>
</ul>
<h2 id="newtons-method">Newton's Method</h2>
<ul>
<li>Second-order optimization algorithm (求最优补偿，定性临界点):
First-order optimization requires adjusting the appropriate learning
rate (step size), otherwise it cannot reach the optimal point or will
produce shaking, and it cannot update the parameters at the critical
point (gradient is 0), which reflects the need for second-order
derivative information of the cost function, for example, when the
function is convex or concave, the predicted value based on the gradient
and the true value of the cost function have a deviation. The Hessian
matrix contains second-order information. Newton's method uses the
information of the Hessian matrix, uses the second-order Taylor
expansion to obtain the function information, and updates the parameters
using the following formula: <img data-src="https://s1.ax1x.com/2018/10/20/i0o9yD.png"
alt="i0o9yD.png" /> </li>
</ul>
<h2 id="constraint-optimization">Constraint Optimization</h2>
<ul>
<li><p>Only contains equality constraint conditions: Lagrange</p></li>
<li><p>Inequality constraint conditions: KTT</p></li>
</ul>
<h1 id="modify-algorithm">Modify algorithm</h1>
<h2 id="modify-the-hypothesis-space">Modify the hypothesis space</h2>
<ul>
<li><p>Machine learning algorithms should avoid overfitting and
underfitting, which can be addressed by adjusting the model capacity
(the ability to fit various functions).</p></li>
<li><p>Adjusting the model capacity involves selecting an appropriate
hypothesis space (assuming input rather than parameters), for example,
previously only fitting polynomial linear functions:</p>
<p><span class="math display">\[
y = b + wx
\]</span></p></li>
<li><p>If nonlinear units, such as higher-order terms, are introduced,
the output remains linearly distributed relative to the parameters:</p>
<p><span class="math display">\[
y= b + w_1 x + w_2 x^2
\]</span></p>
<p>This increases the model's capacity while simplifying the generated
parameters, making it suitable for solving complex problems; however, a
too high capacity may also lead to overfitting.</p></li>
</ul>
<h2 id="regularization">Regularization</h2>
<ul>
<li><p>No free lunch theorem (after averaging over all possible data
generation distributions, each classification algorithm has the same
error rate on points that have not been observed beforehand) indicates
that machine learning algorithms should be designed for specific tasks,
and algorithms should have preferences. Adding regularization to the
cost function introduces preferences, causing the learned parameters to
be biased towards minimizing the regularization term.</p></li>
<li><p>An example is weight decay; the cost function with the addition
of the weight decay regularization term is:</p>
<p><span class="math display">\[
J(w) = MSE_{train} + \lambda w^T w
\]</span></p>
<p>λ controls the preference degree, and the generated models tend to
have small parameters, which can avoid overfitting.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="统计学习方法概论"><font size=5 >统计学习方法概论</font></h1>
<h2
id="统计学习监督学习三要素"><font size=4 >统计学习，监督学习，三要素</font></h2>
<ul>
<li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li>
<li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li>
<li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li>
<li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li>
<li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li>
<li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li>
<li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li>
<li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li>
<li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为<span
class="math inline">\(L(Y,P(Y|X))\)</span>,风险函数(期望损失)是模型在联合分布的平均以下的损失：<span
class="math display">\[R_{exp}(f)=E_p[L(Y,f(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy\]</span>经验风险(经验损失)是模型关于训练集的平均损失:<span
class="math display">\[R_{emp}(f)=\frac 1N
\sum_{i=1}^NL(y_i,f(x_i))\]</span></li>
<li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：<span
class="math display">\[R_{srm}(f)=\frac1N \sum_{i=1}^NL(y_i,f(x_i))+
\lambda J(f)\]</span>,其中J(f)为模型的复杂性，系数<span
class="math inline">\(\lambda\)</span>用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li>
</ul>
<h2 id="模型评估模型选择"><font size=4 >模型评估，模型选择</font></h2>
<ul>
<li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li>
<li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li>
<li>模型选择的两种方法：正则化和交叉验证</li>
</ul>
<h2 id="正则化交叉验证"><font size=4 >正则化，交叉验证</font></h2>
<ul>
<li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li>
<li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li>
<li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li>
</ul>
<h2 id="泛化能力"><font size=4 >泛化能力</font></h2>
<ul>
<li>泛化误差:对未知数据预测的误差</li>
<li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li>
<li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率<span
class="math inline">\(1-\delta\)</span></li>
</ul>
<p><span class="math display">\[
R_{exp}(f)\leq R_{emp}(f)+\varepsilon (d,N,\delta ) \\
\varepsilon (d,N,\delta )=\sqrt{\frac 1{2N}(log d+log \frac 1 \delta)}
\\
\]</span></p>
<h2 id="生成模型判别模型"><font size=4 >生成模型，判别模型</font></h2>
<ul>
<li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li>
<li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li>
<li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li>
<li>判别方法准确率高，可以抽象数据，简化学习问题</li>
</ul>
<h2 id="分类标注回归"><font size=4 >分类，标注，回归</font></h2>
<ul>
<li><p>分类，即输出取离散有限值，分类决策函数也叫分类器</p></li>
<li><p>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN</p>
<p><span class="math display">\[
精确率:P=\frac{TP}{TP+FP} \\
召回率:R=\frac{TP}{TP+FN} \\
1F值:\frac {2}{F_1}=\frac1P+\frac1R \\
\]</span></p></li>
<li><p>标注:输入一个观测序列，输出一个标记序列</p></li>
<li><p>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</p></li>
</ul>
<h1 id="k近邻法"><font size=5 >k近邻法</font></h1>
<h2 id="k近邻法-1"><font size=4 >k近邻法</font></h2>
<ul>
<li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li>
<li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li>
<li>k近邻法是一种懒惰学习，他不对样本进行训练。</li>
</ul>
<h2 id="k近邻算法"><font size=4 >k近邻算法</font></h2>
<ul>
<li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:</p>
<p><span class="math display">\[
y=arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j), \  i=1,2,...,N; \
j=1,2,...,K
\]</span></p>
<p>其中<span class="math inline">\(y_i=c_i\)</span>时<span
class="math inline">\(I=1\)</span>，<span
class="math inline">\(N_k(x)\)</span>是覆盖x的k个近邻点的邻域。</p></li>
<li><p>k=1时称为最近邻算法</p></li>
<li><p>k近邻算法没有显式的学习过程</p></li>
</ul>
<h2 id="k近邻模型"><font size=4 >k近邻模型</font></h2>
<ul>
<li><p>k近邻模型即对特征空间的划分。</p></li>
<li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p></li>
<li><p>距离度量包括：欧氏距离，<span
class="math inline">\(L_p\)</span>距离，Minkowski距离。</p></li>
<li><p>欧氏距离是<span
class="math inline">\(L_p\)</span>距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：</p>
<p><span class="math display">\[
L_p(x_i,x_j)=(\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac1p}
\]</span></p></li>
<li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p></li>
<li><p>一般采用交叉验证法确定k值</p></li>
<li><p>多数表决规则等价于经验风险最小化</p></li>
</ul>
<h2 id="kd树"><font size=4 >kd树</font></h2>
<ul>
<li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li>
<li>kd树的每一个节点对应于一个k维超矩形区域</li>
<li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li>
<li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：
<ul>
<li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li>
<li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li>
<li>递归向上回退，对每个节点做如下操作
<ul>
<li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li>
<li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li>
<li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li>
</ul></li>
</ul></li>
<li>kd树搜索的计算复杂度是<span
class="math inline">\(O(logN)\)</span>，适用于训练实例数远大于空间维数的k近邻搜索</li>
</ul>
<h1 id="支持向量机"><font size=5 >支持向量机</font></h1>
<h2
id="线性可分支持向量机与硬间隔最大化"><font size=4 >线性可分支持向量机与硬间隔最大化</font></h2>
<ul>
<li><p>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程<span
class="math inline">\(wx+b=0\)</span>，由法向量w和截距b决定，可由(w,b)表示</p></li>
<li><p>这里的x是特征向量<span
class="math inline">\((x_1,x_2,...)\)</span>，而y是特征向量的标签</p></li>
<li><p>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为<span
class="math inline">\(wx+b=0\)</span>，以及相应的分类决策函数<span
class="math inline">\(f(x)=sign(wx+b)\)</span>称为线性可分支持向量机</p></li>
<li><p>在超平面<span
class="math inline">\(wx+b=0\)</span>确定的情况下，点(x,y)到超平面的距离可以为</p>
<p><span class="math display">\[
\gamma _i=\frac{w}{||w||}x_i+\frac{b}{||w||}
\]</span></p></li>
<li><p>而<span
class="math inline">\(wx+b\)</span>的符号与类标记y的符号是否一致可以表示分类是否正确，<span
class="math inline">\(y=\pm 1\)</span>，这样就可以得到几何间隔</p>
<p><span class="math display">\[
\gamma _i=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \\
定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值
\\
\gamma=min\gamma _i \\
\]</span></p></li>
<li><p>同时定义相对距离为函数间隔</p>
<p><span class="math display">\[
\gamma _i=y_i(wx_i+b) \\
\gamma =min\gamma _i \\
\]</span></p></li>
<li><p>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</p></li>
<li><p>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题</p>
<p><span class="math display">\[
max_{(w,b)} \gamma \\
s.t. \quad y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq \gamma
,i=1,2,...,N \\
\]</span></p></li>
<li><p>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对<span
class="math inline">\(||w||\)</span>的最小化，因此约束最优化问题可以改写为</p>
<p><span class="math display">\[
min_{(w,b)} \frac12 {||w||}^2 \\
s.t. \quad y_i(wx_i+b)-1 \geq 0 \\
\]</span></p></li>
<li><p>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</p></li>
<li><p>最大间隔分离超平面存在且唯一，证明略</p></li>
<li><p>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</p></li>
<li><p>对<span
class="math inline">\(y_i=1\)</span>的正例点，支持向量在平面<span
class="math inline">\(wx+b=1\)</span>上，同理负例点在平面<span
class="math inline">\(wx+b=-1\)</span>上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为<span
class="math inline">\(\frac 2 {||w||}\)</span></p></li>
<li><p>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</p></li>
<li><p>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</p></li>
<li><p>待补充</p></li>
</ul>
<h1 id="线代基础"><font size=5 >线代基础</font></h1>
<h2 id="moore-penrose"><font size=4 >Moore-penrose</font></h2>
<ul>
<li><p>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose
伪逆 <span class="math display">\[
A^+=lim_{\alpha \rightarrow 0}(A^TA+\alpha I)^{-1}A^T
\]</span></p></li>
<li><p>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：</p>
<p><span class="math display">\[
A^+=VD^+U^T
\]</span></p>
<p>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D
的伪逆<span
class="math inline">\(D^+\)</span>是其非零元素取倒数之后再转置得到的。</p></li>
<li><p>当矩阵 A
的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，<span
class="math inline">\(x =
A^+y\)</span>是方程所有可行解中欧几里得范数最小的一个。</p></li>
<li><p>当矩阵 A
的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的<span
class="math inline">\(x\)</span>使得<span
class="math inline">\(Ax\)</span>和<span
class="math inline">\(y\)</span>的欧几里得距离最小。</p></li>
<li><p>待补充</p></li>
</ul>
<h2 id="迹"><font size=4 >迹</font></h2>
<ul>
<li><p>迹运算返回的是矩阵对角元素的和.</p></li>
<li><p>使用迹运算可以描述矩阵Frobenius范数的方式：</p>
<p><span class="math display">\[
||A_F||=\sqrt{Tr(AA^T)}
\]</span></p></li>
<li><p>迹具有转置不变性和轮换不变性</p></li>
<li><p>标量的迹是其本身</p></li>
</ul>
<h2 id="pca解释"><font size=4 >PCA解释</font></h2>
<ul>
<li>待补充</li>
</ul>
<h1 id="概率论信息论"><font size=5 >概率论信息论</font></h1>
<h2 id="logistic-sigmoid"><font size=4 >Logistic Sigmoid</font></h2>
<ul>
<li><p>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：</p>
<p><span class="math display">\[
\sigma (x) = \frac{1}{1+exp(-x)}
\]</span></p></li>
<li><p>Softmax
是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot
向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：</p>
<p><span class="math display">\[
\sigma (z)_j = \frac{e^z j}{\sum _{k=1}^K e^z k}
\]</span></p></li>
<li><p>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。</p></li>
<li><p>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</p></li>
<li><p>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：
<img data-src="https://s1.ax1x.com/2018/10/20/i0HRXT.png"
alt="i0HRXT.png" /></p></li>
</ul>
<h2 id="kl散度和交叉熵"><font size=4 >KL散度和交叉熵</font></h2>
<ul>
<li><p>KL散度：用以衡量PQ两个分布之间的差异，非负且不对称：</p>
<p><span class="math display">\[
D_{KL}(P||Q) = E_{x \sim P} [log \frac{P(x)}{Q(x)}] = E_{x \sim P} [log
P(x) - log Q(x)]
\]</span></p></li>
<li><p>交叉熵：</p>
<p><span class="math display">\[
H(P,Q) = -E_{x \sim P} log Q(x)
\]</span></p></li>
<li><p>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</p></li>
<li><p>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</p></li>
<li><p>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m y^{(i)} log h_{\theta} (x^{(i)}) +
(1-y^{(i)}) log (1-h_{\theta}(x^{(i)}))]
\]</span></p></li>
<li><p>同理可得多元交叉熵(Softmaxs代价函数):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m \sum _{j=1}^k 1\{ y^{(i)}=j \} log
\frac {e^{\theta _j ^T x^{(i)}}} {\sum _{l=1}^k e^{\theta _j ^T
x^{(i)}}}]
\]</span></p></li>
</ul>
<h2
id="交叉熵与最大对数似然关系"><font size=4 >交叉熵与最大对数似然关系</font></h2>
<ul>
<li><p>已知一个样本数据集X，分布为<span
class="math inline">\(P_{data}(x)\)</span>，我们希望得到一个模型<span
class="math inline">\(P_{model}(x,\theta)\)</span>，其分布尽可能接近<span
class="math inline">\(P_{data}(x)\)</span>。<span
class="math inline">\(P_{model}(x,\theta)\)</span>将任意x映射为实数来估计真实概率<span
class="math inline">\(P_{data}(x)\)</span>。 在<span
class="math inline">\(P_{model}(x,\theta)\)</span>中，对<span
class="math inline">\(\theta\)</span>的最大似然估计为使样本数据通过模型得到概率之积最大的<span
class="math inline">\(\theta\)</span>：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} p_{model} (X;\theta)
\]</span></p></li>
<li><p>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} E_{x \sim p_{data}} log
p_{model}(x;\theta)
\]</span></p></li>
<li><p>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</p></li>
<li><p>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</p></li>
<li><p>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} \sum_{i=1}^m log
P(y^{(i)} | x^{(i)} ; \theta)
\]</span></p></li>
<li><p>最大似然估计具有一致性。</p></li>
</ul>
<h1 id="计算方法"><font size=5 >计算方法</font></h1>
<h2 id="梯度下降"><font size=4 >梯度下降</font></h2>
<ul>
<li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li>
<li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li>
<li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li>
<li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：
<img data-src="https://s1.ax1x.com/2018/10/20/i0opQO.png"
alt="i0opQO.png" /></li>
</ul>
<h2 id="牛顿法"><font size=4 >牛顿法</font></h2>
<ul>
<li><p>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：
<img data-src="https://s1.ax1x.com/2018/10/20/i0o9yD.png"
alt="i0o9yD.png" /></p>
<h2 id="约束优化"><font size=4 >约束优化</font></h2></li>
<li><p>只包含等式约束条件：Lagrange</p></li>
<li><p>包含不等式约束条件：KTT</p></li>
</ul>
<h1 id="修改算法"><font size=5 >修改算法</font></h1>
<h2 id="修改假设空间"><font size=4 >修改假设空间</font></h2>
<ul>
<li><p>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</p></li>
<li><p>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：</p>
<p><span class="math display">\[
y = b + wx
\]</span></p></li>
<li><p>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：</p>
<p><span class="math display">\[
y= b + w_1 x + w_2 x^2
\]</span></p>
<p>此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</p></li>
</ul>
<h2 id="正则化"><font size=4 >正则化</font></h2>
<ul>
<li><p>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</p></li>
<li><p>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：</p>
<p><span class="math display">\[
J(w) = MSE_{train} + \lambda w^T w
\]</span></p>
<p>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/code/" rel="tag"># code</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/01/22/LinearAlgebra3/" rel="prev" title="Note for Linear Algebra 3">
                  <i class="fa fa-angle-left"></i> Note for Linear Algebra 3
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2017/03/09/dachuang/" rel="next" title="Notes for my Android app - Melodia">
                  Notes for my Android app - Melodia <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:58</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2017/02/12/MachineLearningNote/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
