<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Paper Reading 4</title>
    <url>/2019/12/16/201912/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<p>Paper reading on</p>
<ul>
<li>GNN Pooling</li>
<li>Discourse-Aware Summarization</li>
<li>Siamese BERT</li>
<li>Large Chatbot</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="edge-contraction-pooling-for-graph-neural-networks">Edge
Contraction Pooling for Graph Neural Networks</h1>
<ul>
<li><p>A new GNN pooling method that considers edges</p></li>
<li><p>Significance of pooling in GNNs:</p>
<ul>
<li>Identify clusters based on features or structure</li>
<li>Reduce computational complexity</li>
</ul></li>
<li><p>The authors' edgepool method can improve graph classification and
node classification performance.</p></li>
<li><p>There are two types of pooling: fixed and learned. The authors
briefly introduce three learned pooling methods:</p>
<ul>
<li><p>DiffPool: DiffPool learns a probability allocation, using one GNN
to learn embedding and another to learn cluster assignment, treating the
cluster assignment as a soft assign matrix <span
class="math inline">\(S\)</span>. Nodes are assigned to clusters based
on node features, with a predetermined number of clusters. Each layer
pools both embedding and adjacency matrix simultaneously, as
follows:</p>
<p><span class="math display">\[
\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1}
\times d}} \\
{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times
n_{l+1}}}\end{array} \\
\]</span></p>
<p>Problems include: fixed cluster number; assignment based solely on
node features without considering node distances; cluster assignment
matrix linearly related to node count, difficult to scale; challenging
to train</p></li>
<li><p>TopKPool: A straightforward approach that learns a projection
vector, projecting each node's features to a single weighted value,
retaining the top-k nodes. Issues include inability to modify the graph
(add nodes) and potential information loss due to hard
assignment</p></li>
<li><p>SAGPool: An improvement on TopK, using attention-weighted
neighborhood nodes before projection, but still maintaining a hard topk
assignment</p></li>
</ul></li>
<li><p>The edge pooling concept reduces sampling through edge
contraction. Given an edge e with nodes <span
class="math inline">\(v_i\)</span> and <span
class="math inline">\(v_j\)</span>, edge contraction means connecting
all adjacent nodes of i and j to a new node <span
class="math inline">\(v_e\)</span>. This operation can be repeated
multiple times, similar to expanding receptive field in CNNs.</p></li>
<li><p>How to select edges?</p>
<ul>
<li><p>First, calculate edge scores by concatenating and linearly
transforming the embeddings of connected nodes</p>
<p><span class="math display">\[
r(e_{ij}) = W (n_i || n_j) + b
\]</span></p></li>
<li><p>Then normalize all scores using softmax, with the author adding
0.5 to ensure a mean of 1, explained as improving numerical stability
and gradient propagation</p>
<p><span class="math display">\[
s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
\]</span></p></li>
<li><p>Begin contracting edges based on scores, avoiding contraction of
already contracted edge nodes. This reduces nodes by half each
time.</p></li>
</ul></li>
<li><p>The new node score is directly obtained by weighted averaging of
the two endpoint node features:</p>
<p><span class="math display">\[
\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
\]</span></p></li>
</ul>
<h1
id="discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</h1>
<ul>
<li><p>Using a hierarchical LSTM encoder + LSTM decoder for extractive
summarization as a baseline, the authors added a three-layer attention
to incorporate discourse information. Specifically, discourse
information refers to sentence-level elaborate relationships, where one
sentence provides detailed explanation or supplementary information
about another. The authors argue that document summarization, as a
discourse-level task, naturally requires discourse information.</p></li>
<li><p>The authors use attention to learn directed elaborate edges
between sentences, as shown in the following diagram: <img data-src="https://s2.ax1x.com/2020/01/29/1MfDOI.png"
alt="1MfDOI.png" /></p></li>
<li><p>Three components:</p>
<ul>
<li><p>Parent Attention: Use hierarchical encoder to obtain sentence
representations, then use attention to represent the probability of
sentence k being the parent node of sentence i, with elaborate edges
pointing from k to i (without using self-attention)</p>
<p><span class="math display">\[
\begin{aligned} p(k | i, \mathbf{H}) &amp;=\operatorname{softmax}(g(k,
i)) \\ g(k, i) &amp;=v_{a}^{\mathrm{T}} \tanh \left(U_{a} \cdot
H_{k}+W_{a} H_{i}\right) \end{aligned}
\]</span></p></li>
<li><p>Recursive Attention: Calculate multi-hop parent nodes, obtaining
the probability of k being the d-hop parent node of i. This can be
simply achieved by powering the attention matrix, with special handling
for the root sentence (virtual node) which has no parent nodes:</p>
<p><span class="math display">\[
\alpha_{d, k, i}=\left\{\begin{array}{ll}{p(k | i, \mathbf{H})} &amp;
{(d=1)} \\ {\sum_{l=0}^{N} \alpha_{d-1, k, l} \times \alpha_{1, l, i}}
&amp; {(d&gt;1)}\end{array}\right.
\]</span></p></li>
<li><p>Selective Attention: Combine attention information by first
weighted summing parent node information for sentence i at each hop:</p>
<p><span class="math display">\[
\gamma_{d, i}=\sum_{k=0}^{N} \alpha_{d, k, i} H_{k}
\]</span></p>
<p>Then calculate hop weights using selective attention, depending on
sentence i's encoder and decoder states <span
class="math inline">\(H,s\)</span>, and encoder states of all parent
nodes:</p>
<p><span class="math display">\[
\beta_{d, i}=\operatorname{softmax}\left(\mathbf{W}_{\beta}\left[H_{i} ;
s_{i} ; K\right]\right)
\]</span></p>
<p>Obtain weighted information from all hops and append to decoder
input</p>
<p><span class="math display">\[
\Omega_{i}=\sum_{d} \beta_{d, i} \gamma_{d, i} \\
p\left(y_{i} | \mathbf{x},
\theta\right)=\operatorname{softmax}\left(\mathbf{W}_{o} \tanh
\left(\mathbf{W}_{c^{\prime}}\left[H_{i} ; s_{t} ; K ;
\Omega_{i}\right]\right)\right) \\
\]</span></p></li>
</ul></li>
<li><p>The authors mention that Rhetorical Structure Analysis (RST)
currently lacks good off-the-shelf tools with high accuracy. They
propose a joint learning framework, which turns out to mean using
existing RST Parsers to obtain elaborate edges during training to guide
Parent Attention, with no parser needed during testing. The parser's
errors still significantly impact the model. The objective function
is:</p>
<p><span class="math display">\[
-\log p(\mathbf{y} | \mathbf{x})-\lambda \cdot \sum_{k=1}^{N}
\sum_{i=1}^{N} E_{k, i} \log \alpha_{1, k, i}
\]</span></p>
<p>The second term guides attention using parser-obtained edges</p></li>
<li><p>The authors first use HILDA parser to obtain RST discourse
annotations, then convert them to dependency format using a method from
"Single-document summarization as a tree knapsack problem"</p></li>
<li><p>Although still dependent on parser during training, the authors
created two baselines: one without parser using the previous sentence as
the elaborate parent, another letting attention learn independently.
Results showed the parser-informed attention model outperformed
baselines. The model showed more significant advantages on short texts
(75 words) compared to long texts (275 words) in the Daily Mail dataset,
partly due to ROUGE metric's preference for longer texts, indicating
discourse information indeed helps in extracting the most important
information within word count constraints.</p></li>
<li><p>This paper can be seen as an attention model (self-attention +
multi-blocks) injecting prior information to achieve better results in
single-document extractive summarization.</p></li>
</ul>
<h1
id="a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</h1>
<ul>
<li>A NAACL 2018 paper considering discourse information for abstractive
summarization on research paper datasets</li>
<li>Here, discourse is narrowly defined as sections in research papers,
essentially a hierarchical attention model built upon the
pointer-generator, with the following structure: <img data-src="https://s2.ax1x.com/2020/01/29/1MHPr4.png" alt="1MHPr4.png" /></li>
<li>Praiseworthy is the authors' provision of two large-scale
long-document research paper summary datasets, PubMed and arXiv, both
reaching tens of thousands in scale, with average source document
lengths over 3000 and 4900 words, and average summary lengths exceeding
100 words - valuable ultra-long single-document summary datasets.</li>
</ul>
<h1
id="sentence-bert-sentence-embeddings-using-siamese-bert-networks">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</h1>
<ul>
<li>Highlight: Sentence pair regression tasks with BERT are
time-consuming. The authors propose a Siamese BERT network, improving
inference speed by 1,123,200 times</li>
<li>Evidently, this speedup claim is ambiguous. Naive BERT is slow in
semantic matching tasks because each pair requires sending two sentences
through BERT to calculate scores. The authors slightly modify BERT to
use embeddings as sentence feature vectors, directly using cosine
distance for matching</li>
<li>Next, they prove that original BERT embeddings are not good semantic
matching features. SBERT adds regression or classification layers after
BERT and introduces triplet loss, significantly improving performance
over original BERT. It can be seen as a fine-tuning of BERT for semantic
matching tasks.</li>
</ul>
<h1 id="towards-a-human-like-open-domain-chatbot">Towards a Human-like
Open-Domain Chatbot</h1>
<ul>
<li>Highlight: Google-produced, large. Researching detailed design
aspects</li>
<li>2.6 billion parameters. 40 billion token corpus. To capture
multi-turn dialogue quality, the authors propose Sensibleness and
Specificity Average (SSA) as a metric, finding that models optimized for
perplexity achieve the best SSA</li>
<li>The authors use evolved transformer, training a seq2seq model with
multi-turn dialogue input, vocabulary size of 8k (using BPE), achieving
a test set perplexity of only 10.2. The model outperforms other dialogue
systems supplemented with rules, systems, and knowledge, again proving
that deep neural networks can achieve miracles with sufficient data and
training</li>
<li>SSA measures two aspects: sensible and specific. It's a
human-evaluated metric where testers first judge if the response is
sensible, and if so, then judge its specificity, as systems often
scoring well on automatic metrics tend to give vague "I don't know"
responses. The authors found SSA correlates with human assessments of
system human-likeness</li>
<li>SSA has two testing environments: a specified test set of 1,477
multi-turn dialogues, and direct chatting with the system for 14-28
turns</li>
<li>The authors provide numerous training and testing details,
essentially highlighting the model's scale: trained on one TPU-v3 Pod
for 30 days, 164 epochs, observing 10T tokens in total</li>
<li>This powerful yet simple model doesn't require complex decoding to
ensure high-quality, diverse responses. The authors used sample and
rank: dividing logits by temperature T, then softmax, randomly sampling
multiple sequences based on probability and selecting the
highest-probability sequence. Higher temperatures reduce logits'
differences, facilitating context-related rare word generation. Sample
and rank surprisingly outperforms beam search, provided the model
achieves low perplexity. The authors set temperature to 0.88, sampling
20 sentences</li>
<li>Statistical tests revealed a correlation coefficient exceeding 0.9
between perplexity and SSA</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="edge-contraction-pooling-for-graph-neural-networks">Edge
Contraction Pooling for Graph Neural Networks</h1>
<ul>
<li><p>一种新的GNN池化方式，考虑了边</p></li>
<li><p>池化在GNN中的意义：</p>
<ul>
<li>识别基于特征或者基于结构的聚类</li>
<li>减少计算量</li>
</ul></li>
<li><p>作者提出的edgepool能够提高图分类和节点分类的性能。</p></li>
<li><p>pooling有两种，fixed和learned，作者简单介绍了三种learned pooling
method</p>
<ul>
<li><p>DiffPool：DiffPool学习到一种概率分配，用一个GNN学习embedding，用一个GNN学习聚类分配，将聚类分配视为一个soft
assign matrix<span
class="math inline">\(S\)</span>，基于节点特征将每个节点分配给一个聚类，聚类数量事先固定，每一层同时对embedding和邻接矩阵进行pooling，如下：</p>
<p><span class="math display">\[
\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1}
\times d}} \\
{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times
n_{l+1}}}\end{array} \\
\]</span></p>
<p>问题在于：聚类数量不可变；基于节点特征分配而不考虑节点之间距离；聚类分配矩阵与节点数目成线性关系，难以scale；难以训练</p></li>
<li><p>TopKPool：简单粗暴，学习到一个投影向量，将每个节点的特征投影加权为一个单值，取topk个节点保留作为Pooling，问题在于不能改变图（加节点），以及这种hard
assignment容易丢失信息</p></li>
<li><p>SAGPool：对TopK的改进，对邻域节点使用了注意力加权，再投影，不过依然是topk的hard
assignment。</p></li>
</ul></li>
<li><p>edge pooling的思想是通过边的收缩(edge
contraction)来降采样，给定一条边e，两边节点<span
class="math inline">\(v_i\)</span>和<span
class="math inline">\(v_j\)</span>，边收缩指的是将i和j的所有邻接节点全部接到一个新节点<span
class="math inline">\(v_e\)</span>，这个操作显然是可以叠加多次，类似于CNN的不断扩大感受野。</p></li>
<li><p>如何选边？</p>
<ul>
<li><p>先对边计算分数，这里简单的将边连接的两个节点的embedding拼接再线性变换</p>
<p><span class="math display">\[
r(e_{ij}) = W (n_i || n_j) + b
\]</span></p></li>
<li><p>之后对所有的分数做softmax归一化，注意这里作者加了0.5使得均值为1，作者给出的解释是数值计算更稳定且梯度传导更好</p>
<p><span class="math display">\[
s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
\]</span></p></li>
<li><p>按照分数开始收缩边，假如边连接了已经收缩的边节点那就不再收缩了。这样每次都能减少一半的节点。</p></li>
</ul></li>
<li><p>新的节点分数直接用边分数加权两端节点特征和得到：</p>
<p><span class="math display">\[
\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
\]</span></p></li>
</ul>
<h1
id="discourse-aware-hierarchical-attention-network-for-extractive-single-document-summarization">Discourse-Aware
Hierarchical Attention Network for Extractive Single-Document
Summarization</h1>
<ul>
<li><p>以hierarchical lstm encoder+lstm
decoder的抽取式摘要作为baseline，添加了一个三层attention用来加入篇章信息，这里的篇章信息具体指的是句子级别的elaborate关系，即某一句详细阐述或者补充说明了另一句，作者认为document
summarization这种篇章级别的任务当然需要篇章信息。</p></li>
<li><p>作者使用了attention来学习句子之间的elaborate有向边，具体如下图：
<img data-src="https://s2.ax1x.com/2020/01/29/1MfDOI.png"
alt="1MfDOI.png" /></p></li>
<li><p>三个组件</p>
<ul>
<li><p>Parent Attention：使用hierarchical
encoder得到每个句子的表示，之后用attention表示句子k是句子i父节点的概率，即elaborate的边由k指向i（作者没有用self
attention）</p>
<p><span class="math display">\[
\begin{aligned} p(k | i, \mathbf{H}) &amp;=\operatorname{softmax}(g(k,
i)) \\ g(k, i) &amp;=v_{a}^{\mathrm{T}} \tanh \left(U_{a} \cdot
H_{k}+W_{a} H_{i}\right) \end{aligned}
\]</span></p></li>
<li><p>Recursive
Attention：即计算多跳父节点，得到k是i的d跳父节点概率，这里简单的用注意力矩阵幂应该就可以得到，注意要对root句子（虚节点）做特殊处理，root没有父节点：</p>
<p><span class="math display">\[
\alpha_{d, k, i}=\left\{\begin{array}{ll}{p(k | i, \mathbf{H})} &amp;
{(d=1)} \\ {\sum_{l=0}^{N} \alpha_{d-1, k, l} \times \alpha_{1, l, i}}
&amp; {(d&gt;1)}\end{array}\right.
\]</span></p></li>
<li><p>Selective
Attention：综合得到的attention信息，首先将句子i某一跳所有父节点的信息加权求和：</p>
<p><span class="math display">\[
\gamma_{d, i}=\sum_{k=0}^{N} \alpha_{d, k, i} H_{k}
\]</span></p>
<p>之后再用selective
attention计算该跳的权重，依赖于句子i的encoder和decoder state<span
class="math inline">\(H,s\)</span>，以及所有父节点的encoder state：</p>
<p><span class="math display">\[
\beta_{d, i}=\operatorname{softmax}\left(\mathbf{W}_{\beta}\left[H_{i} ;
s_{i} ; K\right]\right)
\]</span></p>
<p>得到权重加权所有跳的信息，并补充进decoder input当中（拼接）</p>
<p><span class="math display">\[
\Omega_{i}=\sum_{d} \beta_{d, i} \gamma_{d, i} \\
p\left(y_{i} | \mathbf{x},
\theta\right)=\operatorname{softmax}\left(\mathbf{W}_{o} \tanh
\left(\mathbf{W}_{c^{\prime}}\left[H_{i} ; s_{t} ; K ;
\Omega_{i}\right]\right)\right) \\
\]</span></p></li>
</ul></li>
<li><p>这里，作者说提到了修辞结构分析（RST）目前没有很好的off-the-shelf
tools，误差大，这是硬伤，因此提出了一个联合学习的框架，后来发现联合学习是指训练集上依然用已有的RST
Parser得到elaborate edges，用以指导Parent
Attention，之后测试集就不需要了，这样的话Parser当中的误差对模型的影响依然很大。目标函数为：</p>
<p><span class="math display">\[
-\log p(\mathbf{y} | \mathbf{x})-\lambda \cdot \sum_{k=1}^{N}
\sum_{i=1}^{N} E_{k, i} \log \alpha_{1, k, i}
\]</span></p>
<p>其中第二项就是用parser得到的边指导attention</p></li>
<li><p>作者先用HILDA
parser得到RST格式的篇章标注信息，然后用Single-document summarization as
a tree knapsack problem一文中的方法转换为dependency的格式</p></li>
<li><p>虽然依然依赖于parser进行训练，但是作者做了两个Baseline，一个是不用parser，直接将前一句作为下一句的elaborate
parent，另一个也不用parser，让attention自己学习，结果发现baseline都不如注入了parser信息的attention模型。让attention自己学习最差，其次是学一个固定的前句父节点。作者提出的模型相比baseline在daily
mail数据集上抽短文本（75)比抽长文本(275)优势更大，这里有ROUGE指标偏爱长文本的原因，也说明在字数限制下，抽最重要的信息方面，discourse的信息确实可以起到帮助。</p></li>
<li><p>这篇文章可以看成一个attention模型(self attention +
multi-blocks)，注入了一些先验信息来帮助在单文档抽取式摘要获得更好的结果。</p></li>
</ul>
<h1
id="a-discourse-aware-attention-model-for-abstractive-summarization-of-long-documents">A
Discourse-Aware Attention Model for Abstractive Summarization of Long
Documents</h1>
<ul>
<li>NAACL
2018的一篇论文，依然是考虑了篇章信息，不过是在科研论文数据集上做生成式摘要。</li>
<li>这里的discourse有些狭义了，指的是科研论文里的每一个section，其实还是一个hierarchical
attention，作者也直接在pointer-generator上改了，结构如下： <img data-src="https://s2.ax1x.com/2020/01/29/1MHPr4.png" alt="1MHPr4.png" /></li>
<li>值得称赞的是作者提供了两个大规模长文档的科研论文摘要数据集，pubmed以及arxiv，均达到十万规模即便，平均原文长度达到3000+和4900+，平均摘要长度也过百，是很有价值的超长单文档摘要数据集。</li>
</ul>
<h1
id="sentence-bert-sentence-embeddings-using-siamese-bert-networks">Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks</h1>
<ul>
<li>亮点：用BERT做句子对回归任务很耗时，作者提出孪生BERT网络，将推理速度提高了1123200倍</li>
<li>显然提高这么多倍的说法是有歧义的，naive
bert在语义匹配任务上耗时，是因为每匹配一对就要将一对句子送进BERT计算出分数，而作者将BERT稍作修改，用BERT得到的embedding作为句子的特征向量，直接用向量的之间的cosine距离来做匹配，当然要快</li>
<li>接下来就是证明原始BERT得到的embedding并不能很好的作为语义匹配的特征向量，SBERT也就是在BERT之后加了回归层或者分类层，引入triplet
loss，得到的效果就比原始BERT好很多。可以看成是BERT在语义匹配任务上的一种微调吧。</li>
</ul>
<h1 id="towards-a-human-like-open-domain-chatbot">Towards a Human-like
Open-Domain Chatbot</h1>
<ul>
<li>亮点：谷歌出品，大。研究一些细节设计。</li>
<li>26亿参数量。400亿token的语料。为了很好的捕捉多轮对话的质量，作者提出了Sensibleness
and Specificity
Average(SSA)作为指标，并且发现最优化perplexity的模型能够达到最好的SSA。</li>
<li>作者使用evolved
transformer，多轮对话作为输入，训练了一个seq2seq，词标大小8k（用了BPE），最后测试集的困惑度只有10.2，且实际表现比其他的补充了规则、系统、知识的复杂的对话系统表现要好，再次证明了深度神经网络，只要数据够多，训得够好，就是可以大力出奇迹。</li>
<li>SSA衡量两个方面：合理且具体。这是一个人工衡量指标，首先问测试人员回答是否合理，假如合理，再问回答是否具体，因为很多时候回答不具体（总是回答i
don't
know）的系统反而在自动指标上取得比较好的成绩。作者也实验发现SSA和人工检测系统是否human-like一致，SSA高的系统表现更加像人类。</li>
<li>SSA有两种测试环境，一种是指定测试集，作者收集了1477个多轮对话作为测试数据集；另一个就是让测试人员直接和系统闲聊，至少14轮，至多28轮</li>
<li>作者给出了很多训练细节和测试细节，具体可见论文，反正就是大，在一块TPU-v3
Pod上训练了30天，164个epoch，模型总共观察了10T个token。</li>
<li>这么强大而简单的模型，在decoding时不需要复杂的处理来保证生成高质量且多样化的回答。作者采用了sample
and
rank：生成Logits之后先除以温度T，再过softmax，按概率随机采样生成多个序列之后取概率最大的那一句作为输出。作者发现温度越高，即logits输出的差异性越小，容易生成与上下文相关的罕见词。作者比对发现sample
and rank虽然简单但是比beam search表现更好，前提是能够训练到low
perplexity。作者将温度设为0.88，采样20句。</li>
<li>统计测试发现perplexity和SSA的相关系数高达0.9以上。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>graph neural network</tag>
        <tag>deep learning</tag>
        <tag>summarization</tag>
        <tag>natural language processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Study Notes for Cognitive Graph</title>
    <url>/2019/08/13/CogGraph/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<p>Note for paper "Cognitive Graph for Multi-Hop Reading Comprehension
at Scale."</p>
<span id="more"></span>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/13/mClGgH.png" alt="mClGgH.png" />
<figcaption aria-hidden="true">mClGgH.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="task">Task</h1>
<ul>
<li>The framework proposed by the author is called CogQA, which is a
framework based on cognitive graphs to address question-answering in
machine reading comprehension, including general question-answering
(selecting entities) and comparative question-answering (the
relationship between two entities). Setting cognitive graphs aside for
the moment, let's look at what this task is.</li>
<li>The uniqueness of this question-answering task lies in its extension
to multi-hop. Multi-hop should not be a task type but rather a method
for completing entity-based question-answering tasks, that is, finding
entities in the question, then locating clues in their corresponding
context descriptions, and using these clues to find the next entity as
the next hop. The next-hop entity is used to jump to the corresponding
context, where clues are then found again, and this process is repeated
until, after multiple hops, the correct entity is found in the correct
description as the answer. For a question, you can solve it using a
single-hop approach or a multi-hop approach. In fact, most
question-answering models based on information retrieval are based on a
single-hop approach, which simply compares the question and the context
to find the most relevant sentence and then extract entities from these
sentences. Essentially, this is a pattern matching approach, but the
problem is that if the question itself is multi-hop, then the single-hop
pattern matching may not be able to find the correct entity at all,
because the answer is not in the candidate sentences.</li>
<li>This is actually very similar to the way humans answer questions.
For example, if we ask who the authors are of the cognitive graph
published in ACL2019, we would first find the two entities, ACL2019 and
cognitive graph, and then separately find all the corresponding authors
of ACL2019 papers and the various meanings of the cognitive graph (it
may be neuroscience, it may be education, it may be natural language
processing), and then find more entities and descriptions (different
authors of different papers, different explanations of meanings),
ultimately finding one or more answers. Humans might directly search for
the term "cognitive graph" in all the paper titles of ACL2019, while
computers might extend and jump between ACL2019 and cognitive graph
multiple times before merging into a single entity, namely the author's
name, and then output it as an answer.</li>
<li>The multi-hop connections among multiple entities and their
topological relationships constitute the cognitive graph, a directed
graph. This directed graph is both inferable and interpretable, unlike
black-box models, with a clear inference path. Therefore, the issue
boils down to:
<ul>
<li>How to construct a graph?</li>
<li>With the diagram, how to reason?</li>
</ul></li>
<li>The author first proposed using the dual-process theory from
cognitive science to explain their approach.</li>
</ul>
<h1 id="two-process-model">Two-process model</h1>
<ul>
<li>The dual-process model in cognitive science refers to the fact that
humans solve problems in two steps:
<ul>
<li>System 1: Initially, attention is allocated through an implicit,
unconscious, and intuitive process to retrieve relevant information</li>
<li>System Two: Reasoning is completed through another explicit,
conscious, and controllable process</li>
<li>System one provides resources to system two, system two guides the
search of system one, and they iterate in this manner</li>
</ul></li>
<li>The above two processes can actually correspond to two major schools
of thought in artificial intelligence, connectionism and symbolism. The
first process, although difficult to explain and completed through
intuition, is not innate but actually obtained through hidden knowledge
gained from life experience. This part can correspond to the black-box
models currently completed by deep learning, which learn from a large
amount of data to produce models that are unexplainable but can achieve
the intended purpose. While the second process requires causal
relationships or explicit structures to assist in reasoning.</li>
<li>In the context of machine question answering, the authors naturally
employed existing neural network models to accomplish these two tasks:
<ul>
<li>The first item requires attention to retrieve relevant information,
so I directly use the self-attention model to complete entity
retrieval.</li>
<li>The second item requires an explicit structure, so I construct a
directed cognitive graph and complete reasoning on the cognitive
graph.</li>
</ul></li>
</ul>
<h1 id="how-to-construct-a-graph">How to Construct a Graph</h1>
<ul>
<li>The author uses BERT to complete the work of System 1. BERT itself
can be used as a one-step machine reading comprehension, and here the
author follows the one-step approach, with the input sentence pairs
being the question and the sentence to be annotated, and the output
being the annotation probability, i.e., the probability that each word
is the start or end position of an entity. However, to achieve
multi-hop, the author made some modifications:
<ul>
<li>The input sentence pairs are not based on the problem as a unit, but
on each entity within each problem. Specifically, the A sentence of each
input sentence pair is composed of the problem and a clue to a certain
entity within the problem, while the B sentence is about all the
sentences in the descriptive context of that entity.
<ul>
<li>sentence A:<span
class="math inline">\([CLS]Question[SEP]clues(x,G)[SEP]\)</span></li>
<li>sentence B:<span class="math inline">\(Para(x)\)</span></li>
</ul></li>
<li>What is the clue of a certain entity? The clue is the sentence
describing the entity extracted from the context of all parent nodes in
the cognitive graph. It may sound a bit awkward, but this design runs
through the entire system and is the essence of its cognitive reasoning,
as illustrated by the examples given in the paper:
<ul>
<li>Who made a movie in 2003 with a scene shot at the Los Angeles
Quality Cafe?</li>
<li>We found the entity "quality cafe," and found its introduction
context: "......This is a coffee shop in Los Angeles. The shop is famous
for being the filming location for several movies, including Old School,
Gone in 60s, and so on."</li>
<li>We then proceed to traverse these movie name entities, and then find
the introduction context of the movies, such as "Old School is an
American comedy film released in 2003, directed by Todd Phillips," and
through other cognitive reasoning, we deduce that this "Todd Phillips"
is the correct answer. Then, what is the clue for this director entity?
What kind of clues do we need as supplementary input to obtain "Old
School is an American comedy film released in 2003, directed by Todd
Phillips," where "Todd Phillips" is the answer we seek?</li>
<li>The answer is "This store is famous for being the filming location
for several movies, including Old School, Gone in 60 Seconds, etc." This
sentence corresponds to the input format in BERT.</li>
<li>sentence A:
<ul>
<li>Who made a movie in 2003 with a scene shot at the Los Angeles
Quality Cafe?</li>
<li>clues(x,G): “This store is famous for being the filming location for
several movies, including 'Old School' and 'Gone in 60 Seconds'
etc.”</li>
</ul></li>
<li>"Old School is an American comedy film released in 2003, directed by
Todd Phillips."</li>
<li>Entity x is referred to as "old school."</li>
</ul></li>
<li>This design completes the iterative part in both System 1 and System
2, connecting the two systems. This part allows System 2 to use graph
structures to guide System 1 in retrieval. And through cycles, it is
possible that System 2 updates the features of a certain entity's parent
node or adds a new parent node, all of which may lead to the acquisition
of new clues. System 1 can then use these clues again to predict and
find new answer entities or next-hop entities that were not previously
identified.</li>
<li>How does System 2 depend on the results of System 1? This also
divides into two parts
<ul>
<li>Perform two span predictions: System 1's BERT separates the
prediction start and end positions of the answer entity and the next-hop
entity, using four parameter vectors to combine the word feature vectors
output by BERT to predict the start and end positions of the answer
entity, the start and end positions of the next-hop entity, totaling
four quantities. After obtaining the answer and next-hop entities, they
are added to the cognitive graph as sub-nodes of the current entity,
connecting the edges.</li>
<li>Of course, it is not enough to just connect the edges; node features
are also required. Just as BERT's position 0 extracts features of the
entire sentence pair, the authors use it as a node feature <span
class="math inline">\(sem(x,Q,clues)\)</span> and supplement it to the
diagram.</li>
</ul></li>
<li>This system one provides topological relationships and node features
for the expansion of the graph, thereby providing resources for system
two.</li>
</ul></li>
</ul>
<h1 id="how-to-reason-on-graphs">How to Reason on Graphs</h1>
<ul>
<li><p>This section directly employs GNN to perform spectral
transformation on a directed graph to extract one layer of node
features</p>
<p><span class="math display">\[
\Delta = \sigma ((AD^{-1})^T) \sigma (XW_1)) \\
X^1 = \sigma (XW_2 + \Delta) \\
\]</span></p></li>
<li><p>The subsequent predictions only require adding a simple network
on top of the transformed node features for regression or
classification.</p></li>
<li><p>Note that although model one extracts the answer span, both the
answer span and the next-hop entity span are added as nodes to the
cognitive graph, because there may be multiple answer nodes that require
judgment of confidence by System Two. The reason for BERT to predict
both the answer and the next-hop separately is:</p>
<ul>
<li>Both should have different features obtained through BERT and
require independent parameter vectors to assist in updating</li>
<li>Both are equally included in the cognitive graph, but only the
next-hop node will continue to input into the system to make further
predictions</li>
</ul></li>
<li><p>The model's loss consists of two parts, namely the System One's
span prediction (answer &amp; next hop) loss and the System Two's answer
prediction loss, both of which are relatively simple and can be directly
referred to in the paper.</p></li>
</ul>
<h1 id="data">Data</h1>
<ul>
<li>Authors used the full-wiki part of HotpotQA for training and
testing, with 84% of the data requiring multi-hop reasoning. Each
question in the training set provided two useful entities, as well as
multiple descriptions of the context and 8 irrelevant descriptions for
negative sampling. During validation and testing, only the questions
were provided, requiring answers and relevant descriptions of the
context.</li>
<li>To construct a gold-only cognitive graph, i.e., the initialized
total cognitive graph, the authors perform fuzzy matching on every
sentence in the description context of all entities y and a certain
entity x. If a match is found, (x, y) is added as an edge to the
initialized graph</li>
</ul>
<h1 id="overall-process">Overall Process</h1>
<ul>
<li>Input: System One, System Two, Issue, Prediction Network, Wiki
Dataset</li>
<li>Initialize the gold-only graph with entities from the problem and
mark these entities as parent nodes, and add the entities found through
fuzzy matching during initialization to the boundary queue (the queue to
be processed)</li>
<li>Repeat the following process
<ul>
<li>Pop an entity x from the boundary queue</li>
<li>Collect clues from all ancestors of x</li>
<li>Input the clues, issues, and descriptive context of the entity into
the unified system, obtaining the cognitive graph node representation
<span class="math inline">\(sem(x^1,Q,clues)\)</span></li>
<li>If entity x is the next-hop node, then:
<ul>
<li>Entity span for generating answers and next-hop</li>
<li>For the next-hop entity span, if it exists in the Wiki database,
create a new next-hop node in the diagram and establish an edge; if it
is already in the diagram but has not established an edge with the
current entity x, add an edge and include the node in the boundary
queue</li>
<li>For the answer entity span, nodes and edges are directly added
without the need for judgment from the Wikipedia database, because the
answer may not be in the database</li>
</ul></li>
<li>Through the second system updating node features</li>
</ul></li>
<li>Until there are no nodes in the boundary queue, or the cognitive
graph is sufficiently large</li>
<li>Through predicting the network's return results.</li>
<li>Through the above process, it can be seen that for each training
data, before using the prediction network to predict the results, two
systems need to interact iteratively multiple times until feature
extraction is complete. The condition for stopping iteration is when the
boundary queue is empty. Then, what kind of nodes will join the boundary
queue? Nodes that have already been in the graph and established new
edges for the next hop may bring new clues, therefore, all such nodes
must be processed, allowing system two to see all clues before making
predictions.</li>
</ul>
<h1 id="other-details">Other details</h1>
<ul>
<li>In System One, there may not be Sentence B, i.e., there may be no
descriptive context for a certain entity. In this case, we can simply
obtain the node features <span
class="math inline">\(sem(x^1,Q,clues)\)</span> through BERT, without
predicting the answer and the next-hop entity, i.e., this node acts as a
leaf node in the directed graph and no longer expands.</li>
<li>At the initialization of the cognitive graph, it is not necessary to
obtain node features; only the prediction of spans is needed to
construct edges</li>
<li>The author found that using the feature at position 0 of the last
layer of BERT as node features was not very good, because the features
of higher layers are transformed to be suitable for span prediction, so
after experimentation, the author took the third-to-last layer of BERT
to construct node features</li>
<li>When performing span prediction, it actually specifies a maximum
span length, then predicts the top k beginning positions, and then
predicts the end positions within the span maximum length</li>
<li>The author also employed negative sampling to prevent span
prediction on irrelevant sentences. Specifically, it first samples
irrelevant samples, sets the [CLS] position probability of these samples
to 0, and sets the position probability of the positive samples to 1. In
this way, BERT can learn the probability that sentence B is a positive
sample at the [CLS] position. Only the topk spans selected previously
will be retained if their begin position probability is greater than the
[CLS] position probability.</li>
<li>In the process described in the pseudo-algorithm, every time the
system updates the cognitive graph structure, system two runs once. In
fact, the author found that it is the same effect, and more efficient,
to let system one traverse all the boundary nodes first, wait until the
graph no longer changes, and then let system two run multiple times. In
actual implementation, this algorithm is also adopted.</li>
<li>HotpotQA includes special questions, non-traditional questions, and
traditional questions. The author has constructed prediction networks
for each, where special questions are regression models, and the other
two types are classification models.</li>
<li>When initializing the cognitive graph, it is not only necessary to
establish edges between entities and the next-hop entities, but also to
mark the begin and end positions of the next-hop entities and feed them
into the BERT model</li>
<li>The author also conducted an ablation study, mainly focusing on the
differences in the initial entity sets, and the experimental results
show that the model is relatively dependent on the quality of the
initial entities</li>
</ul>
<h1 id="results">Results</h1>
<ul>
<li>Dominating the HotpotQA leaderboard for several months before this
April, until recently being surpassed by a new BERT model, but at least
this model can provide a good interpretability, as shown in the three
cognitive graph reasoning scenarios in the following figure <img data-src="https://s2.ax1x.com/2019/08/13/mClr8g.png" alt="mClr8g.png" /></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<ul>
<li>This model can be simply regarded as an extension of GNN in NLP,
with the powerful BERT used for node feature extraction. However, the
difficulty of using GNN in NLP lies in the definition of edge
relationships. This paper presents a very natural definition of
relationships, consistent with the intuition of humans in completing
question-answering tasks, and BERT not only extracts node features but
also completes the construction of edges. I feel that this framework is
a good way to combine black-box models and interpretable models, rather
than necessarily explaining black-box models. The black box will let it
do what it is good at, including feature extraction of natural language
and reasoning networks, while humans can design explicit rules for
adding edge relationships. Both work together, complementing each other
rather than being mutually exclusive.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="任务">任务</h1>
<ul>
<li>作者提出的框架叫CogQA，也就是用基于认知图谱的框架来解决机器阅读理解里的问答，包括一般性问答（选择实体），以及比较性问答（两个实体之间的关系）。先抛开认知图谱不说，看看这个任务是什么。</li>
<li>这个问答任务特殊的地方在于延伸到了多跳，multi-hop。多跳其实不应该是任务类型，而是指完成实体类问答任务的一种方式，即找到问题中的实体，根据这些实体在其对应介绍上下文中找到线索（clues),在这些线索里接着找实体作为下一跳(hop),下一跳的实体用于跳转到对应的介绍上下文，并在其中接着找线索，如此往复，直到多跳之后在正确的描述中找到正确的实体作为答案。一个问答，你可以用一跳的思路解决，也可以用多跳的思路解决。实际上大多数基于信息检索的问答模型就是基于一跳的，这类模型就只是比较问题和上下文，找出最相关的句子，再从这些句子中找出实体。这样做本质上是一种模式匹配，其问题在于，加入问题本身是多跳的，那么基于一跳的模式匹配可能根本找不出正确的实体，因为答案都不在候选的句子里。</li>
<li>这和人类回答问题的方式其实很类似，比方我们问发表于ACL2019的认知图谱的作者是谁，我们会先找到ACL2019和认知图谱这两个实体，再分别到其线索中找到ACL2019所有论文对应作者和认知图谱的多种含义（可能有神经科学，可能有教育学，可能有自然语言处理），再找到更多的实体和描述（不同论文作者、不同含义的解释），最终找到一个或者多个答案。人类的思路可能会直接在ACL2019的所有论文标题里找认知图谱四个字，而计算机处理起来可能是ACL2019和认知图谱两部分延伸多跳之后在某一节点合并到一个实体，即作者的名字，然后作为答案输出。</li>
<li>以上多个实体的多跳以及它们之间的拓扑关系就组成了认知图谱，一个有向图。这个有向图是可推理可解释的，不同于黑箱模型，有清晰的推理路线。那么问题就归结为：
<ul>
<li>如何构造图？</li>
<li>有了图，如何推理？</li>
</ul></li>
<li>作者首先提出了用认知科学里的双过程解释他们的做法。</li>
</ul>
<h1 id="双过程模型">双过程模型</h1>
<ul>
<li>认知科学里的双过程是指，人类解决问题时会分两个步骤：
<ul>
<li>系统一：先通过一个隐式的、无意识的、符合直觉的过程来分配注意力，检索相关信息</li>
<li>系统二：再通过另一个显式的、有意识的、可控的过程来完成推理</li>
<li>系统一给系统二提供资源，系统二指导系统一的检索，两者迭代进行</li>
</ul></li>
<li>这上面两个过程，其实可以对应到人工智能里的两大流派，联结主义和符号主义。第一个过程虽然是难以解释的、通过直觉完成的，但直觉不是天生的，实际上是通过生活经验得到的隐藏知识。这部分可以对应现在用深度学习完成的黑箱模型，通过对大量数据学习得到不可解释，但是能完成目的的模型。而第二个过程需要因果关系，或者需要显式的结构来帮助推理。</li>
<li>具体到机器问答中，作者很自然的用了现有的神经网络模型来完成这两项工作：
<ul>
<li>第一项需要分配注意力来检索相关信息，那么我就直接用自注意力模型，来完成实体的检索。</li>
<li>第二项需要显式的结构，那么我就构造有向认知图谱，在认知图谱上完成推理。</li>
</ul></li>
</ul>
<h1 id="如何构造图">如何构造图</h1>
<ul>
<li>作者使用BERT来完成系统一的工作。BERT本身就可以用作一跳机器阅读理解，在这里作者沿用了一跳的做法，输入的句子对是问题和待标记实体的句子，输出是标记概率，即每个词是实体开始位置或者结束位置的概率。但是为了实现多跳，作者做了一点改动：
<ul>
<li>输入的句子对不是以问题为基本单位，而是以每个问题中的每个实体为基本单位，具体而言，每个输入句子对的A句子由问题和问题中某一实体的线索(clue)拼接而成，而句子B是关于该实体的描述上下文中的所有句子。即
<ul>
<li>sentence A:<span
class="math inline">\([CLS]Question[SEP]clues(x,G)[SEP]\)</span></li>
<li>sentence B:<span class="math inline">\(Para(x)\)</span></li>
</ul></li>
<li>那么某一实体的线索究竟是什么？<strong>线索是该实体在认知图谱里的所有父节点的介绍上下文中，提取出该实体的那一句话</strong>。可能有些拗口，但是这个设计是贯穿了整个系统，是其认知推理断的精髓所在，用论文给出的例子就是：
<ul>
<li>问题：“谁在2003年拍了部电影，其中有一幕是在洛杉矶quality
cafe拍的？”</li>
<li>我们找到实体quality
cafe，找到其介绍上下文：“......这是洛杉矶的一家咖啡店。这家店因其作为多部电影的取景地而出名，包括old
school, gone in 60s等等。......”</li>
<li>我们接着遍历这些电影名实体，接着找电影的介绍上下文，例如“old
school是一部美国喜剧电影，拍于2003年，导演是todd
phillips”，并且通过其他认知推理得到这个“todd
phillips”就是正确答案，那么，这个导演实体的线索是什么？我们需要什么样的线索作为补充输入来得到“old
school是一部美国喜剧电影，拍于2003年，导演是todd phillips”这句话中“todd
phillips”就是我们想要的答案？</li>
<li>答案就是“这家店因其作为多部电影的取景地而出名，包括old school, gone
in 60s等等。”这句话。对应成BERT里的输入格式就是</li>
<li>sentence A:
<ul>
<li>Question：“谁在2003年拍了部电影，其中有一幕是在洛杉矶quality
cafe拍的？”</li>
<li>clues(x,G)：“这家店因其作为多部电影的取景地而出名，包括old school,
gone in 60s等等。”</li>
</ul></li>
<li>sentence B:“old school是一部美国喜剧电影，拍于2003年，导演是todd
phillips”</li>
<li>其中实体x是“old school”</li>
</ul></li>
<li>这个设计完成了系统一和系统二中的迭代部分，将两个系统连接了起来。这部分是让系统二利用图结构来指导系统一检索。并且通过循环往复，可能系统二更新了某一实体的父节点的特征，或者添加了新的父节点，这些都可能会导致有新的线索获得，系统一可以再次把这些线索拿来预测，找出之前没有找出的新的答案实体或者下一跳实体。</li>
<li>那么系统二如何依赖系统一的结果呢？这里也分为两个部分
<ul>
<li>做两个span
prediction：系统一的BERT将答案实体和下一跳实体的预测起始结束位置分开，用四个参数向量来分别结合BERT输出的词特征向量来预测答案实体、下一跳实体的预测开始、结束位置共四个量。获得了答案和下一跳实体之后，将其加入认知图谱当中，作为当前实体的子节点，连上边。</li>
<li>单单连上边当然不够，还需要节点特征。刚好BERT的位置0是提取整个句子对的特征，作者就将其作为节点特征<span
class="math inline">\(sem(x,Q,clues)\)</span>补充到图中。</li>
</ul></li>
<li>这样系统一就为图的扩展提供了拓扑关系和节点特征，从而为系统二提供了资源。</li>
</ul></li>
</ul>
<h1 id="如何在图上推理">如何在图上推理</h1>
<ul>
<li><p>这一部分就直接使用了GNN，在有向图上进行谱变换提取一层节点特征</p>
<p><span class="math display">\[
\Delta = \sigma ((AD^{-1})^T) \sigma (XW_1)) \\
X^1 = \sigma (XW_2 + \Delta) \\
\]</span></p></li>
<li><p>之后的预测也只需要在变换后的节点特征上接一层简单网络来做回归或者分类就好了。</p></li>
<li><p>注意虽然模型一提取了答案的span，但是答案span和下一跳实体span都作为节点加入到认知图谱当中，因为可能有多个答案节点，需要经过系统二来判断置信度，而需要BERT分别预测答案和下一跳的理由是：</p>
<ul>
<li>两者通过BERT得到的特征应该不同，需要独立的参数向量来辅助更新</li>
<li>两者虽然是同等的加入认知图谱当中，但是只有下一跳节点会接着输入系统一来继续预测</li>
</ul></li>
<li><p>模型的损失包含两部分，分别是系统一的span prediction(answer &amp;
next hop) loss和系统二的answer prediction
loss，都比较简单，可以直接看论文。</p></li>
</ul>
<h1 id="数据">数据</h1>
<ul>
<li>作者使用了HotpotQA的full-wiki部分来做训练测试，84%的数据需要多跳推理。训练集中每个问题提供了两个有用的实体，以及多个描述上下文和8个不相关描述上下文用于负采样。验证和测试时只有问题，需要给出答案和相关的描述上下文。</li>
<li>为了构造gold-only认知图谱，即初始化的总的认知图谱，作者将所有的实体y和某一实体x的描述上下文中每一句做模糊匹配，匹配上了就将(x,y)作为一条边加入初始化的图谱中</li>
</ul>
<h1 id="总体流程">总体流程</h1>
<ul>
<li>输入：系统一、系统二、问题、预测网络、维基数据集</li>
<li>用问题里的实体初始化构造gold-only图谱，并把这些实体标记为父节点，把初始化中模糊匹配找到的实体加入边界队列（待处理队列）</li>
<li>重复以下过程
<ul>
<li>从边界队列中弹出一个实体x</li>
<li>从x的所有父节点那收集线索</li>
<li>将该实体的线索、问题、该实体的描述上下文输入系统一，得到认知图谱节点表示<span
class="math inline">\(sem(x^1,Q,clues)\)</span></li>
<li>假如实体x是下一跳节点，那么：
<ul>
<li>生成答案和下一跳的实体span</li>
<li>对于下一跳实体span，假如其在维基数据库当中，就在图中创建新的下一跳节点并建立边；假如已经在图中，但是没有和当前实体x建立边，那就添加一条边，并把该节点加入边界队列</li>
<li>对于答案实体span，直接加节点和边，而不需要经过维基数据库的判断，因为答案有可能不在数据库中</li>
</ul></li>
<li>通过系统二更新节点特征</li>
</ul></li>
<li>直到边界队列中没有节点，或者认知图谱足够大</li>
<li>通过预测网络返回结果。</li>
<li>通过以上流程可以看到，对每一条训练数据，在使用预测网络预测结果之前，需要两个系统交互迭代多次直到特征提取完全。迭代的条件是边界队列为空时停止，那么什么样的节点会加入边界队列？<strong>已经在图中且建立了新的边的下一跳节点</strong>，这一类节点可能带来新的线索，因此必须把这类节点都处理完，让系统二看到所有线索，之后才能做预测。</li>
</ul>
<h1 id="其他细节">其他细节</h1>
<ul>
<li>在系统一当中可能没有sentence
B，即没有某个实体的描述上下文，这时我们可以仅仅通过BERT得到节点特征<span
class="math inline">\(sem(x^1,Q,clues)\)</span>，而不预测答案和下一跳实体，即这个节点就作为有向图中的叶子节点，不再扩展。</li>
<li>在初始化认知图谱时，不需要得到节点特征，仅仅预测span来构建边</li>
<li>作者发现使用BERT的最后一层的位置0的特征作为节点特征不太好，因为高层的特征被转换成适用于span
prediction，因此作者试验之后取BERT的倒数第三层来构建节点特征</li>
<li>在做span prediction的时候，实际上是规定了一个span maximum
length，然后预测top k个begin position，然后在span maximum
length内预测end position</li>
<li>作者还做了负采样来防止在无关句子上做span
prediction，具体做法是先负采样出不相关的样本，将这些样本的[CLS]位置概率设为0，而正样本的该位置概率设为1。这样BERT就能在[CLS]位置学到这个sentence
B是正样本的概率。之前选出的topk个span，只有begin position
probability大于[CLS]位置概率才会被保留下来</li>
<li>在伪算法描述的流程里，每次系统一更新了认知图谱结构，系统二就运行一次，实际上作者发现让系统一先把所有的边界节点遍历完，等图不再改变，再让系统二运行多次是一样的效果，而且效率更高。实际实现上也是采用这种算法。</li>
<li>HotpotQA包含特殊问题、非传统问题和传统问题，作者分别构建了预测网络，其中特殊问题是回归模型，其余两类是分类模型。</li>
<li>初始化认知图谱的时候，不仅仅需要建立实体与下一跳实体之间的边，下一跳实体的begin和end位置也要标出来，feed给BERT模型</li>
<li>作者还做了ablation
study，主要是初始化的实体集不同，可以通过实验结果看出该模型还是比较依赖初始化的实体质量</li>
</ul>
<h1 id="结果">结果</h1>
<ul>
<li>在今年4月份以前一直霸榜HotpotQA几个月，直到最近被一个新的BERT模型打破，但至少该模型能够提供一个很好的解释性，例如下图所示的三种认知图谱推理情况
<img data-src="https://s2.ax1x.com/2019/08/13/mClr8g.png"
alt="mClr8g.png" /></li>
</ul>
<h1 id="结论">结论</h1>
<ul>
<li>这个模型可以简单的看成是GNN在NLP的扩展，只不过用了强大的BERT做节点特征提取，但是GNN用于NLP的难点在于边关系的定义。本文对于关系的定义非常自然，和人类完成问答任务的直觉保持一致，并且BERT也不仅仅是提取节点特征，还完成了边的构建。这样的框架我感觉是很好的将黑盒模型和可解释模型结合起来，而不是一定要解释黑盒模型。黑盒将让他做黑盒擅长的部分，包括自然语言和推理网络的特征提取，而人类可以设计显式的边关系添加规则，两者合作，互补而不是互斥。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>natural language processing</tag>
        <tag>machine learning</tag>
        <tag>gnn</tag>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>Study Notes for Correlation Explaination</title>
    <url>/2019/07/29/CorEx/</url>
    <content><![CDATA[<p>Note for CorEx(Correlation Explaination).</p>
<span id="more"></span>
<p><img data-src="https://s2.ax1x.com/2019/07/31/etsOld.gif" /></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="abstract">Abstract</h1>
<ul>
<li>Correlation Explaination is a type of learning method that can be
used in topic models, yielding results similar to LDA but with a
completely different processing process. Correlation Explaination does
not make any structural prior assumptions about the generation of data,
but, similar to information gain, uses the difference in Total
Correlation to find the topic that best explains the data. One rapid
calculation method is abbreviated as CorEx.</li>
<li>For convenience, the following text uses concepts from LDA to
analogize to concepts in CorEx, including that the background is
document topic modeling, a topic is a discrete random variable, and a
document may contain multiple topics, etc.</li>
</ul>
<h1
id="discovering-structure-in-high-dimensional-data-through-correlation-explanation">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</h1>
<h2 id="define-total-correlation">Define Total Correlation</h2>
<ul>
<li><p>The entropy of the discrete random variable <span
class="math inline">\(X\)</span> is defined as</p>
<p><span class="math display">\[
H(X) \equiv \mathbb{E}_{X}[-\log p(x)]
\]</span></p></li>
<li><p>The mutual information between two random variables is defined
as</p>
<p><span class="math display">\[
I(X_1 : X_2) = H\left(X_{1}\right)+H\left(X_{2}\right)-H\left(X_{1},
X_{2}\right)
\]</span></p></li>
<li><p>We define Total Correlation (or Multivariate Mutual Information)
as</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=\sum_{i \in G}
H\left(X_{i}\right)-H\left(X_{G}\right)
\]</span></p></li>
<li><p>Among them, <span class="math inline">\(G\)</span> is a subset of
<span class="math inline">\(X\)</span> . Intuitively, it is the sum of
the entropy of each random variable in the subset minus the joint
entropy of the subset. When there are only two variables in G, TC is
equivalent to the mutual information between the two variables.</p></li>
<li><p>To facilitate understanding, TC can also be expressed in the form
of KL divergence</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=D_{K L}\left(p\left(x_{G}\right) \| \prod_{i \in
G} p\left(x_{i}\right)\right)
\]</span></p></li>
<li><p>The KL divergence between the joint distribution and the product
of the marginal distributions can be seen as TC, so when TC is 0, the KL
divergence is 0, the joint distribution equals the product of the
marginal distributions, which means the internal correlation of the data
is 0, the variables are mutually independent, and the joint distribution
can be factorized into the product of the marginal
distributions.</p></li>
<li><p>Next, we define conditional TC</p>
<p><span class="math display">\[
T C(X | Y)=\sum_{i} H\left(X_{i} | Y\right)-H(X | Y)
\]</span></p></li>
<li><p>Then we can use the difference between TC and conditional TC to
measure the contribution of a certain condition (variable) to the
correlation of the data, the original text states: measure the extent to
which <span class="math inline">\(Y\)</span> explains the correlations
in <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
T C(X ; Y) \equiv T C(X)-T C(X | Y)=\sum_{i \in \mathbb{N}_{n}}
I\left(X_{i} : Y\right)-I(X : Y)
\]</span></p></li>
<li><p>When <span class="math inline">\(T C(X ; Y)\)</span> is
maximized, <span class="math inline">\(T C(X | Y)\)</span> is 0, which
means that the joint distribution of <span
class="math inline">\(X\)</span> can be decomposed given <span
class="math inline">\(Y\)</span> . This implies that <span
class="math inline">\(Y\)</span> explains all the correlation in <span
class="math inline">\(X\)</span> . We believe that a good topic should
be a representation of the document, which explains the document's Total
Correlation to the maximum extent.</p></li>
<li><p>Now we can treat <span class="math inline">\(Y\)</span> as a
latent variable that explains <span class="math inline">\(X\)</span> ,
that is, the topic. Next, we need to determine the topic. In LDA, the
topic is explicitly defined as a word probability distribution, whereas
in CorEx, we define the topic through <span
class="math inline">\(p(Y|X)\)</span> , meaning it is defined as a
discrete random variable that can affect <span
class="math inline">\(X\)</span> , with <span
class="math inline">\(k\)</span> possible values, unlike LDA which
defines <span class="math inline">\(|V|\)</span> possible
values.</p></li>
<li><p>LDA iteratively updates the topic assignment for each word,
thereby indirectly obtaining the document's topic distribution and the
distribution of words over topics. CorEx, however, is different; it
calculates a topic distribution for both documents and words. CorEx
continuously updates the probability of each topic <span
class="math inline">\(p(y_j)\)</span> , the topic distribution of each
word <span class="math inline">\(p(y_j|x_i)\)</span> , the allocation
matrix from words to topic subsets <span
class="math inline">\(\alpha\)</span> , and the topic distribution of
each document <span class="math inline">\(p(y_j|x)\)</span> .</p></li>
<li><p>At initialization, we randomly set <span
class="math inline">\(\alpha\)</span> and the document's topic
distribution <span class="math inline">\(p(y|x)\)</span></p></li>
<li><p>LDA is a generative model, while CorEX is a discriminative
model.</p></li>
</ul>
<h2 id="iteration">Iteration</h2>
<ul>
<li><p>The topic we need to find is</p>
<p><span class="math display">\[
\max _{p(y | x)} T C(X ; Y) \quad \text { s.t. } \quad|Y|=k
\]</span></p></li>
<li><p>We can find m topics and divide <span
class="math inline">\(X\)</span> into m disjoint subsets for
modeling</p>
<p><span class="math display">\[
\max _{G_{j}, p\left(y_{j} | x_{C_{j}}\right)} \sum_{j=1}^{m} T
C\left(X_{G_{j}} ; Y_{j}\right) \quad \text { s.t. }
\quad\left|Y_{j}\right|=k, G_{j} \cap G_{j^{\prime} \neq j}=\emptyset
\]</span></p></li>
<li><p>Rewrite the above equation in terms of mutual information</p>
<p><span class="math display">\[
\max _{G, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i \in G_{j}}
I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X_{G_{j}}\right)
\]</span></p></li>
<li><p>We further simplify this expression using an indicator function,
removing the subscripts of the subset <span
class="math inline">\(G\)</span> , and uniformly representing the
partition results of the subset with a single <span
class="math inline">\(\alpha\)</span> connectivity matrix</p>
<p><span class="math display">\[
\alpha_{i, j}=\mathbb{I}\left[X_{i} \in G_{j}\right] \in\{0,1\}  \\
\max _{\alpha, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X\right) \\
\]</span></p></li>
<li><p>We must also add a constraint to ensure that the subsets do not
intersect</p>
<p><span class="math display">\[
\sum_{\overline{j}} \alpha_{i, \overline{j}}=1
\]</span></p></li>
<li><p>This is an optimization problem with constraints, which can be
solved using the Lagrange multiplier method</p>
<p><span class="math display">\[
\begin{aligned} p\left(y_{j} | x\right) &amp;=\frac{1}{Z_{j}(x)}
p\left(y_{j}\right) \prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
p\left(y_{j} | x_{i}\right) &amp;=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \end{aligned} \\
\]</span></p></li>
<li><p>Note that this is the optimal theme solution obtained under the
confirmation of matrix <span class="math inline">\(\alpha\)</span> , by
relaxing the conditions for the optimal solution, we can obtain the
iterative formula for <span class="math inline">\(\alpha\)</span> after
the theme update</p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\alpha_{i, j}^{* *}=\exp \left(\gamma\left(I\left(X_{i} :
Y_{j}\right)-\max _{\overline{j}} I\left(X_{i} :
Y_{\overline{j}}\right)\right)\right) \\
\]</span></p></li>
</ul>
<h2 id="pseudo-algorithm">Pseudo-algorithm</h2>
<ul>
<li><p>Pseudo-algorithm description as follows</p>
<p> <span class="math display">\[
\text { input : A matrix of size } n_{s} \times n \text { representing }
n_{s} \text { samples of } n \text { discrete random variables } \\
\]</span> </p>
<p> <span class="math display">\[
\text { set } : \text { Set } m, \text { the number of latent variables,
} Y_{j}, \text { and } k, \text { so that }\left|Y_{j}\right|=k  \\
\]</span></p>
<p> <span class="math display">\[
\text { output: Parameters } \alpha_{i, j}, p\left(y_{j} | x_{i}\right),
p\left(y_{j}\right), p\left(y | x^{(l)}\right) \\
\]</span></p>
<p> <span class="math display">\[
\text { for } i \in \mathbb{N}_{n}, j \in \mathbb{N}_{m}, l \in
\mathbb{N}_{n_{s}}, y \in \mathbb{N}_{k}, x_{i} \in \mathcal{X}_{i} \\
\]</span></p>
<p> <span class="math display">\[
\text { Randomly initialize } \alpha_{i, j}, p\left(y | x^{(l)}\right)
\\
\]</span></p>
<p><span class="math display">\[
\text {repeat} \\
\]</span></p>
<p> <span class="math display">\[
\text { Estimate marginals, } p\left(y_{j}\right), p\left(y_{j} |
x_{i}\right) \text { using  } \\
\]</span> </p>
<p><span class="math display">\[
p\left(y_{j} | x_{i}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \\
\]</span></p>
<p> <span class="math display">\[
\text { Calculate } I\left(X_{i} : Y_{j}\right) \text { from marginals;
} \\
\]</span> </p>
<p> <span class="math display">\[
\text { Update } \alpha \text { using  } \\
\]</span> </p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\]</span></p>
<p> <span class="math display">\[
\text { Calculate } p\left(y | x^{(l)}\right), l=1, \ldots, n_{s} \text
{ using } \\
\]</span> </p>
<p><span class="math display">\[
p\left(y_{j} | x\right)=\frac{1}{Z_{j}(x)} p\left(y_{j}\right)
\prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
\]</span></p>
<p> <span class="math display">\[
\text { until convergence; }
\]</span> </p></li>
</ul>
<h1
id="maximally-informative-hierarchical-representations-of-high-dimensional-data">Maximally
Informative Hierarchical Representations of High-Dimensional Data</h1>
<ul>
<li>This paper analyzes the upper and lower bounds of TC, which helps to
further understand the meaning of TC and proposes an optimization method
for a hierarchical high-dimensional data representation that maximizes
information content. The CorEx mentioned earlier can be considered a
special case of this optimization method.</li>
</ul>
<h2 id="upper-and-lower-bounds">Upper and lower bounds</h2>
<ul>
<li><p>Most definitions are similar to the previous ones, more general
in nature. We extend documents and topics to data <span
class="math inline">\(X\)</span> and representation <span
class="math inline">\(Y\)</span> . When the joint probability can be
decomposed, we call <span class="math inline">\(Y\)</span> a
representation of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
p(x, y)=\prod_{j=1}^{m} p\left(y_{j} | x\right) p(x) \\
\]</span></p></li>
<li><p>Thus, the representation of a data set is completely determined
by the representation of the variable domain and the conditional
probability <span class="math inline">\(p(y_j|x)\)</span> .</p></li>
<li><p>Hierarchical representations can be stacked in layers; we define
hierarchical representation as:</p>
<p><span class="math display">\[
Y^{1 : r} \equiv Y^{1}, \ldots, Y^{r}
\]</span></p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/07/31/eYYvRK.png" alt="eYYvRK.png" />
<figcaption aria-hidden="true">eYYvRK.png</figcaption>
</figure></li>
<li><p>The <span class="math inline">\(Y^k\)</span> represents <span
class="math inline">\(Y^{k-1}\)</span> . We mainly focus on the upper
and lower bounds of the informativeness of hierarchical representations
for data. This type of hierarchical representation is a general
representation, including RBM and autoencoders, etc.</p></li>
<li><p>Definition:</p>
<p><span class="math display">\[
T C_{L}(X ; Y) \equiv \sum_{i=1}^{n} I\left(Y :
X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} : X\right) \\
\]</span></p></li>
<li><p>There exist the following boundaries and decompositions:</p>
<p><span class="math display">\[
T C(X) \geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)
\]</span></p></li>
<li><p>Then you get a lower bound of <span
class="math inline">\(Y\)</span> relative to <span
class="math inline">\(X\)</span> TC value:</p>
<p><span class="math display">\[
T C(X ; Y) \geq T C_{L}(X ; Y)
\]</span></p></li>
<li><p>When <span class="math inline">\(TC(Y)\)</span> is 0, the lower
bound is obtained, at which point <span class="math inline">\(Y\)</span>
are mutually independent and do not contain any information about <span
class="math inline">\(X\)</span> . Extending the inequality above to the
hierarchical representation, we can obtain</p>
<p><span class="math display">\[
T C(X) \geq \sum_{k=1}^{r} T C_{L}\left(Y^{k-1} ; Y^{k}\right)
\]</span></p></li>
<li><p>Attention here is that we define the 0th layer as <span
class="math inline">\(X\)</span> , and we can also find the upper
bound</p>
<p><span class="math display">\[
T C(X) \leq \sum_{k=1}^{r}\left(T C_{L}\left(Y^{k-1} ;
Y^{k}\right)+\sum_{i=1}^{m_{k-1}} H\left(Y_{i}^{k-1} |
Y^{k}\right)\right)
\]</span></p></li>
<li><p>The difference between the upper and lower bounds is a pile of
accumulated conditional entropy.</p></li>
<li><p>The lower and upper bounds of TC can help measure the extent of
interpretation for the data</p>
<h2 id="analysis">Analysis</h2></li>
<li><p>Consider the simplest case first, where the first layer
represents a single variable <span class="math inline">\(Y^{1} \equiv
Y_{1}^{1}\)</span></p>
<p><span class="math display">\[
TC(Y)+TC_L(X;Y)=TC(X;Y) \leq TC(X) \leq TC_L(X;Y)+\sum _{i=1}^{m_0}
H(X_i|Y)
\]</span></p></li>
<li><p>To be supplemented</p>
<h2 id="optimization">Optimization</h2></li>
<li><p>We can optimize layer by layer, so that each layer maximally
explains the correlations in the layer below, which can be achieved by
optimizing the lower bound, taking the first layer as an example</p>
<p><span class="math display">\[
\max _{\forall j, p\left(y_{j}^{1} | x\right)} T C_{L}\left(X ;
Y^{1}\right)
\]</span></p></li>
<li><p>Define the ancestor information as</p>
<p><span class="math display">\[
A I_{\alpha}(X ; Y) \equiv \sum_{i=1}^{n} \alpha_{i} I\left(Y :
X_{i}\right)-I(Y : X) \\
\alpha_{i} \in[0,1] \\
\]</span></p></li>
<li><p>If given a certain <span class="math inline">\(\alpha\)</span> ,
whose <span class="math inline">\(AI_{\alpha}\)</span> is positive, it
implies the existence of common ancestors for some ( <span
class="math inline">\(\alpha\)</span> -dependent) set of <span
class="math inline">\(X_i\)</span> s in any DAG that describes <span
class="math inline">\(X\)</span> , here it is not quite understood, but
it can be seen as a generalization of the aforementioned connected
matrix <span class="math inline">\(\alpha\)</span> , generalizing from
binarization to the 01 interval. The optimization problem can be
represented by <span class="math inline">\(AI_{\alpha}\)</span> and can
be written as</p>
<p><span class="math display">\[
\max _{p(y | x)} \sum_{i=1}^{n} \alpha_{i} I\left(Y : X_{i}\right)-I(Y :
X)
\]</span></p></li>
<li><p>The form has been transformed into the same as the previous
paragraph, and the subsequent solution is also the same</p>
<p><span class="math display">\[
p(y | x)=\frac{1}{Z(x)} p(y) \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}
\]</span></p></li>
<li><p>Taking the logarithmic expectation of the normalized denominator
<span class="math inline">\(Z(x)\)</span> yields the free energy, which
is precisely our optimization objective</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}[\log Z(x)] &amp;=\mathbb{E}\left[\log
\frac{p(y)}{p(y | x)} \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}\right] \\ &amp;=\sum_{i=1}^{n}
\alpha_{i} I\left(Y : X_{i}\right)-I(Y : X) \end{aligned}
\]</span></p></li>
<li><p>For multiple latent variables, the author reconstructed the lower
bound and similarly extended <span class="math inline">\(\alpha\)</span>
to continuous values in the 01 interval. The specific process is
relatively complex, and the final optimization objective changed from
maximizing the <span class="math inline">\(TC_L(X;Y)\)</span> of all
latent units to optimizing the lower bounds of <span
class="math inline">\(p(y_j|x)\)</span> and <span
class="math inline">\(\alpha\)</span> .</p>
<p><span class="math display">\[
\max _{\alpha_{i, j}, p\left(y_{j} | x\right) \atop c_{i,
j}\left(\alpha_{i, j}\right)=0}^{m} \sum_{j=1}^m \left(\sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-I\left(Y_{j} : X\right)\right)
\]</span></p></li>
<li><p>Defined the relationship between <span
class="math inline">\(X_i\)</span> and <span
class="math inline">\(Y_j\)</span> , i.e., the structure. As for
optimizing the structure, the ideal situation is</p>
<p><span class="math display">\[
\alpha _{i,j} = \mathbb{I} [j = argmax _{j} I(X_i : Y_j)]
\]</span></p></li>
<li><p>This structure is rigidly connected, with each node only
connected to a specific hidden layer node in the next layer. Based on
<span class="math inline">\(I(Y_j : X_i | Y_{1:j-1}) \geq \alpha _{i,j}
I(Y_j : X_i)\)</span> , the authors propose a heuristic algorithm to
estimate <span class="math inline">\(\alpha\)</span> . We verify whether
<span class="math inline">\(X_i\)</span> correctly estimates <span
class="math inline">\(Y_j\)</span> .</p>
<p><span class="math display">\[
d_{i,j}^l \equiv \mathbb{I} [argmax_{y_j} \log p(Y_j = y_j|x^{(l)}) =
argmax_{y_j} \log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]
\]</span></p></li>
<li><p>Afterward, we summed up over all the samples, counted the number
of correct estimates, and set the <span
class="math inline">\(\alpha\)</span> value according to the
proportion.</p></li>
</ul>
<h1
id="anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</h1>
<h2 id="abstract-1">Abstract</h2>
<ul>
<li><p>This paper formally applies CorEx to topic modeling, emphasizing
the advantages compared to LDA.</p>
<ul>
<li>No structural assumptions need to be made for the data, and compared
to LDA, CorEX has fewer hyperparameters</li>
<li>Different from LDA, it can be generalized to hierarchical models and
semi-supervised models without any structural modifications to the
model</li>
</ul></li>
<li><p>The iterative process of the model still follows these steps:</p>
<p><span class="math display">\[
p_t(y_j) = \sum _{\overline{x}} p_t(y_j | \overline{x})p(\overline{x})
\\
\]</span></p>
<p><span class="math display">\[
p_t(x_i | y_j) = \sum _{\overline{x}}
p_t(y_j|\overline{x})p(\overline{x}) \mathbb{I} [\overline{x}_i =
x_i]/p_t(y_j) \\
\]</span></p>
<p><span class="math display">\[
\log p_{t+1} (y_j | x^l) = \log p_t(y_j) + \sum _{i=1}^n \alpha _{i,j}^t
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \log \mathbb{Z} _j (x^l) \\
\]</span></p></li>
<li><p>Due to the use of bag-of-words information and the processing of
sparse matrices, the calculation of edge probabilities and conditional
probabilities is very fast. The slowest step in the iteration is the
third formula, which is to calculate the topic distribution of all
documents. We rewrite the summation of logarithmic terms in this
formula:</p>
<p><span class="math display">\[
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \log
\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \log
(\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})
\]</span></p></li>
<li><p>The cumulative calculation is performed for each document,
computing the likelihood over the entire dictionary, however, only a
small portion of the words in the dictionary appear in each document.
When a word in the dictionary does not appear in the document, only the
first term in the above formula is not zero; when the word does appear,
the term <span class="math inline">\(\log
P(X_i^l=1|y_j)/p(X_i^l=1)\)</span> is zero, with the remaining terms
retained, thus the author prioritizes the assumption that the word is
not present in the document and then updates and supplements the
probability terms for those words that do appear. After such
optimization, the calculation speed of CorEx is similar to that of
LDA.</p></li>
<li><p>The greatest benefit of this optimization is that the
computational complexity is only linearly related to the number of
documents and the number of topics, thus making it possible to compute
over large-scale documents with large-scale topics.</p>
<h2 id="semi-supervised">Semi-supervised</h2></li>
<li><p>Some value in the weight matrix can be fixed. The normal <span
class="math inline">\(\alpha\)</span> is in the 01 interval, and the
anchor of the i-th word in the j-th topic can be set to <span
class="math inline">\(\beta _{i,j}\)</span>, where <span
class="math inline">\(\beta\)</span> is the strength of the
anchor.</p></li>
<li><p>This approach can assign an anchor word to each topic, with one
or more words as an anchor, offering great flexibility.</p></li>
<li><p>In business terms, the advantages of CorEx lie in:</p>
<ul>
<li>Extremely fast in training with a very large number of topics.</li>
<li>It is convenient to anchor words to adapt to the field.</li>
<li>The themes in CorEx are non-overlapping; there will be no repeated
themes</li>
</ul></li>
<li><p>Iterative hierarchical topics are based on the hierarchical
method of the previous paper, hierarchical topics can be used to
aggregate concepts and divide subtopics.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="概述">概述</h1>
<ul>
<li>Correlation
Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation
Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total
Correlation之差来找出最能explain数据的Correlation的主题。
其中一种快速计算方法就简写为CorEx。</li>
<li>为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。</li>
</ul>
<h1
id="discovering-structure-in-high-dimensional-data-through-correlation-explanation">Discovering
Structure in High-Dimensional Data Through Correlation Explanation</h1>
<h2 id="定义total-correlation">定义Total Correlation</h2>
<ul>
<li><p>定义<span
class="math inline">\(X\)</span>为一离散随机变量，则其熵为</p>
<p><span class="math display">\[
H(X) \equiv \mathbb{E}_{X}[-\log p(x)]
\]</span></p></li>
<li><p>两个随机变量之间的互信息定义为</p>
<p><span class="math display">\[
I(X_1 : X_2) = H\left(X_{1}\right)+H\left(X_{2}\right)-H\left(X_{1},
X_{2}\right)
\]</span></p></li>
<li><p>我们定义Total Correlation（或者叫多元互信息multivariate mutual
information）为</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=\sum_{i \in G}
H\left(X_{i}\right)-H\left(X_{G}\right)
\]</span></p></li>
<li><p>其中<span class="math inline">\(G\)</span>是<span
class="math inline">\(X\)</span>的一个子集。直观来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。</p></li>
<li><p>为了更方便理解，TC还可以写成KL散度的形式</p>
<p><span class="math display">\[
T C\left(X_{G}\right)=D_{K L}\left(p\left(x_{G}\right) \| \prod_{i \in
G} p\left(x_{i}\right)\right)
\]</span></p></li>
<li><p>也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。</p></li>
<li><p>接着我们定义conditional TC</p>
<p><span class="math display">\[
T C(X | Y)=\sum_{i} H\left(X_{i} | Y\right)-H(X | Y)
\]</span></p></li>
<li><p>那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure
the extent to which <span class="math inline">\(Y\)</span> explains the
correlations in <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
T C(X ; Y) \equiv T C(X)-T C(X | Y)=\sum_{i \in \mathbb{N}_{n}}
I\left(X_{i} : Y\right)-I(X : Y)
\]</span></p></li>
<li><p><span class="math inline">\(T C(X ; Y)\)</span>最大时，<span
class="math inline">\(T C(X | Y)\)</span>为0，也就是已知<span
class="math inline">\(Y\)</span>时<span
class="math inline">\(X\)</span>的联合分布可分解，也就说明<span
class="math inline">\(Y\)</span> explains all the correlation in <span
class="math inline">\(X\)</span>。我们认为好的主题应当是文档的一种表示，其解释的文档Total
Correlation应该最大。</p></li>
<li><p>现在我们就可以把<span
class="math inline">\(Y\)</span>看成时解释<span
class="math inline">\(X\)</span>的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过<span
class="math inline">\(p(Y|X)\)</span>来定义主题，也就是说只将其定义为一个能够影响<span
class="math inline">\(X\)</span>的离散随机变量，取值范围有<span
class="math inline">\(k\)</span>种可能，而不像LDA定义为<span
class="math inline">\(|V|\)</span>种取值可能。</p></li>
<li><p>LDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率<span
class="math inline">\(p(y_j)\)</span>，每个词的主题分布<span
class="math inline">\(p(y_j|x_i)\)</span>，词到主题子集合的分配矩阵<span
class="math inline">\(\alpha\)</span>，以及每篇文档的主题分布<span
class="math inline">\(p(y_j|x)\)</span><br />
</p></li>
<li><p>初始化时，我们随机设定<span
class="math inline">\(\alpha\)</span>以及文档的主题分布<span
class="math inline">\(p(y|x)\)</span></p></li>
<li><p>LDA是生成式模型，而CorEX是判别式模型。</p></li>
</ul>
<h2 id="迭代">迭代</h2>
<ul>
<li><p>我们要找到的主题是</p>
<p><span class="math display">\[
\max _{p(y | x)} T C(X ; Y) \quad \text { s.t. } \quad|Y|=k
\]</span></p></li>
<li><p>我们可以找m个主题，并将<span
class="math inline">\(X\)</span>分为m个不相交的子集来建模</p>
<p><span class="math display">\[
\max _{G_{j}, p\left(y_{j} | x_{C_{j}}\right)} \sum_{j=1}^{m} T
C\left(X_{G_{j}} ; Y_{j}\right) \quad \text { s.t. }
\quad\left|Y_{j}\right|=k, G_{j} \cap G_{j^{\prime} \neq j}=\emptyset
\]</span></p></li>
<li><p>将上式用互信息改写为</p>
<p><span class="math display">\[
\max _{G, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i \in G_{j}}
I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X_{G_{j}}\right)
\]</span></p></li>
<li><p>我们用指示函数进一步简化这个式子，去掉子集<span
class="math inline">\(G\)</span>的下标，统一用一个<span
class="math inline">\(\alpha\)</span>连通矩阵来代表子集的划分结果</p>
<p><span class="math display">\[
\alpha_{i, j}=\mathbb{I}\left[X_{i} \in G_{j}\right] \in\{0,1\}  \\
\max _{\alpha, p\left(y_{j} | x\right)} \sum_{j=1}^{m} \sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} :
X\right) \\
\]</span></p></li>
<li><p>同时我们要加一个限制项保证子集不相交</p>
<p><span class="math display">\[
\sum_{\overline{j}} \alpha_{i, \overline{j}}=1
\]</span></p></li>
<li><p>这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出</p>
<p><span class="math display">\[
\begin{aligned} p\left(y_{j} | x\right) &amp;=\frac{1}{Z_{j}(x)}
p\left(y_{j}\right) \prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
p\left(y_{j} | x_{i}\right) &amp;=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \end{aligned} \\
\]</span></p></li>
<li><p>注意，这是在<span
class="math inline">\(\alpha\)</span>矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后<span
class="math inline">\(\alpha\)</span>的迭代公式</p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\alpha_{i, j}^{* *}=\exp \left(\gamma\left(I\left(X_{i} :
Y_{j}\right)-\max _{\overline{j}} I\left(X_{i} :
Y_{\overline{j}}\right)\right)\right) \\
\]</span></p></li>
</ul>
<h2 id="伪算法">伪算法</h2>
<ul>
<li><p>伪算法描述如下</p>
<p><span class="math display">\[
\text { input : A matrix of size } n_{s} \times n \text { representing }
n_{s} \text { samples of } n \text { discrete random variables } \\
\]</span></p>
<p><span class="math display">\[
\text { set } : \text { Set } m, \text { the number of latent variables,
} Y_{j}, \text { and } k, \text { so that }\left|Y_{j}\right|=k  \\
\]</span></p>
<p><span class="math display">\[
\text { output: Parameters } \alpha_{i, j}, p\left(y_{j} | x_{i}\right),
p\left(y_{j}\right), p\left(y | x^{(l)}\right) \\
\]</span></p>
<p><span class="math display">\[
\text { for } i \in \mathbb{N}_{n}, j \in \mathbb{N}_{m}, l \in
\mathbb{N}_{n_{s}}, y \in \mathbb{N}_{k}, x_{i} \in \mathcal{X}_{i} \\
\]</span></p>
<p><span class="math display">\[
\text { Randomly initialize } \alpha_{i, j}, p\left(y | x^{(l)}\right)
\\
\]</span></p>
<p><span class="math display">\[
\text {repeat} \\
\]</span></p>
<p><span class="math display">\[
\text { Estimate marginals, } p\left(y_{j}\right), p\left(y_{j} |
x_{i}\right) \text { using  } \\
\]</span></p>
<p><span class="math display">\[
p\left(y_{j} | x_{i}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \delta_{\overline{x}_{i}, x_{i}} /
p\left(x_{i}\right) \text { and }
p\left(y_{j}\right)=\sum_{\overline{x}} p\left(y_{j} |
\overline{x}\right) p(\overline{x}) \\
\]</span></p>
<p><span class="math display">\[
\text { Calculate } I\left(X_{i} : Y_{j}\right) \text { from marginals;
} \\
\]</span></p>
<p><span class="math display">\[
\text { Update } \alpha \text { using  } \\
\]</span></p>
<p><span class="math display">\[
\alpha_{i, j}^{t+1}=(1-\lambda) \alpha_{i, j}^{t}+\lambda \alpha_{i,
j}^{* *} \\
\]</span></p>
<p><span class="math display">\[
\text { Calculate } p\left(y | x^{(l)}\right), l=1, \ldots, n_{s} \text
{ using } \\
\]</span></p>
<p><span class="math display">\[
p\left(y_{j} | x\right)=\frac{1}{Z_{j}(x)} p\left(y_{j}\right)
\prod_{i=1}^{n}\left(\frac{p\left(y_{j} |
x_{i}\right)}{p\left(y_{j}\right)}\right)^{\alpha_{i, j}} \\
\]</span></p>
<p><span class="math display">\[
\text { until convergence; }
\]</span></p></li>
</ul>
<h1
id="maximally-informative-hierarchical-representations-of-high-dimensional-data">Maximally
Informative Hierarchical Representations of High-Dimensional Data</h1>
<ul>
<li>本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。</li>
</ul>
<h2 id="上界和下界">上界和下界</h2>
<ul>
<li><p>大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据<span
class="math inline">\(X\)</span>和表示<span
class="math inline">\(Y\)</span>，当联合概率可以分解时，我们称<span
class="math inline">\(Y\)</span>是<span
class="math inline">\(X\)</span>的一种表示</p>
<p><span class="math display">\[
p(x, y)=\prod_{j=1}^{m} p\left(y_{j} | x\right) p(x) \\
\]</span></p></li>
<li><p>这样，一种数据的表示完全由表示变量域和条件概率<span
class="math inline">\(p(y_j|x)\)</span>决定。</p></li>
<li><p>表示可以层次性堆叠，我们定义层次表示为：</p>
<p><span class="math display">\[
Y^{1 : r} \equiv Y^{1}, \ldots, Y^{r}
\]</span></p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/07/31/eYYvRK.png" alt="eYYvRK.png" />
<figcaption aria-hidden="true">eYYvRK.png</figcaption>
</figure></li>
<li><p>其中<span class="math inline">\(Y^k\)</span>是<span
class="math inline">\(Y^{k-1}\)</span>的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。</p></li>
<li><p>定义：</p>
<p><span class="math display">\[
T C_{L}(X ; Y) \equiv \sum_{i=1}^{n} I\left(Y :
X_{i}\right)-\sum_{j=1}^{m} I\left(Y_{j} : X\right) \\
\]</span></p></li>
<li><p>则存在以下的边界和分解：</p>
<p><span class="math display">\[
T C(X) \geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)
\]</span></p></li>
<li><p>同时得到<span class="math inline">\(Y\)</span>关于<span
class="math inline">\(X\)</span>的TC值的一个下界：</p>
<p><span class="math display">\[
T C(X ; Y) \geq T C_{L}(X ; Y)
\]</span></p></li>
<li><p>当<span
class="math inline">\(TC(Y)\)</span>为0时取到下界，这时<span
class="math inline">\(Y\)</span>之间相互独立，不包含关于<span
class="math inline">\(X\)</span>的信息。将上面<span
class="math inline">\(TC(X)\)</span>的不等式扩展到层次表示，则可以得到</p>
<p><span class="math display">\[
T C(X) \geq \sum_{k=1}^{r} T C_{L}\left(Y^{k-1} ; Y^{k}\right)
\]</span></p></li>
<li><p>注意在这里我们定义第0层表示就是<span
class="math inline">\(X\)</span>，我们还能找到上界</p>
<p><span class="math display">\[
T C(X) \leq \sum_{k=1}^{r}\left(T C_{L}\left(Y^{k-1} ;
Y^{k}\right)+\sum_{i=1}^{m_{k-1}} H\left(Y_{i}^{k-1} |
Y^{k}\right)\right)
\]</span></p></li>
<li><p>可以看到上界与下界之间就差了一堆累加的条件熵。</p></li>
<li><p>TC的上下界可以帮助衡量表示对于数据的解释程度，</p>
<h2 id="分析">分析</h2></li>
<li><p>先考虑最简单的情况，即第一层表示只有一个变量<span
class="math inline">\(Y^{1} \equiv Y_{1}^{1}\)</span>，这时</p>
<p><span class="math display">\[
TC(Y)+TC_L(X;Y)=TC(X;Y) \leq TC(X) \leq TC_L(X;Y)+\sum _{i=1}^{m_0}
H(X_i|Y)
\]</span></p></li>
<li><p>待补充</p>
<h2 id="优化">优化</h2></li>
<li><p>我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly
explain the correlations in the layer
below)，这可以通过优化下界得到，以第一层为例</p>
<p><span class="math display">\[
\max _{\forall j, p\left(y_{j}^{1} | x\right)} T C_{L}\left(X ;
Y^{1}\right)
\]</span></p></li>
<li><p>定义<span class="math inline">\(\alpha\)</span>祖先信息为</p>
<p><span class="math display">\[
A I_{\alpha}(X ; Y) \equiv \sum_{i=1}^{n} \alpha_{i} I\left(Y :
X_{i}\right)-I(Y : X) \\
\alpha_{i} \in[0,1] \\
\]</span></p></li>
<li><p>假如给定某个<span class="math inline">\(\alpha\)</span>，其<span
class="math inline">\(AI_{\alpha}\)</span>为正，则 it implies the
existence of common ancestors for some (<span
class="math inline">\(\alpha\)</span>-dependent) set of <span
class="math inline">\(X_i\)</span> ’s in any DAG that describes <span
class="math inline">\(X\)</span>，这里不太懂，但可以看成是上文联通矩阵<span
class="math inline">\(\alpha\)</span>的泛化版本，从binarize泛化到01区间。最优化问题用<span
class="math inline">\(AI_{\alpha}\)</span>表示可以写成</p>
<p><span class="math display">\[
\max _{p(y | x)} \sum_{i=1}^{n} \alpha_{i} I\left(Y : X_{i}\right)-I(Y :
X)
\]</span></p></li>
<li><p>化成了和上文一样的形式，之后的解法也一样</p>
<p><span class="math display">\[
p(y | x)=\frac{1}{Z(x)} p(y) \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}
\]</span></p></li>
<li><p>对归一化分母<span
class="math inline">\(Z(x)\)</span>取对数期望，可以得到自由能量，这正是我们的优化目标</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}[\log Z(x)] &amp;=\mathbb{E}\left[\log
\frac{p(y)}{p(y | x)} \prod_{i=1}^{n}\left(\frac{p\left(y |
x_{i}\right)}{p(y)}\right)^{\alpha_{i}}\right] \\ &amp;=\sum_{i=1}^{n}
\alpha_{i} I\left(Y : X_{i}\right)-I(Y : X) \end{aligned}
\]</span></p></li>
<li><p>对于多个隐变量，作者重构了下界，同样将<span
class="math inline">\(\alpha\)</span>扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的<span
class="math inline">\(TC_L(X;Y)\)</span>变为优化<span
class="math inline">\(p(y_j|x)\)</span>和<span
class="math inline">\(\alpha\)</span>的下界：</p>
<p><span class="math display">\[
\max _{\alpha_{i, j}, p\left(y_{j} | x\right) \atop c_{i,
j}\left(\alpha_{i, j}\right)=0}^{m} \sum_{j=1}^m \left(\sum_{i=1}^{n}
\alpha_{i, j} I\left(Y_{j} : X_{i}\right)-I\left(Y_{j} : X\right)\right)
\]</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span>定义了<span
class="math inline">\(X_i\)</span>和<span
class="math inline">\(Y_j\)</span>之间的关系，即结构。至于优化结构，理想的情况是</p>
<p><span class="math display">\[
\alpha _{i,j} = \mathbb{I} [j = argmax _{j} I(X_i : Y_j)]
\]</span></p></li>
<li><p>这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于<span
class="math inline">\(I(Y_j : X_i | Y_{1:j-1}) \geq \alpha _{i,j} I(Y_j
: X_i)\)</span>,作者提出了一种启发式的算法来估计<span
class="math inline">\(\alpha\)</span>。我们检查<span
class="math inline">\(X_i\)</span>是否正确估计<span
class="math inline">\(Y_j\)</span></p>
<p><span class="math display">\[
d_{i,j}^l \equiv \mathbb{I} [argmax_{y_j} \log p(Y_j = y_j|x^{(l)}) =
argmax_{y_j} \log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]
\]</span></p></li>
<li><p>之后我们在所有样本上累加，统计正确估计数目，并根据比例设置<span
class="math inline">\(\alpha\)</span>值。</p></li>
</ul>
<h1
id="anchored-correlation-explanation-topic-modeling-with-minimal-domain-knowledge">Anchored
Correlation Explanation: Topic Modeling with Minimal Domain
Knowledge</h1>
<h2 id="概述-1">概述</h2>
<ul>
<li><p>本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：</p>
<ul>
<li>不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数</li>
<li>不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型</li>
</ul></li>
<li><p>模型的迭代依然是这几步：</p>
<p><span class="math display">\[
p_t(y_j) = \sum _{\overline{x}} p_t(y_j | \overline{x})p(\overline{x})
\\
\]</span></p>
<p><span class="math display">\[
p_t(x_i | y_j) = \sum _{\overline{x}}
p_t(y_j|\overline{x})p(\overline{x}) \mathbb{I} [\overline{x}_i =
x_i]/p_t(y_j) \\
\]</span></p>
<p><span class="math display">\[
\log p_{t+1} (y_j | x^l) = \log p_t(y_j) + \sum _{i=1}^n \alpha _{i,j}^t
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \log \mathbb{Z} _j (x^l) \\
\]</span></p></li>
<li><p>由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：</p>
<p><span class="math display">\[
\log \frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \log
\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \log
(\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})
\]</span></p></li>
<li><p>这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中<span
class="math inline">\(\log
P(X_i^l=1|y_j)/p(X_i^l=1)\)</span>为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。</p></li>
<li><p>这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。</p>
<h2 id="半监督">半监督</h2></li>
<li><p>半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的<span
class="math inline">\(\alpha\)</span>在01区间之间，而将第i号词anchor在第j号主题可以将<span
class="math inline">\(\alpha_{i,j} = \beta _{i,j}\)</span>，其中<span
class="math inline">\(\beta\)</span>是anchor的强度。</p></li>
<li><p>这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。</p></li>
<li><p>在业务上来说，CorEx的优势在于：</p>
<ul>
<li>在训练超大规模主题数时非常快。</li>
<li>可以方便的anchor词以适应领域。</li>
<li>CorEx的主题之间词是不相交的，不会出现重复主题</li>
</ul></li>
<li><p>层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>corex</tag>
        <tag>math</tag>
        <tag>topic model</tag>
      </tags>
  </entry>
  <entry>
    <title>Lagrange,KKT,PCA,SVM</title>
    <url>/2017/03/18/Lagrange/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/681b34e5ff4f1b7bf5780a5eb7a984ce.png" width="500"/></p>
<p>Introduction of the Lagrange multiplier method and its extension KKT
conditions, as well as their applications in PCA and SVM</p>
<span id="more"></span>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0olwj.png"
alt="i0olwj.png" /> Image from Wikipedia's illustrative introduction to
the Lagrange multiplier method</p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="lagrange-multiplier-method">Lagrange multiplier method</h1>
<ul>
<li><p>Lagrange multiplier method is a method for finding extrema under
constraints, described as</p>
<p><span class="math display">\[
under the constraint condition g(x,y)=c \\
find the extremum of f(x,y) \\
\]</span></p>
<p>The main idea is to synthesize the constraint conditions and the
original function into a single function, convert it into an
unconstrained condition, and then find the partial derivatives to obtain
the extremum.</p></li>
<li><p>It can be seen from the figure that points with equal values of
the function f can form blue rings similar to contour lines, with the
constraint conditions represented by the green path. The problem can be
transformed into finding the point along the green path where the blue
ring associated with that point is either closest to the center or
farthest from the center (maximum or minimum value).</p></li>
<li><p>Clearly, the point of tangency between the green path and the
blue loop attains an extremum, at which their gradients (arrows) are
parallel, described as</p>
<p><span class="math display">\[
\nabla f (x, y) = \nabla (\lambda \left(g \left(x, y \right) - c
\right))
\]</span></p>
<p><span class="math inline">\(\lambda\)</span> is the Lagrange
multiplier, representing the size multiple of the two parallel gradients
in this formula, with the sign indicating that the two gradients are in
opposite directions. The Lagrange multiplier is not equal to 0. The
Lagrange equation is given by <span class="math inline">\(F(x,y)=\nabla
\Big[f \left(x, y \right) + \lambda \left(g \left(x, y \right) - c
\right) \Big]\)</span></p></li>
<li><p>Solving the above equation yields a set of <span
class="math inline">\((x,y,\lambda)\)</span> , which are the critical
points and the Lagrange multipliers at the points of extremum. At this
time, the Lagrange equations <span
class="math inline">\(F(x,y)=f(x,y)\)</span> hold, because when the
extremum is achieved, the constraint condition part must be 0 (we are
moving along the constraint condition to find the tangent points, and
the tangent points are on the constraint path).</p></li>
<li><p>Lagrange coefficients refer to the maximum growth value, <span
class="math inline">\(-\frac{\partial \Lambda}{\partial {c_k}} =
\lambda_k\)</span></p></li>
</ul>
<h1 id="carroll-kuhn-tucker-condition">Carroll-Kuhn-Tucker
condition</h1>
<ul>
<li><p>If the constraint conditions are not only equations but also
include inequality constraints, it is necessary to generalize the
Lagrange multiplier method to the KKT conditions</p></li>
<li><p>The problem of optimization with inequality constraints is
described as</p>
<p><span class="math display">\[
under the constraint condition: \\
h_j(X)=0 j=1,2,...,p \\
g_k(X)\leq 0 k=1,2,...q \\
find the extremum of f(X) \\
\]</span></p></li>
<li><p>Lagrange function is</p>
<p><span class="math display">\[
L(X,\lambda ,\mu)=f(X)+\sum _{j=1}^p \lambda _j h_j(X) + \sum _{k=1}^q
\mu g_k(X)
\]</span></p></li>
<li><p>KKT conditions are given by:</p>
<p><span class="math display">\[
\frac{dL}{dX}=0 \\
\lambda _j \neq 0 \\
\mu _k \geq 0 \\
\mu _k g_k(X)=0 \\
h_j(X)=0 \\
g_k(X) \leq 0\\
\]</span></p></li>
</ul>
<h1 id="pca">PCA</h1>
<ul>
<li><p>PCA stands for Principal Component Analysis, which optimizes the
original dimensions of the data to form a set of new dimensions. These
new dimensions are linear combinations of the original dimensions and
are mutually independent. They are sorted by importance, with one
dimension being understood as a column in a data matrix, where each row
represents a sample</p></li>
<li><p><span class="math inline">\(x_1,...,x_p\)</span> represents the
original p dimensions, and the new dimensions are <span
class="math inline">\(\xi _1,....,\xi _p\)</span></p></li>
<li><p>New dimensions are linear combinations of the original
dimensions, represented as</p>
<p><span class="math display">\[
\xi _i = \sum _{j=1}^{p}  \alpha _{ij} x_j = \alpha _i^T x
\]</span></p></li>
<li><p>In order to unify the scales across all new dimensions, the
vector length of the coefficients of the linear combination of each new
dimension is set to 1, i.e.,</p>
<p><span class="math display">\[
\alpha _i^T \alpha _i=1
\]</span></p></li>
<li><p>Let A be the feature transformation matrix, composed of the
coefficient vectors of the linear combinations of the new dimensions,
then it is necessary to solve for an optimal orthogonal transformation A
such that the variance of the new dimensions reaches an extremum. An
orthogonal transformation ensures that the new dimensions are
uncorrelated; the greater the variance, the more distinct the samples
are in the new dimensions, which facilitates our data
classification.</p></li>
<li><p>The problem is then transformed into an optimization problem with
constraints, where the constraint is <span class="math inline">\(\alpha
_i^T \alpha _i=1\)</span> , and the extremum of <span
class="math inline">\(var(\xi _i)\)</span> is to be found, which can be
solved using the Lagrange multiplier method</p></li>
<li><p>When i=1, we obtain the first new dimension, which is also the
most important (with the largest variance), and then set i=2, adding the
constraint condition <span class="math inline">\(E[\xi _2 \xi _1\-E[\xi
_1][\xi _2]]=0\)</span> , that is, the two new dimensions are
uncorrelated, and obtain the second new dimension</p></li>
<li><p>Sequentially determine p new dimensions</p></li>
<li><p>PCA can optimize the original data, identify dimensions with
discriminative power, and more importantly, if there are correlations
among the dimensions of the original data, PCA can eliminate these
correlations. Even if the correlations in the original data are low, if
we only take the first k (k &lt; q) new dimensions, we can perform
dimensionality reduction with minimal loss of precision, greatly
shortening the training time of the data</p></li>
<li><p>If we take the first k new dimensions and perform the inverse
operation of PCA on them, we can achieve data denoising, because the new
dimensions with very low importance generally reflect the random noise
in the data. By discarding them and restoring the original data, we
achieve the removal of noise</p></li>
<li><p>PCA is unsupervised, does not consider the category or label of
the samples themselves, and is not necessarily the optimal solution in
supervised learning; feature extraction for classification objectives
can be achieved using the K-L transform</p></li>
</ul>
<h1 id="pca-using-covariance-matrix">PCA using Covariance Matrix</h1>
<ul>
<li>The above method for solving PCA is too complicated; it can be
solved through the covariance matrix in practice (because there are
efficient matrix eigenvalue decomposition algorithms)</li>
<li>The optimization objective of PCA is to reselect a set of bases
(features) such that the covariance between different features is 0
under the representation of this set of bases, and the variance within
the same feature is maximized</li>
<li>The problem can be restated as, for the data tensor X,
zero-centering by feature columns (a total of m features) (simplifying
the calculation of covariance), calculate the covariance matrix <span
class="math inline">\(C=\frac 1m X^T X\)</span> . It is hoped to find a
set of bases P such that the covariance matrix D of the transformed data
<span class="math inline">\(Y=PX\)</span> is diagonal, with non-diagonal
elements (covariances) equal to 0. If the diagonal elements (variances)
are arranged from large to small, then taking the first k rows of the P
matrix can reduce the feature dimension from m to k. It is easy to
obtain <span class="math inline">\(D=PCP^T\)</span> , and since C is a
real symmetric matrix, the problem then transforms into diagonalizing
the real symmetric matrix C, and the new set of bases we need are the
eigenvector set.</li>
<li>Algorithm:
<ul>
<li><p>There are m sets of n-dimensional data arranged into a matrix X
with n rows and m columns</p></li>
<li><p>Zero-mean normalization of each row of X</p></li>
<li><p>Determine the covariance matrix C</p></li>
<li><p>Determine the eigenvalues and eigenvectors of the covariance
matrix</p></li>
<li><p>Arrange the feature vectors according to the size of their
eigenvalues from largest to smallest in rows, forming a new basis matrix
P</p></li>
<li><p>If necessary, take the first k rows of P, and the
reduced-dimensional data is <span
class="math inline">\(Y=PX\)</span></p>
<pre><code>def PCA(X, dims):
m = len(X)
mean = np.mean(X, axis=0)
X = X - mean
C = np.dot(X.T, X) / m
Eigen_Value, Eigen_Vector = np.linalg.eig(C)
index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]
PCA_Vector = Eigen_Vector[index]
X_PCA = np.dot(PCA_Vector, X.T)
return X_PCA.T</code></pre></li>
</ul></li>
</ul>
<h1 id="support-vector-machine">Support Vector Machine</h1>
<ul>
<li><p>In classification learning, we need to find a separating
hyperplane that separates samples of different categories, and the best
separating hyperplane is obviously one that is as far away as possible
from the samples of each category, i.e., the hyperplane with the best
tolerance to local perturbations of the training samples</p></li>
<li><p>The hyperplane is described by the equation <span
class="math inline">\(w^Tx+b=0\)</span> , where w is the normal vector
determining the direction of the hyperplane, and b is the displacement
of the hyperplane to the far point</p></li>
<li><p>Solving an SVM, i.e., finding a solution that satisfies the
constraints</p>
<p><span class="math display">\[
\begin{cases}
w^Tx_i+b \geq +1, y_i=+1 \\
w^Tx_i+b \leq -1, y_i=-1 \\
\end{cases}
\]</span></p>
<p>Under the given conditions, to maximize the distance <span
class="math inline">\(\frac{2}{||w||}\)</span> between two different
support vectors to the hyperplane, which can be rewritten as an
optimization problem</p>
<p><span class="math display">\[
min_{w,b} \frac 12 {||w||}^2 \\
s.t. y_i(w^Tx_i+b) \geq 1,i=1,2,...,m \\
\]</span></p>
<p>Derivation can be found in the other two blog posts: Machine Learning
Notes and Statistical Learning Method Notes Handwritten Version</p></li>
<li><p>For this optimization problem, its Lagrange equation is</p>
<p><span class="math display">\[
L(w,b,\alpha )=\frac 12 {||w||}^2+\sum _{i=1}^{m} \alpha _i
(1-y_i(w^Tx_i+b))
\]</span></p>
<p>The term <span class="math inline">\(\alpha\)</span> is the Lagrange
multiplier, taking the partial derivatives of the equation with respect
to w and b, obtaining the dual problem</p>
<p><span class="math display">\[
max _{\alpha } \sum _{i=1}^m \alpha _i -\frac 12 \sum _{i=1}^m \sum
_{j=1}^m \alpha _i \alpha _j y_i y_j x_i^T x_j \\
s.t. \sum _{i=1}^m \alpha _i y_i=0, \\
\alpha _i \geq 0,i=1,2,...,m \\
\]</span></p>
<p>The above equation satisfies the KKT conditions, and is solved by the
SMO algorithm</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="拉格朗日乘子法">拉格朗日乘子法</h1>
<ul>
<li><p>拉格朗日乘子法是一种求约束条件下极值的方法，描述为</p>
<p><span class="math display">\[
在约束条件g(x,y)=c下 \\
求函数f(x,y)的极值 \\
\]</span></p>
<p>其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。</p></li>
<li><p>由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。</p></li>
<li><p>显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为</p>
<p><span class="math display">\[
\nabla f (x, y) = \nabla (\lambda \left(g \left(x, y \right) - c
\right))
\]</span></p>
<p><span
class="math inline">\(\lambda\)</span>是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。
拉格朗日方程即$ F(x,y)=$</p></li>
<li><p>求解上面的式子，就得到一组<span
class="math inline">\((x,y,\lambda)\)</span>，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程<span
class="math inline">\(F(x,y)=f(x,y)\)</span>，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。</p></li>
<li><p>拉格朗日系数的含义是最大增长值，<span
class="math inline">\(-\frac{\partial \Lambda}{\partial {c_k}} =
\lambda_k\)</span></p></li>
</ul>
<h1 id="卡罗需-库恩-塔克条件">卡罗需-库恩-塔克条件</h1>
<ul>
<li><p>如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KKT条件</p></li>
<li><p>包含不等约束的极值问题描述为</p>
<p><span class="math display">\[
在约束条件: \\
h_j(X)=0 j=1,2,...,p \\
g_k(X)\leq 0 k=1,2,...q \\
求函数f(X)的极值 \\
\]</span></p></li>
<li><p>拉格朗日函数为</p>
<p><span class="math display">\[
L(X,\lambda ,\mu)=f(X)+\sum _{j=1}^p \lambda _j h_j(X) + \sum _{k=1}^q
\mu g_k(X)
\]</span></p></li>
<li><p>KKT条件为:</p>
<p><span class="math display">\[
\frac{dL}{dX}=0 \\
\lambda _j \neq 0 \\
\mu _k \geq 0 \\
\mu _k g_k(X)=0 \\
h_j(X)=0 \\
g_k(X) \leq 0\\
\]</span></p></li>
</ul>
<h1 id="pca">PCA</h1>
<ul>
<li><p>PCA即Principal Component
Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本</p></li>
<li><p>记<span
class="math inline">\(x_1,...,x_p\)</span>为原始p个维度，新维度是<span
class="math inline">\(\xi _1,....,\xi _p\)</span></p></li>
<li><p>新维度是原始维度的线性组合，表示为</p>
<p><span class="math display">\[
\xi _i = \sum _{j=1}^{p}  \alpha _{ij} x_j = \alpha _i^T x
\]</span></p></li>
<li><p>为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即</p>
<p><span class="math display">\[
\alpha _i^T \alpha _i=1
\]</span></p></li>
<li><p>令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类</p></li>
<li><p>此时问题就转化为具有约束条件的极值问题，约束条件为<span
class="math inline">\(\alpha _i^T \alpha _i=1\)</span>，求<span
class="math inline">\(var(\xi
_i)\)</span>的极值，可以用拉格朗日乘子法求解</p></li>
<li><p>当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件<span
class="math inline">\(E[\xi _2 \xi _1\-E[\xi _1][\xi
_2]]=0\)</span>即两个新维度不相关，求出第二个新维度</p></li>
<li><p>依次求出p个新维度</p></li>
<li><p>PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k&lt;q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间</p></li>
<li><p>如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除</p></li>
<li><p>PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取</p></li>
</ul>
<h1 id="pca-using-covariance-matrix">PCA using Covariance Matrix</h1>
<ul>
<li>上述求解PCA的方法太过于麻烦，在实际中可以通过协方差矩阵来求解（因为有高效的矩阵特征分解算法）</li>
<li>PCA的优化目标是，重新选取一组基（特征），使得数据在这组基表示下，不同特征之间的协方差为0，同一特征内的数据方差最大化</li>
<li>可以将问题转述为，对数据张量X，按特征列（共m个特征）零均值化（化简协方差的计算），计算协方差矩阵<span
class="math inline">\(C=\frac 1m X^T
X\)</span>，希望求得一组基P，使得特征变换后的数据<span
class="math inline">\(Y=PX\)</span>的协方差矩阵D是对角阵，非对角元素（协方差）为0.若对角元素（方差）按从大到小排列，这时我们取P矩阵的前k行就可以将特征维度从m降到k。易得<span
class="math inline">\(D=PCP^T\)</span>，且C为实对称阵，那么问题就转变为对实对称C对角化，我们需要的新的一组基就是特征向量组。</li>
<li>算法：
<ul>
<li><p>有m条n维数据，排成n行m列矩阵X</p></li>
<li><p>将X的每一行进行零均值化</p></li>
<li><p>求出协方差矩阵C</p></li>
<li><p>求出协方差矩阵的特征值和特征向量</p></li>
<li><p>将特征向量按特征值大小对应从大到小按行排列，组成新的基矩阵P</p></li>
<li><p>如果需要将为，取P的前k行即可，降维后的数据为<span
class="math inline">\(Y=PX\)</span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def PCA(X, dims):</span><br><span class="line">m = len(X)</span><br><span class="line">mean = np.mean(X, axis=0)</span><br><span class="line">X = X - mean</span><br><span class="line">C = np.dot(X.T, X) / m</span><br><span class="line">Eigen_Value, Eigen_Vector = np.linalg.eig(C)</span><br><span class="line">index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]</span><br><span class="line">PCA_Vector = Eigen_Vector[index]</span><br><span class="line">X_PCA = np.dot(PCA_Vector, X.T)</span><br><span class="line">return X_PCA.T</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h1 id="svm">SVM</h1>
<ul>
<li><p>在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面</p></li>
<li><p>划分超平面通过方程<span
class="math inline">\(w^Tx+b=0\)</span>描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移</p></li>
<li><p>求解一个SVM，即找到满足约束</p>
<p><span class="math display">\[
\begin{cases}
w^Tx_i+b \geq +1, y_i=+1 \\
w^Tx_i+b \leq -1, y_i=-1 \\
\end{cases}
\]</span></p>
<p>的条件下，使得两个异类支持向量到超平面的距离<span
class="math inline">\(\frac{2}{||w||}\)</span>最大
这可以重写为最优化问题</p>
<p><span class="math display">\[
min_{w,b} \frac 12 {||w||}^2 \\
s.t. y_i(w^Tx_i+b) \geq 1,i=1,2,...,m \\
\]</span></p>
<p>推导见另两篇博文：机器学习笔记和统计学习方法笔记手写版</p></li>
<li><p>对于这个最优化问题，它的拉格朗日方程是</p>
<p><span class="math display">\[
L(w,b,\alpha )=\frac 12 {||w||}^2+\sum _{i=1}^{m} \alpha _i
(1-y_i(w^Tx_i+b))
\]</span></p>
<p>其中<span
class="math inline">\(\alpha\)</span>是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题</p>
<p><span class="math display">\[
max _{\alpha } \sum _{i=1}^m \alpha _i -\frac 12 \sum _{i=1}^m \sum
_{j=1}^m \alpha _i \alpha _j y_i y_j x_i^T x_j \\
s.t. \sum _{i=1}^m \alpha _i y_i=0, \\
\alpha _i \geq 0,i=1,2,...,m \\
\]</span></p>
<p>上式满足KKT条件，通过SMO算法求解</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Linear Algebra 1</title>
    <url>/2017/01/21/LinearAlgebra1/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png" width="500"/></p>
<h1
id="first-lecture-geometric-interpretation-of-systems-of-equations">First
Lecture: Geometric Interpretation of Systems of Equations</h1>
<ul>
<li>From three perspectives to view the system of equations: row graph,
column graph, matrix</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li>For example, for the system of equations:</li>
</ul>
<p><span class="math display">\[
\begin{cases}
2x-y=0\\
-x+2y=3\\
\end{cases}
\]</span></p>
<h2 id="image-processing">Image processing</h2>
<ul>
<li>Image of the line:</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; -1 \\
-1 &amp; 2 \\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
3 \\
\end{bmatrix}
\]</span></p>
<ul>
<li>Can also be written as</li>
</ul>
<p><span class="math display">\[
Ax=b
\]</span></p>
<ul>
<li>The intersection point of two lines in a two-dimensional plane is
the solution to the equation, and this is generalized to n dimensions as
the intersection point of n lines in an n-dimensional plane</li>
</ul>
<h2 id="list-images">List images</h2>
<ul>
<li><p>List of images:</p>
<p><span class="math display">\[
x
\begin{bmatrix}
2  \\
-1 \\
\end{bmatrix}
+y
\begin{bmatrix}
-1 \\
2 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
3 \\
\end{bmatrix}
\]</span></p></li>
<li><p>The solution to the equation is the linear combination
coefficients of the vector set, under which the vectors combine to form
the target vector</p></li>
</ul>
<h2 id="matrix">Matrix</h2>
<ul>
<li>Consider the list of images, different x, y values can lead to
different linear combinations. Does there exist a solution for any b, x?
Or can the linear combination of these two vectors cover the entire
space? Or are these two (or n) vectors linearly independent?</li>
<li>If so, then the matrix composed of these two (or n) vectors is
called a nonsingular matrix, which is invertible; otherwise, it is
called a singular matrix, which is not invertible</li>
</ul>
<h1
id="second-lecture-elimination-back-substitution-and-replacement">Second
Lecture: Elimination, Back Substitution, and Replacement</h1>
<h2 id="elimination">Elimination</h2>
<ul>
<li><p>Consider the system of equations <span class="math display">\[
\begin{cases}
  x+2y+z=2\\
  3x+8y+z=12\\
  4y+z=2\\
  \end{cases}
\]</span></p></li>
<li><p>His A matrix is</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
3 &amp; 8 &amp; 1  \\
0 &amp; 4 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>After row transformation becomes</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
0 &amp; 2 &amp; -2  \\
0 &amp; 4 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>Re-transformed into</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
0 &amp; 2 &amp; -2  \\
0 &amp; 0 &amp; 5  \\
\end{bmatrix}
\]</span></p></li>
<li><p>This series of transformations is the elimination method</p></li>
<li><p>The rule of transformation is that the ith element of the ith row
is set as the pivot (p), and through row transformations, the elements
before the pivot in each row are successively eliminated, so that matrix
A becomes matrix U (upper triangular matrix)</p></li>
<li><p>Matrix</p>
<p><span class="math display">\[
\left[\begin{array}{c|c}
A &amp; X \\
\end{array}\right]
\]</span></p></li>
<li><p>Augmented matrix. Applying the same transformation to b yields
c.</p></li>
</ul>
<h2 id="back-substitution">Back-substitution</h2>
<ul>
<li>Solving the equation Ax=b is equivalent to solving the equation
Ux=c, and solving Ux=c is very easy to obtain the solution, taking the
three-term equation as an example</li>
<li>Because U is an upper triangular matrix, z can be easily
obtained</li>
<li>Substitute z into the second row to find y</li>
<li>Substitute z, y into the first row to find x</li>
<li>This process is called back substitution</li>
</ul>
<h2 id="replacement">Replacement</h2>
<p><span class="math display">\[
\begin{bmatrix}
a &amp; b &amp; c  \\
\end{bmatrix}*A
\]</span></p>
<ul>
<li>The meaning of this expression is to obtain a row matrix whose
values are a times the first row of A, b times the second row of A, and
c times the third row of A</li>
</ul>
<p>Similarly <span class="math display">\[
A*\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\]</span></p>
<ul>
<li>The meaning of this expression is to obtain a column matrix whose
values are a times column 1 of A plus b times column 2 of A plus c times
column 3 of A</li>
<li>It can be deduced that the matrix obtained by swapping two rows of
matrix A is</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
0 &amp; 1  \\
1 &amp; 0  \\
\end{bmatrix}*A
\]</span></p>
<ul>
<li>The matrix with columns A exchanged is</li>
</ul>
<p><span class="math display">\[
A*\begin{bmatrix}
0 &amp; 1  \\
1 &amp; 0  \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>The multiplication with a matrix completes row and column
transformations; such a matrix is called a permutation matrix</p></li>
<li><p>In elimination, the row and column transformations required to
eliminate the element at the i-th row and j-th column are represented as
a permutation matrix, denoted as <span
class="math inline">\(E_{ij}\)</span></p></li>
<li><p>Elimination can be written as</p>
<p><span class="math display">\[
E_{32}E_{31}E_{21}A=U
\]</span></p></li>
</ul>
<h1 id="third-lecture-multiplication-and-inverse-matrices">Third
Lecture: Multiplication and Inverse Matrices</h1>
<h2 id="matrix-multiplication">Matrix multiplication</h2>
<ul>
<li><p>Consider matrix multiplication</p>
<p><span class="math display">\[
A*B=C
\]</span></p></li>
<li><p>First Algorithm: Dot Product <span
class="math inline">\(C_{ij}=\sum_iA_{ik}B_{kj}\)</span></p></li>
<li><p>Second Algorithm: Viewed as matrix multiplication by a vector,
the C column is a linear combination of the A columns, with the
combination coefficients in the B matrix. For example, the elements in
each row of the first column of B are the linear combination
coefficients of the individual columns in A. After linear combination,
the first column of C is obtained</p></li>
<li><p>Third algorithm: Viewed as a vector multiplying a matrix, the C
row is a linear combination of the B row, with the combination
coefficients in the A matrix; for example, each element in the first row
of the A matrix is the linear combination coefficient for each row in B,
and after the linear combination, the first row of C is
obtained</p></li>
<li><p>Fourth algorithm: Multiply a column of A with a row of B to
obtain a submatrix, and the sum of all submatrices is C</p></li>
<li><p>Fifth Algorithm: Matrix Blocking Algorithm</p></li>
</ul>
<h2 id="invertible-matrix">Invertible matrix</h2>
<ul>
<li>For the inverse matrix <span class="math inline">\(A^{-1}\)</span> ,
there exists <span class="math inline">\(AA^{-1}=I\)</span> , I is the
identity matrix</li>
<li>The inverse matrix on the left and the inverse matrix on the right
are the same</li>
<li>If there exists a non-zero matrix X such that <span
class="math inline">\(AX=0\)</span> , then A is not invertible</li>
<li>Gaussian Jordan idea for finding the inverse matrix: Treat A|I as an
augmented matrix, and when transforming A to I, I is correspondingly
transformed to the inverse matrix of A
<ul>
<li><p>Proof:</p>
<p><span class="math display">\[
EA=I \\
E=A^{-1} \\
EI=A^{-1} \\
\]</span></p></li>
</ul></li>
</ul>
<h1 id="fourth-lecture-lu-decomposition-of-a">Fourth Lecture: LU
Decomposition of A</h1>
<h2 id="lu-decomposition">LU decomposition</h2>
<ul>
<li><p><span
class="math inline">\((AB)^{-1}=B^{-1}A^{-1}\)</span></p></li>
<li><p>The transpose matrix of A, denoted as <span
class="math inline">\(A^T\)</span> , can be easily obtained</p>
<p><span class="math display">\[
AA^{-1}=I \\
(A^{-1})^TA^T=I \\
所以(A^T)^{-1}=(A^{-1})^T \\
\]</span></p></li>
<li><p>For a single matrix, transpose and inverse can be
interchanged</p></li>
<li><p>Matrix Decomposition: A = LU, where U is transformed back into A
through a series of permutation matrices, and L is the cumulative
permutation matrix. Taking a 3x3 matrix as an example</p>
<p><span class="math display">\[
E_{32}E_{31}E_{21}A=U \\
所以可得L: \\
L=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\
\]</span></p></li>
<li><p>Why study A=LU rather than EA=U: Because if there are no row
transformations, the elimination coefficients can be directly written
into L; conversely, if E is studied, the operation of the nth row is
related to the operation of the already eliminated (n-1)th row, and the
elimination coefficients cannot be written down intuitively</p></li>
</ul>
<h2 id="elimination-consumption">Elimination Consumption</h2>
<ul>
<li><p>A single multiplication and a single subtraction in the
elimination process consume one element at a time (the unit of
consumption is the product and subtraction of numbers rather than the
product and subtraction of rows), with the total consumption being</p>
<p><span class="math display">\[
\sum_{i=1}^{n}i*(i-1) \approx \sum_{i=1}^{n}i^2 \approx \frac 13 n^3
\]</span></p></li>
</ul>
<h2 id="群">群</h2>
<ul>
<li>For example, using a 3*3 unitary matrix, there are a total of 6
(i.e., permutation matrices)</li>
<li>For these matrices, <span
class="math inline">\(P^{-1}=P^T\)</span></li>
<li>The permutations and inverses of these 6 matrices still lie within
these 6 matrices, and are called a group</li>
<li>There are n! row permutation matrices for an n*n matrix</li>
</ul>
<h1 id="lecture-5-transposition-permutation-vector-space-r">Lecture 5:
Transposition, Permutation, Vector Space R</h1>
<h2 id="换">换</h2>
<ul>
<li>Replacement matrix is used to perform row exchanges</li>
<li>A = LU, L has 1s on the diagonal, below which are the elimination
multipliers, and U has zeros below the diagonal</li>
<li>PA=LU is used to describe LU decomposition with row
permutations</li>
<li>P(Permutation permutation matrix) is a unit matrix with rows
rearranged, and there are n! permutations of n*n permutation matrices,
which is the number of rearrangements of the rows. They are all
invertible, and finding the inverse is equivalent to finding the
transpose</li>
</ul>
<h2 id="transpose">Transpose</h2>
<ul>
<li>Row exchange is equivalent to transpose, denoted as <span
class="math inline">\(A^T\)</span> , <span
class="math inline">\(A_{ij}=A_{ji}^T\)</span></li>
<li><span class="math inline">\((AB)^T=B^TA^T\)</span></li>
<li>Symmetric matrix (symmetric), <span
class="math inline">\(A^T=A\)</span></li>
<li>For any matrix A, <span class="math inline">\(AA^T\)</span> is
always symmetric, because <span
class="math inline">\((A^TA)^T=(A^TA^{TT})=(A^TA)\)</span></li>
</ul>
<h2 id="vector-space">Vector Space</h2>
<ul>
<li>Vectors can be added and subtracted, and can be dotted</li>
<li><strong>A space represents a set of vectors, not all vectors, and a
vector space is constrained, requiring the condition of closure under
linear combinations</strong></li>
<li>For example, <span class="math inline">\(R^2\)</span> represents the
two-dimensional vector space of all real numbers</li>
<li>Any vector in a vector space remains within the vector space after
linear combination, thus the vector space must contain (0,0)</li>
<li>Not an example of a vector space: Only the first quadrant of <span
class="math inline">\(R^2\)</span> is taken, vector addition within any
space remains within the space, but scalar multiplication is not
necessarily so (it can be multiplied by a negative number), and vector
spaces are closed</li>
<li>A straight line passing through the origin within <span
class="math inline">\(R^2\)</span> can be called the vector subspace
<span class="math inline">\(R^2\)</span> , which still satisfies the
property of self-closedness (addition, subtraction, and scalar
multiplication)</li>
<li>What are the subspaces of <span class="math inline">\(R^2\)</span> ?
<ul>
<li>Itself</li>
<li>A straight line extending infinitely on both sides beyond the origin
(note that this is different from <span
class="math inline">\(R^1\)</span> )</li>
<li>(0,0), abbreviated as Z</li>
</ul></li>
<li>What are the subspaces of <span class="math inline">\(R^3\)</span> ?
<ul>
<li>Itself</li>
<li>A straight line extending infinitely on both sides beyond the origin
(note that this is different from <span
class="math inline">\(R^1\)</span> )</li>
<li>Infinite Plane Beyond Zero Point</li>
<li>(0,0,0)</li>
</ul></li>
</ul>
<h2 id="through-matrix-construction-into-quantum-space">Through matrix
construction into quantum space</h2>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 3  \\
2 &amp; 3  \\
4 &amp; 1  \\
\end{bmatrix}
\]</span></p>
<ul>
<li>Each column belongs to <span class="math inline">\(R^3\)</span> ,
and any linear combination (scalar multiplication and addition) of these
two columns should be in the subspace, which is called the column space,
denoted as C(A). In three-dimensional space, this column space is a
plane, passing through these two column vectors and (0,0,0) <img data-src="http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png"
alt="mark" /></li>
</ul>
<h1 id="lecture-6-column-space-and-null-space">Lecture 6: Column Space
and Null Space</h1>
<h2 id="list-space">List space</h2>
<ul>
<li><p>The previous lecture mentioned two subspaces, the plane P and the
line L. <span class="math inline">\(P \bigcup L\)</span> is not a
subspace, <span class="math inline">\(P \bigcap L\)</span> is a
subspace.</p></li>
<li><p>For any subspaces S, T, <span class="math inline">\(S \bigcap
T\)</span> is a subspace</p></li>
<li><p>Give an example</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\]</span></p></li>
<li><p>C(A) is the <span class="math inline">\(R^4\)</span> subspace,
and the linear combination of these three column vectors yields the
subspace</p></li>
<li><p>The subspace is related to the linear system below</p></li>
<li><p>Are there solutions for Ax=b for any b? How can b make x have a
solution?</p>
<ul>
<li>The answer to the former is no, because with four equations and
three unknowns, the linear combination of three column vectors cannot
fill the entire <span class="math inline">\(R^4\)</span> space, i.e.,
the column space cannot fill the entire four-dimensional space</li>
<li>The latter responds that obviously b=(0,0,0,0) is an answer, and
b=(1,2,3,4) is also an answer, i.e., first write any solution
(x1,x2,x3), and the calculated b is the b that makes x have a solution,
which is equivalent to only when b is in the column space of A, x has a
solution</li>
</ul></li>
<li><p>If we remove the third column, we can still obtain the same
column space, because these three columns are not linearly independent;
the third column is the sum of the first two columns. At this point, we
call the first two columns the main columns, so the column space in this
case is a two-dimensional subspace</p></li>
</ul>
<h2 id="zero-space">Zero Space</h2>
<ul>
<li><p>The null space (null space) is completely different from the
column space; the null space of A contains all solutions x to Ax =
0</p></li>
<li><p>Column space concerns A, the null space concerns x (in the case
where b=0), in the example above, the column space is a subspace of
four-dimensional space, and the null space is a subspace of
three-dimensional space</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\begin{bmatrix}
X_1 \\
X_2 \\
X_3 \\
\end{bmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Clearly, the zero space contains (0,0,0) and (1,1,-1), and these
two vectors determine a line (c,c,-c), so this line is the zero
space</p></li>
<li><p>Why can the zero space be called a space (satisfying the closure
property of vector spaces?): namely, to prove that for any two solutions
of Ax=0, their linear combination is still a solution. Because:
...matrix multiplication can be expanded... the distributive
law...</p></li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\begin{bmatrix}
X_1 \\
X_2 \\
X_3 \\
\end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
3 \\
4 \\
\end{bmatrix}
\]</span></p>
<ul>
<li>We replaced b, and the solution is (1,0,0). Are there other
solutions? If there are, can these solutions form a subspace?</li>
<li>Clearly not a subspace, as the solution does not contain (0,0,0),
which does not satisfy the basic conditions of a vector space, as in
this case, the two solutions (1,0,0), (0,-1,1), but the linear
combination of these vectors does not pass through the origin, and they
cannot form a vector space. Therefore, the discussion of the solution
space or null space is based on the condition that b=0.</li>
<li>Two methods of constructing subspaces are the column space and the
null space
<ul>
<li>From several vectors, a subspace is obtained through linear
combination</li>
<li>From a system of equations, obtain a subspace by making x satisfy
specific conditions</li>
</ul></li>
</ul>
<h1 id="lecture-7-main-variables-special-solutions">Lecture 7: Main
Variables, Special Solutions</h1>
<h2 id="main-variable">Main Variable</h2>
<ul>
<li><p>How to Solve Ax=0 with Algorithms</p></li>
<li><p>Give an example:</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
2 &amp; 4 &amp; 6 &amp; 8  \\
3 &amp; 6 &amp; 8 &amp; 10  \\
\end{bmatrix}
\]</span></p></li>
<li><p>The third line is the sum of the first and second lines; they are
linearly related, which will be reflected in the elimination process
later</p></li>
<li><p>Elimination does not change the set of equations, because
elimination modifies the column space, but not the solution
space</p></li>
<li><p>After the first elimination, only the leading element in the
first row of the first column is non-zero</p></li>
</ul>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>At this point, due to linear correlation between the second and
third columns, the pivot of the second row has shifted to the third
column, and the elimination process continues</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}=U
\]</span></p></li>
<li><p>If we separate the non-zero elements from the zeros, we obtain a
step line, with the number of steps being the number of leading elements
(non-zero), which is 2 in this case, and we call this the rank of the
matrix (the number of equations remaining after elimination), the
columns containing the leading elements are called leading columns
(1,3), and the remaining columns are free columns (2,4)</p></li>
<li><p>Now we can solve Ux=0 and perform back substitution</p></li>
<li><p>The solutions corresponding to the free columns are the free
variables x2, x4, which can be arbitrarily chosen. After selection, the
main variables x1, x3 corresponding to the main columns can be solved
out by back substitution</p></li>
</ul>
<h2 id="special-solution">Special Solution</h2>
<ul>
<li>In this case, if we take x2=1, x4=0, we can obtain x=(-2,1,0,0), and
(-2,1,0,0) multiplied by any real number is still a solution, thus
determining a line. However, is this line the solution space? No.
Because we have two free variables, we can determine more than one line,
for example, taking x2=0, x4=1, we can obtain x=(2,0,-2,1).</li>
<li>So the algorithm first eliminates variables, obtaining the leading
column and the free column, then assigns values (1,0) to the free
variables to complete the solution (-2,1,0,0), and then assigns another
set of values (0,1) to the free variables to obtain another complete
solution (2,0,-2,1).</li>
<li>Two special values for the free variables (one being 1, the rest
being 0, with none being 0, as that would result in a complete solution
that is all zeros) yield two sets of solutions, which are called
particular solutions. Based on the particular solutions, we can obtain
the solution space: the linear combination of the two particular
solutions, a*(-2,1,0,0) + b*(2,0,-2,1)</li>
<li>r represents the number of principal variables, i.e., the number of
principal elements, and only r equations are active. The m*n matrix A
has n-r free variables</li>
</ul>
<h2 id="simplified-row-ladder-form">Simplified row ladder form</h2>
<ul>
<li><p>U can be further simplified</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>In the reduced row echelon form (RREF), all entries above the
leading pivot are also 0</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; -2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>And the leading element must be made into 1, because b=0, so the
second row can be directly divided by 2</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; -2  \\
0 &amp; 0 &amp; 1 &amp; 2  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}=R
\]</span></p></li>
<li><p>Simplified row 阶梯 form contains all the information of the
matrix in its simplest form</p></li>
<li><p>The unit matrix is located at the intersection of the main row
and the main column</p></li>
<li><p>An extremely simplified system of equations is obtained: Rx = 0
(columns can be arbitrarily interchanged), F represents free columns</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I &amp; F \\
0 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p>Among them, I is the unit matrix (principal column), F is the matrix
corresponding to the free columns, R has r rows, I has r columns, and F
has n-r columns</p></li>
</ul>
<h2 id="zero-space-matrix">Zero-space matrix</h2>
<ul>
<li><p>Zero space matrix, whose columns are composed of particular
solutions, denoted as N, it can be seen that if there are a number of
free variables, then N has a columns, and if there are no free
variables, N does not exist, x has only a unique solution or no
solution</p>
<p><span class="math display">\[
R*N=0
\]</span></p>
<p><span class="math display">\[
N=\begin{bmatrix}
-F \\
I  \\
\end{bmatrix}
\]</span></p></li>
<li><p>The entire equation can be written as</p>
<p><span class="math display">\[
\begin{bmatrix}
I &amp; F \\
\end{bmatrix}
\begin{bmatrix}
x_{pivot} \\
x_{free}  \\
\end{bmatrix}=0
\]</span></p>
<p><span class="math display">\[
x_{pivot}=-F
\]</span></p></li>
</ul>
<h2 id="take-an-example-to-go-through-the-algorithm-again">Take an
example to go through the algorithm again</h2>
<ul>
<li><p>Original Matrix</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 4 &amp; 6 \\
2 &amp; 6 &amp; 8 \\
2 &amp; 8 &amp; 10 \\
\end{bmatrix}
\]</span></p></li>
<li><p>First elimination</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 2 \\
0 &amp; 4 &amp; 4 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Second elimination (perform a row swap to make the second pivot
element in the second row)</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 2 &amp; 2 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{bmatrix}=U
\]</span></p></li>
<li><p>Clearly, r=2, 1 degree of freedom, let the degree of freedom be
1, obtain the particular solution x</p>
<p><span class="math display">\[
x=\begin{bmatrix}
-1 \\
-1 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Zero space is denoted as cx, a straight line, with x being the
basis of the zero space</p></li>
<li><p>Next, continue to simplify U</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{bmatrix}=R=
\begin{bmatrix}
I &amp; F  \\
0 &amp; 0  \\
0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
</ul>
<p><span class="math display">\[
F=\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}=U
\]</span></p>
<p><span class="math display">\[
x=\begin{bmatrix}
-F \\
I  \\
\end{bmatrix}=N
\]</span></p>
<h1 id="lecture-8-solvability-and-the-structure-of-solutions">Lecture 8:
Solvability and the Structure of Solutions</h1>
<h2 id="solvability">Solvability</h2>
<p><span class="math display">\[
\begin{cases}
x_1+2x_2+2x_3+2x_4=b_1\\
2x_1+4x_2+6x_3+8x_4=b_2\\
3x_1+6x_2+8x_3+10x_4=b_3\\
\end{cases}
\]</span></p>
<ul>
<li><p>Written in the expanded matrix form:</p>
<p><span class="math display">\[
\left[\begin{array}{c c c c|c}
1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\
2 &amp; 4 &amp; 6 &amp; 8 &amp; b_2 \\
3 &amp; 6 &amp; 8 &amp; 10 &amp; b_3 \\
\end{array}\right]
\]</span></p></li>
<li><p>Elimination yields:</p>
<p><span class="math display">\[
\left[\begin{array}{c c c c|c}
1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\
0 &amp; 0 &amp; 2 &amp; 4 &amp; b_2-2b_1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3-b_2-b_1 \\
\end{array}\right]
\]</span></p></li>
<li><p>The first and third columns are the main columns, while the
second and fourth columns are free columns</p></li>
<li><p>Solvability: What conditions must b satisfy when there is a
solution? The easily obtainable condition is that b must be in the
column space of A</p></li>
<li><p><strong>If the linear combination of each row of A results in 0,
what conditions must b satisfy? Then, the same combination of elements
in b must also be zero</strong></p></li>
<li><p>How to find all solutions of Ax=b?</p>
<ul>
<li><p>First step: Find a particular solution, set all free variables to
0, and solve for all principal variables. In the example, <span
class="math inline">\(x_2和x_4\)</span> is set to 0, which yields <span
class="math inline">\(x_1和x_3\)</span> to be -2 and 1.5,
respectively.</p></li>
<li><p>The second step: The complete solution is a particular solution
plus any vector in the null space</p></li>
<li><p><span class="math inline">\(Ax_{particular}=b \\ Ax_{nullspace}=0
\\ A(x_{particular}+x_{nullspace})=b\)</span></p></li>
<li><p>In this case, the particular solution is (-2,0,1.5,0), and the
solutions in the null space are (-2,1,0,0) and (2,0,-2,1).</p></li>
<li><p>Complete solution is:</p>
<p><span class="math display">\[
x_{complete}=
\begin{bmatrix}
-2 \\
0 \\
1.5 \\
0 \\
\end{bmatrix}+
c_1\begin{bmatrix}
-2 \\
1\\
0 \\
0 \\
\end{bmatrix}+
c_2\begin{bmatrix}
2 \\
0 \\
-2 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>If its image is plotted in a four-dimensional space with four
solutions as axes, it forms a plane, similar to a subspace translating
from the zero point to a particular solution point</p></li>
</ul></li>
</ul>
<h2 id="structure-of-the-explanation">Structure of the Explanation</h2>
<ul>
<li><p>Consider an m*n matrix of rank r, where r &lt;= m and r &lt;= n,
with the case when r is full rank, i.e., r = min(m, n)</p></li>
<li><p>Rank full: r=n</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I \\
0 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Rank full: r = m &lt; n, at this time, no zero row will appear
during elimination, and for any b, Ax = b has a solution. There are n -
r, i.e., n - m, free variables. At this time, the form of r is</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I &amp; F \\
\end{bmatrix}
\]</span></p></li>
<li><p>When r=m=n, A is a reversible matrix, R=I, N(A)={0}, and the
system Ax=b has a solution for any b, and the solution is
unique</p></li>
</ul>
<h2
id="an-explanation-from-a-netizens-perspective-of-the-vector-space">An
explanation from a netizen's perspective of the vector space</h2>
<blockquote>
<p>When the dimension r occupied by the vectors equals the number of
vectors n and also equals the dimension m of the ambient space, these
vectors can be combined to form any vector within the ambient space,
that is, for any value of b, there is always a solution. However, since
all vectors must be combined together to reach any coordinate point in
the entire ambient space, the stretching of each vector must be a
specific amount, meaning x has only one solution. When the dimension r
occupied by the vectors equals the dimension m of the ambient space but
is less than the number of vectors n, that is, the stretching and
combination of some vectors in A can reach any coordinate point in the
ambient space, there exist free vectors here. Regardless of the position
b takes in the space, you can first arbitrarily stretch your free vector
to obtain a new vector, and then combine this new vector with the part
of vectors that can completely reach the ambient space through a
specific contraction to obtain vector b. As long as the stretching
amount of the free vector changes, the contraction amount of the other
vectors must also change, so X has infinitely many solutions. (Expressed
in terms of the x formula, this means you can use the stretching and
combination of some vectors in A (m primitive vectors) to obtain b (this
is the particular solution) and then use m primitive vectors and the
other n-m free vectors to arbitrarily form a zero vector, thus obtaining
infinitely many sets of x.) When the dimension occupied by the vectors
equals the number of vectors but is less than the dimension of the
ambient space, that is, the vectors in A can only cover a subspace of
the ambient space but there are free vectors in this subspace, then if b
is within this subspace, the situation is the same as the second point,
X has infinitely many solutions; if b is outside the subspace, X cannot
be contracted to reach it, so there is no solution.</p>
</blockquote>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li>例如对方程组：</li>
</ul>
<p><span class="math display">\[\begin{cases}
2x-y=0\\
-x+2y=3\\
\end{cases}
\]</span></p>
<h2 id="行图像">行图像</h2>
<ul>
<li>行图像为：</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; -1 \\
-1 &amp; 2 \\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
3 \\
\end{bmatrix}
\]</span></p>
<ul>
<li>也可以写成</li>
</ul>
<p><span class="math display">\[
Ax=b
\]</span></p>
<ul>
<li>即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点</li>
</ul>
<h2 id="列图像">列图像</h2>
<ul>
<li><p>列图像为：</p>
<p><span class="math display">\[
x
\begin{bmatrix}
2  \\
-1 \\
\end{bmatrix}
+y
\begin{bmatrix}
-1 \\
2 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
3 \\
\end{bmatrix}
\]</span></p></li>
<li><p>方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量</p></li>
</ul>
<h2 id="矩阵">矩阵</h2>
<ul>
<li>现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？</li>
<li>如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular
matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆</li>
</ul>
<h1 id="第二讲消元回代和置换">第二讲：消元、回代和置换</h1>
<h2 id="消元">消元</h2>
<ul>
<li><p>考虑方程组 <span class="math display">\[\begin{cases}
x+2y+z=2\\
3x+8y+z=12\\
4y+z=2\\
\end{cases}
\]</span></p></li>
<li><p>他的A矩阵为</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
3 &amp; 8 &amp; 1  \\
0 &amp; 4 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>经过行变换后为</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
0 &amp; 2 &amp; -2  \\
0 &amp; 4 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>再变换为</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 1  \\
0 &amp; 2 &amp; -2  \\
0 &amp; 0 &amp; 5  \\
\end{bmatrix}
\]</span></p></li>
<li><p>这样一系列变换即消元</p></li>
<li><p>变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper
Triangle上三角）</p></li>
<li><p>矩阵</p>
<p><span class="math display">\[
\left[\begin{array}{c|c}
A &amp; X \\
\end{array}\right]
\]</span></p></li>
<li><p>称为增广矩阵(Augmented matrix)。b做同样变换可以得到c</p></li>
</ul>
<h2 id="回代">回代</h2>
<ul>
<li>解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例</li>
<li>因为U为上三角矩阵，z很容易求得</li>
<li>将z代入第二行求得y</li>
<li>将z,y代入第一行求得x</li>
<li>这个过程即回代</li>
</ul>
<h2 id="置换">置换</h2>
<p><span class="math display">\[
\begin{bmatrix}
a &amp; b &amp; c  \\
\end{bmatrix}*A
\]</span></p>
<ul>
<li>这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3</li>
</ul>
<p>同理 <span class="math display">\[
A*\begin{bmatrix}
a \\
b \\
c \\
\end{bmatrix}
\]</span></p>
<ul>
<li>这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3</li>
<li>可以推出，交换A两行的矩阵为</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
0 &amp; 1  \\
1 &amp; 0  \\
\end{bmatrix}*A
\]</span></p>
<ul>
<li>交换A两列的矩阵为</li>
</ul>
<p><span class="math display">\[
A*\begin{bmatrix}
0 &amp; 1  \\
1 &amp; 0  \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵</p></li>
<li><p>在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作<span
class="math inline">\(E_{ij}\)</span></p></li>
<li><p>消元可写成</p>
<p><span class="math display">\[
E_{32}E_{31}E_{21}A=U
\]</span></p></li>
</ul>
<h1 id="第三讲乘法和逆矩阵">第三讲：乘法和逆矩阵</h1>
<h2 id="矩阵乘法">矩阵乘法</h2>
<ul>
<li><p>考虑矩阵乘法</p>
<p><span class="math display">\[
A*B=C
\]</span></p></li>
<li><p>第一种算法：点乘 <span
class="math inline">\(C_{ij}=\sum_iA_{ik}B_{kj}\)</span></p></li>
<li><p>第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列</p></li>
<li><p>第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行</p></li>
<li><p>第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C</p></li>
<li><p>第五种算法：矩阵分块算</p></li>
</ul>
<h2 id="逆矩阵">逆矩阵</h2>
<ul>
<li>对逆矩阵<span class="math inline">\(A^{-1}\)</span>,有<span
class="math inline">\(AA^{-1}=I\)</span>,I为单位矩阵</li>
<li>对方阵，左逆矩阵与右逆矩阵相同</li>
<li>若存在非零矩阵X,使得<span
class="math inline">\(AX=0\)</span>,则A不可逆</li>
<li>求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵
<ul>
<li><p>证明：</p>
<p><span class="math display">\[
EA=I \\
E=A^{-1} \\
EI=A^{-1} \\
\]</span></p></li>
</ul></li>
</ul>
<h1 id="第四讲a的lu分解">第四讲：A的LU分解</h1>
<h2 id="lu分解">LU分解</h2>
<ul>
<li><p><span
class="math inline">\((AB)^{-1}=B^{-1}A^{-1}\)</span></p></li>
<li><p>对A的转置矩阵<span class="math inline">\(A^T\)</span>,易得</p>
<p><span class="math display">\[
AA^{-1}=I \\
(A^{-1})^TA^T=I \\
所以(A^T)^{-1}=(A^{-1})^T \\
\]</span></p></li>
<li><p>对单个矩阵而言，转置和求逆可以互换</p></li>
<li><p>矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例</p>
<p><span class="math display">\[
E_{32}E_{31}E_{21}A=U \\
所以可得L: \\
L=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\
\]</span></p></li>
<li><p>为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数</p></li>
</ul>
<h2 id="消元消耗">消元消耗</h2>
<ul>
<li><p>记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为</p>
<p><span class="math display">\[
\sum_{i=1}^{n}i*(i-1) \approx \sum_{i=1}^{n}i^2 \approx \frac 13 n^3
\]</span></p></li>
</ul>
<h2 id="群">群</h2>
<ul>
<li>以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)</li>
<li>对这些矩阵，<span class="math inline">\(P^{-1}=P^T\)</span></li>
<li>这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群</li>
<li>n*n矩阵共有n!个行置换矩阵</li>
</ul>
<h1 id="第五讲转置置换向量空间r">第五讲：转置、置换、向量空间R</h1>
<h2 id="置换-1">置换</h2>
<ul>
<li>置换矩阵是用来完成行交换的矩阵</li>
<li>A=LU,L对角线上都是1，下方为消元乘数，U下三角为0</li>
<li>PA=LU用于描述包含行交换的lu分解</li>
<li>P(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价</li>
</ul>
<h2 id="转置">转置</h2>
<ul>
<li>行列交换即转置，记作<span class="math inline">\(A^T\)</span>，<span
class="math inline">\(A_{ij}=A_{ji}^T\)</span></li>
<li><span class="math inline">\((AB)^T=B^TA^T\)</span></li>
<li>对称矩阵(symmetric),<span class="math inline">\(A^T=A\)</span></li>
<li>对任意矩阵A，<span
class="math inline">\(AA^T\)</span>总是对称的,因为<span
class="math inline">\((A^TA)^T=(A^TA^{TT})=(A^TA)\)</span></li>
</ul>
<h2 id="向量空间">向量空间</h2>
<ul>
<li>向量可以相加减，点乘</li>
<li><strong>空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件</strong></li>
<li>例如<span
class="math inline">\(R^2\)</span>，代表所有实数的二维向量空间</li>
<li>向量空间内的任何向量进行线性组合后依然在向量空间内，所以<span
class="math inline">\(R^2\)</span>向量空间内必须存在(0,0)</li>
<li>不是向量空间的一个例子：只取<span
class="math inline">\(R^2\)</span>的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的</li>
<li>在<span
class="math inline">\(R^2\)</span>内取一条过零点直线可以称为<span
class="math inline">\(R^2\)</span>的向量子空间，这个子空间依然满足自封闭性(加减和数乘)</li>
<li><span class="math inline">\(R^2\)</span>的子空间都有哪些？
<ul>
<li><span class="math inline">\(R^2\)</span>本身</li>
<li>过零点两端无限延伸的直线(注意这和<span
class="math inline">\(R^1\)</span>不同)</li>
<li>(0,0),简写为Z</li>
</ul></li>
<li><span class="math inline">\(R^3\)</span>的子空间都有哪些？
<ul>
<li><span class="math inline">\(R^3\)</span>本身</li>
<li>过零点两端无限延伸的直线(注意这和<span
class="math inline">\(R^1\)</span>不同)</li>
<li>过零点的无限大平面</li>
<li>(0,0,0)</li>
</ul></li>
</ul>
<h2 id="通过矩阵构造向量子空间">通过矩阵构造向量子空间</h2>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 3  \\
2 &amp; 3  \\
4 &amp; 1  \\
\end{bmatrix}
\]</span></p>
<ul>
<li>各列属于<span
class="math inline">\(R^3\)</span>，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)
<img data-src="http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png"
alt="mark" /></li>
</ul>
<h1 id="第六讲列空间和零空间">第六讲：列空间和零空间</h1>
<h2 id="列空间">列空间</h2>
<ul>
<li><p>上一讲提到两种子空间，平面P和直线L。<span class="math inline">\(P
\bigcup L\)</span>不是一个子空间，<span class="math inline">\(P \bigcap
L\)</span>是一个子空间</p></li>
<li><p>对任意子空间S、T,<span class="math inline">\(S \bigcap
T\)</span>是一个子空间</p></li>
<li><p>举个栗子</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\]</span></p></li>
<li><p>C(A)是<span
class="math inline">\(R^4\)</span>子空间，将这三个列向量做线性组合可以得到子空间</p></li>
<li><p>下面将子空间与线性方程组联系起来</p></li>
<li><p>现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？</p>
<ul>
<li>前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个<span
class="math inline">\(R^4\)</span>空间，<strong>即列空间无法填充整个四维空间</strong></li>
<li>后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，<strong>等同于只有b在A的列空间内，x有解</strong></li>
</ul></li>
<li><p>如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间</p></li>
</ul>
<h2 id="零空间">零空间</h2>
<ul>
<li><p>零空间(null
space)与列空间完全不同，A的零空间包含Ax=0的所有解x</p></li>
<li><p>列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\begin{bmatrix}
X_1 \\
X_2 \\
X_3 \\
\end{bmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}
\]</span></p></li>
<li><p>显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间</p></li>
<li><p>为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：......矩阵乘法可以展开......分配率......</p></li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 2  \\
2 &amp; 1 &amp; 3  \\
3 &amp; 1 &amp; 5  \\
4 &amp; 1 &amp; 5  \\
\end{bmatrix}
\begin{bmatrix}
X_1 \\
X_2 \\
X_3 \\
\end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
3 \\
4 \\
\end{bmatrix}
\]</span></p>
<ul>
<li>我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？</li>
<li>显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0</li>
<li>列空间和零空间是两种构造子空间的方法
<ul>
<li>从几个向量通过线性组合来得到子空间</li>
<li>从一个方程组，通过让x满足特定条件来得到子空间</li>
</ul></li>
</ul>
<h1 id="第七讲主变量特解">第七讲：主变量、特解</h1>
<h2 id="主变量">主变量</h2>
<ul>
<li><p>如何用算法解Ax=0</p></li>
<li><p>举个栗子:</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
2 &amp; 4 &amp; 6 &amp; 8  \\
3 &amp; 6 &amp; 8 &amp; 10  \\
\end{bmatrix}
\]</span></p></li>
<li><p>第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来</p></li>
<li><p>消元不改变方程的组，因为消元改动列空间,不改动解空间</p></li>
<li><p>第一次消元之后,第一列只有第一行的主元不为零</p></li>
</ul>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}=U
\]</span></p></li>
<li><p>如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)</p></li>
<li><p>现在我们可以解Ux=0,并进行回代</p></li>
<li><p>自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的<strong>主变量</strong>x1,x3可以通过回代解出</p></li>
</ul>
<h2 id="特解">特解</h2>
<ul>
<li>在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)</li>
<li>所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。</li>
<li>两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为<strong>特解</strong>，根据特解我们可以得到解空间：两组特解的线性组合,a*(-2,1,0,0)+b*(2,0,-2,1)</li>
<li>秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量</li>
</ul>
<h2 id="简化行阶梯形式">简化行阶梯形式</h2>
<ul>
<li><p>U还能进一步简化</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 2 &amp; 2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>在简化行阶梯形式(reduced row echelon form
RREF)中，主元上方也全是0</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; -2  \\
0 &amp; 0 &amp; 2 &amp; 4  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>而且需将主元化为1,因为b=0,所以第二行可以直接除以2</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; -2  \\
0 &amp; 0 &amp; 1 &amp; 2  \\
0 &amp; 0 &amp; 0 &amp; 0  \\
\end{bmatrix}=R
\]</span></p></li>
<li><p>简化行阶梯形式以最简形式包含了矩阵的所有信息</p></li>
<li><p>单位矩阵位于主行与主列交汇处</p></li>
<li><p>最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I &amp; F \\
0 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p>其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列</p></li>
</ul>
<h2 id="零空间矩阵">零空间矩阵</h2>
<ul>
<li><p>零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解</p>
<p><span class="math display">\[
R*N=0
\]</span></p>
<p><span class="math display">\[
N=\begin{bmatrix}
-F \\
I  \\
\end{bmatrix}
\]</span></p></li>
<li><p>整个方程可以写成</p>
<p><span class="math display">\[
\begin{bmatrix}
I &amp; F \\
\end{bmatrix}
\begin{bmatrix}
x_{pivot} \\
x_{free}  \\
\end{bmatrix}=0
\]</span></p>
<p><span class="math display">\[
x_{pivot}=-F
\]</span></p></li>
</ul>
<h2 id="最后举个栗子过一遍算法">最后举个栗子过一遍算法</h2>
<ul>
<li><p>原矩阵</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 4 &amp; 6 \\
2 &amp; 6 &amp; 8 \\
2 &amp; 8 &amp; 10 \\
\end{bmatrix}
\]</span></p></li>
<li><p>第一遍消元</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 2 \\
0 &amp; 4 &amp; 4 \\
\end{bmatrix}
\]</span></p></li>
<li><p>第二遍消元(进行一次行交换使得第二个主元在第二行)</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 2 &amp; 2 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{bmatrix}=U
\]</span></p></li>
<li><p>显然r=2,1个自由变量,令自由变量为1，得到特解x</p>
<p><span class="math display">\[
x=\begin{bmatrix}
-1 \\
-1 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>零空间就是cx,一条直线，这个x为零空间的基</p></li>
<li><p>接下来继续化简U</p>
<p><span class="math display">\[
U=\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
\end{bmatrix}=R=
\begin{bmatrix}
I &amp; F  \\
0 &amp; 0  \\
0 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
</ul>
<p><span class="math display">\[
F=\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}=U
\]</span></p>
<p><span class="math display">\[
x=\begin{bmatrix}
-F \\
I  \\
\end{bmatrix}=N
\]</span></p>
<h1 id="第八讲可解性与解的结构">第八讲：可解性与解的结构</h1>
<h2 id="可解性">可解性</h2>
<p><span class="math display">\[\begin{cases}
x_1+2x_2+2x_3+2x_4=b_1\\
2x_1+4x_2+6x_3+8x_4=b_2\\
3x_1+6x_2+8x_3+10x_4=b_3\\
\end{cases}
\]</span></p>
<ul>
<li><p>写成增广矩阵形式：</p>
<p><span class="math display">\[
\left[\begin{array}{c c c c|c}
1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\
2 &amp; 4 &amp; 6 &amp; 8 &amp; b_2 \\
3 &amp; 6 &amp; 8 &amp; 10 &amp; b_3 \\
\end{array}\right]
\]</span></p></li>
<li><p>消元得到:</p>
<p><span class="math display">\[
\left[\begin{array}{c c c c|c}
1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\
0 &amp; 0 &amp; 2 &amp; 4 &amp; b_2-2b_1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3-b_2-b_1 \\
\end{array}\right]
\]</span></p></li>
<li><p>第一列和第三列为主列，第二列和第四列是自由列</p></li>
<li><p>可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里</p></li>
<li><p><strong>如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零</strong></p></li>
<li><p>如何求Ax=b的所有解？</p>
<ul>
<li><p>第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，<span
class="math inline">\(x_2和x_4\)</span>设为0，可以解得<span
class="math inline">\(x_1和x_3\)</span>分别为-2、1.5</p></li>
<li><p>第二步：完整的解为一个特解加上零空间中任意向量</p></li>
<li><p><span class="math inline">\(Ax_{particular}=b \\ Ax_{nullspace}=0
\\ A(x_{particular}+x_{nullspace})=b\)</span></p></li>
<li><p>在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)</p></li>
<li><p>完整解为：</p>
<p><span class="math display">\[
x_{complete}=
\begin{bmatrix}
-2 \\
0 \\
1.5 \\
0 \\
\end{bmatrix}+
c_1\begin{bmatrix}
-2 \\
1\\
0 \\
0 \\
\end{bmatrix}+
c_2\begin{bmatrix}
2 \\
0 \\
-2 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点</p></li>
</ul></li>
</ul>
<h2 id="解的结构">解的结构</h2>
<ul>
<li><p>现在考虑秩为r的m*n矩阵，r&lt;=m，r&lt;=n
，r取满秩时的情况,r=min(m,n)</p></li>
<li><p>列满秩：r=n&lt;m，此时没有自由变量
，<strong>N(A)={0}</strong>,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I \\
0 \\
\end{bmatrix}
\]</span></p></li>
<li><p>行满秩：r=m&lt;n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为</p>
<p><span class="math display">\[
R=\begin{bmatrix}
I &amp; F \\
\end{bmatrix}
\]</span></p></li>
<li><p>r=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一</p></li>
</ul>
<h2 id="一个网友从向量空间角度的解释">一个网友从向量空间角度的解释</h2>
<blockquote>
<p>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。
当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）
当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1
2 3）你如何收缩取得？）
当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。</p>
</blockquote>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>linearalgebra</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for ML</title>
    <url>/2017/02/12/MachineLearningNote/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/33690ecfca113fa9961fb6a6a328039c.png" width="500"/></p>
<p>Notes on some concepts and algorithms in machine learning, sourced
from:</p>
<ul>
<li>Elective Course on Pattern Recognition (An elective course for
third-year students at Beijing University of Posts and
Telecommunications, Pattern Recognition, textbook is "Pattern
Recognition" compiled by Zhang Xuegong, published by Tsinghua University
Press)</li>
<li>Watermelon Book</li>
<li>Statistical Learning Methods</li>
<li>Deep Learning (Translated in Chinese: <a
href="https://github.com/exacity/deeplearningbook-chinese">exacity/deeplearningbook-chinese</a>)</li>
</ul>
<p>Update:</p>
<ul>
<li><p>2017-02-12 Overview Update</p></li>
<li><p>2017-03-01 Update k-Nearest Neighbors</p></li>
<li><p>2017-03-08 Update SVM</p></li>
<li><p>2018-01-04 Update of fundamental knowledge of machine learning
and mathematical knowledge in the book "Deep Learning"</p></li>
<li><p>2018-08-09 The content of Statistical Learning Methods has been
posted in another article titled "Handwritten Notes on Statistical
Learning Methods," and it is estimated that it will not be updated
anymore. Later, some remaining contents in "Deep Learning" may be
updated</p></li>
</ul>
<p><span id="more"></span></p>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0H2cV.png" alt="i0H2cV.png" />
<figcaption aria-hidden="true">i0H2cV.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="introduction-to-statistical-learning-methods">Introduction to
Statistical Learning Methods</h1>
<h2
id="statistical-learning-supervised-learning-three-elements">Statistical
Learning, Supervised Learning, Three Elements</h2>
<ul>
<li><p>If a system can improve its performance by executing a certain
process, that is learning</p></li>
<li><p>The methods of statistical learning are based on constructing
statistical models from data for the purpose of prediction and
analysis</p></li>
<li><p>Obtain the training dataset; determine the hypothesis space
containing all possible models; establish the criteria for model
selection; implement the algorithm for solving the optimal model; select
the optimal model through learning methods; use the optimal model for
predicting or analyzing new data</p></li>
<li><p>The task of supervised learning is to learn a model that can make
a good prediction for the corresponding output of any given
input</p></li>
<li><p>Each specific input is an instance, typically represented by a
feature vector. It constitutes a feature space, with each dimension
corresponding to a feature.</p></li>
<li><p>Supervised learning learns a model from a training dataset, which
consists of input-output pairs (samples)</p></li>
<li><p>Supervised learning learns a model from a training set,
represented as a conditional probability distribution or a decision
function</p></li>
<li><p>Statistical Learning Three Elements: Method = Model + Strategy +
Algorithm. The model includes probabilistic models (conditional
probability) and non-probabilistic models (decision functions); the
strategy refers to the method of selecting the optimal model,
introducing the concepts of loss function (cost function) and risk
function, and realizing the optimization of empirical risk or structural
risk; the algorithm refers to the specific computational method for
learning the model.</p></li>
<li><p>Loss function is used to measure the degree of error between the
predicted values and the true values, common ones include: 0-1 loss
function, squared loss function, absolute loss function, logarithmic
loss function, denoted as <span
class="math inline">\(L(Y,P(Y|X))\)</span> , risk function (expected
loss) is the average loss below the joint distribution of the model:
<span class="math display">\[
R_{exp}(f)=E_p[L(Y,f(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy
\]</span> , empirical risk (empirical loss) is the average loss of the
model about the training set: <span class="math display">\[
R_{emp}(f)=\frac 1N \sum_{i=1}^NL(y_i,f(x_i))
\]</span></p></li>
<li><p>Ideally, expected risk can be estimated using empirical risk,
however, when the sample size is small, minimizing empirical risk is
prone to overfitting, thus the concept of structural risk
(regularization) is proposed: <span class="math display">\[
R_{srm}(f)=\frac1N \sum_{i=1}^NL(y_i,f(x_i))+ \lambda J(f)
\]</span> , where J(f) represents the complexity of the model, and the
coefficient <span class="math inline">\(\lambda\)</span> is used to
weigh the empirical risk and model complexity. ML belongs to empirical
risk minimization, while MAP belongs to structural risk
minimization.</p></li>
</ul>
<h2 id="model-evaluation-model-selection">Model evaluation, model
selection</h2>
<ul>
<li>The error of the model on the training set and the test set is
respectively called training error and test error. The loss function
used may not be consistent, and making them consistent is ideal</li>
<li>Overfitting: Learning too much, the complexity of the model is
higher than the real model, performing well on the learned data but poor
in predicting unknown data. To avoid overfitting, the correct number of
features and the correct feature vectors are required.</li>
<li>Two methods for model selection: regularization and
cross-validation</li>
</ul>
<h2 id="regularization-cross-validation">Regularization,
cross-validation</h2>
<ul>
<li>Add a regularization term (penalty) to the empirical risk, with
higher penalties for more complex models</li>
<li>The general method is to randomly divide the dataset into three
parts: the training set, the validation set, and the test set, which are
used respectively for training data, selecting models, and finally
evaluating the learning method. Cross-validation involves repeatedly
randomly splitting the data into training and test sets, learning
multiple times, and conducting testing and model selection.</li>
<li>Cross-validation types: Simple cross-validation; S-fold
cross-validation; Leave-one-out cross-validation</li>
</ul>
<h2 id="generalization-ability">Generalization ability</h2>
<ul>
<li>Generalization error: Error in predicting unknown data</li>
<li>Generalization error upper bound is generally a function of the
sample size; as the sample size increases, the upper bound of
generalization tends to 0. This implies that the larger the hypothesis
space capacity, the harder the model is to learn, and the larger the
upper bound of the generalization error</li>
<li>For binary classification problems, the upper bound of
generalization error: where d is the capacity of the function set, for
any function, at least with probability <span
class="math inline">\(1-\delta\)</span></li>
</ul>
<p><span class="math display">\[
R_{exp}(f)\leq R_{emp}(f)+\varepsilon (d,N,\delta ) \\
\varepsilon (d,N,\delta )=\sqrt{\frac 1{2N}(log d+log \frac 1 \delta)}
\\
\]</span></p>
<h2 id="generative-model-discriminative-model">Generative model,
discriminative model</h2>
<ul>
<li>The generation method is based on learning the joint probability
distribution of data, and then calculating the conditional probability
distribution as the predictive model, i.e., the generative model, such
as the Naive Bayes method and the Hidden Markov Model</li>
<li>Discriminant methods learn decision functions or conditional
probability distributions directly from data as predictive models, i.e.,
discriminant models, such as k-nearest neighbors, perceptrons, decision
trees, logistic regression models, maximum entropy models, support
vector machines, boosting methods, and conditional random fields,
etc</li>
<li>The generation method can recover the joint probability
distribution, has a fast learning convergence speed, and is suitable for
cases with latent variables</li>
<li>High accuracy of discrimination methods, capable of abstracting
data, and simplifying learning problems</li>
</ul>
<h2 id="categorization-annotation-regression">Categorization,
annotation, regression</h2>
<ul>
<li><p>Categorization, which takes discrete finite values as output, is
also known as a classification decision function or classifier</p></li>
<li><p>For binary classification, the total number of four cases:
correctly predicted as correct TP; correctly predicted as incorrect FN;
incorrectly predicted as correct FP; incorrectly predicted as incorrect
TN</p>
<p><span class="math display">\[
Precision:P=\frac{TP}{TP+FP} \\
Recall:R=\frac{TP}{TP+FN} \\
F1 value:\frac {2}{F_1}=\frac1P+\frac1R \\
\]</span></p></li>
<li><p>Annotation: Input an observation sequence, output a marked
sequence</p></li>
<li><p>Regression: Function fitting, the commonly used loss function is
the squared loss function, which is fitted using the least squares
method</p></li>
</ul>
<h1 id="k-nearest-neighbors-method">k-Nearest Neighbors method</h1>
<h2 id="k-nearest-neighbors-method-1">k-Nearest Neighbors method</h2>
<ul>
<li>k-Nearest Neighbor method assumes that a training dataset is given,
with instances of predetermined categories. In classification, for new
instances, predictions are made based on the categories of the k nearest
training instances, through majority voting or other methods.</li>
<li>k-value selection, distance measurement, and classification decision
rules are the three elements of the k-nearest neighbor method.</li>
<li>k-Nearest Neighbor is a lazy learning method; it does not train the
samples.</li>
</ul>
<h2 id="k-nearest-neighbors-algorithm">k-Nearest Neighbors
algorithm</h2>
<ul>
<li><p>For new input instances, find the k nearest instances to the
instance in the training dataset, and if the majority of these k
instances belong to a certain class, classify the instance into that
class. That is:</p>
<p><span class="math display">\[
y=arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j), \  i=1,2,...,N; \
j=1,2,...,K
\]</span></p>
<p>When <span class="math inline">\(y_i=c_i\)</span> is <span
class="math inline">\(I=1\)</span> , the neighborhood <span
class="math inline">\(N_k(x)\)</span> consists of k nearest neighbor
points covering x.</p></li>
<li><p>When k=1, it is called the nearest neighbor algorithm</p></li>
<li><p>k-nearest neighbor algorithm does not have an explicit learning
process</p></li>
</ul>
<h2 id="k-nearest-neighbors-model">k-Nearest Neighbors model</h2>
<ul>
<li><p>k-nearest neighbor model refers to the partitioning of the
feature space.</p></li>
<li><p>In the feature space, for each training instance point, all
points closer to this point than others form a region called a unit.
Each training instance point has a unit, and the units of all training
instance points constitute a partition of the feature space, with the
class of each instance being the class label of all points within its
unit.</p></li>
<li><p>Distance metrics include: Euclidean distance, <span
class="math inline">\(L_p\)</span> distance, Minkowski
distance.</p></li>
<li><p>The Euclidean distance is a special case of the <span
class="math inline">\(L_p\)</span> distance (p=2), and when p=1, it
becomes the Manhattan distance, defined as:</p>
<p><span class="math display">\[
L_p(x_i,x_j)=(\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac1p}
\]</span></p></li>
<li><p>k value is small, approximation error is small, estimation error
is large, the overall model is complex, prone to overfitting. k value is
large, estimation error is small, approximation error is large, the
model is simple</p></li>
<li><p>Generally, the cross-validation method is used to determine the
value of k</p></li>
<li><p>The majority voting rule is equivalent to empirical risk
minimization</p></li>
</ul>
<h2 id="k-d-tree">k-d tree</h2>
<ul>
<li>kd-tree is a special structure used to store training data, which
improves the efficiency of k-nearest neighbor search, which is
essentially a binary search tree</li>
<li>Each node of the k-d tree corresponds to a k-dimensional
hyperrectangle region</li>
<li>Method for constructing a balanced kd-tree: Divide by taking the
median of each dimension sequentially, for example, in three dimensions,
first draw a line at the median of the x-axis, divide it into two parts,
then draw a line at the median of the y-axis for each part, and then
along the z-axis, and then cycle through x, y, z.</li>
<li>After the construction of the kd-tree is completed, it can be used
for k-nearest neighbor searches. The following uses the nearest neighbor
search as an example, where k=1:
<ul>
<li>Starting from the root node, recursively search downwards to the
area where the target point is located, until reaching the leaf
nodes</li>
<li>Taking this leaf node as the current nearest point, the distance
from the current nearest point to the target point is the current
nearest distance, and our goal is to search the tree to find a suitable
node to update the current nearest point and the current nearest
distance</li>
<li>Recursive upward rollback, perform the following operation on each
node
<ul>
<li>If the instance point saved by the node is closer to the target
point than the current nearest point, update the instance point to the
current nearest point, and update the distance from the instance point
to the target point to the current nearest distance</li>
<li>The child nodes of this node are divided into two, one of which
contains our target point. This part we have come from the target point
all the way up recursively, and it has been updated. Therefore, we need
to find the other child node of this node to see if it can be updated:
Check if the corresponding area of the other child node intersects with
the hypersphere centered at the target point and with the current
shortest distance as the radius. If they intersect, move to this child
node and continue the upward search; if they do not intersect, do not
move and continue the upward search.</li>
<li>Until the root node is searched, the current nearest point at this
time is the nearest neighbor of the target point.</li>
</ul></li>
</ul></li>
<li>The computational complexity of k-d tree search is <span
class="math inline">\(O(logN)\)</span> , suitable for k-nearest neighbor
search when the number of training instances is much larger than the
number of spatial dimensions</li>
</ul>
<h1 id="support-vector-machine">Support Vector Machine</h1>
<h2
id="linearly-separable-support-vector-machine-and-hard-margin-maximization">Linearly
separable support vector machine and hard margin maximization</h2>
<ul>
<li><p>The goal of learning is to find a separating hyperplane in the
feature space that can distribute instances into different classes
(binary classification). The separating hyperplane corresponds to the
equation <span class="math inline">\(wx+b=0\)</span> , determined by the
normal vector w and the intercept b, and can be represented by (w,
b)</p></li>
<li><p>Here, x is the feature vector <span
class="math inline">\((x_1,x_2,...)\)</span> , and y is the label of the
feature vector</p></li>
<li><p>Given a linearly separable training dataset, the separating
hyperplane learned by maximizing the margin or solving the equivalent
convex quadratic programming problem, denoted as <span
class="math inline">\(wx+b=0\)</span> , and the corresponding
classification decision function denoted as <span
class="math inline">\(f(x)=sign(wx+b)\)</span> , is called a linearly
separable support vector machine</p></li>
<li><p>Under the determination of the hyperplane <span
class="math inline">\(wx+b=0\)</span> , the distance from the point
(x,y) to the hyperplane can be</p>
<p><span class="math display">\[
\gamma _i=\frac{w}{||w||}x_i+\frac{b}{||w||}
\]</span></p></li>
<li><p>Whether the symbol of <span class="math inline">\(wx+b\)</span>
is consistent with the symbol of class label y indicates whether the
classification is correct, <span class="math inline">\(y=\pm 1\)</span>
, thus obtaining the geometric distance</p>
<p><span class="math display">\[
\gamma _i=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \\
Define the geometric interval of the hyperplane (w,b) with respect to
the training data set T as the minimum geometric interval of all sample
points \\
\gamma=min\gamma _i \\
\]</span></p></li>
<li><p>Simultaneously define the relative distance as the function
interval</p>
<p><span class="math display">\[
\gamma _i=y_i(wx_i+b) \\
\gamma =min\gamma _i \\
\]</span></p></li>
<li><p>Hard margin maximization is for linearly separable hyperplanes,
while soft margin maximization is for approximately linearly separable
cases</p></li>
<li><p>Finding a hyperplane with the maximum geometric margin, which can
be represented as the following constrained optimization problem</p>
<p><span class="math display">\[
max_{(w,b)} \gamma \\
s.t. \quad y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq \gamma
,i=1,2,...,N \\
\]</span></p></li>
<li><p>We can convert geometric intervals into functional intervals,
which has no effect on optimization. Moreover, if we fix the relative
interval as a constant (1), the maximization of the geometric interval
can be transformed into the minimization of <span
class="math inline">\(||w||\)</span> , thus the constrained optimization
problem can be rewritten as</p>
<p><span class="math display">\[
min_{(w,b)} \frac12 {||w||}^2 \\
s.t. \quad y_i(wx_i+b)-1 \geq 0 \\
\]</span></p></li>
<li><p>The above equation is the basic form of SVM, and when the above
optimization problem is solved, we obtain a separating hyperplane with
the maximum margin, which is the maximum margin method</p></li>
<li><p>The maximum margin separating hyperplane exists and is unique,
proof omitted</p></li>
<li><p>In the linearly separable case, the instances of the nearest
sample points to the separating hyperplane in the training dataset are
called support vectors</p></li>
<li><p>For the positive example points of <span
class="math inline">\(y_i=1\)</span> , the support vectors lie on the
plane <span class="math inline">\(wx+b=1\)</span> , similarly, the
negative example points lie on the plane <span
class="math inline">\(wx+b=-1\)</span> . These two planes are parallel
and there are no training data points between them. The distance between
the two planes is the margin, which depends on the normal vector w of
the separating hyperplane, as <span class="math inline">\(\frac 2
{||w||}\)</span></p></li>
<li><p>Thus, it can be seen that support vector machines are determined
by a very small number of important training samples (support
vectors)</p></li>
<li><p>To solve the optimization problem, we introduce the dual
algorithm and also introduce kernel functions to generalize to nonlinear
classification problems</p></li>
<li><p>To be supplemented</p></li>
</ul>
<h1 id="linear-algebra-basics">Linear Algebra Basics</h1>
<h2 id="moore-penrose">Moore-penrose</h2>
<ul>
<li><p>For non-square matrices, their inverse matrix is undefined,
therefore we specially define the pseudoinverse of non-square matrices:
Moore-Penrose pseudoinverse <span class="math display">\[
A^+=lim_{\alpha \rightarrow 0}(A^TA+\alpha I)^{-1}A^T
\]</span></p></li>
<li><p>The actual algorithms for computing the pseudo-inverse do not
base themselves on this definition, but rather use the following
formula:</p>
<p><span class="math display">\[
A^+=VD^+U^T
\]</span></p>
<p>U, D, and V are the matrices obtained from the singular value
decomposition of matrix A, which are diagonal matrices. The
pseudo-inverse <span class="math inline">\(D^+\)</span> of the diagonal
matrix D is obtained by taking the reciprocal of its non-zero elements
and then transposing.</p></li>
<li><p>When the number of columns of matrix A exceeds the number of
rows, using the pseudoinverse to solve the linear equation is one of
many possible methods. Particularly, <span class="math inline">\(x =
A^+y\)</span> is the one with the smallest Euclidean norm among all
feasible solutions to the equation.</p></li>
<li><p>When the number of rows of matrix A exceeds the number of
columns, there may be no solution. In this case, the pseudo-inverse
<span class="math inline">\(x\)</span> obtained minimizes the Euclidean
distance between <span class="math inline">\(Ax\)</span> and <span
class="math inline">\(y\)</span> .</p></li>
<li><p>To be supplemented</p></li>
</ul>
<h2 id="迹">迹</h2>
<ul>
<li><p>Trace operation returns the sum of the diagonal elements of the
matrix.</p></li>
<li><p>Using trace operations, the matrix Frobenius norm can be
described as:</p>
<p><span class="math display">\[
||A_F||=\sqrt{Tr(AA^T)}
\]</span></p></li>
<li><p>Trace has transposition invariance and permutation
invariance</p></li>
<li><p>The trace of a scalar is its own</p></li>
</ul>
<h2 id="pca-explanation">PCA Explanation</h2>
<ul>
<li>To be supplemented</li>
</ul>
<h1 id="probability-theory-and-information-theory">Probability Theory
and Information Theory</h1>
<h2 id="logistic-sigmoid">Logistic Sigmoid</h2>
<ul>
<li><p>Logistic and sigmoid are often used interchangeably, this
function is used to compress real numbers into the interval (0,1),
representing binary classification probabilities:</p>
<p><span class="math display">\[
\sigma (x) = \frac{1}{1+exp(-x)}
\]</span></p></li>
<li><p>Softmax is an extension of the sigmoid, being a smoothed version
of the argmax function (argmax returns a one-hot vector while softmax
returns probabilities for various possible outcomes), extending binary
classification to the multi-class (disjoint) case:</p>
<p><span class="math display">\[
\sigma (z)_j = \frac{e^z j}{\sum _{k=1}^K e^z k}
\]</span></p></li>
<li><p>Both exhibit saturation phenomena when the input is too large or
too small, but when the two functions are introduced as nonlinear
activation units in a neural network, because the cost function takes
the negative logarithm, this saturation phenomenon can be
eliminated.</p></li>
<li><p>Softmax function, due to the inclusion of exponential functions,
also has issues with underflow and overflow. When the input is uniformly
distributed and the number of input samples is large, the denominator
exponential values approach 0, and the summation may also approach 0,
leading to underflow in the denominator. Overflow can also occur when
the parameters of the exponential function are large. The solution is to
process the input x as z = x - max(xi), that is, subtracting the maximum
component from each component of the vector. Adding or subtracting a
scalar from the input vector does not change the softmax function values
(redundancy of the softmax function), but at this point, the maximum
value of the processed input is 0, excluding overflow. After the
exponential function, at least one term in the summation of the
denominator exists as 1, excluding underflow.</p></li>
<li><p>Utilizing the redundancy of the softmax function can also deduce
that sigmoid is a special case of softmax: <img data-src="https://s1.ax1x.com/2018/10/20/i0HRXT.png"
alt="i0HRXT.png" /></p></li>
</ul>
<h2 id="kl-divergence-and-cross-entropy">KL divergence and
cross-entropy</h2>
<ul>
<li><p>KL divergence: A measure of the difference between two
distributions, PQ, non-negative and asymmetric:</p>
<p><span class="math display">\[
D_{KL}(P||Q) = E_{x \sim P} [log \frac{P(x)}{Q(x)}] = E_{x \sim P} [log
P(x) - log Q(x)]
\]</span></p></li>
<li><p>Cross-entropy:</p>
<p><span class="math display">\[
H(P,Q) = -E_{x \sim P} log Q(x)
\]</span></p></li>
<li><p>The cross-entropy form is simple, and the minimization of KL
divergence with respect to Q (actual output) is unrelated to the first
term in the divergence formula, therefore, minimizing KL divergence can
actually be seen as minimizing cross-entropy. Moreover, since KL
divergence represents the difference between PQ (actual output and
correct output), it can be regarded as a loss function</p></li>
<li><p>In dealing with binary classification problems using logistic
regression, q(x) refers to the logistic function, and p(x) refers to the
actual distribution of the data (either 0 or 1)</p></li>
<li><p>The expected self-information of q with respect to p, i.e., the
binary cross-entropy (Logistic cost function):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m y^{(i)} log h_{\theta} (x^{(i)}) +
(1-y^{(i)}) log (1-h_{\theta}(x^{(i)}))]
\]</span></p></li>
<li><p>Similarly, we can obtain the multi-class cross-entropy (Softmax
cost function):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m \sum _{j=1}^k 1\{ y^{(i)}=j \} log
\frac {e^{\theta _j ^T x^{(i)}}} {\sum _{l=1}^k e^{\theta _j ^T
x^{(i)}}}]
\]</span></p></li>
</ul>
<h2
id="cross-entropy-and-maximum-log-likelihood-relationship">Cross-entropy
and maximum log-likelihood relationship</h2>
<ul>
<li><p>Given a sample dataset X with distribution <span
class="math inline">\(P_{data}(x)\)</span> , we aim to obtain a model
<span class="math inline">\(P_{model}(x,\theta)\)</span> whose
distribution is as close as possible to <span
class="math inline">\(P_{data}(x)\)</span> . <span
class="math inline">\(P_{model}(x,\theta)\)</span> maps any x to a real
number to estimate the true probability <span
class="math inline">\(P_{data}(x)\)</span> . In <span
class="math inline">\(P_{model}(x,\theta)\)</span> , the maximum
likelihood estimate of <span class="math inline">\(\theta\)</span> is
the <span class="math inline">\(\theta\)</span> that maximizes the
product of probabilities obtained by the model for the sample data:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} p_{model} (X;\theta)
\]</span></p></li>
<li><p>Because taking the logarithm and scaling transformation does not
change argmax, taking the logarithm transforms it into a summation and
then dividing by the sample size to average it yields:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} E_{x \sim p_{data}} log
p_{model}(x;\theta)
\]</span></p></li>
<li><p>It can be observed that the above expression is the negative of
the cross-entropy, and its value is maximized when Pdata(x) =
Pmodel(x,θ), so:</p></li>
<li><p>Maximum likelihood = minimum negative log-likelihood = minimizing
cross-entropy = minimizing KL divergence = minimizing the gap between
data and model = minimizing the cost function</p></li>
<li><p>Maximum likelihood estimation can be extended to maximum
conditional likelihood estimation, which constitutes the foundation of
most supervised learning: Formula:</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} \sum_{i=1}^m log
P(y^{(i)} | x^{(i)} ; \theta)
\]</span></p></li>
<li><p>Maximum likelihood estimation is consistent.</p></li>
</ul>
<h1 id="computational-method">Computational Method</h1>
<h2 id="gradient-descent">Gradient Descent</h2>
<ul>
<li>How to transform parameters (inputs) to make the function smaller
(minimize the cost function)?</li>
<li>The principle is that moving the input in the opposite direction of
the derivative by a small step can reduce the function's output.</li>
<li>Extend the input to vector-form parameters, treat the function as a
cost function, and thus obtain a gradient-based optimization
algorithm.</li>
<li>First-order optimization algorithm: including gradient descent,
using the Jacobian matrix (including the relationship between partial
derivatives of vectors), and updating the suggested parameter updates
through gradient descent: <img data-src="https://s1.ax1x.com/2018/10/20/i0opQO.png" alt="i0opQO.png" /></li>
</ul>
<h2 id="newtons-method">Newton's Method</h2>
<ul>
<li>Second-order optimization algorithm (求最优补偿，定性临界点):
First-order optimization requires adjusting the appropriate learning
rate (step size), otherwise it cannot reach the optimal point or will
produce shaking, and it cannot update the parameters at the critical
point (gradient is 0), which reflects the need for second-order
derivative information of the cost function, for example, when the
function is convex or concave, the predicted value based on the gradient
and the true value of the cost function have a deviation. The Hessian
matrix contains second-order information. Newton's method uses the
information of the Hessian matrix, uses the second-order Taylor
expansion to obtain the function information, and updates the parameters
using the following formula: <img data-src="https://s1.ax1x.com/2018/10/20/i0o9yD.png"
alt="i0o9yD.png" /> </li>
</ul>
<h2 id="constraint-optimization">Constraint Optimization</h2>
<ul>
<li><p>Only contains equality constraint conditions: Lagrange</p></li>
<li><p>Inequality constraint conditions: KTT</p></li>
</ul>
<h1 id="modify-algorithm">Modify algorithm</h1>
<h2 id="modify-the-hypothesis-space">Modify the hypothesis space</h2>
<ul>
<li><p>Machine learning algorithms should avoid overfitting and
underfitting, which can be addressed by adjusting the model capacity
(the ability to fit various functions).</p></li>
<li><p>Adjusting the model capacity involves selecting an appropriate
hypothesis space (assuming input rather than parameters), for example,
previously only fitting polynomial linear functions:</p>
<p><span class="math display">\[
y = b + wx
\]</span></p></li>
<li><p>If nonlinear units, such as higher-order terms, are introduced,
the output remains linearly distributed relative to the parameters:</p>
<p><span class="math display">\[
y= b + w_1 x + w_2 x^2
\]</span></p>
<p>This increases the model's capacity while simplifying the generated
parameters, making it suitable for solving complex problems; however, a
too high capacity may also lead to overfitting.</p></li>
</ul>
<h2 id="regularization">Regularization</h2>
<ul>
<li><p>No free lunch theorem (after averaging over all possible data
generation distributions, each classification algorithm has the same
error rate on points that have not been observed beforehand) indicates
that machine learning algorithms should be designed for specific tasks,
and algorithms should have preferences. Adding regularization to the
cost function introduces preferences, causing the learned parameters to
be biased towards minimizing the regularization term.</p></li>
<li><p>An example is weight decay; the cost function with the addition
of the weight decay regularization term is:</p>
<p><span class="math display">\[
J(w) = MSE_{train} + \lambda w^T w
\]</span></p>
<p>λ controls the preference degree, and the generated models tend to
have small parameters, which can avoid overfitting.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="统计学习方法概论"><font size=5 >统计学习方法概论</font></h1>
<h2
id="统计学习监督学习三要素"><font size=4 >统计学习，监督学习，三要素</font></h2>
<ul>
<li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li>
<li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li>
<li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li>
<li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li>
<li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li>
<li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li>
<li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li>
<li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li>
<li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为<span
class="math inline">\(L(Y,P(Y|X))\)</span>,风险函数(期望损失)是模型在联合分布的平均以下的损失：<span
class="math display">\[R_{exp}(f)=E_p[L(Y,f(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy\]</span>经验风险(经验损失)是模型关于训练集的平均损失:<span
class="math display">\[R_{emp}(f)=\frac 1N
\sum_{i=1}^NL(y_i,f(x_i))\]</span></li>
<li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：<span
class="math display">\[R_{srm}(f)=\frac1N \sum_{i=1}^NL(y_i,f(x_i))+
\lambda J(f)\]</span>,其中J(f)为模型的复杂性，系数<span
class="math inline">\(\lambda\)</span>用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li>
</ul>
<h2 id="模型评估模型选择"><font size=4 >模型评估，模型选择</font></h2>
<ul>
<li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li>
<li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li>
<li>模型选择的两种方法：正则化和交叉验证</li>
</ul>
<h2 id="正则化交叉验证"><font size=4 >正则化，交叉验证</font></h2>
<ul>
<li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li>
<li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li>
<li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li>
</ul>
<h2 id="泛化能力"><font size=4 >泛化能力</font></h2>
<ul>
<li>泛化误差:对未知数据预测的误差</li>
<li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li>
<li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率<span
class="math inline">\(1-\delta\)</span></li>
</ul>
<p><span class="math display">\[
R_{exp}(f)\leq R_{emp}(f)+\varepsilon (d,N,\delta ) \\
\varepsilon (d,N,\delta )=\sqrt{\frac 1{2N}(log d+log \frac 1 \delta)}
\\
\]</span></p>
<h2 id="生成模型判别模型"><font size=4 >生成模型，判别模型</font></h2>
<ul>
<li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li>
<li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li>
<li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li>
<li>判别方法准确率高，可以抽象数据，简化学习问题</li>
</ul>
<h2 id="分类标注回归"><font size=4 >分类，标注，回归</font></h2>
<ul>
<li><p>分类，即输出取离散有限值，分类决策函数也叫分类器</p></li>
<li><p>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN</p>
<p><span class="math display">\[
精确率:P=\frac{TP}{TP+FP} \\
召回率:R=\frac{TP}{TP+FN} \\
1F值:\frac {2}{F_1}=\frac1P+\frac1R \\
\]</span></p></li>
<li><p>标注:输入一个观测序列，输出一个标记序列</p></li>
<li><p>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</p></li>
</ul>
<h1 id="k近邻法"><font size=5 >k近邻法</font></h1>
<h2 id="k近邻法-1"><font size=4 >k近邻法</font></h2>
<ul>
<li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li>
<li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li>
<li>k近邻法是一种懒惰学习，他不对样本进行训练。</li>
</ul>
<h2 id="k近邻算法"><font size=4 >k近邻算法</font></h2>
<ul>
<li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:</p>
<p><span class="math display">\[
y=arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j), \  i=1,2,...,N; \
j=1,2,...,K
\]</span></p>
<p>其中<span class="math inline">\(y_i=c_i\)</span>时<span
class="math inline">\(I=1\)</span>，<span
class="math inline">\(N_k(x)\)</span>是覆盖x的k个近邻点的邻域。</p></li>
<li><p>k=1时称为最近邻算法</p></li>
<li><p>k近邻算法没有显式的学习过程</p></li>
</ul>
<h2 id="k近邻模型"><font size=4 >k近邻模型</font></h2>
<ul>
<li><p>k近邻模型即对特征空间的划分。</p></li>
<li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p></li>
<li><p>距离度量包括：欧氏距离，<span
class="math inline">\(L_p\)</span>距离，Minkowski距离。</p></li>
<li><p>欧氏距离是<span
class="math inline">\(L_p\)</span>距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：</p>
<p><span class="math display">\[
L_p(x_i,x_j)=(\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac1p}
\]</span></p></li>
<li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p></li>
<li><p>一般采用交叉验证法确定k值</p></li>
<li><p>多数表决规则等价于经验风险最小化</p></li>
</ul>
<h2 id="kd树"><font size=4 >kd树</font></h2>
<ul>
<li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li>
<li>kd树的每一个节点对应于一个k维超矩形区域</li>
<li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li>
<li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：
<ul>
<li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li>
<li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li>
<li>递归向上回退，对每个节点做如下操作
<ul>
<li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li>
<li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li>
<li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li>
</ul></li>
</ul></li>
<li>kd树搜索的计算复杂度是<span
class="math inline">\(O(logN)\)</span>，适用于训练实例数远大于空间维数的k近邻搜索</li>
</ul>
<h1 id="支持向量机"><font size=5 >支持向量机</font></h1>
<h2
id="线性可分支持向量机与硬间隔最大化"><font size=4 >线性可分支持向量机与硬间隔最大化</font></h2>
<ul>
<li><p>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程<span
class="math inline">\(wx+b=0\)</span>，由法向量w和截距b决定，可由(w,b)表示</p></li>
<li><p>这里的x是特征向量<span
class="math inline">\((x_1,x_2,...)\)</span>，而y是特征向量的标签</p></li>
<li><p>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为<span
class="math inline">\(wx+b=0\)</span>，以及相应的分类决策函数<span
class="math inline">\(f(x)=sign(wx+b)\)</span>称为线性可分支持向量机</p></li>
<li><p>在超平面<span
class="math inline">\(wx+b=0\)</span>确定的情况下，点(x,y)到超平面的距离可以为</p>
<p><span class="math display">\[
\gamma _i=\frac{w}{||w||}x_i+\frac{b}{||w||}
\]</span></p></li>
<li><p>而<span
class="math inline">\(wx+b\)</span>的符号与类标记y的符号是否一致可以表示分类是否正确，<span
class="math inline">\(y=\pm 1\)</span>，这样就可以得到几何间隔</p>
<p><span class="math display">\[
\gamma _i=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \\
定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值
\\
\gamma=min\gamma _i \\
\]</span></p></li>
<li><p>同时定义相对距离为函数间隔</p>
<p><span class="math display">\[
\gamma _i=y_i(wx_i+b) \\
\gamma =min\gamma _i \\
\]</span></p></li>
<li><p>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</p></li>
<li><p>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题</p>
<p><span class="math display">\[
max_{(w,b)} \gamma \\
s.t. \quad y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq \gamma
,i=1,2,...,N \\
\]</span></p></li>
<li><p>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对<span
class="math inline">\(||w||\)</span>的最小化，因此约束最优化问题可以改写为</p>
<p><span class="math display">\[
min_{(w,b)} \frac12 {||w||}^2 \\
s.t. \quad y_i(wx_i+b)-1 \geq 0 \\
\]</span></p></li>
<li><p>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</p></li>
<li><p>最大间隔分离超平面存在且唯一，证明略</p></li>
<li><p>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</p></li>
<li><p>对<span
class="math inline">\(y_i=1\)</span>的正例点，支持向量在平面<span
class="math inline">\(wx+b=1\)</span>上，同理负例点在平面<span
class="math inline">\(wx+b=-1\)</span>上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为<span
class="math inline">\(\frac 2 {||w||}\)</span></p></li>
<li><p>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</p></li>
<li><p>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</p></li>
<li><p>待补充</p></li>
</ul>
<h1 id="线代基础"><font size=5 >线代基础</font></h1>
<h2 id="moore-penrose"><font size=4 >Moore-penrose</font></h2>
<ul>
<li><p>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose
伪逆 <span class="math display">\[
A^+=lim_{\alpha \rightarrow 0}(A^TA+\alpha I)^{-1}A^T
\]</span></p></li>
<li><p>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：</p>
<p><span class="math display">\[
A^+=VD^+U^T
\]</span></p>
<p>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D
的伪逆<span
class="math inline">\(D^+\)</span>是其非零元素取倒数之后再转置得到的。</p></li>
<li><p>当矩阵 A
的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，<span
class="math inline">\(x =
A^+y\)</span>是方程所有可行解中欧几里得范数最小的一个。</p></li>
<li><p>当矩阵 A
的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的<span
class="math inline">\(x\)</span>使得<span
class="math inline">\(Ax\)</span>和<span
class="math inline">\(y\)</span>的欧几里得距离最小。</p></li>
<li><p>待补充</p></li>
</ul>
<h2 id="迹"><font size=4 >迹</font></h2>
<ul>
<li><p>迹运算返回的是矩阵对角元素的和.</p></li>
<li><p>使用迹运算可以描述矩阵Frobenius范数的方式：</p>
<p><span class="math display">\[
||A_F||=\sqrt{Tr(AA^T)}
\]</span></p></li>
<li><p>迹具有转置不变性和轮换不变性</p></li>
<li><p>标量的迹是其本身</p></li>
</ul>
<h2 id="pca解释"><font size=4 >PCA解释</font></h2>
<ul>
<li>待补充</li>
</ul>
<h1 id="概率论信息论"><font size=5 >概率论信息论</font></h1>
<h2 id="logistic-sigmoid"><font size=4 >Logistic Sigmoid</font></h2>
<ul>
<li><p>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：</p>
<p><span class="math display">\[
\sigma (x) = \frac{1}{1+exp(-x)}
\]</span></p></li>
<li><p>Softmax
是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot
向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：</p>
<p><span class="math display">\[
\sigma (z)_j = \frac{e^z j}{\sum _{k=1}^K e^z k}
\]</span></p></li>
<li><p>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。</p></li>
<li><p>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</p></li>
<li><p>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：
<img data-src="https://s1.ax1x.com/2018/10/20/i0HRXT.png"
alt="i0HRXT.png" /></p></li>
</ul>
<h2 id="kl散度和交叉熵"><font size=4 >KL散度和交叉熵</font></h2>
<ul>
<li><p>KL散度：用以衡量PQ两个分布之间的差异，非负且不对称：</p>
<p><span class="math display">\[
D_{KL}(P||Q) = E_{x \sim P} [log \frac{P(x)}{Q(x)}] = E_{x \sim P} [log
P(x) - log Q(x)]
\]</span></p></li>
<li><p>交叉熵：</p>
<p><span class="math display">\[
H(P,Q) = -E_{x \sim P} log Q(x)
\]</span></p></li>
<li><p>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</p></li>
<li><p>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</p></li>
<li><p>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m y^{(i)} log h_{\theta} (x^{(i)}) +
(1-y^{(i)}) log (1-h_{\theta}(x^{(i)}))]
\]</span></p></li>
<li><p>同理可得多元交叉熵(Softmaxs代价函数):</p>
<p><span class="math display">\[
J(\theta) = - \frac 1m [\sum _{i=1}^m \sum _{j=1}^k 1\{ y^{(i)}=j \} log
\frac {e^{\theta _j ^T x^{(i)}}} {\sum _{l=1}^k e^{\theta _j ^T
x^{(i)}}}]
\]</span></p></li>
</ul>
<h2
id="交叉熵与最大对数似然关系"><font size=4 >交叉熵与最大对数似然关系</font></h2>
<ul>
<li><p>已知一个样本数据集X，分布为<span
class="math inline">\(P_{data}(x)\)</span>，我们希望得到一个模型<span
class="math inline">\(P_{model}(x,\theta)\)</span>，其分布尽可能接近<span
class="math inline">\(P_{data}(x)\)</span>。<span
class="math inline">\(P_{model}(x,\theta)\)</span>将任意x映射为实数来估计真实概率<span
class="math inline">\(P_{data}(x)\)</span>。 在<span
class="math inline">\(P_{model}(x,\theta)\)</span>中，对<span
class="math inline">\(\theta\)</span>的最大似然估计为使样本数据通过模型得到概率之积最大的<span
class="math inline">\(\theta\)</span>：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} p_{model} (X;\theta)
\]</span></p></li>
<li><p>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} E_{x \sim p_{data}} log
p_{model}(x;\theta)
\]</span></p></li>
<li><p>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</p></li>
<li><p>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</p></li>
<li><p>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：</p>
<p><span class="math display">\[
\theta _{ML} = \mathop{argmax}\limits_{\theta} \sum_{i=1}^m log
P(y^{(i)} | x^{(i)} ; \theta)
\]</span></p></li>
<li><p>最大似然估计具有一致性。</p></li>
</ul>
<h1 id="计算方法"><font size=5 >计算方法</font></h1>
<h2 id="梯度下降"><font size=4 >梯度下降</font></h2>
<ul>
<li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li>
<li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li>
<li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li>
<li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：
<img data-src="https://s1.ax1x.com/2018/10/20/i0opQO.png"
alt="i0opQO.png" /></li>
</ul>
<h2 id="牛顿法"><font size=4 >牛顿法</font></h2>
<ul>
<li><p>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：
<img data-src="https://s1.ax1x.com/2018/10/20/i0o9yD.png"
alt="i0o9yD.png" /></p>
<h2 id="约束优化"><font size=4 >约束优化</font></h2></li>
<li><p>只包含等式约束条件：Lagrange</p></li>
<li><p>包含不等式约束条件：KTT</p></li>
</ul>
<h1 id="修改算法"><font size=5 >修改算法</font></h1>
<h2 id="修改假设空间"><font size=4 >修改假设空间</font></h2>
<ul>
<li><p>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</p></li>
<li><p>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：</p>
<p><span class="math display">\[
y = b + wx
\]</span></p></li>
<li><p>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：</p>
<p><span class="math display">\[
y= b + w_1 x + w_2 x^2
\]</span></p>
<p>此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</p></li>
</ul>
<h2 id="正则化"><font size=4 >正则化</font></h2>
<ul>
<li><p>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</p></li>
<li><p>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：</p>
<p><span class="math display">\[
J(w) = MSE_{train} + \lambda w^T w
\]</span></p>
<p>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Linear Algebra 2</title>
    <url>/2017/01/21/LinearAlgebra2/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png" width="500"/></p>
<h1 id="lecture-9-linear-correlation-basis-dimension">Lecture 9: Linear
Correlation, Basis, Dimension</h1>
<h2 id="linear-correlation">Linear Correlation</h2>
<ul>
<li>Background knowledge: Assume a matrix A, where m &lt; n, i.e., the
number of unknowns is greater than the number of equations. Therefore,
in the null space, there are vectors other than the zero vector, up to m
leading principal elements, and there exist n-m free vectors, and the
entire equation system has non-zero solutions.</li>
<li>Under what conditions is the vector <span
class="math inline">\(x_1,x_2,x_3...x_n\)</span> linearly independent?
If there exists a combination of coefficients not all equal to zero such
that the linear sum results in 0, then it is linearly dependent;
otherwise, it is linearly independent.</li>
<li>If there exists a zero vector in the set of vectors, then the set of
vectors cannot be linearly independent.</li>
<li>If three vectors are randomly drawn in two-dimensional space, they
must be linearly dependent. Why? This can be deduced from background
knowledge.</li>
<li>For a matrix A, we are concerned with whether the columns are
linearly dependent; if there exists a non-zero vector in the null space,
then the columns are dependent.</li>
<li>When <span class="math inline">\(v_1,v_2...v_n\)</span> is the
columns of A, if they are unrelated, then what is the null space of A?
Only the zero vector. If they are related, then in addition to the zero
vector, there exists a non-zero vector in the null space.</li>
<li>When the column vectors are linearly independent, all column vectors
are leading vectors, and the rank is n. When the column vectors are
linearly dependent, the rank is less than n.</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h2 id="generated-space-base">Generated space, base</h2>
<ul>
<li><p>Generated a space, referring to the space containing all linear
combinations of these vectors.</p></li>
<li><p>A set of basis in a vector space refers to a group of vectors
that have two characteristics: they are linearly independent, and they
generate the entire space.</p></li>
<li><p>For example: The most easily thought of basis for <span
class="math inline">\(R^3\)</span> is</p>
<p><span class="math display">\[
\begin{bmatrix}
1   \\
0   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
0   \\
1   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
0   \\
0   \\
1   \\
\end{bmatrix}
\]</span></p></li>
<li><p>This is a set of standard bases, another example:</p>
<p><span class="math display">\[
\begin{bmatrix}
1   \\
1   \\
2   \\
\end{bmatrix}
,
\begin{bmatrix}
2   \\
2   \\
5   \\
\end{bmatrix}
\]</span></p></li>
<li><p>It is evident that a space cannot be formed, as taking any vector
that does not lie in the plane spanned by these two vectors will
suffice.</p></li>
<li><p>How to test if they form a basis? Treat them as columns to form a
matrix, which must be invertible (since it is a square matrix in this
example).</p></li>
<li><p>If the two vectors in Example 2 cannot form a basis for
three-dimensional space, then what space can they form a basis for? The
plane formed by these two vectors.</p></li>
<li><p>The basis is not uniquely determined, but all bases share a
common feature: the number of vectors in the basis is the same.</p></li>
</ul>
<h2 id="dimension">Dimension</h2>
<ul>
<li>The number of all the basis vectors mentioned above is the same, and
this number is the dimension of the space. It is not the dimension of
the basis vectors, but the number of basis vectors.</li>
</ul>
<h2 id="give-an-example">Give an example</h2>
<p>On matrix A <span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp;1  \\
1 &amp; 1 &amp; 2 &amp; 1   \\
1 &amp; 2 &amp; 3 &amp; 1 \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>Four columns are not linearly independent; the first and second
columns can be taken as the main columns</p></li>
<li><p>2 = rank of A = number of leading columns = dimension of column
space</p></li>
<li><p>The first and second columns form a set of basis for the column
space.</p></li>
<li><p>If you know the dimension of the column space, you have
determined the number of vectors, and as long as they are linearly
independent, these vectors can form a basis.</p></li>
<li><p>What is the dimension of the null space? In this example, the two
vectors in the null space (special solutions) are:</p>
<p><span class="math display">\[
\begin{bmatrix}
-1   \\
-1  \\
1   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
-1   \\
0  \\
0   \\
1    \\
\end{bmatrix}
\]</span></p></li>
<li><p>Yes, these two special solutions form a basis for the null space.
The dimension of the null space is the number of free variables, which
is n-r, i.e., 4-2=2 in this case.</p></li>
</ul>
<h1 id="tenth-lecture-four-basic-subspaces">Tenth Lecture: Four Basic
Subspaces</h1>
<ul>
<li><p>C(A), N(A), C( <span class="math inline">\(A^T\)</span> ), N(
<span class="math inline">\(A^T\)</span> ).</p></li>
<li><p>Respectively located in the <span
class="math inline">\(R^m、R^n、R^n、R^m\)</span> space</p></li>
<li><p>The dimension of the column space and the row space are both rank
r, the dimension of the null space is n-r, and the dimension of the left
null space is m-r</p></li>
<li><p>Basis of the column space: the leading columns, a total of r
columns. Basis of the null space: special solutions (free columns), a
total of n-r columns. Basis of the row space: non-zero rows in the
reduced form of R, a total of r rows.</p></li>
<li><p>Row transformations are linear combinations of row vectors, thus
the row spaces of A and R are the same, but the column spaces have
changed</p></li>
<li><p>Why is it called the left null space?</p>
<p><span class="math display">\[
rref\begin{bmatrix}
A_{m*n} &amp; I_{m*n}
\end{bmatrix}\rightarrow
\begin{bmatrix}
R_{m*n} &amp; E_{m*n}
\end{bmatrix} \\
\]</span></p></li>
<li><p>rref=E, i.e., EA=R</p></li>
<li><p>Through E, the left zero 空洞 can be calculated</p></li>
<li><p>Find the row combination that generates a zero row
vector</p></li>
<li><p>The basis of the left null space is the rows corresponding to the
non-zero rows of R, totaling m-r rows</p></li>
</ul>
<h1
id="eleventh-lecture-matrix-spaces-rank-1-matrices-and-small-world-graphs">Eleventh
Lecture: Matrix Spaces, Rank-1 Matrices, and Small-World Graphs</h1>
<h2 id="matrix-space">Matrix Space</h2>
<ul>
<li><p>Can be regarded as a vector space, can be multiplied by a scalar,
and can be added together</p></li>
<li><p>For the matrix space M with <span
class="math inline">\(3*3\)</span> as an example, a basis for the space
consists of 9 matrices, each containing only one element, 1. This is a
set of standard basis, and thus the dimension of this matrix space is
9</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 1 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 1  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}.....
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>The dimension of the subspace S of symmetric matrices in the
matrix space <span class="math inline">\(3*3\)</span> is studied again,
and it can be seen that among the 9 matrices in the original space
basis, 3 belong to the subspace of symmetric matrices, and there are
also 3 matrices that are symmetric both above and below the diagonal, so
the dimension of the subspace of symmetric matrices is 6</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 1  \\
0 &amp; 0 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
0 &amp; 1 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>For the subspace U of the upper triangular matrix, it is easy to
obtain that its dimension is 6, and the basis of the element space
includes the basis of the subspace</p></li>
<li><p>Next, let's study <span class="math inline">\(S \bigcap
U\)</span> , and it can be easily obtained that this subspace is the
diagonal matrix D, with a dimension of 3</p></li>
<li><p>If it is <span class="math inline">\(S \bigcup U\)</span> , their
union basis can obtain all bases of M, so its dimension is 9</p></li>
<li><p>Organize accordingly</p>
<p><span class="math display">\[
dim(S)=6,dim(U)=6,dim(S \bigcap U)=3,dim(S \bigcup U)=3 \\
dim(S)+dim(U)=dim(S \bigcap U)+dim(S \bigcup U) \\
\]</span></p></li>
<li><p>Another example can be given to illustrate that a vector space
does not necessarily contain vectors, such as the following vector space
based on differential equations</p>
<p><span class="math display">\[
\frac{d^2y}{dx^2}+y=0
\]</span></p></li>
<li><p>His several solutions are</p>
<p><span class="math display">\[
y=cos(x),y=sin(x)
\]</span></p></li>
<li><p>Complete solution is</p>
<p><span class="math display">\[
y=c_1cos(x)+c_2sin(x)
\]</span></p></li>
<li><p>A vector space is obtained, with a basis of 2</p></li>
</ul>
<h2 id="rank-1-matrix">Rank 1 matrix</h2>
<ul>
<li><p>Write a simple rank-1 matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 4 &amp; 5 \\
2 &amp; 8 &amp; 10 \\
\end{bmatrix}=
\begin{bmatrix}
1  \\
2  \\
\end{bmatrix}*
\begin{bmatrix}
1 &amp; 4 &amp; 5 \\
\end{bmatrix}
\]</span></p></li>
<li><p>All rank 1 matrices can be represented as a column multiplied by
a row</p></li>
<li><p>Rank 1 matrices are like building blocks, for example, a rank 4
matrix can be constructed from 4 rank 1 matrices</p></li>
<li><p>Consider an example of a rank-1 matrix, in four-dimensional
space, let vector <span
class="math inline">\(v=(v_1,v_2,v_3,v_4)\)</span> , set <span
class="math inline">\(S=\{v|v_1+v_2+v_3+v_4=0\}\)</span> , if S is
regarded as the zero space, then the matrix A in the corresponding
equation <span class="math inline">\(Av=0\)</span> is</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Easily obtainable <span
class="math inline">\(dimN(A)=n-r\)</span> , thus the dimension of S is
<span class="math inline">\(4-1=3\)</span> , and a set of basis for S
is</p>
<p><span class="math display">\[
\begin{bmatrix}
-1  \\
1  \\
0  \\
0  \\
\end{bmatrix},
\begin{bmatrix}
-1  \\
0  \\
1  \\
0  \\
\end{bmatrix},
\begin{bmatrix}
-1  \\
0  \\
0  \\
1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>Four subspaces of matrix A: the rank (dimension) of the null
space and the column space are both 1, the row space <span
class="math inline">\(C(A^T)=\{a,a,a,a\}​\)</span> , the column space
<span class="math inline">\(C(A)=R^1​\)</span> , the null space <span
class="math inline">\(N(A)​\)</span> , which is the linear combination of
the basis of S, <span class="math inline">\(N(A^T)={0}​\)</span></p></li>
<li><p>Organize</p>
<p><span class="math display">\[
dim(N(A))+dim(C(A^T))=3+1=4=n \\
dim(C(A))+dim(N(A^T))=1+0=1=m \\
\]</span></p></li>
</ul>
<h2 id="small-world-graph">Small World Graph</h2>
<ul>
<li>Just introduced the concept of graphs, preparing for the next
lecture</li>
</ul>
<h1 id="twelfth-lecture-graphs-and-networks">Twelfth Lecture: Graphs and
Networks</h1>
<h2 id="图">图</h2>
<ul>
<li>Some basic concepts of graphs, omitted</li>
</ul>
<h2 id="internet">Internet</h2>
<ul>
<li><p>The adjacency matrix A of the graph, with columns as the nodes of
the graph, rows as the edges of the matrix, the starting point as -1,
the endpoint as 1, and the rest as 0</p></li>
<li><p>Several rows of linear correlation constitute the circuit, where
the circuit implies correlation</p></li>
<li><p>The adjacency matrix A describes the topological structure of the
graph</p></li>
<li><p><span class="math inline">\(dimN(A^T)=m-r​\)</span></p></li>
<li><p>If the nodes of the graph are voltages, <span
class="math inline">\(Ax\)</span> where x represents the voltage, <span
class="math inline">\(Ax=0\)</span> yields a set of voltage difference
equations, the null space is one-dimensional, <span
class="math inline">\(A^Ty\)</span> where y represents the current on
the edges, the relationship between current and voltage difference is
Ohm's law, <span class="math inline">\(A^Ty=0\)</span> obtains
Kirchhoff's laws, the null space includes two solutions of Kirchhoff's
current equations, which, from the circuit diagram, correspond to two
small loops</p></li>
<li><p>Tree is a graph without cycles</p></li>
<li><p>Take another look at <span
class="math inline">\(dimN(A^T)=m-r\)</span></p></li>
<li><p><span class="math inline">\(dimN(A^T)\)</span> = Number of
irrelevant circuits</p></li>
<li><p><span class="math inline">\(m\)</span> = Number of edges</p></li>
<li><p><span class="math inline">\(r=n-1\)</span> = number of nodes - 1
(since the null space is one-dimensional)</p></li>
<li><p>The: number of nodes - number of edges + number of circuits = 1
(Euler's formula)</p></li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li><p>Potential is denoted as e, <span
class="math inline">\(e=Ax\)</span></p></li>
<li><p>Potential difference causes the generation of current, <span
class="math inline">\(y=Ce\)</span></p></li>
<li><p>Current satisfies Kirchhoff's Current Law, <span
class="math inline">\(A^Ty=0\)</span></p></li>
<li><p>Combine the three equations:</p>
<p><span class="math display">\[
A^TCAx=f
\]</span></p>
<p>This is the most basic balance equation in applied
mathematics</p></li>
</ul>
<h1 id="lecture-thirteen-orthogonal-vectors-and-subspaces">Lecture
Thirteen: Orthogonal Vectors and Subspaces</h1>
<h2 id="orthogonal-vectors">Orthogonal vectors</h2>
<ul>
<li><p>Orthogonal means perpendicular, indicating that in n-dimensional
space, the angles between these vectors are 90 degrees</p></li>
<li><p>When <span class="math inline">\(x^Ty=0\)</span> , x and y are
orthogonal, prove:</p></li>
<li><p>If x is orthogonal to y, it follows that:</p>
<p><span class="math display">\[
{||x||}^2+{||y||}^2={||x+y||}^2 \\
\]</span></p></li>
<li><p>That is to say:</p>
<p><span class="math display">\[
x^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\
\]</span></p></li>
<li><p>That is to say:</p>
<p><span class="math display">\[
x^Ty=0 \\
\]</span></p></li>
<li><p>Subspaces are orthogonal if all vectors within one subspace are
orthogonal to every vector in another subspace. It is obvious that if
two two-dimensional subspaces intersect at some vector, then these two
spaces are not orthogonal</p></li>
<li><p>If two subspaces are orthogonal, they must not intersect at any
non-zero vector, because such a non-zero vector exists in both subspaces
simultaneously, and it cannot be perpendicular to itself</p></li>
<li><p>The row space is orthogonal to the null space because <span
class="math inline">\(Ax=0\)</span> , i.e., the dot product of each row
of the matrix and these linear combinations of the rows (row space) with
the solution vector (null space) is 0. This proves the left half of the
figure.</p></li>
<li><p>In the right half of the figure, the column space and left null
space are the row space and null space of the transpose of matrix A,
respectively. The proof given earlier is still valid, thus the column
space and left null space are orthogonal, and the right half of the
figure holds</p></li>
<li><p>The figure presents the orthogonal subspaces of n-dimensional and
m-dimensional spaces, the orthogonal subspace of the n-dimensional
space: r-dimensional row space and (n-r)-dimensional null space. The
orthogonal subspace of the m-dimensional space: r-dimensional column
space and (m-r)-dimensional left null space.</p></li>
</ul>
<h2 id="orthogonal-subspace">Orthogonal subspace</h2>
<ul>
<li>For example, in three-dimensional space, if the row space is
one-dimensional, the null space is two-dimensional. The row space is a
straight line, and the null space is a plane perpendicular to this line.
This orthogonality can be intuitively seen from a geometric
perspective</li>
<li>Because the null space is the orthogonal complement of the row
space, the null space contains all vectors orthogonal to the row
space</li>
<li>This is all the knowledge about solving <span
class="math inline">\(Ax=0\)</span> . What should we do if we need to
solve an unsolvable equation, or to find the optimal solution? We
introduce an important matrix <span
class="math inline">\(A^TA\)</span></li>
<li><span class="math inline">\(A^TA\)</span> is a <span
class="math inline">\(n*n\)</span> square matrix, and it is also
symmetric</li>
<li>Transforming bad equation into good equation, multiply both sides by
<span class="math inline">\(A^T\)</span></li>
<li>Not always reversible; if reversible, then <span
class="math inline">\(N(A^TA)=N(A)\)</span> , and their ranks are the
same</li>
<li>Reversible if and only if the null space contains only the zero
vector, i.e., the columns are linearly independent; these properties
will be proven in the next lecture</li>
</ul>
<h1 id="th-lecture-subspace-projection">14th Lecture: Subspace
Projection</h1>
<h2 id="projection">Projection</h2>
<ul>
<li><p>Discussing projection in two-dimensional cases</p></li>
<li><p>A projection of a point b onto another line a, which is the
perpendicular line segment drawn from this point to intersect line a at
point p; p is the projection point of b onto a, and the vector from the
origin to p is the projection vector p. The perpendicular line segment
is the error e, where e = b - p</p></li>
<li><p>p in the one-dimensional subspace of a is x times a, i.e., p =
xa</p></li>
<li><p>a perpendicular to e, i.e</p>
<p><span class="math display">\[
a^T(b-xa)=0 \\
xa^Ta=a^Tb \\
x= \frac {a^Tb}{a^Ta} \\
p=a\frac {a^Tb}{a^Ta} \\
\]</span></p></li>
<li><p>From the equation, it can be seen that if b is doubled, the
projection is also doubled; if a changes, the projection remains
unchanged, because the numerator and denominator cancel each other
out</p></li>
</ul>
<h2 id="projection-matrix">Projection matrix</h2>
<ul>
<li>Projection matrix P one-dimensional pattern</li>
<li>Multiplying the projection matrix by any vector b will always lie on
a line through vector a (i.e., the projection of b onto a, denoted as
p), thus the column space of the projection matrix is this line, and the
rank of the matrix is 1</li>
<li>Other two properties of the projection matrix:
<ul>
<li>Symmetry, i.e., <span class="math inline">\(P^T=P\)</span></li>
<li>Two projections at the same location, i.e., <span
class="math inline">\(P^2=P\)</span></li>
</ul></li>
</ul>
<h2 id="the-significance-of-projection">The Significance of
Projection</h2>
<ul>
<li><p>Below is discussed in the high-dimensional case</p></li>
<li><p>When the number of equations exceeds the number of unknowns,
there is often no solution, and in this case, we can only find the
closest solution</p></li>
<li><p>How to find? Refine b such that b is in the column space</p></li>
<li><p>How to fine-tune? Change b to p, which is the one closest to b in
the column space, i.e., the projection of b onto the column space when
solving <span class="math inline">\(Ax^{&#39;}=p\)</span> , p</p></li>
<li><p>Now we require <span class="math inline">\(x^{&#39;}\)</span> ,
<span class="math inline">\(p=Ax^{&#39;}\)</span> , the error vector
<span class="math inline">\(e=b-Ax^{&#39;}\)</span> , and according to
the definition of projection, e needs to be orthogonal to the column
space of A</p></li>
<li><p>In summary</p>
<p><span class="math display">\[
A^T(b-Ax^{&#39;})=0 \\
\]</span></p></li>
<li><p>From the above equation, it can be seen that e is in the left
null space of A, orthogonal to the column space. Solving the equation
yields</p>
<p><span class="math display">\[
x^{&#39;}=(A^TA)^{-1}A^Tb \\
p=Ax^{&#39;}=A(A^TA)^{-1}A^Tb \\
\]</span></p></li>
<li><p>The n-dimensional mode of the projection matrix P:</p>
<p><span class="math display">\[
A(A^TA)^{-1}A^T \\
\]</span></p></li>
<li><p>The n-dimensional mode of projection matrix P still retains the
two properties of the 1-dimensional mode</p></li>
<li><p>Returning to the pursuit of the optimal solution, a common
example is to fit a straight line using the least squares
method</p></li>
<li><p>Known three points <span
class="math inline">\(a_1,a_2,a_3\)</span> , find a straight line to fit
close to the three points, <span
class="math inline">\(b=C+Da\)</span></p></li>
<li><p>If <span
class="math inline">\(a_1=(1,1),a_2=(2,2),a_3=(3,2)\)</span> , then</p>
<p><span class="math display">\[
C+D=1 \\
C+2D=2 \\
C+3D=2 \\
\]</span></p>
<p>Written in linear form as:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1  \\
1 &amp; 2  \\
1 &amp; 3  \\
\end{bmatrix}
\begin{bmatrix}
C  \\
D  \\
\end{bmatrix}
\begin{bmatrix}
1  \\
2  \\
2  \\
\end{bmatrix}
\]</span></p></li>
<li><p>Ax=b, the number of equations is greater than the number of
unknowns. If both sides are multiplied by the transpose of A, that is,
to find <span class="math inline">\(x^{&#39;}\)</span> , then the
fitting line can be obtained. The next lecture will continue with this
example.</p></li>
</ul>
<h1 id="lecture-15-projection-matrices-and-least-squares-method">Lecture
15: Projection Matrices and Least Squares Method</h1>
<h2 id="projection-matrix-1">Projection matrix</h2>
<ul>
<li><p>Reviewing, <span class="math inline">\(P=A(A^TA)^{-1}A^T\)</span>
, <span class="math inline">\(Pb\)</span> is the projection of b onto
the column space of A. Now consider two extreme cases: b being in the
column space and b being orthogonal to the column space: b in the column
space: <span class="math inline">\(Pb=b\)</span> ; Proof: If b is in the
column space, it can be expressed as <span
class="math inline">\(b=Ax\)</span> , under the condition that the
columns of A are linearly independent, <span
class="math inline">\((A^TA)\)</span> is invertible, substituting <span
class="math inline">\(P=A(A^TA)^{-1}A^T\)</span> yields <span
class="math inline">\(Pb=b\)</span> ; b is orthogonal to the column
space, <span class="math inline">\(Pb=0\)</span> ; Proof: If b is
orthogonal to the column space, then b is in the left null space, i.e.,
<span class="math inline">\(A^Tb=0\)</span> , it is obvious that
substituting <span class="math inline">\(P=A(A^TA)^{-1}A^T\)</span>
gives <span class="math inline">\(Pb=0\)</span></p></li>
<li><p>p is the projection of b onto the column space, since the column
space is orthogonal to the left null space, and thus e is the projection
of b onto the left null space, as shown in the figure:</p>
<p><span class="math display">\[
b=p+e \\
p=Pb \\
\]</span></p></li>
<li><p>Therefore</p>
<p><span class="math display">\[
e=(I-P)b \\
\]</span></p></li>
<li><p>Therefore, the projection matrix of the left null space is <span
class="math inline">\((I-P)\)</span></p></li>
</ul>
<h2 id="least-squares-method">Least Squares Method</h2>
<ul>
<li><p>Returning to the example from the previous lecture, find the
optimal straight line that approximates three points, minimizing the
error, as shown in the figure</p></li>
<li><p>Establish the line as <span class="math inline">\(y=C+Dt\)</span>
, substitute the coordinates of three points to obtain a system of
equations</p>
<p><span class="math display">\[
C+D=1 \\
C+2D=2 \\
C+3D=2 \\
\]</span></p></li>
<li><p>This equation set has no solution but has an optimal price, from
an algebraic perspective:</p>
<p><span class="math display">\[
||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\
\]</span></p></li>
<li><p>分别对 C 和 D 求偏导为 0，得到方程组:</p>
<p><span class="math display">\[
\begin{cases}
3C+6D=5\\
6C+14D=11\\
\end{cases}
\]</span></p></li>
<li><p>Written in matrix form, here, <span
class="math inline">\(C,D\)</span> exists in only one form, and they are
unsolvable. To solve <span class="math inline">\(C,D\)</span> , it is
treated as a fitting line, i.e., b is replaced by <span
class="math inline">\(C,D\)</span> when p is the projection.</p>
<p><span class="math display">\[
Ax=b \\
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
\end{bmatrix}
\begin{bmatrix}
C \\
D \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
2 \\
2 \\
\end{bmatrix}
\]</span></p></li>
<li><p>A satisfies the linear independence of each column, b is not in
the column space of A, now we want to minimize the error <span
class="math inline">\(e=Ax-b\)</span> , how to quantify the error? By
squaring its length <span class="math inline">\(||e||^2\)</span> , which
in the graph is the sum of the squares of the distances of points to the
fitted line along the y-axis. The error line segments <span
class="math inline">\(b_1,b_2,b_3\)</span> of these points <span
class="math inline">\(e_1,e_2,e_3\)</span> intersect with the fitted
line at <span class="math inline">\(p_1,p_2,p_3\)</span> , and when the
three b points are replaced by three p points, the system of equations
has a solution.</p></li>
<li><p>To solve <span class="math inline">\(x^{&#39;},p\)</span> , given
<span class="math inline">\(p=Ax^{&#39;}=A(A^TA)^{-1}A^Tb\)</span> ,
<span class="math inline">\(Ax=b\)</span> , multiplying both sides by
<span class="math inline">\(A^T\)</span> and combining them yields</p>
<p><span class="math display">\[
A^TAx^{&#39;}=A^Tb
\]</span></p></li>
<li><p>Substituting the values yields</p>
<p><span class="math display">\[
\begin{cases}
3C+6D=5\\
6C+14D=11\\
\end{cases}
\]</span></p></li>
<li><p>As with the result of taking partial derivatives algebraically,
it is then possible to solve out <span
class="math inline">\(C,D\)</span> , thus obtaining the fitting
line</p></li>
<li><p>Review the two preceding figures, one explaining the relationship
<span class="math inline">\(b,p,e\)</span> , and the other using <span
class="math inline">\(C,D\)</span> to determine the fitting line, with
the column combination determined by <span
class="math inline">\(C,D\)</span> being vector p</p></li>
<li><p>If the columns of matrix A are linearly independent, then <span
class="math inline">\(A^TA\)</span> is invertible, and this is a
prerequisite for the use of the least squares method. Proof: If a matrix
is invertible, then its null space consists only of the zero vector,
i.e., x in <span class="math inline">\(A^TAx=0\)</span> must be the zero
vector</p>
<p><span class="math display">\[
A^TAx=0 \\
x^TA^TAx=0 \\
(Ax)^T(Ax)=0 \\
Ax=0 \\
\]</span></p></li>
<li><p>Since the columns of A are linearly independent, therefore</p>
<p><span class="math display">\[
x=0
\]</span></p></li>
<li><p>Proof by construction</p></li>
<li><p>For handling mutually perpendicular unit vectors, we introduce
the standard orthogonal vector group, where the columns of this matrix
are both standard orthogonal and unit vectors. The next lecture will
introduce more about the standard orthogonal vector group</p></li>
</ul>
<h1
id="lecture-16-orthogonal-matrices-and-gram-schmidt-orthogonalization">Lecture
16: Orthogonal Matrices and Gram-Schmidt Orthogonalization</h1>
<h2 id="orthogonal-matrix">Orthogonal matrix</h2>
<ul>
<li><p>A set of orthogonal vectors is known</p>
<p><span class="math display">\[
q_i^Tq_j=
\begin{cases}
0 \quad if \quad i \neq j \\
1 \quad if \quad i=j \\
\end{cases}
\]</span></p>
<p><span class="math display">\[
Q=
\begin{bmatrix}
q_1 &amp; q_2 &amp; ... &amp; q_n \\
\end{bmatrix} \\
Q^TQ=I \\
\]</span></p></li>
<li><p>Therefore, for a square matrix with standard orthogonal columns,
<span class="math inline">\(Q^TQ=I\)</span> , <span
class="math inline">\(Q^T=Q^{-1}\)</span> , i.e., orthogonal matrices,
for example</p>
<p><span class="math display">\[
Q=\begin{bmatrix}
cos \theta &amp; -sin \theta \\
sin \theta &amp; cos \theta \\
\end{bmatrix}or
\frac {1}{\sqrt 2}
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Q is not necessarily a square matrix. The columns of Q will be
the standard orthogonal basis of the column space.</p></li>
<li><p>What is the projection matrix P onto the column space of Q for
Q?</p></li>
</ul>
<h2 id="gram-schmidt-orthogonalization">Gram-Schmidt
orthogonalization</h2>
<ul>
<li><p>Given two non-orthogonal vectors a and b, we wish to obtain two
orthogonal vectors A, B from a, b, where A can be set as a, and B is the
error vector e, which is the projection of b onto A:</p>
<p><span class="math display">\[
B=e=b-\frac{A^Tb}{A^TA}A
\]</span></p></li>
<li><p>Orthogonal basis is A, B divided by their lengths <span
class="math inline">\(q_1=\frac{A}{||A||}\)</span></p></li>
<li><p>Extended to the case of three vectors, A, B, and C, from the
above formula we know that A, B, and similarly, C needs to have the
projection components onto A and B subtracted</p>
<p><span class="math display">\[
C=c- \frac {A^Tc}{A^TA}A- \frac {B^Tc}{B^TB} B
\]</span></p></li>
<li><p>The matrix A composed of column vectors a, b is orthogonalized
into an orthogonal matrix Q through Schmidt orthogonalization, and it
can be seen from the formula derivation that the columns <span
class="math inline">\(q_1,q_2,...\)</span> and <span
class="math inline">\(a,b,....\)</span> of Q are in the same column
space; orthogonalization can be written as</p>
<p><span class="math display">\[
A=QR \\
\]</span></p></li>
<li><p>即</p>
<p><span class="math display">\[
\begin{bmatrix}
a &amp; b \\
\end{bmatrix}=
\begin{bmatrix}
q_1 &amp; q_2 \\
\end{bmatrix}
\begin{bmatrix}
q_1^Ta &amp; q_1^Tb \\
q_2^Ta &amp; q_2^Tb \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>Among them, because of <span
class="math inline">\(QQ^T=I\)</span></p></li>
<li><p>Therefore <span class="math inline">\(R=Q^TA\)</span></p></li>
<li><p><span class="math inline">\(q_2\)</span> is orthogonal to <span
class="math inline">\(q_1\)</span> , while <span
class="math inline">\(q_1\)</span> is just the unitization of <span
class="math inline">\(a\)</span> , therefore <span
class="math inline">\(q_2^Ta=0\)</span> , i.e., <span
class="math inline">\(R\)</span> , is an upper triangular
matrix</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h2 id="生成空间基">生成空间、基</h2>
<ul>
<li><p><span
class="math inline">\(v_1...,v_l\)</span>生成了一个空间，是指这个空间包含这些向量的所有线性组合。</p></li>
<li><p>向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。</p></li>
<li><p>举个栗子：求<span
class="math inline">\(R^3\)</span>的一组基，最容易想到的是</p>
<p><span class="math display">\[
\begin{bmatrix}
1   \\
0   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
0   \\
1   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
0   \\
0   \\
1   \\
\end{bmatrix}
\]</span></p></li>
<li><p>这是一组标准基，另一个栗子:</p>
<p><span class="math display">\[
\begin{bmatrix}
1   \\
1   \\
2   \\
\end{bmatrix}
,
\begin{bmatrix}
2   \\
2   \\
5   \\
\end{bmatrix}
\]</span></p></li>
<li><p>显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。</p></li>
<li><p>如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。</p></li>
<li><p>若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。</p></li>
<li><p>基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。</p></li>
</ul>
<h2 id="维数">维数</h2>
<ul>
<li>上面提到的所有基向量的个数相同，这个个数就是空间的维数。<strong>不是基向量的维数，而是基向量的个数</strong></li>
</ul>
<h2 id="最后举个栗子">最后举个栗子</h2>
<p>对矩阵A <span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 3 &amp;1  \\
1 &amp; 1 &amp; 2 &amp; 1   \\
1 &amp; 2 &amp; 3 &amp; 1 \\
\end{bmatrix}
\]</span></p>
<ul>
<li><p>四列并不线性无关，可取第一列第二列为主列</p></li>
<li><p>2=A的秩=主列数=列空间维数</p></li>
<li><p>第一列和第二列构成列空间的一组基。</p></li>
<li><p>如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。</p></li>
<li><p>零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：</p>
<p><span class="math display">\[
\begin{bmatrix}
-1   \\
-1  \\
1   \\
0   \\
\end{bmatrix}
,
\begin{bmatrix}
-1   \\
0  \\
0   \\
1    \\
\end{bmatrix}
\]</span></p></li>
<li><p>这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。</p></li>
</ul>
<h1 id="第十讲四个基本子空间">第十讲：四个基本子空间</h1>
<ul>
<li><p>列空间C(A)，零空间N(A)，行空间C(<span
class="math inline">\(A^T\)</span>)，左零空间N(<span
class="math inline">\(A^T\)</span>)。</p></li>
<li><p>分别处于<span
class="math inline">\(R^m、R^n、R^n、R^m\)</span>空间中</p></li>
<li><p>列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r</p></li>
<li><p>列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行</p></li>
<li><p>行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化</p></li>
<li><p>为什么叫做左零空间？</p>
<p><span class="math display">\[
rref\begin{bmatrix}
A_{m*n} &amp; I_{m*n}
\end{bmatrix}\rightarrow
\begin{bmatrix}
R_{m*n} &amp; E_{m*n}
\end{bmatrix} \\
\]</span></p></li>
<li><p>易得rref=E，即EA=R</p></li>
<li><p>通过E可以计算左零空</p></li>
<li><p>求左零空间即找一个产生零行向量的行组合</p></li>
<li><p>左零空间的基就是R非0行对应的E行,共m-r行</p></li>
</ul>
<h1
id="第十一讲矩阵空间秩1矩阵和小世界图">第十一讲：矩阵空间、秩1矩阵和小世界图</h1>
<h2 id="矩阵空间">矩阵空间</h2>
<ul>
<li><p>可以看成向量空间，可以数乘，可以相加</p></li>
<li><p>以<span
class="math inline">\(3*3\)</span>矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 1 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 1  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}.....
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>再来研究<span
class="math inline">\(3*3\)</span>矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 1  \\
0 &amp; 0 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
\end{bmatrix}，
\begin{bmatrix}
0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1  \\
0 &amp; 1 &amp; 0  \\
\end{bmatrix}
\]</span></p></li>
<li><p>对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基</p></li>
<li><p>接着再来研究<span class="math inline">\(S \bigcap U\)</span>
，易得这个子空间即对角矩阵D，维度为3</p></li>
<li><p>如果是$S U
$呢？他们的并的基可以得到所有M的基，所以其维数是9</p></li>
<li><p>整理一下可得</p>
<p><span class="math display">\[
dim(S)=6,dim(U)=6,dim(S \bigcap U)=3,dim(S \bigcup U)=3 \\
dim(S)+dim(U)=dim(S \bigcap U)+dim(S \bigcup U) \\
\]</span></p></li>
<li><p>再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间</p>
<p><span class="math display">\[
\frac{d^2y}{dx^2}+y=0
\]</span></p></li>
<li><p>他的几个解为</p>
<p><span class="math display">\[
y=cos(x),y=sin(x)
\]</span></p></li>
<li><p>完整解为</p>
<p><span class="math display">\[
y=c_1cos(x)+c_2sin(x)
\]</span></p></li>
<li><p>即得到一个向量空间，基为2</p></li>
</ul>
<h2 id="秩1矩阵">秩1矩阵</h2>
<ul>
<li><p>先写一个简单的秩1矩阵</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 4 &amp; 5 \\
2 &amp; 8 &amp; 10 \\
\end{bmatrix}=
\begin{bmatrix}
1  \\
2  \\
\end{bmatrix}*
\begin{bmatrix}
1 &amp; 4 &amp; 5 \\
\end{bmatrix}
\]</span></p></li>
<li><p>所有的秩1矩阵都可以表示为一列乘一行</p></li>
<li><p>秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成</p></li>
<li><p>再来看一个秩1矩阵的栗子，在四维空间中，设向量<span
class="math inline">\(v=(v_1,v_2,v_3,v_4)\)</span>,集合<span
class="math inline">\(S=\{v|v_1+v_2+v_3+v_4=0\}\)</span>,假如把S看成零空间，则相应的方程<span
class="math inline">\(Av=0\)</span>中的矩阵A为</p>
<p><span class="math display">\[
A=\begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>易得<span
class="math inline">\(dimN(A)=n-r\)</span>，所以S的维数是<span
class="math inline">\(4-1=3\)</span>，S的一组基为</p>
<p><span class="math display">\[
\begin{bmatrix}
-1  \\
1  \\
0  \\
0  \\
\end{bmatrix},
\begin{bmatrix}
-1  \\
0  \\
1  \\
0  \\
\end{bmatrix},
\begin{bmatrix}
-1  \\
0  \\
0  \\
1  \\
\end{bmatrix}
\]</span></p></li>
<li><p>矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间<span
class="math inline">\(C(A^T)=\{a,a,a,a\}​\)</span>，列空间<span
class="math inline">\(C(A)=R^1​\)</span>，零空间<span
class="math inline">\(N(A)​\)</span>即S的基线性组合，<span
class="math inline">\(N(A^T)={0}​\)</span></p></li>
<li><p>整理一下</p>
<p><span class="math display">\[
dim(N(A))+dim(C(A^T))=3+1=4=n \\
dim(C(A))+dim(N(A^T))=1+0=1=m \\
\]</span></p></li>
</ul>
<h2 id="小世界图">小世界图</h2>
<ul>
<li>仅仅引入了图的概念，为下一讲准备</li>
</ul>
<h1 id="第十二讲图和网络">第十二讲：图和网络</h1>
<h2 id="图">图</h2>
<ul>
<li>图的一些基础概念，略过</li>
</ul>
<h2 id="网络">网络</h2>
<ul>
<li><p>图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0</p></li>
<li><p>构成回路的几行线性相关，回路意味着相关</p></li>
<li><p>关联矩阵A描述了图的拓扑结构</p></li>
<li><p><span class="math inline">\(dimN(A^T)=m-r​\)</span></p></li>
<li><p>假如图的节点是电势，<span
class="math inline">\(Ax\)</span>中x即电势，<span
class="math inline">\(Ax=0\)</span>得到一组电势差方程，零空间是一维的，<span
class="math inline">\(A^Ty\)</span>中y即边上的电流，电流与电势差的关系即欧姆定律，<span
class="math inline">\(A^Ty=0\)</span>得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路</p></li>
<li><p>树就是没有回路的图</p></li>
<li><p>再来看看<span
class="math inline">\(dimN(A^T)=m-r\)</span></p></li>
<li><p><span
class="math inline">\(dimN(A^T)\)</span>=无关回路数</p></li>
<li><p><span class="math inline">\(m\)</span>=边数</p></li>
<li><p><span class="math inline">\(r=n-1\)</span>=节点数-1
(因为零空间是一维的)</p></li>
<li><p>即:节点数-边数+回路数=1(欧拉公式)</p></li>
</ul>
<h2 id="总结"><font size=4>总结</h2>
<ul>
<li><p>将电势记为e,<span class="math inline">\(e=Ax\)</span></p></li>
<li><p>电势差导致电流产生，<span
class="math inline">\(y=Ce\)</span></p></li>
<li><p>电流满足基尔霍夫电流方程,<span
class="math inline">\(A^Ty=0\)</span></p></li>
<li><p>将三个方程联立：</p>
<p><span class="math display">\[
A^TCAx=f
\]</span></p>
<p>这就是应用数学中最基本的平衡方程</p></li>
</ul>
<h1 id="第十三讲正交向量与子空间">第十三讲：正交向量与子空间</h1>
<h2 id="正交向量">正交向量</h2>
<ul>
<li><p>正交即垂直，意味着在n维空间内，这些向量的夹角是90度</p></li>
<li><p>当<span
class="math inline">\(x^Ty=0\)</span>,x与y正交，证明：</p></li>
<li><p>若x与y正交，易得:</p>
<p><span class="math display">\[
{||x||}^2+{||y||}^2={||x+y||}^2 \\
\]</span></p></li>
<li><p>即：</p>
<p><span class="math display">\[
x^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\
\]</span></p></li>
<li><p>即：</p>
<p><span class="math display">\[
x^Ty=0 \\
\]</span></p></li>
<li><p>子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交</p></li>
<li><p>若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己</p></li>
<li><p>行空间正交于零空间，因为<span
class="math inline">\(Ax=0\)</span>，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分</p></li>
<li><p>图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立</p></li>
<li><p>图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。</p></li>
</ul>
<h2 id="正交子空间">正交子空间</h2>
<ul>
<li>例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交</li>
<li>因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量</li>
<li>以上是所有关于解<span
class="math inline">\(Ax=0\)</span>的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵<span
class="math inline">\(A^TA\)</span></li>
<li><span class="math inline">\(A^TA\)</span>是一个<span
class="math inline">\(n*n\)</span>的方阵，而且对称</li>
<li>坏方程转换为好方程，两边同乘<span
class="math inline">\(A^T\)</span></li>
<li><span class="math inline">\(A^TA\)</span>不总是可逆，若可逆，则<span
class="math inline">\(N(A^TA)=N(A)\)</span>，且他们的秩相同</li>
<li><span
class="math inline">\(A^TA\)</span>可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质</li>
</ul>
<h1 id="第十四讲子空间投影">第十四讲：子空间投影</h1>
<h2 id="投影">投影</h2>
<ul>
<li><p>在二维情况下讨论投影</p></li>
<li><p>一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p</p></li>
<li><p>p在a的一维子空间里，是a的x倍，即p=xa</p></li>
<li><p>a垂直于e，即</p>
<p><span class="math display">\[
a^T(b-xa)=0 \\
xa^Ta=a^Tb \\
x= \frac {a^Tb}{a^Ta} \\
p=a\frac {a^Tb}{a^Ta} \\
\]</span></p></li>
<li><p>从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了</p></li>
</ul>
<h2 id="投影矩阵">投影矩阵</h2>
<ul>
<li>现在可以引入投影矩阵P的一维模式(projection matrix)，<span
class="math inline">\(p=Pb\)</span>,<span class="math inline">\(P= \frac
{aa^T}{a^Ta}\)</span></li>
<li>用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1</li>
<li>投影矩阵的另外两条性质：
<ul>
<li>对称,即<span class="math inline">\(P^T=P\)</span></li>
<li>两次投影在相同的位置，即<span
class="math inline">\(P^2=P\)</span></li>
</ul></li>
</ul>
<h2 id="投影的意义">投影的意义</h2>
<ul>
<li><p>下面在高维情况下讨论</p></li>
<li><p>当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解</p></li>
<li><p>如何找？将b微调，使得b在列空间中</p></li>
<li><p>怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解<span
class="math inline">\(Ax^{&#39;}=p\)</span>,p时b在列空间上的投影</p></li>
<li><p>现在我们要求<span class="math inline">\(x^{&#39;}\)</span>,<span
class="math inline">\(p=Ax^{&#39;}\)</span>，误差向量<span
class="math inline">\(e=b-Ax^{&#39;}\)</span>，由投影定义可知e需要垂直于A的列空间</p></li>
<li><p>综上可得</p>
<p><span class="math display">\[
A^T(b-Ax^{&#39;})=0 \\
\]</span></p></li>
<li><p>由上式可以看出e在A的左零空间，与列空间正交。解上式可得</p>
<p><span class="math display">\[
x^{&#39;}=(A^TA)^{-1}A^Tb \\
p=Ax^{&#39;}=A(A^TA)^{-1}A^Tb \\
\]</span></p></li>
<li><p>即投影矩阵P的n维模式:</p>
<p><span class="math display">\[
A(A^TA)^{-1}A^T \\
\]</span></p></li>
<li><p>投影矩阵P的n维模式依然保留了1维模式的两个性质</p></li>
<li><p>现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线</p></li>
<li><p>已知三个点<span
class="math inline">\(a_1,a_2,a_3\)</span>，找出一条直线拟合接近三个点,<span
class="math inline">\(b=C+Da\)</span></p></li>
<li><p>假如<span
class="math inline">\(a_1=(1,1),a_2=(2,2),a_3=(3,2)\)</span>,则</p>
<p><span class="math display">\[
C+D=1 \\
C+2D=2 \\
C+3D=2 \\
\]</span></p>
<p>写成线代形式为:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1  \\
1 &amp; 2  \\
1 &amp; 3  \\
\end{bmatrix}
\begin{bmatrix}
C  \\
D  \\
\end{bmatrix}
\begin{bmatrix}
1  \\
2  \\
2  \\
\end{bmatrix}
\]</span></p></li>
<li><p>即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求<span
class="math inline">\(x^{&#39;}\)</span>，这样就可以求出拟合直线。下一讲继续此例</p></li>
</ul>
<h1
id="第十五讲投影矩阵和最小二乘法">第十五讲：投影矩阵和最小二乘法</h1>
<h2 id="投影矩阵-1">投影矩阵</h2>
<ul>
<li><p>回顾，<span
class="math inline">\(P=A(A^TA)^{-1}A^T\)</span>,<span
class="math inline">\(Pb\)</span>即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：
b在列空间上：<span
class="math inline">\(Pb=b\)</span>；证明：若b在列空间上，则可以表示为<span
class="math inline">\(b=Ax\)</span>，在A各列线性无关的条件下，<span
class="math inline">\((A^TA)\)</span>可逆，代入<span
class="math inline">\(P=A(A^TA)^{-1}A^T\)</span>有<span
class="math inline">\(Pb=b\)</span> b正交于列空间，<span
class="math inline">\(Pb=0\)</span>；证明：若b正交于列空间则b在左零空间内，即<span
class="math inline">\(A^Tb=0\)</span>，显然代入<span
class="math inline">\(P=A(A^TA)^{-1}A^T\)</span>有<span
class="math inline">\(Pb=0\)</span></p></li>
<li><p>p是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：</p>
<p><span class="math display">\[
b=p+e \\
p=Pb \\
\]</span></p></li>
<li><p>所以</p>
<p><span class="math display">\[
e=(I-P)b \\
\]</span></p></li>
<li><p>所以左零空间的投影矩阵为<span
class="math inline">\((I-P)\)</span></p></li>
</ul>
<h2 id="最小二乘法">最小二乘法</h2>
<ul>
<li><p>回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图</p></li>
<li><p>设直线为<span
class="math inline">\(y=C+Dt\)</span>，代入三个点坐标得到一个方程组</p>
<p><span class="math display">\[
C+D=1 \\
C+2D=2 \\
C+3D=2 \\
\]</span></p></li>
<li><p>此方程组无解但是存在最优价，从代数角度看：</p>
<p><span class="math display">\[
||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\
\]</span></p></li>
<li><p>分别对C和D求偏导为0，得到方程组:</p>
<p><span class="math display">\[
\begin{cases}
3C+6D=5\\
6C+14D=11\\
\end{cases}
\]</span></p></li>
<li><p>写成矩阵形式，这里的<span
class="math inline">\(C,D\)</span>仅仅存在一个形式，他们无解，要解出<span
class="math inline">\(C,D\)</span>是将其作为拟合直线，即b被替换为投影p时的<span
class="math inline">\(C,D\)</span>。</p>
<p><span class="math display">\[
Ax=b \\
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 2 \\
1 &amp; 3 \\
\end{bmatrix}
\begin{bmatrix}
C \\
D \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
2 \\
2 \\
\end{bmatrix}
\]</span></p></li>
<li><p>A满足各列线性无关，b不在A的列空间里，现在我们想最小化误差<span
class="math inline">\(e=Ax-b\)</span>，怎么量化误差？求其长度的平方<span
class="math inline">\(||e||^2\)</span>，在图中即y轴方向上点到拟合直线的距离的平方和。这些点<span
class="math inline">\(b_1,b_2,b_3\)</span>的误差线段<span
class="math inline">\(e_1,e_2,e_3\)</span>与拟合直线交于<span
class="math inline">\(p_1,p_2,p_3\)</span>，当将三个b点用三个p点取代时，方程组有解。</p></li>
<li><p>现在要解出<span
class="math inline">\(x^{&#39;},p\)</span>，已知<span
class="math inline">\(p=Ax^{&#39;}=A(A^TA)^{-1}A^Tb\)</span>，<span
class="math inline">\(Ax=b\)</span>，两边同乘<span
class="math inline">\(A^T\)</span>联立有</p>
<p><span class="math display">\[
A^TAx^{&#39;}=A^Tb
\]</span></p></li>
<li><p>代入数值可得</p>
<p><span class="math display">\[
\begin{cases}
3C+6D=5\\
6C+14D=11\\
\end{cases}
\]</span></p></li>
<li><p>与代数求偏导数结果一样,之后可以解出<span
class="math inline">\(C,D\)</span>，也就得到了拟合直线</p></li>
<li><p>回顾一下上面两幅图，一张解释了<span
class="math inline">\(b,p,e\)</span>的关系，另一张用<span
class="math inline">\(C,D\)</span>确定了拟合直线，由<span
class="math inline">\(C,D\)</span>确定的列组合就是向量p</p></li>
<li><p>如果A的各列线性无关，则<span
class="math inline">\(A^TA\)</span>是可逆的，这时最小二乘法使用的前提，证明：
如果矩阵可逆，则它的零空间仅为零向量，即<span
class="math inline">\(A^TAx=0\)</span>中x必须是零向量</p>
<p><span class="math display">\[
A^TAx=0 \\
x^TA^TAx=0 \\
(Ax)^T(Ax)=0 \\
Ax=0 \\
\]</span></p></li>
<li><p>又因为A各列线性无关，所以</p>
<p><span class="math display">\[
x=0
\]</span></p></li>
<li><p>即证</p></li>
<li><p>对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容</p></li>
</ul>
<h1
id="第十六讲正交矩阵和gram-schmidt正交化">第十六讲：正交矩阵和Gram-Schmidt正交化</h1>
<h2 id="正交矩阵">正交矩阵</h2>
<ul>
<li><p>已知一组正交向量集</p>
<p><span class="math display">\[
q_i^Tq_j=
\begin{cases}
0 \quad if \quad i \neq j \\
1 \quad if \quad i=j \\
\end{cases}
\]</span></p>
<p><span class="math display">\[
Q=
\begin{bmatrix}
q_1 &amp; q_2 &amp; ... &amp; q_n \\
\end{bmatrix} \\
Q^TQ=I \\
\]</span></p></li>
<li><p>所以，对有标准正交列的方阵，<span
class="math inline">\(Q^TQ=I\)</span>,<span
class="math inline">\(Q^T=Q^{-1}\)</span>,即正交矩阵，例如</p>
<p><span class="math display">\[
Q=\begin{bmatrix}
cos \theta &amp; -sin \theta \\
sin \theta &amp; cos \theta \\
\end{bmatrix}or
\frac {1}{\sqrt 2}
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Q不一定是方阵。Q的各列将是列空间的标准正交基</p></li>
<li><p>对Q，投影到Q的列空间的投影矩阵P是什么？<span
class="math inline">\(P=Q(Q^TQ)^{-1}Q^T=QQ^T\)</span></p></li>
</ul>
<h2 id="gram-schmidt正交化">Gram-Schmidt正交化</h2>
<ul>
<li><p>给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：</p>
<p><span class="math display">\[
B=e=b-\frac{A^Tb}{A^TA}A
\]</span></p></li>
<li><p>正交基就是A,B除以他们的长度<span
class="math inline">\(q_1=\frac{A}{||A||}\)</span></p></li>
<li><p>扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量</p>
<p><span class="math display">\[
C=c- \frac {A^Tc}{A^TA}A- \frac {B^Tc}{B^TB} B
\]</span></p></li>
<li><p>由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列<span
class="math inline">\(q_1,q_2,...\)</span>与<span
class="math inline">\(a,b,....\)</span>在同一列空间内，正交化可以写成</p>
<p><span class="math display">\[
A=QR \\
\]</span></p></li>
<li><p>即</p>
<p><span class="math display">\[
\begin{bmatrix}
a &amp; b \\
\end{bmatrix}=
\begin{bmatrix}
q_1 &amp; q_2 \\
\end{bmatrix}
\begin{bmatrix}
q_1^Ta &amp; q_1^Tb \\
q_2^Ta &amp; q_2^Tb \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>其中，因为<span class="math inline">\(QQ^T=I\)</span></p></li>
<li><p>所以<span class="math inline">\(R=Q^TA\)</span></p></li>
<li><p><span class="math inline">\(q_2\)</span>与<span
class="math inline">\(q_1\)</span>正交，而<span
class="math inline">\(q_1\)</span>只是<span
class="math inline">\(a\)</span>的单位化，所以<span
class="math inline">\(q_2^Ta=0\)</span>，即<span
class="math inline">\(R\)</span>是上三角矩阵</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>linearalgebra</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP Basics</title>
    <url>/2018/03/07/NLPBasic/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>Recorded some basic knowledge of deep learning learned when recording
the seq2seq model in the entry-level NLP.</p>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0I5WV.jpg" alt="i0I5WV.jpg" />
<figcaption aria-hidden="true">i0I5WV.jpg</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="neural-networks-with-feedforward-architecture">Neural Networks
with Feedforward Architecture</h1>
<ul>
<li><p>When the dimensionality of the data is very high, the capacity of
the sample space may be much greater than the number of training
samples, leading to the curse of dimensionality.</p></li>
<li><p>Activation function. Used to represent the nonlinear
transformation in neural networks. Without an activation function, if
the neural network only has a weight matrix, it is a combination of
linear transformations and remains a linear transformation. The use of
nonlinear transformations can extract features that are convenient for
linear transformations, avoiding the dimensionality disaster
(?).</p></li>
<li><p>Softmax and sigmoid significance: Maximum entropy, convenient for
differentiation, universal approximation theorem (with squeezing
property)</p></li>
<li><p>Hidden layers can use the activation function ReLU, i.e.,
rectified linear, but the drawback is that it cannot use gradient
learning to activate the function to 0 for the samples, so three
extensions have been developed:</p>
<p><span class="math display">\[
g(z,\alpha)_i = max(0,z_i) + \alpha _i min(0,z_i)
\]</span></p>
<p>Absolute value rectification: right coefficient is -1; leakage
rectification: right coefficient is fixed to a smaller value;
parameterized rectification: coefficients are placed in the model for
learning</p></li>
</ul>
<h1 id="backpropagation">Backpropagation</h1>
<ul>
<li>Backpropagation calculates the parameter update values by
determining the output biases through gradients, updating parameters
layer by layer from the output layer to the input layer, propagating the
gradients used by the previous layer, and utilizing the chain rule of
calculus for vectors. The update amount for each layer = the Jacobian
matrix of the current layer * the gradient of the previous layer.</li>
<li>Backpropagation in Neural Networks: <img data-src="https://s1.ax1x.com/2018/10/20/i0IIzT.png" alt="i0IIzT.png" />
Initialize the gradient table. The last layer's output calculates the
gradient with respect to the output, so the initial value is 1. The loop
proceeds from the end to the beginning, with the gradient table of the
current layer being the product of the current layer's Jacobian matrix
and the gradient table of the previous layer (i.e., chain rule
differentiation). The current layer uses the gradient table of the
previous layer for calculation and storage, avoiding repeated
calculations in the chain rule.</li>
</ul>
<h1 id="recurrent-neural-network-rnn">Recurrent Neural Network
(RNN)</h1>
<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>
<ul>
<li><p>Features: All hidden layers share parameters, treating hidden
layers as state variables for convenient parameterization.</p></li>
<li><p>Using hyperbolic tangent as the hidden layer activation
function</p></li>
<li><p>Input x, x passes through a weighted matrix and is activated by a
hidden layer to obtain h, h passes through a weighted matrix to output
o, the cost L, and o is activated by an output layer to obtain
y</p></li>
<li><p>Basic Structure (Expanded and Non-Expanded): <img data-src="https://s1.ax1x.com/2018/10/20/i0I7yF.png"
alt="i0I7yF.png" /></p></li>
<li><p>Several variants:</p>
<ul>
<li>Each time step has an output, with recurrent connections between
hidden layers: <img data-src="https://s1.ax1x.com/2018/10/20/i0ITQU.png"
alt="i0ITQU.png" /></li>
<li>Each time step has an output, and there is a recurrent connection
between the output and the hidden layer: <img data-src="https://s1.ax1x.com/2018/10/20/i0IqeJ.png" alt="i0IqeJ.png" /></li>
<li>Read the entire sequence and produce a single output: <img data-src="https://s1.ax1x.com/2018/10/20/i0oCOe.png" alt="i0oCOe.png" /></li>
</ul></li>
<li><p>Common forward propagation, softmax processing of output,
negative log-likelihood as the loss function, and the cost of
backpropagation through time is too high. Feedforward process:</p>
<p><span class="math display">\[
\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\
h^{(t)} = tanh(a^{(t)}), \\
o^{(t)} = c + Vh^{(t)}, \\
y^{(t)} = softmax(o^{(t)}) \\
\]</span></p>
<p>Cost function:</p>
<p><span class="math display">\[
L(\{ x^{(1)} , ... , x^{(\tau)}\},\{ y^{(1)} , ... , y^{(\tau)}\}) \\
=\sum _t L^{(t)} \\
= - \sum _t log p_{model} (y^{(t)}|\{ x^{(1)} , ... , x^{(\tau)}\}) \\
\]</span></p></li>
<li><p>Replaced with the second RNN, using output-to-hidden layer loops,
eliminating hidden-to-hidden layer loops, decoupling parallel(?), using
a mentor-driven model (train the loop network W to the hidden layer with
correct outputs, and use the actual output close to the correct output
during testing) Mentor-driven model: <img data-src="https://s1.ax1x.com/2018/10/20/i0ILw9.png"
alt="i0ILw9.png" /></p></li>
</ul>
<h2 id="bidirectional-rnn">Bidirectional RNN</h2>
<ul>
<li>Considering the dependency on future information, it is equivalent
to the combination of two hidden layers</li>
</ul>
<h2 id="sequence-to-sequence">Sequence to sequence</h2>
<ul>
<li>Using encoders and decoders, it is possible to have different
lengths for input and output sequences, generating representations
(input sequences to vectors), and then generating sequences (a vector
input mapped to a sequence). Sequence-to-sequence is a class of
frameworks, and the specific models used by the encoder and decoder can
be customized. For example, in machine translation, both the encoder and
decoder can use LSTM. End-to-end models utilize the intermediate
representation c, making the output depend only on the representation
and the previously output sequence. <img data-src="https://s1.ax1x.com/2018/10/20/i0IOoR.png" alt="i0IOoR.png" /></li>
</ul>
<h2 id="deep-rnn">Deep RNN</h2>
<ul>
<li>A. Deepen the cyclic state, decomposing it into multiple
hierarchical groups, i.e., deepen horizontally (hidden layer updates
within a single cycle are performed multiple times)</li>
<li>Introducing a neural network between input and hidden, hidden to
output, and hidden to hidden, i.e., deepening the hidden layer states
not only horizontally (time steps) but also vertically (a single
training).</li>
<li>C. Introducing skip connections to alleviate the path elongation
effect caused by deepening the network <img data-src="https://s1.ax1x.com/2018/10/20/i0IjF1.png" alt="i0IjF1.png" /></li>
</ul>
<h2 id="long-term-dependency-problem-in-rnn">Long-term dependency
problem in RNN</h2>
<ul>
<li>Long-term dependency problem: As models become deeper, they lose the
ability to learn previous information</li>
<li>Perform eigenvalue decomposition on the weight matrix, repeatedly
perform linear transformations, which is equivalent to matrix power
operations, and the eigenvalues are also subjected to power operations.
Eigenvalues with a magnitude greater than 1 will explode, and those less
than 1 will disappear. A severely deviated gradient value can lead to a
gradient cliff (learning a very large update). If it has exploded, the
solution is to use gradient clipping, using the direction of the
calculated gradient but limiting the size to within a small step
length.</li>
<li>It is best to avoid gradient explosion. In recurrent neural
networks, transformations between hidden layers do not introduce
nonlinear transformations, which is equivalent to performing power
operations on the weight matrix, causing eigenvalues to explode or
vanish, and correspondingly, the gradients of long-term interactions
become exponentially small. Ways to avoid this include introducing skip
connections in the time dimension (adding edges over long time spans),
introducing leaky units (setting linear self-connected units with
weights close to 1), and removing edges over short time spans (retaining
only edges over long time spans).</li>
</ul>
<h2 id="gated-rnn">Gated RNN</h2>
<p>- Addressing the long-term dependency problem using a method similar
to the leaky unit, gated RNNs, including LSTM and GRU, were
introduced.</p>
<ul>
<li>Leakage Unit (?): We apply an update to µ(t) for certain v values as
µ(t) ← αµ(t−1) + (1−α)v(t), accumulating a moving average µ(t), where α
is an example of a linear self-connection from µ(t−1) to µ(t). When α
approaches 1, the moving average can remember information for a long
time in the past, while when α approaches 0, information about the past
is quickly discarded. The hidden unit µ with linear self-connection can
simulate the behavior of the moving average. This hidden unit is called
a leakage unit.</li>
</ul>
<h1 id="long-short-term-memory">Long Short-Term Memory</h1>
<ul>
<li>LSTM: makes the weights of the self-recurrent connections
context-dependent (gated control of the weights of this recurrence)</li>
<li>LSTM modifies the hidden layer nodes (cells) in the ordinary RNN,
with the internal structure as shown in the figure below: <img data-src="https://s1.ax1x.com/2018/10/20/i0IvJx.png" alt="i0IvJx.png" /></li>
<li>Visible in addition to the recurrent connections between cells in
RNNs, there is an internal loop containing a forget gate control (how
much to forget). The cell has an internal state s, which is different
from the cell output h used for hidden layer updates between different
time steps.</li>
<li>All gate units have sigmoid nonlinearities, with input units being
ordinary neurons that can use any nonlinear activation function.</li>
<li>Three gates receive the same type of input, i.e., the current input
x, the output of the cell at the previous time step h (not the internal
state s), each having an independent weight matrix and bias. The outputs
all pass through a sigmoid function to produce a value between 0 and 1,
respectively representing the degree of memory of the current internal
state s for the previous internal state, the degree of memory of the
current internal state for the current input, and the degree of
dependence of the current output on the current internal state of the
cell.</li>
<li>s: Updated internally based on two pieces of information: the
previous internal state controlled by the forget gate, and the sum of
the input controlled by the input gate and the cell output from the
previous time step (not depicted in the figure?).</li>
<li>Cell outputs h, the internal state overactivates the activation
function, controlled by the output gate.</li>
<li>Another more easily understandable figure: <img data-src="https://s1.ax1x.com/2018/10/20/i0IxW6.png" alt="i0IxW6.png" /></li>
</ul>
<h1 id="bidirectional-lstm">Bidirectional LSTM</h1>
<ul>
<li>As with bidirectional RNNs, each hidden layer node is an LSTM node,
and there are no connections between the two hidden layer nodes in the
bidirectional structure. Both hidden layers must be fully updated before
the output layer can be computed, and the output at each time step
depends on six weight matrices from w1 to w6.</li>
<li>Because each output layer node receives the output of two hidden
layer nodes, a processing step is required, and there are multiple ways
to do this:
<ul>
<li>Direct Connection (concat)</li>
<li>Sum <img data-src="https://s1.ax1x.com/2018/10/20/i0oSSK.png"
alt="i0oSSK.png" /></li>
</ul></li>
</ul>
<h1 id="word-embedding-word2vec">Word embedding, Word2Vec</h1>
<ul>
<li>Using the distributed representation of words (word embeddings or
word vectors) to model natural language sequences, through the training
of context-word pairs (one-hot vectors), a neural network is obtained,
and the weight matrix from the input layer to the hidden layer is
considered to contain all word vectors in the dictionary, i.e., the word
vector matrix. At this point, by passing the individual one-hot word
vectors through the neural network, a low-dimensional word embedding of
this word can be obtained in the hidden layer with the help of the
weight matrix (word vector matrix). Since word vectors are obtained as
by-products of training with context-word pairs, the distances between
word vectors in space have actual significance, i.e., words semantically
related have vectors that are closer together. A problem with this
generation method is the high dimensionality, because the output of the
neural network is the word vector, which is reduced to a one-hot vector
by softmax, representing the probability of each word. When the
dictionary capacity is very large, this leads to a very large
computational load in the final output layer. W2V is a practical scheme
for generating word vectors, which optimizes the generation of word
vectors based on the NLM model and solves the high-dimensional problem.
It utilizes two optimization schemes:</li>
<li>Hierarchical softmax: The output is no longer a probability vector
of dictionary size, but a tree, with leaf nodes being words, internal
nodes representing word groups, represented by conditional
probabilities, and using a logistic regression model. W2V utilizes this
model, eliminating the hidden layer, directly projecting the output of
the projection layer into the tree, and improving the tree to a Huffman
tree. Because the hidden layer is eliminated, W2V avoids large-scale
matrix computations linearly related to the dictionary size from the
projection layer to the hidden layer and from the hidden layer back to
the output layer, but because the number of leaf nodes in the tree is
still the same as the dictionary size, the final normalization
computation of probabilities is still very costly.</li>
<li>Important Sampling (?): This method reduces the computational load
by reducing the gradients that need to be calculated during
backpropagation. Each output word with the highest probability (positive
phase) should contribute the most to the gradient, while the negative
phase items with lower probabilities should contribute less. Therefore,
instead of calculating the gradients for all negative phase items, a
sampling of some is computed.</li>
<li>Incomplete understanding of several simplification methods for the
computation of softmax layers, to be improved. Recommended blog post:
Technology | Series of Blogs on Word Embeddings Part 2: Comparison of
Several Methods for Approximating Softmax in Language Modeling</li>
</ul>
<h1 id="attention-mechanism">Attention Mechanism</h1>
<ul>
<li>In the seq2seq model, the information provided by the encoder is all
compressed into an intermediate representation, i.e., the output of the
hidden layer state at the last time step of the encoder, and the decoder
decodes only based on this intermediate representation and the word
decoded in the previous step. However, when there are many time steps in
the encoder, the intermediate representation generally suffers from
severe information loss, and to solve this problem, an attention
mechanism is introduced.</li>
<li>The actual performance of attention is to generate intermediate
representations by weighted averaging over various time steps at the
encoding end, rather than generating uniformly at the final step of the
loop. The time steps at the encoding end with higher weights, which are
referred to as the attention points, contribute more information to the
decoding end.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="前馈神经网络相关">前馈神经网络相关</h1>
<ul>
<li><p>数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。</p></li>
<li><p>激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。</p></li>
<li><p>Softmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）</p></li>
<li><p>隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：</p>
<p><span class="math display">\[
g(z,\alpha)_i = max(0,z_i) + \alpha _i min(0,z_i)
\]</span></p>
<p>绝对值整流：右边系数为-1 渗漏整流：右边系数固定为一个较小值
参数化整流：系数放到模型中学习</p></li>
</ul>
<h1 id="反向传播">反向传播</h1>
<ul>
<li>反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。
每一层更新量=本层Jacobian矩阵*上一层梯度。</li>
<li>神经网络中的反向传播： <img data-src="https://s1.ax1x.com/2018/10/20/i0IIzT.png" alt="i0IIzT.png" />
初始化梯度表 最后一层输出对输出求梯度，因此初始值为1
从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。
本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。</li>
</ul>
<h1 id="rnn循环神经网络">RNN循环神经网络</h1>
<h2 id="rnn">RNN</h2>
<ul>
<li><p>特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。</p></li>
<li><p>用双曲正切作为隐藏层激活函数</p></li>
<li><p>输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y</p></li>
<li><p>基本结构（展开和非展开）： <img data-src="https://s1.ax1x.com/2018/10/20/i0I7yF.png"
alt="i0I7yF.png" /></p></li>
<li><p>几种变式：</p>
<ul>
<li>每一个时间步均有输出，隐藏层之间有循环连接： <img data-src="https://s1.ax1x.com/2018/10/20/i0ITQU.png" alt="i0ITQU.png" /></li>
<li>每一个时间步均有输出，输出与隐藏层之间有循环连接： <img data-src="https://s1.ax1x.com/2018/10/20/i0IqeJ.png" alt="i0IqeJ.png" /></li>
<li>读取整个序列后产生单个输出： <img data-src="https://s1.ax1x.com/2018/10/20/i0oCOe.png" alt="i0oCOe.png" /></li>
</ul></li>
<li><p>普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。
前馈过程：</p>
<p><span class="math display">\[
\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\
h^{(t)} = tanh(a^{(t)}), \\
o^{(t)} = c + Vh^{(t)}, \\
y^{(t)} = softmax(o^{(t)}) \\
\]</span></p>
<p>代价函数：</p>
<p><span class="math display">\[
L(\{ x^{(1)} , ... , x^{(\tau)}\},\{ y^{(1)} , ... , y^{(\tau)}\}) \\
=\sum _t L^{(t)} \\
= - \sum _t log p_{model} (y^{(t)}|\{ x^{(1)} , ... , x^{(\tau)}\}) \\
\]</span></p></li>
<li><p>改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）
导师驱动模型： <img data-src="https://s1.ax1x.com/2018/10/20/i0ILw9.png"
alt="i0ILw9.png" /></p></li>
</ul>
<h2 id="双向rnn">双向RNN</h2>
<ul>
<li>考虑对未来信息的依赖，相当于两类隐藏层结合在一起</li>
</ul>
<h2 id="序列到序列">序列到序列</h2>
<ul>
<li>采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。
<img data-src="https://s1.ax1x.com/2018/10/20/i0IOoR.png"
alt="i0IOoR.png" /></li>
</ul>
<h2 id="深度rnn">深度RNN</h2>
<ul>
<li>A.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）</li>
<li>B.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深</li>
<li>C.引入跳跃连接来缓解加深网络后导致的路径延长效应 <img data-src="https://s1.ax1x.com/2018/10/20/i0IjF1.png" alt="i0IjF1.png" /></li>
</ul>
<h2 id="rnn中的长期依赖问题">RNN中的长期依赖问题</h2>
<ul>
<li>长期依赖问题：模型变深，失去了学习到先前信息的能力</li>
<li>对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。</li>
<li>最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）</li>
</ul>
<h2 id="门控rnn">门控RNN</h2>
<p>-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。</p>
<ul>
<li>渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t)
累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t)
线性自连接的例子。当 α 接近 1
时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近
0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。</li>
</ul>
<h1 id="lstm">LSTM</h1>
<ul>
<li>LSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）</li>
<li>LSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图： <img data-src="https://s1.ax1x.com/2018/10/20/i0IvJx.png" alt="i0IvJx.png" /></li>
<li>可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h</li>
<li>所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。</li>
<li>三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。</li>
<li>内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。</li>
<li>细胞输出h，内部状态过激活函数，由输出门控制。</li>
<li>另一张更好理解的图： <img data-src="https://s1.ax1x.com/2018/10/20/i0IxW6.png" alt="i0IxW6.png" /></li>
</ul>
<h1 id="双向lstm">双向LSTM</h1>
<ul>
<li>同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。</li>
<li>因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：
<ul>
<li>直接连接(concat)<br />
</li>
<li>求和 <img data-src="https://s1.ax1x.com/2018/10/20/i0oSSK.png"
alt="i0oSSK.png" /></li>
</ul></li>
</ul>
<h1 id="词嵌入word2vec">词嵌入、Word2Vec</h1>
<ul>
<li>使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：</li>
<li>分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。</li>
<li>重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。</li>
<li>没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：<a
href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650720050&amp;idx=2&amp;sn=9fedc937d3128462c478ef7911e77687&amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;scene=21#wechat_redirect">技术
| 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</a></li>
</ul>
<h1 id="注意力机制">注意力机制</h1>
<ul>
<li>在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。</li>
<li>注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>abstractive summarization</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Reading 2</title>
    <url>/2018/07/03/PaperReading2/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li>Distractor Mechanism</li>
<li>External Information Attention</li>
<li>Pointer Copy Network PGNet</li>
<li>Extractive Summary Based on RNN</li>
<li>Transformer</li>
<li>Selection gate mechanism</li>
</ul>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0o47d.jpg" alt="i0o47d.jpg" />
<figcaption aria-hidden="true">i0o47d.jpg</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="distraction-based-neural-networks-for-document-summarization">Distraction-Based
Neural Networks for Document Summarization</h1>
<ul>
<li><p>Not only using attention mechanisms but also attention dispersion
mechanisms to better capture the overall meaning of the document.
Experiments have shown that this mechanism is particularly effective
when the input is long text. <img data-src="https://s1.ax1x.com/2018/10/20/i0oh0H.png"
alt="i0oh0H.png" /></p></li>
<li><p>Introducing a control layer between the encoder and decoder to
achieve attention concentration and attention dispersion, using two
layers of GRU:</p>
<p><span class="math display">\[
s_t = GRU _1 (s_t^{temp},c_t) \\
s_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\
\]</span></p></li>
<li><p>This control layer captures the connection between <span
class="math inline">\(s_t^{&#39;}\)</span> and <span
class="math inline">\(c_t\)</span> , where the former encodes the
current and previous output information, and the latter encodes the
current input that has been processed through attention focusing and
attention dispersion, while <span
class="math inline">\(e(y_{t-1})\)</span> is the embedding of the
previous input.</p></li>
<li><p>Three Attention Diversion Models</p>
<ul>
<li><p>M1: Calculate c_t for the control layer, distribute it over the
inputs, where c_t is the context c_t^{temp} encoded by a standard
attention mechanism, obtained by subtracting the historical context,
similar to a coverage mechanism</p>
<p><span class="math display">\[
c_t = tanh (W_c c_t^{temp} - U_c \sum _{j=1}^{t-1} c_j) \\
c_t^{temp} = \sum _{i=1}^{T_x} \alpha _{t,i} h_i \\
\]</span></p></li>
<li><p>M2: Distribute the attention weights, similarly, subtract the
historical attention and then normalize</p>
<p><span class="math display">\[
\alpha _{t,i}^{temp} = v_{\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a
\sum _{j=1}^{t-1}\alpha _{j,i}) \\
\alpha _{t,i} = \frac {exp(\alpha _{t,i}^{temp})}{\sum _{j=1}^{T_x}
exp(\alpha _{t,j}^{temp})} \\
\]</span></p></li>
<li><p>M3: Perform dispersion at the decoding end, calculate the
distances between the current <span class="math inline">\(c_t\)</span> ,
<span class="math inline">\(s_t\)</span> , <span
class="math inline">\(\alpha _t\)</span> , and the historical <span
class="math inline">\(c_t\)</span> , <span
class="math inline">\(s_t\)</span> , <span class="math inline">\(\alpha
_t\)</span> , and output the probabilities together as the scores relied
on during the 束 search during decoding.</p>
<p><span class="math display">\[
d_{\alpha , t} = \min KL(\alpha _t , \alpha _i) \\
d_{c , t} = \max cosine(c _t , c _i) \\
d_{s , t} = \max cosine(s _t , s _i) \\
\]</span></p></li>
</ul></li>
</ul>
<h1
id="document-modeling-with-external-attention-for-sentence-extraction">Document
Modeling with External Attention for Sentence Extraction</h1>
<ul>
<li>A retrieval-based summarization model was constructed, consisting of
a hierarchical document encoder and an extractor based on external
information attention. In the summarization task, the external
information is image captions and document titles.</li>
<li>By implicitly estimating the local and global relevance of each
sentence to the document and explicitly considering external
information, it determines whether each sentence should be included in
the abstract.</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oLjS.png" alt="i0oLjS.png" />
<figcaption aria-hidden="true">i0oLjS.png</figcaption>
</figure>
<ul>
<li>Sentence-level Encoder: As shown in the figure, using CNN encoding,
each sentence is encoded with three convolutional kernels of sizes 2 and
4 respectively, and the resulting vectors are subjected to maxpooling to
generate a single value, thus the final vector is 6-dimensional.</li>
<li>Document-level encoder: Input the 6-dimensional vector of a
document's sentence sequentially into LSTM for encoding.</li>
<li>Sentence Extractor: Composed of an LSTM with attention mechanism,
unlike the general generative seq2seq, the encoding of the sentence is
not only used as the encoding input in the seq2seq but also as the
decoding input, with one being in reverse order and the other in normal
order. The extractor relies on the encoding side input <span
class="math inline">\(s_t\)</span> , the previous time step state on the
decoding side <span class="math inline">\(h_t\)</span> , and the
attention-weighted external information <span
class="math inline">\(h_t^{&#39;}\)</span> .</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oIAA.png" alt="i0oIAA.png" />
<figcaption aria-hidden="true">i0oIAA.png</figcaption>
</figure>
<h1
id="get-to-the-point-summarization-with-pointer-generator-networks">Get
To The Point: Summarization with Pointer-Generator Networks</h1>
<ul>
<li><p>Presented two mechanisms, Pointer-Generator addresses the OOV
problem, and coverage resolves the issue of repeated words</p></li>
<li><p>Pointer-Generator: Learning pointer probabilities through
context, the decoder state of the current timestep, and input</p>
<p><span class="math display">\[
p_{gen} = \sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\
P(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \sum _{i:w_i = w} a_i^t \\
\]</span></p></li>
<li><p>Pointer probability indicates whether a word should be normally
generated or sampled from the input according to the current attention
distribution, in the above formula. If the current label is OOV, the
left part is 0, maximizing the right part to allow the attention
distribution to indicate the position of the copied word; if the label
is a newly generated word (not mentioned in the original text), the
right part is 0, and maximizing the left part means generating words
normally using the decoder. Overall, it learns the correct pointer
probability.</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ootI.png" alt="i0ootI.png" />
<figcaption aria-hidden="true">i0ootI.png</figcaption>
</figure>
<ul>
<li><p>Coverage: Utilizing the coverage mechanism to adjust attention,
so that words that received more attention in previous timesteps receive
less attention</p></li>
<li><p>Common Attention Calculation</p>
<p><span class="math display">\[
e_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\
a^t = softmax(e^t) \\
\]</span></p></li>
<li><p>Maintain a coverage vector indicating how much attention each
word has received prior to this:</p>
<p><span class="math display">\[
c^t = \sum _{t^{temp} = 0}^t-1 a^{t^{temp}}
\]</span></p></li>
<li><p>Then use its corrected attention generation to make the attention
generation consider the previous accumulated attention</p>
<p><span class="math display">\[
e_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})
\]</span></p></li>
<li><p>And add a coverage loss to the loss function</p>
<p><span class="math display">\[
covloss_t = \sum _i \min (a_i^t , c_i^t)
\]</span></p></li>
<li><p>The use of min means that we only penalize the overlapping parts
of the attention and coverage distributions, i.e., if coverage is large
and attention is also large, then covloss is large; if coverage is
small, regardless of the attention, covloss is small</p></li>
</ul>
<h1
id="summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents">SummaRuNNer:
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oTht.png" alt="i0oTht.png" />
<figcaption aria-hidden="true">i0oTht.png</figcaption>
</figure>
<ul>
<li><p>Using RNN for extractive summarization, the model decision
process can be visualized, and an end-to-end training method is
employed</p></li>
<li><p>Treat extraction as a sentence classification task, visit each
sentence in the order of the original text, and decide whether to
include it in the abstract, with this decision considering the results
of previous decisions.</p></li>
<li><p>Encoding at the word level using a bidirectional GRU, followed by
encoding at the sentence level using another bidirectional GRU; the
encodings from both layers are concatenated in reverse order and then
averaged through pooling</p>
<p><span class="math display">\[
d = tanh(W_d \frac {1}{N_d} \sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)
\]</span></p></li>
<li><p>d is the encoding of the entire document, <span
class="math inline">\(h_j^f\)</span> and <span
class="math inline">\(h_j^b\)</span> represent the forward and reverse
encodings of the sentence through GRU</p></li>
<li><p>Afterward, a neural network is trained for binary classification
based on the coding of the entire document, the coding of the sentences,
and the dynamic representation of the abstract at the current sentence
position, to determine whether each sentence should be included in the
abstract:</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ob1f.png" alt="i0ob1f.png" />
<figcaption aria-hidden="true">i0ob1f.png</figcaption>
</figure>
<ul>
<li><p>sj represents the abstraction generated up to position j,
obtained by weighted summation of the encoding of previous sentences
using the binary classification probability of each sentence:</p>
<p><span class="math display">\[
s_j = \sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)
\]</span></p></li>
<li><p>First line: The parameter is the encoding of the current
sentence, representing the content of the current sentence</p></li>
<li><p>Second line: Parameters are document encoding and sentence
encoding, indicating the significance of the current sentence to the
document</p></li>
<li><p>Third line: The parameters are the sentence encoding and the
dynamic encoding of the summary, indicating the redundancy of the
current sentence to the generated summary.</p></li>
<li><p>Fourth and fifth lines: Considered the relative and absolute
positions of sentences within the document. (The absolute position
denotes the actual sentence number, whereas the relative position refers
to a quantized representation that divides each document into a fixed
number of segments and computes the segment ID of a given
sentence.)</p></li>
<li><p>Finally, perform the maximum likelihood estimation on the entire
model:</p>
<p><span class="math display">\[
l(W,b) = -\sum _{d=1}^N \sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 |
h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))
\]</span></p></li>
<li><p>The author applies this extraction method to generative
summarization corpora, that is, how to label each sentence in the
original text with a binary classification. The author believes that the
subset of sentences labeled as 1 should correspond to the maximum ROUGE
value of the generative summary, but finding all subsets is too
time-consuming, so a greedy method is used: sentences are added one by
one to the subset, and if no remaining sentence can increase the ROUGE
value of the current subset, it is not added. In this way, the
generative summarization corpora are converted into extraction
summarization corpora.</p></li>
<li><p>Another approach is to train directly on the generative abstract
corpus, taking the dynamic abstract representation mentioned above,
specifically the last sentence which contains the entire document's
abstract representation s, and inputting it into a decoder to generate
the generative abstract. Since the abstract representation is the only
input to the decoder, training the decoder also allows learning good
abstract representations, thereby completing the task of extractive
summarization.</p></li>
<li><p>Because several components are included in generating the binary
classification probabilities, normalizing them allows for the
visualization of the contributions made by each component, thereby
illustrating the decision-making process:</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oqc8.png" alt="i0oqc8.png" />
<figcaption aria-hidden="true">i0oqc8.png</figcaption>
</figure>
<h1 id="attention-is-all-you-need">Attention Is All You Need</h1>
<ul>
<li>Abandoned RNN and CNN for seq2seq tasks, directly using multi-head
attention to compose network blocks and stack them, adding BN layers and
residual connections to construct a deep network</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oXng.png" alt="i0oXng.png" />
<figcaption aria-hidden="true">i0oXng.png</figcaption>
</figure>
<ul>
<li>The benefit of using attention exclusively is speed.</li>
<li>In order to utilize residuals, all submodules (multi-head attention
and fully connected) are unified to output dimensions of 512</li>
<li>Encoding end: 6 blocks, each containing an attention and a fully
connected sub-module, both using residuals and batch normalization.</li>
<li>Decoder side: Also consists of 6 blocks, the difference being the
addition of an attention mechanism to process the output from the
encoding side, and the attention mechanism connected to the decoder
input uses a mask to ensure directionality, that is, the output at the
i-th position is only related to the output at previous positions.</li>
<li>The six blocks of encoding and decoding are all stacked
(stacked)</li>
<li>The general attention model refers to a mechanism that maps a query
and a series of key-value pairs to an output, where the output is a
weighted sum of the values, and the weight of each value is calculated
by a compatibility function corresponding to the key and the query
input. The traditional attention keys and values are the same, both
being the hidden layer states at each input position, with the query
being the current output, and the compatibility function being various
attention calculation methods. The three arrows pointing to attention in
the diagram represent key, value, and query respectively.</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TSNn.png" alt="i0TSNn.png" />
<figcaption aria-hidden="true">i0TSNn.png</figcaption>
</figure>
<ul>
<li><p>Multi-head attention is composed of multiple parallel scaled
dot-product attention mechanisms.</p></li>
<li><p>Scaled dot-product attention, as shown, first performs a dot
product between the query and key, then scales, and if it is the
attention from the decoder input, a mask is added. After that, it passes
through the softmax function to perform a dot product with the value to
obtain the attention weights. In actual computation, to accelerate, a
series of queries, keys, and values are calculated together, so Q, K,
and V are all matrices. The scaling is to prevent the dot product
attention from being at the ends of softmax when the dimension of k is
too large, resulting in small gradients.</p>
<p><span class="math display">\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}}) V
\]</span></p></li>
<li><p>Multi-head attention is a scaled dot-product attention with h
projections on V, K, and Q, learning different features, and finally
concatenating and performing a linear transformation. The authors
believe that this multi-head design allows the model to learn the
information of representation subspaces at different positions.</p>
<p><span class="math display">\[
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\
where \ \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\
\]</span></p></li>
<li><p>In the paper, 8 heads are taken, and to ensure dimension
consistency, the dimensions of individual q, k, and v are set to
512/8=64</p></li>
<li><p>This multi-head attention is used in three places in the
model:</p>
<ul>
<li>-based attention mechanism.</li>
<li>Self-attention between encoding blocks and blocks</li>
<li>Decoding blocks and inter-block self-attention</li>
</ul></li>
<li><p>In each block, there is also a fully connected layer, which
contains two linear transformations, with ReLU activation inserted in
between, and the same parameters are used at each input position, but
the parameters of the fully connected layers in different blocks are
different</p>
<p><span class="math display">\[
FFN(x) =\max (0,xW_1+b_1)W_2 +b_2 \\
\]</span></p></li>
<li><p>The complete use of attention would discard the sequential order
information of the sequence; to utilize this information, trigonometric
positional encoding is added to make use of relative positional
information:</p>
<p><span class="math display">\[
PE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\
\]</span></p></li>
</ul>
<h1
id="a-joint-selective-mechanism-for-abstractive-sentence-summarization">A
Joint Selective Mechanism for Abstractive Sentence Summarization</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/11/15/ivQCE8.png" alt="ivQCE8.png" />
<figcaption aria-hidden="true">ivQCE8.png</figcaption>
</figure>
<ul>
<li><p>Abstracts differ from translations; end-to-end frameworks should
model the loss (information compression) rather than simply align as
translations do</p></li>
<li><p>The author made two improvements to loss modeling:</p>
<ul>
<li>After encoding is completed, a threshold is added for trimming the
encoded information</li>
<li>Added a selection loss, focusing on both input and output, to assist
the threshold operation</li>
</ul></li>
<li><p>The selection of the threshold considers both the hidden layer
states after encoding and the original word embeddings, and acts upon
the hidden layer states, truncating the hidden vectors before passing
them through attention-weighted generation of context. The authors
believe that this process is equivalent to allowing the network to
observe the word embeddings before and after the rnn processing, thereby
knowing which word in the input is important for generating the
abstract:</p>
<p><span class="math display">\[
g_i = \sigma (W_g h_i + U_g u_i) \\
h_i^{&#39;} = h_i \cdot g_i \\
\]</span></p></li>
<li><p>The selection of the loss function constructs a review threshold
at the decoding end, considering the hidden layers of the encoding end
and the original input, the hidden layers of the decoding end and the
original input, and the review threshold at each position of the
decoding end is the average of the review thresholds at all positions of
the encoding end:</p>
<p><span class="math display">\[
r_{i,t} = \sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\
r_i = \frac 1m \sum _{t=2}^{m+1} r_{i,t} \\
\]</span></p></li>
<li><p>The author believes that the role of the review threshold is
equivalent to allowing the network reading to generate abstracts and to
review the input text, so that it knows how to select
abstracts.</p></li>
<li><p>Afterward, use the Euclidean distance with the selection
threshold and review threshold as the selection loss, and add it to the
total loss:</p>
<p><span class="math display">\[
d(g,r) = \frac 1n \sum _{i=1}^n |r_i - g_i | \\
L = -p(y|x,\theta) + \lambda d(g,r) \\
\]</span></p></li>
<li><p>The author does not explain why the Euclidean distance between
the review threshold and the selection threshold is taken as the loss
function, nor does it clarify the distinction between the selection
threshold and attention. It seems like a type of attention mechanism
that considers the original input embedding, and it first trims each
hidden layer at every time step before traditional attention weighting.
The selected visual examples are also very clever, precisely
demonstrating that this selection mechanism can identify shifts in
sentences, thus changing the selected words, which contrasts with the
original paper proposing the selection threshold, Selective Encoding for
Abstractive Sentence Summarization. The original paper also does not
explain the motivation for this design.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="distraction-based-neural-networks-for-document-summarization">Distraction-Based
Neural Networks for Document Summarization</h1>
<ul>
<li><p>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oh0H.png"
alt="i0oh0H.png" /></p></li>
<li><p>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：</p>
<p><span class="math display">\[
s_t = GRU _1 (s_t^{temp},c_t) \\
s_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\
\]</span></p></li>
<li><p>这个控制层捕捉<span
class="math inline">\(s_t^{&#39;}\)</span>和<span
class="math inline">\(c_t\)</span>之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而<span
class="math inline">\(e(y_{t-1})\)</span>是上一次输入的embedding。</p></li>
<li><p>三种注意力分散模型</p>
<ul>
<li><p>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制</p>
<p><span class="math display">\[
c_t = tanh (W_c c_t^{temp} - U_c \sum _{j=1}^{t-1} c_j) \\
c_t^{temp} = \sum _{i=1}^{T_x} \alpha _{t,i} h_i \\
\]</span></p></li>
<li><p>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化</p>
<p><span class="math display">\[
\alpha _{t,i}^{temp} = v_{\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a
\sum _{j=1}^{t-1}\alpha _{j,i}) \\
\alpha _{t,i} = \frac {exp(\alpha _{t,i}^{temp})}{\sum _{j=1}^{T_x}
exp(\alpha _{t,j}^{temp})} \\
\]</span></p></li>
<li><p>M3：在解码端做分散，计算当前的<span
class="math inline">\(c_t\)</span>，<span
class="math inline">\(s_t\)</span>，<span class="math inline">\(\alpha
_t\)</span>和历史的<span class="math inline">\(c_t\)</span>，<span
class="math inline">\(s_t\)</span>，<span class="math inline">\(\alpha
_t\)</span>之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。</p>
<p><span class="math display">\[
d_{\alpha , t} = \min KL(\alpha _t , \alpha _i) \\
d_{c , t} = \max cosine(c _t , c _i) \\
d_{s , t} = \max cosine(s _t , s _i) \\
\]</span></p></li>
</ul></li>
</ul>
<h1
id="document-modeling-with-external-attention-for-sentence-extraction">Document
Modeling with External Attention for Sentence Extraction</h1>
<ul>
<li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。
在文摘任务中，外部信息是图片配字和文档标题。</li>
<li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oLjS.png" alt="i0oLjS.png" />
<figcaption aria-hidden="true">i0oLjS.png</figcaption>
</figure>
<ul>
<li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li>
<li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li>
<li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入<span
class="math inline">\(s_t\)</span>，解码端的上一时间步状态<span
class="math inline">\(h_t\)</span>，以及进行了注意力加权的外部信息<span
class="math inline">\(h_t^{&#39;}\)</span>：</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oIAA.png" alt="i0oIAA.png" />
<figcaption aria-hidden="true">i0oIAA.png</figcaption>
</figure>
<h1
id="get-to-the-point-summarization-with-pointer-generator-networks">Get
To The Point: Summarization with Pointer-Generator Networks</h1>
<ul>
<li><p>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</p></li>
<li><p>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率</p>
<p><span class="math display">\[
p_{gen} = \sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\
P(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \sum _{i:w_i = w} a_i^t \\
\]</span></p></li>
<li><p>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ootI.png" alt="i0ootI.png" />
<figcaption aria-hidden="true">i0ootI.png</figcaption>
</figure>
<ul>
<li><p>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</p></li>
<li><p>普通注意力计算</p>
<p><span class="math display">\[
e_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\
a^t = softmax(e^t) \\
\]</span></p></li>
<li><p>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</p>
<p><span class="math display">\[
c^t = \sum _{t^{temp} = 0}^t-1 a^{t^{temp}}
\]</span></p></li>
<li><p>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</p>
<p><span class="math display">\[
e_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})
\]</span></p></li>
<li><p>并在损失函数里加上coverage损失</p>
<p><span class="math display">\[
covloss_t = \sum _i \min (a_i^t , c_i^t)
\]</span></p></li>
<li><p>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</p></li>
</ul>
<h1
id="summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents">SummaRuNNer
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oTht.png" alt="i0oTht.png" />
<figcaption aria-hidden="true">i0oTht.png</figcaption>
</figure>
<ul>
<li><p>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</p></li>
<li><p>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</p></li>
<li><p>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</p>
<p><span class="math display">\[
d = tanh(W_d \frac {1}{N_d} \sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)
\]</span></p></li>
<li><p>其中d是整篇文档的编码，<span
class="math inline">\(h_j^f\)</span>和<span
class="math inline">\(h_j^b\)</span>代表句子经过GRU的正反向编码</p></li>
<li><p>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ob1f.png" alt="i0ob1f.png" />
<figcaption aria-hidden="true">i0ob1f.png</figcaption>
</figure>
<ul>
<li><p>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</p>
<p><span class="math display">\[
s_j = \sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)
\]</span></p></li>
<li><p>第一行：参数为当前句子编码，表示当前句子的内容</p></li>
<li><p>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</p></li>
<li><p>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We
squash the summary representation using the tanh operation so that the
magnitude of summary remains the same for all time-steps.）</p></li>
<li><p>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The
absolute position denotes the actual sentence number, whereas the
relative position refers to a quantized representation that divides each
document into a fixed number of segments and computes the segment ID of
a given sentence.）</p></li>
<li><p>最后对整个模型做最大似然估计:</p>
<p><span class="math display">\[
l(W,b) = -\sum _{d=1}^N \sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 |
h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))
\]</span></p></li>
<li><p>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</p></li>
<li><p>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</p></li>
<li><p>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oqc8.png" alt="i0oqc8.png" />
<figcaption aria-hidden="true">i0oqc8.png</figcaption>
</figure>
<h1 id="attention-is-all-you-need">Attention Is All You Need</h1>
<ul>
<li>抛弃了RNN和CNN做seq2seq任务，直接用multi head
attention组成网络块叠加，加入BN层和残差连接构造深层网络</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oXng.png" alt="i0oXng.png" />
<figcaption aria-hidden="true">i0oXng.png</figcaption>
</figure>
<ul>
<li>完全使用attention的一个好处就是快。</li>
<li>为了使用残差，所有的子模块（multi-head
attention和全连接）都统一输出维度为512</li>
<li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li>
<li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li>
<li>编码与解码的6个块都是堆叠的(stack)，</li>
<li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TSNn.png" alt="i0TSNn.png" />
<figcaption aria-hidden="true">i0TSNn.png</figcaption>
</figure>
<ul>
<li><p>Multi-head attention由多个scaled dot-product
attention并行组成。</p></li>
<li><p>Scaled dot-product
attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</p>
<p><span class="math display">\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}}) V
\]</span></p></li>
<li><p>Multi-head attention就是有h个scaled dot-product
attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</p>
<p><span class="math display">\[
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\
where \ \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\
\]</span></p></li>
<li><p>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</p></li>
<li><p>这种multi-head attention用在了模型的三个地方：</p>
<ul>
<li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked
multi-head attention的输出。也就是传统的attention位置</li>
<li>编码端块与块之间的自注意力</li>
<li>解码端块与块之间的自注意力</li>
</ul></li>
<li><p>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</p>
<p><span class="math display">\[
FFN(x) =\max (0,xW_1+b_1)W_2 +b_2 \\
\]</span></p></li>
<li><p>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</p>
<p><span class="math display">\[
PE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\
\]</span></p></li>
</ul>
<h1
id="a-joint-selective-mechanism-for-abstractive-sentence-summarization">A
Joint Selective Mechanism for Abstractive Sentence Summarization</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/11/15/ivQCE8.png" alt="ivQCE8.png" />
<figcaption aria-hidden="true">ivQCE8.png</figcaption>
</figure>
<ul>
<li><p>文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模</p></li>
<li><p>作者针对损失建模，做了两点改进：</p>
<ul>
<li>在编码完成之后添加了一个门限用于裁剪编码信息</li>
<li>添加了一个选择损失，同时关注输入和输出，辅助门限工作</li>
</ul></li>
<li><p>选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：</p>
<p><span class="math display">\[
g_i = \sigma (W_g h_i + U_g u_i) \\
h_i^{&#39;} = h_i \cdot g_i \\
\]</span></p></li>
<li><p>而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：</p>
<p><span class="math display">\[
r_{i,t} = \sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\
r_i = \frac 1m \sum _{t=2}^{m+1} r_{i,t} \\
\]</span></p></li>
<li><p>作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。</p></li>
<li><p>之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:</p>
<p><span class="math display">\[
d(g,r) = \frac 1n \sum _{i=1}^n |r_i - g_i | \\
L = -p(y|x,\theta) + \lambda d(g,r) \\
\]</span></p></li>
<li><p>作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective
Encoding for Abstractive Sentence Summarization
也没有说出这种设计的动机。</p></li>
</ul>
</div>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>abstractive summarization</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Reading 3</title>
    <url>/2019/01/03/PaperReading3/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li><p>Convolutional Sequence to Sequence</p></li>
<li><p>Robust Unsupervised Cross-Lingual Word Embedding Mapping</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="convolutional-sequence-to-sequence-learning">Convolutional
Sequence to Sequence Learning</h1>
<ul>
<li>Extremely straightforward, both the encoder and decoder use
sequence-to-sequence learning with convolutional neural networks</li>
<li>Whether using a transformer or a CNN as an encoder, it is necessary
to capture the semantic information of the entire sentence. Given the
current situation where both are significantly ahead of RNNs, tree
structures are more suitable as prior structures for natural language
data.</li>
<li>transformer directly models the so-called self-attention in the
first layer, personally, I believe this self-attention is modeling the
parse relationships of sentences, modeling all parse pairs (a word to
all other words in the sentence), all dimensions (it may be syntactic
parse, entity relationship, coreference resolution, or dependency
parse), and using the mechanism of attention to filter out unnecessary
relationships, and then reorganizes them through a layer of fully
connected layers. By iterating this parse + reorganization block
multiple layers, abstracting features step by step, and adding
structures commonly used in deep network design such as batch
normalization and residual, the transformer is formed. Therefore, the
transformer constructs the global parse relationships all at once and
then gradually reorganizes, abstracts, and filters.</li>
<li>The structure of CNN conforms more to the conventional 套路 of
syntactic parsing, still modeling across various dimensions, but it does
not construct the global relationships all at once. Instead, it first
analyzes local relationships (n-gram of kernel size) at the bottom
level, and then summarizes and abstracts these local relationships
through stacked layers.</li>
<li>Facebook uses a CNN block with a standard one-dimensional
convolution in this paper, but employs a gated linear unit, i.e.,
multi-convolves to double the channel as the input for the gate
structure, utilizing the gate structure to filter information and
construct non-linear relationships, similar to the gating design of
LSTM, and also achieving a similar effect of self-attention, while
giving up the pooling design. On the decoder side, CNN is also used (I
feel it's not very necessary), and the decoder still follows a process
of generating from left to right in word order. To ensure this order
relationship, the input to the decoder is masked, but I still haven't
understood how the specific code implements it... Doing the masking and
generating step by step in this way does not fully utilize the
acceleration of CNN.</li>
<li>In this paper, attention is also introduced, which is the
traditional encoder-decoder attention; the difference lies in
<ul>
<li>Employed multi-layer attention; although the key remains the output
of the encoder's last layer, attention is individually introduced to
each layer of the decoder. However, the authors themselves also say that
the decoder does not require many layers, two are sufficient, so this
multi-layer attention may not be fully utilized. Moreover, multi-layers
represent more context needed to decode each word, it seems that CNN as
a decoder does not need much context, or has not fully utilized the
longer context.</li>
<li>The value of attention is not the same as the key; instead, it is
the output of the last layer of the encoder plus the embedding of the
encoder input. The authors believe that this approach can
comprehensively consider both specific and abstract representations, and
the actual effect is indeed better.</li>
</ul></li>
<li>The author mentioned bytenet as a reference, but for some reason,
did not adopt the dilation convolution design from bytenet.</li>
</ul>
<h1
id="a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</h1>
<ul>
<li>Completely unsupervised cross-lingual word embedding mapping</li>
<li>Cross-lingual word embeddings, which refer to the use of the same
word embedding matrix across multiple languages, allowing for
cross-lingual model transfer of large-scale pre-trained word embeddings
and/or language models</li>
<li>The general approach is to use word embedding matrices of two
languages, map them to the same cross-lingual word embedding space, and
establish word correspondence between the two languages</li>
<li>This type of research has been popular recently, and the most
well-known downstream application it has spawned should be Facebook's
unsupervised machine translation from 2018</li>
<li>Previous methods are divided into three categories:
<ul>
<li>Supervised, using bilingual dictionaries, constructing thousands of
supervised word pairs, treating learning mapping as a regression
problem: modeling with the minimum mean square objective function, which
subsequently gave rise to various methods: canonical correlation
analysis; orthogonal methods; maximum margin methods. These methods can
all be categorized as linear transformation mappings of word embedding
matrices of the two languages into the same space.</li>
<li>Semi-supervised, achieved through seed dictionary and bootstrap,
such methods depend on good seeds and are prone to falling into local
optima</li>
<li>Another category is unsupervised generative methods, but the
existing methods are too dependent on specific tasks, have poor
generalization ability, and it is difficult to achieve good results for
two different languages of the language system.</li>
</ul></li>
<li>The text provided does not contain any source text to translate.
Please provide the source text you wish to have translated into
English.</li>
<li><img data-src="https://s2.ax1x.com/2019/03/11/APCWNT.png" title="fig:"
alt="APCWNT.png" /></li>
</ul>
<h2 id="model">Model</h2>
<ul>
<li>Let <span class="math inline">\(X\)</span> and <span
class="math inline">\(Z\)</span> be the word embedding matrices of two
languages, the goal is to learn linear transformation matrices <span
class="math inline">\(W_x\)</span> and <span
class="math inline">\(W_z\)</span> such that the mapped matrices of the
two languages are in the same cross-lingual space, forming a new
cross-lingual word embedding matrix</li>
<li>The iterative update of the model depends on an alignment matrix
<span class="math inline">\(D\)</span> , <span
class="math inline">\(D_{ij}=1\)</span> , which is aligned when and only
when the <span class="math inline">\(i\)</span> word of Language A
corresponds to the <span class="math inline">\(j\)</span> word of
Language B. The alignment relationship reflected by this matrix is
unidirectional</li>
<li>Model is divided into four steps: preprocessing, fully unsupervised
initialization, a robust self-learning process, and further improvement
of results through symmetric weight reallocation</li>
</ul>
<h2 id="preprocessing">Preprocessing</h2>
<ul>
<li>Normalize the length of word embeddings</li>
<li>Perform mean removal for each dimension again</li>
<li>The first two preprocessing steps mentioned in the author's previous
paper "Learning principled bilingual mappings of word embeddings while
preserving monolingual invariance" aim to simplify the problem to
seeking cosine similarity and maximum covariance. This paper discusses
supervised methods and is to be read.</li>
<li>Perform another length normalization to ensure that each word
embedding has a unit length, making the inner product of two word
embeddings equivalent to the cosine distance</li>
</ul>
<h2 id="completely-unsupervised-initialization">Completely unsupervised
initialization</h2>
<ul>
<li><p>Initialization is difficult to perform because the word embedding
matrices of the two languages are not aligned in two dimensions (each
word, each dimension of the embedding)</p></li>
<li><p>The approach in this paper is to first construct two matrices
<span class="math inline">\(X^{&#39;}\)</span> and <span
class="math inline">\(Z^{&#39;}\)</span> , with the word embeddings in
each dimension of these matrices aligned</p></li>
<li><p><span class="math inline">\(X^{&#39;}\)</span> and <span
class="math inline">\(Z^{&#39;}\)</span> are obtained by calculating the
square root of the similarity matrix of the original word embedding
matrix, i.e., <span class="math inline">\(X^{&#39;} = \sqrt
sorted{XX^T}\)</span> and <span class="math inline">\(Z^{&#39;} = \sqrt
sorted{ZZ^T}\)</span> . The product of a matrix with its transpose
results in a similarity matrix under the same language (because
preprocessing was done previously). Based on previous observations, two
words representing the same meaning in two languages should have a
similar single-language similarity distribution. Therefore, we sort each
row of the similarity matrices of the two languages separately, from
large to small. If two words have the same meaning, the corresponding
rows in their sorted similarity matrices within their own language
should have a similar distribution.</p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/03/13/Ak0cYd.png" alt="Ak0cYd.png" />
<figcaption aria-hidden="true">Ak0cYd.png</figcaption>
</figure></li>
<li><p>This skips the direct alignment of each dimension of word
embeddings, converting it to alignment based on dictionary similarity.
Afterward, only word alignment is needed, i.e., sorting each row of the
similarity matrix individually and establishing the correspondence
between words with similar row distributions.</p></li>
<li><p>Established word alignment, i.e., established the initial <span
class="math inline">\(D\)</span> matrix</p></li>
</ul>
<h2 id="robust-self-learning-process">Robust self-learning process</h2>
<ul>
<li><p>Compute orthogonal mappings to maximize the similarity of the
current <span class="math inline">\(D\)</span> matrix</p>
<p><span class="math display">\[
argmax_{W_x,W_z} \sum _i \sum _j D_{ij}((X_{i^*}W_X) \cdot (Z_{j^*}W_Z))
\\
\]</span></p>
<p>The optimal solution can be directly calculated as: <span
class="math inline">\(W_X=U,W_Z=V\)</span> , where <span
class="math inline">\(U,V\)</span> comes from <span
class="math inline">\(USV^T\)</span> , which is the singular value
decomposition of <span class="math inline">\(X^TDZ\)</span></p></li>
<li><p>After mapping the word embeddings of the two languages into a
cross-lingual word embedding space (still two word embedding matrices,
but within the same cross-lingual space), for each word in Language A,
find its nearest word in Language B within the cross-lingual word
embedding space, establish a mapping relationship, and update the <span
class="math inline">\(D\)</span> matrix.</p>
<p><span class="math display">\[
D_{ij} = 1 \ \ \ if  \ \ j = argmax _k (X_{i^*}W_X) \cdot (Z_{j^*}W_Z)
\\
else \ \ D_{ij} = 0 \\
\]</span></p></li>
<li><p>Repetitive iteration, <span class="math inline">\(W_X,W_Z
\rightarrow D \rightarrow W_X,W_Z \rightarrow D \rightarrow W_X,W_Z
\rightarrow D \rightarrow W_X,W_Z\)</span></p></li>
<li><p>Using a completely unsupervised initialization <span
class="math inline">\(D\)</span> matrix results in better performance
than random initialization, but it still falls into local optima.
Therefore, the authors proposed several small tricks for the second step
of the iteration, i.e., updating the <span
class="math inline">\(D\)</span> matrix, to make the learning more
robust</p>
<ul>
<li>Random dictionary induction: In each iteration, set elements of the
D matrix model to 0 with a certain probability, forcing the model to
explore more possibilities</li>
<li>Based on word frequency dictionary truncation: Only update the top k
most frequent words during each dictionary induction, to avoid noise
from low-frequency words, with a truncation limit of 20,000</li>
<li>CSLS retrieval: Previous methods find the nearest j for each i by
mapping it to the cross-lingual word embedding space, updating <span
class="math inline">\(D_{ij}\)</span> to 1. This nearest neighbor method
is affected by the dimensionality disaster and does not perform well
(the specific phenomenon caused is called hubs, where words cluster
together, and hubs words are the nearest neighbors of many words with
little difference). CSLS, which stands for cross-domain similarity local
scaling, penalizes these hub words.</li>
<li>Bidirectional dictionary induction, not only for finding j from i,
but also for finding i from j</li>
<li>These tricks differ in the initialization of the constructed
matrices, do not perform random inductive, and the dictionary truncation
limit is set to 4000</li>
</ul></li>
</ul>
<h2
id="further-improving-results-through-reweighting-with-symmetric-weights">Further
improving results through reweighting with symmetric weights</h2>
<ul>
<li><p>After the iterative process is completed</p>
<p><span class="math display">\[
W_X = US^{\frac 12} \\
W_Z = UV^{\frac 12} \\
\]</span></p></li>
<li><p>Compared to previous papers, this method encourages the model to
explore a wider search space by performing whitening and de-whitening
before and after each iteration, and it is insensitive to
direction</p></li>
<li><p>The reasons for reallocating weights were mentioned in the
previous paper, to be read</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="convolutional-sequence-to-sequence-learning">Convolutional
Sequence to Sequence Learning</h1>
<ul>
<li>非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习</li>
<li>无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。</li>
<li>transformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch
normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。</li>
<li>CNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal
size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。</li>
<li>Facebook在本论文中采用的CNN
block采用了普通的一维卷积，但是使用了gated linear
unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的......这样做mask然后一步一步生成其实并没有充分利用CNN的加速。</li>
<li>在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于
<ul>
<li>采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。</li>
<li>注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。</li>
</ul></li>
<li>作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation
convolution设计。</li>
</ul>
<h1
id="a-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings">A
robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings</h1>
<ul>
<li>完全无监督的跨语言词嵌入映射</li>
<li>跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移</li>
<li>一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系</li>
<li>这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译</li>
<li>之前的方法分三种：
<ul>
<li>有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical
correlation
analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。</li>
<li>半监督的，通过seed
dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；</li>
<li>另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。</li>
</ul></li>
<li>本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed
dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。</li>
<li><img data-src="https://s2.ax1x.com/2019/03/11/APCWNT.png" title="fig:"
alt="APCWNT.png" /></li>
</ul>
<h2 id="模型">模型</h2>
<ul>
<li>令<span class="math inline">\(X\)</span>和<span
class="math inline">\(Z\)</span>分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵<span
class="math inline">\(W_x\)</span>和<span
class="math inline">\(W_z\)</span>，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵</li>
<li>模型的迭代更新依赖一个对齐矩阵<span
class="math inline">\(D\)</span>，<span
class="math inline">\(D_{ij}=1\)</span>当且仅当A语言的第<span
class="math inline">\(i\)</span>个词对应着B语言的第<span
class="math inline">\(j\)</span>个词，该矩阵反映的对齐关系是单向的</li>
<li>模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果</li>
</ul>
<h2 id="预处理">预处理</h2>
<ul>
<li>对词嵌入做长度归一化</li>
<li>再针对每一维做去均值</li>
<li>前两个预处理在作者之前的论文Learning principled bilingual mappings
of word embeddings while preserving monolingual
invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读</li>
<li>再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离</li>
</ul>
<h2 id="完全无监督的初始化">完全无监督的初始化</h2>
<ul>
<li><p>初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐</p></li>
<li><p>本文的做法是，先构造两个矩阵<span
class="math inline">\(X^{&#39;}\)</span>和<span
class="math inline">\(Z^{&#39;}\)</span>，这两个矩阵的词嵌入每一维是对齐的</p></li>
<li><p><span class="math inline">\(X^{&#39;}\)</span>和<span
class="math inline">\(Z^{&#39;}\)</span>分别通过计算原词嵌入矩阵的相似矩阵开方得到，即<span
class="math inline">\(X^{&#39;} = \sqrt sorted{XX^T}\)</span>，<span
class="math inline">\(Z^{&#39;} = \sqrt
sorted{ZZ^T}\)</span>。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布</p></li>
<li><figure>
<img data-src="https://s2.ax1x.com/2019/03/13/Ak0cYd.png" alt="Ak0cYd.png" />
<figcaption aria-hidden="true">Ak0cYd.png</figcaption>
</figure></li>
<li><p>这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。</p></li>
<li><p>建立了词语的对齐，即建立了初始化的<span
class="math inline">\(D\)</span>矩阵</p></li>
</ul>
<h2 id="鲁棒的自学习过程">鲁棒的自学习过程</h2>
<ul>
<li><p>计算正交映射以最大化当前<span
class="math inline">\(D\)</span>矩阵的相似度</p>
<p><span class="math display">\[
argmax_{W_x,W_z} \sum _i \sum _j D_{ij}((X_{i^*}W_X) \cdot (Z_{j^*}W_Z))
\\
\]</span></p>
<p>最优解可以直接计算得到：<span
class="math inline">\(W_X=U,W_Z=V\)</span>，其中<span
class="math inline">\(U,V\)</span>来自<span
class="math inline">\(USV^T\)</span>，是<span
class="math inline">\(X^TDZ\)</span>的奇异值分解</p></li>
<li><p>将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新<span
class="math inline">\(D\)</span>矩阵。</p>
<p><span class="math display">\[
D_{ij} = 1 \ \ \ if  \ \ j = argmax _k (X_{i^*}W_X) \cdot (Z_{j^*}W_Z)
\\
else \ \ D_{ij} = 0 \\
\]</span></p></li>
<li><p>反复迭代，<span class="math inline">\(W_X,W_Z \rightarrow D
\rightarrow W_X,W_Z \rightarrow D \rightarrow W_X,W_Z \rightarrow D
\rightarrow W_X,W_Z\)</span></p></li>
<li><p>使用完全无监督的初始化<span
class="math inline">\(D\)</span>矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新<span
class="math inline">\(D\)</span>矩阵时提了几个小trick使得学习更为鲁棒</p>
<ul>
<li>随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能</li>
<li>基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000</li>
<li>CSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新<span
class="math inline">\(D_{ij}\)</span>为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语</li>
<li>双向词典归纳，不仅针对i找j，也针对j找i</li>
<li>这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000</li>
</ul></li>
</ul>
<h2
id="通过对称权重重分配进一步改善结果">通过对称权重重分配进一步改善结果</h2>
<ul>
<li><p>即迭代完成之后计算</p>
<p><span class="math display">\[
W_X = US^{\frac 12} \\
W_Z = UV^{\frac 12} \\
\]</span></p></li>
<li><p>比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感</p></li>
<li><p>重新分配权重的原因在之前的论文中提到，待阅读</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>abstractive summarization</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Reading 1</title>
    <url>/2018/03/07/PaperReading/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li><p>Opening Work on Attention (Machine Translation)</p></li>
<li><p>Luong attention, global and local attention,</p></li>
<li><p>Opening Work on Attention (Automatic Text Summarization)</p></li>
<li><p>Generative Summary Techniques Collection: LVT, Switching
Networks, Hierarchical Attention</p></li>
<li><p>Dialogue System, End-to-End Hierarchical RNN</p></li>
<li><p>Weibo summary, supplement micropoints</p></li>
<li><p>disan, directed transformer, attention mask</p></li>
<li><p>Attention Extractor</p></li>
<li><p>Generative Summary Based on Reinforcement Learning</p></li>
<li><p>w2v, negative sampling</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural
Machine Translation By Jointly Learning To Align And Translate</h1>
<ul>
<li>Published in 2015.5 (ICLR2015), author Dzmitry Bahdanau.</li>
<li>Encoder-decoder model, translation task.</li>
<li>The bidirectional GRU serves as the encoder. The encoding hidden
layer vectors are composed of bidirectional connections.</li>
<li>Different representations are generated for each word.</li>
<li>The weight is determined by the vector of the hidden layer of all
steps and the vector of the hidden layer of the previous decoding
step.</li>
<li>Generate weighted representations of the hidden layer vectors for
all step encoding. <img data-src="https://s1.ax1x.com/2018/10/20/i0oB79.png"
alt="i0oB79.png" /></li>
</ul>
<h1
id="effective-approaches-to-attention-based-neural-machine-translation">Effective
Approaches to Attention-based Neural Machine Translation</h1>
<ul>
<li><p>Published in August 2015, the author (Minh-Thang Luong) used the
RNN encoder-decoder model for the translation task.</p></li>
<li><p>During the decoding process, the attentional representation and
the decoding hidden layer vector corresponding to the target word are
concatenated and then passed through an activation function to generate
the attention vector:</p>
<p><span class="math display">\[
h_t = tanh(W_c[c_t;h_t])
\]</span></p>
<p>Afterward, the attention vector is passed through softmax to generate
a probability distribution.</p></li>
</ul>
<h2 id="global-attention">Global Attention</h2>
<ul>
<li>The article first introduces the global attention model, which is an
attention-weighted generation of representations for all encoding hidden
layer information, leading to an unpredictable length of alignment
vectors (alignment vectors weigh the input information, with the length
being the same as the number of words in the input sentence). The model
proposed by Dzmitry Bahdanau in the aforementioned text is the global
attention model. The global attention model presented in this article is
more generalized: it does not use bidirectional RNN concatenation of
input vectors but employs a regular RNN instead; it calculates weights
directly using the hidden layer vector at the current step, rather than
the previous step, thus avoiding complex computations. <img data-src="https://s1.ax1x.com/2018/10/20/i0oH9P.png" alt="i0oH9P.png" /></li>
<li>Afterward, two effective approaches were introduced, namely local
attention and input-feeding.</li>
</ul>
<h2 id="local-attention">Local Attention</h2>
<ul>
<li><p>Local Attention: Instead of using all input information, it first
generates an alignment position for each output word, then only
generates representations with attention weighted to the input
information within the window around the alignment position. The article
proposes two methods for generating alignment positions:</p>
<ul>
<li><p>Monotonic alignment: Simply setting the alignment position of the
ith output word to i is obviously not advisable in abstracts.</p></li>
<li><p>Predictive Alignment: Training Alignment Positions.</p>
<p><span class="math display">\[
p_t = S \cdots sigmoid(v_p^T tanh(W_ph_t)) \\
\]</span></p>
<p><span class="math inline">\(h_t\)</span> is the hidden layer vector
of the t-th generated word, and <span class="math inline">\(W_p\)</span>
and <span class="math inline">\(v_p\)</span> are the weights that need
to be trained. S is the length of the input word, and when multiplied by
the sigmoid, it yields the value at any position in the input
sentence</p></li>
</ul></li>
<li><p>To maximize the weight of alignment positions, first generate a
Gaussian distribution with alignment positions as the expectation and
half-window length as the standard deviation, and then generate weights
based on this.</p>
<p><span class="math display">\[
a_t(s) = align(h_t,h_s)exp(-\frac{(s-p_t)^2}{2\sigma ^2})
\]</span></p>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ost1.png" alt="i0ost1.png" />
<figcaption aria-hidden="true">i0ost1.png</figcaption>
</figure></li>
</ul>
<h2 id="input-feeding">Input-feeding</h2>
<ul>
<li>Input-feeding: When generating alignment, it still needs to rely on
past alignments. The actual implementation involves using the attention
vector from the previous step as the feed for the next decoding hidden
layer. The benefit is that the model can fully understand the previous
alignment decisions and creates a very deep network both horizontally
and vertically.</li>
<li>Experimental results indicate that the use of the prediction
alignment-based local attention model performs the best. <img data-src="https://s1.ax1x.com/2018/10/20/i0oyfx.png" alt="i0oyfx.png" /></li>
</ul>
<h1
id="a-neural-attention-model-for-abstractive-sentence-summarization">A
Neural Attention Model for Abstractive Sentence Summarization</h1>
<ul>
<li>Published in September 2015, author Alexander M. Rush,
Decoder-Encoder Model, Abstract Task.</li>
<li>Proposed an attention encoder using a standard NNLM decoder.</li>
<li>Not using RNN, directly using word vectors.</li>
<li>Utilize all input information and partial output information (yc) to
construct attention weights.</li>
<li>Directly weighting the word vector matrix of the smoothed input
sentence rather than the RNN hidden layer vector.</li>
<li>Model as shown in the figure: <img data-src="https://s1.ax1x.com/2018/10/20/i0ocp6.png" alt="i0ocp6.png" /></li>
</ul>
<h1
id="abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond">Abstractive
Text Summarization using Sequence-to-sequence RNNs and Beyond</h1>
<ul>
<li>Published in August 2016, author Ramesh Nallapati. Encoder-decoder
model, using RNN, attention, summarization task.</li>
<li>Based on the machine translation model by Dzmitry Bahdanau
(bidirectional GRU encoding, unidirectional GRU decoding) for
improvement.</li>
<li>Improved techniques include LVT (large vocabulary trick),
feature-rich encoder, switching generator-pointer, and hierarchical
attention.</li>
</ul>
<h2 id="lvt">LVT</h2>
<ul>
<li>Reduce the size of the softmax layer in the decoder to accelerate
computation and convergence. The actual implementation is that the
decoder's dictionary is limited to the input text within each
mini-batch, and the most frequently occurring words from the decoder
dictionary of the previous batch are added to the decoder dictionary of
the subsequent batch (until a limit is reached).</li>
</ul>
<h2 id="feature-rich-encoder">Feature-rich Encoder</h2>
<ul>
<li>Not using simple word vectors that only represent semantic distance,
but constructing new word vectors by integrating various semantic
features such as entity information, and separately forming vectors to
concatenate them. <img data-src="https://s1.ax1x.com/2018/10/20/i0og1K.png"
alt="i0og1K.png" /></li>
</ul>
<h2 id="switching-generator-pointer">Switching Generator-pointer</h2>
<ul>
<li>Resolve the issues of rare words and additional words. The
dictionary of the decoder is fixed, and how to deal with words outside
the dictionary in the test text. The proposed solution is to add a
switch to the decoder, where when the switch is on, it uses its own
dictionary to generate the abstract normally, and when the switch is
off, it generates a pointer to a word in the input text, copying it into
the abstract. <img data-src="https://s1.ax1x.com/2018/10/20/i0oRXD.png"
alt="i0oRXD.png" /> ) Switching generator/pointer model When the switch
is G, use the traditional method to generate the abstract. When the
switch is P, copy words from the input to the abstract.</li>
</ul>
<h2 id="hierarchical-attention">Hierarchical Attention</h2>
<ul>
<li>Traditional attention refers to focusing on the positions of key
words in sentences, and the hierarchical structure includes the upper
level, that is, the positions of key sentences in the text. Two-layer
bidirectional RNNs are used to capture attention at both the word level
and the sentence level. The attention mechanism runs simultaneously at
both levels, with the attention weights at the word level being
reweighted and adjusted by the attention weights at the sentence level.
<img data-src="https://s1.ax1x.com/2018/10/20/i0ofne.png"
alt="i0ofne.png" /></li>
</ul>
<h1 id="recurrent-neural-network-regularization">Recurrent Neural
Network Regularization</h1>
<ul>
<li>This paper introduces how to use dropout in recurrent neural
networks to prevent overfitting</li>
<li>Dropout refers to randomly dropping certain nodes of some hidden
layers in deep neural networks during each training, while not dropping
nodes during testing but multiplying the node outputs by the dropout
probability. This method can effectively solve the time-consuming and
prone to overfitting problems in deep learning.</li>
<li>Two understandings of dropout exist: 1: It forces a neural unit to
work together with other randomly selected neural units to achieve good
results. It eliminates the weakened joint adaptability between neuron
nodes, enhancing generalization ability. 2: It is equivalent to creating
some noisy data, increasing the sparsity of the training data, and
enhancing the discriminability of features.</li>
<li>It is not possible to directly discard hidden layer nodes in RNNs
because doing so would lose the long-term dependency information
required by RNNs, introducing a significant amount of noise that
disrupts the learning process.</li>
<li>The author proposes hierarchical node dropping, that is, using a
multi-layer RNN structure, even if a node in one layer at a certain time
step is lost, the nodes at the same time step in other layers can still
pass through, without disrupting the long-term dependency information of
the sequence</li>
<li>Effect as shown: <img data-src="https://s1.ax1x.com/2018/10/20/i0ov7j.png" alt="i0ov7j.png" /></li>
</ul>
<h1
id="building-end-to-end-dialogue-systems-using-generative-hierarchical-neural-network-models">Building
End-To-End Dialogue Systems Using Generative Hierarchical Neural Network
Models</h1>
<ul>
<li>This paper introduces the construction of a non-goal-driven natural
language dialogue system using a fully data-driven hierarchical RNN
end-to-end model.</li>
<li>Training data consists of triples extracted from movie subtitles,
where two speakers complete three expressions in the order of
A-B-A.</li>
<li>The hierarchical structure is not simply an increase in the number
of RNN hidden layers, but rather the construction of two RNNs at the
word level and sentence level, as shown in the figure below. The
three-part dialogue is contained within two word-level RNN end-to-end
systems, and the sentences generated in the middle serve as hidden layer
vectors for the higher-level sentence-level RNN.</li>
<li>The article employs the bootstrapping technique, using some
pre-trained data as the initial values for the model. For example, word
embedding matrices are initially trained in large corpora using w2v. In
this article, the entire model is even pre-trained, with the principle
being to pre-train the entire model using a two-stage dialogue large
corpus of a QA system, with the third segment set to empty. In actual
training, word embeddings are pre-trained first to complete the
initialization of word embeddings, followed by pre-training the entire
model for 4 epochs, and finally, the word embeddings are fixed and the
entire model is pre-trained to the optimal value.</li>
<li>The system infers the third segment from the first two segments of a
given three-segment dialogue. Evaluation uses two criteria: word
perplexity measures grammatical accuracy, and word classification error
measures semantic accuracy.</li>
<li>The paper finally summarized the reasons for the occurrence of a bad
phenomenon. During the output of the maximum a posteriori probability,
some usual answers often appear, such as "I don't know" or "I'm sorry."
The author believes there are three reasons: 1: Insufficient data.
Because dialogue systems have inherent ambiguity and multimodality,
large-scale data is needed to train better results. 2: Punctuation and
pronouns occupy a large proportion in dialogue, but the system finds it
difficult to distinguish the meanings of punctuation and pronouns in
different contextual environments. 3: Dialogues are generally very
short, so a triple can provide too little information during inference,
resulting in insufficient differentiation. Therefore, when designing
natural language neural models, it is best to differentiate semantics
and grammatical structures. The author also found that if the maximum a
posteriori probability is not used and random output is employed, this
problem does not occur, and the inferred sentences generally maintain
the topic and appear with special words related to the topic.</li>
</ul>
<h1 id="news-event-summarization-complemented-by-micropoints">News Event
Summarization Complemented by Micropoints</h1>
<ul>
<li>This is a paper from Peking University that uses data from Weibo.
The work involves constructing some micropoints in Weibo posts with the
same theme to supplement the abstracts extracted from traditional news.
The experiment proves that this supplemented abstract can achieve better
scores in ROUGE.</li>
<li>The focus of the article's exposition is on extracting micropoints
rather than on how to integrate micropoints into the original
abstract.</li>
<li>This team previously proposed two tools: a text clustering model CDW
for extracting keywords from news articles; and a Snippet Growth Model
for segmenting a blog post into segments (a few sentences each) that
possess relatively complete meanings.</li>
<li>Micropoints generation main steps: filtering blog posts,
categorizing blog posts by topic, segmenting blog posts into fragments,
selecting some fragments from blog posts of the same topic to form
micropoints, and filtering micropoints.</li>
<li>Screening blog posts considers two indicators: relevance and
distinctiveness, which must be related to the original news abstract's
theme without being too repetitive to cause redundancy. The author uses
CDW to extract keywords from the original news from multiple
perspectives, calculates the cosine similarity between the blog posts
and these keywords to filter out multiple relevant blog posts.
Additionally, the author utilizes Joint Topic Modeling (Joint topic
modeling for event summarization across news and social media streams)
to calculate the distinctiveness between the blog posts and the
abstracts. The harmonic mean of the two calculated indicators is taken
as the overall screening indicator.</li>
<li>Categorize the blog posts by topic: obtain p(topic|tweet) by using
LDA with restricted use, then construct a vector v(t) = (p(topic 1 |t),
p(topic 2 |t), ..., p(topic n |t)) for each blog post using this
conditional probability, and finally use DBSCAN to complete the topic
clustering.</li>
<li>Using the Snippet Growth Model proposed by the team before, the blog
posts are divided into snippets, with the general method being to first
take a sentence, then calculate the text similarity, distance measure,
and influence measure between other sentences and this sentence to
decide whether to add other sentences to the snippet where this sentence
is located.</li>
<li>A pile of fragments categorized by topic has been obtained, and the
next step is to select several fragments that best represent the topic
within a single topic's fragments. The method is to pick the few
fragments with the smallest average distance to other fragments of the
same topic. Since the sentences are not very long, the author represents
a fragment with a bag-of-words, where the bag contains all the words
that make up all the sentences in the fragment, represented by word
vectors. The distance is measured using KL divergence. If the newly
selected fragments are too close to the already selected fragments, they
are discarded to ensure that the selected fragments still maintain
diversity.</li>
<li>The obtained fragments will form micropoints, but they need to be
filtered before being supplemented into the abstract. The authors
propose three indicators: information quantity, popularity, and
conciseness. Information quantity refers to the information entropy gain
of the abstract after supplementation; the higher the information
quantity, the better. Popularity is measured by the number of comments
on the original post, with more popular posts being less likely to be
extreme. Popularity is preferred to prevent the supplementation of
abstracts with extreme or morally incorrect posts. The higher the
popularity, the better. Conciseness is described by the ratio of the
length of the supplemented part to the length of the original abstract;
the smaller the ratio, the more concise the supplementation, and it will
not overshadow the original. At this point, the problem is reduced to a
discrete optimization problem with constraints under a given conciseness
requirement, where each fragment can bring benefits in terms of
information quantity and popularity while consuming a certain amount of
conciseness. The goal is to select fragments to maximize the benefits,
which can be abstracted as a 0-1 knapsack problem and solved using
dynamic programming. The authors also set thresholds and use a piecewise
function: when popularity exceeds the threshold, the contribution of
information gain to the benefit will be greater. This setting is to
ensure that the abstract will not be supplemented with fragments where
one side has a very high popularity or information gain while the other
side is almost non-existent.</li>
</ul>
<h1
id="disan-directional-self-attention-network-for-rnncnn-free-language-understanding">DiSAN:
Directional Self-Attention Network for RNN/CNN-Free Language
Understanding</h1>
<ul>
<li>Update, the author later released fast disan, it seems to have
modified the calculation of attention, details to be supplemented</li>
<li>The author proposes a directed self-attention network that can also
perform the encoding task in NLP problems without relying on RNN or CNN
structures.</li>
<li>The authors believe that among the existing encoders, RNN can
capture the sequence information well, but it is slow. The use of a pure
attention mechanism (just like the attention weighting of a sequence of
word vectors without using RNN in A Neural Attention Model for
Abstractive Sentence Summarization) can be used to speed up the
operation using existing distributed or parallel computing frameworks,
but the sequence information is lost. Therefore, the authors propose a
pure attention encoder structure that can capture sequential order
information, which combines the advantages of both.</li>
<li>The author first proposed three attention concepts:
multi-dimensional attention, self-attention, and directed
attention.</li>
<li>Traditional attention assigns weights to each word in a sentence,
with scalar values. In multi-dimensional attention, the weights are
vectors, with dimensions matching those of the word vectors. The
rationale for using multi-dimensional attention is that it applies
attention weighting to each feature of every word. Word vectors
inherently have polysemy, and the traditional attention mechanism that
weights the entire word vector cannot effectively distinguish between
the same word in different contextual environments. Multi-dimensional
attention applies weighting to each component of the word vector,
allowing for more attention weight to be given to features that can
represent the current contextual environment. My understanding is that
applying attention weighting to the components of the word vector is
equivalent to having slightly different representations of the same word
in different contextual environments, which can be used for distinction.
The figure below illustrates the difference between traditional
attention and multi-dimensional attention. <img data-src="https://s1.ax1x.com/2018/10/20/i0ojBQ.png" alt="i0ojBQ.png" /> On
the right is multi-dimensional attention, where the attention weights
have become vectors, matching the dimensionality of the input word
vectors.</li>
<li>The general attention weights are generated with encoding input and
a decoding output as parameters, and the weights are related to the
current output. Self-attention is unrelated to the decoding end, either
replacing the decoding output with each word in the sentence or with the
entire input sentence. The former, combined with multi-dimensions, forms
token2token attention, while the latter, combined with multi-dimensions,
forms source2token.</li>
<li>Directed attention involves adding a mask matrix when generating
token2token attention, with matrix elements being 0 or negative
infinity. The matrix can be upper triangular or lower triangular,
representing masks for two directions, for example, from i to j is 0,
and from j to i is negative infinity. This gives the attention between
words in token2token a direction; attention in the incorrect direction
is reduced to 0 after softmax, while attention in the correct direction
is unaffected. The mask matrix also has a third type, a non-diagonal
matrix, where the diagonal values are negative infinity. This way, a
word in token2token does not generate attention for itself. Directed
attention is as shown in the figure: <img data-src="https://s1.ax1x.com/2018/10/20/i0ozAs.png" alt="i0ozAs.png" /></li>
<li>The final architecture of the self-attentional network utilizes the
above three types of attention. Firstly, the combination of the upper
and lower triangular masks with multi-dimensional token2token generates
two self-attention vectors, similar to BLSTM, and then these vectors are
connected, passing through a multi-dimensional source2token to produce
the final encoded output. The authors tested that this encoding can
achieve the best level in natural language prediction and sentiment
analysis tasks and can also be used as part of other models for other
tasks.</li>
</ul>
<h1 id="neural-summarization-by-extracting-sentences-and-words">Neural
Summarization by Extracting Sentences and Words</h1>
<ul>
<li>This paper employs a fully data-driven model to accomplish
extractive summarization. The model structure consists of a hierarchical
text encoder and an attention-based extractor.</li>
<li>The difference from the generative attention mechanism summarization
lies in: using CNN instead of w2v to construct word embeddings;
attention is used to directly extract words rather than to weight and
generate intermediate representations.</li>
<li>Because this paper uses data-driven extractive summarization, it
requires a large amount of extractive summarization training data. Such
training data is scarce, so the authors propose a method for generating
extractive training data at the word and sentence levels: For sentence
extraction, the authors' approach is to convert generative summarization
into extractive summarization. First, obtain generative summarization,
then compare each sentence in the original text with the generative
summarization to decide whether it should be extracted. The comparison
criteria include the position of the sentence in the document, the
overlap of unigram and bigram grammar, the number of named entities
appearing, etc.; for word extraction, the same approach is used to
compare the degree of semantic overlap between the generative
summarization and the words in the original text to decide whether the
word should be extracted. For words that appear in the generative
summarization but not in the original text, the authors' solution is to
substitute with words that have a similar embedding distance to the
original text words to form the training data.</li>
<li>During encoding, use CNN to form word embeddings, represent
sentences as sequences of word embeddings, and then use RNN to encode at
the document level (with one sentence as an input at each time
step).</li>
<li>When performing sentence extraction, unlike generative models, the
dependency of the extracted RNN output is on the previous extracted
sentence multiplied by a confidence coefficient, which represents the
probability of the previous sentence being extracted.</li>
<li>As with generative models, there are differences between train and
infer, and the issues that arise during the initial infer phase will
accumulate and grow over time. To address this problem, the authors
employ a "curriculum learning strategy": initially setting the
confidence level to 1 when the training cannot accurately predict, and
then gradually restoring the confidence level to the value trained out
as the training progresses.</li>
<li>Compared to sentence extraction, word extraction is more closely
aligned with generative algorithms and can be regarded as a generative
summary at the word level under dictionary constraints.</li>
<li>Extractions-based abstracts have advantages in handling sparse
vocabulary and named entities, allowing the model to check the context
and relative position of these words or entities in the sentence to
reduce attention weights and minimize the impact of such words.</li>
<li>The problem to be addressed in the sampling method is to determine
the number of samples. The authors select the three sentences with the
highest sampling confidence as the abstract. Another issue is that the
dictionary for each batch is generally different. The authors adopt a
negative sampling solution.</li>
</ul>
<h1 id="a-deep-reinforced-model-for-abstractive-summarization">A DEEP
REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1>
<ul>
<li><p>Using reinforcement learning to optimize the current end-to-end
generative summarization model <img data-src="https://s1.ax1x.com/2018/10/20/i0TicT.png"
alt="i0TicT.png" /></p></li>
<li><p>Addressing the issues of long text summarization and repetitive
phrase generation</p></li>
<li><p>Enhanced learning requires external feedback for the model, here
the authors use 人工 evaluation of the generated abstracts and provide
feedback to the model, enabling it to produce more readable
abstracts</p></li>
<li><p>The improvement of the model mainly focuses on two points:
internal attention was added to both the encoding and decoding ends,
where the encoding end is a previously proposed method, and this paper
mainly introduces the internal attention mechanism at the decoding end;
a new objective function is proposed, which combines cross-entropy loss
with rewards from reinforcement learning</p></li>
<li><p>The inner attention at both ends of encoding and decoding
addresses the repetition phrase issue from two aspects, as the
repetition problem is more severe in long text summarization compared to
short text.</p></li>
<li><p>The addition of inner attention at the encoding end is based on
the belief that repetition arises from the uneven distribution of
attention over the input long text across different decoding time steps,
which does not fully utilize the long text. The distribution of
attention may be similar across different decoding time steps, leading
to the generation of repetitive segments. Therefore, the authors
penalize input positions that have already received high attention
weights in the model, ensuring that all parts of the input text are
fully utilized. The method of introducing the penalty is to divide the
attention weight of a certain encoding input position at a new decoding
time step by the sum of attention weights from all previous time steps,
so that if a large attention weight was produced in the past, the newly
generated attention weight will be smaller.</p></li>
<li><p>The addition of internal attention at the decoding end is based
on the belief that repetition also originates from the repetition of the
hidden states within the decoding end itself. The authors argue that the
information relied upon during decoding should not only include the
hidden layer state of the decoding end from the previous time step, but
also the hidden layer states from all past time steps, with attentional
weighting given. Therefore, a similar internal attention mechanism and
penalty mechanism are introduced at the decoding end.</p></li>
<li><p>In this end-to-end model, attention is not a means of
communication between the encoding and decoding ends, but is independent
at both ends, depending only on the state before and the current state
of the encoding/decoding ends, thus it is intrinsic attention
(self-attention).</p></li>
<li><p>In constructing the end-to-end model, the authors also adopted
some other techniques proposed by predecessors, such as using copy
pointers and switches to solve the sparse word problem, encoding and
decoding the shared word embedding matrix, and also particularly
proposed a small trick: based on observation, repeated three-word
phrases generally do not appear in abstracts, so in the 束 search at the
decoding end, if a repeated three-word phrase appears, it should be
pruned.</p></li>
<li><p>Afterward, the author analyzed two reasons why static supervised
learning often fails to achieve ideal results in abstract evaluation
criteria: one is exposure bias, where the model is exposed to the
correct output (ground truth) during training but lacks a correct output
for correction during inference, thus if a word is misinterpreted during
inference, the error accumulates increasingly; the other is that the
generation of abstracts itself is not static, lacks a standard answer,
and good abstracts have many possibilities (these possibilities are
generally considered in abstract evaluation criteria), but the static
learning method using the maximum likelihood objective function kills
these possibilities.</p></li>
<li><p>Therefore, the authors introduced policy learning, a strategy
search reinforcement learning method, for the abstracting task beyond
supervised learning. In reinforcement learning, the model is not aimed
at generating outputs most similar to the labels, but at maximizing a
certain indicator. Here, the authors refer to a reinforcement learning
algorithm from the image annotation task: the self-critical policy
gradient training algorithm:</p>
<p><span class="math display">\[
L_{rl} = (r(y)-r(y^s))\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)
\]</span></p>
<p>r is the 人工 evaluation reward function; the parameters of the two r
functions are: the former is the baseline sentence obtained by
maximizing the output probability, and the latter is the sentence
obtained by sampling from the conditional probability distribution of
each step; the goal is to minimize this L objective function. If the
manually awarded sentence obtained from the sampling has more rewards
than the baseline sentence, then this minimization of the objective
function is equivalent to maximizing the conditional probability of the
sampled sentence (after the calculation of the first two r functions, it
becomes a negative sign)</p></li>
<li><p>Afterward, the author combines the two objective functions of
supervised learning and reinforcement learning:</p></li>
</ul>
<h1
id="distributed-representations-of-words-and-phrases-and-their-compositionality">Distributed
Representations of Words and Phrases and their Compositionality</h1>
<ul>
<li><p>Described the negative sampling version of w2v.</p></li>
<li><p>Training with phrases as the basic unit rather than words can
better represent some idiomatic phrases.</p></li>
<li><p>Using NCE (Noise Contrast Estimation) instead of hierarchical
softmax, NCE approximates the maximization of the logarithmic
probability of softmax, as in W2V, we only care about learning good
representations, therefore, a simplified version of NCE, negative
sampling, is used to replace the conditional probability of the output
with the following formula:</p>
<p><span class="math display">\[
p(w_O | w_I) = \frac {exp(v_{w_O}^T v_{w_I})}{\sum _{w=1}^W
exp(v_{w_O}^T v_{w_I})}
\]</span></p>
<p>$$ log \sigma (v_{w_O}^T v_{w_I}) + \sum_{i=1}^k E[w_i \sim P_n(w)]
[log \sigma (v_{w_O}^T v_{w_I})]</p></li>
<li><p>Each time, only the target label and k noise labels (i.e.,
non-target labels) are activated in the softmax output layer, i.e., for
each word, there are k+1 samples, 1 positive sample, and k negative
samples obtained by sampling, which are then classified using logistic
regression. The above expression is the likelihood function of logistic
regression, where Pn is the probability distribution of the
noise.</p></li>
<li><p>Downsample common words because the vector representation of
common words is easy to stabilize; even after several million training
iterations, there is little change, so each word's training is skipped
with a certain probability:</p></li>
<li><p>The skip-gram model trained in this way has good additive
semantic compositionality (the component-wise addition of two vectors),
i.e., Russia + river is close to the Volga River, because the vectors
are logarithmically related to the probabilities of the output layer,
and the sum of two vectors is related to the product of two contexts,
which is equivalent to logical AND: high probability multiplied by high
probability results in high probability, and the rest is low
probability. Therefore, it has this simple arithmetic semantic
compositionality.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0o00J.jpg" alt="i0o00J.jpg" />
<figcaption aria-hidden="true">i0o00J.jpg</figcaption>
</figure>
<h1
id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural
Machine Translation By Jointly Learning To Align And Translate</h1>
<ul>
<li>发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。</li>
<li>编码器解码器模型，翻译任务。</li>
<li>其中双向GRU做编码器。编码隐藏层向量由双向连接而成。</li>
<li>生成每一个单词时有不同的表示。</li>
<li>权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。</li>
<li>对所有步编码隐藏层向量加权生成表示。 <img data-src="https://s1.ax1x.com/2018/10/20/i0oB79.png" alt="i0oB79.png" /></li>
</ul>
<h1
id="effective-approaches-to-attention-based-neural-machine-translation">Effective
Approaches to Attention-based Neural Machine Translation</h1>
<ul>
<li><p>发布于2015.8，作者（Minh-Thang
Luong）使用RNN编码器解码器模型，翻译任务。</p></li>
<li><p>其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：</p>
<p><span class="math display">\[
h_t = tanh(W_c[c_t;h_t])
\]</span></p>
<p>之后注意力向量过softmax生成概率分布。</p></li>
</ul>
<h2 id="全局注意力">全局注意力</h2>
<ul>
<li>文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry
Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oH9P.png"
alt="i0oH9P.png" /></li>
<li>之后引入了两种Effective
Approaches，即局部注意力和input-feeding。</li>
</ul>
<h2 id="局部注意力">局部注意力</h2>
<ul>
<li><p>局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：</p>
<ul>
<li><p>单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。</p></li>
<li><p>预测对齐：训练对齐位置。</p>
<p><span class="math display">\[
p_t = S \cdots sigmoid(v_p^T tanh(W_ph_t)) \\
\]</span></p>
<p>其中<span
class="math inline">\(h_t\)</span>是第t个生成单词的隐藏层向量 <span
class="math inline">\(W_p\)</span>和<span
class="math inline">\(v_p\)</span>都是需要训练的权重
S是输入单词长度,与sigmoid相乘就得到输入句中任意位置</p></li>
</ul></li>
<li><p>另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。</p>
<p><span class="math display">\[
a_t(s) = align(h_t,h_s)exp(-\frac{(s-p_t)^2}{2\sigma ^2})
\]</span></p>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ost1.png" alt="i0ost1.png" />
<figcaption aria-hidden="true">i0ost1.png</figcaption>
</figure></li>
</ul>
<h2 id="input-feeding">Input-feeding</h2>
<ul>
<li>Input-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。</li>
<li>实验结果表明使用预测对齐的局部注意力模型表现最好。 <img data-src="https://s1.ax1x.com/2018/10/20/i0oyfx.png" alt="i0oyfx.png" /></li>
</ul>
<h1
id="a-neural-attention-model-for-abstractive-sentence-summarization">A
Neural Attention Model for Abstractive Sentence Summarization</h1>
<ul>
<li>发布于2015.9，作者Alexander M.
Rush，解码器编码器模型，文摘任务。</li>
<li>提出了一种注意力编码器，使用普通的NNLM解码器。</li>
<li>未使用RNN，直接用词向量。</li>
<li>使用全部输入信息,局部输出信息(yc)构建注意力权重。</li>
<li>直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。</li>
<li>模型如下图: <img data-src="https://s1.ax1x.com/2018/10/20/i0ocp6.png"
alt="i0ocp6.png" /></li>
</ul>
<h1
id="abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond">Abstractive
Text Summarization using Sequence-to-sequence RNNs and Beyond</h1>
<ul>
<li>发布于2016.8，作者Ramesh
Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。</li>
<li>基于Dzmitry
Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。</li>
<li>改进包括LVT（large vocabulary trick)、Feature-rich
encoder、switching generator-pointer、分层注意力。</li>
</ul>
<h2 id="lvt">LVT</h2>
<ul>
<li>减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。</li>
</ul>
<h2 id="feature-rich-encoder">Feature-rich Encoder</h2>
<ul>
<li>不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量
<img data-src="https://s1.ax1x.com/2018/10/20/i0og1K.png"
alt="i0og1K.png" /></li>
</ul>
<h2 id="switching-generator-pointer">Switching Generator-pointer</h2>
<ul>
<li>解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oRXD.png"
alt="i0oRXD.png" />) Switching generator/pointer model
开关为G时就用传统方法生成文摘 开关为P时就从输入中拷贝单词到文摘中</li>
</ul>
<h2 id="分层注意力">分层注意力</h2>
<ul>
<li>传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。
<img data-src="https://s1.ax1x.com/2018/10/20/i0ofne.png"
alt="i0ofne.png" /></li>
</ul>
<h1 id="recurrent-neural-network-regularization">Recurrent Neural
Network Regularization</h1>
<ul>
<li>本文介绍了如何在循环神经网络中使用dropout来防止过拟合</li>
<li>Dropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。</li>
<li>对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。</li>
<li>在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。</li>
<li>作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息</li>
<li>效果如图: <img data-src="https://s1.ax1x.com/2018/10/20/i0ov7j.png"
alt="i0ov7j.png" /></li>
</ul>
<h1
id="building-end-to-end-dialogue-systems-using-generative-hierarchical-neural-network-models">Building
End-To-End Dialogue Systems Using Generative Hierarchical Neural Network
Models</h1>
<ul>
<li>本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。</li>
<li>训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。</li>
<li>分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。
<img data-src="https://s1.ax1x.com/2018/10/20/i0TC90.png" alt="i0TC90.png" />
三段式对话包含在两个词水平RNN端到端系统中
中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量</li>
<li>文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。</li>
<li>系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word
perplexity衡量语法准确度，word classification error衡量语义准确度</li>
<li>论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i
don’t know”或者”i’m
sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。</li>
</ul>
<h1 id="news-event-summarization-complemented-by-micropoints">News Event
Summarization Complemented by Micropoints</h1>
<ul>
<li>这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。</li>
<li>文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。</li>
<li>这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet
Growth Model。</li>
<li>Micropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。</li>
<li>筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint
Topic Modeling(Joint topic modeling for event summarization across news
and social media
streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。</li>
<li>将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t)
= (p(topic 1 |t), p(topic 2 |t), ..., p(topic n
|t))，最后使用DBSCAN完成主题聚类。</li>
<li>使用团队之前提出的Snippet Growth
Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。</li>
<li>现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。</li>
<li>得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。</li>
</ul>
<h1
id="disan-directional-self-attention-network-for-rnncnn-free-language-understanding">DiSAN:
Directional Self-Attention Network for RNN/CNN-Free Language
Understanding</h1>
<ul>
<li>更新，作者后来又推出了fast
disan，貌似是改了注意力的计算，细节待补充</li>
<li>作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。</li>
<li>作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A
Neural Attention Model for Abstractive Sentence
Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。</li>
<li>作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。</li>
<li>传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。
<img data-src="https://s1.ax1x.com/2018/10/20/i0ojBQ.png" alt="i0ojBQ.png" />
右边是多维度注意力
可以看到注意力权重变成了向量，与输入词向量维度数相同</li>
<li>一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。</li>
<li>有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：
<img data-src="https://s1.ax1x.com/2018/10/20/i0ozAs.png"
alt="i0ozAs.png" /></li>
<li>最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。</li>
</ul>
<h1 id="neural-summarization-by-extracting-sentences-and-words">Neural
Summarization by Extracting Sentences and Words</h1>
<ul>
<li>本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。</li>
<li>与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。</li>
<li>因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。</li>
<li>编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。</li>
<li>句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。</li>
<li>与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。</li>
<li>与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。</li>
<li>抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。</li>
<li>抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。</li>
</ul>
<h1 id="a-deep-reinforced-model-for-abstractive-summarization">A DEEP
REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1>
<ul>
<li><p>使用强化学习来优化当前的端到端生成式文摘模型 <img data-src="https://s1.ax1x.com/2018/10/20/i0TicT.png"
alt="i0TicT.png" /></p></li>
<li><p>解决长文摘生成和重复短语问题</p></li>
<li><p>强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘</p></li>
<li><p>模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励</p></li>
<li><p>编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。</p></li>
<li><p>编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。</p></li>
<li><p>解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。</p></li>
<li><p>在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。</p></li>
<li><p>在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。</p></li>
<li><p>之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure
bias，即模型在训练时是接触到了正确的输出(ground
truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。</p></li>
<li><p>因此作者在监督学习之外为文摘任务引入了policy
learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical
policy gradient training algorithm：</p>
<p><span class="math display">\[
L_{rl} = (r(y)-r(y^s))\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)
\]</span></p>
<p>r是人工评价奖励函数
两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子
目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）</p></li>
<li><p>之后作者将监督学习和强化学习的两种目标函数结合起来：</p></li>
</ul>
<h1
id="distributed-representations-of-words-and-phrases-and-their-compositionality">Distributed
Representations of Words and Phrases and their Compositionality</h1>
<ul>
<li><p>介绍了w2v的负采样版本。</p></li>
<li><p>以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic
phrase。</p></li>
<li><p>用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：</p>
<p><span class="math display">\[
p(w_O | w_I) = \frac {exp(v_{w_O}^T v_{w_I})}{\sum _{w=1}^W
exp(v_{w_O}^T v_{w_I})}
\]</span></p>
<p>被替换成 $$ log (v_{w_O}^T v_{w_I}) + <em>{i=1}^k E</em>{w_i P_n(w)}
[log (v_{w_O}^T v_{w_I})]</p></li>
<li><p>每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。</p></li>
<li><p>对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以一定概率跳过：</p></li>
<li><p>这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>abstractive summarization</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Summarization-Related Papers Reading (ACL/NAACL 2019)</title>
    <url>/2019/08/15/acl2019-summarization/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>Selected Reading of ACL/NAACL 2019 Automatic Summarization Papers</p>
<ul>
<li><p>DPPs Similarity Measurement Improvement</p></li>
<li><p>STRASS: Backpropagation for Extractive Summarization</p></li>
<li><p>Translate first, then generate the abstract</p></li>
<li><p>Reading Comprehension + Automatic Abstract</p></li>
<li><p>BiSET: Retrieve + Fast Rerank + Selective Encoding + Template
Based</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</h1>
<ul>
<li>This is very similar to what the others in our group have done,
using DPPs to process extractive abstractive summaries</li>
<li>In the abstract indicator paper mentioned in the preceding text, it
is also mentioned that creating an abstract, especially a key sentence
abstract, is all about three words: qd, qd, and still qd!
<ul>
<li>q: quality, which sentence is important and needs to be extracted as
an abstract. This step is feature construction.</li>
<li>d: diversity, the sentences extracted should not be redundant, and
should not repeatedly use the same sentences. If there are too many
important sentences that are the same, they become less significant.
This step is the construction of the sampling method.</li>
</ul></li>
<li>Determinantal Point Processes (DPPs) are a sampling method that
ensures the extracted sentences are important (based on precomputed
importance values) and non-repetitive. The authors of this paper have a
very clear line of thought: I want to improve the DPPs in extractive
summarization, how to improve them? DPPs rely on the similarity between
sentences to avoid extracting duplicate sentences, so I will directly
improve the calculation of similarity, thus the problem is shifted to a
very mature field: semantic similarity computation.</li>
<li>Next, just use the web to do semantic similarity calculation. The
author is quite innovative, using capsule networks, which were
originally proposed to solve the problem of relative changes in object
positions in computer vision. The author believes that it can be
generalized to extract spatial and directional information of low-level
semantic features. Here, I am not very familiar with capsule networks
and their applications in NLP, but based on the comparative experiments
provided by the author, the improvement is actually just one point, and
the entire DPP is only 2 points better than the best system before
(2009), which seems a bit forced.</li>
<li>The network provided by the author is truly complex, not in terms of
principle, but due to the use of many components, including:
<ul>
<li>CNN with three to seven different sizes of convolutional kernels for
extracting low-level features</li>
<li>Capsule networks extract high-level features, utilizing recent
techniques such as parameter sharing and routing</li>
<li>One-hot vectors were still used, i.e., whether a word exists in a
certain sentence</li>
<li>Fusion of various features, including inner product, absolute
difference, and concatenation with all independent features, to predict
the similarity between two sentences</li>
<li>And the similarity is only part of the goal; the authors also used
LSTM to reconstruct two sentences, incorporating the reconstruction loss
into the final total loss <img data-src="https://s2.ax1x.com/2019/08/12/ezCge0.png" alt="ezCge0.png" /></li>
</ul></li>
<li>Absolutely, at first glance, one would assume this is a CV's
work</li>
<li>The author at least used the latest available techniques, creating
an integrated network, which may not be as concise and elegant in an
academic sense, but in terms of industry, many such network integration
operations are very effective</li>
<li>Another point is that although it is a sampled abstract, the
author's work is fully supervised, so a dataset still needs to be
constructed
<ul>
<li>Constructing Supervised Extractive Summary Datasets from Generative
Summary Datasets</li>
<li>Constructing a supervised sentence similarity calculation dataset
from generative abstract data sets</li>
<li>This structure also limits its generalization ability to some
extent</li>
</ul></li>
<li>Authors' starting point is actually very good: because traditional
similarity is at the word level, without delving into semantic features,
the direction of constructing a network to extract features is correct,
albeit somewhat complex. Moreover, since only the sentence feature
extraction part of the similarity calculation in the extraction-based
abstracting method has been improved, the overall impact is not
particularly significant. The final result may have outperformed many
traditional methods, but it has not improved much compared to the
traditional best method, and it is only about 1 point better than pure
DPPs.</li>
</ul>
<h1
id="strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/14/mkkJHO.png" alt="mkkJHO.png" />
<figcaption aria-hidden="true">mkkJHO.png</figcaption>
</figure>
<ul>
<li><p>Another paper using supervised methods for extractive
summarization, the content can be roughly guessed from the title, based
on embedding, and aiming to be light and effective, the simplest goal is
to keep the embedding of the summary consistent with the gold
embedding.</p></li>
<li><p>The difficulty lies in the fact that it is extractive and
discrete, thus requiring a process to unify the three parts of
extraction, embedding, and comparison scoring, softening it to be
differentiable, enabling end-to-end training. The authors propose four
steps:</p>
<ul>
<li>Mapping document embeddings to a comparison space</li>
<li>Extract sentences to form an abstract</li>
<li>Extraction-based abstract embedding</li>
<li>Comparison with gold summary embedding</li>
</ul></li>
<li><p>First, given a document, directly obtain the doc embedding and
the sentence embedding for each sentence in the document using
sent2vec</p></li>
<li><p>After that, only one fully connected layer is used as the mapping
function f(d), where the author proposes the first hypothesis: the
extracted abstract sentences should have similarity to the document:</p>
<p><span class="math display">\[
sel(s,d,S,t) = sigmoid (ncos^{+}(s,f(d),S)-t)
\]</span></p></li>
<li><p>s is the sentence embedding, S represents the set of sentences, t
is the threshold. sel represents select, i.e., the confidence of
selecting this sentence to form the summary. This formula indicates that
the similarity between the selected sentence embedding and the document
embedding should be greater than the threshold t, and sigmoid is used
for softening, converting {0,1} to [0,1].</p></li>
<li><p>Afterward, further softening is applied; the author does not
select sentences to form the abstract based on scores, but directly
approximates the abstract's embedding based on scores</p>
<p><span class="math display">\[
app(d,S,t) = \sum _{s \in S} s * nb_w(s) * sel(s,d,S,t)
\]</span></p></li>
<li><p>nb_w is the number of words, i.e., the sum of the embedding of
all sentences weighted by the number of words in each sentence and the
select score to obtain the embedding of the generated summary</p></li>
<li><p>The final step involves comparing the embedding similarity
calculation loss with the gold summary, where the authors introduce a
regularization term to aim for a higher compression ratio of the
extracted summary. I feel that this is a compensation brought about by a
series of softening operations in the previous step, as no sentences are
selected; instead, all sentences are weighted, thus necessitating
regularization to force the model to discard some sentences:</p>
<p><span class="math display">\[
loss = \lambda * \frac{nb_w(gen_sum)}{nb_w(d)} + (1-\lambda) *
cos_{sim}(app(d,S,t),ref_{sum})
\]</span></p></li>
<li><p>What is the method for obtaining the embedding of the gold
summary?</p></li>
<li><p>The authors also normalized the results of the cosine similarity
calculation to ensure that the same threshold could be applied to all
documents</p></li>
<li><p>The results actually show that ROUGE is not as good as generative
methods, of course, one reason is that the dataset is inherently
generative, but it is strong in simplicity, speed, and when using
supervised methods for extraction, there is no need to consider the
issue of redundancy.</p></li>
</ul>
<h1 id="a-robust-abstractive-system-for-cross-lingual-summarization">A
Robust Abstractive System for Cross-Lingual Summarization</h1>
<ul>
<li>In fact, one sentence can summarize this paper: Generate
multilingual abstracts by first translating, while others first abstract
and then translate</li>
<li>All are implemented using existing frameworks
<ul>
<li>Marian: Fast Neural Machine Translation in C++</li>
<li>Abstract: Pointer-generator</li>
</ul></li>
<li>The author actually has sufficient supervisory data; it was
previously thought that the abstracts were multilingual, with small
amounts of corpus, or were summaries not relying on translation, which
could extract common abstract features across multiple languages</li>
<li>However, this paper indeed achieved robustness: generally, to
achieve robustness, one introduces noise, and this paper exactly used
back-translation to introduce noise: first, English is translated into a
minor language, then translated back, and trained a generative abstract
model on this bad English document, making the model more robust to
noise. The final results were also significantly improved, and it also
achieved good effects on Arabic that had not been trained, indicating
that different people's translations have their own correctness, while
the errors in machine translation are always similar.</li>
</ul>
<h1
id="answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/21/mtX0eS.png" alt="mtX0eS.png" />
<figcaption aria-hidden="true">mtX0eS.png</figcaption>
</figure>
<ul>
<li><p>This paper comes from the Smart Lab of NTT, the so-called largest
telecommunications company in the world, and proposes a multi-task
model: Reading Comprehension + Automatic Summary</p></li>
<li><p>The paper conducted many experiments and analyses, and provided a
detailed analysis of the conditions under which their module works, and
also utilized many techniques proposed in recent years for the abstract
section, rather than simply patching together.</p></li>
<li><p>This paper is also based on the HotpotQA dataset, similar to the
one in CogQA, but that one used the full wiki setting, which means there
was no gold evidence. This paper, however, requires gold evidence, so it
used the HotpotQA distractor setting.</p></li>
<li><p>For the distractor setting of HotpotQA, the supervisory signal
consists of two parts: answer and evidence, with the input also having
two parts: query and context, where the evidence is a sentence within
the context. The authors adopt the baseline from the HotpotQA paper:
Simple and effective multi-paragraph reading comprehension, and all
parts except the Query-Focused Extractor shown in the above figure. The
basic idea is to combine the query and context, add a lot of fully
connected (FC), attention, and BiRNN to extract features, and finally
output a classification of answer type and a sequence labeling of answer
span in the answer part, while directly applying the output of the BiRNN
to each sentence for binary classification. <img data-src="https://s2.ax1x.com/2019/08/21/mtXczq.png"
alt="mtXczq.png" /></p></li>
<li><p>The author refines the supervision task of evidence into a
query-based summarization, adding a module called Query-Focused
Extractor (QFE) after the BiRNN, emphasizing that the evidence should be
a summary extracted from the context under the query conditions,
satisfying:</p>
<ul>
<li>Sentences within the summary should not be redundant</li>
<li>sentences within the summary should have different attention based
on the query</li>
</ul></li>
<li><p>For the first point, the author designed an RNN within the QFE,
which allows attention to be paid to previously extracted sentences
during the generation of attention and even the extraction of summaries.
The time step of the RNN is defined as each time a sentence is
extracted, with the input being the vector of the sentence extracted at
that time step</p></li>
<li><p>In response to the second point, the author added an attention
mechanism for the query within the QFE, with the weighted query vector
referred to as glimpse. Note that this is the attention from the QA
context to the QA query; both the key and value in the attention are the
QA query, while the query in the attention does not directly take the
entire QA context but rather the output of the RNN, i.e., the context
encoded by the RNN after extracting a set of sentences. Such a design is
also intuitive.</p></li>
<li><p>After the RNN encodes the extracted sentences and forms glimpse
vectors with attention-weighted queries, QFE receives these two vectors,
combines them with the vectors of unextracted sentences for each
context, to output the probability of each sentence being extracted, and
then selects the sentence with the highest probability to add to the set
of extracted sentences. Subsequently, the system continues to cyclically
calculate the RNN and glimpse. The dependency relationships of the
entire system are clearly shown in the figure above.</p></li>
<li><p>Due to the variable number of sentences in gold evidence, the
author employs the method of adding a dummy sentence with an EOE to
dynamically extract, and when an EOE is extracted, the model no longer
continues to extract sentences.</p></li>
<li><p>During training, the loss function for evidence is:</p>
<p><span class="math display">\[
L_E = - \sum _{t=1}^{|E|} \log (max _{i \in E / E^{t-1}} Pr(i;E^{t-1}))
+ \sum _i min(c_i^t, \alpha _i^t)
\]</span></p>
<p>Here, <span class="math inline">\(E\)</span> is the set of sentences
of gold evidence, <span class="math inline">\(E^t\)</span> is the set of
sentences extracted by QFE, <span class="math inline">\(\alpha
_i^t\)</span> is the attention of the i-th word in the query at time
step t, where the time step is consistent with the previous text, being
the time step for extracting sentences. <span class="math inline">\(c^t
= \sum _{i=1}^{t-1} \alpha ^i\)</span> is the coverage vector. The first
half of the loss refers to the negative log-likelihood loss of the gold
evidence, finding the gold sentence with the highest QFE prediction
probability in the extracted sentence set, calculating the loss, and
then excluding this sentence to find the next highest, until all gold
sentences are found or no gold sentence can be found in the extracted
sentence set. The second half is a regularization application of the
coverage mechanism to ensure that the sentences selected for loss
calculation do not have overly repetitive (concentrated) attention on
the query.</p></li>
<li><p>Authors achieved results on the HotpotQA and textual entailment
dataset FEVER, with the evidence part of the indicators far superior to
the baseline, while the answer part also saw a significant improvement,
though not as pronounced as the evidence part, and slightly inferior to
the BERT model. On the full wiki test set, it was also comprehensively
surpassed by CogQA. Here, the authors state that there is a dataset
shift problem. However, at least this paper achieved an 8-point
improvement on the answer part by simply adding a small module to the
baseline, demonstrating that a well-designed summarization part indeed
helps in the selection of answers in multi-task learning.</p></li>
</ul>
<h1
id="biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</h1>
<ul>
<li>Another model pieced together from various components, the title
actually spells it all out: Bi-directional, selective encoding,
template, together forming the BiSET module, and the other two preceding
processes: Retrieve and Fast Rerank also follow the architecture from
the paper "Retrieve, Rerank and Rewrite: Soft Template Based Neural
Summarization." It should be based on soft template summarization, with
the mechanism of selective encoding added, so these two papers are put
together to discuss template-based generative summarization and its
improvements.</li>
<li>The idea behind the soft template approach is not to let the model
generate sentences entirely, but rather for humans to provide the
template and for the model to only fill in the words. However, if the
template is completely designed manually, it would regress to the
methods of several decades ago. The author's approach is to
automatically extract templates from existing gold summaries.</li>
<li>Generally divided into three steps:
<ul>
<li>Retrieve: Extract candidates from the training corpus</li>
<li>Rerank: Learning Template Saliency Measurement for seq2seq
Models</li>
<li>Rewriting: Let the seq2seq model learn to generate the final
summary</li>
</ul></li>
<li>This method should be more suitable for long sentence compression,
or for single-sentence generative summarization, where the long
sentences to be compressed can be used as queries for retrieval</li>
</ul>
<h2 id="retrieve">Retrieve</h2>
<ul>
<li>Utilizing the existing Lucene search engine, given a long sentence
to be compressed as a query, search the document collection to identify
the summaries of the top 30 documents as candidate templates</li>
</ul>
<h2 id="rerank">Rerank</h2>
<ul>
<li><p>The abstracts (soft template) retrieved through the search are
sorted by relevance, but we require sorting by similarity. Therefore, we
use the ROUGE score to measure the similarity between the soft template
and the gold summary. Here, reranking is not about sorting out the
results but rather considering the rank of each template comprehensively
during the generation of the summary, and the loss can be observed in
the parts that are omitted.</p></li>
<li><p>Specifically, first use a BiLSTM encoder to encode the input x
and a certain candidate template r; here, the hidden layer states are
encoded separately, but the same encoder is used, and then input the two
hidden layer states into a Bilinear network to predict the ROUGE value
between the gold summary y corresponding to the input x and r, which is
equivalent to a network that makes a saliency prediction for r given
x:</p>
<p><span class="math display">\[
h_x = BiLSTM(x) \\
h_r = BiLSTM(r) \\
ROUGE(r,y) = sigmoid(h_r W_s h_x^T + b_s) \\
\]</span></p></li>
<li><p>This completes the supervised part of reranking</p></li>
</ul>
<h2 id="rewrite">Rewrite</h2>
<ul>
<li>This part is a standard seq2seq, still using the previously encoded
<span class="math inline">\(h_x, h_r\)</span> to concatenate it and feed
it into an attentional RNN decoder to generate an abstract, and
calculate the loss</li>
</ul>
<h2 id="jointly-learning">Jointly Learning</h2>
<ul>
<li>The model's loss is divided into two parts. The Rerank part ensures
that the encoded template and the input, after passing through bilinear
processing, can correctly predict the ROUGE value. The Rewrite part
ensures the generation of a correct summary. This is equivalent to, in
addition to the ordinary seq2seq summary generation, I also candidate
some other gold summaries as input. This candidate is initially filtered
through retrieval. When used, the Rerank part guarantees that the
encoded part is the template component within the summary, i.e., the
part that can be taken out and compared with the gold summary, thereby
assisting the decoder in the Rewrite part's generation.</li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>We know that in summarization, the decoder is actually very
dependent on the encoder's input, which includes both the template and
the original input. The authors provide several ideal examples, where
the output summary is basically in the format of the template, but the
key entities are extracted from the original input and filled into the
template summary.</li>
<li>Although a somewhat esoteric rerank loss method was used for
extracting the soft template, the role of the template is indeed
evident. The model actually finds a summary that is very close to the
gold summary as input, and makes slight modifications (rewrites) on this
basis, which is much more efficient than end-to-end seq2seq. The authors
also tried removing the retrieve step and directly finding the ROUGE
score highest summary from the entire corpus as the template, with the
final model's results reaching 50 ROUGE-1 and 48 ROUGE-L</li>
<li>This operation of taking the output as input is actually a
compensation for the insufficient abstract ability of the decoder, and
it is an empirical method derived from the observation of the dataset,
which can effectively solve the problem</li>
</ul>
<h2 id="biset">biset</h2>
<ul>
<li>Replaced the rerank part with CNN+GLU to encode documents and
queries, and then computed the sim matrix using the encoded vectors</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="improving-the-similarity-measure-of-determinantal-point-processes-for-extractive-multi-document-summarization">Improving
the Similarity Measure of Determinantal Point Processes for Extractive
Multi-Document Summarization</h1>
<ul>
<li>这和我们组其他人做的很像，用DPPs处理抽取式文摘</li>
<li>在上文的文摘指标论文中也提到了，搞文摘，尤其是抽取式文摘，就是三个词，qd，qd，还是qd!
<ul>
<li>q:quality，哪个句子重要，需要被抽出来作为文摘。这一步是特征的构造</li>
<li>d:diversity,抽取出来的句子之间不能冗余，不能老抽一样的句子。重要的一样的句子多了，也就变得不重要了。这一步是抽样方法的构造</li>
</ul></li>
<li>DPPs(Determinantal Point
Processes)就是一种抽样方法，保证抽出来的句子重要（根据特征计算好的重要性数值），而且不重复。本文作者就是思路非常清晰：我要改进抽取式文摘中的DPPs，怎么改进？DPPs依赖句子之间的相似度来避免抽重复的句子，那我就改进不管DPPs怎么改，直接改相似度的计算，所以问题就换到了一个非常成熟的领域：语义相似度计算。</li>
<li>接下来就随便套网络来做语义相似度计算就行了。作者比较新潮，用了胶囊网络，这个网络本来是提出用于解决计算机视觉中物体的位置相对变化问题的，作者认为可以将其泛化到提取底层语义特征的空间与方位信息，这里我对胶囊网络及其在NLP的应用不太了解，但是就作者给出的对比实验来说，改进其实也就1个点，而整个DPP相比之前最好系统（2009）也就好2个点，感觉还是有点刻意为之。</li>
<li>另外作者给出的网络是真的复杂，不是原理复杂，而是用了很多组件，包括：
<ul>
<li>三四五六七大小卷积核的CNN用于提取底层特征</li>
<li>胶囊网络提取高层特征，用到了近年来的参数共享和路由等技巧</li>
<li>还是用了one-hot向量，即一个词是否存在在某一个句子里</li>
<li>各种特征的融合，包括内积，绝对差，再和所有独立的特征全部拼接到一起，预测两个句子的相似度</li>
<li>而且相似度还只是一部分目标，另外作者还用了LSTM来重构两个句子，将重构损失加入最终的总损失
<img data-src="https://s2.ax1x.com/2019/08/12/ezCge0.png"
alt="ezCge0.png" /></li>
</ul></li>
<li>粗看图片绝对以为这是一篇CV的work</li>
<li>作者好歹是把最近能用的技巧都用上了，做了一个集大成的网络，可能学术上看没那么简洁优美，但是就工业上来说很多这样的网络集成操作就是很work</li>
<li>另外虽然是抽取式摘要，但作者的工作是完全监督，因此还需要构造数据集
<ul>
<li>从生成式摘要数据集中构造有监督抽取式摘要数据集</li>
<li>从生成式摘要数据集中构造有监督句子相似度计算据集</li>
<li>这种构造也一定程度上限制了其泛化能力</li>
</ul></li>
<li>作者的出发点其实非常好：因为传统的相似度停留在词的粒度，没有深入到语义特征，因此构造网络提取特征的方向没错，只不过稍显复杂，而且由于仅仅改进了抽取式文摘中某一种抽样方法的相似度计算中的句子特征提取部分，对整体的影响并没有特别大，最后的结果虽然吊打了很多传统方法，但是相比传统最佳方法并没有提高多少，相比纯DPPs更是只有1个点左右的提高。</li>
</ul>
<h1
id="strass-a-light-and-effective-method-for-extractive-summarization-based-on-sentence-embeddings">STRASS:
A Light and Effective Method for Extractive Summarization Based on
Sentence Embeddings</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/14/mkkJHO.png" alt="mkkJHO.png" />
<figcaption aria-hidden="true">mkkJHO.png</figcaption>
</figure>
<ul>
<li><p>又是一篇用监督方法做抽取式文摘的，从标题就可以大致猜出来内容，基于embedding，还要light
and effective，那最简单的目标就是把摘要的embedding和gold
embedding保持一致。</p></li>
<li><p>困难的地方在于这是抽取式的，是离散的，因此需要一个流程把抽取、embedding、比较打分三个部分统一起来，软化使其可导，可以端到端训练，作者给出的是四个步骤：</p>
<ul>
<li>将文档embedding映射到比较空间</li>
<li>抽句子组成摘要</li>
<li>去近似抽出的摘要的embedding</li>
<li>和gold summary的embedding比较</li>
</ul></li>
<li><p>首先给定一篇文档，直接用sent2vec获得doc
embedding和文档里每一句的sentence embedding</p></li>
<li><p>之后只用一层全连接作为映射函数f(d)，这里作者给出了第一个假设：抽取的摘要句子应该和文档具有相似度：</p>
<p><span class="math display">\[
sel(s,d,S,t) = sigmoid (ncos^{+}(s,f(d),S)-t)
\]</span></p></li>
<li><p>其中s是句子embedding，S代表句子集，t为阈值。
sel代表select，即选择这个句子组成摘要的置信度，这个式子说明选出句子的embedding和文档embedding之间的相似度应该大于阈值t，且使用sigmoid做了软化，将{0,1}软化为[0,1]</p></li>
<li><p>之后进一步软化，作者并不根据分数选出句子组成文摘，而是根据分数直接近似文摘的embedding</p>
<p><span class="math display">\[
app(d,S,t) = \sum _{s \in S} s * nb_w(s) * sel(s,d,S,t)
\]</span></p></li>
<li><p>其中nb_w是number of words，即使用每个句子的字数和select
score对所有的句子embedding加权求和得到generated
summary的embedding</p></li>
<li><p>最后一步，和gold
summary比较embedding相似度计算损失，这里作者加入了一个正则项，希望提出来的摘要压缩比越高越好，这里我感觉是上一步一系列软化操作带来的补偿，因为没有选择句子，而是对所有句子加权，因此需要正则强迫模型放弃一些句子：</p>
<p><span class="math display">\[
loss = \lambda * \frac{nb_w(gen_sum)}{nb_w(d)} + (1-\lambda) *
cos_{sim}(app(d,S,t),ref_{sum})
\]</span></p></li>
<li><p>这里有一个问题，gold summary的embedding是怎么得到的？</p></li>
<li><p>另外为了保证能够对所有文档使用同一个阈值，作者还对cosine相似度计算的结果做了归一化</p></li>
<li><p>从结果来看其实ROUGE还不如生成式的方法好，当然一方面原因是因为数据集本来就是生成式的，但是强在简单，快，而且用监督的方法做抽取式也不用考虑redundency的问题。</p></li>
</ul>
<h1 id="a-robust-abstractive-system-for-cross-lingual-summarization">A
Robust Abstractive System for Cross-Lingual Summarization</h1>
<ul>
<li>其实一句话就能概括这篇论文：做多语言生成式文摘，别人是先摘要再翻译，这篇文章是先翻译再生成摘要</li>
<li>均是用已有框架实现
<ul>
<li>翻译：Marian: Fast Neural Machine Translation in C++</li>
<li>摘要：pointer-generator</li>
</ul></li>
<li>作者其实有充足的监督数据，之前以为是多语言、小语料的摘要，或者是不借助于翻译的摘要，能够挖掘多语种共有的摘要特征</li>
<li>但是这篇论文确实实现了robust：一般要做robust就是引入噪声，本文正好用了回译引入噪声：先把英语翻译成小语种，翻译回来，在这样的bad
english
document上训练生成式摘要模型，使得模型对噪声更加鲁棒，最后的结果也是提高了许多，并且对没有训练过的阿拉伯语也取得了较好的效果，说明不同的人翻译各有各的正确，而机器翻译的错误总是相似的。</li>
</ul>
<h1
id="answering-while-summarizing-multi-task-learning-for-multi-hop-qa-with-evidence-extraction">Answering
while Summarizing: Multi-task Learning for Multi-hop QA with Evidence
Extraction</h1>
<figure>
<img data-src="https://s2.ax1x.com/2019/08/21/mtX0eS.png" alt="mtX0eS.png" />
<figcaption aria-hidden="true">mtX0eS.png</figcaption>
</figure>
<ul>
<li><p>这篇论文来自号称全球最大电信公司NTT的智能实验室，提出了一个多任务模型：阅读理解+自动摘要</p></li>
<li><p>论文做了很多实验和分析，并且详细分析了在何种情况下他们的module
works，对于摘要部分也利用了许多近年来提出的技巧，而不是简单的拼凑。</p></li>
<li><p>这篇论文同样也是在HotpotQA数据集上做，和CogQA那一篇一样，但那一篇用的是full
wiki setting，即没有gold evidence，而这篇需要gold
evidence因此用了HotpotQA 的distractor setting。</p></li>
<li><p>对于HotpotQA的distractor
setting，监督信号有两部分：answer和evidence，输入有两部分:query和context，其中evidence是context当中的句子。作者沿用了HotpotQA论文里的baseline:Simple
and effective multi-paragraph reading
comprehension，及上图中Query-Focused
Extractor以外的部分。基本思想就是将query和context结合，加上一堆FC,attention，BiRNN提取特征，最终在answer部分输出一个answer
type的分类和answer span的sequence
labelling，而在evidence部分直接接BiRNN输出的结果对每个句子做二分类。
<img data-src="https://s2.ax1x.com/2019/08/21/mtXczq.png"
alt="mtXczq.png" /></p></li>
<li><p>作者将evidence这边的监督任务细化为一个query
based的summarization，就在BiRNN后面加了一个模块，称之为Query-Focused
Extractor(QFE)，相比原始的简单二分类，QFE强调了evidence应该是在query条件下从context中抽取出来的summary，因满足：</p>
<ul>
<li>summary内的句子之间应该不能冗余</li>
<li>summary内不同句子应该有着query上不同的注意力</li>
</ul></li>
<li><p>针对第一点，作者在QFE内设计了一个RNN，使得在生成注意力乃至抽取摘要时都能注意到之前已经抽取出来的句子,其中RNN的时间步定义为每一次抽取一个句子，输入即某一时间步抽取出的句子的vector</p></li>
<li><p>针对第二点，作者在QFE内增加了一个针对query的注意力，加权之后的query向量称为glimpse，注意这里是QA
context对QA
query的注意力，attention里的key和value都是QA的query，而attention里的query不是直接拿整个QA
context，而是RNN的输出，即已经抽取出的句子集经过RNN编码的context，这样的设计也是符合直觉的。</p></li>
<li><p>在RNN编码已抽取句子、注意力加权query形成glimpse向量之后，QFE拿到这两部分向量，结合每一个context未抽取句子的向量来输出每一个句子被抽取的概率，并选择最大概率的句子加入已抽取句子集合，然后接着循环计算RNN和glimpse。整个系统的依赖关系在上图中展示的很清晰。</p></li>
<li><p>由于gold evidence的句子数目不固定，作者采用添加一个EOE的dummy
sentence的方法来动态抽取，当抽取到EOE时，模型就不再接着抽取句子。</p></li>
<li><p>在训练时，evidence这边的损失函数为：</p>
<p><span class="math display">\[
L_E = - \sum _{t=1}^{|E|} \log (max _{i \in E / E^{t-1}} Pr(i;E^{t-1}))
+ \sum _i min(c_i^t, \alpha _i^t)
\]</span></p>
<p>这里<span class="math inline">\(E\)</span>是gold
evidence的句子集合，<span
class="math inline">\(E^t\)</span>是QFE抽取出来的句子集合，<span
class="math inline">\(\alpha
_i^t\)</span>是t时间步query里第i个词的注意力，这里的时间步和前文一致，是抽句子的时间步。而<span
class="math inline">\(c^t = \sum _{i=1}^{t-1} \alpha
^i\)</span>是coverage向量。 损失的前半部分指的是gold
evidence的负对数似然损失，依次在抽取句子集合里找拥有最大QFE预测概率的gold
sentence，算损失，然后排除这个句子接着找下一个最大的，直到gold
sentence找完或者抽取句子集合里找不到gold
sentence，后半部分是coverage机制的一个正则化应用，保证挑出来计算损失的句子不会在query上拥有过于重复（集中）的注意力。</p></li>
<li><p>作者在HotpotQA和文本蕴含数据集FEVER上做了结果，evidence部分的指标远好于baseline，而answer部分的指标也有较大提升，但不如evidence部分明显，且与BERT模型部分相比还差一点，在full
wiki setting的测试集上也被CogQA全面超过，这里作者说存在dataset
shift问题。但至少本文仅仅在baseline上添加了一个小模块，就获得了answer部分的8个点的提升，说明精心设计的summarization部分在多任务学习中确实帮助到了answer的选取。</p></li>
</ul>
<h1
id="biset-bi-directional-selective-encoding-with-template-for-abstractive-summarization">BiSET:
Bi-directional Selective Encoding with Template for Abstractive
Summarization</h1>
<ul>
<li>又是一篇将各个组件拼拼凑凑出来的一个模型，标题其实已经全写出来了：Bi-directional，
selective encoding，
template，共同组成了BiSET模块，另外两个前置过程：Retrieve和Fast
Rerank也是沿用Retrieve, Rerank and Rewrite: Soft Template Based Neural
Summarization这篇论文里的架构。应该大体是基于soft
template的summarization，加上了selective
encoding的机制，因此就把这两篇论文放在一起，讨论基于模板的生成式摘要及其改进。</li>
<li>基于软模板的思想是，不要完全让模型来生成句子，而是人给出模板，模型只负责填词。然而完全人工设计模板那就退化到几十年前的方式了，作者的思路是，从已有的gold
summary中自动提取模板。</li>
<li>大体分为三步：
<ul>
<li>Retrieve：从训练语料中检索出候选软模板</li>
<li>Rerank：让seq2seq模型学习到template saliency measurement</li>
<li>Rewrite：让seq2seq模型学习到final summary generation</li>
</ul></li>
<li>这种方法应该比较适用于长句压缩，或者说单句生成式摘要，这样待压缩的长句可以作为query进行retrieve</li>
</ul>
<h2 id="retrieve">Retrieve</h2>
<ul>
<li>使用现成的Lucene搜索引擎，给定要压缩的一个长句作为query，从文档集中搜索出top
30篇文档的summary作为候选模板</li>
</ul>
<h2 id="rerank">Rerank</h2>
<ul>
<li><p>经过搜索搜出来的摘要（soft
template)是按照搜索相关度排序的，但我们需要的是按照摘要相似度排序，因此我们使用ROUGE值衡量soft
template和gold
summary之间的相似程度，这里的rerank并不是要真的排序出来，而是在生成摘要时综合考虑每个template的rank程度，之后在损失部分可以看出来。</p></li>
<li><p>具体而言，先用一个BiLSTM编码器编码输入x和某一个候选模板r，这里是分别编码隐层状态，但是共用编码器，之后将两个隐层状态输入一个Bilinear网络预测出输入x对应的gold
summary y和r之间的ROUGE值，相当于这是一个给定x，给r做出saliency
prediction的网络：</p>
<p><span class="math display">\[
h_x = BiLSTM(x) \\
h_r = BiLSTM(r) \\
ROUGE(r,y) = sigmoid(h_r W_s h_x^T + b_s) \\
\]</span></p></li>
<li><p>这就完成了rerank的监督部分</p></li>
</ul>
<h2 id="rewrite">Rewrite</h2>
<ul>
<li>这部分就是普通的seq2seq，依然是利用之前编码好的<span
class="math inline">\(h_x, h_r\)</span>，将其拼接起来送入一个attentional
RNN decoder生成摘要，计算损失</li>
</ul>
<h2 id="jointly-learning">Jointly Learning</h2>
<ul>
<li>模型的损失分为两部分，Rerank部分要保证编码出来的template和输入再经过bilinear之后能正确预测ROUGE值，Rewrite部分要保证生成正确的摘要，相当于在普通的seq2seq生成摘要之外，我还候选了一些其他的gold
summary作为输入，这个候选是通过retrieve的方式粗筛选的，具体使用时通过Rerank部分保证encode出来的部分是summary里的template成分，即可以拿出来和gold
summary比对的部分，从而辅助rewrite部分decoder的生成。</li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>我们知道做summarization，decoder通过注意力其实是很依赖encoder的输入的，这里的encoder输入既包含template，又包含原始输入，作者给出了几个比较理想的例子，即输出的summary基本上按照template的格式，但是在关键实体部分从原始输入中提取实体填到template
summary当中。</li>
<li>虽然如何提取soft template这方面使用了一个比较玄学的rerank
loss的方式，但是template的作用确实很明显，模型实际上是找到和gold
summary很接近的一个summary作为输入，在此基础上稍加更改(rewrite)，效率远比端到端的seq2seq好，作者还尝试了去掉retrieve，直接从整个语料中找ROUGE最高的summary作为template，最后模型出来的结果高达50的ROUGE-1，48的ROUGE-L</li>
<li>这种找输出作为输入的操作，其实是对decoder抽象能力不足的一种补偿，是对数据集观察得出的经验方法，能很实际的解决问题</li>
</ul>
<h2 id="biset">biset</h2>
<ul>
<li>将rerank部分换成了CNN+GLU来编码文档和查询，编码后的向量计算sim
matrix，</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>summarization</tag>
        <tag>natural language processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Outstanding Papers Reading (ACL 2019)</title>
    <url>/2019/07/28/acl2019/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<p>Selected readings from ACL 2019 award-winning papers.</p>
<ul>
<li>Using Oracle for sentence-level teacher forcing</li>
<li>speaker commitment</li>
<li>A set of evaluation index frameworks applicable to abstracts,
combining multiple indicators</li>
<li>Zero-Shot Entity Linking</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="bridging-the-gap-between-training-and-inference-for-neural-machine-translation">Bridging
the Gap between Training and Inference for Neural Machine
Translation</h1>
<h2 id="background">Background</h2>
<ul>
<li>The best long papers, this direction is very attractive, it is very
common, everyone knows but chooses to ignore, or cannot find an elegant
and effective solution.</li>
<li>Attempts to address all the issues encountered by seq2seq, namely
the inconsistency between training and inference, i.e., exposure
bias.</li>
<li>exposure bias is the bias produced during decoding. Normally, we
generate a sentence from left to right, character by character, how so?
The model generates a character, and then this character is input into
the decoder to decode the next character, that is, the preceding text
used to decode each character is the previously decoded sentence
fragment. However, this training converges very slowly and is prone to
cumulative errors. Think about it, the model is already difficult to
generate the correct character at the beginning, and now it has to
generate the next character based on this incorrect character, which is
adding insult to injury. Therefore, during general training, it is
necessary to use the teacher forcing method: forcing the model to
generate each character based on the correct preceding text in the
training data, that is, regardless of the characters already generated,
only generating the correct character based on the premise of
correctness. However, this technique can only be used for training;
during testing, there is no ground truth for teacher forcing.</li>
<li>This issue is neither particularly large nor particularly small; it
has also been encountered in previous summarization tasks, leading to
good training responses but poor testing performance or inexplicable
biases. Today, seq2seq models have made significant progress in the
encoding end, with feature extractors such as CNN and Transformer having
moved beyond unidirectional extraction methods. However, regardless of
the model, at the decoding end, they must generate from left to right in
a straightforward manner, and exposure bias cannot be avoided.</li>
<li>For translation, exposure bias also packages another issue that
affects the quality of translation: the cross-entropy loss calculated
word by word. The model needs to learn to generate the correct word at
the correct position, and this dual correctness standard is too
stringent for translation, making it difficult for the model to learn
flexible translation relationships, i.e., over correction.</li>
<li>What are the existing methods for solving exposure bias and
word-level CrossEntropy Loss?
<ul>
<li>In generating words, sometimes we use ground truth, sometimes our
own predicted output, and sample a moderate amount, i.e., scheduled
sampling</li>
<li>Using pre-trained models, performing Masked Seq2seq pretraining</li>
<li>Utilizing sentence-level loss functions, the goal is to achieve the
highest score for the entire sentence, rather than greedy optimization
on a word-by-word basis, which includes various optimization criteria
and reinforcement learning methods, such as mixed incremental
cross-entropy reinforcement</li>
<li>Among them, the pre-trained method is a relatively new approach,
while the other two types of methods were proposed as early as 2015, and
the authors also compared their own method with theirs</li>
</ul></li>
</ul>
<h2 id="methods">Methods</h2>
<ul>
<li>This paper aims to address the above two issues, and at first
glance, the approach is still the same: by sampling from the ground
truth and predicted results to mitigate bias, and by using
sentence-level optimization metrics to relax the constraints on
loss.</li>
<li>How to sample specifically? The method provided by the authors is
shown in the figure below (isn't this the figure for scheduled
sampling...): <img data-src="https://s2.ax1x.com/2019/08/05/e26fV1.png"
alt="e26fV1.png" />
<ul>
<li><p>First select the oracle word, i.e., the word predicted by the
model: Note that the word predicted by the model here is not very
accurate, as the predicted word is deterministic, obtained by taking the
maximum of the dictionary probability distribution decoded by the
decoder (excluding beam search). However, the oracle here should be
expressed as "not ground truth," i.e., not the true word. If we directly
use the predicted word, we will make mistakes on top of mistakes; if we
use the ground truth, there will be exposure bias. Therefore, the author
took a compromise, different from the previous probabilistic compromise
(which may take the predicted word or the ground truth), and also
optimized the word selection, not simply taking the predicted word as
the oracle. Specifically:</p>
<ul>
<li><p>If the word with the highest predicted probability by the decoder
is directly taken as the Oracle, that is ordinary scheduled
sampling.</p></li>
<li><p>However, the author adjusts the predictive probability
distribution using the Gumbel-Max regularization method, introducing two
parameters: one calculated from a uniform distribution variable <span
class="math inline">\(u\)</span> as Gumbel noise <span
class="math inline">\(\eta\)</span> ; and one temperature variable <span
class="math inline">\(\gamma\)</span> . Assuming the original
probability distribution is <span class="math inline">\(o\)</span> , the
adjusted probability distribution <span class="math inline">\(P\)</span>
is</p>
<p><span class="math display">\[
\eta = - \log ( - \log u) \\
\overline{o} _{j-1} = (o_{j-1} + \eta) / \gamma \\
\overline{P} _{j-1} = softmax(\overline{o} _{j-1}) \\
y_{j-1}^{\text {oracle
}}=y_{j-1}^{\mathrm{WO}}=\operatorname{argmax}\left(\tilde{P}_{j-1}\right)
\\
\]</span></p></li>
<li><p>The process of adding noise only affects the selection of the
oracle and not the model's loss. The operation of adding Gumbel noise
makes the argmax operation equivalent to a sampling operation based on
the probabilities of softmax, making the probability distribution
obtained by softmax meaningful rather than simply taking the maximum.
Here, only Gumbel-Max is used (the softmax in the formula is actually
not necessary). Another more common application of Gumbel is
Gumbel-Softmax, which is used to achieve reparameterization when the
distribution of the assumed latent variable is a categorical
distribution. Compared to the ordinary softmax, Gumbel-Softmax's effect
is equivalent to calculating a series of samples using softmax, which
are sampled probabilistically according to the original softmax
probabilities.</p></li>
</ul></li>
<li><p>This is a word-level oracle selection, and it can also be done at
the sentence level; the specific method is</p>
<ul>
<li>Firstly, using a word-level method, combined with beam search,
several candidate sentences are selected</li>
<li>Select the best sentence through BLEU, ROUGE, and other metrics, and
take each word of this sentence as an oracle</li>
<li>There is an obvious issue here, which is to ensure that the oracle
sentences generated by beam search are of the same length as the ground
truth sentences. The authors introduce force decoding, where if the
decoded sentence is still shorter than the ground truth length and an
EOS is decoded, the EOS is excluded, and the beam search is performed on
the top k words with the highest probabilities; if the length is already
sufficient but EOS has not been decoded, the decoding is forced to EOS
and terminated</li>
</ul></li>
<li><p>Re-calculate the probability to decide whether to use oracle or
ground truth: Like scheduled sampling, it also involves setting a
dynamic sampling probability. Initially, during training, more ground
truth is used, and then the proportion of oracle is gradually increased.
The probability setting given by the authors is:</p>
<p><span class="math display">\[
p = \frac{\mu}{\mu + exp(e / \mu)}
\]</span></p></li>
</ul></li>
<li>The results are undoubtedly better than those of naive RNN and
Transformer, with a 2-point improvement in BLEU. The authors also
conducted a large number of experiments to test the impact of
hyperparameters. It's simple and effective, especially the method of
introducing sentence-level optimization is straightforward, much more
intuitive than a bunch of changes to the objective functions.</li>
</ul>
<h1
id="do-you-know-that-florence-is-packed-with-visitors-evaluating-state-of-the-art-models-of-speaker-commitment">Do
you know that Florence is packed with visitors? Evaluating
state-of-the-art models of speaker commitment</h1>
<ul>
<li>Best short paper, studying a very interesting direction: speaker
commitment, also known as event fact.</li>
<li>Speaker's commitment refers to determining whether an event has
occurred through the speaker's description, specifically divided into
three categories: factual, unfactual, and uncertain. The model needs to
extract the factual status of the event from the speaker's description.
Traditional methods focus on modal verbs and verb phrases, but the
author introduces the CommitmentBank dataset to test various existing
models, indicating that existing datasets cannot capture the lexical and
syntactic diversity of natural language, especially in spoken language,
and finds that models incorporating linguistic knowledge are superior to
LSTM, setting another goal for deep learning to conquer.</li>
<li>For example, to illustrate the issue of speaker commitment, consider
the following two statements: "I never believed I would study NLP," and
"I do not believe I can study NLP." Both sentences have "believe" as the
verb and both contain the negative words "never" and "not." The event in
question is "I study NLP," and whether this event has occurred. Clearly,
the former suggests that the event has already happened, while the
latter suggests that it has not yet occurred. There are also more
complex scenarios, such as given the statements of two debaters,
guessing whether a certain fact discussed by them has occurred.
Generally, each sample would also have context, and the speaker
commitment task is to provide context, speaker expression, and an event,
and to judge whether the event is a fact.</li>
<li>Authors tested two models on the CommitmentBank dataset: rule-based
and neural network-based
<ul>
<li>Rule-Based: Integrating Deep Linguistic Features in Factuality
Prediction over Unified Datasets. Linguistic knowledge is applied by
manually assigning factual scores to various predicate words/phrases,
identifying the hidden signature of the predicate, and connecting
adjectives and modal verbs based on syntactic tree analysis to enhance
or reverse the scores. Finally, the scores from various human knowledge
bases and syntactic structures are input as features into the SVM
regression model to calculate the scores.</li>
<li>Based on Neural Networks: Neural Models of Factuality. Sentence
modeling is performed using multi-layer bidirectional LSTM and
tree-LSTM, followed by a multi-layer MLP to calculate regression scores.
The authors tested three models: bidirectional, tree, and ensemble.</li>
</ul></li>
<li>The main part of the article is in the results analysis, with rich
data presentation. However, the authors do not provide excessive cause
analysis; they merely state which types of facts, states, corpora, and
modalities result in better performance for which types of models.
Perhaps, since I do not work in this field, I do not feel that there are
any research points that can be 挖掘 from these conclusions. In the end,
the overall conclusion is drawn that human knowledge has stronger
generalization ability, and deep models need to integrate human
knowledge; the conclusion is somewhat broad.</li>
<li>This paper won an award, indicating that the academic community
still values diversity in NLP research. Challenging tasks like this one
are not undertaken by many, but once completed, they can greatly enhance
downstream tasks such as information extraction and dialogue.</li>
</ul>
<h1 id="a-simple-theoretical-model-of-importance-for-summarization">A
Simple Theoretical Model of Importance for Summarization</h1>
<ul>
<li>One of the outstanding papers, simply because I also do
summarization, I picked it out to read. The author presents a simple
theoretical model for the quantitative analysis of the importance of
abstracts, which had no direct, explicit definition before. The author
integrates semantic knowledge into the concept of information entropy,
proposes semantic units, and generalizes the three major concepts that
have always been used in summarization: redundancy, relevance, and
informativeness (Redundancy, Relevance, and Informativeness), unifying
these three concepts under the category of importance. The author also
points out that the importance indicator highly aligns with human
judgments, unlike previous automatic measurement indicators that are
difficult to ensure the quality of abstracts.</li>
<li>Firstly, it must be said that the relevant work in the paper is very
thorough, extending from the 1950s to the present, weaving together
several threads, and the reference list is well worth reading.</li>
</ul>
<h2 id="definition">Definition</h2>
<ul>
<li><p>Semantic unit: the atomic unit of information, the set of
semantic units is denoted as <span class="math inline">\(\Omega\)</span>
, a document can be expressed as a probability distribution over the set
of semantic units. Semantic units are applicable to many frameworks,
such as frames, for example, topic models, and for example, embeddings
commonly used in deep learning. All semantic units share a unified
feature: they are discrete and independent, and the meaning of language
is based on these semantic units. We mark documents and abstracts as
<span class="math inline">\(D\)</span> and <span
class="math inline">\(S\)</span> , respectively, and the corresponding
probability distributions over the semantic units are denoted as <span
class="math inline">\(P_D, P_S\)</span> .</p></li>
<li><p>entropy: Entropy can be calculated with the concept distribution:
<span class="math inline">\(H = - \sum _{w} P(w) \log
(P(w))\)</span></p></li>
<li><p>Redundancy (Redundancy): Redundancy is defined as the difference
between maximum entropy and entropy:</p>
<p><span class="math display">\[
Red(S) = H_{max} - H(S)
\]</span></p>
<p>The maximum entropy is achieved at the uniform distribution. In fact,
it is the conversion of the entropy, which measures uncertainty, into
the redundancy, which measures determinacy. The abstract should have low
redundancy, i.e., a small entropy, otherwise, the information obtained
in the document collection is largely repetitive, and does not lead to a
reduction in the abstract entropy. Since the maximum entropy is fixed
for a given corpus, it can be abbreviated as <span
class="math inline">\(Red(S) = -H(S)\)</span></p></li>
<li><p>Relevance: The author defines relevance as follows: When we
observe an abstract to infer the information of the original text, the
difference (loss) from the true information of the original text should
be minimized. Therefore, we define relevance as the opposite of this
loss. The simplest definition of loss is the cross-entropy between the
semantic unit distributions of the document and the abstract:</p>
<p><span class="math display">\[
Rel(S,D) = - CrossEntrophy(S,D) \\
= \sum _{w_i} P_S(w_i) \log (P_D(w_i)) \\
\]</span></p>
<p>At the same time, we note that:</p>
<p><span class="math display">\[
KL(S||D) = Red(S) - Rel(S,D)
\]</span></p></li>
<li><p>Low redundancy and high relevance abstracts result in the minimum
KL divergence between the abstract and the original text.</p></li>
<li><p>Informativeness: We define the informativeness of an abstract as
the ability to alter one's common sense or knowledge. The author
introduces background knowledge <span class="math inline">\(K\)</span>
and its probability distribution <span
class="math inline">\(P_K\)</span> , and defines informativeness as</p>
<p><span class="math display">\[
Inf(S,K) = CrossEntrophy(S,K)
\]</span></p></li>
<li><p>High informativeness should be able to bring information that is
not present in the background knowledge. Next is how to define
background knowledge:</p>
<ul>
<li>Background knowledge should allocate known semantic units with a
high probability, representing that these semantic units have a high
intensity in the user's memory</li>
<li>Generally speaking, background knowledge can be set to none, i.e.,
uniformly distributed, but background knowledge provides Summarization
with a controllable choice, that is, users can specify queries
indicating the semantic units they are interested in, and then the
background knowledge should assign low probabilities to these semantic
units.</li>
<li>In multi-document summarization, background knowledge can be
simplified to documents that have already generated summaries</li>
</ul></li>
<li><p>Next, we can define importance to integrate the above three
indicators: Importance should measure the importance of semantic units;
we want to retain only relatively important semantic units in the
abstract, which means we need to find a probability distribution that
unifies the document and background knowledge, and encode the expected
semantic units that need to be retained in the abstract</p></li>
</ul>
<h2 id="importance">Importance</h2>
<ul>
<li><p>Should be able to extract the useful parts from the information
in document <span class="math inline">\(D\)</span> for users with
background knowledge <span class="math inline">\(K\)</span> , we
define</p>
<ul>
<li>Semantic unit <span class="math inline">\(d_i = P_D(w_i)\)</span>
probability in the document <span
class="math inline">\(w_i\)</span></li>
<li>Semantic unit <span class="math inline">\(k_i = P_K(w_i)\)</span>
probability in background knowledge <span
class="math inline">\(w_i\)</span></li>
<li>Function for encoding the importance of semantic units, which should
satisfy:
<ul>
<li>Informational: <span class="math inline">\(\forall i \not= j \
\text{if} \ d_i=d_j \ \text{and} \ k_i &gt; k_j \ \text{then} \
f(d_i,k_i) &lt; f(d_j,k_j)\)</span></li>
<li>Relevance: <span class="math inline">\(\forall i \not= j \ \text{if}
\ d_i&gt;d_j \ \text{and} \ k_i = k_j \ \text{then} \ f(d_i,k_i) &gt;
f(d_j,k_j)\)</span></li>
<li>Additivity: <span class="math inline">\(I(f(d_i,k_i)) \equiv \alpha
I(d_i) + \beta I(k_i)\)</span></li>
<li>Normality: <span class="math inline">\(\sum _i f(d_i,k_i) =
1\)</span></li>
</ul></li>
<li>The formulaic expression of the four properties is simple and easy
to understand, where <span class="math inline">\(I\)</span> represents
self-information. The first two properties describe semantic units that
we want to be related to the document and that can bring new knowledge.
Additivity ensures consistency with the definition of self-information,
while normalization guarantees that this function is a probability
distribution.</li>
</ul></li>
<li><p>The importance coding function that satisfies the above
properties is:</p>
<p><span class="math display">\[
P_{\frac DK}(w_i) = \frac 1C \frac {d_i^{\alpha}}{k_i^{\beta}} \\
C = \sum _i \frac {d_i^{\alpha}}{k_i^{\beta}}, \alpha, \beta \in
\mathbb{R} ^{+} \\
\]</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> represent the intensity of
relevance and informativeness</p></li>
<li><p>Based on the definition of importance, we can identify the
criteria that the best abstract should meet:</p>
<p><span class="math display">\[
S^* = \text{argmax}_S \theta _I = \text{argmin} _S KL(S || P_{\frac DK})
\]</span></p></li>
<li><p>Therefore, we take <span class="math inline">\(\theta _I\)</span>
as a measure of the quality of abstracts:</p>
<p><span class="math display">\[
\theta _I (S,D,K) = -KL(P_S||P_{\frac DK})
\]</span></p></li>
<li><p>Entropy of importance probability can measure the number of
potential good abstract candidates</p></li>
<li><p>The measurement indicator <span class="math inline">\(\theta
_I\)</span> can actually be divided into the three indicators mentioned
earlier:</p>
<p><span class="math display">\[
\theta _I (S,D,K) \equiv -Red(S) + \alpha Rel(S,D) + \beta Inf(S,K)
\]</span></p></li>
</ul>
<h2 id="results">Results</h2>
<ul>
<li>Authors use the simplest words as semantic units, employ word
frequency normalization as a probability distribution, and set both
hyperparameters <span class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> to 1. For incremental
summarization, the background knowledge is the document that has already
been summarized, while for general summarization, the background
knowledge is set to none, i.e., uniformly distributed</li>
<li>The results show that the importance measurement indicators are
closer to human judgments than traditional indicators and are more
discriminative.</li>
<li>The author of this paper proposes only a framework, with background
knowledge and the definition of semantic units being flexible according
to the task and model. The evaluation issue of abstracts has always
lacked good indicators, and this paper also tackles this tough problem,
offering a simple and effective method.</li>
</ul>
<h1
id="zero-shot-entity-linking-by-reading-entity-descriptions">Zero-Shot
Entity Linking by Reading Entity Descriptions</h1>
<h2 id="task-description">Task Description</h2>
<ul>
<li>Outstanding paper, which investigates zero-shot learning in entity
linking and proposes a domain-adaptive pre-training strategy to address
the domain bias problem existing when linking unknown entities in new
domains.</li>
<li>Entity linking task refers to given a query containing the entities
to be linked, as well as a series of candidate entity descriptions, the
model needs to establish correct entity linking and eliminate
ambiguity.</li>
<li>The author provided an interesting example, in the game The Elder
Scrolls, the description of the query is "The Burden spell is the
opposite of Feather, increasing a character's encumbrance......Clearly,
here the 'Burden' describes the name of the spell. There are Burdens in
the candidate entities as spell names, as spell effects, and of course,
as other interpretations in the conventional dictionary. The model needs
to link the 'Burden' in the query to the 'Burden' as a spell name. For
such specific noun tasks, it is relatively simple; the difficulty lies
in linking various pronouns, such as 'he' and 'this person', to specific
individuals. Entity linking tasks are closely related to reading
comprehension tasks.</li>
<li>Zeroth-order learning refers to the scenario where the training set
is only trained on the domain dataset of The Elder Scrolls games, yet it
is required to correctly predict test sets from other domains, such as
the Lego game dataset and the Coronation Street TV series dataset.</li>
<li>This requires the model to achieve natural language understanding
rather than simple domain-specific pattern memorization.</li>
<li>In the zero-shot learning entity linking task, there is no alias
table or frequency prior to refer to; the model needs to read the
description of each candidate entity and establish a correspondence with
the context.</li>
<li>General entity linking tasks involve the following assumptions:
<ul>
<li>Single entity set: Training and testing are performed on the same
entity set</li>
<li>Alias Table: For each query entity, there is a candidate entity
table, or what is referred to as the alias table of the query entity,
which does not require manual search</li>
<li>Frequency statistical information: Information obtained from the
statistics of a large annotated corpus, which can be used to estimate
the popularity of entities and the probability of a text linking to an
entity, can serve as an important prior knowledge supplement to the
model</li>
<li>Structured Data: Some systems provide relational tuples to assist
models in disambiguation</li>
</ul></li>
<li>However, zero-shot learning abandons all the above assumptions,
assuming only the existence of an entity dictionary, that is, all
entities have at least a corresponding description, reducing the
anthropomorphic assumptions in the entity linking task to the minimum,
which can be said to be the most difficult and extreme case. The task is
obviously divided into two parts:
<ul>
<li>For each query entity, find the candidate linked entity set</li>
<li>Rank the candidate link entity set</li>
</ul></li>
</ul>
<h2 id="two-step-approach">Two-step approach</h2>
<ul>
<li>Candidate set generation adopted a simple and quick approach: all
candidates were found using an information retrieval method. The authors
used BM25 to measure the similarity between queries and documents,
identifying the top-64 most similar documents as the candidate set.</li>
<li>The subsequent ranking task is similar to reading comprehension or
natural language inference, and the authors used a transformer-based
model as a strong baseline.
<ul>
<li>Formal definition should be called Mention rather than query,
referring to the context where the entity to be linked exists, denoted
as <span class="math inline">\(m\)</span> ; while the description of the
candidate entities is denoted as <span
class="math inline">\(e\)</span></li>
<li>Input <span class="math inline">\(m\)</span> and <span
class="math inline">\(e\)</span> as sentence pairs to the BERT model,
<span class="math inline">\(m\)</span> also adds additional embeddings
to distinguish from <span class="math inline">\(e\)</span></li>
<li>BERT encodes the sentence pairs, then computes the dot product
between the encoded vectors and the word vectors of the entities to
obtain scores</li>
<li>To demonstrate the importance of the self-attention in the joint
training of <span class="math inline">\(m\)</span> and <span
class="math inline">\(e\)</span> , the authors also conducted two
comparative naive BERT models with controlled variables, but that is not
worth mentioning here, as the importance of self-attention is already a
common knowledge and does not require further emphasis.</li>
</ul></li>
<li>The above baseline is actually quite strong, because after
pre-training, BERT has gained some ability for domain transfer, as can
be seen from the results. The average accuracy of pre-trained and
non-pretrained BERT differs by a factor of three, and the difference
between using src, tgt, or both for pre-trained BERT is not significant,
but it is much higher than traditional methods.</li>
</ul>
<h2 id="zeroth-order-learning">Zeroth-order learning</h2>
<ul>
<li>Next is the author's proposed zero-shot learning method, which
mainly still utilizes pre-training; there are two types of traditional
pre-training transfer:
<ul>
<li>Task-adaptive pretraining: Pretrain on unsupervised corpus of src
and tgt, and fine-tune on supervised corpus of src</li>
<li>Open Corpus Pretraining: This is like BERT, which pretrains on a
large-scale unsupervised corpus regardless of src and tgt, and then
fine-tunes on the supervised corpus of src</li>
</ul></li>
<li>Authors propose domain adaptation: that is, to insert a pre-training
process that is only on the tgt corpus after the above two
pre-trainings, for the reason that the expression capacity of the model
is limited, and the representation in the tgt domain should be optimized
first</li>
</ul>
<h2 id="results-1">Results</h2>
<ul>
<li>The results are, of course, that the field adaptation effect
proposed by the author is somewhat better, but the difference is not
significant, at most 1 to 2 points, and the method proposed is not
particularly new; it merely adds an additional pre-training process by
changing the corpus. The entire paper seems to have been infused with a
new field using BERT, just like the training guide for pre-trained
models. Perhaps the key contribution is also the proposal of a dataset
for a zero-shot learning entity linking task.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="bridging-the-gap-between-training-and-inference-for-neural-machine-translation">Bridging
the Gap between Training and Inference for Neural Machine
Translation</h1>
<h2 id="background">Background</h2>
<ul>
<li>最佳长论文，这个方向就很吸引人，属于很常见，大家都知道但都选择无视，或者找不出优雅有效解法的问题。</li>
<li>本文试图解决所有seq2seq都会遇到的问题，训练与推理的不一致，即exposure
bias。</li>
<li>exposure
bias是解码时产生的偏差。正常来讲，我们生成一句话，是从左往右逐字生成，怎么个逐字？模型生成一个字，然后这个字接着输入解码器解码出下一个字，也就是解码出每一个字时使用的上文是之前解码出的句子片段。但是这样训练收敛很慢，容易导致错误的累积。想想模型一开始本来就难以生成正确的字，现在还要基于这个错误的字生成接下来的字，那就是错上加错了。因此一般训练时，都需要使用teacher
forcing的方法：forcing模型在生成每一个字的时候，依靠的是训练数据中正确的上文，也就是不管已经生成的字，只管前提正确的情况下去生成正确的字。但是这种技巧只能用于训练，测试的时候没有ground
truth来teacher forcing。</li>
<li>这个问题说大不大，说小不小，之前做summarization也会遇到这个问题，导致训练的反应很好，但是测试效果差，或者出现难以解释的偏差。如今的seq2seq在编码端已经取得了长足的进步，CNN和Transformer等特征抽取器已经摆脱了单向的抽取方式，但是无论什么模型，在解码端，都得老老实实从左往右生成，都避免不了exposure
bias。</li>
<li>对于翻译，exposure
bias还和另一个问题打包影响了翻译的质量：逐字计算的交叉熵损失。模型需要学习到在正确的位置生成正确的词，这个双重正确的标准对于翻译来说太过苛刻，模型难以学到灵活的翻译关系，也就是over
correction.</li>
<li>现有的解决exposure bias以及word-level CrossEntrophy
Loss的方法有哪些？
<ul>
<li>在生成词的时候，有时用ground
truth，有时用自己的预测的输出，采样中庸一下，即scheduled sampling</li>
<li>使用预训练模型，做Masked Seq2seq pretraining</li>
<li>使用句子级别的损失函数，目标是整个句子的分数最高，而不是逐字贪心，这里包括了各种各样的优化指标以及强化学习的方法，例如mixed
incremental cross-entrophy reinforce</li>
<li>其中预训练是比较新的方法，其余两类方法早在2015年就已经提出，作者也把自己的方法与他们的方法做了对比</li>
</ul></li>
</ul>
<h2 id="methods">Methods</h2>
<ul>
<li>本文想要解决以上两个问题，粗看思路还是和以前一样：通过从ground truth
和 predicted
results中采样来中和偏差，以及使用句子级别的优化指标来放宽损失的约束。</li>
<li>具体怎么采样？作者给出的方法如下图（这不就是scheduled
sampling的图吗。。。。）： <img data-src="https://s2.ax1x.com/2019/08/05/e26fV1.png" alt="e26fV1.png" />
<ul>
<li><p>先选出oracle
word，即模型预测的词：注意，这里用模型预测的词其实不太准确，因为模型预测的词是确定的，是decoder解码出词典概率分布取最大得到的（不考虑束搜索的话），然而这里的oracle应该表述为not
ground
truth，即非真实词。假如我们直接用预测的词，那就会错上加错；假如我们用ground
truth，那就会有exposure
bias。因此作者取了个折中，不同于之前概率上的折中（可能取预测词可能取ground
truth），还做了选词上的优化，不是简单的拿预测出的词作为oracle，具体而言：</p>
<ul>
<li><p>假如直接取decoder预测概率最大的词作为Oracle,那就是普通的scheduled
sampling。</p></li>
<li><p>然而作者使用Gumbel-Max正则化方法对预测概率分布调整，引入两个参数：一个由01均匀分布变量<span
class="math inline">\(u\)</span>计算得来的Gumbel noise <span
class="math inline">\(\eta\)</span>；以及一个温度变量<span
class="math inline">\(\gamma\)</span>。假设原始概率分布为<span
class="math inline">\(o\)</span>，则调整后的概率分布<span
class="math inline">\(P\)</span>为</p>
<p><span class="math display">\[
\eta = - \log ( - \log u) \\
\overline{o} _{j-1} = (o_{j-1} + \eta) / \gamma \\
\overline{P} _{j-1} = softmax(\overline{o} _{j-1}) \\
y_{j-1}^{\text {oracle
}}=y_{j-1}^{\mathrm{WO}}=\operatorname{argmax}\left(\tilde{P}_{j-1}\right)
\\
\]</span></p></li>
<li><p>这个加入噪音的过程只影响选择
oracle，而不影响模型的损失。增加Gumbel
noise的操作可以使得argmax操作等效于依据softmax的概率进行采样操作，使得softmax得到的概率分布有意义，而不是单纯取最大。这里只是用了Gumbel-Max（式子里那个softmax其实不需要）。Gumbel的另一个更为常见的应用是Gumbel-Softmax，用于在假设隐变量分布为category
distribution时实现重参数化(reparameterization),相比普通的softmax，Gumbel-Softmax的效果等价于用softmax计算出了一系列样本，这些样本是按照原始softmax概率依概率采样得到。</p></li>
</ul></li>
<li><p>这是单词级别的oracle选择，还可以做句子级别的选择，具体做法是</p>
<ul>
<li>先用单词级别的方法，加上beam search，选出几个候选句</li>
<li>通过BLEU，ROUGE等指标选出最好的句子，将这个句子的每一个词作为oracle</li>
<li>显然这里有一个问题，就是得保证beam search出的oracle句子和ground
truth的句子长度一致，作者引入了force
decoding，当解码出的句子还不够ground
truth长度时，假如解码出了EOS，就排除EOS，取剩下的概率最大前k个单词做beam
search；假如长度已经够了，但是还没解码出EOS，就强制设置为EOS并结束解码</li>
</ul></li>
<li><p>再计算概率，决定是用oracle还是ground truth：和scheduled
sampling一样，也是要设置动态采样概率，刚开始训练的时候多用ground
truth，然后慢慢提高oracle的比例，作者给出的概率设置为：</p>
<p><span class="math display">\[
p = \frac{\mu}{\mu + exp(e / \mu)}
\]</span></p></li>
</ul></li>
<li>结果当然是比naive RNN and
Transformer要好，BLEU能有2个点的提升。作者也做了大量实验来测试超参数的影响。很简单很work，尤其是引入句子层级优化的方法简单明了，比一堆目标函数的改动要直观的多。</li>
</ul>
<h1
id="do-you-know-that-florence-is-packed-with-visitors-evaluating-state-of-the-art-models-of-speaker-commitment">Do
you know that Florence is packed with visitors? Evaluating
state-of-the-art models of speaker commitment</h1>
<ul>
<li>最佳短论文，研究了一个非常有意思的方向：speaker
commitment，叫说话人承诺，或者叫事件事实。</li>
<li>说话人承诺是指，通过说话人的描述，来判断某一事件是否发生，具体而言分三类：事实、非事实、不确定。模型需要从说话人的描述当中挖掘出事件的事实状态。传统的方法关注情态动词、动词短语，但作者引入了CommitmentBank数据集来测试各种已有模型，说明已有的数据集不能捕捉自然语言，尤其是口语当中的词法和句法多样性，且发现引入语言学知识的模型要优于LSTM，为深度学习树立了另一个有待攻克的目标。</li>
<li>举个例子来形象说明一下说话人承诺问题，“我从没相信我会研究NLP”，“我不相信我可以研究NLP”，两句话都有“相信”作为动词，且都具有否定词“从没”、“不”，那么事件是“我研究NLP”，这个事件究竟有没有发生？显然前者倾向于事件已经发生，而后者倾向于事件还未发生。还有更复杂的情形，例如给定辩论双方的陈述，猜测双方讨论的某一事实是否发生。一般而言每一条样本还会有上下文，说话人承诺任务就是给定上下文、说话人表述和事件，判断事件是否是事实。</li>
<li>作者在CommitmentBank数据集上测试了两个模型：基于规则的和基于神经网络的
<ul>
<li>基于规则：Integrating Deep Linguistic Features in Factuality
Prediction over Unified Datasets。
基于语言学的知识即人为给各种谓语词语/短语打上事实分数，找到谓语的隐藏签名，并根据句法树剖析来联系上形容词和情态动词，进行分数的增强或者反转，最后将各种人类知识库得分和句法结构作为特征输入SVM回归模型，计算出分数</li>
<li>基于神经网络：Neural models of
factuality。使用多层双向LSTM和tree-LSTM对句子建模，然后过一个多层MLP计算出回归分数。作者测试了双向、树、集成三种模型。</li>
</ul></li>
<li>文章的主要部分在结果分析，数据展示很丰富，但是作者也没有给出过多的原因分析，只是在陈述哪类事实、哪类状态、哪类语料、哪类情态下哪类模型表现更好。可能是我不做这方面工作，没有感受到从这些结论里能有哪些可以挖掘的研究点。最后得出总的结论，人类知识具有更强的泛化能力，深度模型需要整合人类知识，结论有点宽泛。</li>
<li>这篇论文得了奖，表明学界还是希望NLP研究具有多样性，像这样具有挑战性的任务并不会有太多人做，但做好之后能给下游任务例如信息抽取、对话以极大的提升。</li>
</ul>
<h1 id="a-simple-theoretical-model-of-importance-for-summarization">A
Simple Theoretical Model of Importance for Summarization</h1>
<ul>
<li>杰出论文之一，单纯是因为我也做summarization才拎出来看。作者给出了一种简单的理论模型来定量分析文摘的重要性，在此之前重要性都没有直接的、显示的定义出来。作者将语义知识融入信息熵的概念，提出了语义单元，并泛化了之前summarization一直用的三大概念：冗余度、相关性和信息性（Redundancy,
Relevance and
Informativeness），将这三个概念统一于重要性之下，作者还指出重要性指标与人类直接高度吻合，而不像以前的自动衡量指标一样难以保证文摘质量，</li>
<li>首先得说论文的相关工作做的很足，从上世纪50年代一直做到现在，串起了几条线，参考文献列表都值得一读。</li>
</ul>
<h2 id="定义">定义</h2>
<ul>
<li><p>语义单元：信息的原子单位，语义单元的集合记为<span
class="math inline">\(\Omega\)</span>，一篇文档可以表述为在语义单元集合上的概率分布。语义单元适用于许多框架，例如frame，例如主题模型，例如深度学习常用的embedding。所有的语义单元形式都具有统一的一个特征：他们离散且独立，语言的意义基于这些语义单元产生。我们把文档和文摘标记为<span
class="math inline">\(D\)</span>和<span
class="math inline">\(S\)</span>，对应的在语义单元上的概率分布是<span
class="math inline">\(P_D, P_S\)</span>。</p></li>
<li><p>熵：有了概念分布就可以计算熵:<span class="math inline">\(H = -
\sum _{w} P(w) \log (P(w))\)</span></p></li>
<li><p>冗余度（Redundancy）：冗余度定义为最大熵与熵之差：</p>
<p><span class="math display">\[
Red(S) = H_{max} - H(S)
\]</span></p>
<p>最大熵在均匀分布取到。实际上就是将衡量不确定性的熵转成衡量确定性的冗余度。文摘应该具有低冗余度，即熵小，否则获取的信息在文档集中大量重复，并不能带来文摘熵的减少。由于对于给定语料，最大熵是固定的，因此可以简写为<span
class="math inline">\(Red(S) = -H(S)\)</span></p></li>
<li><p>相关性（Relevance）：作者如此定义相关：当我们观察文摘来推断原文的信息时，与原文真实的信息之差（损失）应该最小。既然如此我们就用这个损失的相反数来定义相关性。损失的最简单定义就是文档和文摘的语义单元分布之间的交叉熵：</p>
<p><span class="math display">\[
Rel(S,D) = - CrossEntrophy(S,D) \\
= \sum _{w_i} P_S(w_i) \log (P_D(w_i)) \\
\]</span></p>
<p>同时我们注意到：</p>
<p><span class="math display">\[
KL(S||D) = Red(S) - Rel(S,D)
\]</span></p></li>
<li><p>低冗余，高相关的文摘，所带来的文摘与原文之间的KL散度最小。</p></li>
<li><p>信息性（Informativeness）：我们定义文摘的信息性为，能够改变人的常识或者知识。作者引入了背景知识<span
class="math inline">\(K\)</span>以及其概率分布<span
class="math inline">\(P_K\)</span>，并定义信息性为</p>
<p><span class="math display">\[
Inf(S,K) = CrossEntrophy(S,K)
\]</span></p></li>
<li><p>即高信息性应该能够带来背景知识里没有的信息。接下来就是如何定义背景知识：</p>
<ul>
<li>背景知识应该分配已知的语义单元以高概率，代表这些语义单元在用户记忆中强度很高</li>
<li>一般来讲背景知识可以设为无，即均匀分布，但是背景知识给了Summarization一种可控的选择，即用户可以给出查询表明他们感兴趣的语义单元，那么背景知识就应该给这些语义单元低概率。</li>
<li>在多文档摘要中，背景知识可以简化为已经生成摘要的文档</li>
</ul></li>
<li><p>接下来就可以定义重要性来整合以上三种指标：<strong>重要性应该是衡量语义单元的重要性，我们想在文摘中只保留相对重要的语义单元，这意味着我们需要找一个概率分布统一文档和背景知识，编码需要保留在文摘中的语义单元的期望</strong></p></li>
</ul>
<h2 id="重要性">重要性</h2>
<ul>
<li><p>摘要<span class="math inline">\(S\)</span>应该能够从文档<span
class="math inline">\(D\)</span>的信息里提取出对拥有背景知识<span
class="math inline">\(K\)</span>的用户有用的部分，我们定义</p>
<ul>
<li><span class="math inline">\(d_i = P_D(w_i)\)</span>：语义单元<span
class="math inline">\(w_i\)</span>在文档中的概率</li>
<li><span class="math inline">\(k_i = P_K(w_i)\)</span>：语义单元<span
class="math inline">\(w_i\)</span>在背景知识中的概率</li>
<li><span
class="math inline">\(f(d_i,k_i)\)</span>：编码语义单元重要性的函数，这个函数应该满足：
<ul>
<li>信息性：<span class="math inline">\(\forall i \not= j \ \text{if} \
d_i=d_j \ \text{and} \ k_i &gt; k_j \ \text{then} \ f(d_i,k_i) &lt;
f(d_j,k_j)\)</span></li>
<li>相关性：<span class="math inline">\(\forall i \not= j \ \text{if} \
d_i&gt;d_j \ \text{and} \ k_i = k_j \ \text{then} \ f(d_i,k_i) &gt;
f(d_j,k_j)\)</span></li>
<li>可加性：<span class="math inline">\(I(f(d_i,k_i)) \equiv \alpha
I(d_i) + \beta I(k_i)\)</span></li>
<li>归一性：<span class="math inline">\(\sum _i f(d_i,k_i) =
1\)</span></li>
</ul></li>
<li>四条性质的公式表述很简单易懂，其中<span
class="math inline">\(I\)</span>是自信息。前两条说明我们想要与文档相关的，且能带来新知识的语义单元。可加性保证了与自信息定义的一致性，归一性保证这个函数是一个概率分布</li>
</ul></li>
<li><p>满足以上性质的重要性编码函数为：</p>
<p><span class="math display">\[
P_{\frac DK}(w_i) = \frac 1C \frac {d_i^{\alpha}}{k_i^{\beta}} \\
C = \sum _i \frac {d_i^{\alpha}}{k_i^{\beta}}, \alpha, \beta \in
\mathbb{R} ^{+} \\
\]</span></p></li>
<li><p>其中<span class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>代表了相关性和信息性的强度</p></li>
<li><p>基于重要性的定义，我们可以找出最好的文摘应该满足：</p>
<p><span class="math display">\[
S^* = \text{argmax}_S \theta _I = \text{argmin} _S KL(S || P_{\frac DK})
\]</span></p></li>
<li><p>因此我们取<span class="math inline">\(\theta
_I\)</span>作为衡量文摘质量的指标：</p>
<p><span class="math display">\[
\theta _I (S,D,K) = -KL(P_S||P_{\frac DK})
\]</span></p></li>
<li><p>重要性概率的熵可以衡量可能的好文摘候选数量</p></li>
<li><p>衡量指标<span class="math inline">\(\theta
_I\)</span>其实可以拆分为之前提到的三个指标：</p>
<p><span class="math display">\[
\theta _I (S,D,K) \equiv -Red(S) + \alpha Rel(S,D) + \beta Inf(S,K)
\]</span></p></li>
</ul>
<h2 id="结果">结果</h2>
<ul>
<li>作者用最简单的词作为语义单元，用词频归一化作为概率分布，两个超参数<span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>均设置为1，对于增量摘要，背景知识是已经生成摘要的文档，对于普通摘要，背景知识设置为无，即均匀分布</li>
<li>结果发现重要性衡量指标比传统指标更贴近人类的判断，且更具有区分性。</li>
<li>本文作者提出的只是一个框架，背景知识、语义单元的定义可根据任务、模型灵活定义。文摘的评价问题一直缺乏好的指标，本文也算是啃了这个硬骨头，而且给出的方法简单有效。</li>
</ul>
<h1
id="zero-shot-entity-linking-by-reading-entity-descriptions">Zero-Shot
Entity Linking by Reading Entity Descriptions</h1>
<h2 id="任务描述">任务描述</h2>
<ul>
<li>杰出论文，研究了实体链接的零次学习，提出了领域自适应预训练策略来解决链接新领域未知实体时存在的领域偏差问题。</li>
<li>实体链接任务即给定一个query，其中包含待链接的实体，以及一系列候选实体描述，模型需要建立正确的实体链接，消除歧义。</li>
<li>作者给出了个很有趣的例子，在上古卷轴游戏里，query的描述时The
<strong>Burden</strong> spell is the opposite of Feather , increasing a
character ' s
encumbrance.......显然这里的Burden描述的是法术的名字，待候选实体里有作为法术名字描述的Burden，也有作为法术效果描述的Burden，当然还有Burden在常规词典里的其他几种解释，模型需要将query里的Burden链接到作为法术名字描述的Burden上。对于这种具体名词任务还相对简单，困难的是将各种代词，例如“他”，“这个人”与具体的人物链接起来。实体链接任务与阅读理解任务紧密关联。</li>
<li>零次学习即，训练集只在上古卷轴游戏的领域数据集上训练，但是要正确预测其他领域的测试集，例如乐高游戏数据集、冠冕街电视剧数据集。</li>
<li>这就需要模型做到自然语言理解，而不是简单的领域内模式记忆。</li>
<li>在零次学习实体链接任务中，没有别名表或者频率先验可以参考，模型需要阅读每一个候选实体的描述并建立与上下文的对应关系。</li>
<li>一般的实体链接任务包含以下假设：
<ul>
<li>单一实体集：训练和测试是在同一实体集合上做的</li>
<li>别名表：对于每一个query实体，有一个候选实体表，或者叫query实体的别名表，不需要自己找</li>
<li>频率统计信息：从大型标注语料中统计得到的信息，可以用来估计实体的popularity和一段文本链接到实体的概率，可以作为很重要的先验知识补充给模型</li>
<li>结构化数据：一些系统提供了关系元组来帮助模型消歧</li>
</ul></li>
<li>然而零次学习抛弃了以上所有假设，只假定存在实体词典，即所有的实体好歹有一个对应的描述，将实体链接任务的人为假设降到最低，可以说是最难最极限的情况了。接下来任务显然就分成了两部分：
<ul>
<li>对于每个query实体，找出其候选链接实体集合</li>
<li>将候选链接实体集合进行ranking</li>
</ul></li>
</ul>
<h2 id="两步走">两步走</h2>
<ul>
<li>候选集生成采取了简单快速的方式：用信息检索的方式找出所有candidate。作者使用BM25来衡量query和document之间的相似度，找出来最相似的top-64篇文档作为候选集</li>
<li>接下来的ranking任务类似于阅读理解或者自然语言推断，作者使用了transformer
based模型作为strong baseline。
<ul>
<li>正式的定义应该叫Mention而不是query，即需要去找链接的实体存在的上下文，记为<span
class="math inline">\(m\)</span>；而候选集实体的描述，记作<span
class="math inline">\(e\)</span></li>
<li>将<span class="math inline">\(m\)</span>和<span
class="math inline">\(e\)</span>作为句子对输入BERT模型，<span
class="math inline">\(m\)</span>还加上了额外的embedding以示区别于<span
class="math inline">\(e\)</span></li>
<li>BERT得到句子对编码之后，将编码的向量与entity的词向量内积得到分数</li>
<li>为了证明联合训练<span class="math inline">\(m\)</span>和<span
class="math inline">\(e\)</span>的自注意力的重要性，作者还控制变量做了两个对比的naive
bert模型，不过那就暂过不表了，毕竟self
attention很重要已经是通识了，不需要再强调了。</li>
</ul></li>
<li>以上的baseline其实很强大了，因为BERT经过预训练之后多少获得了领域迁移的能力，从结果也可以看出来，预训练和不预训练的BERT在平均准确度上差了3倍，而预训练的BERT无论使用src还是tgt还是都使用，差别都不大，不过都远远高于传统方法。</li>
</ul>
<h2 id="零次学习">零次学习</h2>
<ul>
<li>接下来就是作者提出的零次学习方法，主要还是利用预训练，传统的预训练迁移有两种：
<ul>
<li>任务自适应预训练：在src和tgt的无监督语料上预训练，在src的监督语料上微调</li>
<li>开放语料预训练：就是BERT这一类的，不管src和tgt，自己先在超大规模无监督语料上预训练，再到src的监督语料上微调</li>
</ul></li>
<li>作者提出领域自适应：即在以上两种预训练之后插入一段仅仅在tgt语料上预训练的过程，理由是模型的表达容量有限，应该优先优化tgt领域的表示</li>
</ul>
<h2 id="结果-1">结果</h2>
<ul>
<li>结果当然是作者提出的领域自适应效果好一些，但其实差别不大，顶多1到2个点，而且提出的方法也不算新，仅仅改变语料多了一个预训练过程，整篇论文像是用BERT灌了一个新领域，和预训练模型训练指南一样。可能关键贡献也是提出了一个零次学习实体链接任务数据集吧。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>BERTology</title>
    <url>/2020/03/02/bertology/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/22e2424125902e1e41bef204406c76eb.png" width="500"/></p>
<p>note for <a href="https://arxiv.org/pdf/2002.12327.pdf">A Primer in
BERTology: What we know about how BERT works</a></p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="bert-embeddings">BERT Embeddings</h1>
<ul>
<li>As an NLU encoder, BERT generates context-dependent embeddings, with
research focusing on:
<ul>
<li>BERT embeddings form clear clusters related to word sense</li>
<li>Some researchers found that embeddings of the same word vary with
position, seemingly related to the Next Sentence Prediction (NSP)
task</li>
<li>Studies on word representations across different layers revealed
that higher layers are more context-related, and embeddings become more
dense (occupy a narrow cone in the vector space) in higher layers, with
cosine distances between random words being closer than expected in an
isotropic space</li>
</ul></li>
</ul>
<h1 id="syntactic-knowledge">Syntactic Knowledge</h1>
<ul>
<li>BERT's representation is hierarchical rather than linear, capturing
more syntactic tree-like information than word order</li>
<li>BERT encodes POS, chunk, and other information, but doesn't fully
capture syntactic information (some long-distance dependencies are
ignored)</li>
<li>Syntactic information is not directly encoded in self-attention
weights but requires transformation</li>
<li>BERT considers subject-predicate agreement in cloze tasks</li>
<li>BERT cannot understand negation and is not sensitive to malformed
input</li>
<li>Its predictions remain unchanged by word order reversal, sentence
splitting, or subject-predicate removal</li>
<li>In essence, BERT encodes syntactic information without fully
utilizing it</li>
</ul>
<h1 id="semantic-knowledge">Semantic Knowledge</h1>
<ul>
<li>Probing tasks in Masked Language Model (MLM) suggest BERT can encode
some semantic role information</li>
<li>BERT encodes entity types, relations, semantic roles, and
proto-roles</li>
<li>Due to wordpiece preprocessing, BERT performs poorly in numerical
encoding and reasoning</li>
</ul>
<h1 id="world-knowledge">World Knowledge</h1>
<ul>
<li>In certain relationships, BERT outperforms knowledge base-based
methods, capable of knowledge extraction with good template
sentences</li>
<li>However, BERT cannot use this knowledge for reasoning</li>
<li>Research has found that BERT's knowledge is often guessed through
stereotypical character combinations, not factually accurate (e.g., it
would predict that a person with an Italian-sounding name is Italian,
even when it is factually incorrect)</li>
</ul>
<h1 id="self-attention-heads">Self-Attention Heads</h1>
<ul>
<li>Research has categorized attention heads into several types:
<ul>
<li>Attending to self, adjacent words, sentence end</li>
<li>Attending to adjacent words, CLS, SEP, or distributed across the
entire sequence</li>
<li>Or the following 5 types <img data-src="https://s2.ax1x.com/2020/03/02/3Wlqsg.png" alt="3Wlqsg.png" /></li>
</ul></li>
<li>Attention weight meaning: How other words are weighted when
calculating the next layer representation</li>
<li>Self-attention does not directly encode linguistic information, as
most heads are heterogeneous or vertical, related to excessive
parameters</li>
<li>Few heads encode words' syntactic roles</li>
<li>A single head cannot capture complete syntactic tree
information</li>
<li>Even heads that capture semantic relationships are not necessary for
improving related tasks</li>
</ul>
<h1 id="layers">Layers</h1>
<ul>
<li>Lower layers contain the most linear word order relationships;
higher layers have weaker word order information and stronger knowledge
information</li>
<li>BERT's middle layers contain the strongest syntactic information,
potentially capable of reconstructing syntactic trees</li>
<li>Middle layers have the best transfer performance and capabilities
<img data-src="https://s2.ax1x.com/2020/03/02/3WJDg0.png"
alt="3WJDg0.png" /></li>
<li>However, this conclusion is conflicting: some find lower layers
better for chunking, higher layers for parsing, while others find middle
layers best for tagging and chunking</li>
<li>During fine-tuning, lower layers' changes have minimal performance
impact; the last layer changes most significantly</li>
<li>Semantic information exists across all layers</li>
</ul>
<h1 id="pre-training">Pre-training</h1>
<ul>
<li>Original tasks were MLM and NSP, with research proposing improved
training objectives:
<ul>
<li>Removing NSP has minimal impact, especially in multilingual
versions</li>
<li>NSP can be extended to predict adjacent sentences or use inverted
sentences as negative samples</li>
<li>Dynamic masking can improve performance</li>
<li>Beyond-sentence MLM: replacing sentences with arbitrary strings</li>
<li>Permutation language modeling (XLNet): shuffling word order,
predicting from left to right</li>
<li>Span boundary objective: using span boundary words for
prediction</li>
<li>Phrase masking and named entity masking</li>
<li>Continual learning</li>
<li>Conditional MLM: replacing segmentation embedding with label
embedding</li>
<li>Replacing MASK token with [UNK] token</li>
</ul></li>
<li>Another improvement path involves datasets, attempting to integrate
structured data or common-sense information through entity embeddings or
semantic role information (e.g., E-BERT, ERNIE, SemBERT)</li>
<li>Regarding pre-training necessity: it makes models more robust, but
effectiveness varies by task</li>
</ul>
<h1 id="model-architecture">Model Architecture</h1>
<ul>
<li>Layer count is more important than head count</li>
<li>Large batches can accelerate model convergence (batch size of 32k
can reduce training time without performance degradation)</li>
<li>"A robustly optimized BERT pretraining approach" published optimal
parameter settings</li>
<li>Since higher layer self-attention weights resemble lower layers,
training shallow layers first and copying parameters to deeper layers
can improve training efficiency by 25%</li>
</ul>
<h1 id="fine-tuning">Fine-tuning</h1>
<ul>
<li>Some view fine-tuning as teaching BERT what information to
ignore</li>
<li>Fine-tuning suggestions:
<ul>
<li>Consider weighted outputs from multiple layers, not just the last
layer</li>
<li>Two-stage fine-tuning</li>
<li>Adversarial token perturbations</li>
</ul></li>
<li>Adapter modules can accelerate fine-tuning</li>
<li>Initialization is important, but no papers have systematically
investigated this</li>
</ul>
<h1 id="overparametrization">Overparametrization</h1>
<ul>
<li>BERT doesn't effectively utilize its massive parameters; most heads
can be pruned</li>
<li>Heads in one layer are mostly similar, potentially reducible to a
single head</li>
<li>Some layers and heads can degrade model performance</li>
<li>On subject-predicate agreement and subject detection, larger BERT
models sometimes perform worse than smaller ones</li>
<li>Using the same MLP and attention dropout in a layer might contribute
to head redundancy</li>
</ul>
<h1 id="compression">Compression</h1>
<ul>
<li>Two primary methods: quantization and knowledge distillation</li>
<li>Other approaches include progressive model replacing, embedding
matrix decomposition, and converting multiple layers to a single
recurrent layer</li>
</ul>
<h1 id="multilingual-bert">Multilingual BERT</h1>
<ul>
<li>Multilingual BERT performs excellently in zero-shot transfer for
many tasks but poorly in language generation</li>
<li>Improvement methods:
<ul>
<li>Fixing lower layers during fine-tuning</li>
<li>Translation language modeling</li>
<li>Improving word alignment in fine-tuning</li>
<li>Combining 5 pre-training tasks (monolingual and cross-lingual MLM,
translation language modeling, cross-lingual word recovery, and
paraphrase classification)</li>
</ul></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="bert-embeddings">BERT embeddings</h1>
<ul>
<li>BERT作为一个NLU编码器，其生成的embedding是上下文相关的，关于embedding的研究有
<ul>
<li>BERT embedding形成了明显的聚类，与word sense相关</li>
<li>也有人发现，相同单词的embedding随着Position不同有差别，且该现象貌似与NSP任务有关</li>
<li>有人研究了同一单词在不同层的representation，发现高层的表示更加与上下文相关，且越高层，embedding在高维空间中越密集（occupy
a narrow cone in the vector
space），在各向同性的情况下，两个随机单词之间的cosine距离比想象中的更为接近</li>
</ul></li>
</ul>
<h1 id="syntactic-knowledge">Syntactic knowledge</h1>
<ul>
<li>BERT的表示是层次而非线性的，其捕捉到的更像是句法树的信息而不是词序信息。</li>
<li>BERT编码了pos,chunk等信息，但是并没有捕捉完整的句法信息（有些远距离的依赖被忽略）</li>
<li>句法信息没有直接编码在self attention weight当中，而是需要转换</li>
<li>BERT在完形填空任务中考虑了主谓一致（subject-predicate
agreement）</li>
<li>BERT不能理解否定，对于malformed input不敏感</li>
<li>其预测不会因为词序颠倒、句子切分、主谓语移除而改变</li>
<li>即BERT编码了句法信息，但是没有利用上</li>
</ul>
<h1 id="semantic-knowledge">Semantic knowledge</h1>
<ul>
<li>通过在MLM任务中设置探针，一些研究表明BERT能编码一些语义角色信息</li>
<li>BERT编码了实体类型、关系、语义角色和proto-roles</li>
<li>由于wordpiece的预处理，BERT在数字编码、推理上表现的并不好</li>
</ul>
<h1 id="world-knowledge">World knowledge</h1>
<ul>
<li>在某些关系中，BERT比基于知识库的方法更好，只要有好的模板句，BERT可以用于抽取知识</li>
<li>但是BERT不能利用这些知识进行推理</li>
<li>另外有研究发现BERT的知识是通过刻板的字符组合猜出来的，并不符合事实it
would predict that a person with an Italian-sounding name is Italian,
even when it is factually incorrect.</li>
</ul>
<h1 id="self-attention-heads">Self-attention heads</h1>
<ul>
<li>研究发现attention heads可以分成几类
<ul>
<li>attend to自己、前后单词、句子结尾</li>
<li>attend to前后单词、CLS、SEP,或者在整个序列上都有分布</li>
<li>或者是以下5种 <img data-src="https://s2.ax1x.com/2020/03/02/3Wlqsg.png"
alt="3Wlqsg.png" /></li>
</ul></li>
<li>attention
weight的含义：计算该词的下一层表示时，其他的单词如何加权</li>
<li>self
attention并没有直接编码语言学信息，因为大部分的head都是heterogeneous或者vertical的，与参数量过多有关</li>
<li>少数的head编码了词的句法角色</li>
<li>单一的head无法捕捉完整的句法树信息</li>
<li>即便一些head能够捕捉语义关系，它们也不是带来相关任务上的提升的必须条件</li>
</ul>
<h1 id="layers">layers</h1>
<ul>
<li>底层包含了最多的线性词序关系，越高层，词序信息越弱，知识信息越强</li>
<li>BERT的中间层包含最强的句法信息，甚至可以设法还原句法树</li>
<li>BERT中间层的迁移表现和性能最好 <img data-src="https://s2.ax1x.com/2020/03/02/3WJDg0.png" alt="3WJDg0.png" /></li>
<li>但是该结论存在冲突，有些人发现底层做chunking更好，高层做parsing更好，有些人则发现中间层做tagging和chunking都是最好的</li>
<li>fine-tune时，底层不变对性能影响不大；最后一层在微调时变化最大</li>
<li>语义信息在各个层中都存在</li>
</ul>
<h1 id="pre-training">pre-training</h1>
<ul>
<li>原始的任务是MLM和NSP，有研究提出了更好的训练目标
<ul>
<li>NSP移除影响不大，尤其在多语言版本中</li>
<li>NSP可以扩展为预测前后两句，也可以将前后翻转的句子作为negative
sample，而不是从其他的文档中随便找一句作为negative sample</li>
<li>dynamic mask可以改善性能</li>
<li>Beyond-sentence MLM，将句子替换为任意字符串</li>
<li>Permutation language
modeling，即XLNET当中的打乱单词顺序，再从左往右预测，结合了非回归和自回归的特点，既考虑了上下文，又不会有mask导致pre-training和fine-tune目标不一致</li>
<li>Span boundary objective，只用span边界的词来预测span</li>
<li>Phrase masking and named entity masking</li>
<li>Continual learning，持续学习</li>
<li>Conditional MLM，将segmentation embedding替换为label
embedding以适应序列标注任务</li>
<li>replacing the MASK token with [UNK] token</li>
</ul></li>
<li>另外一条改进路线是数据集，有些研究试图将结构化数据融入BERT的pre-training，更为常见的融入常识信息的是加入entity
embedding或者semantic role information，例如E-BERT、ERNIE、SemBERT</li>
<li>关于是否需要预训练，预训练使得模型更鲁棒，但是依然看任务，有些任务从头训练和预训练差别不大</li>
</ul>
<h1 id="model-architecture">Model architecture</h1>
<ul>
<li>层数比head数更重要</li>
<li>大的batch能加速模型的收敛，with a batch size of 32k BERT’s training
time can be significantly reduced with no degradation in
performance</li>
<li>A robustly optimized BERT pretraining approach
公开了一些最优的参数设定</li>
<li>因为高层的一些self attention
weight和底层很像，所以可以先训练浅层，在把浅层参数复制到深层，能带来25%的训练效率提升</li>
</ul>
<h1 id="fine-tuning">fine-tuning</h1>
<ul>
<li>有人认为fine-tuning是告诉BERT该忽略哪些信息</li>
<li>一些fine-tune的建议
<ul>
<li>考虑多层的加权输出，而不仅仅使用最后一层去predict</li>
<li>Two-stage fine-tuning</li>
<li>Adversarial token perturbations</li>
</ul></li>
<li>可以插入adapter modules来加速fine-tune</li>
<li>初始化很重要，但没有论文针对初始化系统的试验过</li>
</ul>
<h1 id="overparametrization">Overparametrization</h1>
<ul>
<li>BERT没有很好的利用其庞大的参数，大部分head可以被裁剪掉</li>
<li>一层的head大部分相似，甚至可以将一层的head数裁剪为一个</li>
<li>有些层和head会降低模型的性能</li>
<li>在主谓一致、主语检测任务上，大的BERT模型表现反而不如小的</li>
<li>同一层使用同一个MLP、attention
dropout可能导致了这样的head冗余现象</li>
</ul>
<h1 id="compression">Compression</h1>
<ul>
<li>主要两种方式：量化与知识蒸馏</li>
<li>还有一些方式例如progressive model
replacing、对embedding矩阵做分解、化多层为循环单层等等。</li>
</ul>
<h1 id="multilingual-bert">Multilingual BERT</h1>
<ul>
<li>多语言版本的BERT在许多任务的零次学习迁移上表现非常好，但是在语言生成任务上表现不佳</li>
<li>一些改进mBERT的手段
<ul>
<li>fine-tune时固定低层</li>
<li>translation language modeling</li>
<li>improving word alignment in fine-tuning</li>
<li>combine 5 pre-training tasks (monolingual and cross-lingual MLM,
translation language modeling, cross-lingual word recovery and
paraphrase classification）</li>
</ul></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>A summary of my Android apps:BuptRoom</title>
    <url>/2017/01/16/buptroomreview/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/50e93d01eefd2d64738c372694d4f1fd.png" width="500"/></p>
<h1 id="introduction">Introduction</h1>
<p>Write an app to query the school's empty classrooms Pull information
from the school's registration website, classify and display it, and add
some miscellaneous things After all, it's my first time writing Android,
so I want to try everything Download here: <a
href="https://fir.im/buptroom">BuptRoom</a> repository address: <a
href="https://github.com/thinkwee/BuptRoom">A simple Beiyou self-study
room query system</a> It took about 3 weekends to complete the first
version, and then I spent about 1 month updating miscellaneous things
After that, I spent about 1 month updating miscellaneous things Many
things written in an unstandardized manner, and I just looked up and
used them temporarily Summarize the experience of writing the App:</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="learning-content">Learning content</h1>
<ul>
<li>Android basic architecture, components, lifecycle</li>
<li>Fragment</li>
<li>Java library and library calls</li>
<li>Github usage</li>
<li>Deploy app</li>
<li>Image processing methods</li>
<li>A stupid way to pull web content</li>
<li>Utilize GitHub third-party libraries</li>
<li>Color knowledge</li>
<li>Android Material Design</li>
<li>Simple optimization</li>
<li>Multithreading and Handler</li>
</ul>
<h1 id="solved-problems">Solved problems</h1>
<p>Mainly solved the following problems:</p>
<ul>
<li>Android 6.0 and above versions seem to require dynamic permission
verification, and the current version only supports 5.0 and below, used
permisson:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.INTERNET&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.SYSTEM_ALERT_WINDOW&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.WRITE_EXTERNAL_STORAGE&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.READ_EXTERNAL_STORAGE&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.MOUNT_UNMOUNT_FILESYSTEMS&quot;</span>&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>The webpage is a jsp dynamic webpage, which cannot be simply parsed,
so I finally used loadurl in webview to execute javascript commands, and
need to download the jsoup-1.9.2.jar package and add it to the library
file</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">MyWebViewClient</span> <span class="keyword">extends</span> <span class="title class_">WebViewClient</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">shouldOverrideUrlLoading</span><span class="params">(WebView view, String url)</span> &#123;</span><br><span class="line">        view.loadUrl(url);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPageStarted</span><span class="params">(WebView view, String url, Bitmap favicon)</span> &#123;</span><br><span class="line">        Log.d(<span class="string">&quot;WebView&quot;</span>,<span class="string">&quot;onPageStarted&quot;</span>);</span><br><span class="line">        <span class="built_in">super</span>.onPageStarted(view, url, favicon);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPageFinished</span><span class="params">(WebView view, String url)</span> &#123;</span><br><span class="line">        Log.d(<span class="string">&quot;WebView&quot;</span>,<span class="string">&quot;onPageFinished &quot;</span>);</span><br><span class="line">        view.loadUrl(<span class="string">&quot;javascript:window.handler.getContent(document.body.innerHTML);&quot;</span>);</span><br><span class="line">        <span class="built_in">super</span>.onPageFinished(view, url);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Write a handler to respond to javascript commands, so that the body
content in the html file is obtained in String form</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span>  <span class="keyword">class</span> <span class="title class_">JavascriptHandler</span>&#123;</span><br><span class="line">    <span class="meta">@JavascriptInterface</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getContent</span><span class="params">(String htmlContent)</span>&#123;</span><br><span class="line">        Log.i(Tag,<span class="string">&quot;html content: &quot;</span>+htmlContent);</span><br><span class="line">        document= Jsoup.parse(htmlContent);</span><br><span class="line">        htmlstring=htmlContent;</span><br><span class="line">        content=document.getElementsByTag(<span class="string">&quot;body&quot;</span>).text();</span><br><span class="line">        Toast.makeText(MainActivity.<span class="built_in">this</span>,<span class="string">&quot;加载完成&quot;</span>,Toast.LENGTH_SHORT).show();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>After that, string processing, according to the format provided by
the registration website</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Remove commas</span><br><span class="line">String contenttemp=content;</span><br><span class="line">content=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String[] contentstemp=contenttemp.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (String temp:contentstemp)&#123;</span><br><span class="line">    content=content+temp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Group</span><br><span class="line">contents=content.split(<span class="string">&quot; |:&quot;</span>);</span><br><span class="line">String showcontent=<span class="string">&quot;&quot;</span>;</span><br><span class="line">count=<span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> tsgflag=<span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> cishu=<span class="number">0</span>;</span><br><span class="line">j12.clear();</span><br><span class="line">j34.clear();</span><br><span class="line">j56.clear();</span><br><span class="line">j78.clear();</span><br><span class="line">j9.clear();</span><br><span class="line">j1011.clear();</span><br><span class="line"><span class="keyword">if</span> (keyword.contains(<span class="string">&quot;图书馆&quot;</span>)) tsgflag=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (String temp:contents)&#123;</span><br><span class="line">    <span class="keyword">if</span> (temp.contains(keyword))&#123;</span><br><span class="line">        cishu++;</span><br><span class="line">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class="line">    &#125;</span><br><span class="line">    count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SaveBuildingInfo is to classify and store classrooms by building, and then classify and store them by time period in j12,j34.....</span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span> == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (contents[k].contains(<span class="string">&quot;楼&quot;</span>) || contents[k].contains(<span class="string">&quot;节&quot;</span>) || contents[k].contains(<span class="string">&quot;图&quot;</span>))</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    ;</span><br><span class="line">    <span class="keyword">switch</span> (c) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            j12.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">            j34.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">            j56.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">            j78.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">            j9.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">            j1011.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    k++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>The NavigationView is a simple one, with no special design,
because there are no multiple interfaces, just use the refresh TextView
to fake multiple interfaces</p></li>
<li><p>Tried the MaterialDesign components, added some things about
system time</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">Calendar</span> <span class="variable">c</span> <span class="operator">=</span> Calendar.getInstance();</span><br><span class="line"> c.setTimeZone(TimeZone.getTimeZone(<span class="string">&quot;GMT+8:00&quot;</span>));</span><br><span class="line"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class="comment">// 获取当前年份</span></span><br><span class="line"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class="number">1</span>);<span class="comment">// 获取当前月份</span></span><br><span class="line"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class="comment">// 获取当前月份的日期号码</span></span><br><span class="line"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class="line"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class="line"> mMinute= c.get(Calendar.MINUTE);</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (mHour&gt;=<span class="number">8</span>&amp;&amp;mHour&lt;<span class="number">10</span>)&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是一二节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> (mHour&gt;=<span class="number">10</span>&amp;&amp;mHour&lt;<span class="number">12</span>)&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是三四节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">13</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">14</span>)||(mHour==<span class="number">15</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是五六节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">15</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">16</span>)||(mHour==<span class="number">17</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是七八节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">17</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">18</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是第九节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">18</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">19</span>)||(mHour==<span class="number">20</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是十、十一节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line">nowtime=<span class="string">&quot;现在是休息时间&quot;</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span>(<span class="string">&quot;1&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;天&quot;</span>;</span><br><span class="line">     daycount=<span class="number">6</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;2&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;一&quot;</span>;</span><br><span class="line">     daycount=<span class="number">0</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;3&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;二&quot;</span>;</span><br><span class="line">     daycount=<span class="number">1</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;4&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;三&quot;</span>;</span><br><span class="line">     daycount=<span class="number">2</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;5&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;四&quot;</span>;</span><br><span class="line">     daycount=<span class="number">3</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;6&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;五&quot;</span>;</span><br><span class="line">     daycount=<span class="number">4</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;7&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;六&quot;</span>;</span><br><span class="line">     daycount=<span class="number">5</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> Timestring=mYear + <span class="string">&quot;年&quot;</span> + mMonth + <span class="string">&quot;月&quot;</span> + mDay+<span class="string">&quot;日&quot;</span>+<span class="string">&quot;星期&quot;</span>+mWay;</span><br><span class="line"></span><br><span class="line"> <span class="type">FloatingActionButton</span> <span class="variable">fab</span> <span class="operator">=</span> (FloatingActionButton) findViewById(R.id.fab);</span><br><span class="line"> fab.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View view)</span> &#123;</span><br><span class="line">         Snackbar.make(view, <span class="string">&quot;今天是&quot;</span>+Timestring+<span class="string">&quot;\n&quot;</span>+nowtime+<span class="string">&quot;  &quot;</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class="line">                 .setAction(<span class="string">&quot;Action&quot;</span>, <span class="literal">null</span>).show();</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure>
<h1 id="what-i-learned-on-github">What I learned on GitHub</h1>
<p>In addition to trying to use other GitHub libraries, I learned a lot,
including color palettes, shake modules, fir update modules, sliding
card interfaces, etc. Some GitHub repository links are here</p>
<ul>
<li>滑动卡片界面：<a
href="https://github.com/romannurik/Android-SwipeToDismiss">Android-SwipeToDismiss</a></li>
<li>fir更新模块:<a
href="https://github.com/hugeterry/UpdateDemo">UpdateDemo</a></li>
</ul>
<p>Some of them are directly written in the code, and I forgot the
original address....</p>
<ul>
<li><p>Sensor call for shake</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShakeService</span> <span class="keyword">extends</span> <span class="title class_">Service</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TAG</span> <span class="operator">=</span> <span class="string">&quot;ShakeService&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> SensorManager mSensorManager;</span><br><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> flag=<span class="literal">false</span>;</span><br><span class="line"><span class="keyword">private</span> ShakeBinder shakebinder= <span class="keyword">new</span> <span class="title class_">ShakeBinder</span>();</span><br><span class="line"><span class="keyword">private</span> String htmlbody=<span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCreate</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="built_in">super</span>.onCreate();</span><br><span class="line">   mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class="line">   Log.i(TAG,<span class="string">&quot;Shake Service Create&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onDestroy</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   flag=<span class="literal">false</span>;</span><br><span class="line">   <span class="built_in">super</span>.onDestroy();</span><br><span class="line">   mSensorManager.unregisterListener(mShakeListener);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onStart</span><span class="params">(Intent intent, <span class="type">int</span> startId)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="built_in">super</span>.onStart(intent, startId);</span><br><span class="line">   Log.i(TAG,<span class="string">&quot;Shake Service Start&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">onStartCommand</span><span class="params">(Intent intent, <span class="type">int</span> flags, <span class="type">int</span> startId)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   mSensorManager.registerListener(mShakeListener,</span><br><span class="line">           mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class="line">           <span class="comment">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class="line">           <span class="number">50</span> * <span class="number">1000</span>); <span class="comment">//batch every 50 milliseconds</span></span><br><span class="line">   htmlbody=intent.getStringExtra(<span class="string">&quot;htmlbody&quot;</span>);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">super</span>.onStartCommand(intent, flags, startId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">SensorEventListener</span> <span class="variable">mShakeListener</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SensorEventListener</span>() &#123;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">float</span> <span class="variable">SENSITIVITY</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">BUFFER</span> <span class="operator">=</span> <span class="number">5</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="type">float</span>[] gravity = <span class="keyword">new</span> <span class="title class_">float</span>[<span class="number">3</span>];</span><br><span class="line">   <span class="keyword">private</span> <span class="type">float</span> <span class="variable">average</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="type">int</span> <span class="variable">fill</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onAccuracyChanged</span><span class="params">(Sensor sensor, <span class="type">int</span> acc)</span> &#123;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSensorChanged</span><span class="params">(SensorEvent event)</span> &#123;</span><br><span class="line">       <span class="keyword">final</span> <span class="type">float</span> <span class="variable">alpha</span> <span class="operator">=</span> <span class="number">0.8F</span>;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line">           gravity[i] = alpha * gravity[i] + (<span class="number">1</span> - alpha) * event.values[i];</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="type">float</span> <span class="variable">x</span> <span class="operator">=</span> event.values[<span class="number">0</span>] - gravity[<span class="number">0</span>];</span><br><span class="line">       <span class="type">float</span> <span class="variable">y</span> <span class="operator">=</span> event.values[<span class="number">1</span>] - gravity[<span class="number">1</span>];</span><br><span class="line">       <span class="type">float</span> <span class="variable">z</span> <span class="operator">=</span> event.values[<span class="number">2</span>] - gravity[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class="line">           average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class="line">           fill++;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           Log.i(TAG, <span class="string">&quot;average:&quot;</span>+average);</span><br><span class="line">           Log.i(TAG, <span class="string">&quot;average / BUFFER:&quot;</span>+(average / BUFFER));</span><br><span class="line">           <span class="keyword">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class="line">               handleShakeAction();<span class="comment">//如果达到阈值则处理摇一摇响应</span></span><br><span class="line">           &#125;</span><br><span class="line">           average = <span class="number">0</span>;</span><br><span class="line">           fill = <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">handleShakeAction</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   flag=<span class="literal">true</span>;</span><br><span class="line">   Toast.makeText(getApplicationContext(), <span class="string">&quot;摇一摇成功&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">   Intent intent= <span class="keyword">new</span> <span class="title class_">Intent</span>();</span><br><span class="line">   intent.putExtra(<span class="string">&quot;htmlbody&quot;</span>,htmlbody);</span><br><span class="line">   intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class="line">   intent.setClassName(<span class="built_in">this</span>,<span class="string">&quot;thinkwee.buptroom.ShakeTestActivity&quot;</span>);</span><br><span class="line">   startActivity(intent);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> IBinder <span class="title function_">onBind</span><span class="params">(Intent intent)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="keyword">return</span> shakebinder;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShakeBinder</span> <span class="keyword">extends</span> <span class="title class_">Binder</span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Independent network pull and multi-threading</span><br><span class="line">-     In the previous structure, network pull was integrated in the welcome activity, in order to add a refresh function in the main interface and call the network pull at any time, I wrote the network pull as a separate class, which can be called when needed</span><br><span class="line">-     However, in the welcome activity, the welcome animation and network pull are in two separate threads (to prevent the animation from being blocked), so there may be a situation where the welcome animation is completed and the main interface is entered, but the network pull is not completed, and the content pulled cannot be passed to the main interface. The final solution is to set a 2s timeout for the network pull. If it is not pulled, an incorrect parameter is passed to the activity that starts the main interface, prompting a refresh</span><br><span class="line">```Java</span><br><span class="line">        webget = new Webget();</span><br><span class="line">        webget.init(webView);</span><br><span class="line">        HaveNetFlag = webget.WebInit();</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                //execute the task</span><br><span class="line">                ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class="line">                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);</span><br><span class="line">                animation.setFillAfter(true);</span><br><span class="line">                img.startAnimation(animation);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, 50);</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                //execute the task</span><br><span class="line">                WrongNet = webget.getWrongnet();</span><br><span class="line">                HaveNetFlag = webget.getHaveNetFlag();</span><br><span class="line">                htmlbody = webget.getHtmlbody();</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2HaveNetFlag: &quot; + HaveNetFlag);</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2Wrongnet: &quot; + WrongNet);</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2html: &quot; + htmlbody);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, 2000);</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);</span><br><span class="line">                intent.putExtra(&quot;WrongNet&quot;, WrongNet);</span><br><span class="line">                intent.putExtra(&quot;HtmlBody&quot;, htmlbody);</span><br><span class="line">                startActivity(intent);</span><br><span class="line">                WelcomeActivity.this.finish();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;, 2500);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0IHL4.png" alt="i0IHL4.png" />
<figcaption aria-hidden="true">i0IHL4.png</figcaption>
</figure>
<h1 id="学习的内容">学习的内容</h1>
<ul>
<li>Android基本架构，组件，生命周期</li>
<li>Fragment的使用</li>
<li>Java库与库之间的调用</li>
<li>Github的使用</li>
<li>部署app</li>
<li>图像处理的一些方法</li>
<li>一个愚蠢的拉取网页内容的方式</li>
<li>GitHub第三方库的利用</li>
<li>颜色方面的知识</li>
<li>Android Material Design</li>
<li>简单的优化</li>
<li>多线程与Handler</li>
</ul>
<h1 id="解决的问题">解决的问题</h1>
<p>主要解决了这么几个问题</p>
<ul>
<li>Android6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.INTERNET&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.SYSTEM_ALERT_WINDOW&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.WRITE_EXTERNAL_STORAGE&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.READ_EXTERNAL_STORAGE&quot;</span>&gt;&lt;/uses-permission&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">&quot;android.permission.MOUNT_UNMOUNT_FILESYSTEMS&quot;</span>&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">MyWebViewClient</span> <span class="keyword">extends</span> <span class="title class_">WebViewClient</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">shouldOverrideUrlLoading</span><span class="params">(WebView view, String url)</span> &#123;</span><br><span class="line">        view.loadUrl(url);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPageStarted</span><span class="params">(WebView view, String url, Bitmap favicon)</span> &#123;</span><br><span class="line">        Log.d(<span class="string">&quot;WebView&quot;</span>,<span class="string">&quot;onPageStarted&quot;</span>);</span><br><span class="line">        <span class="built_in">super</span>.onPageStarted(view, url, favicon);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPageFinished</span><span class="params">(WebView view, String url)</span> &#123;</span><br><span class="line">        Log.d(<span class="string">&quot;WebView&quot;</span>,<span class="string">&quot;onPageFinished &quot;</span>);</span><br><span class="line">        view.loadUrl(<span class="string">&quot;javascript:window.handler.getContent(document.body.innerHTML);&quot;</span>);</span><br><span class="line">        <span class="built_in">super</span>.onPageFinished(view, url);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span>  <span class="keyword">class</span> <span class="title class_">JavascriptHandler</span>&#123;</span><br><span class="line">    <span class="meta">@JavascriptInterface</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">getContent</span><span class="params">(String htmlContent)</span>&#123;</span><br><span class="line">        Log.i(Tag,<span class="string">&quot;html content: &quot;</span>+htmlContent);</span><br><span class="line">        document= Jsoup.parse(htmlContent);</span><br><span class="line">        htmlstring=htmlContent;</span><br><span class="line">        content=document.getElementsByTag(<span class="string">&quot;body&quot;</span>).text();</span><br><span class="line">        Toast.makeText(MainActivity.<span class="built_in">this</span>,<span class="string">&quot;加载完成&quot;</span>,Toast.LENGTH_SHORT).show();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>之后是字符串处理，根据教务处给的格式精简分类</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">去逗号</span><br><span class="line">String contenttemp=content;</span><br><span class="line">content=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String[] contentstemp=contenttemp.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (String temp:contentstemp)&#123;</span><br><span class="line">    content=content+temp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">分组</span><br><span class="line">contents=content.split(<span class="string">&quot; |:&quot;</span>);</span><br><span class="line">String showcontent=<span class="string">&quot;&quot;</span>;</span><br><span class="line">count=<span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> tsgflag=<span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> cishu=<span class="number">0</span>;</span><br><span class="line">j12.clear();</span><br><span class="line">j34.clear();</span><br><span class="line">j56.clear();</span><br><span class="line">j78.clear();</span><br><span class="line">j9.clear();</span><br><span class="line">j1011.clear();</span><br><span class="line"><span class="keyword">if</span> (keyword.contains(<span class="string">&quot;图书馆&quot;</span>)) tsgflag=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (String temp:contents)&#123;</span><br><span class="line">    <span class="keyword">if</span> (temp.contains(keyword))&#123;</span><br><span class="line">        cishu++;</span><br><span class="line">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class="line">    &#125;</span><br><span class="line">    count++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....</span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span> == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (contents[k].contains(<span class="string">&quot;楼&quot;</span>) || contents[k].contains(<span class="string">&quot;节&quot;</span>) || contents[k].contains(<span class="string">&quot;图&quot;</span>))</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    ;</span><br><span class="line">    <span class="keyword">switch</span> (c) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            j12.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">            j34.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">            j56.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">            j78.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">            j9.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">            j1011.add(contents[k]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    k++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面</p></li>
<li><p>尝试了MaterialDesign组件，加入一点系统时间方面的东西</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="type">Calendar</span> <span class="variable">c</span> <span class="operator">=</span> Calendar.getInstance();</span><br><span class="line"> c.setTimeZone(TimeZone.getTimeZone(<span class="string">&quot;GMT+8:00&quot;</span>));</span><br><span class="line"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class="comment">// 获取当前年份</span></span><br><span class="line"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class="number">1</span>);<span class="comment">// 获取当前月份</span></span><br><span class="line"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class="comment">// 获取当前月份的日期号码</span></span><br><span class="line"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class="line"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class="line"> mMinute= c.get(Calendar.MINUTE);</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (mHour&gt;=<span class="number">8</span>&amp;&amp;mHour&lt;<span class="number">10</span>)&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是一二节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> (mHour&gt;=<span class="number">10</span>&amp;&amp;mHour&lt;<span class="number">12</span>)&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是三四节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">13</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">14</span>)||(mHour==<span class="number">15</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是五六节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">15</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">16</span>)||(mHour==<span class="number">17</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是七八节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">17</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">18</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是第九节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line"> <span class="keyword">if</span> ((mHour==<span class="number">18</span>&amp;&amp;mMinute&gt;=<span class="number">30</span>)||(mHour==<span class="number">19</span>)||(mHour==<span class="number">20</span>&amp;&amp;mMinute&lt;<span class="number">30</span>))&#123;</span><br><span class="line">     nowtime=<span class="string">&quot;现在是十、十一节课&quot;</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span></span><br><span class="line">nowtime=<span class="string">&quot;现在是休息时间&quot;</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span>(<span class="string">&quot;1&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;天&quot;</span>;</span><br><span class="line">     daycount=<span class="number">6</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;2&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;一&quot;</span>;</span><br><span class="line">     daycount=<span class="number">0</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;3&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;二&quot;</span>;</span><br><span class="line">     daycount=<span class="number">1</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;4&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;三&quot;</span>;</span><br><span class="line">     daycount=<span class="number">2</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;5&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;四&quot;</span>;</span><br><span class="line">     daycount=<span class="number">3</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;6&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;五&quot;</span>;</span><br><span class="line">     daycount=<span class="number">4</span>;</span><br><span class="line"> &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">&quot;7&quot;</span>.equals(mWay))&#123;</span><br><span class="line">     mWay =<span class="string">&quot;六&quot;</span>;</span><br><span class="line">     daycount=<span class="number">5</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> Timestring=mYear + <span class="string">&quot;年&quot;</span> + mMonth + <span class="string">&quot;月&quot;</span> + mDay+<span class="string">&quot;日&quot;</span>+<span class="string">&quot;星期&quot;</span>+mWay;</span><br><span class="line"></span><br><span class="line"> <span class="type">FloatingActionButton</span> <span class="variable">fab</span> <span class="operator">=</span> (FloatingActionButton) findViewById(R.id.fab);</span><br><span class="line"> fab.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View view)</span> &#123;</span><br><span class="line">         Snackbar.make(view, <span class="string">&quot;今天是&quot;</span>+Timestring+<span class="string">&quot;\n&quot;</span>+nowtime+<span class="string">&quot;  &quot;</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class="line">                 .setAction(<span class="string">&quot;Action&quot;</span>, <span class="literal">null</span>).show();</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure>
<h1 id="在github上学到的">在GitHub上学到的</h1>
<p>此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等
部分GitHub repository链接在这里</p>
<ul>
<li>滑动卡片界面：<a
href="https://github.com/romannurik/Android-SwipeToDismiss">Android-SwipeToDismiss</a></li>
<li>fir更新模块:<a
href="https://github.com/hugeterry/UpdateDemo">UpdateDemo</a></li>
</ul>
<p>还有一些直接写在代码里了，忘记原地址了....</p>
<ul>
<li><p>摇一摇的传感器调用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShakeService</span> <span class="keyword">extends</span> <span class="title class_">Service</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TAG</span> <span class="operator">=</span> <span class="string">&quot;ShakeService&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> SensorManager mSensorManager;</span><br><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> flag=<span class="literal">false</span>;</span><br><span class="line"><span class="keyword">private</span> ShakeBinder shakebinder= <span class="keyword">new</span> <span class="title class_">ShakeBinder</span>();</span><br><span class="line"><span class="keyword">private</span> String htmlbody=<span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCreate</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="built_in">super</span>.onCreate();</span><br><span class="line">   mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class="line">   Log.i(TAG,<span class="string">&quot;Shake Service Create&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onDestroy</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   flag=<span class="literal">false</span>;</span><br><span class="line">   <span class="built_in">super</span>.onDestroy();</span><br><span class="line">   mSensorManager.unregisterListener(mShakeListener);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onStart</span><span class="params">(Intent intent, <span class="type">int</span> startId)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="built_in">super</span>.onStart(intent, startId);</span><br><span class="line">   Log.i(TAG,<span class="string">&quot;Shake Service Start&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">onStartCommand</span><span class="params">(Intent intent, <span class="type">int</span> flags, <span class="type">int</span> startId)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   mSensorManager.registerListener(mShakeListener,</span><br><span class="line">           mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class="line">           <span class="comment">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class="line">           <span class="number">50</span> * <span class="number">1000</span>); <span class="comment">//batch every 50 milliseconds</span></span><br><span class="line">   htmlbody=intent.getStringExtra(<span class="string">&quot;htmlbody&quot;</span>);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">super</span>.onStartCommand(intent, flags, startId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">SensorEventListener</span> <span class="variable">mShakeListener</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SensorEventListener</span>() &#123;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">float</span> <span class="variable">SENSITIVITY</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">BUFFER</span> <span class="operator">=</span> <span class="number">5</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="type">float</span>[] gravity = <span class="keyword">new</span> <span class="title class_">float</span>[<span class="number">3</span>];</span><br><span class="line">   <span class="keyword">private</span> <span class="type">float</span> <span class="variable">average</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="type">int</span> <span class="variable">fill</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onAccuracyChanged</span><span class="params">(Sensor sensor, <span class="type">int</span> acc)</span> &#123;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSensorChanged</span><span class="params">(SensorEvent event)</span> &#123;</span><br><span class="line">       <span class="keyword">final</span> <span class="type">float</span> <span class="variable">alpha</span> <span class="operator">=</span> <span class="number">0.8F</span>;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line">           gravity[i] = alpha * gravity[i] + (<span class="number">1</span> - alpha) * event.values[i];</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="type">float</span> <span class="variable">x</span> <span class="operator">=</span> event.values[<span class="number">0</span>] - gravity[<span class="number">0</span>];</span><br><span class="line">       <span class="type">float</span> <span class="variable">y</span> <span class="operator">=</span> event.values[<span class="number">1</span>] - gravity[<span class="number">1</span>];</span><br><span class="line">       <span class="type">float</span> <span class="variable">z</span> <span class="operator">=</span> event.values[<span class="number">2</span>] - gravity[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class="line">           average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class="line">           fill++;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           Log.i(TAG, <span class="string">&quot;average:&quot;</span>+average);</span><br><span class="line">           Log.i(TAG, <span class="string">&quot;average / BUFFER:&quot;</span>+(average / BUFFER));</span><br><span class="line">           <span class="keyword">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class="line">               handleShakeAction();<span class="comment">//如果达到阈值则处理摇一摇响应</span></span><br><span class="line">           &#125;</span><br><span class="line">           average = <span class="number">0</span>;</span><br><span class="line">           fill = <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">handleShakeAction</span><span class="params">()</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   flag=<span class="literal">true</span>;</span><br><span class="line">   Toast.makeText(getApplicationContext(), <span class="string">&quot;摇一摇成功&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">   Intent intent= <span class="keyword">new</span> <span class="title class_">Intent</span>();</span><br><span class="line">   intent.putExtra(<span class="string">&quot;htmlbody&quot;</span>,htmlbody);</span><br><span class="line">   intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class="line">   intent.setClassName(<span class="built_in">this</span>,<span class="string">&quot;thinkwee.buptroom.ShakeTestActivity&quot;</span>);</span><br><span class="line">   startActivity(intent);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> IBinder <span class="title function_">onBind</span><span class="params">(Intent intent)</span> &#123;</span><br><span class="line">   <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">   <span class="keyword">return</span> shakebinder;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShakeBinder</span> <span class="keyword">extends</span> <span class="title class_">Binder</span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 独立网络拉取，并使用多线程</span><br><span class="line">-    在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用</span><br><span class="line">-    然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新</span><br><span class="line">```Java</span><br><span class="line">        webget = new Webget();</span><br><span class="line">        webget.init(webView);</span><br><span class="line">        HaveNetFlag = webget.WebInit();</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                //execute the task</span><br><span class="line">                ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class="line">                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);</span><br><span class="line">                animation.setFillAfter(true);</span><br><span class="line">                img.startAnimation(animation);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, 50);</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                //execute the task</span><br><span class="line">                WrongNet = webget.getWrongnet();</span><br><span class="line">                HaveNetFlag = webget.getHaveNetFlag();</span><br><span class="line">                htmlbody = webget.getHtmlbody();</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2HaveNetFlag: &quot; + HaveNetFlag);</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2Wrongnet: &quot; + WrongNet);</span><br><span class="line">                Log.i(&quot;welcome&quot;, &quot;2html: &quot; + htmlbody);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, 2000);</span><br><span class="line"></span><br><span class="line">        new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);</span><br><span class="line">                intent.putExtra(&quot;WrongNet&quot;, WrongNet);</span><br><span class="line">                intent.putExtra(&quot;HtmlBody&quot;, htmlbody);</span><br><span class="line">                startActivity(intent);</span><br><span class="line">                WelcomeActivity.this.finish();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;, 2500);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title>CLSciSumm summary</title>
    <url>/2020/03/27/clscisumm/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>A brief note on the CLSciSumm Workshop that the CIST lab participated
in, the main focus is on methods. The experiments are analysised in
detail in papers. Papers:</p>
<ul>
<li><a href="http://ceur-ws.org/Vol-1610/paper18.pdf">2016</a></li>
<li><a
href="http://ceur-ws.org/Vol-2002/cistclscisumm2017.pdf">2017</a></li>
<li><a href="http://ceur-ws.org/Vol-2132/paper8.pdf">2018</a></li>
<li><a href="http://ceur-ws.org/Vol-2414/paper20.pdf">2019</a></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="task">Task</h1>
<ul>
<li>Task 1a: Given a citing paper (CP) and a reference paper (RP), find
the cited text span (CTS) in RP that is referenced by a specific
citation in CP. Essentially, this is a sentence pair similarity
calculation task.</li>
<li>Task 1b: After identifying the CTS, determine which facet of the RP
this text span belongs to, which is a text classification task.</li>
<li>Task 2: Generate a summary of the RP, which is an automatic
summarization task.</li>
</ul>
<h1 id="section">2016</h1>
<h2 id="task-1a">Task 1a</h2>
<ul>
<li>Convert input text span pairs into feature vectors and feed them
into a classifier to determine if the two text spans are linked</li>
<li>Features include:
<ul>
<li>High-frequency words in RP, expanded using WordNet and word
vectors</li>
<li>LDA trained on RP and CP together to obtain topic features</li>
<li>Co-occurrence frequency of CTS and citance words</li>
<li>IDF: Co-occurrence word IDF within the RP sentence set</li>
<li>Jaccard similarity of the two text spans</li>
<li>Context-aware similarity: multiplication and square root of the
similarity between the current sentence's preceding and following
sentences with the matching sentence</li>
<li>Word2Vec-based similarity between two text spans, calculated by
taking the maximum word similarity and symmetrically normalizing based
on sentence length</li>
<li>Doc2Vec: directly obtaining sentence vectors and calculating
similarity</li>
</ul></li>
<li>Classifiers: SVM and manual weight scoring</li>
<li>The dataset is severely imbalanced, with unmatched samples 125 times
more numerous than matched samples. The author attempted to split
negative samples, training 125 SVMs and voting, but the results were
poor. Therefore, a manual weight scoring method was adopted.</li>
<li>Jaccard distance performed best, used as the primary scoring
feature, with other features' experimental effects used as supplementary
weights</li>
</ul>
<h2 id="task-1b">Task 1b</h2>
<ul>
<li>Rule-based approach
<ul>
<li>Facets include Hypothesis, Implication, Aim, Results, and Method.
Directly classify if the sentence contains these words</li>
<li>Calculate high-frequency words for each facet and expand them. Set a
threshold, and add the corresponding facet to the candidate set if the
number of high-frequency words exceeds the threshold. Select the facet
with the highest coverage</li>
</ul></li>
<li>SVM
<ul>
<li>Extract four features: paragraph position, document position ratio,
paragraph position ratio, RCTS position</li>
</ul></li>
<li>Voting
<ul>
<li>Combine results from all approaches</li>
</ul></li>
<li>Fusion
<ul>
<li>Each Task 1a run obtains a CTS result, calculate a Task 1b run for
each result, and select the best one</li>
</ul></li>
</ul>
<h2 id="task-2">Task 2</h2>
<ul>
<li>Feature extraction:
<ul>
<li>hLDA: Hierarchical topic features. Two ways to utilize hLDA
features: sentences sharing the same path have similar topic
distributions, so first cluster sentences, and in evaluation tasks, use
facets as clustering results. Another method is to calculate the hLDA
score for each word, composed of two parts: layer (assigned topic)
weight * word probability in the layer + word probability in the current
topic node. Through experience, a three-layer hLDA model shows that
high-layer words are most abstract, bottom-layer words are most
concrete, and middle-layer words' abstraction level is most likely to
appear in summary sentences, so middle-layer words are given higher
weights.</li>
<li>Sentence length: Gaussian modeling of gold summary sentence
length</li>
<li>Sentence position</li>
<li>Task 1a features: If extracted as CTS, use a weak score due to
potential Task 1a errors</li>
<li>RST features: Based on Rhetorical Structure Theory</li>
</ul></li>
<li>Weighted feature scoring with additional operations
<ul>
<li>Convert first-person pronouns in result sentences to
third-person</li>
<li>Extract sentences for each facet or hLDA cluster</li>
<li>Remove highly redundant sentences</li>
</ul></li>
</ul>
<h1 id="section-1">2017</h1>
<h2 id="task-1a-1">Task 1a</h2>
<ul>
<li>Added to 2016 approach:
<ul>
<li>Use WordNet to calculate similarity between words with the same POS,
including 6 types: cn, lin, lch, res, wup, and path similarity. Convert
word similarity to sentence similarity using the same method as word2vec
features</li>
<li>Use CNN to train and calculate sentence similarity, using CNN
results as a scoring feature</li>
</ul></li>
</ul>
<h2 id="task-1b-1">Task 1b</h2>
<ul>
<li>Basically the same as 2016, with minor differences in SVM training
details</li>
</ul>
<h2 id="task-2-1">Task 2</h2>
<ul>
<li>Mainly introduced determinantal point process sampling to balance
summary quality and diversity</li>
<li>When training hLDA, include not only RP but also all related
citations</li>
<li>Features
<ul>
<li>Added sentence topic distribution to hLDA features</li>
<li>Added title similarity as a feature</li>
</ul></li>
<li>Introduced determinantal point process sampling, treating sentences
as points to sample. Given each point's quality (score) and inter-point
similarity, sample a subset (summary) with high quality and low
inter-element similarity: <img data-src="https://s1.ax1x.com/2020/03/27/Gic2ef.png" alt="Gic2ef.png" /></li>
</ul>
<h1 id="section-2">2018</h1>
<h2 id="task-1a-2">Task 1a</h2>
<ul>
<li>Compared to the previous year, used Word Mover's Distance (WMD) as a
feature vector similarity measure, applied to Task 1a similarity
features and Task 2 DPPs</li>
<li>Improved LDA feature utilization. Previously only using hidden
topics as a dictionary, now also calculate LDA distribution similarity
between two sentences, considering not just the number of words in the
same topic but also the internal topic distribution</li>
</ul>
<h2 id="task-1b-2">Task 1b</h2>
<ul>
<li>Tried many machine learning methods, including SVM, DT, KNN, RF, GB,
but only RF achieved performance comparable to rule-based scoring</li>
</ul>
<h2 id="task-2-2">Task 2</h2>
<ul>
<li>Still rule-based scoring + DPPs, but when constructing the L matrix
for DPPs, used WMD to calculate similarity</li>
</ul>
<h1 id="section-3">2019</h1>
<h2 id="task-1a-3">Task 1a</h2>
<ul>
<li>When calculating LDA similarity, used Jaccard distance due to
typically sparse topic distributions</li>
<li>When using CNN, adopted Word2Vec_H feature:
<ul>
<li>First use SVD to reduce the embedding matrix dimensions for both
sentences</li>
<li>Calculate word-level similarity matrix, where <span
class="math inline">\(L_{ij}\)</span> is the cosine distance of the
dimensionality-reduced word vectors for the i-th word in sentence a and
j-th word in sentence b</li>
<li>Use the L matrix as CNN input</li>
</ul></li>
</ul>
<h2 id="task-1b-3">Task 1b</h2>
<ul>
<li>Added CNN as a classification method, but still unable to outperform
traditional feature scoring</li>
</ul>
<h2 id="task-2-3">Task 2</h2>
<ul>
<li>Mapped WMD distance to [0,1] interval using inverse proportional,
linear, and exponential mappings</li>
<li>When constructing the L matrix, used both QS and Gram decomposition,
eliminating the need to explicitly calculate features and similarities.
Just input each sentence's feature vector. Tried word2vec and LSA for
feature vector construction</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="task">Task</h1>
<ul>
<li>task
1a:给定论文CP和被引论文RP，给定CP当中的citation，找出RP中被这个citation引用的cited
text span(CTS)，即content linking，本质上是一个句子对相似度计算任务</li>
<li>task 1b:在找出CTS之后，判断这个text
span属于RP中的哪一个facet，即文本分类任务</li>
<li>task 2:生成RP的摘要，属于自动文摘任务</li>
</ul>
<h1 id="section">2016</h1>
<h2 id="task-1a">task 1a</h2>
<ul>
<li>将输入的text span pair转换成特征向量，送入分类器判断两个text
span是否link</li>
<li>特征包括
<ul>
<li>RP当中的高频词，用wordnet和word vector扩充</li>
<li>RP和CP一起训练LDA，得到主题特征</li>
<li>CTS和citance的词共现频率</li>
<li>idf：共现词在RP句子集当中的idf</li>
<li>两个text span的jaccard 相似度</li>
<li>考虑上下文的相似度，即当前句的前后两句与待匹配句的相似度相乘再开方</li>
<li>word2vec得到的两个text
span相似度，这里两个句子当中的词相似度最大值，再针对两句长度做对称归一化得到</li>
<li>doc2vec，直接得到句向量，做相似度计算</li>
</ul></li>
<li>分类器：SVM和人为赋予权重直接打分</li>
<li>可以看到这个任务的数据集严重不平衡，实际上不匹配的样本数量是匹配的样本数量的125倍，作者尝试切分负样本，分别训练了125个SVM并通过投票得到结果，但是效果非常差，因此采取了直接人为赋予权重打分的方法</li>
<li>jaccard距离的效果最好，以此为主要打分特征，根据其余特征的单独实验效果赋予权重作为补充特征</li>
</ul>
<h2 id="task-1b">task 1b</h2>
<ul>
<li>基于规则的
<ul>
<li>facet指 Hypothesis, Implication, Aim, Results and
Method之类的论文部分，假如句子中包含这些词就直接分类</li>
<li>计算每个facet下的高频词，并扩充。设定阈值，当句子高频词的数量超过阈值之后，就将对应facet加入候选集，选择coverage最高的facet作为结果</li>
</ul></li>
<li>SVM
<ul>
<li>提取四种特征：段落位置、文档位置比例、段落位置比例、RCTS位置</li>
</ul></li>
<li>投票
<ul>
<li>结合上面的所有结果投票</li>
</ul></li>
<li>融合
<ul>
<li>每一个1a的run都能得到一次CTS的结果，对每一次结果计算一个1b的run，取最好的</li>
</ul></li>
</ul>
<h2 id="task-2">task 2</h2>
<ul>
<li>提取特征：
<ul>
<li>hLDA：层次主题特征。hLDA的特征有两种利用方式：共享同一条路径的句子具有相似的主题分布，所以可以先进行句子的聚类，另外在评测任务当中我们还可以将facet作为聚类结果；另一种方式就是计算每个词的hlda得分，得分由两部分组成：词所在层的层（被分配主题）权重*词在所在层上的概率
+
当前主题节点内该词的概率。通过经验发现，若以三层hlda建模，高层是最抽象的词，底层是最具体的词，中间层次的词的抽象程度比较容易出现在摘要句当中，因此中间层赋予较高权重。</li>
<li>句子长度：对gold summary的句子长度做一次高斯建模</li>
<li>句子位置</li>
<li>1a特征：假如是1a中提取到的CTS，那么就基于一个弱分数，因为1a的结果包含错误</li>
<li>RST特征：基于修辞结构理论的特征</li>
</ul></li>
<li>加权特征进行打分，做了一些细节操作
<ul>
<li>将结果句当中的第一人称改为第三人称</li>
<li>为每一个facet或者每一个hlda聚类结果单独抽取句子</li>
<li>移除重复度高的句子</li>
</ul></li>
</ul>
<h1 id="section-1">2017</h1>
<h2 id="task-1a-1">task 1a</h2>
<ul>
<li>在2016的基础上添加了
<ul>
<li>利用Wordnet计算相同pos的词之间的相似度，共6种：cn, lin, lch, res,
wup and path
similarit。利用与word2vec特征同样的处理方法将词相似度转成句相似度</li>
<li>用CNN训练，计算两个句子的相似度，再将CNN的结果作为打分特征的一种</li>
</ul></li>
</ul>
<h2 id="task-1b-1">task 1b</h2>
<ul>
<li>基本同2016年一样，除了一些SVM训练细节稍有不同</li>
</ul>
<h2 id="task-2-1">task 2</h2>
<ul>
<li>相比前一年主要引入了行列式点过程采样来保证摘要结果质量与多样性的均衡</li>
<li>在训练hlda时，不仅仅将RP作为单篇文档的语料，也引入了该RP相关的所有citation</li>
<li>特征
<ul>
<li>hlda的特征增加了句子的主题分布</li>
<li>增加了标题相似度作为特征</li>
</ul></li>
<li>引入了行列式点过程抽样，将句子作为待抽样的点，给定每个点的质量（得分）以及点之间的相似度，采样出一个子集（摘要），使得子集的质量高且子集内元素相似度低：
<img data-src="https://s1.ax1x.com/2020/03/27/Gic2ef.png"
alt="Gic2ef.png" /></li>
</ul>
<h1 id="section-2">2018</h1>
<h2 id="task-1a-2">task 1a</h2>
<ul>
<li>相比前一年，作者采用了WMD距离作为特征向量之间的相似度度量，将其应用于task
1a的similarity feature以及task2中DPPs</li>
<li>改进了LDA特征的利用。之前只是将隐主题作为词典，现在还计算了两个句子的LDA分布相似度，不仅考虑了属于同一主题的词的数量，还考虑了内部的主题分布</li>
</ul>
<h2 id="task-1b-2">task 1b</h2>
<ul>
<li>尝试了很多机器学习方法，包括SVM、DT、KNN、RF、GB，但只有RF取得了和规则打分方法相当的效果</li>
</ul>
<h2 id="task-2-2">task 2</h2>
<ul>
<li>依然是规则打分+DPPs，只不过在构建DPPs所需的L矩阵时，采用了WMD计算相似度</li>
</ul>
<h1 id="section-3">2019</h1>
<h2 id="task-1a-3">task 1a</h2>
<ul>
<li>在计算LDA相似度时，采用jaccard距离，因为LDA的主题分布通常很稀疏</li>
<li>使用CNN时，采用了Word2Vec_H特征：
<ul>
<li>先用SVD对两个句子的embedding矩阵进行降维</li>
<li>再计算词级别的相似度矩阵，<span
class="math inline">\(L_{ij}\)</span>是句子a的i号词和句子b的j号词的降维之后的词向量计算cosine距离</li>
<li>将L矩阵作为CNN的输入</li>
</ul></li>
</ul>
<h2 id="task-1b-3">task 1b</h2>
<ul>
<li>添加了CNN作为分类方法，但是依然比不过传统特征打分</li>
</ul>
<h2 id="task-2-3">task 2</h2>
<ul>
<li>将WMD距离经过反比例映射、线性映射、和指数映射压缩入[0,1]区间内</li>
<li>构建L矩阵时除了QS分解，还采取了Gram分解，这样不需要显式计算特征与相似度，只需要输入每一句的特征向量即可。尝试了word2vec与LSA构建特征向量</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>natural language processing</tag>
        <tag>machine learning</tag>
        <tag>workshop</tag>
      </tags>
  </entry>
  <entry>
    <title>Future of Computing Salon - Reading Comprehension Session</title>
    <url>/2018/10/13/compute-future/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/d1c3632ae2916e58df8591e7ac188747.png" width="500"/></p>
<p>Attended a light salon at Tsinghua University's FIT, which introduced
some advancements in machine reading comprehension. Interestingly, the
PhD who spoke at 9 am also mentioned an unpublished work: BERT, which is
very impressive and well-funded; it took eight p100 GPUs to train for a
year. By 10:30, Machine Intelligence had already published a report, and
by the afternoon, Zhihu was buzzing with discussions, saying that a new
era for NLP had arrived... This salon is part of a series, and there may
be future sessions on machine translation, deep Bayesian, transfer
learning, and knowledge graphs, so if you have the time, you might as
well listen and take notes.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="machine-reading-comprehension">Machine Reading
Comprehension</h1>
<ul>
<li>Three speeches, the first being an overview; the second a
presentation by the author of nlnet, which won first place on SQuAD2.0,
a collaboration between National University of Defense Technology and
Microsoft; the third by a Ph.D. from Tsinghua University, who introduced
his research on noise filtering and information aggregation in
open-domain question answering.</li>
</ul>
<h2 id="abstract">Abstract</h2>
<ul>
<li>The current reading comprehension is far behind what is expected
from artificial intelligence reading comprehension. Researchers have
decomposed the reading comprehension process into tasks such as word
selection, span selection, and generating short texts. Before the rise
of deep learning, it involved some manually designed features and
Pipeline operations. With the advent of deep learning, the focus shifted
to end-to-end research from input to output, bypassing many elements
required in the reading comprehension process.</li>
<li>Previous research on reading comprehension can be used as a testing
method to assess the model's ability in lexical, rhetorical, and
knowledge utilization skills.</li>
<li>The current large-scale machine reading comprehension datasets are
at a very low level of inference, as mentioned in a paper: "Efficient
and Robust Question Answering from Minimal Context over Documents." It
discusses that if deep learning is used, training only with the span you
find, cutting out the context, the results actually won't be much
different. Therefore, end-to-end learning does not involve a "reading
the entire text to grasp the main idea" process, but rather "you ask, I
answer, don't ask me why I answer this way, just memorize." It mentions
a work from the University of Tokyo that established evaluation
indicators for the model's reading comprehension ability, over 30,
including the elimination of ambiguity, coreference resolution, etc.
Large and simple datasets cannot reflect these features, and the
cleverly designed datasets are not large enough in scale.</li>
<li>Towards AI-Complete Question Answering: A Set of Prerequisite Toy
Tasks</li>
<li>Industrial issues to be solved in the use of reading comprehension:
simplifying models or accelerating models, introduces techniques such as
SKIM-RNN, which become more complex during training but can accelerate
inference. Paper: Neural Speed Reading via SKIM-RNN</li>
<li>BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding. Another form of transfer learning is to directly use a
trained module from one task for another, such as directly taking the
encoder trained in seq2seq to calculate semantic representations (deep
learning is essentially representation learning). Remember, fast Disan
directly returns a function that returns the vector representation of a
sentence, which is similar in thought.</li>
<li>Latest Research Field: open domain question-answering and learning
to ask. The former actually adds an information retrieval process, where
the relevant corpus needed for reading comprehension is retrieved
through asking questions. The latter reverses the answer task to asking
questions, with the speaker mentioning that the reverse can assist in
reading comprehension, and it has an industrially useful design: instead
of comparing queries and documents (or document keywords) during
retrieval, it compares them with the questions generated for the
documents, which is equivalent to calculating similarity between two
questions.</li>
<li>The speaker mentioned his view on attention: attention involves
filtering information from the model, which does not imply that the
original model lacks the ability to represent this information.</li>
<li>Presented several currently popular datasets, using MCTest and
ProcessBank before 2015, CNNDM, SQuAD, and MS MARCO between 2015 and
2017, and TriviaQA, SQuAD2.0, CoQA, QuAC, and HotpotQA after 2017.
(However, the abstract is still using CNNDM...)</li>
</ul>
<h2 id="nlnet">NLNet</h2>
<ul>
<li>Paper: It can be seen on Squad, but it seems that it hasn't been
published yet?</li>
<li>NLNet was originally designed to address the robustness and
effectiveness in reading comprehension problems, both of which are
targeted at ensemble models. Therefore, NLNet adds a distillation
process on top of ensemble models, using a single model to improve
efficiency, and also includes a read and verify process to enhance
robustness. Consequently, it performs exceptionally well on the SQuAD2.0
dataset with adversarial samples, currently ranking first. It lags
behind the four-overpowering BERT on version 1.0, but the gap is not
significant. However, the ensemble version of NLNet in version 1.0 is
better than the single model version, while the 2.0 version did not
submit an ensemble version, which is quite perplexing...</li>
<li>The meaning of distillation was not fully understood, the effect is
to compress 12 models into one model, with the structure of the models
completely the same but with different initializations. It is not simply
selecting the best; the single model is trained, and the paper refers to
the 12 models as teachers and the single model as a student, with the
student using the training results of the teacher to guide its
training.</li>
<li>Designed a read and verify mechanism, which, after extracting a span
to answer a question, also calculates a confidence score based on the
answer and the question. If the confidence score is too low, it is
considered that there is no answer, which is akin to the adversarial
sample scenario in SQuAD 2.0. It feels like if there is an issue, loss
is added.</li>
<li>It is said that some details of feature selection were not presented
in the paper, and the model was optimized with reinforcement learning at
the end?</li>
</ul>
<h2 id="open-domain-qa-noise-filtering-and-information-aggregation">Open
Domain QA Noise Filtering and Information Aggregation</h2>
<ul>
<li>Paper (ACL 2018): Denoising Distantly Supervised Open-Domain
Question Answering</li>
<li>This noise refers to the situation where many relevant documents are
found during the retrieval process but do not provide the correct
answers, which is a filtering of documents. This step of filtering
should have been placed within the retrieval process, but the author
ultimately solved it by using a deep learning algorithm to calculate
probabilities and loss.</li>
<li>The denoising process is a document selector, and then reading
comprehension is a reader, the author believes that it corresponds to
the fast skimming and careful reading &amp; summarizing that humans do
in reading comprehension.</li>
<li>The information set did not pay much attention to listening, but
fully utilized the information extraction from multiple documents to
provide answers</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="机器阅读理解">机器阅读理解</h1>
<ul>
<li>三场演讲，第一场是概述；第二场是当时在SQuAD2.0上拿到第一名的nlnet作者的presentation，国防科大和微软合作的成果；第三场是一位清华的博士，介绍了他关于开放领域问答中噪声过滤和信息集合的研究。</li>
</ul>
<h2 id="概述">概述</h2>
<ul>
<li>现在的阅读理解和人们所期望的人工智能阅读理解差了太多，研究者把阅读理解的过程分解成了任务，例如选词、选span、生成短文本。深度学习兴起之前都是一些手工设计特征，一些Pipiline的操作，使用深度学习之后就专注于输入到输出的端到端研究，绕过了很多阅读理解过程所需要的东西。</li>
<li>以前的关于阅读理解的研究可以作为一个测试方法，检验模型对于词法、修辞、利用知识的能力。</li>
<li>目前的大规模机器阅读理解数据集处于很低级的推断阶段，提到了一篇论文：Efficient
and Robust Question Answering from Minimal Context over
Documents。里面讲到如果用深度学习，只用你找出的span来训练，砍掉上下文，其实结果不会差很多，因此端到端的学习并没有“通读全文掌握大意”的过程，而是“你问什么我答什么，别问我为什么这么答，背的”。提到了东京大学一份工作，建立了对模型阅读理解能力的评价指标，30多项，包括消除歧义、指代消解等等，大而简单的数据集无法体现这些特征，而设计巧妙的数据集规模不够大。</li>
<li>还提到了一篇关于衡量模型推断能力的论文，TOWARDS AI-COMPLETE QUESTION
ANSWERING:A SET OF PREREQUISITE TOY TASKS。</li>
<li>工业上使用阅读理解所需要解决的问题：简化模型或者加速模型，介绍了诸如SKIM-RNN之类的技巧，虽然训练的时候会变复杂，但推断时能加速。论文：NEURAL
SPEED READING VIA SKIM-RNN</li>
<li>现在NLP的迁移学习，预训练词嵌入或者预训练语言模型，用的最多最广泛的，比如Glove和Elmo，然后就提到了BERT，4亿参数，无监督，最重要的是其模型是双向设计，且充分训练了上层参数：BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding.另外一种迁移学习是直接将某一任务中训练好的模块做其他任务，例如直接将seq2seq中训练好的encoder拿出来计算语义表示（深度学习本来就是表示学习），记得fast
Disan就直接一个函数返回句子的向量表示，也是类似的思想。</li>
<li>最新的研究领域：open domain question-answering和learning to
ask。前者实际上是加了一个信息检索的过程，阅读理解所需要的相关语料是通过提问检索到的。后者是将回答任务反过来做提问，演讲者提到反向可以辅助阅读理解，且有一个工业上比较有用的设计：做检索时不用query和文档（或者文档关键词）相比较，而是和针对文档生成的提问相比较，相当于两个提问之间计算相似度。</li>
<li>演讲者提到了他关于attention的一个观点：attention是从模型中筛选信息，不代表原模型没有表示出此信息的能力。</li>
<li>介绍了当前比较流行的几个数据集，2015之前用MCTest、ProcessBank，15到17年之间用CNNDM、SQuAD、MS
MARCO，17年之后用TriviaQA、SQuAD2.0、CoQA、QuAC、HotpotQA。（然而文摘还在用CNNDM......）</li>
</ul>
<h2 id="nlnet">NLNet</h2>
<ul>
<li>论文：Squad上可以看到，但是好像还没发？</li>
<li>NLNet的设计初衷是为了解决阅读理解问题中的鲁棒性和有效性，都是针对集成模型说的，所以NLNet是在集成模型的基础上加了一个蒸馏的过程，使用单模型提升效率，另外还有一个read
and
verify的过程来提升鲁棒性，所以在加入了对抗样本的SQuAD2.0数据集上表现优异，目前第一。在1.0上落后于四处碾压的BERT，但其实落后也不多。不过1.0版本中nlnet的ensemble版本要好于单模型版本，2.0中没有提交ensemble版本，就很迷......</li>
<li>蒸馏的意思没太听明白，效果是12个模型压缩成一个模型，模型的结构完全相同，但是初始化不同。不是简单的选最优，单一的模型是训练出来的，论文里叫那12个模型为teacher，单一模型为student，student使用teacher训练的结果来指导训练。</li>
<li>设计了一个read and
verify机制，在抽取出span回答问题之后还会根据该回答和问题计算一个置信度，置信度太低就认为是没有答案，也就是squad2.0里对抗样本的情况。感觉听下来就是有问题就加loss。</li>
<li>听说一些选取特征的细节没有在论文中表出，而且最后用强化学习优化了一下模型？</li>
</ul>
<h2 id="open-domain-qa噪声过滤和信息集合">Open Domain
QA噪声过滤和信息集合</h2>
<ul>
<li>论文（ACL 2018）：Denoising Distantly Supervised Open-Domain
Question Answering</li>
<li>这个噪声是指在检索文档的过程搜到了很多相关但提供的不是正确答案的文档，是对文档的过滤。这一步的过滤本来应该放在检索的过程里，但是作者最后也是用深度学习算概率加loss的方式解决了。</li>
<li>去噪过程是一个document
selector，然后阅读理解是一个reader，作者认为对应于人做阅读理解的fast
skimming 和careful reading &amp; summarizing。</li>
<li>信息集合没太注意听，就是充分利用多篇文档的信息提取出答案</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>comprehension</tag>
        <tag>NLI</tag>
      </tags>
  </entry>
  <entry>
    <title>Study Notes for CS224w</title>
    <url>/2020/03/30/cs224w/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/3c5661a8e29179a9b49020309941090c.png" width="500"/></p>
<p>Study notes for Stanford CS224W: Machine Learning with Graphs by Jure
Leskovec.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="network-and-random-graph-properties">Network and Random Graph
Properties</h1>
<ul>
<li>Degree Distribution: P(k)
<ul>
<li>Represents the distribution of node degrees</li>
</ul></li>
<li>Path Length: h
<ul>
<li>Path: A route that can intersect itself and pass through the same
edge multiple times</li>
<li>Distance: The shortest path between two points</li>
<li>Diameter: The maximum shortest path length between any two nodes in
a graph</li>
<li>Average Path Length: Sum of distances divided by the number of node
pairs</li>
</ul></li>
<li>Clustering Coefficient: C
<ul>
<li><p>Measures the connectivity of a node's neighbors: Number of edges
between neighbors divided by the node's degree:</p>
<p><span class="math display">\[
C_{i}=\frac{2 e_{i}}{k_{i}\left(k_{i}-1\right)}
\]</span></p></li>
</ul></li>
<li>Connected Components
<ul>
<li>The size of the largest component (giant component) is called
connectivity, which is the number of nodes in the largest connected
subgraph</li>
</ul></li>
<li>Random Graph Model
<ul>
<li><p>Viewing a graph as the result of a random process, with two
parameters n and p: n total nodes, with edges independently established
with probability p. Clearly, these two parameters are not sufficient to
uniquely determine a graph. Consider its degree distribution, path
length, and clustering coefficient</p></li>
<li><p>P(k):</p>
<p><span class="math display">\[
P(k)=\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) p^{k}(1-p)^{n-1-k}
\]</span></p></li>
<li><p>h: Olog(n)</p></li>
<li><p>C:</p>
<p><span class="math display">\[
=\frac{p \cdot
k_{i}\left(k_{i}-1\right)}{k_{i}\left(k_{i}-1\right)}=p=\frac{\bar{k}}{n-1}
\approx \frac{\bar{k}}{n}
\]</span></p></li>
<li><p>Connectivity: As p increases, the graph becomes increasingly
likely to have connected subgraphs, specifically as follows: <img data-src="https://s1.ax1x.com/2020/03/30/GuQgQf.png"
alt="GuQgQf.png" /></p></li>
</ul></li>
<li>Small World Model
<ul>
<li>Maintaining a high clustering coefficient while having a short
diameter <a href="https://imgchr.com/i/GulXgP"><img data-src="https://s1.ax1x.com/2020/03/30/GulXgP.png"
alt="GulXgP.png" /></a></li>
<li>How to construct such a graph? Start with a high-clustering,
long-diameter graph, and introduce shortcuts: <img data-src="https://s1.ax1x.com/2020/03/30/Gu3QeS.png" alt="Gu3QeS.png" /></li>
</ul></li>
<li>Kronecker Graphs: Recursively generating large realistic graphs</li>
</ul>
<h1 id="graph-features-texture-subgraphs-small-graphs">Graph Features:
Texture, Subgraphs, Small Graphs</h1>
<ul>
<li><p>Subgraphs, taking a three-node subgraph as an example: <img data-src="https://s1.ax1x.com/2020/03/30/Gu8hj0.png"
alt="Gu8hj0.png" /></p></li>
<li><p>Assuming all nodes are identical, subgraphs focus on structural
features of nodes and edges. If we define a significance for each
subgraph, we can construct a feature vector.</p></li>
<li><p>Defining Texture: Motifs, "recurring, significant patterns of
interconnections", i.e., small induced subgraphs that appear frequently
and are more significant than expected in randomly generated
networks</p></li>
<li><p>The significance of a motif can be defined by its occurrence
ratio in real and random graphs:</p>
<p><span class="math display">\[
Z_{i}=\left(N_{i}^{\mathrm{real}}-\bar{N}_{i}^{\mathrm{rand}}\right) /
\operatorname{std}\left(N_{i}^{\mathrm{rand}}\right)
\]</span></p></li>
<li><p>RECAP Algorithm: Finding motifs in a graph</p>
<ul>
<li>Based on the real graph, define a random graph with the same number
of nodes, edges, and degree distribution</li>
<li>Find the significance of each subgraph in the real and corresponding
random graphs. Subgraphs with high significance are motifs</li>
</ul></li>
<li><p>Graphlets: Node feature vectors</p>
<ul>
<li>Graphlet Definition: Connected non-isomorphic subgraphs</li>
<li>In graphlets, we inject node-level features. For three nodes, there
are only two graphlets: a triangle or a line connecting three
points</li>
<li>In a triangle, each node is equivalent (relative to other nodes in
the graphlet)</li>
<li>In a line, the two end nodes are equivalent, and the middle node is
another type</li>
<li>Graphlets are very sparse: 11,716,571 types for n=10, not counting
different node types</li>
</ul></li>
<li><p>Generalizing the degree concept, Graphlet Degree Vector (GDV)
represents the graphlets a node touches, with each type occupying a
feature and its value being the number of such graphlets touched: <img data-src="https://s1.ax1x.com/2020/03/30/GutuVK.png"
alt="GutuVK.png" /></p></li>
<li><p>GDV measures the local topological state of a node</p></li>
<li><p>Finding graphlets/motifs: Three types of algorithms</p>
<ul>
<li>Exact Subgraph Enumeration (ESU) [Wernicke 2006]</li>
<li>Kavosh [Kashani et al. 2009]</li>
<li>Subgraph Sampling [Kashtan et al. 2004]</li>
</ul></li>
<li><p>Graph Isomorphism: How to determine if two graphs are
topologically equivalent?</p>
<ul>
<li>Defining Roles: The function of nodes determined by structural
information</li>
<li>Roles can be defined as a collection of nodes with similar positions
in a network</li>
<li>Difference from communities: Nodes in the same role do not
necessarily need to be directly connected or have indirect interactions,
but occupy the same position in the neighborhood structure</li>
<li>Structural equivalence: Nodes are structurally equivalent if they
have the same relationships to all other nodes</li>
<li>How to discover roles? ROIX algorithm <img data-src="https://s1.ax1x.com/2020/03/30/GuUd3Q.png" alt="GuUd3Q.png" /></li>
<li>Recursive feature extraction: Construct base features, continuously
aggregate and iterate, prune using correlation</li>
<li>Role extraction: Essentially matrix decomposition, viewing roles as
latent topics. RolX uses non-negative matrix factorization for
clustering, MDL for model selection, and KL divergence to measure
likelihood</li>
</ul></li>
</ul>
<h1 id="spectral-clustering">Spectral Clustering</h1>
<ul>
<li><p>Three-step approach</p>
<ul>
<li>Preprocessing: Obtain a matrix containing the entire graph's
information</li>
<li>Decomposition: Perform eigenvalue decomposition, mapping each node
to a low-dimensional embedding</li>
<li>Grouping: Cluster based on low-dimensional embedding</li>
</ul></li>
<li><p>Problem Definition: Graph partition, dividing graph nodes into
mutually exclusive sets</p></li>
<li><p>A good partition should maximize internal connections and
minimize inter-set connections</p></li>
<li><p>Define cut(A,B) as the sum of connection weights between nodes in
sets A and B</p></li>
<li><p>To consider internal connections, define vol(A) as the sum of
node degrees within A</p></li>
<li><p>Conductance metric:</p>
<p><span class="math display">\[
\phi(A, B)=\frac{\operatorname{cut}(A, B)}{\min (\operatorname{vol}(A),
\operatorname{vol}(B))}
\]</span></p></li>
<li><p>Finding a good partition is NP-hard</p></li>
<li><p>Spectral-based partitioning details omitted for brevity due to
mathematical complexity</p></li>
<li><p>Spectral clustering approaches:</p>
<ul>
<li>Preprocessing: Construct Laplacian matrix</li>
<li>Decomposition: Eigenvalue decomposition of L matrix</li>
<li>Grouping: Sort nodes by component values, find a split value</li>
<li>Visualization shows optimal splits correspond well to clustering
<img data-src="https://s1.ax1x.com/2020/03/31/GKd2KH.png"
alt="GKd2KH.png" /></li>
<li>Two multi-class clustering methods:
<ol type="1">
<li>Iterative clustering</li>
<li>K-class approach using k eigenvectors and k-means</li>
</ol></li>
<li>Determine number of clusters by largest gap between k-th and
(k-1)-th eigenvalues</li>
</ul></li>
<li><p>Motif-based spectral clustering</p>
<ul>
<li>Upgrade edge concept to motifs</li>
<li>Construct a new graph based on motifs</li>
<li>Perform spectral clustering on the new graph</li>
</ul></li>
</ul>
<h1 id="message-passing-and-node-classification">Message Passing and
Node Classification</h1>
<ul>
<li>Semi-supervised node classification in transductive learning</li>
<li>Three techniques:
<ul>
<li>Relational classification</li>
<li>Iterative classification</li>
<li>Belief propagation</li>
</ul></li>
<li>Key relationships:
<ul>
<li>Homophily</li>
<li>Influence</li>
<li>Confounding</li>
</ul></li>
<li>Collective classification makes Markov assumptions</li>
<li>Three approximate inference methods detailed</li>
<li>Methods include relational classification, iterative classification,
and belief propagation</li>
</ul>
<h1 id="graph-representation-learning">Graph Representation
Learning</h1>
<ul>
<li>Unsupervised method to learn task-independent node features</li>
<li>Framework similar to word embedding</li>
<li>Similarity defined through various methods:
<ul>
<li>DeepWalk: Based on random walk sequences</li>
<li>node2vec: Improved random walk strategy</li>
<li>TransE: Embedding for knowledge graphs</li>
</ul></li>
<li>Graph embedding techniques include:
<ul>
<li>Node-level averaging</li>
<li>Virtual node method</li>
<li>Anonymous Walk Embedding</li>
</ul></li>
</ul>
<h1 id="graph-neural-networks">Graph Neural Networks</h1>
<ul>
<li>Introduce deep neural networks for graph encoding</li>
<li>Key architectures:
<ul>
<li>GCN (Graph Convolutional Network)</li>
<li>GraphSage</li>
<li>Kipf GCN</li>
<li>GAT (Graph Attention Network)</li>
</ul></li>
<li>Training techniques:
<ul>
<li>Preprocessing tricks</li>
<li>Adam optimization</li>
<li>ReLU activation</li>
<li>No activation in output layer</li>
<li>Add bias to each layer</li>
</ul></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="网络随机图的属性">网络、随机图的属性</h1>
<ul>
<li>degree distribution:P(k)
<ul>
<li>即节点度的分布</li>
</ul></li>
<li>path length:h
<ul>
<li>path：即路径，可以与自己相交并通过同一条边多次</li>
<li>distance：两点之间最短路径</li>
<li>diameter：一个图的直径即该图上任意两点最短路径的最大值</li>
<li>average path length：distance之和除以节点对数量</li>
</ul></li>
<li>clustering coefficient:C
<ul>
<li><p>衡量了节点的邻居的连接状况：邻居之间相连的边数除以节点的度：</p>
<p><span class="math display">\[
C_{i}=\frac{2 e_{i}}{k_{i}\left(k_{i}-1\right)}
\]</span></p></li>
</ul></li>
<li>connected components
<ul>
<li>largest component(giant
component)的size称为connectivity，即最大连通子图的节点数</li>
</ul></li>
<li>Random Graph Model
<ul>
<li><p>将图看成随机过程生成的结果，两个参数，n,p，即总共n个节点，每条边独立同分布按概率p建立，显然这两个参数不足以唯一确定一个图，考虑这样一个图上的degree
distribution, path length以及clustering coefficient</p></li>
<li><p>P(k):</p>
<p><span class="math display">\[
P(k)=\left(\begin{array}{c}
n-1 \\
k
\end{array}\right) p^{k}(1-p)^{n-1-k}
\]</span></p></li>
<li><p>h:Olog(n)</p></li>
<li><p>C:</p>
<p><span class="math display">\[
=\frac{p \cdot
k_{i}\left(k_{i}-1\right)}{k_{i}\left(k_{i}-1\right)}=p=\frac{\bar{k}}{n-1}
\approx \frac{\bar{k}}{n}
\]</span></p></li>
<li><p>connectivity:随着p的增大，图越来越可能出现连接子图，具体如下：
<img data-src="https://s1.ax1x.com/2020/03/30/GuQgQf.png"
alt="GuQgQf.png" /></p></li>
</ul></li>
<li>small world model
<ul>
<li>在内聚程度很高的图上依然有很短的直径 <a
href="https://imgchr.com/i/GulXgP"><img data-src="https://s1.ax1x.com/2020/03/30/GulXgP.png"
alt="GulXgP.png" /></a></li>
<li>如何构造这样的图？首先以高内聚长直径的图作为起始图，在其中引入shortcut就可以了：
<img data-src="https://s1.ax1x.com/2020/03/30/Gu3QeS.png"
alt="Gu3QeS.png" /></li>
</ul></li>
<li>Kronecker graphs：递归的产生 large realistic graphs</li>
</ul>
<h1 id="图的特征纹理子图小图">图的特征：纹理、子图、小图</h1>
<ul>
<li><p>子图，以三个节点构成的子图为例： <img data-src="https://s1.ax1x.com/2020/03/30/Gu8hj0.png"
alt="Gu8hj0.png" /></p></li>
<li><p>这里假设所有的节点都是相同的，子图只关注节点和边构成的结构特征。假如用一个significance来衡量每个子图，那么就可以构建一个feature
vector。</p></li>
<li><p>定义纹理：motifs,“recurring, significant patterns of
interconnections”，即图中出现很多次，具有显著程度（More frequent than
expected, i.e., in randomly generated networks）的Small induced
subgraph</p></li>
<li><p>通过某一个motifs在真实图和随机图中的出现次数之比就可以定义这个motifs的significance：</p>
<p><span class="math display">\[
Z_{i}=\left(N_{i}^{\mathrm{real}}-\bar{N}_{i}^{\mathrm{rand}}\right) /
\operatorname{std}\left(N_{i}^{\mathrm{rand}}\right)
\]</span></p></li>
<li><p>RECAP算法：找出一个图的motifs,</p>
<ul>
<li>根据真实图，定义一个随机图，其拥有和真实图一样的节点数、边数和度分布</li>
<li>找出每一个子图在真实图和其对应的随机图上的significance，那么significance大的subgraph就是motifs</li>
</ul></li>
<li><p>小图(graphlet)：节点特征向量,graphlet的定义：connected
non-isomorphic
subgraphs，在graphlet中，我们灌注node-level的特征，例如三个节点只有两种graphlet，即三角形或者一条直线连接三个点。在三角形中，每个节点是等价的（相对于graphlet中其他节点而言），而在一条直线中，两端的节点等价，中间的节点是另一类。graphlet非常稀疏，n=10的graphlet有11716571种，且不算其中不同类的节点。</p></li>
<li><p>将度的概念泛化，Graphlet degree
vector（GDV）指一个节点接触到的graphlet，每一类占一个feature，值为接触到的这一类graphlet的数量：
<img data-src="https://s1.ax1x.com/2020/03/30/GutuVK.png"
alt="GutuVK.png" /></p></li>
<li><p>GDV衡量了节点的局部拓扑状态</p></li>
<li><p>寻找graphlet/motifs：三类算法</p>
<ul>
<li>Exact subgraph enumeration (ESU) [Wernicke 2006]</li>
<li>Kavosh [Kashani et al. 2009]</li>
<li>Subgraph sampling [Kashtan et al. 2004]</li>
</ul></li>
<li><p>图的同构：如何判断两个图是topologically equivalent的？</p>
<ul>
<li>定义roles:节点在图中所起的功能，通过结构信息来衡量，比如可以认为星型子图的中心节点具有相似的功能，或者直接归为一类功能，因此role也可以定义为A
collection of nodes which have similar positions in a
network。其和communities的区别：同一类role之间并不需要结构上互相连接或者有一定程度上的间接交互，而是他们在邻域结构内处于相同的位置</li>
<li>那么可以定义节点级别的structurally equivalent: Nodes are
structurally equivalent if they have the same relationships to all other
nodes</li>
<li>如何发现role?ROIX算法 <img data-src="https://s1.ax1x.com/2020/03/30/GuUd3Q.png" alt="GuUd3Q.png" /></li>
<li>recursive feature extraction：构建一些base
feature，然后不断的aggregate不断的迭代，例如取mean，取sum，再通过相关性剪枝</li>
<li>role extraction，实际上是一个矩阵的分解，将role看成latent
topic?：RolX uses non negative matrix factorization for clustering, MDL
for model selection, and KL divergence to measure likelihood。</li>
</ul></li>
</ul>
<h1 id="图谱聚类">图谱聚类</h1>
<ul>
<li><p>三步走</p>
<ul>
<li>预处理：得到一个能够包含整图信息的矩阵</li>
<li>分解：做特征分解，将每个节点映射到低维嵌入</li>
<li>分组：根据低维嵌入做聚类</li>
</ul></li>
<li><p>定义问题： graph
partition，即将图的节点分为几个互不相交的集合，一个好的划分应该保证集合内节点之间的连接尽量多，集合之间节点之间的连接尽量少。</p></li>
<li><p>定义cut(A,B)为AB两个集合节点之间的连接权重之和，那么最小切就是使得cut(A,B)最小的AB划分</p></li>
<li><p>如何再考虑上AB集合内部的连接，定义vol(A)为A内部节点的度加权之和，那么可以得到衡量partition的一个指标Conductance：</p>
<p><span class="math display">\[
\phi(A, B)=\frac{\operatorname{cut}(A, B)}{\min (\operatorname{vol}(A),
\operatorname{vol}(B))}
\]</span></p></li>
<li><p>找到一个好的partition是np-hard</p></li>
<li><p>基于图谱的划分：</p>
<ul>
<li><p>我们定义A为无向图的邻接矩阵，x为节点相关的向量，那么Ax得到就是邻域求和的结果</p>
<p><span class="math display">\[
y_{i}=\sum_{j=1}^{n} A_{i j} x_{j}=\sum_{(i, j) \in E} x_{j}
\]</span></p></li>
<li><p>我们定义"spectrum of
graph"，即图谱为A的特征向量构成的矩阵，按特征值<span
class="math inline">\(\lambda\)</span>大小排序，<span
class="math inline">\(Ax=\lambda x\)</span></p></li>
<li><p>那么假设图是d-regular，即所有节点的度都是d，那么很容易得到<span
class="math inline">\(\lambda = d,
x=(1,1,...,1)\)</span>是该图的一组特征值/特征向量，且可以证明d是最大的特征值</p></li>
<li><p>假如图是有两个连通分量，分别都是d-regular，那么对应的特征值依然是d，对应的特征向量有两个，分别是A分量里的节点置1，B置0以及vice
versa</p></li>
<li><p>这样根据node eigen
vector的分量是1还是0就可以做一个划分，将节点分为两部分。显然两个d-regular的分量，用少数几条边连接起来，这应该是整图的一个好的划分。</p></li>
<li><p>那么现在假设存在一个好的划分，整图是一个d-regular的图（这样两个分量就不是d-regular了，因为要考虑划分之间的连接所有节点的度才为d），分量之间有很少的连接。我们已知一个特征向量是全1的，且对应着最大的特征值d，那么直觉上第二大的特征值应该和d非常接近，因为我们知道断开的两个分量构成的图最大的特征值也为d，现在的图跟断开的图差别不是很大。而且由于特征向量之间相互正交而已知一个特征向量是全1，那么第二大的特征值对应的特征向量应该和为1，有正有负。类比于断开成两个分量的场景，我们也可以根据特征向量中分量的正负来划分节点。当然这都是直觉上的推测，下面引入Laplacian矩阵来详细说明。</p></li>
<li><p>邻接矩阵的性质：n*n方阵，对称，n个实特征值，特征向量相互正交</p></li>
<li><p>再定义度矩阵D，这是一个对角阵，第i个对角值存储第i个节点的度</p></li>
<li><p>定义Laplacian矩阵，<span
class="math inline">\(L=D-A\)</span>，显然<span
class="math inline">\(\lambda = 0,
x=(1,1,...,1)\)</span>是该图的一组特征值/特征向量。L矩阵的一些性质包括：</p>
<ul>
<li>所有特征值非负</li>
<li>半正定</li>
<li>可以分解为<span class="math inline">\(N^TN\)</span></li>
<li>实际上三个性质是等价的</li>
</ul></li>
<li><p>那么L矩阵的二次型的含义是什么？</p>
<p><span class="math display">\[
\begin{array}{l}
x^{T} L x=\sum_{i, j=1}^{n} L_{i j} x_{i} x_{j}=\sum_{i,
j=1}^{n}\left(D_{i j}-A_{i j}\right) x_{i} x_{j} \\
=\sum_{i} D_{i i} x_{i}^{2}-\sum_{(i, j) \in E} 2 x_{i} x_{j} \\
=\sum_{(i, j) \in E_{1}}\left(x_{i}^{2}+x_{j}^{2}-2 x_{i}
x_{j}\right)=\sum_{(i, j) \in E}\left(x_{i}-x_{j}\right)^{2}
\end{array}
\]</span></p></li>
<li><p>可以证明，二次型等价于矩阵的特征值加权x在对应特征向量上的坐标的平方求和</p>
<p><span class="math display">\[
x = \sum _{i=1}^n \alpha _i w_i \\
x^TMx = \sum _i \lambda _i \alpha _i^2 \\
\]</span></p></li>
<li><p>回到我们要找第二大的特征值，可以证明，对于对称阵：</p>
<p><span class="math display">\[
\lambda_{2}=\min _{x: x^{T} w_{1}=0} \frac{x^{T} M x}{x^{T} x} \\
\left(\mathbf{w}_{1} \text { is eigenvector corresponding to }
\lambda_{1}\right) \\
\]</span></p></li>
<li><p>当这个对称阵是L矩阵时，有</p>
<p><span class="math display">\[
\lambda _ 2 = min \frac{\sum _{i,j \in E}(x_i - x_j)^2}{\sum _i x_i^2}
\\
\sum x_i = 0 \\
\]</span></p></li>
<li><p>根据Fiedler'73的寻找最佳划分的方法，令节点label为1,-1来表示划分，所有节点求和为0来强制两个划分的集合大小一致，其提出的最佳划分是</p>
<p><span class="math display">\[
\arg \min _{y \in\{-1,+1\}^{n}} f(y)=\sum_{(i, j) \in
E}\left(y_{i}-y_{j}\right)^{2}
\]</span></p></li>
<li><p>可以发现将其label的限制从离散的1，-1放宽到实数值之后，等价于我们找L矩阵的第二大特征值和特征向量，第二特征值对应的特征向量分量的正负决定了划分情况（节点的分配情况）</p></li>
</ul></li>
<li><p>回到spectral clustering</p>
<ul>
<li>预处理：构建L矩阵</li>
<li>分解：对L矩阵做特征分解，得到每个节点在第二大特征值对应的特征向量上的分量</li>
<li>分组：将节点按分量大小排序，找一个切分值，大于切分值的划为一组，小于切分值的划为一组。怎么找切分值？naive的方法就是设为0，expensive的方法就是都试一遍，取最好的</li>
<li>通过可视化结果可以看到最优切分能够很好的对应聚类，而且多类也是一样，存在明显的分量差异：
<img data-src="https://s1.ax1x.com/2020/03/31/GKd2KH.png"
alt="GKd2KH.png" /></li>
<li>聚多类的两种方式：迭代式的聚类，每次聚两类；聚k类，找k个特征向量，而不仅仅是第二大的，然后相当于每个节点有k维特征，用k-means做聚类</li>
<li>怎么确定聚几类？聚成k类时，第k大特征值和第k-1大特征值之间的差应该最大</li>
</ul></li>
<li><p>基于motifs的spectral clustering</p>
<ul>
<li>当我们把边的概念升级到motifs时，就可以得到以motifs为特征的谱聚类</li>
<li>同样的，我们可以得到基于motifs的Conductance，直接找也依然是NP-hard</li>
<li>事实上，给定图G和motifs
M，可以构建一个新图，在新图上做谱聚类即可</li>
<li>定义新图为<span class="math inline">\(W\)</span>，则<span
class="math inline">\(W_{ij}^M\)</span> = # times edge (i,j)
participates in the motif M</li>
</ul></li>
</ul>
<h1 id="消息传递和节点分类">消息传递和节点分类</h1>
<ul>
<li>在直推式学习中，已知图上部分节点的label，如何根据图的结构和已知节点，推断出其他节点的label？这就是半监督节点分类</li>
<li>三种技术：
<ul>
<li>Relational classification</li>
<li>Iterative classification</li>
<li>Belief propagation</li>
</ul></li>
<li>关键在于利用网络（图）当中的关系（边），有三种重要的关系
<ul>
<li>Homophily: the tendency of individuals to associate and bond with
similar others</li>
<li>Influence: social connections can influence the individual
characteristics of a person</li>
<li>Confounding</li>
</ul></li>
<li>最直观的想法，相邻的节点具有相似的label</li>
<li>collective
classification做出了马尔可夫假设，即节点的分类只受其一阶邻居影响
<ul>
<li>load classifier：不使用网络信息，先根据节点特征做出分类</li>
<li>relational
classifier：学习到一个分类器，输入邻接节点的label和特征，输出中心节点的label</li>
<li>collective
inference：不断的传播网络的相关性，对每个节点迭代的使用relational
classifier</li>
</ul></li>
<li>精确的推断是np-hard的，这里介绍三种近似推断的方法：
<ul>
<li><p>Relational classification：对邻域节点的label概率加权求和</p>
<p><span class="math display">\[
P\left(Y_{i}=c\right)=\frac{1}{\sum_{(i, j) \in E} W(i, j)} \sum_{(i, j)
\in E} W(i, j) P\left(Y_{j}=c\right)
\]</span></p>
<p>缺点：不保证收敛，且没有用到节点特征</p></li>
<li><p>Iterative
classification：先对每个节点初始化一个特征向量，训练一个分类器（用有gold
label的节点训练），输入特征向量输出Label，这是一个Local
classifier，不考虑网络结构。等到每个节点都预测了label之后，根据网络结构进行消息传递，更新节点的特征向量，然后再用local
classifier预测label,如此迭代。一篇应用论文<a
href="https://cs.stanford.edu/~srijan/pubs/rev2-wsdm18.pdf">REV2:
Fraudulent User Prediction in Rating Platforms</a></p></li>
<li><p>Belief propagation：基于动态规划的方法，"Belief Propagation is a
dynamic programming approach to answering conditional probability
queries in a graphical model"</p>
<ul>
<li>定义label的矩阵<span
class="math inline">\(\psi\)</span>,其定义了邻域节点label为i时，中心节点的label为j的概率<span
class="math inline">\(\psi _{ij}\)</span></li>
<li>给定每个节点的初始概率（先验）<span
class="math inline">\(\phi\)</span></li>
<li><span class="math inline">\(m_{i \rightarrow
j}\left(Y_{j}\right)\)</span>代表i到j的belief，即邻居i对于节点j的label为<span
class="math inline">\(Y_j\)</span>的估计</li>
<li>则有： <a href="https://imgchr.com/i/GKoNtK"><img data-src="https://s1.ax1x.com/2020/03/31/GKoNtK.png" alt="GKoNtK.png" /></a>
即每次我们根据先验、label的转移概率、上一轮邻域节点的信念更新这一轮的信念；收敛之后根据先验和最终的邻域信念就可以推断中心节点的label</li>
<li>参考论文：Netprobe: A Fast and Scalable System for Fraud Detection
in Online Auction Networks Pandit et al., World Wide Web conference
2007</li>
</ul></li>
</ul></li>
</ul>
<h1 id="图表示学习">图表示学习</h1>
<ul>
<li>希望用无监督的方法学习到任务无关的节点通用特征</li>
<li>和word
embedding一样，一个目标是，网络中相邻的节点，在embedding当中应该相似，因此框架是
<ul>
<li>定义一个编码器，将节点编码成embedding</li>
<li>定义一个节点相似度函数</li>
<li>优化编码器参数，使得<span class="math inline">\(similarity(u,v)
\approx z_v^Tz_u\)</span></li>
</ul></li>
<li>最简单的方法：编码器只是一个embedding look
up，每个节点预先分配一个embedding
vector，例如DeepWalk,node2vec,TransE</li>
<li>因此主要区别在于如何定义similarity</li>
<li>DeepWalk：
<ul>
<li>每次随机游走采样出固定长序列，希望游走出发点和序列中的点比较相似，因此这里similarity定义为随机游走序列中的共现</li>
<li>优化目标是softmax之后的log-likelihood，和word
embedding一样，这里需要对所有节点计算来配分母，计算量太大，解决方案也是与word2vec类似，采用negative
sampling，只采样部分负样本，用Noise Contrastive
Estimation的目标函数来近似softmax之后的结果 <img data-src="https://s1.ax1x.com/2020/03/31/GMWPvn.png" alt="GMWPvn.png" /></li>
<li>假如我们对所有节点无差别的sample出一个定长的随机游走序列，那就是DeepWalk模型</li>
</ul></li>
<li>node2vec
<ul>
<li>同样是基于随机游走，node2vec在游走策略上做出了改进</li>
<li>游走时不仅做dfs，还做bfs：前者获取Global macroscopic
view；后者获取Local microscopic view</li>
<li>node2vec在两者之间做了插值，定义参数q为Moving outwards (DFS) vs.
inwards (BFS)的比例，同时还定义了返回参数p，即返回到初始点</li>
<li>具体的游走策略如下： <img data-src="https://s1.ax1x.com/2020/03/31/GMfTOI.png" alt="GMfTOI.png" /></li>
</ul></li>
<li>TransE
<ul>
<li><p>在知识图谱中，三元组(subject,predicate,object)表示了图中两个节点以及相连的边，都有feature
vector</p></li>
<li><p>TransE的目标是学习到所有实体节点和关系边的embedding，该算法将边解释为一种翻译，即predicate将subject翻译为Object</p></li>
<li><p>数学表示就是简单的<span class="math inline">\(u_{subject} +
v_{predicate} = u_{object}\)</span></p></li>
<li><p>训练采用了对比学习，采样负样本，用margin loss</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum \quad \nabla\left[\gamma+d(\boldsymbol{h}+\ell,
\boldsymbol{t})-d\left(\boldsymbol{h}^{\prime}+\ell,
\boldsymbol{t}^{\prime}\right)\right]_{+}\\
&amp;\left((h, \ell, t),\left(h^{\prime}, \ell, t^{\prime}\right)\right)
\in T_{\text {batch}} \quad
\end{aligned}
\]</span></p></li>
</ul></li>
<li>如何embedding整张图？
<ul>
<li>最简单的方法：学到节点的embedding，然后所有节点做平均或者求和</li>
<li>或者引入一个虚节点，其和所有节点相连接，然后跑node-level
embedding的方法，得到这个虚节点的embedding作为图的embedding，这种方法还可以得到子图的embedding</li>
<li>Anonymous Walk
Embedding：我们将节点匿名，用其在随机游走学列中第一次出现的index来代表，那么长度为3的游走序列就有111，112，121，122，123五种可能。那么就可以：
<ul>
<li>统计图中长度为l的所有序列，将图表示为匿名序列的特征向量，分量的值是该序列在图中出现的次数</li>
<li>对每张图采样m个匿名序列，再统计特征向量</li>
<li>学习到每个匿名序列的embedding，进而得到图的embedding。类似于语言模型，已知前n个匿名序列，预测第n+1个匿名序列，建立模型来学习参数</li>
</ul></li>
</ul></li>
</ul>
<h1 id="图神经网络">图神经网络</h1>
<ul>
<li>基于随机游走的方法其实是在similarity这部分做文章，在encoder这一块依然采用最简单的embedding
lookup</li>
<li>图神经网络就为encoder引入了deep neural network</li>
<li>GCN
<ul>
<li>idea：邻接关系定义了计算图，GCN利用图的结构来传播信息，更新节点的embedding</li>
<li>通过aggregate邻域节点来生成node
embedding，aggregate的过程使用神经网络，每一个节点基于其邻接状态定义了一个计算图</li>
<li>最简单的aggregate：收集邻域节点的embedding，做平均，然后当前节点embedding和邻域平均embedding做拼接输入一个神经网络，获得当前节点下一层的embedding:
<a href="https://imgchr.com/i/GQNyxe"><img data-src="https://s1.ax1x.com/2020/03/31/GQNyxe.png"
alt="GQNyxe.png" /></a></li>
<li>这样就完成了encoder的定义，可以直接进行supervised
learning，也可以接上random walks里的各种无监督目标函数来训练node
embedding</li>
<li>由于aggregate的参数在所有节点上共享，因此对于图上其他未知的节点也可以使用同一套encoder，新的一张图也可以。</li>
</ul></li>
<li>GraphSage
<ul>
<li>在aggregate的形式上做了进一步扩展</li>
<li>除了mean，其实任意的将多个embedding映射到一个embedding的函数都可以用来做聚合，例如pool，例如LSTM，</li>
</ul></li>
<li>Kipf GCN
<ul>
<li>将每一层的aggregate&amp;update操作写成矩阵形式，就可以利用高效的稀疏矩阵操作来提速
<img data-src="https://s1.ax1x.com/2020/03/31/GQaGnJ.png"
alt="GQaGnJ.png" /></li>
<li>其中的A都加了自环，邻域中包含了自己，而不是邻域与自己拼接，这里稍有不同。Kipf的形式其实是加权求和时权重做了对称归一化，考虑了邻域节点的度，而不仅仅考虑自身的度。</li>
</ul></li>
<li>GAT
<ul>
<li>可以看到GCN和GraphSage在聚合邻域节点时，不同邻居节点的权重都是一样的，那么自然可以使用注意力机制，根据邻域节点和自身的embedding计算出注意力作为权重再聚合。</li>
<li>参考transformer，使用multi-head attention</li>
</ul></li>
<li>GNN的训练技巧
<ul>
<li>预处理很重要，Use renormalization tricks、 Variance-scaled
initialization、 Network data whitening</li>
<li>adam优化，relu激活</li>
<li>output layer不需要激活</li>
<li>每一层记得加bias</li>
</ul></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>gnn</tag>
        <tag>math</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for my Android app - Melodia</title>
    <url>/2017/03/09/dachuang/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/881fef7085a3a58b245072cf7c2b8e81.png" width="500"/></p>
<p>The school's innovation project has a simple app that implements the
following functions: recording sound and saving it as a wav file, using
JSON to communicate with the server, uploading the wav file to the
server, converting it to a midi file on the server, downloading the midi
file and sheet music from the server for playback. At the same time, the
modified electronic piano can also communicate with the server, with the
phone providing auxiliary parameters to the electronic piano, which
reads the intermediate key value file of the music from the server via
Arduino to play.</p>
<span id="more"></span>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0o26O.gif"
alt="i0o26O.gif" /> cover use <a
href="https://github.com/qiao">qiao</a>'s <a
href="https://github.com/qiao/euphony">euphony</a></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="midi-playback">midi playback</h1>
<p>Invoke MediaPlayer class for playback; due to irresistible factors,
only Android 5.1 can be used, and since there is no MIDI library, a
simple playback is implemented.</p>
<ul>
<li>MediaPlayer can access and play media files using four methods:
external storage, assert, self-built raw folder, or URI</li>
<li>From the raw folder, read directly using player =
MediaPlayer.create(this, R.raw.test1)</li>
<li>Uri or external storage read
new-&gt;setDataSource-&gt;prepare-&gt;start</li>
</ul>
<h1 id="recording-and-replaying-sound">Recording and replaying
sound</h1>
<p>Refer to the use of AudioRecord in Android</p>
<pre><code>    private class RecordTask extends AsyncTask&lt;Void, Integer, Void&gt; {
        @Override
        protected Void doInBackground(Void... arg0) {
            isRecording = true;
            try {
                //开通输出流到指定的文件
                DataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(pcmFile)));
                //根据定义好的几个配置，来获取合适的缓冲大小
                int bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);
                //实例化AudioRecord
                AudioRecord record = new AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);
                //定义缓冲
                short[] buffer = new short[bufferSize];

                //开始录制
                record.startRecording();

                int r = 0; //存储录制进度
                //定义循环，根据isRecording的值来判断是否继续录制
                while (isRecording) {
                    //从bufferSize中读取字节，返回读取的short个数
                    //这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决
                    int bufferReadResult = record.read(buffer, 0, buffer.length);
                    //循环将buffer中的音频数据写入到OutputStream中
                    for (int i = 0; i &lt; bufferReadResult; i++) {
                        dos.writeShort(buffer[i]);
                    }
                    publishProgress(new Integer(r)); //向UI线程报告当前进度
                    r++; //自增进度值
                }
                //录制结束
                record.stop();
                convertWaveFile();
                dos.close();
            } catch (Exception e) {
                // TODO: handle exception
            }
            return null;
        }
    }</code></pre>
<h1 id="pcm-header-file-converted-to-wav">pcm header file converted to
wav</h1>
<p>Because the recording is in a raw file, in PCM format, it requires
the addition of a WAV header manually</p>
<pre><code>    private void WriteWaveFileHeader(FileOutputStream out, long totalAudioLen, long totalDataLen, long longSampleRate,
                                     int channels, long byteRate) throws IOException {
        byte[] header = new byte[45];
        header[0] = &#39;R&#39;; // RIFF
        header[1] = &#39;I&#39;;
        header[2] = &#39;F&#39;;
        header[3] = &#39;F&#39;;
        header[4] = (byte) (totalDataLen &amp; 0xff);//数据大小
        header[5] = (byte) ((totalDataLen &gt;&gt; 8) &amp; 0xff);
        header[6] = (byte) ((totalDataLen &gt;&gt; 16) &amp; 0xff);
        header[7] = (byte) ((totalDataLen &gt;&gt; 24) &amp; 0xff);
        header[8] = &#39;W&#39;;//WAVE
        header[9] = &#39;A&#39;;
        header[10] = &#39;V&#39;;
        header[11] = &#39;E&#39;;
        //FMT Chunk
        header[12] = &#39;f&#39;; // &#39;fmt &#39;
        header[13] = &#39;m&#39;;
        header[14] = &#39;t&#39;;
        header[15] = &#39; &#39;;//过渡字节
        //数据大小
        header[16] = 16; // 4 bytes: size of &#39;fmt &#39; chunk
        header[17] = 0;
        header[18] = 0;
        header[19] = 0;
        //编码方式 10H为PCM编码格式
        header[20] = 1; // format = 1
        header[21] = 0;
        //通道数
        header[22] = (byte) channels;
        header[23] = 0;
        //采样率，每个通道的播放速度
        header[24] = (byte) (longSampleRate &amp; 0xff);
        header[25] = (byte) ((longSampleRate &gt;&gt; 8) &amp; 0xff);
        header[26] = (byte) ((longSampleRate &gt;&gt; 16) &amp; 0xff);
        header[27] = (byte) ((longSampleRate &gt;&gt; 24) &amp; 0xff);
        //音频数据传送速率,采样率*通道数*采样深度/8
        header[28] = (byte) (byteRate &amp; 0xff);
        header[29] = (byte) ((byteRate &gt;&gt; 8) &amp; 0xff);
        header[30] = (byte) ((byteRate &gt;&gt; 16) &amp; 0xff);
        header[31] = (byte) ((byteRate &gt;&gt; 24) &amp; 0xff);
        // 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数
        header[32] = (byte) (1 * 16 / 8);
        header[33] = 0;
        //每个样本的数据位数
        header[34] = 16;
        header[35] = 0;
        //Data chunk
        header[36] = &#39;d&#39;;//data
        header[37] = &#39;a&#39;;
        header[38] = &#39;t&#39;;
        header[39] = &#39;a&#39;;
        header[40] = (byte) (totalAudioLen &amp; 0xff);
        header[41] = (byte) ((totalAudioLen &gt;&gt; 8) &amp; 0xff);
        header[42] = (byte) ((totalAudioLen &gt;&gt; 16) &amp; 0xff);
        header[43] = (byte) ((totalAudioLen &gt;&gt; 24) &amp; 0xff);
        header[44] = 0;
        out.write(header, 0, 45);
    }</code></pre>
<h1 id="json-transmission-and-reception">JSON transmission and
reception</h1>
<p>Based on our actual situation, use JSON for sending, storing three
parameters and the WAV content, as the WAV recording is short, the
entire WAV can be written into the JSON. Send the JSON twice, first
sending the parameters and the file, obtaining the timestamp with the
MD5 encoding, and then secondly adding this timestamp to the JSON to
request the corresponding MIDI file</p>
<pre><code>    private JSONObject makejson(int request, String identifycode, String data) {
        if (identifycode == &quot;a&quot;) {
            try {
                JSONObject pack = new JSONObject();
                pack.put(&quot;request&quot;, request);
                JSONObject config = new JSONObject();
                config.put(&quot;n&quot;, lowf);
                config.put(&quot;m&quot;, highf);
                config.put(&quot;w&quot;, interval);
                pack.put(&quot;config&quot;, config);
                pack.put(&quot;data&quot;, data);
                return pack;
            } catch (JSONException e) {
                e.printStackTrace();
            }
        } else {
            try {
                JSONObject pack = new JSONObject();
                pack.put(&quot;request&quot;, request);
                pack.put(&quot;config&quot;, &quot;&quot;);
                pack.put(&quot;data&quot;, identifycode);
                return pack;
            } catch (JSONException e) {
                e.printStackTrace();
            }

        }
        return null;
    }</code></pre>
<h1 id="socket-communication">socket communication</h1>
<p>A separate thread is opened to start the socket, and another thread
is used to send and receive JSON twice. Note that when sending and
receiving JSON, the JSON string should be decoded and encoded with
base64, as Java's own string may contain errors. Additionally, because
the wav string is long, the server receives it in chunks. The normal
practice is to add a dictionary item to store the wav length and read
wav according to the length, but here we take a shortcut by adding a
special character segment at the end of the file to determine whether
the reception is complete, "endbidou". Don't ask me what it means; it's
something thought up by brothers who do conversion algorithms</p>
<pre><code>private class MsgThread extends Thread {
        @Override
        public void run() {
            File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + &quot;/data/files/Melodia.wav&quot;);
            FileInputStream reader = null;
            try {
                reader = new FileInputStream(file);
                int len = reader.available();
                byte[] buff = new byte[len];
                reader.read(buff);
                String data = Base64.encodeToString(buff, Base64.DEFAULT);
                String senda = makejson(1, &quot;a&quot;, data).toString();
                Log.i(TAG, &quot;request1: &quot; + senda);
                OutputStream os = null;
                InputStream is = null;
                DataInputStream in = null;
                try {
                    os = soc.getOutputStream();
                    BufferedReader bra = null;
                    os.write(senda.getBytes());
                    os.write(&quot;endbidou1&quot;.getBytes());
                    os.flush();
                    Log.i(TAG, &quot;request1 send successful&quot;);
                    if (soc.isConnected()) {
                        is = soc.getInputStream();
                        bra = new BufferedReader(new InputStreamReader(is));
                        md5 = bra.readLine();
                        Log.i(TAG, &quot;md5: &quot; + md5);
                        bra.close();
                    } else
                        Log.i(TAG, &quot;socket closed while reading&quot;);
                } catch (IOException e) {
                    e.printStackTrace();
                }
                soc.close();
                startflag = 1;

                StartThread st = new StartThread();
                st.start();

                while (soc.isClosed()) ;

                String sendb = makejson(2, md5, &quot;request2&quot;).toString();
                Log.i(TAG, &quot;request2: &quot; + sendb);
                os = soc.getOutputStream();
                os.write(sendb.getBytes());
                os.write(&quot;endbidou1&quot;.getBytes());
                os.flush();
                Log.i(TAG, &quot;request2 send successful&quot;);

                is = soc.getInputStream();
                byte buffer[] = new byte[1024 * 100];
                is.read(buffer);
                Log.i(TAG, &quot;midifilecontent: &quot; + buffer.toString());
                soc.close();
                File filemid = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + &quot;/data/files/Melodia.mid&quot;);
                FileOutputStream writer = null;
                writer = new FileOutputStream(filemid);
                writer.write(buffer);
                writer.close();
                Message msg = myhandler.obtainMessage();
                msg.what = 1;
                myhandler.sendMessage(msg);
            } catch (IOException e) {
                e.printStackTrace();
            }


        }
    }</code></pre>
<h1 id="recording-effects">Recording Effects</h1>
<p>Audio image animation effect from Github: ShineButton. Additionally,
an effect has been made for the recording button, press to record,
release to complete, and slide out a certain distance to cancel</p>
<pre><code>    fabrecord.setOnTouchListener(new View.OnTouchListener() {
                @Override
                public boolean onTouch(View v, MotionEvent event) {
                    switch (event.getAction()) {
                        case MotionEvent.ACTION_DOWN:
                            uploadbt.setVisibility(View.INVISIBLE);
                            if (isUploadingIcon) {
                                isPressUpload = false;
                                uploadbt.performClick();
                                isPressUpload = true;
                                isUploadingIcon = !isUploadingIcon;
                            }

                            Log.i(TAG, &quot;ACTION_DOWN&quot;);
                            if (!shinebtstatus) {
                                shinebt.performClick();
                                shinebtstatus = true;
                            }
                            ox = event.getX();
                            oy = event.getY();

                            isRecording = true;
                            recLen = 0;
                            recTime = 0;
                            pb.setValue(0);
                            fabrecord.setImageResource(R.drawable.ic_stop_white_24dp);
                            Snackbar.make(fabrecord, &quot;开始录音&quot;, Snackbar.LENGTH_SHORT)
                                    .setAction(&quot;Action&quot;, null).show();

                            recorder = new RecordTask();
                            recorder.execute();
                            handler.postDelayed(runrecord, 0);

                            break;
                        case MotionEvent.ACTION_UP:
                            handler.removeCallbacks(runrecord);
                            Log.i(TAG, &quot;ACTION_UP&quot;);
                            if (shinebtstatus) {
                                shinebt.performClick();
                                shinebtstatus = false;
                            }
                            float x1 = event.getX();
                            float y1 = event.getY();
                            float dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);

                            isRecording = false;
                            pb.setValue(0);
                            fabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);
                            if (dis1 &gt; 30000) {
                                Snackbar.make(fabrecord, &quot;取消录音&quot;, Snackbar.LENGTH_SHORT)
                                        .setAction(&quot;Action&quot;, null).show();
                            } else {
                                if (!isUploadingIcon) {
                                    uploadbt.setVisibility(View.VISIBLE);
                                    isPressUpload = false;
                                    uploadbt.performClick();
                                    isPressUpload = true;
                                    isUploadingIcon = !isUploadingIcon;
                                } else {

                                }

                                Snackbar.make(fabrecord, &quot;录音完成&quot;, Snackbar.LENGTH_SHORT)
                                        .setAction(&quot;Action&quot;, null).show();
                                handler.postDelayed(runreplay, 0);
                                replay();
                            }
                            break;
                        case MotionEvent.ACTION_MOVE:
                            float x2 = event.getX();
                            float y2 = event.getY();
                            float dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);
                            if (dis2 &gt; 30000) {
                                fabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);
                            } else {
                                fabrecord.setImageResource(R.drawable.ic_stop_white_24dp);
                            }
                            break;
                    }
                    return true;
                }
            });</code></pre>
<h1 id="display-musical-score">Display musical score</h1>
<ul>
<li><p>Initially, the plan was to send and receive images through
sockets, but later it was deemed too 麻烦, so the scheme was changed to
use Apache to generate a corresponding image link for each conversion,
which can be accessed online directly via timestamp and MD5, and if the
image needs to be shared, it is first saved locally before sharing</p>
<pre><code>public void init() {
   md5 = getArguments().getString(&quot;md5&quot;);
   final String imageUri = &quot;服务器地址&quot; + md5 + &quot;_1.png&quot;;
   Log.i(&quot;play&quot;, &quot;pngfile: &quot; + imageUri);
   new Handler().postDelayed(new Runnable() {
       public void run() {
           //execute the task
           imageLoader.displayImage(imageUri, showpic);
       }
   }, 2000);

}</code></pre></li>
</ul>
<h1 id="communication-with-an-electronic-keyboard">Communication with an
Electronic Keyboard</h1>
<ul>
<li><p>Similar to uploading to a server, it also uses socket
communication. After the electronic piano is modified, it receives two
parameters, octave and speed, from the mobile client. Upon receiving the
parameters, the Arduino plays the music and then disconnects the
connection</p>
<pre><code>pianobt.setOnClickListener(new View.OnClickListener() {
           @Override
           public void onClick(View v) {
               if (!isconnected) {
                   pianoaddr = etpianoaddr.getText().toString();
                   pianoport = Integer.valueOf(etpianoport.getText().toString());
                   param[0] = 0x30;
                   StartThread st = new StartThread();
                   st.start();
                   while (!isconnected) ;
                   MsgThread ms = new MsgThread();
                   ms.start();
                   YoYo.with(Techniques.Wobble)
                           .duration(300)
                           .repeat(6)
                           .playOn(seekBaroctave);
                   while (soc.isConnected()) ;
                   try {
                       soc.close();
                   } catch (IOException e) {
                       e.printStackTrace();
                   }
                   isconnected = false;
                   Log.i(&quot;piano&quot;, &quot;socket closed&quot;);
               }


          }
      });

      samplebt.setOnClickListener(new View.OnClickListener() {
          @Override
          public void onClick(View v) {
              pianoaddr = etpianoaddr.getText().toString();
              pianoport = Integer.valueOf(etpianoport.getText().toString());
              param[0] = 0x31;
              StartThread st = new StartThread();
              st.start();
              while (!isconnected) ;
              MsgThread ms = new MsgThread();
              ms.start();
              YoYo.with(Techniques.Wobble)
                      .duration(300)
                      .repeat(6)
                      .playOn(seekBaroctave);
              while (soc.isConnected()) ;
              try {
                  soc.close();
              } catch (IOException e) {
                  e.printStackTrace();
              }
              isconnected = false;
              Log.i(&quot;piano&quot;, &quot;socket closed&quot;);

          }
      });


  }

  private class StartThread extends Thread {
      @Override
      public void run() {
          try {
              soc = new Socket(pianoaddr, pianoport);
              if (soc.isConnected()) {//成功连接获取soc对象则发送成功消息
                  Log.i(&quot;piano&quot;, &quot;piano is Connected&quot;);
                  if (!isconnected)
                      isconnected = !isconnected;

              } else {
                  Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                          .setAction(&quot;Action&quot;, null).show();
                  Log.i(&quot;piano&quot;, &quot;Connect Failed&quot;);
                  soc.close();
              }
          } catch (IOException e) {
              Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();
              Log.i(&quot;piano&quot;, &quot;Connect Failed&quot;);
              e.printStackTrace();
          }
      }
  }

  private class MsgThread extends Thread {
      @Override
      public void run() {
          try {
              OutputStream os = soc.getOutputStream();
              os.write(param);
              os.flush();
              Log.i(&quot;piano&quot;, &quot;piano msg send successful&quot;);
              Snackbar.make(pianobt, &quot;正在启动启动电子琴教学&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();

              soc.close();
          } catch (IOException e) {
              Log.i(&quot;piano&quot;, &quot;piano msg send successful failed&quot;);
              Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();
              e.printStackTrace();
          }

      }
  }</code></pre>
<h1 id="乐谱分享">乐谱分享</h1>
<ul>
<li>显示乐谱的是Github上一个魔改的ImageView:<a
href="https://github.com/boycy815/PinchImageView">PinchImageView</a></li>
<li>定义其长按事件，触发一个分享的intent <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        showpic.setOnLongClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnLongClickListener() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">onLongClick</span><span class="params">(View v)</span> &#123;</span><br><span class="line">                        <span class="type">Bitmap</span> <span class="variable">drawingCache</span> <span class="operator">=</span> getViewBitmap(showpic);</span><br><span class="line">                        <span class="keyword">if</span> (drawingCache == <span class="literal">null</span>) &#123;</span><br><span class="line">                            Log.i(<span class="string">&quot;play&quot;</span>, <span class="string">&quot;no img to save&quot;</span>);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">try</span> &#123;</span><br><span class="line">                                <span class="type">File</span> <span class="variable">imageFile</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(Environment.getExternalStorageDirectory(), <span class="string">&quot;saveImageview.jpg&quot;</span>);</span><br><span class="line">                                <span class="type">Toast</span> <span class="variable">toast</span> <span class="operator">=</span> Toast.makeText(getActivity(),</span><br><span class="line">                                        <span class="string">&quot;&quot;</span>, Toast.LENGTH_LONG);</span><br><span class="line">                                toast.setGravity(Gravity.TOP, <span class="number">0</span>, <span class="number">200</span>);</span><br><span class="line">                                toast.setText(<span class="string">&quot;分享图片&quot;</span>);</span><br><span class="line">                                toast.show();</span><br><span class="line">                                FileOutputStream outStream;</span><br><span class="line">                                outStream = <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(imageFile);</span><br><span class="line">                                drawingCache.compress(Bitmap.CompressFormat.JPEG, <span class="number">100</span>, outStream);</span><br><span class="line">                                outStream.flush();</span><br><span class="line">                                outStream.close();</span><br><span class="line">    </span><br><span class="line">                                <span class="type">Intent</span> <span class="variable">sendIntent</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Intent</span>();</span><br><span class="line">                                sendIntent.setAction(Intent.ACTION_SEND);</span><br><span class="line">                                sendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class="line">                                sendIntent.setType(<span class="string">&quot;image/png&quot;</span>);</span><br><span class="line">                                getActivity().startActivity(Intent.createChooser(sendIntent, <span class="string">&quot;分享到&quot;</span>));</span><br><span class="line">    </span><br><span class="line">                            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                                Log.i(<span class="string">&quot;play&quot;</span>, <span class="string">&quot;share img wrong&quot;</span>);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;% endlang_content %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% lang_content zh %&#125;</span><br><span class="line"># midi播放</span><br><span class="line"></span><br><span class="line">调用MediaPlayer类播放，因为不可抗因素，只能用android5<span class="number">.1</span>，没有midi库，就做简单的播放</span><br><span class="line"></span><br><span class="line">- MediaPlayer可以用外部存储，<span class="keyword">assert</span>,自建raw文件夹或者uri四种方式访问媒体文件并播放</span><br><span class="line">- 从raw文件夹中读取可以直接用player = MediaPlayer.create(<span class="built_in">this</span>, R.raw.test1)</span><br><span class="line">- Uri或者外部存储读取<span class="keyword">new</span>-&gt;setDataSource-&gt;prepare-&gt;start</span><br><span class="line"></span><br><span class="line"># 录制声音并重放</span><br><span class="line"></span><br><span class="line">参考[android中AudioRecord使用](http:<span class="comment">//blog.csdn.net/jiangliloveyou/article/details/11218555)</span></span><br><span class="line"></span><br><span class="line">```Java</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">RecordTask</span> <span class="keyword">extends</span> <span class="title class_">AsyncTask</span>&lt;Void, Integer, Void&gt; &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> Void <span class="title function_">doInBackground</span><span class="params">(Void... arg0)</span> &#123;</span><br><span class="line">            isRecording = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//开通输出流到指定的文件</span></span><br><span class="line">                <span class="type">DataOutputStream</span> <span class="variable">dos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DataOutputStream</span>(<span class="keyword">new</span> <span class="title class_">BufferedOutputStream</span>(<span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(pcmFile)));</span><br><span class="line">                <span class="comment">//根据定义好的几个配置，来获取合适的缓冲大小</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">bufferSize</span> <span class="operator">=</span> AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);</span><br><span class="line">                <span class="comment">//实例化AudioRecord</span></span><br><span class="line">                <span class="type">AudioRecord</span> <span class="variable">record</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">AudioRecord</span>(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);</span><br><span class="line">                <span class="comment">//定义缓冲</span></span><br><span class="line">                <span class="type">short</span>[] buffer = <span class="keyword">new</span> <span class="title class_">short</span>[bufferSize];</span><br><span class="line"></span><br><span class="line">                <span class="comment">//开始录制</span></span><br><span class="line">                record.startRecording();</span><br><span class="line"></span><br><span class="line">                <span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> <span class="number">0</span>; <span class="comment">//存储录制进度</span></span><br><span class="line">                <span class="comment">//定义循环，根据isRecording的值来判断是否继续录制</span></span><br><span class="line">                <span class="keyword">while</span> (isRecording) &#123;</span><br><span class="line">                    <span class="comment">//从bufferSize中读取字节，返回读取的short个数</span></span><br><span class="line">                    <span class="comment">//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决</span></span><br><span class="line">                    <span class="type">int</span> <span class="variable">bufferReadResult</span> <span class="operator">=</span> record.read(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">                    <span class="comment">//循环将buffer中的音频数据写入到OutputStream中</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; bufferReadResult; i++) &#123;</span><br><span class="line">                        dos.writeShort(buffer[i]);</span><br><span class="line">                    &#125;</span><br><span class="line">                    publishProgress(<span class="keyword">new</span> <span class="title class_">Integer</span>(r)); <span class="comment">//向UI线程报告当前进度</span></span><br><span class="line">                    r++; <span class="comment">//自增进度值</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//录制结束</span></span><br><span class="line">                record.stop();</span><br><span class="line">                convertWaveFile();</span><br><span class="line">                dos.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                <span class="comment">// <span class="doctag">TODO:</span> handle exception</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h1 id="pcm写头文件转成wav">pcm写头文件转成wav</h1>
<p>因为录制的是裸文件，pcm格式，需要自己加上wav头</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">WriteWaveFileHeader</span><span class="params">(FileOutputStream out, <span class="type">long</span> totalAudioLen, <span class="type">long</span> totalDataLen, <span class="type">long</span> longSampleRate,</span></span><br><span class="line"><span class="params">                                 <span class="type">int</span> channels, <span class="type">long</span> byteRate)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    <span class="type">byte</span>[] header = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">45</span>];</span><br><span class="line">    header[<span class="number">0</span>] = <span class="string">&#x27;R&#x27;</span>; <span class="comment">// RIFF</span></span><br><span class="line">    header[<span class="number">1</span>] = <span class="string">&#x27;I&#x27;</span>;</span><br><span class="line">    header[<span class="number">2</span>] = <span class="string">&#x27;F&#x27;</span>;</span><br><span class="line">    header[<span class="number">3</span>] = <span class="string">&#x27;F&#x27;</span>;</span><br><span class="line">    header[<span class="number">4</span>] = (<span class="type">byte</span>) (totalDataLen &amp; <span class="number">0xff</span>);<span class="comment">//数据大小</span></span><br><span class="line">    header[<span class="number">5</span>] = (<span class="type">byte</span>) ((totalDataLen &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">6</span>] = (<span class="type">byte</span>) ((totalDataLen &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">7</span>] = (<span class="type">byte</span>) ((totalDataLen &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">8</span>] = <span class="string">&#x27;W&#x27;</span>;<span class="comment">//WAVE</span></span><br><span class="line">    header[<span class="number">9</span>] = <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line">    header[<span class="number">10</span>] = <span class="string">&#x27;V&#x27;</span>;</span><br><span class="line">    header[<span class="number">11</span>] = <span class="string">&#x27;E&#x27;</span>;</span><br><span class="line">    <span class="comment">//FMT Chunk</span></span><br><span class="line">    header[<span class="number">12</span>] = <span class="string">&#x27;f&#x27;</span>; <span class="comment">// &#x27;fmt &#x27;</span></span><br><span class="line">    header[<span class="number">13</span>] = <span class="string">&#x27;m&#x27;</span>;</span><br><span class="line">    header[<span class="number">14</span>] = <span class="string">&#x27;t&#x27;</span>;</span><br><span class="line">    header[<span class="number">15</span>] = <span class="string">&#x27; &#x27;</span>;<span class="comment">//过渡字节</span></span><br><span class="line">    <span class="comment">//数据大小</span></span><br><span class="line">    header[<span class="number">16</span>] = <span class="number">16</span>; <span class="comment">// 4 bytes: size of &#x27;fmt &#x27; chunk</span></span><br><span class="line">    header[<span class="number">17</span>] = <span class="number">0</span>;</span><br><span class="line">    header[<span class="number">18</span>] = <span class="number">0</span>;</span><br><span class="line">    header[<span class="number">19</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//编码方式 10H为PCM编码格式</span></span><br><span class="line">    header[<span class="number">20</span>] = <span class="number">1</span>; <span class="comment">// format = 1</span></span><br><span class="line">    header[<span class="number">21</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//通道数</span></span><br><span class="line">    header[<span class="number">22</span>] = (<span class="type">byte</span>) channels;</span><br><span class="line">    header[<span class="number">23</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//采样率，每个通道的播放速度</span></span><br><span class="line">    header[<span class="number">24</span>] = (<span class="type">byte</span>) (longSampleRate &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">25</span>] = (<span class="type">byte</span>) ((longSampleRate &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">26</span>] = (<span class="type">byte</span>) ((longSampleRate &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">27</span>] = (<span class="type">byte</span>) ((longSampleRate &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    <span class="comment">//音频数据传送速率,采样率*通道数*采样深度/8</span></span><br><span class="line">    header[<span class="number">28</span>] = (<span class="type">byte</span>) (byteRate &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">29</span>] = (<span class="type">byte</span>) ((byteRate &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">30</span>] = (<span class="type">byte</span>) ((byteRate &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">31</span>] = (<span class="type">byte</span>) ((byteRate &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    <span class="comment">// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数</span></span><br><span class="line">    header[<span class="number">32</span>] = (<span class="type">byte</span>) (<span class="number">1</span> * <span class="number">16</span> / <span class="number">8</span>);</span><br><span class="line">    header[<span class="number">33</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//每个样本的数据位数</span></span><br><span class="line">    header[<span class="number">34</span>] = <span class="number">16</span>;</span><br><span class="line">    header[<span class="number">35</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//Data chunk</span></span><br><span class="line">    header[<span class="number">36</span>] = <span class="string">&#x27;d&#x27;</span>;<span class="comment">//data</span></span><br><span class="line">    header[<span class="number">37</span>] = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    header[<span class="number">38</span>] = <span class="string">&#x27;t&#x27;</span>;</span><br><span class="line">    header[<span class="number">39</span>] = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    header[<span class="number">40</span>] = (<span class="type">byte</span>) (totalAudioLen &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">41</span>] = (<span class="type">byte</span>) ((totalAudioLen &gt;&gt; <span class="number">8</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">42</span>] = (<span class="type">byte</span>) ((totalAudioLen &gt;&gt; <span class="number">16</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">43</span>] = (<span class="type">byte</span>) ((totalAudioLen &gt;&gt; <span class="number">24</span>) &amp; <span class="number">0xff</span>);</span><br><span class="line">    header[<span class="number">44</span>] = <span class="number">0</span>;</span><br><span class="line">    out.write(header, <span class="number">0</span>, <span class="number">45</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="json收发">json收发</h1>
<p>根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中
json发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> JSONObject <span class="title function_">makejson</span><span class="params">(<span class="type">int</span> request, String identifycode, String data)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (identifycode == <span class="string">&quot;a&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">pack</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONObject</span>();</span><br><span class="line">            pack.put(<span class="string">&quot;request&quot;</span>, request);</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONObject</span>();</span><br><span class="line">            config.put(<span class="string">&quot;n&quot;</span>, lowf);</span><br><span class="line">            config.put(<span class="string">&quot;m&quot;</span>, highf);</span><br><span class="line">            config.put(<span class="string">&quot;w&quot;</span>, interval);</span><br><span class="line">            pack.put(<span class="string">&quot;config&quot;</span>, config);</span><br><span class="line">            pack.put(<span class="string">&quot;data&quot;</span>, data);</span><br><span class="line">            <span class="keyword">return</span> pack;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (JSONException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">pack</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JSONObject</span>();</span><br><span class="line">            pack.put(<span class="string">&quot;request&quot;</span>, request);</span><br><span class="line">            pack.put(<span class="string">&quot;config&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">            pack.put(<span class="string">&quot;data&quot;</span>, identifycode);</span><br><span class="line">            <span class="keyword">return</span> pack;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (JSONException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="socket通信">socket通信</h1>
<p>单开一个线程用于启动socket，再开一个线程写两次json收发
注意收发json时将json字符串用base64解码编码，java自己的string会存在错误
另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，"endbidou"，不要问我是什么意思，做转换算法的兄弟想的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">MsgThread</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="type">File</span> <span class="variable">file</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class="string">&quot;/data/files/Melodia.wav&quot;</span>);</span><br><span class="line">            <span class="type">FileInputStream</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                reader = <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(file);</span><br><span class="line">                <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> reader.available();</span><br><span class="line">                <span class="type">byte</span>[] buff = <span class="keyword">new</span> <span class="title class_">byte</span>[len];</span><br><span class="line">                reader.read(buff);</span><br><span class="line">                <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> Base64.encodeToString(buff, Base64.DEFAULT);</span><br><span class="line">                <span class="type">String</span> <span class="variable">senda</span> <span class="operator">=</span> makejson(<span class="number">1</span>, <span class="string">&quot;a&quot;</span>, data).toString();</span><br><span class="line">                Log.i(TAG, <span class="string">&quot;request1: &quot;</span> + senda);</span><br><span class="line">                <span class="type">OutputStream</span> <span class="variable">os</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                <span class="type">InputStream</span> <span class="variable">is</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                <span class="type">DataInputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    os = soc.getOutputStream();</span><br><span class="line">                    <span class="type">BufferedReader</span> <span class="variable">bra</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                    os.write(senda.getBytes());</span><br><span class="line">                    os.write(<span class="string">&quot;endbidou1&quot;</span>.getBytes());</span><br><span class="line">                    os.flush();</span><br><span class="line">                    Log.i(TAG, <span class="string">&quot;request1 send successful&quot;</span>);</span><br><span class="line">                    <span class="keyword">if</span> (soc.isConnected()) &#123;</span><br><span class="line">                        is = soc.getInputStream();</span><br><span class="line">                        bra = <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(is));</span><br><span class="line">                        md5 = bra.readLine();</span><br><span class="line">                        Log.i(TAG, <span class="string">&quot;md5: &quot;</span> + md5);</span><br><span class="line">                        bra.close();</span><br><span class="line">                    &#125; <span class="keyword">else</span></span><br><span class="line">                        Log.i(TAG, <span class="string">&quot;socket closed while reading&quot;</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                soc.close();</span><br><span class="line">                startflag = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">                <span class="type">StartThread</span> <span class="variable">st</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StartThread</span>();</span><br><span class="line">                st.start();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (soc.isClosed()) ;</span><br><span class="line"></span><br><span class="line">                <span class="type">String</span> <span class="variable">sendb</span> <span class="operator">=</span> makejson(<span class="number">2</span>, md5, <span class="string">&quot;request2&quot;</span>).toString();</span><br><span class="line">                Log.i(TAG, <span class="string">&quot;request2: &quot;</span> + sendb);</span><br><span class="line">                os = soc.getOutputStream();</span><br><span class="line">                os.write(sendb.getBytes());</span><br><span class="line">                os.write(<span class="string">&quot;endbidou1&quot;</span>.getBytes());</span><br><span class="line">                os.flush();</span><br><span class="line">                Log.i(TAG, <span class="string">&quot;request2 send successful&quot;</span>);</span><br><span class="line"></span><br><span class="line">                is = soc.getInputStream();</span><br><span class="line">                <span class="type">byte</span> buffer[] = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">1024</span> * <span class="number">100</span>];</span><br><span class="line">                is.read(buffer);</span><br><span class="line">                Log.i(TAG, <span class="string">&quot;midifilecontent: &quot;</span> + buffer.toString());</span><br><span class="line">                soc.close();</span><br><span class="line">                <span class="type">File</span> <span class="variable">filemid</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">File</span>(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class="string">&quot;/data/files/Melodia.mid&quot;</span>);</span><br><span class="line">                <span class="type">FileOutputStream</span> <span class="variable">writer</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">                writer = <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(filemid);</span><br><span class="line">                writer.write(buffer);</span><br><span class="line">                writer.close();</span><br><span class="line">                <span class="type">Message</span> <span class="variable">msg</span> <span class="operator">=</span> myhandler.obtainMessage();</span><br><span class="line">                msg.what = <span class="number">1</span>;</span><br><span class="line">                myhandler.sendMessage(msg);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="录音特效">录音特效</h1>
<p>录音图像动画效果来自Github：<a
href="https://github.com/ChadCSong/ShineButton">ShineButton</a>
另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">fabrecord.setOnTouchListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnTouchListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">onTouch</span><span class="params">(View v, MotionEvent event)</span> &#123;</span><br><span class="line">                <span class="keyword">switch</span> (event.getAction()) &#123;</span><br><span class="line">                    <span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line">                        uploadbt.setVisibility(View.INVISIBLE);</span><br><span class="line">                        <span class="keyword">if</span> (isUploadingIcon) &#123;</span><br><span class="line">                            isPressUpload = <span class="literal">false</span>;</span><br><span class="line">                            uploadbt.performClick();</span><br><span class="line">                            isPressUpload = <span class="literal">true</span>;</span><br><span class="line">                            isUploadingIcon = !isUploadingIcon;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        Log.i(TAG, <span class="string">&quot;ACTION_DOWN&quot;</span>);</span><br><span class="line">                        <span class="keyword">if</span> (!shinebtstatus) &#123;</span><br><span class="line">                            shinebt.performClick();</span><br><span class="line">                            shinebtstatus = <span class="literal">true</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        ox = event.getX();</span><br><span class="line">                        oy = event.getY();</span><br><span class="line"></span><br><span class="line">                        isRecording = <span class="literal">true</span>;</span><br><span class="line">                        recLen = <span class="number">0</span>;</span><br><span class="line">                        recTime = <span class="number">0</span>;</span><br><span class="line">                        pb.setValue(<span class="number">0</span>);</span><br><span class="line">                        fabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class="line">                        Snackbar.make(fabrecord, <span class="string">&quot;开始录音&quot;</span>, Snackbar.LENGTH_SHORT)</span><br><span class="line">                                .setAction(<span class="string">&quot;Action&quot;</span>, <span class="literal">null</span>).show();</span><br><span class="line"></span><br><span class="line">                        recorder = <span class="keyword">new</span> <span class="title class_">RecordTask</span>();</span><br><span class="line">                        recorder.execute();</span><br><span class="line">                        handler.postDelayed(runrecord, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> MotionEvent.ACTION_UP:</span><br><span class="line">                        handler.removeCallbacks(runrecord);</span><br><span class="line">                        Log.i(TAG, <span class="string">&quot;ACTION_UP&quot;</span>);</span><br><span class="line">                        <span class="keyword">if</span> (shinebtstatus) &#123;</span><br><span class="line">                            shinebt.performClick();</span><br><span class="line">                            shinebtstatus = <span class="literal">false</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="type">float</span> <span class="variable">x1</span> <span class="operator">=</span> event.getX();</span><br><span class="line">                        <span class="type">float</span> <span class="variable">y1</span> <span class="operator">=</span> event.getY();</span><br><span class="line">                        <span class="type">float</span> <span class="variable">dis1</span> <span class="operator">=</span> (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);</span><br><span class="line"></span><br><span class="line">                        isRecording = <span class="literal">false</span>;</span><br><span class="line">                        pb.setValue(<span class="number">0</span>);</span><br><span class="line">                        fabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);</span><br><span class="line">                        <span class="keyword">if</span> (dis1 &gt; <span class="number">30000</span>) &#123;</span><br><span class="line">                            Snackbar.make(fabrecord, <span class="string">&quot;取消录音&quot;</span>, Snackbar.LENGTH_SHORT)</span><br><span class="line">                                    .setAction(<span class="string">&quot;Action&quot;</span>, <span class="literal">null</span>).show();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">if</span> (!isUploadingIcon) &#123;</span><br><span class="line">                                uploadbt.setVisibility(View.VISIBLE);</span><br><span class="line">                                isPressUpload = <span class="literal">false</span>;</span><br><span class="line">                                uploadbt.performClick();</span><br><span class="line">                                isPressUpload = <span class="literal">true</span>;</span><br><span class="line">                                isUploadingIcon = !isUploadingIcon;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            Snackbar.make(fabrecord, <span class="string">&quot;录音完成&quot;</span>, Snackbar.LENGTH_SHORT)</span><br><span class="line">                                    .setAction(<span class="string">&quot;Action&quot;</span>, <span class="literal">null</span>).show();</span><br><span class="line">                            handler.postDelayed(runreplay, <span class="number">0</span>);</span><br><span class="line">                            replay();</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line">                        <span class="type">float</span> <span class="variable">x2</span> <span class="operator">=</span> event.getX();</span><br><span class="line">                        <span class="type">float</span> <span class="variable">y2</span> <span class="operator">=</span> event.getY();</span><br><span class="line">                        <span class="type">float</span> <span class="variable">dis2</span> <span class="operator">=</span> (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);</span><br><span class="line">                        <span class="keyword">if</span> (dis2 &gt; <span class="number">30000</span>) &#123;</span><br><span class="line">                            fabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            fabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>
<h1 id="展示乐谱">展示乐谱</h1>
<ul>
<li><p>本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> &#123;</span><br><span class="line">   md5 = getArguments().getString(<span class="string">&quot;md5&quot;</span>);</span><br><span class="line">   <span class="keyword">final</span> <span class="type">String</span> <span class="variable">imageUri</span> <span class="operator">=</span> <span class="string">&quot;服务器地址&quot;</span> + md5 + <span class="string">&quot;_1.png&quot;</span>;</span><br><span class="line">   Log.i(<span class="string">&quot;play&quot;</span>, <span class="string">&quot;pngfile: &quot;</span> + imageUri);</span><br><span class="line">   <span class="keyword">new</span> <span class="title class_">Handler</span>().postDelayed(<span class="keyword">new</span> <span class="title class_">Runnable</span>() &#123;</span><br><span class="line">       <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">           <span class="comment">//execute the task</span></span><br><span class="line">           imageLoader.displayImage(imageUri, showpic);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;, <span class="number">2000</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="与电子琴通信">与电子琴通信</h1>
<ul>
<li><p>类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">pianobt.setOnClickListener(<span class="keyword">new</span> <span class="title class_">View</span>.OnClickListener() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onClick</span><span class="params">(View v)</span> &#123;</span><br><span class="line">               <span class="keyword">if</span> (!isconnected) &#123;</span><br><span class="line">                   pianoaddr = etpianoaddr.getText().toString();</span><br><span class="line">                   pianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class="line">                   param[<span class="number">0</span>] = <span class="number">0x30</span>;</span><br><span class="line">                   <span class="type">StartThread</span> <span class="variable">st</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StartThread</span>();</span><br><span class="line">                   st.start();</span><br><span class="line">                   <span class="keyword">while</span> (!isconnected) ;</span><br><span class="line">                   <span class="type">MsgThread</span> <span class="variable">ms</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MsgThread</span>();</span><br><span class="line">                   ms.start();</span><br><span class="line">                   YoYo.with(Techniques.Wobble)</span><br><span class="line">                           .duration(<span class="number">300</span>)</span><br><span class="line">                           .repeat(<span class="number">6</span>)</span><br><span class="line">                           .playOn(seekBaroctave);</span><br><span class="line">                   <span class="keyword">while</span> (soc.isConnected()) ;</span><br><span class="line">                   <span class="keyword">try</span> &#123;</span><br><span class="line">                       soc.close();</span><br><span class="line">                   &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                       e.printStackTrace();</span><br><span class="line">                   &#125;</span><br><span class="line">                   isconnected = <span class="literal">false</span>;</span><br><span class="line">                   Log.i(<span class="string">&quot;piano&quot;</span>, <span class="string">&quot;socket closed&quot;</span>);</span><br><span class="line">               &#125;</span><br></pre></td></tr></table></figure>
<pre><code>          }
      });

      samplebt.setOnClickListener(new View.OnClickListener() {
          @Override
          public void onClick(View v) {
              pianoaddr = etpianoaddr.getText().toString();
              pianoport = Integer.valueOf(etpianoport.getText().toString());
              param[0] = 0x31;
              StartThread st = new StartThread();
              st.start();
              while (!isconnected) ;
              MsgThread ms = new MsgThread();
              ms.start();
              YoYo.with(Techniques.Wobble)
                      .duration(300)
                      .repeat(6)
                      .playOn(seekBaroctave);
              while (soc.isConnected()) ;
              try {
                  soc.close();
              } catch (IOException e) {
                  e.printStackTrace();
              }
              isconnected = false;
              Log.i(&quot;piano&quot;, &quot;socket closed&quot;);

          }
      });


  }

  private class StartThread extends Thread {
      @Override
      public void run() {
          try {
              soc = new Socket(pianoaddr, pianoport);
              if (soc.isConnected()) {//成功连接获取soc对象则发送成功消息
                  Log.i(&quot;piano&quot;, &quot;piano is Connected&quot;);
                  if (!isconnected)
                      isconnected = !isconnected;

              } else {
                  Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                          .setAction(&quot;Action&quot;, null).show();
                  Log.i(&quot;piano&quot;, &quot;Connect Failed&quot;);
                  soc.close();
              }
          } catch (IOException e) {
              Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();
              Log.i(&quot;piano&quot;, &quot;Connect Failed&quot;);
              e.printStackTrace();
          }
      }
  }

  private class MsgThread extends Thread {
      @Override
      public void run() {
          try {
              OutputStream os = soc.getOutputStream();
              os.write(param);
              os.flush();
              Log.i(&quot;piano&quot;, &quot;piano msg send successful&quot;);
              Snackbar.make(pianobt, &quot;正在启动启动电子琴教学&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();

              soc.close();
          } catch (IOException e) {
              Log.i(&quot;piano&quot;, &quot;piano msg send successful failed&quot;);
              Snackbar.make(pianobt, &quot;启动电子琴教学失败&quot;, Snackbar.LENGTH_SHORT)
                      .setAction(&quot;Action&quot;, null).show();
              e.printStackTrace();
          }

      }
  }</code></pre></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 乐谱分享</span><br><span class="line">-    显示乐谱的是Github上一个魔改的ImageView:[PinchImageView](https://github.com/boycy815/PinchImageView)</span><br><span class="line">-    定义其长按事件，触发一个分享的intent</span><br><span class="line">```Java</span><br><span class="line">    showpic.setOnLongClickListener(new View.OnLongClickListener() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public boolean onLongClick(View v) &#123;</span><br><span class="line">                    Bitmap drawingCache = getViewBitmap(showpic);</span><br><span class="line">                    if (drawingCache == null) &#123;</span><br><span class="line">                        Log.i(&quot;play&quot;, &quot;no img to save&quot;);</span><br><span class="line">                    &#125; else &#123;</span><br><span class="line">                        try &#123;</span><br><span class="line">                            File imageFile = new File(Environment.getExternalStorageDirectory(), &quot;saveImageview.jpg&quot;);</span><br><span class="line">                            Toast toast = Toast.makeText(getActivity(),</span><br><span class="line">                                    &quot;&quot;, Toast.LENGTH_LONG);</span><br><span class="line">                            toast.setGravity(Gravity.TOP, 0, 200);</span><br><span class="line">                            toast.setText(&quot;分享图片&quot;);</span><br><span class="line">                            toast.show();</span><br><span class="line">                            FileOutputStream outStream;</span><br><span class="line">                            outStream = new FileOutputStream(imageFile);</span><br><span class="line">                            drawingCache.compress(Bitmap.CompressFormat.JPEG, 100, outStream);</span><br><span class="line">                            outStream.flush();</span><br><span class="line">                            outStream.close();</span><br><span class="line"></span><br><span class="line">                            Intent sendIntent = new Intent();</span><br><span class="line">                            sendIntent.setAction(Intent.ACTION_SEND);</span><br><span class="line">                            sendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class="line">                            sendIntent.setType(&quot;image/png&quot;);</span><br><span class="line">                            getActivity().startActivity(Intent.createChooser(sendIntent, &quot;分享到&quot;));</span><br><span class="line"></span><br><span class="line">                        &#125; catch (IOException e) &#123;</span><br><span class="line">                            Log.i(&quot;play&quot;, &quot;share img wrong&quot;);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    return true;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br></pre></td></tr></table></figure>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepBayes 2018</title>
    <url>/2018/09/22/deepbayes2018/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/fb319b1deaab8213b5f89adf14512e93.png" width="500"/></p>
<p>Deep-Bayes 2018 Summer Camp的习题 填不动了，就到这吧</p>
<span id="more"></span>
<h1
id="deepbayes-summer-school.-practical-session-on-em-algorithm">Deep<span
style="color:green">|</span>Bayes summer school. Practical session on EM
algorithm</h1>
<ul>
<li>第一题就是应用EM算法还原图像，人像和背景叠加在一起，灰度值的概率分布形式已知，设计人像在背景中的位置为隐变量，进行EM迭代推断。</li>
<li>具体说明在官网和下面的notebook注释中有，实际上公式已经给出，想要完成作业就是把公式打上去，可以自己推一下公式。</li>
</ul>
<p>One of the school organisers decided to prank us and hid all games
for our Thursday Game Night somewhere.</p>
<p>Let's find the prankster!</p>
<p>When you recognize <a href="http://deepbayes.ru/#speakers">him or
her</a>, send:</p>
<ul>
<li>name</li>
<li>reconstructed photo</li>
<li>this notebook with your code (doesn't matter how awful it is :)</li>
</ul>
<p><strong>privately</strong> to <a
href="https://www.facebook.com/nadiinchi">Nadia Chirkova</a> at Facebook
or to info@deepbayes.ru. The first three participants will receive a
present. Do not make spoilers to other participants!</p>
<p>Please, note that you have only <strong>one attempt</strong> to send
a message!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DATA_FILE = <span class="string">&quot;data_em&quot;</span></span><br><span class="line">w = <span class="number">73</span> <span class="comment"># face_width</span></span><br></pre></td></tr></table></figure>
<h3 id="data">Data</h3>
<p>We are given a set of <span class="math inline">\(K\)</span> images
with shape <span class="math inline">\(H \times W\)</span>.</p>
<p>It is represented by a numpy-array with shape <span
class="math inline">\(H \times W \times K\)</span>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.load(DATA_FILE)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.shape <span class="comment"># H, W, K</span></span><br></pre></td></tr></table></figure>
<pre><code>(100, 200, 1000)</code></pre>
<p>Example of noisy image:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(X[:, :, <span class="number">0</span>], cmap=<span class="string">&quot;Greys_r&quot;</span>)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(X[<span class="number">1</span>,:,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.
   0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.
 148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.
   0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.
   0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.
   0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.
   0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.
   0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.
 255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.
 233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.
 226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.
   0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.
 255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.
 255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.
 102.  25.  30.  73.]</code></pre>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0Ihiq.png" alt="i0Ihiq.png" />
<figcaption aria-hidden="true">i0Ihiq.png</figcaption>
</figure>
<h3 id="goal-and-plan">Goal and plan</h3>
<p>Our goal is to find face <span class="math inline">\(F\)</span>
(<span class="math inline">\(H \times w\)</span>).</p>
<p>Also, we will find:</p>
<ul>
<li><span class="math inline">\(B\)</span>: background (<span
class="math inline">\(H \times W\)</span>)</li>
<li><span class="math inline">\(s\)</span>: noise standard deviation
(float)</li>
<li><span class="math inline">\(a\)</span>: discrete prior over face
positions (<span class="math inline">\(W-w+1\)</span>)</li>
<li><span class="math inline">\(q(d)\)</span>: discrete posterior over
face positions for each image ((<span
class="math inline">\(W-w+1\)</span>) x <span
class="math inline">\(K\)</span>)</li>
</ul>
<p>Implementation plan:</p>
<ol type="1">
<li>calculating <span class="math inline">\(log\, p(X \mid
d,\,F,\,B,\,s)\)</span></li>
<li>calculating objective</li>
<li>E-step: finding <span class="math inline">\(q(d)\)</span></li>
<li>M-step: estimating <span class="math inline">\(F,\, B, \,s,
\,a\)</span></li>
<li>composing EM-algorithm from E- and M-step</li>
</ol>
<h3 id="implementation">Implementation</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### Variables to test implementation</span></span><br><span class="line">tH, tW, tw, tK = <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">tX = np.arange(tH*tW*tK).reshape(tH, tW, tK)</span><br><span class="line">tF = np.arange(tH*tw).reshape(tH, tw)</span><br><span class="line">tB = np.arange(tH*tW).reshape(tH, tW)</span><br><span class="line">ts = <span class="number">0.1</span></span><br><span class="line">ta = np.arange(<span class="number">1</span>, (tW-tw+<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">ta = ta / ta.<span class="built_in">sum</span>()</span><br><span class="line">tq = np.arange(<span class="number">1</span>, (tW-tw+<span class="number">1</span>)*tK+<span class="number">1</span>).reshape(tW-tw+<span class="number">1</span>, tK)</span><br><span class="line">tq = tq / tq.<span class="built_in">sum</span>(axis=<span class="number">0</span>)[np.newaxis, :]</span><br></pre></td></tr></table></figure>
<h4 id="implement-calculate_log_probability">1. Implement
calculate_log_probability</h4>
<p>For <span class="math inline">\(k\)</span>-th image <span
class="math inline">\(X_k\)</span> and some face position <span
class="math inline">\(d_k\)</span>: <span
class="math display">\[p(X_k  \mid d_k,\,F,\,B,\,s) = \prod_{ij}
    \begin{cases}
      \mathcal{N}(X_k[i,j]\mid F[i,\,j-d_k],\,s^2),
      &amp; \text{if}\, (i,j)\in faceArea(d_k)\\
      \mathcal{N}(X_k[i,j]\mid B[i,j],\,s^2), &amp; \text{else}
    \end{cases}\]</span></p>
<p>Important notes:</p>
<ul>
<li>Do not forget about logarithm!</li>
<li>This implementation should use no more than 1 cycle!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_log_probability</span>(<span class="params">X, F, B, s</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and</span></span><br><span class="line"><span class="string">    all possible face position d_k.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array, shape (H, W, K)</span></span><br><span class="line"><span class="string">        K images of size H x W.</span></span><br><span class="line"><span class="string">    F : array, shape (H, w)</span></span><br><span class="line"><span class="string">        Estimate of prankster&#x27;s face.</span></span><br><span class="line"><span class="string">    B : array, shape (H, W)</span></span><br><span class="line"><span class="string">        Estimate of background.</span></span><br><span class="line"><span class="string">    s : float</span></span><br><span class="line"><span class="string">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    ll : array, shape(W-w+1, K)</span></span><br><span class="line"><span class="string">        ll[dw, k] - log-likelihood of observing image X_k given</span></span><br><span class="line"><span class="string">        that the prankster&#x27;s face F is located at position dw</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    H = X.shape[<span class="number">0</span>]</span><br><span class="line">    W = X.shape[<span class="number">1</span>]</span><br><span class="line">    K = X.shape[<span class="number">2</span>]</span><br><span class="line">    w = F.shape[<span class="number">1</span>]</span><br><span class="line">    ll = np.zeros((W-w+<span class="number">1</span>,K))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        X_minus_B = X[:,:,k]-B[:,:]</span><br><span class="line">        XB = np.multiply(X_minus_B,X_minus_B)</span><br><span class="line">        <span class="keyword">for</span> dk <span class="keyword">in</span> <span class="built_in">range</span>(W-w+<span class="number">1</span>):</span><br><span class="line">            F_temp = np.zeros((H,W))</span><br><span class="line">            F_temp[:,dk:dk+w] = F</span><br><span class="line">            X_minus_F = X[:,:,k] - F_temp[:,:]</span><br><span class="line">            XF = np.multiply(X_minus_F,X_minus_F)</span><br><span class="line">            XB_mask = np.ones((H,W))</span><br><span class="line">            XB_mask[:,dk:dk+w] = <span class="number">0</span></span><br><span class="line">            XF_mask = <span class="number">1</span>-XB_mask</span><br><span class="line">            XB_temp = np.multiply(XB,XB_mask)</span><br><span class="line">            XF_temp = np.multiply(XF,XF_mask)   </span><br><span class="line">            Sum = (np.<span class="built_in">sum</span>(XB_temp+XF_temp))*(-<span class="number">1</span>/(<span class="number">2</span>*s**<span class="number">2</span>))-H*W*np.log(np.sqrt(<span class="number">2</span>*np.pi)*s)</span><br><span class="line">            ll[dk][k]=Sum    </span><br><span class="line">    <span class="keyword">return</span> ll</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run this cell to test your implementation</span></span><br><span class="line">expected = np.array([[-<span class="number">3541.69812064</span>, -<span class="number">5541.69812064</span>],</span><br><span class="line">       [-<span class="number">4541.69812064</span>, -<span class="number">6741.69812064</span>],</span><br><span class="line">       [-<span class="number">6141.69812064</span>, -<span class="number">8541.69812064</span>]])</span><br><span class="line">actual = calculate_log_probability(tX, tF, tB, ts)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(actual, expected)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>OK</code></pre>
<h4 id="implement-calculate_lower_bound">2. Implement
calculate_lower_bound</h4>
<p><span class="math display">\[\mathcal{L}(q, \,F, \,B,\, s,\, a) =
\sum_k \biggl (\mathbb{E} _ {q( d_k)}\bigl ( \log p(  X_{k}  \mid
{d}_{k} , \,F,\,B,\,s) +
    \log p( d_k  \mid a)\bigr) - \mathbb{E} _ {q( d_k)} \log q(
d_k)\biggr) \]</span></p>
<p>Important notes:</p>
<ul>
<li>Use already implemented calculate_log_probability!</li>
<li>Note that distributions <span class="math inline">\(q( d_k)\)</span>
and <span class="math inline">\(p( d_k \mid a)\)</span> are discrete.
For example, <span class="math inline">\(P(d_k=i \mid a) =
a[i]\)</span>.</li>
<li>This implementation should not use cycles!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_lower_bound</span>(<span class="params">X, F, B, s, a, q</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculates the lower bound L(q, F, B, s, a) for </span></span><br><span class="line"><span class="string">    the marginal log likelihood.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array, shape (H, W, K)</span></span><br><span class="line"><span class="string">        K images of size H x W.</span></span><br><span class="line"><span class="string">    F : array, shape (H, w)</span></span><br><span class="line"><span class="string">        Estimate of prankster&#x27;s face.</span></span><br><span class="line"><span class="string">    B : array, shape (H, W)</span></span><br><span class="line"><span class="string">        Estimate of background.</span></span><br><span class="line"><span class="string">    s : float</span></span><br><span class="line"><span class="string">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class="line"><span class="string">    a : array, shape (W-w+1)</span></span><br><span class="line"><span class="string">        Estimate of prior on position of face in any image.</span></span><br><span class="line"><span class="string">    q : array</span></span><br><span class="line"><span class="string">        q[dw, k] - estimate of posterior </span></span><br><span class="line"><span class="string">                   of position dw</span></span><br><span class="line"><span class="string">                   of prankster&#x27;s face given image Xk</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    L : float</span></span><br><span class="line"><span class="string">        The lower bound L(q, F, B, s, a) </span></span><br><span class="line"><span class="string">        for the marginal log likelihood.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    K = X.shape[<span class="number">2</span>]</span><br><span class="line">    ll = calculate_log_probability(X,F,B,s)</span><br><span class="line">    ll_expectation = np.multiply(ll,q)</span><br><span class="line">    q_expectation = np.multiply(np.log(q),q)</span><br><span class="line">    dk_expection = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        dk_expection += np.multiply(np.log(a),q[:,k])</span><br><span class="line">    L = np.<span class="built_in">sum</span>(ll_expectation)-np.<span class="built_in">sum</span>(q_expectation)+np.<span class="built_in">sum</span>(dk_expection)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> L</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run this cell to test your implementation</span></span><br><span class="line">expected = -<span class="number">12761.1875</span></span><br><span class="line">actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(actual, expected)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>OK</code></pre>
<h4 id="implement-e-step">3. Implement E-step</h4>
<p><span class="math display">\[q(d_k) = p(d_k \mid X_k, \,F, \,B,
\,s,\, a) =
\frac {p(  X_{k}  \mid {d}_{k} , \,F,\,B,\,s)\, p(d_k \mid a)}
{\sum_{d&#39;_k} p(  X_{k}  \mid d&#39;_k , \,F,\,B,\,s) \,p(d&#39;_k
\mid a)}\]</span></p>
<p>Important notes:</p>
<ul>
<li>Use already implemented calculate_log_probability!</li>
<li>For computational stability, perform all computations with
logarithmic values and apply exp only before return. Also, we recommend
using this trick: <span class="math display">\[\beta_i =
\log{p_i(\dots)} \quad\rightarrow \quad
\frac{e^{\beta_i}}{\sum_k e^{\beta_k}} =
\frac{e^{(\beta_i - \max_j \beta_j)}}{\sum_k e^{(\beta_k- \max_j
\beta_j)}}\]</span></li>
<li>This implementation should not use cycles!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_e_step</span>(<span class="params">X, F, B, s, a</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given the current esitmate of the parameters, for each image Xk</span></span><br><span class="line"><span class="string">    esitmates the probability p(d_k|X_k, F, B, s, a).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array, shape(H, W, K)</span></span><br><span class="line"><span class="string">        K images of size H x W.</span></span><br><span class="line"><span class="string">    F  : array_like, shape(H, w)</span></span><br><span class="line"><span class="string">        Estimate of prankster&#x27;s face.</span></span><br><span class="line"><span class="string">    B : array shape(H, W)</span></span><br><span class="line"><span class="string">        Estimate of background.</span></span><br><span class="line"><span class="string">    s : float</span></span><br><span class="line"><span class="string">        Eestimate of standard deviation of Gaussian noise.</span></span><br><span class="line"><span class="string">    a : array, shape(W-w+1)</span></span><br><span class="line"><span class="string">        Estimate of prior on face position in any image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    q : array</span></span><br><span class="line"><span class="string">        shape (W-w+1, K)</span></span><br><span class="line"><span class="string">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class="line"><span class="string">        of prankster&#x27;s face given image Xk</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    ll = calculate_log_probability(X,F,B,s)</span><br><span class="line">    K = X.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        max_ll = ll[:,k].<span class="built_in">max</span>()</span><br><span class="line">        ll[:,k] -= max_ll</span><br><span class="line">        ll[:,k] = np.exp(ll[:,k])*a</span><br><span class="line">        denominator = np.<span class="built_in">sum</span>(ll[:,k])</span><br><span class="line">        ll[:,k] /= denominator</span><br><span class="line">    q = ll</span><br><span class="line">    <span class="keyword">return</span> q</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run this cell to test your implementation</span></span><br><span class="line">expected = np.array([[ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">                   [ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">                   [ <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line">actual = run_e_step(tX, tF, tB, ts, ta)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(actual, expected)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>OK</code></pre>
<h4 id="implement-m-step">4. Implement M-step</h4>
<p><span class="math display">\[a[j] = \frac{\sum_k q( d_k = j
)}{\sum_{j&#39;}  \sum_{k&#39;} q( d_{k&#39;} = j&#39;)}\]</span> <span
class="math display">\[F[i, m] = \frac 1 K  \sum_k \sum_{d_k} q(d_k)\,
X^k[i,\, m+d_k]\]</span> <span class="math display">\[B[i, j] = \frac
{\sum_k \sum_{ d_k:\, (i, \,j) \,\not\in faceArea(d_k)} q(d_k)\, X^k[i,
j]}
      {\sum_k \sum_{d_k: \,(i, \,j)\, \not\in faceArea(d_k)}
q(d_k)}\]</span> <span class="math display">\[s^2 = \frac 1
{HWK}   \sum_k \sum_{d_k} q(d_k)
      \sum_{i,\, j}  (X^k[i, \,j] - Model^{d_k}[i, \,j])^2\]</span></p>
<p>where <span class="math inline">\(Model^{d_k}[i, j]\)</span> is an
image composed from background and face located at <span
class="math inline">\(d_k\)</span>.</p>
<p>Important notes:</p>
<ul>
<li>Update parameters in the following order: <span
class="math inline">\(a\)</span>, <span
class="math inline">\(F\)</span>, <span
class="math inline">\(B\)</span>, <span
class="math inline">\(s\)</span>.</li>
<li>When the parameter is updated, its <strong>new</strong> value is
used to update other parameters.</li>
<li>This implementation should use no more than 3 cycles and no embedded
cycles!</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_m_step</span>(<span class="params">X, q, w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Estimates F, B, s, a given esitmate of posteriors defined by q.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array, shape (H, W, K)</span></span><br><span class="line"><span class="string">        K images of size H x W.</span></span><br><span class="line"><span class="string">    q  :</span></span><br><span class="line"><span class="string">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class="line"><span class="string">                   of prankster&#x27;s face given image Xk</span></span><br><span class="line"><span class="string">    w : int</span></span><br><span class="line"><span class="string">        Face mask width.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    F : array, shape (H, w)</span></span><br><span class="line"><span class="string">        Estimate of prankster&#x27;s face.</span></span><br><span class="line"><span class="string">    B : array, shape (H, W)</span></span><br><span class="line"><span class="string">        Estimate of background.</span></span><br><span class="line"><span class="string">    s : float</span></span><br><span class="line"><span class="string">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class="line"><span class="string">    a : array, shape (W-w+1)</span></span><br><span class="line"><span class="string">        Estimate of prior on position of face in any image.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    K = X.shape[<span class="number">2</span>]</span><br><span class="line">    W = X.shape[<span class="number">1</span>]</span><br><span class="line">    H = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    a = np.<span class="built_in">sum</span>(q,axis=<span class="number">1</span>) / np.<span class="built_in">sum</span>(q)</span><br><span class="line"></span><br><span class="line">    F = np.zeros((H,w))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        <span class="keyword">for</span> dk <span class="keyword">in</span> <span class="built_in">range</span>(W-w+<span class="number">1</span>):</span><br><span class="line">            F+=q[dk][k]*X[:,dk:dk+w,k]</span><br><span class="line">    F = F / K</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    B = np.zeros((H,W))</span><br><span class="line">    denominator = np.zeros((H,W))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        <span class="keyword">for</span> dk <span class="keyword">in</span> <span class="built_in">range</span>(W-w+<span class="number">1</span>):</span><br><span class="line">            mask = np.ones((H,W))</span><br><span class="line">            mask[:,dk:dk+w] = <span class="number">0</span></span><br><span class="line">            B += np.multiply(q[dk][k]*X[:,:,k],mask)</span><br><span class="line">            denominator += q[dk][k]*mask</span><br><span class="line">    denominator = <span class="number">1</span>/denominator</span><br><span class="line">    B = B * denominator</span><br><span class="line"></span><br><span class="line">    s = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        <span class="keyword">for</span> dk <span class="keyword">in</span> <span class="built_in">range</span>(W-w+<span class="number">1</span>):</span><br><span class="line">            F_B = np.zeros((H,W))</span><br><span class="line">            F_B[:,dk:dk+w]=F</span><br><span class="line">            mask = np.ones((H,W))</span><br><span class="line">            mask[:,dk:dk+w] = <span class="number">0</span></span><br><span class="line">            Model = F_B + np.multiply(B,mask)</span><br><span class="line">            temp = X[:,:,k]-Model[:,:]</span><br><span class="line">            temp = np.multiply(temp,temp)</span><br><span class="line">            temp = np.<span class="built_in">sum</span>(temp)</span><br><span class="line">            temp *= q[dk][k]</span><br><span class="line">            s += temp</span><br><span class="line">    s = np.sqrt(s /(H*W*K))          </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> F,B,s,a</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run this cell to test your implementation</span></span><br><span class="line">expected = [np.array([[ <span class="number">3.27777778</span>],</span><br><span class="line">                      [ <span class="number">9.27777778</span>]]),</span><br><span class="line"> np.array([[  <span class="number">0.48387097</span>,   <span class="number">2.5</span>       ,   <span class="number">4.52941176</span>],</span><br><span class="line">           [  <span class="number">6.48387097</span>,   <span class="number">8.5</span>       ,  <span class="number">10.52941176</span>]]),</span><br><span class="line">  <span class="number">0.94868</span>,</span><br><span class="line"> np.array([ <span class="number">0.13888889</span>,  <span class="number">0.33333333</span>,  <span class="number">0.52777778</span>])]</span><br><span class="line">actual = run_m_step(tX, tq, tw)</span><br><span class="line"><span class="keyword">for</span> a, e <span class="keyword">in</span> <span class="built_in">zip</span>(actual, expected):</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(a, e)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>OK</code></pre>
<h4 id="implement-em_algorithm">5. Implement EM_algorithm</h4>
<p>Initialize parameters, if they are not passed, and then repeat E- and
M-steps till convergence.</p>
<p>Please note that <span class="math inline">\(\mathcal{L}(q, \,F, \,B,
\,s, \,a)\)</span> must increase after each iteration.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_EM</span>(<span class="params">X, w, F=<span class="literal">None</span>, B=<span class="literal">None</span>, s=<span class="literal">None</span>, a=<span class="literal">None</span>, tolerance=<span class="number">0.001</span>,</span></span><br><span class="line"><span class="params">           max_iter=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Runs EM loop until the likelihood of observing X given current</span></span><br><span class="line"><span class="string">    estimate of parameters is idempotent as defined by a fixed</span></span><br><span class="line"><span class="string">    tolerance.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X : array, shape (H, W, K)</span></span><br><span class="line"><span class="string">        K images of size H x W.</span></span><br><span class="line"><span class="string">    w : int</span></span><br><span class="line"><span class="string">        Face mask width.</span></span><br><span class="line"><span class="string">    F : array, shape (H, w), optional</span></span><br><span class="line"><span class="string">        Initial estimate of prankster&#x27;s face.</span></span><br><span class="line"><span class="string">    B : array, shape (H, W), optional</span></span><br><span class="line"><span class="string">        Initial estimate of background.</span></span><br><span class="line"><span class="string">    s : float, optional</span></span><br><span class="line"><span class="string">        Initial estimate of standard deviation of Gaussian noise.</span></span><br><span class="line"><span class="string">    a : array, shape (W-w+1), optional</span></span><br><span class="line"><span class="string">        Initial estimate of prior on position of face in any image.</span></span><br><span class="line"><span class="string">    tolerance : float, optional</span></span><br><span class="line"><span class="string">        Parameter for stopping criterion.</span></span><br><span class="line"><span class="string">    max_iter  : int, optional</span></span><br><span class="line"><span class="string">        Maximum number of iterations.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    F, B, s, a : trained parameters.</span></span><br><span class="line"><span class="string">    LL : array, shape(number_of_iters + 2,)</span></span><br><span class="line"><span class="string">        L(q, F, B, s, a) at initial guess, </span></span><br><span class="line"><span class="string">        after each EM iteration and after</span></span><br><span class="line"><span class="string">        final estimate of posteriors;</span></span><br><span class="line"><span class="string">        number_of_iters is actual number of iterations that was done.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    H, W, N = X.shape</span><br><span class="line">    <span class="keyword">if</span> F <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        F = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (H, w))</span><br><span class="line">    <span class="keyword">if</span> B <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        B = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (H, W))</span><br><span class="line">    <span class="keyword">if</span> a <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        a = np.ones(W - w + <span class="number">1</span>)</span><br><span class="line">        a /= np.<span class="built_in">sum</span>(a)</span><br><span class="line">    <span class="keyword">if</span> s <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        s = np.random.rand()*<span class="built_in">pow</span>(<span class="number">64</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    LL = [-<span class="number">100000</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        q = run_e_step(X,F,B,s,a)</span><br><span class="line">        F,B,s,a = run_m_step(X,q,w)</span><br><span class="line">        LL.append(calculate_lower_bound(X,F,B,s,a,q))</span><br><span class="line">        <span class="keyword">if</span> LL[-<span class="number">1</span>]-LL[-<span class="number">2</span>] &lt; tolerance :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    LL = np.array(LL)</span><br><span class="line">    <span class="keyword">return</span> F,B,s,a,LL</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># run this cell to test your implementation</span></span><br><span class="line">res = run_EM(tX, tw, max_iter=<span class="number">10</span>)</span><br><span class="line">LL = res[-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">assert</span> np.alltrue(LL[<span class="number">1</span>:] - LL[:-<span class="number">1</span>] &gt; <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>OK</code></pre>
<h3 id="who-is-the-prankster">Who is the prankster?</h3>
<p>To speed up the computation, we will perform 5 iterations over small
subset of images and then gradually increase the subset.</p>
<p>If everything is implemented correctly, you will recognize the
prankster (remember he is the one from <a
href="http://deepbayes.ru/#speakers">DeepBayes team</a>).</p>
<p>Run EM-algorithm:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show</span>(<span class="params">F, i=<span class="number">1</span>, n=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    shows face F at subplot i out of n</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, n, i)</span><br><span class="line">    plt.imshow(F, cmap=<span class="string">&quot;Greys_r&quot;</span>)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">F, B, s, a = [<span class="literal">None</span>] * <span class="number">4</span></span><br><span class="line">LL = []</span><br><span class="line">lens = [<span class="number">50</span>, <span class="number">100</span>, <span class="number">300</span>, <span class="number">500</span>, <span class="number">1000</span>]</span><br><span class="line">iters = [<span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, (l, it) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(lens, iters)):</span><br><span class="line">    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)</span><br><span class="line">    show(F, i+<span class="number">1</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0omSf.png" alt="i0omSf.png" />
<figcaption aria-hidden="true">i0omSf.png</figcaption>
</figure>
<p>And this is the background:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show(B)</span><br></pre></td></tr></table></figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0I4J0.png" alt="i0I4J0.png" />
<figcaption aria-hidden="true">i0I4J0.png</figcaption>
</figure>
<h3 id="optional-part-hard-em">Optional part: hard-EM</h3>
<p>If you have some time left, you can implement simplified version of
EM-algorithm called hard-EM. In hard-EM, instead of finding posterior
distribution <span class="math inline">\(p(d_k|X_k, F, B, s, A)\)</span>
at E-step, we just remember its argmax <span
class="math inline">\(\tilde d_k\)</span> for each image <span
class="math inline">\(k\)</span>. Thus, the distribution q is replaced
with a singular distribution: <span class="math display">\[q(d_k) =
\begin{cases} 1, \, if d_k = \tilde d_k \\ 0, \,
otherwise\end{cases}\]</span> This modification simplifies formulas for
<span class="math inline">\(\mathcal{L}\)</span> and M-step and speeds
their computation up. However, the convergence of hard-EM is usually
slow.</p>
<p>If you implement hard-EM, add binary flag hard_EM to the parameters
of the following functions:</p>
<ul>
<li>calculate_lower_bound</li>
<li>run_e_step</li>
<li>run_m_step</li>
<li>run_EM</li>
</ul>
<p>After implementation, compare overall computation time for EM and
hard-EM till recognizable F.</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>Easy Reinforcement Learning Notes</title>
    <url>/2019/09/23/easyrl/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/d67b97f7a9ddae88afabc277d249b281.png" width="500"/></p>
<p>rl study note, minimalist style</p>
<ul>
<li>Q-learning</li>
<li>Sarsa</li>
<li>Sarsa(<span class="math inline">\(\lambda\)</span>)</li>
<li>DQN</li>
<li>Double DQN</li>
<li>DQN with Prioritized Experience replay</li>
<li>Dueling DQN</li>
<li>Policy Gradient</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="definition">Definition</h1>
<ul>
<li>agent: An agent, which, in a certain state, takes an action based on
some policy to reach the next state</li>
<li>s:status, status</li>
<li>a: action, action</li>
<li>r:reward, reward</li>
<li>s,a: Can be called a step. The behavior of the agent can be
described as a series of steps. In reinforcement learning, the behavior
of the agent can be represented by DP (decision process), where s is the
node state in the DP and a is the transition path between states.</li>
<li>Q: Q-table, Q(s,a) refers to the value of action a under state s
(probability during inference), estimated by the expected reward</li>
</ul>
<h1 id="q-learning">Q-learning</h1>
<ul>
<li><p>Random Q-table, initial state, start iteration, <span
class="math inline">\(\epsilon\)</span> Greedy</p></li>
<li><p>Taking action a in state s, observing reward r and state <span
class="math inline">\(s \prime\)</span></p></li>
<li><p>Key Iteration Formula:</p>
<p><span class="math display">\[
Q(s,a) = Q(s,a) + \alpha [r + \gamma max_{a \prime}Q(s \prime,a \prime)
- Q(s,a)] \\
\]</span></p></li>
<li><p>Q's update includes two parts, one of which is naturally the
reward (if this step is rewarded), and the other is the time difference,
or TD-error, which is the difference between reality and estimation.
Here, reality refers to the reward obtained after taking the best action
in the new state, while estimation refers to all our rewards except for
the actual reward obtained in the final step. For the intermediate
steps, the estimated reality reward is obtained using the Q-table
values, so the update of the Q-table should be to add (reality -
estimation), making the Q-table closer to reality.</p></li>
<li><p>It is noteworthy that only when the last step is rewarded (if
there is only one reward at the end), is the reality truly the reward;
otherwise, it is still estimated using the Q-table.</p></li>
<li><p>Here, the update of the Q-table is only related to future states
and actions. Initially, the updates of all steps except the last one
with actual rewards are uncertain (because reality also uses the Q-table
for estimation, only the last step is truly real), but after the first
iteration, the value update of the last step is determined (knowing how
to approach the treasure is known), and unlike LSTM's time series, it
does not update from the last time step backwards with BPTT, but updates
the transition value of a state (which action is better), and this state
may appear at multiple time steps in each iteration (or it is unrelated
to the time step). When updating the state adjacent to this state, the
estimated reality from the Q-table will be more accurate, and gradually
the entire algorithm reaches convergence.</p></li>
</ul>
<h1 id="sarsa">Sarsa</h1>
<ul>
<li><p>Q-learning is off-policy because of the gap between reality and
estimation, we optimize towards reality, while the actual actions are
based on the estimated reality from the Q-table, i.e., asynchronous
(off).</p></li>
<li><p>Sarsa is on-policy, differing from Q-learning in algorithmic
terms:</p>
<ul>
<li>Q-learning: Select action based on Q-table - Execute action, observe
reward and new state - Update Q-table - Update state</li>
<li>Sarsa: Execute action, observe reward and new state - update state -
select action on the new state based on the Q-table, and select an
action based on the Q-table before the iteration</li>
</ul></li>
<li><p>It can be seen that Sarsa has changed the order of steps in the
learning algorithm; what effect does this change bring? The effect is to
bring the selection of new states and actions forward before the Q-table
update, so that when the Q-table is updated, the part of the gap that is
realized does not need to be estimated with <span
class="math inline">\(max_{a \prime}Q(s \prime,a \prime)\)</span> but is
directly estimated using the new state and action:</p>
<p><span class="math display">\[
Q(s,a) = Q(s,a) + \alpha [r + \gamma Q(s \prime,a \prime) - Q(s,a)] \\
\]</span></p></li>
<li><p>That is, both reality and the estimate use the same strategy
(determined actions, rather than selecting the maximum value based on
given states), and then still use this gap to update.</p></li>
<li><p>In the updating process, Sarsa follows through on its promises
(because the reality is adopting new states and actions, the agent will
definitely update accordingly), while Q-learning is more optimistic
(because the reality is adopting the maximum value under the new state,
but the actual action taken may not necessarily be the one with the
maximum value, with <span class="math inline">\(\epsilon\)</span>
disturbance). Sarsa is more conservative (wishing to do well at every
step), while Q-learning is more aggressive (rushing in first and
worrying about it later).</p></li>
</ul>
<h1 id="sarsalambda">Sarsa(<span
class="math inline">\(\lambda\)</span>)</h1>
<ul>
<li><p>naive version of Sarsa can be seen as Sarsa(0), because the
Q-table is updated with every step, i.e., <span
class="math inline">\(Q(s \prime,a \prime)\)</span> will only bring the
value update of the previous step <span
class="math inline">\(Q(s,a)\)</span> . If we consider the updates for
all steps, and steps closer to the final reward have higher weights
while steps farther from the final reward have lower weights, then the
hyperparameter adjusting this weight is <span
class="math inline">\(\lambda\)</span> , with a weight of 0 meaning not
considering previous steps, i.e., naive Sarsa.</p></li>
<li><p>The specific implementation involves adding a trace matrix E
(with each element corresponding to a step (s, a)) to save the weights
of all steps in the current path. The closer to the final reward, the
higher the weight. Therefore, with each step taken, the corresponding
element in the matrix is incremented by 1, and then the corresponding
trace matrix value is used as a weight to multiply onto the reward to
update the Q-table (here, all steps and the entire E matrix are
multiplied together). Afterward, the matrix values are decayed by
multiplying with the update decay factor <span
class="math inline">\(\gamma\)</span> and the weight hyperparameter
<span class="math inline">\(\lambda\)</span> . Clearly, when <span
class="math inline">\(\gamma\)</span> is 1, all states receive the same
update (since all steps and the entire E matrix are multiplied
together); when <span class="math inline">\(\gamma\)</span> is 0, only
the executed step element is incremented by 1, and the entire E matrix
is set to 0, so only the executed step receives an update, i.e., naive
Sarsa. By expanding the iteration of the E matrix, one can obtain an
expansion similar to naive Sarsa, except that an E(s,a) is added during
the decay to record the distance of a step from the reward.</p>
<p><span class="math display">\[
\delta = r + \gamma Q(s \prime,a \prime) - Q(s,a) \\
E(s,a) = E(s,a) + 1 \\
Q = Q + \alpha \delta E \\
E = \gamma \lambda E \\
update  \ \ s,a \\
\]</span></p></li>
<li><p>This E-matrix is the eligibility trace. For a certain step, if it
is executed, its value is slightly increased, and then it decays slowly
until it is executed again. If it is executed multiple times in the
short term, its value will rise too high, at which point a threshold can
be set to limit the increase in eligibility. This value can be
interpreted as the contribution of this step to finding the final reward
in this iteration.</p></li>
</ul>
<h1 id="dqn">DQN</h1>
<ul>
<li>Q-learning + deep neural network</li>
<li>Neural networks are used to parameterize the update process of the
Q-table, inputting a state vector to output the value of all actions.
The training of the neural network replaces the simple iterative update.
This can solve the dimensionality disaster problem caused by an
excessive number of states.</li>
<li>It is not easy to train by simply replacing this way, DQN introduces
two techniques
<ul>
<li>experience replay</li>
<li>fix q</li>
</ul></li>
<li>Examine the input, output, and loss of the neural network
<ul>
<li>Two neural networks are involved, one participating in training
(evaluation network), and the other replicating the parameters trained
by the first network to generate ground truth, i.e., the target
network</li>
<li>The network input for training is a state represented as an
s-dimensional feature vector, and the output is an a-dimensional value
vector obtained from each action, with the loss being the mean squared
error between this vector and the a-dimensional ground truth
vector.</li>
</ul></li>
<li>DQN also includes a memory matrix, with dimensions [number of memory
items, <span class="math inline">\(2 * s + 2\)</span> ], where each item
contains the reward, action, old state, and new state, i.e., all the
information of a single step. The memory only saves the last few memory
items executed steps.</li>
<li>Where do the input and output of the neural network come from? In
fact, it samples a batch of data from memory, inputs the old state to
the evaluation network to obtain the model output, inputs the new state
to the target network to obtain the ground truth, and then calculates
the loss.</li>
<li>Every so often, the target network replicates the evaluation
network; during this period, the target network remains unchanged, i.e.,
fix q</li>
<li>Experience replay</li>
<li>It is noteworthy that we do not directly use the value vector output
by the evaluation network to calculate the loss, as no action has been
taken yet. Therefore, we first take actions based on the value vector
output by the network, update the value corresponding to these actions
in the value vector with rewards, and then use this updated value vector
to participate in the loss calculation.</li>
</ul>
<h1 id="double-dqn">Double DQN</h1>
<ul>
<li>Double DQN solves the DQN overestimate problem</li>
<li>The difference from DQN is that the evaluation network not only
accepts an old state to produce an output <span
class="math inline">\(q_{eval}\)</span> , but also accepts a new state
to produce an output <span class="math inline">\(q_{eval4next}\)</span>
, and then selects an action to update <span
class="math inline">\(q_{eval}\)</span> based on <span
class="math inline">\(q_{eval4next}\)</span> , rather than selecting an
action to update based on <span class="math inline">\(q_{eval}\)</span>
itself.</li>
</ul>
<h1 id="dqn-with-prioritized-experience-replay">DQN with Prioritized
Experience replay</h1>
<ul>
<li>Priorly, it was to randomly sample a segment of memory from the
memory bank, which would lead to an excessive number of training steps
for the model (random sampling is difficult to ensure reaching the final
reward)</li>
<li>Memory should naturally be allocated priority, with higher priority
having a greater sampling probability</li>
<li>Priority can be measured by the TD-error value, with larger errors
naturally requiring more sampling and optimization</li>
<li>Given the priority distribution, if one wishes to obtain a sampling
sequence that satisfies this priority distribution, then this is a Monte
Carlo problem, which can be solved using MCMC, importance sampling, or
the SumTree mentioned in the paper</li>
</ul>
<h1 id="dueling-dqn">Dueling DQN</h1>
<ul>
<li><p>Dueling DQN refines the input-output relationship of the internal
network in the original DQN:</p>
<p><span class="math display">\[
Q(s,a;\theta,\alpha,\beta) = Value(s;\theta,\beta) +
Advantage(s,a;\theta,\alpha)
\]</span></p></li>
<li><p>The value brought by the state and the value (advantage) brought
by actions on that state have been split</p></li>
<li><p>Why consider states separately? Because some states are unrelated
to actions, and no action will bring about a change in value regardless
of what action is taken</p></li>
</ul>
<h1 id="policy-gradient">Policy Gradient</h1>
<ul>
<li><p>Translated Text: The previously introduced methods are all based
on value reinforcement learning, i.e., querying the value brought by
each action at a certain state and selecting the action with the highest
value to execute</p></li>
<li><p>Policy gradient（策略梯度）directly outputs actions from the
policy network input state, skipping the step of calculating value, and
is a method based on the policy</p></li>
<li><p>Strategy networks do not calculate a certain loss for
backpropagation but propagate back based on rewards, and the updated
algorithm is as follows:</p>
<p><span class="math display">\[
\theta = \theta + \alpha \nabla _{\theta} \log \pi _{\theta} (s_t, a_t)
v_t \\
\]</span></p></li>
<li><p>It should be noted the following key points:</p>
<ul>
<li>The PG algorithm neural network here is responsible for receiving
the state as input, the reward as the gradient adjustment value, the
actual action executed as the gold label, and outputs an action vector.
The entire network itself is a policy <span
class="math inline">\(\pi\)</span> (input state, output action
probability distribution)</li>
<li>Afterward, the entire model still selects actions based on action
probabilities (network outputs) using a policy (network), transitions
states, and observes the environment to receive rewards, just like
DQN</li>
<li>The first question is, where does this strategy gradient come from?
As can be seen, the gradient is still the derivative of the loss with
respect to the parameters, <span class="math inline">\(- \nabla
_{\theta} \log \pi _{\theta} (s_t, a_t) v_t\)</span> . Where does the
loss come from? It is actually the probability of executing an action
multiplied by the reward, which can be considered as the cross-entropy
between the action probability distribution and the one-hot vector of
the executed action, multiplied by the reward (cross-entropy itself has
a similar lookup effect). Let's look at its original meaning: the
probability of executing an action multiplied by the reward. The network
itself is just a strategy. What kind of strategy is a good strategy? If
the good reward is obtained after executing the action selected by this
strategy, then this is a good strategy network. That is, a strategy
network that can select good actions is a good network. It is obvious
that the objective function of the network should be the probability of
executing an action (the actions recognized by the network) multiplied
by the reward (the good actions recognized by the reward).</li>
<li>The second question is, for an environment where rewards can be
given at any time, the agent can take a step and update the strategy
network once according to the reward. But what about the situation where
only the last step can yield a reward? In fact, we adopt round updates,
whether it is an environment where rewards can be given at every step or
only at the last step, we record all the steps within a round (s, a, r)
(except for the last step, the rewards for the other steps are 0 or -1).
Then, replace the reward for each step with the cumulative reward and
multiply it by a decay coefficient to make the reward decay with the
episode steps. Note that unlike discrete value-based learning,
policy-based PG can still calculate an action probability to compute the
loss even for steps without rewards, so even in the absence of rewards
(or negative rewards), it can make the strategy network optimize to
reduce the probability of bad actions.</li>
</ul></li>
</ul>
<h1 id="actor-critic">Actor-Critic</h1>
<ul>
<li>Clearly, strategy-based methods abandon values to gain an advantage
(continuous actions), but also bring disadvantages (after each step,
there is no value left to estimate immediate rewards, and only round
updates can be performed). A natural thought, then, is to combine these
two points: choose a strategy-based network as the actor to train the
strategy; choose a value-based network as the critic to provide values
to the strategy network and estimate immediate rewards.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="定义">定义</h1>
<ul>
<li>agent：智能体，智能体处在某一状态下，依据某种策略(policy)，采取一个动作，到达下一个状态</li>
<li>s:status，状态</li>
<li>a:action，动作</li>
<li>r:reward，奖励</li>
<li>s,a：可以称之为一步。智能体的行为可以描述为一系列步。强化学习里agent的行为可以用DP（决策过程）表示，s是DP里的节点状态，a是状态之间的转移路径</li>
<li>Q：Q表，Q(s,a)即状态s下执行动作a的价值（infer时的可能性），用奖励期望来估计</li>
</ul>
<h1 id="q-learning">Q-learning</h1>
<ul>
<li><p>随机Q表，初始化状态，开始迭代，<span
class="math inline">\(\epsilon\)</span>贪心</p></li>
<li><p>在状态s采取动作a，观察到奖励r和状态<span class="math inline">\(s
\prime\)</span></p></li>
<li><p>关键迭代公式：</p>
<p><span class="math display">\[
Q(s,a) = Q(s,a) + \alpha [r + \gamma max_{a \prime}Q(s \prime,a \prime)
- Q(s,a)] \\
\]</span></p></li>
<li><p>Q的更新包括两部分，一部分自然是奖励（假如这一步得到奖励的话），另一部分是时间差值，即TD-error，是现实和估计之间的差值。这里的现实是在新状态采取了最好动作后得到的奖励，这里的估计是指我们所有的奖励，除了最终步得到的真实奖励，其余中间步都是用Q表值来估计现实奖励，因此Q表的更新应该是加上（现实-估计），即让Q表更加贴近现实。</p></li>
<li><p>值得注意的是，只有最后一步得到奖励时（假如我们只有终点一个奖励），现实才真的是现实的奖励，否则还是用Q表估计的。</p></li>
<li><p>这里Q表的更新只与未来的状态和动作有关，在最开始，应该除了真正有奖励的最后一步，其余步骤的更新都是不确定的（因为现实也是用Q表估计的，只有最后一步现实才是现实），但第一次迭代之后最后一步的价值更新是确定的（在宝藏边上还是知道怎么走向宝藏），且与LSTM那种时间序列不同，它不是从最后一个时间步往前BPTT，而是更新了一个状态的转移价值（取哪个动作好），这个状态可能出现在每一次迭代的多个时间步上（或者说和时间步无关），接下来与该状态相邻的状态更新时，用Q表估计的现实就会准确一些，慢慢的整个算法达到收敛。</p></li>
</ul>
<h1 id="sarsa">Sarsa</h1>
<ul>
<li><p>Q-learning是off-policy的，因为存在着现实和估计的差距，我们朝着现实优化，而实际采取的是根据Q表估计的现实，即异步的(off)。</p></li>
<li><p>Sarsa是on-policy的，与Q-learning在算法上的区别：</p>
<ul>
<li>Q-learning:根据Q表选动作-执行动作观察到奖励和新状态-更新Q表-更新状态</li>
<li>Sarsa：执行动作观察到奖励和新状态-更新状态-根据Q表在新状态上选动作，在迭代之前先来一次根据Q表选动作</li>
</ul></li>
<li><p>可以看到Sarsa更改了学习算法的步骤顺序，这种更改带来了什么效果？效果就是将新状态和新动作的选取提前到Q表更新之前，这样Q表更新时，差距里现实的部分不用<span
class="math inline">\(max_{a \prime}Q(s \prime,a
\prime)\)</span>来估计，而直接用新状态和新动作：</p>
<p><span class="math display">\[
Q(s,a) = Q(s,a) + \alpha [r + \gamma Q(s \prime,a \prime) - Q(s,a)] \\
\]</span></p></li>
<li><p>也就是说现实和估计采用的是同一策略（确定的动作，而不是给定状态选最大价值），然后依然使用这个差距来更新。</p></li>
<li><p>在更新的过程中，sarsa说到做到（因为现实采用的就是新状态和新动作，agent一定会按照这样更新），而Q-learning则比较乐观（因为现实采用的是新状态下的最大价值，但实际走不一定会采取最大价值的行动，有<span
class="math inline">\(\epsilon\)</span>的扰动）。Sarsa更为保守（每一步都想走好），而Q-learning更为激进（先rush到再说）。</p></li>
</ul>
<h1 id="sarsalambda">Sarsa(<span
class="math inline">\(\lambda\)</span>)</h1>
<ul>
<li><p>naive版本的Sarsa可以看成是Sarsa(0)，因为每走一步就更新了Q表，即<span
class="math inline">\(Q(s \prime,a \prime)\)</span>只会带来上一步<span
class="math inline">\(Q(s,a)\)</span>的价值更新。假如我们考虑对所有步的更新，且离最终奖励近的步权重大，离最终奖励远的步权重小，那么调整这个权重的超参就是<span
class="math inline">\(\lambda\)</span>，权重为0就是不考虑之前的步数，即naive
Sarsa。</p></li>
<li><p>具体实现是添加一个trace矩阵E（矩阵中每一个元素对应一步(s,a)）保存所有步在该次路径中的权重，离最终奖励越近，权重越大，因此每走一步，执行的那一步元素对应矩阵值加1，然后用对应的trace矩阵值作为权重乘到奖励上来更新Q表（这里是所有的步和整个E矩阵乘起来），之后矩阵值会衰减一下，乘以更新衰减因子<span
class="math inline">\(\gamma\)</span>和权重超参<span
class="math inline">\(\lambda\)</span>。显然<span
class="math inline">\(\gamma\)</span>为1时，就是所有的状态得到了一样的更新（因为是所有步和整个E矩阵相乘）；当<span
class="math inline">\(\gamma\)</span>为0时，除了执行步元素加了1，然后整个E矩阵都置0，因此只有执行步得到了更新，即naive
Sarsa。将E矩阵的迭代展开，就可以得到与naive
sarsa一样的展开，只不过衰减的时候加了一个E(s,a)来记录某一步距离奖励的距离。</p>
<p><span class="math display">\[
\delta = r + \gamma Q(s \prime,a \prime) - Q(s,a) \\
E(s,a) = E(s,a) + 1 \\
Q = Q + \alpha \delta E \\
E = \gamma \lambda E \\
update  \ \ s,a \\
\]</span></p></li>
<li><p>这个E矩阵就是eligibility
trace。对于某一步，如果被执行了，就增加一点值，之后慢慢衰减，直到又被执行。假如短期内被执行多次，就会上升到过高值，这时可以设置阈值来限制eligibility的增加。这个值可以解释为该步在该次迭代中对于找到最终奖励的贡献程度。</p></li>
</ul>
<h1 id="dqn">DQN</h1>
<ul>
<li>Q-learning + deep neural network</li>
<li>神经网络用于把Q-table的更新过程参数化，输入一个状态向量，神经网络输出所有动作的价值，用神经网络的训练来替代简单的迭代更新。这样可以解决状态数过多导致的维度灾难问题。</li>
<li>直接这么替换不容易训练，DQN引入两个技巧
<ul>
<li>experience replay</li>
<li>fix q</li>
</ul></li>
<li>先看神经网络的输入输出和损失
<ul>
<li>有两个神经网络，一个网络参与训练（评价网络），一个网络只复制另一个网络训练得到的参数，用来生成ground
truth，即目标网络</li>
<li>参与训练的网络输入是状态，表示为s维特征向量，输出是各个动作获得的a维价值向量，损失是这个向量和ground
truth的a维向量之间的均方误差。</li>
</ul></li>
<li>DQN还包含一个记忆矩阵，维度是[记忆条数，$ 2 * s + 2$
]，每一条包含奖励、动作，老状态和新状态，即一步的所有信息。记忆只保存最近记忆条数次执行步。</li>
<li>之后神经网络的输入输出从哪里来？其实是从记忆中采样一个Batch的数据，将老状态输入评价网络得到model
output，将新状态输入目标网络得到ground truth，之后计算损失。</li>
<li>每隔一段时间目标网络才会复制评价网络，在此期间目标网络都是固定不变的，即fix
q</li>
<li>而输入是从最近的记忆当中抽取的，即experience replay</li>
<li>值得注意的是我们并不直接把评价网络输出价值向量用于计算损失，因为还没有采取动作。因此我们要先根据网络输出的价值向量采取动作，将价值向量里这些动作对应的价值用奖励更新，之后这个更新过的价值向量再参与损失计算。</li>
</ul>
<h1 id="double-dqn">Double DQN</h1>
<ul>
<li>Double DQN解决DQN over estimate的问题</li>
<li>与DQN不同之处在于，评价网络不仅接受老状态产生一个输出<span
class="math inline">\(q_{eval}\)</span>，还接受新状态产生一个输出<span
class="math inline">\(q_{eval4next}\)</span>，之后依据<span
class="math inline">\(q_{eval4next}\)</span>选取动作更新<span
class="math inline">\(q_{eval}\)</span>，而不是根据<span
class="math inline">\(q_{eval}\)</span>本身选取动作来更新。</li>
</ul>
<h1 id="dqn-with-prioritized-experience-replay">DQN with Prioritized
Experience replay</h1>
<ul>
<li>之前是从记忆库中随机采样一段记忆，这个随机采样会导致模型训练步数过多（随机很难保证到达最终奖励）</li>
<li>那么自然而然想到记忆应该分配优先级，优先级高的采样概率大</li>
<li>优先级可以用TD-error值来衡量，error大的自然要多采样多优化</li>
<li>已知优先级分布，希望得到满足该优先级分布的一个采样序列，那么这就是一个蒙特卡洛问题了，可以用MCMC，可以用Importance
sampling，也可以用论文里提到的SumTree</li>
</ul>
<h1 id="dueling-dqn">Dueling DQN</h1>
<ul>
<li><p>Dueling DQN细化了原始DQN的内部网络输入输出关系：</p>
<p><span class="math display">\[
Q(s,a;\theta,\alpha,\beta) = Value(s;\theta,\beta) +
Advantage(s,a;\theta,\alpha)
\]</span></p></li>
<li><p>即拆分成了状态带来的价值和动作在该状态上带来的价值（advantage）</p></li>
<li><p>为什么要单独考虑状态？因为有些状态是与动作无关的，无论采取什么动作都不会带来价值的改变</p></li>
</ul>
<h1 id="policy-gradient">Policy Gradient</h1>
<ul>
<li><p>之前介绍的都是基于值的强化学习方法，即在某状态查询各个动作带来的价值，选择最大价值动作执行</p></li>
<li><p>policy
gradient（策略梯度）通过策略网络输入状态，直接输出动作，跳过了计算价值的步骤，是基于策略的方法</p></li>
<li><p>策略网络并不计算某种损失进行反向传播，而是依据奖励来反向传播，更新的算法如下：</p>
<p><span class="math display">\[
\theta = \theta + \alpha \nabla _{\theta} \log \pi _{\theta} (s_t, a_t)
v_t \\
\]</span></p></li>
<li><p>需要注意以下几个关键点：</p>
<ul>
<li>这里的PG算法神经网络只负责接收状态作为输入，奖励作为梯度调整值，实际执行的动作作为gold
label，输出一个action vector，整个网络本身就是一个策略<span
class="math inline">\(\pi\)</span>（输入状态，输出动作概率分布）</li>
<li>之后整个模型依然像DQN那样，借助策略（网络）按动作概率（网络输出）选择动作，转移状态，观察环境得到奖励</li>
<li>那么第一个问题，这个策略梯度是怎么来的？可以看到梯度依然是损失对参数求导的形式，<span
class="math inline">\(- \nabla _{\theta} \log \pi _{\theta} (s_t, a_t)
v_t\)</span>，哪来的损失？实际上是执行动作的概率乘以奖励，可以看作是动作概率分布和执行动作one-hot向量之间的交叉熵（交叉熵本来就有类似look
up的效果）乘以奖励。我们就看其本来的含义：执行动作的概率乘以奖励，网络本身只是一个策略，什么样的策略是好策略？假如通过这个策略选出来的动作执行之后得到了好的奖励，那么这是一个好的策略网络，也就是能选出好动作的策略网络是好网络，显然网络的目标函数就应该是执行动作的概率（网络认可的动作）乘以奖励（奖励认可的好动作）。</li>
<li>第二个问题，对于随时能给出奖励的环境，agent可以走一步，根据奖励更新一次策略网络，那对于那些只有最后一步能够得到奖励的该咋办？事实上我们采取的是回合更新，无论是每一步都能给出奖励的环境还是只有最后一步有奖励的环境，我们都将一个回合内的所有步(s,a,r)都记录下来（除了最后一步，其余步的奖励都是0或者-1），之后每一步的奖励替换成累加奖励，并乘以一个衰减系数使得奖励随episode
steps衰减。注意不同于离散的基于值的学习，基于策略的PG在没有得到奖励的那些步也能算出一个动作概率来计算损失，因此即便是无奖励（或者负奖励），也能使得策略网络优化去让不好的动作概率降低。</li>
</ul></li>
</ul>
<h1 id="actor-critic">Actor Critic</h1>
<ul>
<li>显然基于策略的方法抛弃了值，获得了优势（连续动作），也带来了劣势（每执行一步之后没得值可以用来估计即时奖励，只能进行回合更新），那么一个自然而然的想法就是结合这两点，选择一个基于策略的网络作为actor，训练出策略；选择一个基于值的网络作为critic，用来给策略网络提供值，估计即时奖励。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Scaling the Environment</title>
    <url>/2025/07/17/env-matrix/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/18/2e9daed2f82508d5db2c449cbb90188a.png" width="400"></p>
<p>What I Talk About When I Talk About Scaling the Environment?
<span id="more"></span></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="scaling-environments">Scaling Environments</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/1844572f3d9fd6443aa0d80ced17728a.gif" width="400"></p>
<ul>
<li>In the previous blog post, we mentioned the importance of scaling
environments in the era of RL for LLMs.</li>
<li>Similar to the pre-LLM era where we scaled the quantity and quality
of data, in the RL era, we scale the difficulty of environments.</li>
<li>We can evaluate the difficulty of an environment from three
dimensions:
<ul>
<li><strong>Generation Difficulty</strong>: The difficulty of collecting
new problems/goals/tasks within the environment</li>
<li><strong>Solving Difficulty</strong>: The difficulty of the problems
assigned to the agent within the environment</li>
<li><strong>Verification Difficulty</strong>: The difficulty of
verifying whether the agent’s output is correct after completing a
task</li>
</ul></li>
<li>These difficulties determine how easy it is to build an environment,
and whether the constructed environment is sufficient to train powerful
agents.</li>
<li>A precise terminological distinction is that verification typically
refers to checking if a prediction matches a ground truth using a
verifier model; whereas a reward model generally evaluates the quality
of a prediction without having ground truth. The former emphasizes
consistency checking, the latter emphasizes obtaining a ground truth.
For the sake of discussion, we do not strictly differentiate between the
two in this post. After all, if it is hard to obtain ground truth, then
it is naturally hard to verify. For tasks that are difficult to verify,
under the discussion of a general reward model, they also correspond to
tasks that are difficult to model a reward function for.</li>
<li>Each dimension can be categorized as either simple or difficult.
This binary classification mainly emphasizes relative difficulty—e.g.,
generation being harder than solving, or verification being easier than
generation. This classification helps us clarify the goals and direction
when scaling environments. Under this classification, we can form an
environment matrix with eight subspaces.</li>
<li>It’s worth noting that we ignore two subspaces where verification is
harder than solving:
<ul>
<li>Because most RL-applicable problems exhibit a
generator-discriminator/verificator gap, i.e., it is easier to judge
whether a policy is good than to get a good policy.</li>
<li>If getting a policy is easy (e.g., proposing a mathematical
conjecture or making a future prediction), but verification is hard
(requiring higher intelligence or a long time), then in that domain, the
problem that needs solving is verification itself.</li>
<li>From this perspective, verification becomes the policy that needs to
be learned, while "policy generation" becomes more akin to problem
generation.</li>
<li>Therefore, if such subspaces are to be solved with AI methods, they
can be categorized into other subspaces. However, for completeness, we
still illustrate them here and mark them in gray to indicate their
objective existence.</li>
</ul></li>
</ul>
<h1 id="first-layer-generation-solving-and-verification-are-all-easy">First
Layer: Generation, Solving, and Verification Are All Easy</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/416ea48192da73fc891db6780832f390.png" width="500"></p>
<ul>
<li>Tasks in this category are simple in every aspect. Typically, humans
already have mature and robust templates or rules for them—e.g., unit
conversion, spelling correction, arithmetic, etc.</li>
<li>Strategies for such tasks can be written using rules and do not
require complex AI systems to learn.</li>
<li>Interestingly, although these are the simplest environment settings,
LLMs are not necessarily the best strategies here—in fact, they do not
need to be. For example, with arithmetic for countless attempts, we have
no theoretical guarantee that a language model never make mistakes,
whereas a calculator will never make mistakes. Or for arithmetic on
billion-digit numbers, the language model’s context cannot hold it all,
but a simple big integer algorithm can handle it easily.</li>
<li>Does this imply that language models are not good? Of course not.
Rather, it means that for different types of problems, we need
differently designed intelligences. Language models can simply solve
such problems by calling a calculator or writing code. When simple
problems challenge the robustness of high-level intelligence, high-level
intelligence can use induction, reasoning to organize lower-level
intelligences to solve them.</li>
</ul>
<h1 id="second-layer-either-solving-or-generation-is-very-difficult">Second
Layer: Either Solving or Generation Is Very Difficult</h1>
<div class="image-row" style="display: flex; justify-content: center; align-items: center; gap: 20px; background: white; padding: 20px;">
<p><img data-src="https://i.mji.rip/2025/07/18/347cf0be1465655fa6bd861e550a1f7a.png" width="500" style="margin: 0; display: block;">
<img data-src="https://i.mji.rip/2025/07/18/5152a5ca7844a1f2ea7c837178dc2407.png" width="500" style="margin: 0; display: block;"></p>
</div>
<style>
.image-row p {
  display: flex !important;
  margin: 0 !important;
  padding: 0 !important;
}
</style>
<ul>
<li>This layer corresponds to most current RL research for LLMs. Two
representative directions are RLHF and RLVR:
<ul>
<li><strong>RLHF</strong> corresponds to scenarios where generating
high-quality problems (data collection) is very difficult. For
product-grade LLMs, we need to collect real-world queries from actual
user logs rather than relying on simple datasets for preference
learning. Therefore, constructing challenging tasks/goals is very
difficult. Initially, the challenge of RLHF seemed to be verification
difficulty, but a series of works have shown that with high-quality
human preference data, reward models can indeed learn accurate human
preferences. Good data consists of two parts: good questions and good
model answers (not just good answer, but good rollouts from better
trained policy llm). All this depends on deploying on-policy RL into
product-grade LLMs and achieving data flywheels.</li>
<li><strong>RLVR</strong> corresponds to scenarios where solving
problems is very difficult. It was only two years after RLHF became
common in LLM post-training that the RLVR paradigm emerged. Before RLVR
was applied to mathematics, there was no shortage of math problems or
easy verification for their results. However, when the base model's
capabilities were insufficient and the search space was not optimized,
it was hard to explore strategies in early-stage RL that would yield
positive feedback. It’s like a monkey typing on a keyboard—while it
could theoretically type Shakespeare, we don’t know how long it would
take. But if RL starts from a strong base, it's like a PhD in literature
typing, making the probability of generating literature much higher.
People now realize the importance of pretraining a strong base model for
RLVR, and some mid-training efforts are also emerging.</li>
</ul></li>
</ul>
<h1 id="third-layer-only-verification-is-easy-only-generation-is-easy">Third
Layer: Only Verification Is Easy / Only Generation Is Easy</h1>
<div class="image-row" style="display: flex; justify-content: center; align-items: center; gap: 20px; background: white; padding: 20px;">
<p><img data-src="https://i.mji.rip/2025/07/18/7be89f0c106e83509ee5926096aa6d75.png" width="500" style="margin: 0; display: block;">
<img data-src="https://i.mji.rip/2025/07/18/b8d96a4a7d43b700e2fa37ba101ca386.png" width="500" style="margin: 0; display: block;"></p>
</div>
<ul>
<li>This layer corresponds to directions we are about to explore. It is
hard, but as more effort is invested, these environments will be
gradually constructed to train more advanced intelligence.
<ul>
<li><strong>Only Verification Is Easy</strong>: Both generation and
solving are difficult. A typical example would be the highest-difficulty
math problems. Math problems with standard answers are always easy to
verify, and currently, most high-difficulty math problems can be solved
by LLMs. To further improve intelligence, we need even harder math
problems. But where do we collect them? That’s the difficulty—requiring
the smartest human minds to continuously produce more difficult (but
solvable) problems to train the models. This process is clearly
unsustainable and cannot scale up. Humanity's final mathematical
frontier can be updated annually, but it will become thinner and
thinner, and the fact that problems must be solvable by humans limits
the upper bound of this type of intelligence. If AI generates and solves
the problems, it violates the Generator-Verifier Gap. Therefore,
constructing this type of environment is
resource-constrained—specifically, limited by human intellectual
resources.</li>
<li><strong>Only Generation Is Easy</strong>: Both verification and
solving are difficult. The main challenge here lies in
verification—tasks that are subjective, require semantic understanding,
lack unified evaluation standards, or have high time/labor verification
costs, such as artistic/literary creation, policymaking, education, and
healthcare. In these areas, we have a vast number of problems to solve,
but it is very difficult to determine whether AI has solved them. A key
feature of this subspace is human participation. AI will become part of
human civilization, participating in and influencing social activities,
receiving feedback from human society. This is a far more challenging
direction—optimizing a system that includes both humans and AI.</li>
</ul></li>
</ul>
<h1 id="final-layer-expert-level-superhuman">Final Layer: Expert-Level /
Superhuman</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/f1060d46cd95ba9f11ecbbbf1b9f9f7a.png" width="500"></p>
<ul>
<li>This subspace is difficult in all dimensions. We cannot take
shortcuts by leveraging one dimension being easier than the others to
train intelligence. I currently cannot give an example of this subspace,
but it must exist. Perhaps at this level, AI will develop AI, regulate
AI, and leverage AI.</li>
</ul>
<h1 id="citation">Citation</h1>
<p>If you found the topics in this blog post interesting and would like
to cite it, you may use the following BibTeX entry: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_scaling_202507,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Scaling the Environment},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {7},</span><br><span class="line">  url = {https://thinkwee.top/2025/07/17/env-matrix/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="scaling-环境">Scaling 环境</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/1844572f3d9fd6443aa0d80ced17728a.gif" width="600"></p>
<ul>
<li>在上一篇博文中，我们提到在RL for LLM时代，scaling环境的重要性。</li>
<li>类似前LLM时代，我们scale数据的数量和质量，在RL时代，我们scale环境的难度。</li>
<li>我们可以从三个维度来衡量环境的难度：
<ul>
<li>生成难度：生成一个环境并在环境中收集新的问题/目标/任务时的难度</li>
<li>解决难度：在环境中，智能体解决问题需要多高的智能水平</li>
<li>验证难度：智能体完成任务之后，验证这个智能体的交付结果是否正确的难度</li>
</ul></li>
<li>这些难度决定了环境是否容易构建，以及构建出来的环境是否足以训练强大的智能体。</li>
<li>一个严谨的术语上的区别是，验证往往指给定ground truth，verifier
model会判断prediction是否和ground truth一致；而reward
model通常会在没有ground
truth的情况下直接判断prediction的好坏。前者强调判断一致，后者强调得到ground
truth。本文为了方便讨论，不严格区分两者，毕竟如果难以得到ground
truth的话，自然也就难以验证。而对于难以验证的问题，在general reward
model的讨论下，也对应着难以建模reward model。</li>
<li>每个维度都可以分为简单和困难，这种二分类主要是强调相对难度，即生成比解决困难，或者验证比生成容易。这种划分方式可以帮助我们梳理scaling环境时的目标和方向。在这种划分方式下，我们可以得到环境矩阵，其包含八个子空间。</li>
<li>值得注意的是，我们忽略了验证比解决困难的两个子空间，
<ul>
<li>因为大部分适用于RL的问题都存在generator-discriminator/verificator
gap，即判断策略好不好比得到一个最优策略容易。</li>
<li>如果得到策略容易（比如提出一个数学猜想，或者对未来做一些预测），但是很难验证（需要高智力水平或者长时间来验证），那么在这个领域，需要解决的问题是完成验证，而不是提出猜想/策略。</li>
<li>从这个视角来看，完成验证反而成为了需要学习的策略，而“得到策略”更像是生成问题</li>
<li>因此这类子空间，如果要用AI的方式解决，则可以归为其他子空间，但在本文中，我们依然将其画出来，并标为灰色，代表其客观存在。</li>
</ul></li>
</ul>
<h1 id="第一层生成解决验证均简单">第一层，生成、解决、验证均简单</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/416ea48192da73fc891db6780832f390.png" width="500"></p>
<ul>
<li>这类任务无论从哪个角度都很简单，一般而言人类早已形成成熟的，鲁棒的模板或者规则，例如单位换算、拼写纠错、加减法等等</li>
<li>这类任务的策略可以用规则编写而成，无需使用复杂的AI系统学习</li>
<li>有趣的是虽然这是最简单的环境设置，但是大语言模型不一定是最好的策略，或者说，无需在该设置下成为最好的策略。例如加减法，进行无数次加减法，我们没有理论保障语言模型不出错，但计算器永远不会出错，又或者进行上亿位数字的加减法，语言模型的context无法装下，但是一个简单的大整数算法就能处理</li>
<li>这难道说明语言模型不行吗？当然不是，而是对于不同类型的问题，我们需要不同设计的智能。语言模型也可以简单的通过function
call调用计算器或者写代码来解决这些问题。当简单的问题挑战了高级智能的鲁棒性时，高级智能可以通过归纳、推理，使用低级智能来解决这类问题。</li>
</ul>
<h1 id="第二层解决或者生成很困难">第二层：解决或者生成很困难</h1>
<div class="image-row" style="display: flex; justify-content: center; align-items: center; gap: 20px; background: white; padding: 20px;">
<p><img data-src="https://i.mji.rip/2025/07/18/347cf0be1465655fa6bd861e550a1f7a.png" width="500" style="margin: 0; display: block;">
<img data-src="https://i.mji.rip/2025/07/18/5152a5ca7844a1f2ea7c837178dc2407.png" width="500" style="margin: 0; display: block;"></p>
</div>
<ul>
<li>这一层对应着当前LLM大部分的RL研究，两个代表性方向就是RLHF和RLVR
<ul>
<li>RLHF对应着生成新的问题（收集数据）很困难的情况。对于产品级别的LLM，我们需要从实际的用户使用日志中收集真实世界的query，而不是在简单的数据集上完成偏好学习。因此构建有挑战的任务/目标非常困难。RLHF一开始的挑战似乎在于很难验证，但一系列工作表明如果有了高质量的人类偏好数据，reward
model是足以学习到准确的人类偏好。好的数据包含两部分，好的提问，好的模型的回答（不仅仅是好的回答，而且是训练得到的更好的policy
model的rollouts），这一切依赖于将on-policy
RL部署到产品级LLM里，通过数据飞轮实现。</li>
<li>RLVR对应着解决问题很困难的情况。在LLM
post-training普遍使用RLHF两年之后，才出现RLVR范式。在RLVR应用于数学领域之前，数学题的资源并不缺乏，数学题结果也非常容易验证，但是在基座模型能力不够强的情况下，搜索的空间没有经过优化，我们难以在RL早期探索出得到正向反馈的策略。类似猴子敲键盘，虽然理论上可以敲出莎士比亚著作，但不知敲到猴年马月。但如果从一个好的基座开始RL，就如同让一个文学博士敲键盘，敲出著作的概率要大大增加。现在人们意识到了pre-train一个优秀底座对于RLVR的重要性，很多mid-training的工作也开始兴起。</li>
</ul></li>
</ul>
<h1 id="第三层仅仅验证容易仅仅生成容易">第三层：仅仅验证容易/仅仅生成容易</h1>
<div class="image-row" style="display: flex; justify-content: center; align-items: center; gap: 20px; background: white; padding: 20px;">
<p><img data-src="https://i.mji.rip/2025/07/18/7be89f0c106e83509ee5926096aa6d75.png" width="500" style="margin: 0; display: block;">
<img data-src="https://i.mji.rip/2025/07/18/b8d96a4a7d43b700e2fa37ba101ca386.png" width="500" style="margin: 0; display: block;"></p>
</div>
<ul>
<li>这一层对应着我们接下来探索的方向。虽然很难，但我相信随着人们投入的增加，这两类环境会被逐渐构筑起来，训练更高水平的智能
<ul>
<li>仅仅验证容易：生成和解决都很困难。一个典型的例子是，最顶尖难度的数学题。拥有标准答案的数学题永远容易验证，而当前世界上绝大部分高难度数学题也能被LLM解决，如果我们想要进一步提升智能，就需要更高难度的数学题。从何处收集？这是一个难题，需要人类最聪明的头脑不断的产生更难的（且人类已经解出）的难题，然后训练模型完成。这个过程显然不可持续也无法scale
up。人类最后的数学防线每年都可以更新，但会越来越单薄，而且人类可解的前提限制了这一类智能的上限。如果是AI出题、AI解题，则违背了Generator-Verifier
Gap。因此这类环境构建的难度在于资源（人类的智力资源）</li>
<li>仅仅生成容易：验证和解决都很困难。这类任务主要的难点在于验证，即一些主观的/需要语义判断的/验证标准不统一的/需要高额时间成本或者人力成本验证的任务，例如艺术/文学创作、政策制定、教育和健康。在这些领域我们有海量的问题需要解决，但是很难判断AI能否解决这些问题。这个子空间一个重要的特征就是人的参与。AI会作为人类文明的一部分，参与并影响人类的社会活动，从人类社会获得反馈。这是更加难以探索的一个方向，将人与AI作为一个整体的系统去优化。</li>
</ul></li>
</ul>
<h1 id="最后一层专家级别超出人类级别">最后一层：专家级别/超出人类级别</h1>
<p><img data-src="https://i.mji.rip/2025/07/18/f1060d46cd95ba9f11ecbbbf1b9f9f7a.png" width="500"></p>
<ul>
<li>这个子空间无论从哪个维度来看，都很难。我们没法借助一个维度比另一个维度更加简单的特点来取巧去训练智能。我暂时无法给出这个子空间的例子，但其必然存在。也许到了这个层次，AI会发展AI，
监管AI，利用AI。</li>
</ul>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:
</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_scaling_202507,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Scaling the Environment},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {7},</span><br><span class="line">  url = {https://thinkwee.top/2025/07/17/env-matrix/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>rl</tag>
        <tag>scaling</tag>
        <tag>llm</tag>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Glove Embedding - Mathematical Derivation</title>
    <url>/2019/01/13/glove/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/681b34e5ff4f1b7bf5780a5eb7a984ce.png" width="500"/></p>
<ul>
<li>Record the mathematical derivation of GloVe word vectors, as the
original paper does not derive the model graphically but rather
calculates the objective function through pure mathematical operations.
This design approach is very interesting, and it also writes out and
compares the mathematical essence of word2vec.</li>
<li>GloVe: Global Vectors for Word Representation</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="word-vectors">Word vectors</h1>
<ul>
<li>Whether based on global matrix factorization or local window-based
word vectors, the method of extracting semantics is to mine meaning from
the co-occurrence statistical information between words.</li>
<li>Clearly, the global approach does not make use of the advantages of
the local one: for example, global techniques such as LSA are
insensitive to local contextual information, making it difficult to mine
synonyms based on context; the local approach does not make use of the
advantages of the global one, as it only relies on independent local
contexts, and if the window is too small, it cannot effectively utilize
the information of the entire document or corpus.</li>
<li>The GloVe approach is to utilize the global word co-occurrence
matrix while also calculating relevance using local contextual
relationships.</li>
<li>The result of word vectors is a mapping that generates meaningful
semantic relationships based on distance relationships. To achieve this
goal, GloVe designed a log-bilinear regression model and specifically
adopted a weighted least mean square regression model to train word
vectors.</li>
</ul>
<h1 id="discovery">Discovery</h1>
<ul>
<li>Definition:
<ul>
<li>For a single word.</li>
<li>The number of occurrences of <span
class="math inline">\(x_j\)</span> in the context of <span
class="math inline">\(x_i\)</span> .</li>
<li>The number of times all words appear in the context of <span
class="math inline">\(x_i\)</span> .</li>
<li>The probability of <span class="math inline">\(x_j\)</span>
appearing in the context of <span class="math inline">\(x_i\)</span> ,
which is the probabilization of the frequency count of the context
occurrence, referred to as "co-occurrence probabilities" in the
paper.</li>
<li><span class="math inline">\(r = \frac {P_{ik}}{P_{jk}}\)</span> :
Introduce an intermediate word <span class="math inline">\(x_k\)</span>
, referred to as "probe word" in the paper, by introducing this <span
class="math inline">\(x_k\)</span> , it can indirectly measure the
relationship between <span class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> , represented by <span
class="math inline">\(r\)</span> , i.e., the ratio.</li>
</ul></li>
<li>The role of introduction is reflected in two aspects:
<ul>
<li>For the <span class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> to be compared, filter out the <span
class="math inline">\(x_k\)</span> without discriminative power, which
is noise. When <span class="math inline">\(r \approx 1\)</span> , <span
class="math inline">\(x_k\)</span> is considered noise.</li>
<li>Given <span class="math inline">\(x_k\)</span> , such that those
<span class="math inline">\(r &gt;&gt; 1\)</span> of <span
class="math inline">\(x_i\)</span> have similar meanings, and those
<span class="math inline">\(r &lt;&lt; 1\)</span> of <span
class="math inline">\(x_j\)</span> have similar meanings.</li>
</ul></li>
<li>Therefore, we can filter out noise and only mine word sense
relationships from co-occurrence data where <span
class="math inline">\(r\)</span> is very large or very small.</li>
</ul>
<h1 id="design">Design</h1>
<ul>
<li><p>Next, the author directly applies the target design
function.</p></li>
<li><p>The goal is: the distance calculation results between the word
vectors designed should reflect the ratio previously discovered from the
word co-occurrence matrix, specifically for the triplet, words i, j, and
the probe word k, the word vectors of these three words should embody
r</p></li>
<li><p>Then, directly designing, defining <span
class="math inline">\(w_i\)</span> as the word vector corresponding to
<span class="math inline">\(x_i\)</span> , we assume <span
class="math inline">\(F\)</span> to be the function for calculating
distance:</p>
<p><span class="math display">\[
F(w_i,w_j,w^{*}_k) = r \\
= \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>The word vectors of above <span
class="math inline">\(w_k\)</span> are distinguished by asterisks from
the word vectors of <span class="math inline">\(w_i\)</span> and <span
class="math inline">\(w_j\)</span> , because <span
class="math inline">\(w_k\)</span> is an independent context word
vector, parallel to the required word vectors, similar to the forward
and backward word embedding matrices in word2vec.</p></li>
<li><p>Next, a natural thought is to reduce the parameters, i.e., only
the word vectors and the context word vectors are needed, because it is
a distance calculation function and the vector space is a linear space;
we use the vector difference between <span
class="math inline">\(w_i\)</span> and <span
class="math inline">\(w_j\)</span> as the parameters:</p>
<p><span class="math display">\[
F(w_i - w_j,w^{*}_k) = \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>The current function takes a vector as its parameter, and outputs
a tensor. The simplest structure is to perform a dot product:</p>
<p><span class="math display">\[
F((w_i-w_j)^T w^{*}_k) = \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>The next key point is symmetry. Noticing that although context
and non-context word vectors are distinguished, since the co-occurrence
matrix <span class="math inline">\(X\)</span> is symmetric, the two sets
of word vectors <span class="math inline">\(w\)</span> and <span
class="math inline">\(w^{\*}\)</span> should have the same effect. This
is because the values of the two sets of word vectors are different due
to different random initialization, but they should be the same in terms
of measuring similarity, that is, <span class="math inline">\(w_i^T
w^{\*}_j\)</span> and <span class="math inline">\(w_j^T
w^{\*}_i\)</span> should be the same.</p></li>
<li><p>Due to symmetry, <span class="math inline">\(x_i,x_j,x_k\)</span>
can be any word in the corpus, so the two parameters of the <span
class="math inline">\(F\)</span> function should be interchangeable (
<span class="math inline">\(w\)</span> and <span
class="math inline">\(w^{\*}\)</span> , <span
class="math inline">\(X\)</span> and <span
class="math inline">\(X^T\)</span> ), and here a bit of mathematical
technique is further applied to symmetrize the function:</p>
<ul>
<li><p>Design:</p>
<p><span class="math display">\[
F((w_i-w_j)^T w^{*}_k) = \frac {F(w_i w^{*}_k)} {F(w_j w^{*}_k)} \\
\]</span></p></li>
<li><p>Then both the numerator and denominator are of the same form,
that is</p>
<p><span class="math display">\[
F(w_i w^{*}_k) = P_{ik} = \frac {X_{ik}} {X_i} \\
\]</span></p></li>
<li><p>To satisfy the above <span class="math inline">\(F\)</span> , it
can be decomposed into two sub- <span class="math inline">\(F\)</span> ,
and then <span class="math inline">\(F\)</span> can be the <span
class="math inline">\(exp\)</span> function, i.e</p>
<p><span class="math display">\[
w_i^T w_k^{*} = log(X_{ik}) - log {X_i} \\
\]</span></p></li>
<li><p>The indices k, i, j can be interchanged without changing the
meaning. Since the numerator and denominator have the same form, we only
need to ensure that this form is satisfied; the fraction will naturally
satisfy the mapping from the triplet to the ratio thereafter.</p></li>
<li><p>Noticing that in the above formula, the inner product of the two
vectors on the left remains unchanged when the i,k symbols are
interchanged, while the subtraction of the two log expressions on the
right does not satisfy this symmetry. Therefore, we add an <span
class="math inline">\(log{x_k}\)</span> to make it symmetric and
simplify it to the bias <span class="math inline">\(b^{*}\)</span> .
Similarly, after interchanging the i,k symbols, we add an <span
class="math inline">\(Log{x_i}\)</span> to make it symmetric, i.e., the
bias <span class="math inline">\(b_i\)</span> . The bias, like word
vectors, also consists of two sets:</p>
<p><span class="math display">\[
w_i^Tw_k^{*} + b_i + b_k^{*} = log(X_{ik}) \\
\]</span></p></li>
<li><p>Finally, add smoothing to prevent the log parameter from being
0:</p>
<p><span class="math display">\[
w_i^Tw_k^{*} + b_i + b_k^{*} = log(1 + X_{ik}) \\
\]</span></p></li>
</ul></li>
<li><p>Here we have preliminarily completed the design of the <span
class="math inline">\(F\)</span> function, but there is still an issue
that it averages the weights of each co-occurrence, while in general
corpora, most co-occurrences have very low frequencies</p></li>
<li><p>The solution for Glove is to use weighted functions. After
weighting, the training of word vectors is regarded as a least mean
square error regression of the F function, and the loss function is
designed:</p>
<p><span class="math display">\[
J = \sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{*} + b_i + b_j^{*} - log (1 +
X_{ij}))^2 \\
\]</span></p></li>
<li><p>Among which, f is the weighted function, with its parameters
being the co-occurrence frequency; the author points out that this
function must satisfy three properties:</p>
<ul>
<li>Clearly, if no co-occurrence occurs, the weight is 0.</li>
<li>Non-decreasing: The higher the co-occurrence frequency, the greater
the weight.</li>
<li>relatively small for large X: To prevent over-weighting for certain
common co-occurrences with high frequencies, which may affect the
results.</li>
</ul></li>
<li><p>Based on the above three properties, the author designed a
truncated weighted function within the threshold <span
class="math inline">\(X_{max}\)</span></p>
<p><span class="math display">\[
f(x) = (\frac {x}{X_{max}}) ^ {\alpha} \\
\]</span></p>
<p>If exceeding the threshold, the function value is 1.</p></li>
</ul>
<h1 id="comparing-with-word2vec">Comparing with Word2vec</h1>
<ul>
<li><p>For the skip-gram model in Word2vec, the goal is to maximize the
probability of predicting the correct central word given the context,
which is generally probabilized through the softmax function, i.e.:</p>
<p><span class="math display">\[
Q_{ij} = \frac {exp (w_i^T w_j^{*})} { \sum _{k=1}^V exp(w_i^T w_k^{*})}
\\
\]</span></p></li>
<li><p>Through gradient descent, the overall loss function can be
written as:</p>
<p><span class="math display">\[
J = - \sum _{i \in corpus , j \in context(i)} log Q_{ij} \\
\]</span></p></li>
<li><p>Group the same <span class="math inline">\(Q_{ij}\)</span> first
and then sum up to get:</p>
<p><span class="math display">\[
J = - \sum _{i=1}^V \sum _{j=1}^V X_{ij} log Q_{ij} \\
\]</span></p></li>
<li><p>Next, further transformations are made using the previously
defined symbols:</p>
<p><span class="math display">\[
J = - \sum _{i=1^V} X_i \sum _{j=1}^V P_{ij} log Q_{ij} \\
= \sum _{i=1}^V X_i H(P_i,Q_i) \\
\]</span></p></li>
<li><p>That is to say, the loss function of Word2vec is actually
weighted cross-entropy, however, cross-entropy is only one possible
measure and has many drawbacks:</p>
<ul>
<li>Probability requiring normalization as a parameter</li>
<li>Softmax computation is computationally intensive, referred to as the
model's computational bottleneck</li>
<li>For long-tailed distributions, cross-entropy often assigns too much
weight to less likely items</li>
</ul></li>
<li><p>Solution to the above problems: Simply do not normalize, directly
use co-occurrence counts, do not use cross-entropy and softmax, directly
use mean squared error, let <span class="math inline">\(Q_{ij} =
exp(w_i^T w_j^{*})\)</span> , <span class="math inline">\(P_{ij} =
X_{ij}\)</span> , then:</p>
<p><span class="math display">\[
J = \sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\
\]</span></p></li>
<li><p>However, non-normalization can cause numerical overflow, so take
the logarithm again:</p>
<p><span class="math display">\[
J = \sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\
=  \sum _{i,j} X_i (w_i^T w_j^{*} - log X_{ij})^2 \\
\]</span></p></li>
<li><p>Thus, the simplest objective function of GloVe is
obtained.</p></li>
<li><p>The authors of Word2vec found that filtering out some common
words could improve the effectiveness of word vectors, and the weighted
function in Word2vec is denoted as <span
class="math inline">\(f(X_i)=X_i\)</span> , thus filtering out common
words is equivalent to designing a non-decreasing weighted function.
GloVe designed a more sophisticated weighted function.</p></li>
<li><p>Therefore, from the perspective of mathematical derivation, GloVe
simplifies the objective function of Word2vec, replacing cross-entropy
with mean squared error and redesigning the weighting function.</p></li>
</ul>
<h1 id="concept">Concept</h1>
<ul>
<li>The paper provides a good idea for designing a model, namely,
designing the objective function based on evaluation indicators, and
then training the model to obtain the parameters (by-products) as the
desired results.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="词向量">词向量</h1>
<ul>
<li>无论是基于全局矩阵分解的还是基于局部窗口的词向量，其提取semantic的方式都是从词与词的共现统计信息中挖掘意义。</li>
<li>显然，全局的方式没有利用到局部的优点：全局例如LSA等技术对于局部上下文信息不敏感，难以根据上下文挖掘近义词；局部的方式没有利用到全局的优点，它只依赖于独立的局部上下文，窗口太小的话不能有效利用整个文档乃至语料的信息。</li>
<li>Glove的思路是利用全局的词与词共现矩阵，同时利用局部上下文关系计算相关性。</li>
<li>词向量的结果是能产生有意义的语义关系到距离关系的映射，针对这个目标，Glove设计了一个log-bilinear回归模型，并具体采用一个加权最小均方回归模型来训练词向量。</li>
</ul>
<h1 id="发现">发现</h1>
<ul>
<li>定义：
<ul>
<li><span class="math inline">\(x\)</span>：为单个词。</li>
<li><span class="math inline">\(X_{ij}\)</span>：<span
class="math inline">\(x_j\)</span> 出现在<span
class="math inline">\(x_i\)</span>的上下文中的次数。</li>
<li><span class="math inline">\(X_i = \sum _k
x_{ik}\)</span>：所有词出现在<span
class="math inline">\(x_i\)</span>的上下文中的次数。</li>
<li><span class="math inline">\(P_{ij} = P(j|i) = \frac {x_{ij}}
{X_i}\)</span>：<span class="math inline">\(x_j\)</span>出现在<span
class="math inline">\(x_i\)</span>的上下文中的概率，即上下文出现频次计数概率化，论文中称之为"co-occurrence
probabilities"。</li>
<li><span class="math inline">\(r = \frac
{P_{ik}}{P_{jk}}\)</span>：引入中间词<span
class="math inline">\(x_k\)</span>，论文中叫"probe
word"，通过引入这个<span
class="math inline">\(x_k\)</span>可以间接的衡量<span
class="math inline">\(x_i\)</span>和<span
class="math inline">\(x_j\)</span>的关系，通过<span
class="math inline">\(r\)</span>即ratio表示。</li>
</ul></li>
<li><span class="math inline">\(r\)</span>引入的作用体现在两个方面：
<ul>
<li>对于要比较的<span class="math inline">\(x_i\)</span>和<span
class="math inline">\(x_j\)</span>，筛除对于没有区分度的<span
class="math inline">\(x_k\)</span>，也就是噪音。当<span
class="math inline">\(r \approx 1\)</span>时，<span
class="math inline">\(x_k\)</span>即为噪音。</li>
<li>给定<span class="math inline">\(x_k\)</span>，使得<span
class="math inline">\(r &gt;&gt; 1\)</span>的那些<span
class="math inline">\(x_i\)</span>具有相近的词义，使得<span
class="math inline">\(r &lt;&lt; 1\)</span>的那些<span
class="math inline">\(x_j\)</span>具有相近的词义。</li>
</ul></li>
<li>因此，我们可以过滤噪音，仅仅在<span
class="math inline">\(r\)</span>很大或很小的词共现数据中挖掘词义关系。</li>
</ul>
<h1 id="设计">设计</h1>
<ul>
<li><p>接下来，作者直接根据目标设计函数。</p></li>
<li><p>目标是：设计出来的词向量之间的距离计算结果应该能够反映之前我们从词共现矩阵中发现的ratio，具体而言是对于三元组，词i,j和probe
word k，这三个词的词向量能够体现r</p></li>
<li><p>那么直接设计,定义<span class="math inline">\(w_i\)</span>为<span
class="math inline">\(x_i\)</span>对应的词向量，则假设<span
class="math inline">\(F\)</span>为计算距离的函数：</p>
<p><span class="math display">\[
F(w_i,w_j,w^{*}_k) = r \\
= \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>上面<span
class="math inline">\(w_k\)</span>的词向量加了星号区别于<span
class="math inline">\(w_i\)</span>和<span
class="math inline">\(w_j\)</span>的词向量，因为<span
class="math inline">\(w_k\)</span>是独立的上下文词向量，与我们需要的词向量是平行的两套，类似于word2vec里面的前后词嵌入矩阵。</p></li>
<li><p>接下来，一个自然的想法是，减少参数，即只需要词向量和上下文词向量，因为是距离计算函数且向量空间是线性空间，我们使用<span
class="math inline">\(w_i\)</span>和<span
class="math inline">\(w_j\)</span>的向量差作为参数：</p>
<p><span class="math display">\[
F(w_i - w_j,w^{*}_k) = \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>现在函数的参数是向量，输出是张量，最简单的一个结构就是做点乘：</p>
<p><span class="math display">\[
F((w_i-w_j)^T w^{*}_k) = \frac {P_{ik}}{P_{jk}} \\
\]</span></p></li>
<li><p>接下来的一个关键点：对称。注意到虽然区分了上下文和非上下文词向量，但是由于共现矩阵<span
class="math inline">\(X\)</span>是对称的，因此两套词向量<span
class="math inline">\(w\)</span>和<span
class="math inline">\(w^{\*}\)</span>应该具有相同的效果，只是由于随机初始化不同，两套词向量的值不一样，在衡量相似度时应该是一样的目标，即<span
class="math inline">\(w_i^T w^{\*}_j\)</span>和<span
class="math inline">\(w_j^T w^{\*}_i\)</span>一样。</p></li>
<li><p>由于对称性，<span
class="math inline">\(x_i,x_j,x_k\)</span>可以是语料中任意词，因此<span
class="math inline">\(F\)</span>函数的两个参数应该是可以交换位置（<span
class="math inline">\(w\)</span>和<span
class="math inline">\(w^{\*}\)</span>，<span
class="math inline">\(X\)</span>和<span
class="math inline">\(X^T\)</span>），那这里进一步运用了一点数学技巧将函数对称化：</p>
<ul>
<li><p>设计：</p>
<p><span class="math display">\[
F((w_i-w_j)^T w^{*}_k) = \frac {F(w_i w^{*}_k)} {F(w_j w^{*}_k)} \\
\]</span></p></li>
<li><p>那么分子分母都是一样的形式，即</p>
<p><span class="math display">\[
F(w_i w^{*}_k) = P_{ik} = \frac {X_{ik}} {X_i} \\
\]</span></p></li>
<li><p>要满足上面<span
class="math inline">\(F\)</span>可以拆分为两个子<span
class="math inline">\(F\)</span>的比，则<span
class="math inline">\(F\)</span>可以为<span
class="math inline">\(exp\)</span>函数，即</p>
<p><span class="math display">\[
w_i^T w_k^{*} = log(X_{ik}) - log {X_i} \\
\]</span></p></li>
<li><p>这样k,i,j下标可互换位置且表达意思一致。由于分子分母形式一致，因此我们只要关注这个形式能够满足就行了，之后求分数自然会满足从三元组到ratio的映射。</p></li>
<li><p>注意到上面式子当中，左边的两个向量内积，i,k符号互换值不变，而右边的两个log式子相减并不满足这种对称，因此我们补上一个<span
class="math inline">\(log{x_k}\)</span>使之对称，并将其简化为偏置<span
class="math inline">\(b^{*}\)</span>，同样的道理，i,k符号互换后，补上一个<span
class="math inline">\(Log{x_i}\)</span>使之对称，即偏置<span
class="math inline">\(b_i\)</span>，偏置和词向量一样，也是两套：</p>
<p><span class="math display">\[
w_i^Tw_k^{*} + b_i + b_k^{*} = log(X_{ik}) \\
\]</span></p></li>
<li><p>最后加上平滑，防止log的参数取0：</p>
<p><span class="math display">\[
w_i^Tw_k^{*} + b_i + b_k^{*} = log(1 + X_{ik}) \\
\]</span></p></li>
</ul></li>
<li><p>到这里我们已经初步完成了<span
class="math inline">\(F\)</span>函数的设计，但这个还存在的一个问题是，它是平均加权每一个共现的，而一般语料中大部分共现都频次很低</p></li>
<li><p>Glove的解决办法是使用加权函数。加权之后将词向量的训练看成是F函数的最小均方误差回归，设计损失函数：</p>
<p><span class="math display">\[
J = \sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{*} + b_i + b_j^{*} - log (1 +
X_{ij}))^2 \\
\]</span></p></li>
<li><p>其中f为加权函数，其参数是共现频次，作者指出该函数必须满足三条性质：</p>
<ul>
<li><span
class="math inline">\(f(0)=0\)</span>：显然，没有出现共现则权重为0。</li>
<li>Non-decreasing：共现频次越大则权重越大。</li>
<li>relatively small for large
X：防止对于某些频次很高的常见共现加权过大，影响结果。</li>
</ul></li>
<li><p>基于以上三种性质，作者设计了截尾的加权函数，在阈值<span
class="math inline">\(X_{max}\)</span>以内：</p>
<p><span class="math display">\[
f(x) = (\frac {x}{X_{max}}) ^ {\alpha} \\
\]</span></p>
<p>超过阈值则函数值为1.</p></li>
</ul>
<h1 id="与word2vec比较">与Word2vec比较</h1>
<ul>
<li><p>对于Word2vec中的skip-gram模型，其目标是最大化给定上下文之后预测正确中心词的概率，一般通过softmax函数将其概率化，即：</p>
<p><span class="math display">\[
Q_{ij} = \frac {exp (w_i^T w_j^{*})} { \sum _{k=1}^V exp(w_i^T w_k^{*})}
\\
\]</span></p></li>
<li><p>通过梯度下降求解，则整体损失函数可以写成：</p>
<p><span class="math display">\[
J = - \sum _{i \in corpus , j \in context(i)} log Q_{ij} \\
\]</span></p></li>
<li><p>将相同的<span
class="math inline">\(Q_{ij}\)</span>先分组再累加，得到：</p>
<p><span class="math display">\[
J = - \sum _{i=1}^V \sum _{j=1}^V X_{ij} log Q_{ij} \\
\]</span></p></li>
<li><p>接下来用之前定义的符号进一步变换：</p>
<p><span class="math display">\[
J = - \sum _{i=1^V} X_i \sum _{j=1}^V P_{ij} log Q_{ij} \\
= \sum _{i=1}^V X_i H(P_i,Q_i) \\
\]</span></p></li>
<li><p>也就是说，Word2vec的损失函数实际上是加权的交叉熵，然而交叉熵只是一种可能的度量，且具有很多缺点：</p>
<ul>
<li>需要归一化的概率作为参数</li>
<li>softmax计算量大，称为模型的计算瓶颈</li>
<li>对于长尾分布，交叉熵常常分配给不太可能的项太多权重</li>
</ul></li>
<li><p>解决以上问题的方法：干脆不归一化，直接用共现计数，不用交叉熵和softmax，直接用均方误差，令<span
class="math inline">\(Q_{ij} = exp(w_i^T w_j^{*})\)</span>，<span
class="math inline">\(P_{ij} = X_{ij}\)</span>，则：</p>
<p><span class="math display">\[
J = \sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\
\]</span></p></li>
<li><p>但是不归一化会造成数值上溢，那就再取个对数：</p>
<p><span class="math display">\[
J = \sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\
=  \sum _{i,j} X_i (w_i^T w_j^{*} - log X_{ij})^2 \\
\]</span></p></li>
<li><p>这样就得到了Glove最朴素的目标函数。</p></li>
<li><p>Word2vec的作者发现筛除一些常见词能够提高词向量效果，而Word2vec中的加权函数即<span
class="math inline">\(f(X_i)=X_i\)</span>，因此筛除常见词等价于设计一个非降的加权函数。Glove则设计了更为精巧的加权函数。</p></li>
<li><p>因此从数学公式推导上看，Glove简化了Word2vec的目标函数，用均方误差替换交叉熵，并重新设计了加权函数。</p></li>
</ul>
<h1 id="思路">思路</h1>
<ul>
<li>该文提供了一个很好的设计模型的思路，即根据评测指标设计目标函数，反过来训练模型，得到函数的参数（副产品）作为所需的结果。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>glove</tag>
        <tag>word embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Towards General Reasoning</title>
    <url>/2025/09/13/general-reasoning/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/09/13/6afdb242505675e387a7e2498a292346.png" width="400"></p>
<p>How far have we gone towards general reasoning? How far are we from
the general reasoning? <span id="more"></span></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><p><img data-src="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png" width="400"></p>
<p>Recently our paper <em>NOVER: Incentive Training for Language Models
via Verifier-Free Reinforcement Learning</em> was accepted to EMNLP.
NOVER uses the LLM's perplexity of the ground truth conditioned on the
reasoning trajectory as the reward, which extends the RLVR paradigm
beyond math and code, enabling learning of general reasoning on
arbitrary text-to-text tasks without extra models or verifiers.</p>
<p><img data-src="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png" width="600"></p>
<p>When we began NOVER in February, most RLVR work focused on
mathematical and code reasoning; RLVR research that targets general or
hard-to-verify domains was scarce. Nearly six months later many
interesting related papers have emerged. Due to limited resources, many
ideas in our experiments were not fully validated and were left out of
the paper. This post organizes those ideas and surveys recent relevant
work to assess how far we have come on general reasoning and how far we
still must go.</p>
<h1 id="what">What</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png" width="1000">
<figcaption>
NOVER extends RLVR to general reasoning like Science Explanation and
Proof, Social Reasoning, Creative Writing and Translation
</figcaption>
</figure>
<p>NOVER’s target problem is, for tasks whose answers are unstructured
natural text and therefore unsuitable for rule-based verifiers, how can
we apply RLVR to acquire reasoning ability?</p>
<p>Such cases are common. A math problem often has a unique canonical
answer that can be placed in a <code>\boxed{}</code>, while explanations
of physical or chemical phenomena can legitimately take many different
forms (think of long, opinionated answers on Quora-like sites). For
entity-level QA we can use exact match, but for long-form generation
such as document-level summarization/translation/creative writing, there
is no reliable rule (ROUGE, BLEU and similar metrics have long been
shown to be unreliable). The same applies across vertical domains
(medicine, law, social sciences, psychology, literature, education)
where many ground-truth instances are free-form.</p>
<p>A few concepts are easy to conflate; NOVER focuses only on the
following:</p>
<ul>
<li>the ground truth exists (this is not unsupervised learning); we do
not desire a reward model to directly give a judgment, but rather a
verifier that compares ground truth and prediction;</li>
<li>the ground truth may be subjective or objective, but the dataset
contains at least one reference.</li>
<li>Even when the ground truth is objective, rule-based verifiers are
often failed: objective GTs can be expressed in many textual forms,
otherwise we would simply use exact match as the reward. Moreover, even
for math, where answers are seemingly easy to verify (multiple choice
labels, numbers, formulas, short text, booleans), model responses vary
widely and rule-based verifiers are not robust
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png" width="600">
<figcaption>
Error patterns on easy-to-verify tasks from compass verifier
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>
</figcaption>
</figure>
<h1 id="why">Why</h1>
<p>Why pursue general reasoning? Many non-math/code tasks (creative,
humanities, or long-form scenarios) appear to be not a suitable targets
for a “reasoning” model. But consider:</p>
<ul>
<li>It is still unknown whether RLVR is the ultimate correct paradigm
for training reasoning models.</li>
<li>It is still unknown whether CoT genuinely corresponds to human-style
reasoning.</li>
<li>It is still unknown whether CoTs learned via RLVR truly represent a
model’s internal reasoning.</li>
</ul>
<p>Currently RLVR is better seen as a method to train native CoT
outputs, and CoT is simply “say something before the final answer.” That
something is not necessarily reasoning (some works find that even
repeating the question can increase accuracy); it is model-generated
tokens that, when produced before the answer, help the LLM better
exploit learned patterns and increase the prediction probability of the
next correct answer tokens.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png" width="400">
<figcaption>
DataAlchemy's show that CoT's gains can arise from reuse and
interpolation of patterns near the training distribution
<sup class="refplus-num"><a href="#ref-is_cot_mirage">[2]</a></sup>
</figcaption>
</figure>
<p>From that practical viewpoint, producing a bit more text that
improves answer quality is no harm (users often try a “deeper thinking”
mode in chat systems expecting better answers).</p>
<p>Another reason to study general reasoning is that, <strong>for an
LLM, task difficulty is tied to verification difficulty</strong>. We aim
to keep pushing the frontier of problems that models can solve: some
tasks that are hard for humans (e.g., olympiad math) might still be
learnable by models if supplied with a correct, sufficiently informative
reward. Conversely, tasks whose rewards are hard to formalize are harder
for models to learn.</p>
<h1 id="what-it-actually-is">What it actually is</h1>
<p>What we need for RLVR on free-form text is a good verified signal,
which is actually a reward function that measures semantic agreement
between ground truth and model prediction. That is exactly what we
pursue in the Natural Language Generation. The most basic target is
cross-entropy (ppl). <strong>From this perspective NOVER essentially
moves the SFT loss into the RLVR setting, and recent work shows SFT and
RL differences are often not large.</strong></p>
<p>Although NOVER used ppl, perplexity may not be optimal. We can
arrange verified signals along an axis from fine to coarse granularity:
the coarser the signal, the more information is lost and the sparser the
reward becomes. On this axis three main approaches appear:</p>
<ul>
<li>Perplexity-based signals.</li>
<li>Rubrics / checklists.</li>
<li>Trained verifier models that yield binary (0/1) rewards.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png" width="800">
<figcaption>
The Axis of Verified Signals
</figcaption>
</figure>
<p>Compared with binary rewards, ppl provides a denser signal, extends
naturally to free-form text, and avoids reward saturation; but it loses
the absolute correctness signal, i.e., the model never observes a strict
correct/incorrect label and we cannot use pass@k-style metrics to assess
sample difficulty. Rubrics/checklists sit between these extremes: they
are more fine-grained than binary rewards but still sparser than ppl.
High-quality rubrics typically require sample-wise, human expert
annotation. Several recent works explore rubric-style
solutions<sup class="refplus-num"><a href="#ref-ace_rl">[3]</a></sup><sup class="refplus-num"><a href="#ref-checklists_are_better">[4]</a></sup><sup class="refplus-num"><a href="#ref-ticking_all_the_boxes">[5]</a></sup><sup class="refplus-num"><a href="#ref-rubric_anchors">[6]</a></sup><sup class="refplus-num"><a href="#ref-rubrics_as_rewards">[7]</a></sup>.
Baichuan-M2 in particular develops a fairly detailed Verifier System
that functions as a model-driven environment, with a Patient Simulator
(data generator) and a Rubrics Generator (rollout evaluator)
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png" width="600">
<figcaption>
Baichuan-M2's Verifier
System<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>
</figcaption>
</figure>
<p>Rubrics also enable controlled synthetic data generation for
debiasing reward
models<sup class="refplus-num"><a href="#ref-robust_reward_modeling">[9]</a></sup>,
so the reward model focuses on true causal factors and resists hacks
stemming from format, length, or tone. OpenAI’s Deliberative Alignment
can be seen as an outcome-RL approach that uses safety-oriented rubrics
<sup class="refplus-num"><a href="#ref-deliberative_alignment">[10]</a></sup>.</p>
<h1 id="how">How</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png" width="400">
<figcaption>
NOVER's reward is derived from the policy model's conditional ppl of the
ground truth given the reasoning trajectory
</figcaption>
</figure>
<p>NOVER applies a crude but direct approach: for a rollout, compute the
policy model’s conditional ppl of the ground-truth answer given the
rollout's reasoning trajectory as the reward.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png" width="600">
<figcaption>
The idea of reasoning advantage (RA).
</figcaption>
</figure>
<p>The idea of reasoning-ppl based improvements has appeared before. A
short NeurIPS 2024 LanGame workshop paper called this notion reasoning
advantage (RA), essentially the relative change in reasoning ppl
compared to a no-reasoning baseline. That paper used RA for data
selection, which is essentially keeping CoT examples with high RA for
SFT, so it can be viewed as an offline-RL style method
<sup class="refplus-num"><a href="#ref-on_reward_functions">[11]</a></sup>.</p>
<p>Fortuitously, I experimented with relative reasoning ppl in NOVER and
later found the LANGame writeup: it is an intuitive and reasonable
design.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png" width="600">
<figcaption>
The idea of longPPL.
</figcaption>
</figure>
<p>Another related refinement on ppl is longPPL which measures ppl on a
context-dependent subset of tokens: longPPL subtracts the ppl without
long context from the ppl with long context, thereby focusing evaluation
on tokens that truly depend on long-range context
<sup class="refplus-num"><a href="#ref-what_is_wrong_with_perplexity">[12]</a></sup>.
RA shares the same spirit: we want the reward to come from those tokens
in the ground truth that genuinely require CoT reasoning.</p>
<p>More Interestingly, in GRPO a simple group normalization makes
relative ppl improvements and absolute ppl effectively equivalent on
advantage calculation, so absolute reasoning ppl itself is a solid
reward signal.</p>
<p>But applying ppl directly has issues.</p>
<ul>
<li>First, ppl is numerically unstable: advantage estimates vary across
batches and exhibits length bias. NOVER converted ppl into in-group
quantiles to produce more stable rewards. QRPO applies quantile
transforms more rigorously: it maps rewards to quantiles of the
base-policy reward distribution across the dataset, making the partition
function tractable and enabling numerically stable pointwise rewards
even in offline RL
<sup class="refplus-num"><a href="#ref-quantile_reward_policy_optimization">[13]</a></sup>.</li>
<li>Which model should be used to compute ppl? In principle a stronger
external model could be a more accurate verifier, but the gap between
large and small model cause problems, which is similar to bad
distillation results when using DPO to train small models from
GPT-distilled labels. NOVER uses the policy model itself to compute ppl,
which saves extra models and eases scaling. We found that using a
separate large verifier (closed-source SOTA or a specialized verifier)
often leads to LM-hack-LM issues, whereas using the policy model’s own
ppl yields smoother learning curves.</li>
<li>With small batches and limited compute, training is unstable. NOVER
introduced a policy-proxy sync: periodically copy policy parameters to a
proxy model and compute ppl from the proxy during training. This
effectively increases the batch size (similar in spirit to gradient
accumulation) and stabilizes reward estimates.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png" width="400">
<figcaption>
RLPR shows that ppl can accurately measure the reasoning advantage.
</figcaption>
</figure>
<p>Several contemporaneous works adopt related ideas but differ in how
they stabilize ppl numerics.</p>
<ul>
<li>VeriFree
<sup class="refplus-num"><a href="#ref-reinforcing_general_reasoning">[14]</a></sup>
uses reasoning ppl directly, but restricts to short answers (≤7 tokens)
where ppl is less unstable, and shows ppl can approach or exceed
verifier-based baselines on short QA.</li>
<li>RLPR <sup class="refplus-num"><a href="#ref-rlpr">[15]</a></sup>
uses relative token probabilities (the per-token mean probability,
clipped, then advantage computed) rather than ppl and provides detailed
ablations showing direct ppl can lose 20 points if used naively.</li>
<li>DRO
<sup class="refplus-num"><a href="#ref-direct_reasoning_optimization">[16]</a></sup>
targets long answers and uses relative reasoning ppl with per-token
weighting for high-variance ground-truth tokens and local weight
decay.</li>
<li>DeepWriter
<sup class="refplus-num"><a href="#ref-reverse_engineered_reasoning">[17]</a></sup>
focuses on long-form writing but uses reasoning ppl purely as a scoring
metric to filter and iteratively rewrite drafts (not an RL loop),
avoiding numeric instability by staying in a supervised selection
regime.</li>
</ul>
<h1 id="observations">Observations</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png" width="600">
<figcaption>
Collapse modes in training.
</figcaption>
</figure>
<p>We experienced many collapse modes early in training: completion
lengths exploding, ill rollouts where the model produces garbled text,
and simultaneous blowups of format rewards. We applied the tricks above
to stabilize training (see the paper’s ablation for details on the
“curse of proxy”).</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png" width="600">
<figcaption>
The curse of proxy.
</figcaption>
</figure>
<p>A small but useful trick is reward dependency: when multiple reward
terms are simply summed the model can be uncertain which objective
produced a given penalty or bonus. Practically, we found it effective to
gate task rewards on a strict format reward: unless the format reward is
satisfied, set all other rewards to zero. When the format reward gained,
the model is usually “sane”, no hallucination or gibberish. This
dependency can also pull the model back from training collapse.</p>
<p>We also found that excessive strictness on format rewards may hinder
exploration
<sup class="refplus-num"><a href="#ref-simplerl_zoo">[18]</a></sup>.
For example, one interesting reward hacking on format we observed was
nested <code>&lt;think&gt;</code> tags in CoT: models can nest a
sub-reasoning reflection inside an outer <code>&lt;think&gt;</code>
block to game the signal, e.g.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;think&gt;</span><br><span class="line">  inner thoughts</span><br><span class="line">  &lt;think&gt;</span><br><span class="line">    reflection on the earlier thoughts</span><br><span class="line">  &lt;/think&gt;</span><br><span class="line">  continue reasoning</span><br><span class="line">&lt;/think&gt;</span><br><span class="line">&lt;answer&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/answer&gt;</span><br></pre></td></tr></table></figure>
<p>Stronger base models exploit dense semantic signals better. For
example, we converted multiple-choice questions into free-form answers
where the model must output both the option letter and the full option
text; comparing 7B vs 3B, the 7B model better leverages ppl to rank
rollouts:</p>
<ul>
<li>rank 1: option letter and option text both correct</li>
<li>rank 2: letter wrong, option text correct</li>
<li>rank 3: letter correct, option text similar to another option</li>
<li>rank 4: letter correct, option text completely wrong</li>
<li>…</li>
<li>lowest: everything wrong</li>
</ul>
<p>Looking at rollouts beyond the answer, ppl can indirectly reflect
differences in the reasoning details. In an astronomy example that
required an explanation plus numeric computation, we asked GPT to
analyze each rollout (reasoning plus result) sorted by reasoning ppl;
the model’s qualitative analyses correlated with ppl rankings.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png" width="600">
<figcaption>
The correlation between ppl rankings and GPT's qualitative analyses.
</figcaption>
</figure>
<p>NOVER also partially works on non-Qwen models, though weaker bases
(e.g., some Mistral checkpoints) show erratic behavior. Zero-shot CoT
can be seen as an untrained base exploration strategy; if that baseline
is close to or exceeds the base model, RL typically provides gains.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png" width="600">
<figcaption>
NOVER partially works on non-Qwen models.
</figcaption>
</figure>
<p>We also observed (without exhaustive experiments) that many
general-reasoning datasets are annotated by closed-source large models
and thus are not perfectly objective or correct (loose definitions,
symbol misuse). Perplexity can still provide a useful guiding signal: in
some cases models learned complex reasoning patterns from the ppl signal
that can produce arguably more correct answers than the original ground
truth.</p>
<h1 id="is-changing-only-the-reward-enough">Is changing only the reward
enough?</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png" width="1000">
<figcaption>
Some works on reproducing GRPO.
</figcaption>
</figure>
<p>No, but reward design is the most obvious gap when extending
rule-based verification to general reasoning. What's more, from the
bitter-lesson viewpoint many algorithmic tricks are spurious: data and
compute dominate. By March many people were reproducing GRPO and noting
its fragility; our NOVER training surfaced similar issues. Many
algorithmic “tricks” proposed in these papers have marginal effects
compared with data and scale.</p>
<p>So advancing general reasoning faces larger challenges in data and
base models; algorithmic work will be required later to make training
more efficient and stable.</p>
<ul>
<li>Data: existing general-reasoning datasets vary widely in quality;
cleaning consumes substantial effort, and much data is LLM-annotated
(distilled from GPT or similar) rather than human-curated. The data are
static and finite. RL itself is sample-efficient in some senses; the
cost-effective path to scaling is not simply more examples but
higher-quality environments and feedback.</li>
<li>Base model: the base model governs exploration in RL. Practically,
it should already possess zero-shot instruction following and CoT
capability; richer knowledge helps. Debates over whether RL can raise
the ultimate capability ceiling are not the key point: post-training
often elicits latent abilities rather than creates them. Some works
already explore combining memory and elicitation, and I believe
mid-training vs post-training may form new positive feedback loops.</li>
</ul>
<h1 id="one-more-thing-climb-the-solververifier-asymmetry">One more
thing: Climb the Solver–Verifier Asymmetry</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png" width="600">
<figcaption>
The Solver-Verifier Asymmetry.
</figcaption>
</figure>
<p>A central concept in RLVR is the solver-verifier asymmetry: for some
tasks verification is easier than solving, while for others verification
is harder. Much of RLVR excels when verification is simpler than
solving. The opposite side, where verification is harder, includes:</p>
<ul>
<li>General Reasoning with hard-to-verify free-form answers</li>
<li>Situations requiring long time horizons to obtain a return (e.g., a
business plan whose real feedback arrives after weeks, months, or
years). Those cases resemble deep conversion problems in recommender
systems: we need accurate attribution and systems that handle extremely
sparse feedback.</li>
<li>Scenarios that may require large human labeling efforts or
hard-to-acquire real users to verify the solution, which motivates the
development of effective user simulators.</li>
</ul>
<p>The verifier-free design of NOVER introduces a new possibility
(though not yet tested):</p>
<p><strong>whether it is feasible to synchronize the intelligence of the
policy model to the verifier model, thereby enabling co-evolution of
solver and verifier along the Solver-Verifier Asymmetry
diagonal</strong>.</p>
<p>A stronger policy model would lead to a stronger verifier model,
which in turn could train an even stronger policy model. The key lies in
the transmission of intelligence. NOVER’s design of using perplexity as
the reward naturally <strong>unifies the form of intelligence in both
solver and verifier: both aim to increase the probability of generating
the ground truth on good reasoning trajectories.</strong> In this way,
co-evolution can be achieved through standard RL without the need to
design additional adversarial or self-play tasks. Here, the direction of
intelligence transfer is from solving to verifying. A symmetric related
work is LLaVA-Critic-R1, which found that a strong preference model can
yield a strong policy model, though it required constructing an
additional task.
<sup class="refplus-num"><a href="#ref-llava_critic_r1">[19]</a></sup>.</p>
<p>If we want to achieve such fully automatic co-climbing, we have RL
training which performs horizontal climbing (fix verifier y, improve
solver x), we have Intelligence Sync which would perform vertical
climbing (fix solver x, improve verifier y). However, we also need a
third variable: tasks and data. Each point in the solver–verifier grid
corresponds to specific tasks and datasets. As argued in my earlier post
on <a href="https://thinkwee.top/2025/07/17/env-matrix/#more">Scaling
the Environment</a>, beyond solver and verifier there is also the
question generator. Most current reasoning-evolution work focuses on
self-improvement via model consistency or entropy patterns; some
approaches implement co-evolution of two modules, while a tri-evolution
of three modules has not been explored:</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png" width="600">
<figcaption>
The Trinity of Solver-Verifier-Generator.
</figcaption>
</figure>
<ul>
<li>R-Zero and Self-Questioning Language Models consider adversarial
generation between a generator and a solver
<sup class="refplus-num"><a href="#ref-r_zero">[20]</a></sup><sup class="refplus-num"><a href="#ref-self_questioning">[21]</a></sup>.</li>
<li>URPO reframes verification as a solving task and unifies data
training. COOPER trains a verifier from positive/negative samples
constructed from current policy rollouts. Both lines implement
solver–verifier co-evolution
<sup class="refplus-num"><a href="#ref-urpo">[22]</a></sup><sup class="refplus-num"><a href="#ref-cooper">[23]</a></sup>.</li>
</ul>
<p>Another route to continual solver improvement is self-play: with a
suitable environment, two solvers can game and thereby improve each
other without worrying about asymmetry. For general reasoning such
environments are hard to design because the “rules” are nebulous. Recent
works have proved that models can learn rules
<sup class="refplus-num"><a href="#ref-llms_can_learn_rules">[24]</a></sup>
and <a href="https://husky-morocco-f72.notion.site/From-f-x-and-g-x-to-f-g-x-LLMs-Learn-New-Skills-in-RL-by-Composing-Old-Ones-2499aba4486f802c8108e76a12af3020">combine
atom skills to learn new skills</a> through synthetic data and task, but
existing real general-reasoning datasets are limited enumerations rather
than comprehensive rule sets. This is still essentially static
datset/benchmark-driven RL. In the AI “second half,” we should seek
real-world environments and problems rather than static datasets.</p>
<p>Between static data and the real world lies a middleware: simulators.
Simulators trade fidelity for feedback speed—like reward models or
verifier models—and for general reasoning a useful simulator might look
like a patient simulator in medical domains (see Baichuan-M2’s case),
since real patients raise ethical and regulatory issues and validation
can be slow
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<p>A different idea is to forgo task-specific environments and instead
play games: self-play on games could improve math and general reasoning
if reasoning patterns transfer across games and tasks
<sup class="refplus-num"><a href="#ref-play_to_generalize">[25]</a></sup><sup class="refplus-num"><a href="#ref-spiral">[26]</a></sup>.
If feasible, we could use game environments and self-play to continually
evolve general-reasoning models.</p>
<h1 id="citation">Citation</h1>
<p>If you found the topics in this blog post interesting and would like
to cite it, you may use the following BibTeX entry: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{general_reasoning_202509,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Towards General Reasoning},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {9},</span><br><span class="line">  url = {https://thinkwee.top/2025/09/13/general-reasoning/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><p><img data-src="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png" width="600"></p>
<p>以下为机器翻译，一个中文原生的版本请参考<a href="https://mp.weixin.qq.com/s/ocpI3j3rwlt_9Zo1wYAF6Q">公众号文章</a></p>
<p>最近，我们的论文 <em>NOVER: Incentive Training for Language Models
via Verifier-Free Reinforcement Learning</em> 被 EMNLP 录用。NOVER 将
LLM 在推理轨迹下对真实数据的困惑度作为奖励，这一创新将 RLVR
范式从数学和代码领域拓展至更广泛的文本处理，使得模型能够在任意文本到文本任务中学习通用推理，且无需依赖额外的模型或验证器。</p>
<p><img data-src="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png" width="600"></p>
<p>当我们在二月启动 NOVER 项目时，大多数 RLVR
研究主要集中在数学和代码推理领域；而针对通用或难以验证领域的 RLVR
研究则相对匮乏。近六个月来，涌现了许多有趣的相关论文。由于资源有限，我们实验中的许多想法未能得到充分验证，因此未能纳入论文。本文旨在整理这些未充分验证的想法，并综述近期相关研究，以评估我们在通用推理方面取得的进展以及未来仍需努力的方向。</p>
<h1 id="是什么">是什么</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png" width="1000">
<figcaption>
NOVER 将 RLVR
拓展至通用推理领域，包括科学解释与证明、社会推理、创意写作及翻译
</figcaption>
</figure>
<p>NOVER 的目标问题在于，对于那些答案为非结构化自然文本的任务，如何应用
RLVR 来获取推理能力？</p>
<p>此类情况并不少见。数学问题通常有一个唯一的规范答案，可以放在
<code>\boxed{}</code>
中；而物理或化学现象的解释可以合法地采用多种形式（想想 Quora
等类似网站上的长篇、主观性答案）。对于实体级
QA，我们可以使用精确匹配，但对于长文本生成（如文档级摘要、翻译、创意写作），没有可靠的规则（ROUGE、BLEU
等指标已被证明不可靠）。这种现象在垂直领域（医学、法律、社会科学、心理学、文学、教育）同样存在，许多真实示例的答案为自由文本。</p>
<p>有几个概念容易混淆；NOVER 仅关注以下几点：</p>
<ul>
<li>真实答案存在（这不是无监督学习）；我们不希望奖励模型直接给出判断，而是需要一个验证器来比较真实答案和预测；</li>
<li>真实答案可能是主观的或客观的，但数据集至少包含一个参考。</li>
<li>即使真实答案是客观的，基于规则的验证器也经常失败：客观的真实答案可以以多种文本形式表达，否则我们只需使用精确匹配作为奖励。此外，即使是数学问题，其答案看似容易验证（多选标签、数字、公式、短文本、布尔值），但模型响应差异很大，基于规则的验证器也不可靠
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png" width="600">
<figcaption>
Compass Verifier在易验证任务中发现的错误模式
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>
</figcaption>
</figure>
<h1 id="为什么">为什么</h1>
<p>为什么追求通用推理？许多非数学/代码任务（创意、人文、长文本场景）似乎不适合“推理”模型。但考虑：</p>
<ul>
<li>目前仍不清楚 RLVR 是否是训练推理模型的终极正确范式。</li>
<li>目前仍不清楚 CoT 是否真正对应于人类推理风格。</li>
<li>目前仍不清楚通过 RLVR 学习的 CoT 是否真正代表模型的内部推理。</li>
</ul>
<p>所以当下我们可以将 RLVR视为训练原生 CoT 输出的方法，而CoT
只是“在最终答案之前说一些话”。这个“一些话”不一定是推理（一些工作发现重复问题也能提高准确性）；它是模型生成的在最终答案之前的tokens，帮助
LLM 更好地利用学习到的模式并增加下一个正确答案令牌的预测概率。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png" width="400">
<figcaption>
DataAlchemy 证明了 CoT 的收益源于训练分布附近模式的重复和插值
<sup class="refplus-num"><a href="#ref-is_cot_mirage">[2]</a></sup>
</figcaption>
</figure>
<p>那么从实际角度来看，产生更多文本以提高答案质量并无害处（用户经常在聊天系统中尝试“深度思考”模式以获得更好的答案）。</p>
<p>另一个研究通用推理的原因是，<strong>对于
LLM，任务难度与验证难度相关</strong>。我们致力于推动模型可以解决的问题的边界：一些对人类来说很困难的任务（例如，奥林匹克数学）只要有足够的奖励，模型很容易学习。相反，任务的奖励难以形式化，对模型来说更难学习。</p>
<h1 id="本质是什么">本质是什么</h1>
<p>我们需要在自由文本上进行 RLVR
的验证信号，这实际上是一个奖励函数，用于衡量真实答案和模型预测之间的语义一致性。这正是我们在自然语言生成中追求的。最基本的目标是交叉熵（ppl）。<strong>从这一角度来看，NOVER
本质上将 SFT 损失转移到 RLVR 设置中，而最近的工作表明 SFT 和 RL
之间的差异通常并不大。</strong></p>
<p>尽管 NOVER 使用了
ppl，但困惑度可能不是最优的。我们可以沿着从细到粗的轴线排列验证信号：信号越粗糙，信息损失越多，奖励越稀疏。在这个轴线上，出现了三种主要方法：</p>
<ul>
<li>困惑度为基础的信号。</li>
<li>Rubrics/Checklists。</li>
<li>训练验证器模型，产生二进制（0/1）奖励。</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png" width="800">
<figcaption>
验证信号坐标轴
</figcaption>
</figure>
<p>与二元奖励相比, ppl
提供了更密集的信号，自然扩展到自由文本，并避免了奖励饱和；但它失去了绝对正确性的信号，即模型从未观察到严格的正确/不正确标签，我们无法使用
pass@k
风格的指标来评估样本难度。评分/检查清单介于这些极端之间：它们比二元奖励更细粒度，但仍比
ppl
更稀疏。高质量的评分通常需要样本级、人工专家标注。最近的一些工作探索了Rubrics/Checklists解决方案<sup class="refplus-num"><a href="#ref-ace_rl">[3]</a></sup><sup class="refplus-num"><a href="#ref-checklists_are_better">[4]</a></sup><sup class="refplus-num"><a href="#ref-ticking_all_the_boxes">[5]</a></sup><sup class="refplus-num"><a href="#ref-rubric_anchors">[6]</a></sup><sup class="refplus-num"><a href="#ref-rubrics_as_rewards">[7]</a></sup>.
特别是 Baichuan-M2
开发了一个相当详细的验证器系统，作为模型驱动的环境，具有患者模拟器（数据生成器）和评分生成器（rollout
评估器）
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png" width="600">
<figcaption>
Baichuan-M2
的验证器系统<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>
</figcaption>
</figure>
<p>Rubrics/Checklists
也促进了有控制的合成数据生成，以减少奖励模型的偏差<sup class="refplus-num"><a href="#ref-robust_reward_modeling">[9]</a></sup>,
这样奖励模型专注于真正的因果因素，并抵抗来自格式、长度或语气等hack。OpenAI
的 Deliberative Alignment 可以被视为一种 outcome-RL
方法，它使用安全导向的评分/检查清单
<sup class="refplus-num"><a href="#ref-deliberative_alignment">[10]</a></sup>.</p>
<h1 id="怎么做">怎么做</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png" width="400">
<figcaption>
NOVER 的奖励是从策略模型在推理轨迹下对真实答案的条件困惑度
</figcaption>
</figure>
<p>NOVER 应用了一个粗糙但直接的方法：对于一个
rollout，计算策略模型在推理轨迹下对真实答案的条件困惑度作为奖励。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png" width="600">
<figcaption>
推理优势（RA）的想法。
</figcaption>
</figure>
<p>推理-ppl 基于的改进想法之前已经出现过。一篇短篇 NeurIPS 2024 LanGame
研讨会论文称这个概念为推理优势（RA），本质上是指推理困惑度与无推理基线的相对变化。该论文使用
RA 进行数据选择，即保持 CoT 示例，使其具有较高的 RA，以便用于
SFT，因此可以被视为一种离线 RL 风格的方法
<sup class="refplus-num"><a href="#ref-on_reward_functions">[11]</a></sup>.</p>
<p>巧合的是我先在NOVER中尝试了相对reasoning
perplexity的想法，然后才发现这篇有关RA的workshop论文：这说明相对提升的想法非常符合直觉。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png" width="600">
<figcaption>
longPPL 的想法。
</figcaption>
</figure>
<p>另一个与 ppl 相关的改进是
longPPL，它测量上下文依赖的子集令牌的困惑度：longPPL
从带有长上下文的困惑度中减去没有长上下文的困惑度，从而专注于那些真正依赖于长距离上下文的令牌
<sup class="refplus-num"><a href="#ref-what_is_wrong_with_perplexity">[12]</a></sup>.
RA 共享相同的理念：我们希望奖励来自那些在真实答案中真正需要 CoT
推理的令牌。</p>
<p>更有趣的是，在 GRPO 中，一个简单的组归一化使相对 ppl 改进和绝对 ppl
在优势计算上有效等价，因此绝对推理 ppl 本身是一个 solid 奖励信号。</p>
<p>但直接应用 ppl 有以下问题：</p>
<ul>
<li>首先，ppl
是数值不稳定的：优势估计在批次之间变化并表现出长度偏差。NOVER 将 ppl
转换为组量化，以产生更稳定的奖励。QRPO
应用量化变换更严格：它将奖励映射到数据集上基策略奖励分布的量化，使分区函数可处理，即使在离线
RL 中也能实现数值稳定的逐点奖励
<sup class="refplus-num"><a href="#ref-quantile_reward_policy_optimization">[13]</a></sup>.</li>
<li>应该使用哪个模型来计算
ppl？原则上，一个更强大的外部模型可以是一个更准确的验证器，但大模型和小模型之间的差距会导致问题，这与使用
DPO 从 GPT 蒸馏标签训练小模型时的糟糕蒸馏结果类似。NOVER
使用策略模型本身来计算
ppl，这节省了额外模型并降低了缩放难度。我们发现使用单独的大型验证器（闭源
SOTA 或专用验证器）通常会导致 LM-hack-LM 问题，而使用策略模型的 ppl
产生更平滑的学习曲线。</li>
<li>在小批次和有限计算的情况下，训练不稳定。NOVER
引入了一个策略代理同步：定期将策略参数复制到代理模型，并在训练期间从代理计算
ppl。这有效地增加了批次大小（类似于梯度累积）并稳定了奖励估计。</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png" width="400">
<figcaption>
RLPR 证明了 ppl 可以准确测量推理优势。
</figcaption>
</figure>
<p>许多同时期的作品采用了相关想法，但差异在于如何稳定 ppl 数值。</p>
<ul>
<li>VeriFree
<sup class="refplus-num"><a href="#ref-reinforcing_general_reasoning">[14]</a></sup>
直接使用推理 ppl，但限制为短答案（≤7 个令牌），其中 ppl 更稳定，并展示了
ppl 可以在短 QA 上接近或超过基于验证器的基线。</li>
<li>RLPR <sup class="refplus-num"><a href="#ref-rlpr">[15]</a></sup>
使用相对令牌概率（每个令牌的平均概率，裁剪，然后计算优势）而不是
ppl，并提供了详细的消融实验，表明直接使用 ppl 如果使用不当会损失 20
分。</li>
<li>DRO
<sup class="refplus-num"><a href="#ref-direct_reasoning_optimization">[16]</a></sup>
针对长答案，使用相对推理 ppl
进行每个令牌加权，用于高方差真实答案令牌和局部权重衰减。</li>
<li>DeepWriter
<sup class="refplus-num"><a href="#ref-reverse_engineered_reasoning">[17]</a></sup>
专注于长文本写作，但纯粹使用推理 ppl
作为评分指标来过滤和迭代重写草稿（不是 RL
训练），通过保持在监督选择制度中避免数值不稳定性。</li>
</ul>
<h1 id="观察">观察</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png" width="600">
<figcaption>
训练中的崩溃模式。
</figcaption>
</figure>
<p>我们早期训练中遇到了许多崩溃模式：completion
length爆炸，模型产生混乱文本的糟糕
rollout，以及格式奖励的同时爆炸。我们应用了上述技巧来稳定训练（见论文的消融实验，详细介绍“代理的诅咒”）。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png" width="600">
<figcaption>
The curse of proxy.
</figcaption>
</figure>
<p>一个小的但有用的小技巧是奖励依赖：当多个奖励项简单相加时，模型可能不确定哪个目标产生了给定的惩罚或奖励。实际上，我们发现将任务奖励限制在严格的格式奖励上有效：除非格式奖励满足，否则将所有其他奖励设置为零。当格式奖励获得时，模型通常是“合理的”，没有幻觉或乱码。这种依赖也可以将模型从训练崩溃中拉回来。</p>
<p>我们发现，对格式奖励的过度严格可能会阻碍探索
<sup class="refplus-num"><a href="#ref-simplerl_zoo">[18]</a></sup>.
例如，我们观察到的一种有趣的格式奖励 hack 是 CoT 中的嵌套
<code>&lt;think&gt;</code> 标签：模型可以在外层
<code>&lt;think&gt;</code> 块内嵌套一个子推理反射，以游戏信号，例如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;think&gt;</span><br><span class="line">  inner thoughts</span><br><span class="line">  &lt;think&gt;</span><br><span class="line">    reflection on the earlier thoughts</span><br><span class="line">  &lt;/think&gt;</span><br><span class="line">  continue reasoning</span><br><span class="line">&lt;/think&gt;</span><br><span class="line">&lt;answer&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/answer&gt;</span><br></pre></td></tr></table></figure>
<p>更强大的基础模型能更有效地利用密集的语义信号。例如，我们将选择题转换为开放式答案，要求模型同时输出选项字母和完整的选项文本；在比较
7B 和 3B 模型时，7B 模型在利用 ppl 对输出结果进行排序方面表现更优：</p>
<ul>
<li>rank 1: 选项字母和选项内容均正确</li>
<li>rank 2: 字母填错，但选项文本正确</li>
<li>rank 3: 字母正确，选项文本与另一个选项相似</li>
<li>rank 4: 字母正确但选项文本完全错误</li>
<li>…</li>
<li>lowest: 完全错误</li>
</ul>
<p>通过分析答案之外的推理输出，人们可以间接了解推理过程的差异。在一个需要解释和数值计算的天文问题中，我们让
GPT
分析每个推理过程（包括推理和结果），并按照推理质量进行排序；模型的定性分析结果与质量排名相符。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png" width="600">
<figcaption>
PPL 排名和 GPT 定性分析之间的关系。
</figcaption>
</figure>
<p>NOVER 也部分适用于非 Qwen 模型，但一些较弱的基座（例如某些 Mistral
检查点）会表现出异常行为。零样本 CoT
可以看作是一种未训练的基座探索策略；如果该基线接近或超过基础模型，强化学习（RL）通常能带来收益。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png" width="600">
<figcaption>
NOVER 在一定程度上可以支持非 Qwen 模型。
</figcaption>
</figure>
<p>我们还注意到（并未进行详尽实验），许多通用推理数据集是由封闭式大型模型标注的，因此其客观性和准确性并不完美（定义松散，符号误用）。困惑度依然能提供有价值的指导信号：在某些情况下，模型通过困惑度信号学习到了复杂的推理模式，这些模式产生的答案可能比原始的真实标签更为合理。</p>
<h1 id="修改奖励就足够了吗">修改奖励就足够了吗</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png" width="1000">
<figcaption>
Some works on reproducing GRPO.
</figcaption>
</figure>
<p>不，但在将基于规则的验证扩展到通用推理时，奖励设计是最明显的不足。此外，从经验教训来看，许多算法技巧都是徒劳的：数据和计算才是关键。到三月，许多人都在复制
GRPO 并指出其脆弱性；我们的 NOVER
训练也暴露了类似的问题。与数据和规模相比，这些论文中提出的许多算法“技巧”的效果并不显著。</p>
<p>推进通用推理在数据基础模型方面面临更大挑战，后期需要通过算法工作来提升训练的效率和稳定性。</p>
<ul>
<li>现有的通用推理数据集质量参差不齐；清理数据需要耗费大量精力，而且许多数据是由
LLM 标注的（源自 GPT
或类似模型），而非人工精心编辑。这些数据是静态且数量有限的。强化学习在样本效率方面具有优势；实现规模化扩展的具成本效益的路径并非简单地增加更多示例，而是要提升环境和反馈的质量。</li>
<li>基础模型负责强化学习中的探索。实际上，它应已具备零样本指令跟随和思维链（CoT）能力，更丰富的知识会更有利。关于强化学习能否达到最终能力上限的讨论并非重点：训练后往往能激发潜在能力而非创造新能力。部分研究已探索结合记忆与启发式方法，我认为中期训练与训练后可能形成新的正反馈循环。</li>
</ul>
<h1 id="one-more-thing-攀爬solververifier不对称性">One more thing:
攀爬Solver–Verifier不对称性</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png" width="600">
<figcaption>
The Solver-Verifier Asymmetry.
</figcaption>
</figure>
<p>RLVR
的一个核心概念是求解器-验证器的不对称性：对于某些任务，验证比求解要容易，而对于另一些任务，验证则更困难。当验证相对求解较为简单时，RLVR
的表现尤为出色。而验证更困难的情况则包括：</p>
<ul>
<li>面对难以验证的自由回答的一般性推理</li>
<li>需要长时间才能获得回报的情境（例如，一个商业计划，其真实反馈可能在数周、数月甚至数年后才出现）。这些情况类似于推荐系统中的深度转化问题：我们需要精确的归因方法以及能够处理极度稀疏反馈的系统。</li>
<li>可能需要大量人工标注或难以获取的真实用户来验证解答的场景，这推动了高效用户模拟器的开发。</li>
</ul>
<p>NOVER 的 无验证器（verifier-free）
设计带来了一个新的可能性（尽管尚未实验）：</p>
<p><strong>是否可以将策略模型（policy
model）的智能同步到验证器模型（verifier
model），从而使求解器与验证器能够沿着 Solver-Verifier Asymmetry
的对角线共同进化？</strong></p>
<p>更强的策略模型会带来更强的验证器模型，而更强的验证器模型又能训练出更强的策略模型。关键在于智能的传递。NOVER
基于困惑度（perplexity）作为奖励的设计，自然地统一了求解器和验证器的智能形式：二者都旨在提高在良好推理轨迹上生成真值（ground
truth）的概率。 因此，可以通过标准的 RL
来实现共同进化，而无需额外设计对抗或自博弈任务。在这里，智能的传递方向是从求解到验证。一个对称的相关工作是
LLaVA-Critic-R1，它发现强大的偏好模型可以带来强大的策略模型，但它需要构造一个额外的任务。<sup class="refplus-num"><a href="#ref-llava_critic_r1">[19]</a></sup></p>
<p>如果我们希望实现这种完全自动化的共同攀爬，那么现有的 RL
训练相当于执行 横向攀爬（固定 verifier y，提升 solver x），而
智能同步（Intelligence Sync） 则会执行 纵向攀爬（固定 solver x，提升
verifier
y）。然而，我们还需要第三个变量：任务与数据。在求解器–验证器网格中的每一个点，都对应着特定的任务和数据集。正如我在早先关于
Scaling the Environment
的文章中所论述的，除了求解器与验证器之外，还有出题器（question
generator）。目前大多数关于推理进化的研究都集中在通过模型一致性或熵模式实现自我改进；部分方法实现了两个模块的共同进化，但三个模块的三重进化（tri-evolution）尚未被探索：</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png" width="600">
<figcaption>
The Trinity of Solver-Verifier-Generator.
</figcaption>
</figure>
<ul>
<li>R-Zero 和 Self-Questioning Language Models
考虑了生成器与求解器之间的对抗式生成
<sup class="refplus-num"><a href="#ref-r_zero">[20]</a></sup><sup class="refplus-num"><a href="#ref-self_questioning">[21]</a></sup>。</li>
<li>URPO 将验证重新表述为一个求解任务并统一了数据训练；COOPER
则从当前策略的 rollout
构造正/负样本来训练验证器。这两条路线都实现了求解器–验证器的共同进化
<sup class="refplus-num"><a href="#ref-urpo">[22]</a></sup><sup class="refplus-num"><a href="#ref-cooper">[23]</a></sup>。</li>
</ul>
<p>另一条持续改进求解器的路径是
自博弈（self-play）：在合适的环境下，两个求解器可以通过对弈来相互提升，而不必担心非对称性。对于一般性推理，这类环境很难设计，因为“规则”本身是模糊的。近期有研究证明模型能够学习规则
<sup class="refplus-num"><a href="#ref-llms_can_learn_rules">[24]</a></sup>，并且可以通过合成数据和任务
组合原子技能以学习新技能，但现有的真实通用推理数据集仍然只是有限的枚举，而非全面的规则集。这依旧本质上是静态数据集/基准驱动的
RL。在 AI
的“下半场”，我们应当寻求真实世界的环境与问题，而非停留在静态数据集。</p>
<p>在静态数据与真实世界之间存在一种中间件：模拟器（simulators）。模拟器以牺牲真实性换取反馈速度——类似奖励模型或验证器模型——而在通用推理场景中，一个有用的模拟器可能会类似医学领域的病人模拟器（参考
Baichuan-M2 的案例），因为真实病人涉及伦理与监管问题，验证也往往较慢
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>。</p>
<p>另一种思路是放弃特定任务环境，而转向
博弈环境：如果推理模式能够跨游戏与任务迁移，那么在游戏中的自博弈可能提升数学与通用推理能力
<sup class="refplus-num"><a href="#ref-play_to_generalize">[25]</a></sup><sup class="refplus-num"><a href="#ref-spiral">[26]</a></sup>。若可行，我们就能够利用游戏环境和自博弈来持续进化通用推理模型。</p>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:
</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{general_reasoning_202509,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Towards General Reasoning},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {9},</span><br><span class="line">  url = {https://thinkwee.top/2025/09/13/general-reasoning/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>
<ul id="refplus"><li id="ref-compass_verifier" data-num="1">[1]  CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward.</li><li id="ref-is_cot_mirage" data-num="2">[2]  Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens.</li><li id="ref-ace_rl" data-num="3">[3]  ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning.</li><li id="ref-checklists_are_better" data-num="4">[4]  Checklists Are Better Than Reward Models For Aligning Language Models.</li><li id="ref-ticking_all_the_boxes" data-num="5">[5]  TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation.</li><li id="ref-rubric_anchors" data-num="6">[6]  Reinforcement Learning with Rubric Anchors.</li><li id="ref-rubrics_as_rewards" data-num="7">[7]  Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains.</li><li id="ref-baichuan_m2" data-num="8">[8]  Baichuan-M2: Scaling Medical Capability with Large Verifier System.</li><li id="ref-robust_reward_modeling" data-num="9">[9]  Robust Reward Modeling via Causal Rubrics.</li><li id="ref-deliberative_alignment" data-num="10">[10]  Deliberative Alignment: Reasoning Enables Safer Language Models.</li><li id="ref-on_reward_functions" data-num="11">[11]  On Reward Functions For Self-Improving Chain-of-Thought Reasoning Without Supervised Datasets.</li><li id="ref-what_is_wrong_with_perplexity" data-num="12">[12]  What is Wrong with Perplexity for Long-context Language Modeling?</li><li id="ref-quantile_reward_policy_optimization" data-num="13">[13]  Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions.</li><li id="ref-reinforcing_general_reasoning" data-num="14">[14]  Reinforcing General Reasoning without Verifiers.</li><li id="ref-rlpr" data-num="15">[15]  RLPR: Extrapolating RLVR to General Domains without Verifier.</li><li id="ref-direct_reasoning_optimization" data-num="16">[16]  Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks.</li><li id="ref-reverse_engineered_reasoning" data-num="17">[17]  Reverse-Engineered Reasoning for Open-Ended Generation.</li><li id="ref-simplerl_zoo" data-num="18">[18]  SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild.</li><li id="ref-llava_critic_r1" data-num="19">[19]  LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model.</li><li id="ref-r_zero" data-num="20">[20]  R-Zero: Self-Evolving Reasoning LLM from Zero Data.</li><li id="ref-self_questioning" data-num="21">[21]  Self-Questioning Language Models.</li><li id="ref-urpo" data-num="22">[22]  URPO: A Unified Reward &amp; Policy Optimization Framework for Large Language Models.</li><li id="ref-cooper" data-num="23">[23]  COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS.</li><li id="ref-llms_can_learn_rules" data-num="24">[24]  Large Language Models can Learn Rules.</li><li id="ref-play_to_generalize" data-num="25">[25]  Play to Generalize: Learning to Reason Through Game Play.</li><li id="ref-spiral" data-num="26">[26]  SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning.</li></ul>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>rl</tag>
        <tag>llm</tag>
        <tag>agent</tag>
        <tag>inference</tag>
        <tag>reasoning</tag>
        <tag>questions</tag>
      </tags>
  </entry>
  <entry>
    <title>Debates between GPTs</title>
    <url>/2023/06/05/gpt-debate/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/b5a1dc675aad283afdf2928bfd7163bf.png" width="500"/></p>
<ul>
<li>A webpage based on <a
href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT-Shortcut</a>
that shows some interesting debates that took place between GPTs.</li>
<li>The experience website is <a
href="https://thinkwee.top/debate/">here</a></li>
</ul>
<span id="more"></span>
<h1 id="intro">Intro</h1>
<ul>
<li>这是一个基于<a
href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT-Shortcut</a>更改的项目，展示一些GPT自己和自己辩论的记录。这是一个纯前端展示页面，不包含任何的模型、数据、训练过程，也不是一个平台，没有ChatGPT-Shortcut的登录和平台共享功能。</li>
<li>这只是一个爱好、偏收集的项目，没有研究目的和商业目的。此类项目也已经有很多不错的尝试，比如b站上的<a
href="https://space.bilibili.com/405083326">AI-talk</a>或者油管上的<a
href="https://www.youtube.com/watch?v=OdixRqJsA_4">Watch GPT-4 Debate
with Itself! (About whether it is an AGI)</a></li>
<li>网址在<a href="https://thinkwee.top/debate/">这</a></li>
<li>This is a project based on <a
href="https://github.com/rockbenben/ChatGPT-Shortcut">ChatGPT-Shortcut</a>
to showcase some of GPT's own records of debates with themselves. This
is a pure front-end display page, containing no models, data, training
process, nor is it a platform with the login and platform sharing
features of ChatGPT-Shortcut.</li>
<li>This is just a hobby, collective-favour project, with no research
purpose or commercial purpose. There have been many good attempts at
such projects, such as <a
href="https://space.bilibili.com/405083326">AI-talk</a> on bilibili or
<a href="https://www.youtube.com/watch?v=OdixRqJsA_4">Watch GPT-4 Debate
with Itself! (About whether it is an AGI)</a> on YouTube</li>
<li>The website is <a href="https://thinkwee.top/debate/">here</a></li>
</ul>
<h1 id="prompt">Prompt</h1>
<ul>
<li>以单句辩论为例，给予GPT的background
prompt类似于："你是一个具有顶尖水平的专业辩手。现在你参加了一个特殊的辩论，每次发言不能超过一句话。你将会得到一个辩题和你方观点，你需要引经据典，整理语言，逻辑严谨的为这个辩题辩护。你需要首先阐述观点，之后你会得到多轮对方的阐释，你需要不断驳斥他直到说服对方。记住，每次发言不能超过一句话。所有回答以中文呈现。"</li>
<li>之后给出论点：“辩题为：{}.你方观点是支持/反对。”</li>
<li>之后在两个GPT bots之间传递观点：“对方发言：“
{}”，请反驳他，依然发言不超过一句。不要重复你方观点。不要重复之前的发言。尽可能找出对方观点漏洞。尽可能提出新证据攻击对方。”</li>
<li>Take the example of a one-sentence debate, where the background
prompt given to the GPT is something like: "You are a professional
debater at the top of your game. Now you are taking part in a special
debate where you can speak no more than one sentence at a time. You will
be given a topic and your side of the argument, and you will be required
to defend it logically, using quotations from the classics and
organising your language. You will be given several rounds of
elucidation from your opponent, and you will have to refute him until
you are convinced. Remember, no more than one sentence per statement.
All responses will be presented in Chinese."</li>
<li>The argument is then given: "The debate is entitled: {}. Your side's
argument is for/against."</li>
<li>Then pass the argument between the two GPT bots: "The other side
speaks:" {}", please rebut him, still speaking in no more than one
sentence. Do not repeat your side of the argument. Do not repeat what
you have said before. Find as many holes in the other person's argument
as possible. Present new evidence to attack the other person whenever
possible."</li>
</ul>
<h1 id="discovery">Discovery</h1>
<ul>
<li>该项目想通过辩论这一极具挑战和思辨的语言应用来探索一下GPT的语言能力、逻辑能力，以及探索人类的思想究竟是否可以被概率所拟合</li>
<li>可以设计许多有意思的场景，观察GPT如何给出他的最优解，例如：
<ul>
<li>限制每次只能发言一句进行辩论</li>
<li>设计一个反事实的辩题</li>
<li>引入第三个gpt作为裁判</li>
<li>三方乃至n方辩论</li>
<li>只提供背景，gpt自己设计辩题</li>
<li>何时一个GPT bot才会被另一个GPT bot说服</li>
<li>and more</li>
</ul></li>
<li>The project aims to explore the linguistic and logical capabilities
of the GPT through the challenging and discursive use of language in
debate, and to explore whether human thought can be fitted to
probabilities.</li>
<li>A number of interesting scenarios can be devised to see how the GPT
gives his optimal solution, for example
<ul>
<li>Limit debate to one sentence at a time</li>
<li>devising a counterfactual debate question</li>
<li>Introducing a third GPT as a referee</li>
<li>Three-way or even n-way debates</li>
<li>Provide only the background, the gpt designs his own debate</li>
<li>When will one GPT bot be convinced by another GPT bot</li>
<li>and more</li>
</ul></li>
</ul>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>GPT</tag>
        <tag>NLP</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for Computational Linguistics</title>
    <url>/2018/11/16/coling/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/d1c3632ae2916e58df8591e7ac188747.png" width="500"/></p>
<p>Course Notes on Computational Linguistics, Reference Textbook: Speech
and Language Processing: An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="chapter-2-regular-expressions-and-automata">Chapter 2: Regular
Expressions and Automata</h1>
<ul>
<li>Regular Expressions: A tool used for finding substrings that match a
specific pattern or for defining a language in a standardized form, this
chapter mainly discusses its function in finding substrings. Regular
expressions represent some string sets in an algebraic form.</li>
<li>Regular expressions receive a pattern and then search for substrings
that match this pattern throughout the corpus, and this function can be
realized through the design of a finite state automaton.</li>
<li>A string is viewed as a sequence of symbols, where all characters,
numbers, spaces, tabs, punctuation marks, and whitespace are considered
symbols.</li>
</ul>
<h2 id="basic-regular-expression-patterns">Basic Regular Expression
Patterns</h2>
<ul>
<li>Using double slashes to indicate the beginning and end of regular
expressions (in Perl's format)
<ul>
<li>Find substring, case-sensitive: /woodchuck/-&gt; woodchuck</li>
<li>[One of them is represented by square brackets, or: /[Ww]oodchuck/
-&gt; woodchuck or Woodchuck]</li>
<li>[±], take or within the range: /[2-5]/-&gt;/[2345]</li>
<li>Insertion symbols are placed after the left square bracket,
representing all symbols that do not appear after them in the pattern,
i.e., the negation: /^Ss/ -&gt; neither uppercase S nor lowercase s</li>
<li>Question mark represents the possibility of the previous symbol
appearing once or not at all: /colou?r/ -&gt; color or colour</li>
<li>Asterisks represent multiple occurrences or non-occurrences of the
preceding symbol: /ba*/ -&gt; b or ba or baa or baaa......</li>
<li>The plus sign indicates that the preceding symbol appears at least
once: /ba+/ -&gt; ba or baa or baaa.......</li>
<li>Decimal point represents a wildcard, matching any symbol except the
return character: /beg.n/-&gt;begin or begun or beg’n or .......</li>
<li>Anchor symbol, used to represent a substring at a specific location;
the insertion symbol represents the beginning of a line, the dollar
symbol represents the end of a line; \b represents a word boundary, \B
represents a non-word boundary; Perl defines a word as a sequence of
digits, underscores, or letters, and symbols not included in this are
considered as word boundaries.</li>
</ul></li>
</ul>
<h2 id="extraction-combination-and-priority">Extraction, combination,
and priority</h2>
<ul>
<li>Using vertical bars to represent disjunction, or: /cat|dog/ -&gt;
cat or dog</li>
<li>(gupp(y|ies))/-&gt;guppy or guppies</li>
<li>Priority: Round brackets &gt; Counters &gt; Sequences and Anchors
&gt; Disjunction operator</li>
</ul>
<h2 id="advanced-operators">Advanced Operators</h2>
<ul>
<li>Any number</li>
<li>Any non-numeric character</li>
<li>Any letter, number, or space</li>
<li>Opposite to \w</li>
<li>\s: blank area</li>
<li>S: Opposite to \s</li>
<li>{n}: The preceding pattern appears n times</li>
<li>{n,m}: The preceding pattern appears n to m times</li>
<li>{n,} : The preceding pattern appears at least n times</li>
<li>: newline</li>
<li>Tab character</li>
</ul>
<h2 id="replacement-register">Replacement, Register</h2>
<ul>
<li>Replace s/A/B/: Replace A with B</li>
<li>s/(A)/&lt;\1&gt;/: Using the numeric operator \1 to refer to A,
placing angle brackets on both sides of A</li>
<li>In searches, numerical operators can also be used to represent the
content within parentheses, and multiple operators can represent
multiple contents within parentheses</li>
<li>Here, the numerical operator acts as a register</li>
</ul>
<h2 id="finite-state-automaton">Finite State Automaton</h2>
<ul>
<li>Finite state automata and regular expressions are mutually
symmetric, and regular expressions are a method to characterize regular
languages. Regular expressions, regular grammars, and finite automata
are all forms of expressing regular languages. FSA is represented by a
directed graph, where circles or dots represent states, arrows or arcs
represent state transitions, and double circles represent final states.
The state machine diagram below illustrates the recognition of the
regular expression /baa+/: <img data-src="https://s2.ax1x.com/2019/01/03/FoVj3V.png" alt="FoVj3V.png" /></li>
<li>The finite state machine starts from the initial state, reads in
symbols sequentially, and if the conditions are met, it performs a state
transition. If the sequence of read-in symbols matches the pattern, the
finite state machine can reach the final state; if the sequence of
symbols does not match the pattern, or if the automaton gets stuck in a
non-final state, it is said that the automaton has rejected this
input.</li>
<li>Another representation is the state transition table: <img data-src="https://s2.ax1x.com/2019/01/03/FoVqNn.png" alt="FoVqNn.png" /></li>
<li>A finite automaton can be defined by 5 parameters:
<ul>
<li>Finite set of states {q_i}</li>
<li>Finite input alphabet</li>
<li>Initial State</li>
<li>Ultimate State Set</li>
<li>The transition function or transition matrix between states, is a
relationship from <span class="math inline">\(Q × \Sigma\)</span> to
<span class="math inline">\(2^Q\)</span></li>
</ul></li>
<li>The automaton described above is deterministic, i.e., a DFA, which
always knows how to perform state transitions based on the lookup table
when the states recorded in the state transition table are known. The
algorithm is as follows: given the input and the automaton model, the
algorithm determines whether the input is accepted by the state machine:
<img data-src="https://s2.ax1x.com/2019/01/03/FoZpB4.png"
alt="FoZpB4.png" /></li>
<li>When an unlisted state occurs, the automaton will malfunction, and a
failure state can be added to handle these situations.</li>
</ul>
<h2 id="formal-language">Formal language</h2>
<ul>
<li>Formal language is a model that can and only generate and recognize
some symbol strings of a certain language that satisfy the definition of
formal language. Formal language is a special type of regular language.
Formal languages are usually used to simulate certain parts of natural
languages. Taking the example /baa+!/ for instance, let the
corresponding automaton model be m, the input symbol table be <span
class="math inline">\(\Sigma = {a,b,!}\)</span> , and <span
class="math inline">\(L(m)\)</span> represents the formal language
described by m, which is an infinite set <span
class="math inline">\({baa!,baaa!,baaaa!,…}\)</span> .</li>
</ul>
<h2 id="non-deterministic-finite-automaton">Non-deterministic finite
automaton</h2>
<ul>
<li>Non-deterministic Finite Automaton (NFSA), by slightly modifying the
previous example, moving the self-loop to state 2, it becomes an NFSA,
because at this point, in state 2, with input a, there are two possible
transitions, and the automaton cannot determine the transition path:
<img data-src="https://s2.ax1x.com/2019/01/03/FoVLhq.png"
alt="FoVLhq.png" /></li>
<li>Another form of NFSA involves the introduction of <span
class="math inline">\(\epsilon\)</span> transitions, which means that
transitions can be made without the need for an input symbol, as shown
in the figure below, where at state 3, it is still uncertain how to
proceed with the transition: <img data-src="https://s2.ax1x.com/2019/01/03/FoVX90.png" alt="FoVX90.png" /></li>
<li>At the NFSA, when faced with a transfer choice, the automaton may
make an erroneous choice, and there are three solutions to this:
<ul>
<li>Rollback: Mark this state, and revert to this state after confirming
an error in the selection</li>
<li>Prospective: Looking ahead in the input to assist in making
choices</li>
<li>Parallel: Performing all possible transfers in parallel</li>
</ul></li>
<li>In automata, the states that need to be marked when using the
backtracking algorithm are called search states, which include two
parts: state nodes and input positions. For NFSA, the state transition
table also undergoes corresponding changes, as shown in the figure, with
the addition of a column representing the <span
class="math inline">\(\epsilon\)</span> transition, <span
class="math inline">\(\epsilon\)</span> , and the transition can move to
multiple states: <img data-src="https://s2.ax1x.com/2019/01/03/FoZE36.png"
alt="FoZE36.png" /></li>
<li>Adopting the fallback strategy, the algorithm for the
nondeterministic automaton is as follows, which is a search algorithm:
<img data-src="https://s2.ax1x.com/2019/01/03/FoZSuF.png"
alt="FoZSuF.png" /></li>
<li>值</li>
<li>The subfunction ACCEPT-STATE accepts a search state, determining
whether to accept it, and the accepted search state should be a tuple of
the final state and the input end position.</li>
<li>Algorithm uses an agenda (process table) to record all search
states, initially including only the initial search state, i.e., the
initial state node of the automaton and the input start. After that, it
continuously loops, extracting search states from the agenda, first
calling ACCEPT-STATE to determine if the search is successful, and then
calling GENERATE-NEW-STATES to generate new search states and add them
to the agenda. The loop continues until the search is successful or the
agenda is empty (all possible transitions have been attempted and
failed) and returns a rejection.</li>
<li>The NFSA algorithm is a state space search that can improve search
efficiency by changing the order of search states, for example, by using
a stack to implement the process table for depth-first search (DFS); or
by using a queue to implement the process table for breadth-first search
(BFS).</li>
<li>For any NFSA, there exists a completely equivalent DFSA.</li>
</ul>
<h2 id="regular-languages-and-nfsa">Regular Languages and NFSA</h2>
<ul>
<li>The alphabet \(\sum\) is defined as the set of all input symbols;
the empty symbol string <span class="math inline">\(\epsilon\)</span> ,
the empty symbol string does not contain in the alphabet; the empty set
∅. The class (or regular set) of regular languages over \(\sum\) can be
defined as follows:
<ul>
<li>The empty set is a regular language</li>
<li>∀a ∈ <span class="math inline">\(\sum\)</span> ∪ <span
class="math inline">\(\epsilon\)</span> , {a} is a formal language</li>
<li>If <span class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> are regular languages, then:</li>
<li>Concatenation of <span class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> is regular language</li>
<li>The conjunction and disjunction of <span
class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> are also regular languages</li>
<li>The Kleene closure of <span class="math inline">\(L_1\)</span> is
also a regular language, i.e., <span
class="math inline">\(L_1\)</span></li>
</ul></li>
<li>Three basic operators of regular languages: concatenation,
conjunction and disjunction, and Kleene closure. Any regular expression
can be written in the form that uses only these three basic
operators.</li>
<li>Regular languages are also closed under the following operations (
<span class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> are both regular languages):
<ul>
<li>The intersection of the symbol string sets of <span
class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> also constitutes a regular
language</li>
<li>The difference set of the symbol sequences of <span
class="math inline">\(L_1\)</span> and <span
class="math inline">\(L_2\)</span> also constitutes a regular
language</li>
<li>The language consisting of sets not in the symbol string collection
of <span class="math inline">\(L_1\)</span> is also a regular
language</li>
<li>The set of reverses of all symbol strings constitutes a regular
language</li>
</ul></li>
<li>It can be proven that regular expressions and automata are
equivalent. A method to prove that any regular expression can be
constructed into a corresponding automaton is to, according to the
definition of regular languages, construct basic automata representing
the single symbol a in <span class="math inline">\(\epsilon\)</span> ,
∅, and <span class="math inline">\(\sum\)</span> , and then represent
the three basic operators as operations on the automata, inductively
applying these operations on the basic automata to obtain new basic
automata, thus constructing an automaton that satisfies any regular
expression, as shown in the following figure: <img data-src="https://s2.ax1x.com/2019/01/03/FoVxjU.png" alt="FoVxjU.png" />
Basic Automaton <img data-src="https://s2.ax1x.com/2019/01/03/FoZPE9.png"
alt="FoZPE9.png" /> Concatenation Operator <img data-src="https://s2.ax1x.com/2019/01/03/FoZ9HJ.png" alt="FoZ9HJ.png" />
Kleene Closure Operator <img data-src="https://s2.ax1x.com/2019/01/03/FoZiNR.png" alt="FoZiNR.png" />
Conjunction Disjunction Operator</li>
</ul>
<h1
id="chapter-3-morphology-and-finite-state-transcription-machines">Chapter
3: Morphology and Finite State Transcription Machines</h1>
<ul>
<li>Analysis: Take an input and produce various structures about this
input</li>
</ul>
<h2 id="introduction-to-english-morphology">Introduction to English
Morphology</h2>
<ul>
<li>Morphological study analyzes the structure of words, which can be
further decomposed into morphemes. Morphemes can be divided into stems
and affixes, and affixes can be further categorized into prefixes,
infixes, suffixes, and positional affixes.</li>
<li>Inflectional morphology: In English, nouns include only two
inflectional forms: one affix indicates plural, and one affix indicates
possession:
<ul>
<li>Plural: -s, -es, irregular plural forms</li>
<li>Ownership: -'s, -s'</li>
</ul></li>
<li>Inflectional changes in verbs include those of regular verbs and
irregular verbs:
<ul>
<li>Rule verbs: main verbs and basic verbs, -s, -ing, -ed,</li>
<li>Irregular verbs</li>
</ul></li>
<li>Derivative morphology: Derivation combines a stem with a grammatical
morpheme to form new words
<ul>
<li>Nominalization: -ation, -ee, -er, -ness</li>
<li>Derived adjectives: -al, -able, -less</li>
</ul></li>
</ul>
<h2 id="morphological-analysis">Morphological analysis</h2>
<ul>
<li>Example: We hope to establish a morphological analyzer that takes a
word as input and outputs its stem and related morphological features,
as shown in the table below; our goal is to produce the second and
fourth columns: <img data-src="https://s2.ax1x.com/2019/01/03/FoZA9x.png"
alt="FoZA9x.png" /></li>
<li>We at least need:
<ul>
<li>Lexicon: List of stems and affixes and their basic information</li>
<li>Morphotactic rules: What morphemes follow what morphemes</li>
<li>Orthographic rule: Changes in spelling rules during morpheme
combination</li>
</ul></li>
<li>Generally, word lists are not constructed directly but are designed
based on morphological order rules to generate words by inflecting
stems. For example, a simple automaton for pluralizing nouns is shown as
follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZmuD.png"
alt="FoZmuD.png" /></li>
<li>reg-noun represents the regular noun, which can be pluralized by
adding an "s," and it ignores irregular singular nouns (irreg-sg-noun)
and irregular plural nouns (irreg-pl-noun). Another automaton for
simulating the inflectional changes of modal verbs is shown below: <img data-src="https://s2.ax1x.com/2019/01/03/FoZQUA.png" alt="FoZQUA.png" /></li>
<li>A method for using FSA to solve morphological recognition problems
(determining whether an input symbol string is legal) is to subdivide
state transitions to the letter level, but there will still be some
issues: <img data-src="https://s2.ax1x.com/2019/01/03/FoZZjO.png"
alt="FoZZjO.png" /></li>
</ul>
<h2 id="finite-state-transcription-machine">Finite State Transcription
Machine</h2>
<ul>
<li>Double-layer morphology: Representing a word as a lexical layer and
a surface layer, the lexical layer indicates the simple adjacency
(concatenation) of morphemes, and the surface layer represents the
actual final spelling of the word. A finite state transducer is a finite
state automaton, but it implements transcription, achieving
correspondence between the lexical layer and the surface layer. It has
two inputs, producing and recognizing string pairs, and each state
transition arc has two labels, representing the two inputs. <img data-src="https://s2.ax1x.com/2019/01/03/FoZVgK.png" alt="FoZVgK.png" /></li>
<li>From four perspectives to view FST:
<ul>
<li>As a recognizer: The FST accepts a pair of strings as input and
outputs acceptance if the pair of strings is in the string pairs of the
language, otherwise it rejects</li>
<li>As a generator: generating string pairs of language</li>
<li>As a translator: Read in a string, output another</li>
<li>As an associative: calculating the relationship between two
sets</li>
</ul></li>
<li>Define finite state transducer:
<ul>
<li>A limited state set for state {q_i}</li>
<li>Finite input alphabet</li>
<li>Δ: Finite output symbol alphabet</li>
<li>Initial State</li>
<li>Ultimate State Set</li>
<li>Transition function or transition matrix between states, a relation
from Q×Σ to 2^Q, where q is the state and w is the string, returning the
new state set</li>
<li>Output function: Given each state and input, returns the set of
possible output strings, which is a relation from <span
class="math inline">\(Q × \Sigma\)</span> to <span
class="math inline">\(2^∆\)</span></li>
</ul></li>
<li>In FST, the elements of the alphabet are not single symbols, but
symbol pairs, known as feasible pairs. Analogous to FSA and regular
languages, FST and regular relations are isomorphic, closed under the
union operation, and generally not closed under the difference,
complement, and intersection operations.</li>
<li>Additionally, FST,
<ul>
<li>On the inverse (inverse of the inverse) closure, the inverse is used
to facilitate the transformation from an FST as an analyzer to an FST as
a generator</li>
<li>On composite (nested) closures, used to replace multiple
transcribing machines with a more complex transcribing machine.</li>
</ul></li>
<li>Transcription machines are generally non-deterministic; if the
search algorithm of a finite state automaton (FSA) is used, it will be
very slow, and if a non-deterministic to deterministic conversion
algorithm is used, some finite state transducers (FSTs) themselves
cannot be converted to be deterministic.</li>
<li>Sequential transducer is a deterministic transducer with input,
where each state transition is determined after the given state and
input, unlike the FST in the figure above, where state 0 has two state
transitions upon input b (transferring to the same state but with
different outputs). Sequential transducer can use the symbol <span
class="math inline">\(\epsilon\)</span> , but it can only be added to
the output string, not the input string, as shown in the following
figure: <img data-src="https://s2.ax1x.com/2019/01/03/FoZuHH.png"
alt="FoZuHH.png" /></li>
<li>The output of a sequential transducer is not necessarily sequential,
that is, different transitions from the same state may produce the same
output. Therefore, the inverse of a sequential transducer is not
necessarily a sequential transducer. Hence, when defining a sequential
transducer, direction must be defined, and the transition function and
output function need to be slightly modified, with the output space
reduced to Q and ∆.</li>
<li>A generalized form of the sequential transcription machine is the
concurrent transcription machine, which outputs an additional string in
the final state, appended to the string already output. Sequential and
concurrent transcription machines are highly efficient, and there are
effective algorithms for their determinization and minimization, making
them very important. The P-concurrent transcription machine can resolve
ambiguity issues on this basis.</li>
</ul>
<h2
id="using-a-finite-state-transducer-for-morphological-analysis">Using a
finite state transducer for morphological analysis</h2>
<ul>
<li>Viewing words as the relationship between the lexical layer and the
surface layer, as shown in the figure below: <img data-src="https://s2.ax1x.com/2019/01/03/FoZnDe.png" alt="FoZnDe.png" /></li>
<li>On the basis of the previously defined double-layer morphology, the
mapping from a self to itself is defined as a basic pair, represented by
a single letter; the symbol ^ represents the morpheme boundary; and the
symbol # represents the word boundary. In the task, it is mentioned that
it is necessary to output features such as +SG for morphemes, which do
not have corresponding output symbols on another output. Therefore, they
are mapped to an empty string or boundary symbol. We connect
input-output pairs with a colon, or they can also be written above and
below an arc. An abstract representation of an English noun plural
inflection transducer is shown as follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZl4I.png" alt="FoZl4I.png" /></li>
<li>Afterward, we need to update the lexicon so that irregular plural
nouns can be parsed into the correct stems: <img data-src="https://s2.ax1x.com/2019/01/03/FoZMEd.png" alt="FoZMEd.png" /></li>
<li>Afterwards, the abstract transcription machine is concretized into a
transcription machine composed of letter transfer arcs, as shown in the
figure below, which only displays the concretized part after the
irregular plural and singular nouns: <img data-src="https://s2.ax1x.com/2019/01/03/FoZ3Ct.png" alt="FoZ3Ct.png" /></li>
</ul>
<h2 id="transcription-machines-and-orthographic-rules">Transcription
Machines and Orthographic Rules</h2>
<ul>
<li>Using spelling rules, also known as orthographic rules, to handle
the issue of frequent spelling errors at morpheme boundaries in
English.</li>
<li>Here are some examples of spelling rules:
<ul>
<li>Consonant cluster: beg/beggin</li>
<li>Deletion of E: make/making</li>
<li>E insertion: watch/watches</li>
<li>Y's replacement: try/tries</li>
<li>K's insertion: panic/panicked</li>
</ul></li>
<li>To achieve spelling rules, we introduce an intermediate layer
between the lexical and surface layers, taking specific rule-based
morpheme concatenation as input and modifying it to produce correct
morpheme concatenation as output, for example, the input "fox +N +PL" is
transcribed into the intermediate layer, resulting in "fox ^ s#", and
then during the second transcription from the intermediate layer to the
surface layer, the special morpheme concatenation "x^ and s#" is
detected, and an "e" is inserted between the "x" and "s" on the surface
layer, yielding "foxes." The following diagram of the transcription
machine illustrates this process: <img data-src="https://s2.ax1x.com/2019/01/03/FoZ88P.png" alt="FoZ88P.png" /></li>
<li>This transducer only considers the positive lexical rule of
inserting e when x^ and s# are contiguous</li>
<li>Other words can pass normally</li>
<li><span class="math inline">\(Q_0\)</span> represents irrelevant words
passing through, indicating an accepted state</li>
<li><span class="math inline">\(Q_1\)</span> represents seeing zsx,
serving as an intermediate state, the last z, s, x connected to
morphemes are always saved; if other letters appear, it returns to q0,
and it can also serve as an accepting state</li>
<li><span class="math inline">\(Q_2\)</span> Represents seeing morphemes
connected to z, s, x, followed by four transitions
<ul>
<li>Received <span class="math inline">\(x\)</span> , <span
class="math inline">\(z\)</span> , returned to <span
class="math inline">\(q_1\)</span> , that is, believing that reconnected
to x, z that may be connected to morphemes</li>
<li>Received <span class="math inline">\(s\)</span> , it is divided into
two cases. One is the normal case where e needs to be inserted, in which
case it is transferred through <span
class="math inline">\(\epsilon\)</span> to <span
class="math inline">\(q_3\)</span> and then to <span
class="math inline">\(q_4\)</span> . The other is the case where <span
class="math inline">\(e\)</span> needs to be inserted from the
beginning, reaching <span class="math inline">\(q_5\)</span> , after
which it may retreat to <span class="math inline">\(q_1\)</span> , <span
class="math inline">\(q_0\)</span> , or <span
class="math inline">\(s\)</span> , or return to <span
class="math inline">\(q_2\)</span> due to contiguous morphemes. The two
cases are uncertain and need to be resolved through search.</li>
<li>Reaching word boundaries and other symbols, returning to <span
class="math inline">\(q_0\)</span></li>
<li>It can also function as the passive voice itself</li>
</ul></li>
</ul>
<h2 id="combine">Combine</h2>
<ul>
<li>Now, a three-layer structure can be used, combining a transducer for
generating morphemes and performing morphological rule correction. From
the lexical layer to the intermediate layer, a transducer generates
morphemes, and from the intermediate layer to the surface layer,
multiple transducers can be used in parallel for morphological rule
correction.</li>
<li>Two types of transcribing machines can be rewritten as one type when
superimposed, at which point the Cartesian product of the state sets of
the two types of state machines needs to be calculated, and a state
needs to be established for each element in the new set.</li>
<li>This three-level structure is reversible, but ambiguity issues may
arise during analysis (from the surface to the lexical level), that is,
a single word may be analyzed into multiple morpheme combinations. In
this case, relying solely on a transcribing machine is insufficient to
resolve the ambiguity, and context is needed.</li>
</ul>
<h2 id="other-applications-brief-introduction">Other Applications (brief
introduction)</h2>
<ul>
<li>FST without a lexicon, PORTER stemmer: Implementing cascading
rewriting rules with FST to extract the stems of words.</li>
<li>Tokenization and sentence segmentation: A simple English tokenizer
can be implemented based on regular expressions, and a simple Chinese
tokenizer can be implemented using maxmatch (a greedy search algorithm
based on maximum length matching).</li>
<li>Spelling Check and Correction: The FST using projection operations
can complete the detection of non-word errors, and then correction based
on the minimum edit distance (implemented using dynamic programming
algorithms) can be performed. Normal word error detection and correction
require the assistance of N-gram language models.</li>
</ul>
<h2 id="how-humans-perform-morphological-processing">How Humans Perform
Morphological Processing</h2>
<ul>
<li>The study indicates that the human mental lexicon stores a portion
of morphological structures, while other structures are not combined in
the mental lexicon and require separate extraction and combination. The
study elucidates two issues:
<ul>
<li>Productive morphological forms, particularly inflectional changes,
play a role in the human mental lexicon, and the phonological lexicon
and the orthographic lexicon may have the same structure.</li>
<li>For example, many properties of morphology, a language processing
field, can be applied to the understanding and generation of
language.</li>
</ul></li>
</ul>
<h1 id="chapter-4-n-gram-grammar">Chapter 4: N-gram Grammar</h1>
<ul>
<li>Language models are statistical models of word sequences, and the
N-gram model is one of them. It predicts the Nth word based on the
previous N-1 words, and such conditional probabilities can constitute
the joint probability of the entire word sequence (sentence).</li>
</ul>
<h2 id="count-words-in-a-corpus">Count words in a corpus</h2>
<ul>
<li><p>Difference: Word type, or vocabulary size V, represents the
number of different words in the corpus, while tokens, without
duplicates, represent the size of the corpus. Some studies suggest that
the dictionary size should not be less than the square root of the
number of tokens. Non-smooth N-gram grammar model</p></li>
<li><p>Task: Infer the probability of the next word based on previous
words: <span class="math inline">\(P(w|h)\)</span> , and calculate the
probability of the entire sentence: <span
class="math inline">\(P(W)\)</span> .</p></li>
<li><p>The simplest approach is to use the classical probability model,
counting the number of occurrences of the segment composed of historical
h and current word w in the corpus, and dividing it by the number of
occurrences of the historical h segment in the corpus. The probability
of the sentence is also generated using a similar method. Drawback: It
depends on a large corpus, and the language itself is variable, making
the calculation too strict.</p></li>
<li><p>Next, the N-gram grammar model is introduced. Firstly, through
the chain rule of probability, the relationship between the conditional
probability <span class="math inline">\(P(w|h)\)</span> and the joint
probability of the entire sentence <span
class="math inline">\(P(W)\)</span> can be obtained:</p>
<p><span class="math display">\[
P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\
= \prod _{k=1}^n P(w_k|w_1^{k-1}) \\
\]</span></p></li>
<li><p>N-gram grammar model relaxes the constraints on conditional
probability, making a Markov assumption: the probability of each word is
only related to the previous N-1 words, for example, in the bigram
grammar model, it is only related to the previous word, using this
conditional probability to approximate <span
class="math inline">\(P(w|h)\)</span></p>
<p><span class="math display">\[
P(w_n|w_1^{n-1}) \approx P(w_n|w_{n-1}) \\
\]</span></p></li>
<li><p>The conditional probability in the N-gram model is estimated
using maximum likelihood, counting the occurrences of various N-gram
patterns in the statistical corpus and normalizing them, with a
simplification being that, for example, in the case of bigram grammar,
the total number of bigrams starting with a given word must equal the
count of the unigram of that word:</p>
<p><span class="math display">\[
P(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\
\]</span></p></li>
<li><p>After using N-gram grammar, the chain decomposition of sentence
probability becomes easy to calculate, and we can determine whether a
sentence contains misspellings by calculating the probabilities of
various sentences, or calculate the likelihood of certain sentences
appearing in a given context, because N-gram grammar can capture some
linguistic features or some usage habits. When there is sufficient
corpus, we can use the trigram grammar model to achieve better
results.</p></li>
</ul>
<h2 id="training-set-and-test-set">Training set and test set</h2>
<ul>
<li>The N-gram grammar model is highly sensitive to the training set.
The larger the N in N-gram grammar, the more contextual information it
depends on, and the smoother the sentences generated by the N-gram
grammar model become. However, they may not be "too smooth" because the
N-gram probability matrix is very large and sparse, especially in the
case of N being large, such as in a four-gram, where once the first word
is generated, the available choices are very few. After generating the
second word, the choices become even fewer, often only one choice,
resulting in a sentence that is identical to one in the original text
with a certain four-gram. Over-reliance on the training set will degrade
the model's generalization ability. Therefore, the training set and test
set we choose should come from the same subfield.</li>
<li>Sometimes, there may be words in the test set that are not present
in the training set dictionary, i.e., out-of-vocabulary (OOV) words. In
open dictionary systems, we first fix the size of the dictionary and
replace all OOV words with special symbols before training.</li>
</ul>
<h2 id="evaluation-of-n-gram-grammar-models-perplexity">Evaluation of
N-gram Grammar Models: Perplexity</h2>
<ul>
<li><p>The evaluation of models is divided into two types: external
evaluation and internal evaluation. External evaluation is an end-to-end
evaluation that examines whether the improvement of a certain module has
improved the overall effectiveness of the model. The purpose of internal
evaluation is to quickly measure the potential improvement effect of the
module. The potential improvement effect of the internal evaluation does
not necessarily lead to an increase in the end-to-end external
evaluation, but generally, there is some positive correlation between
the two.</p></li>
<li><p>Perplexity (PP) is an intrinsic evaluation method for
probabilistic models. The perplexity of a language model on a test set
is a function of the probabilities assigned by the language model to the
test set. Taking binary grammar as an example, the perplexity on the
test set is:</p>
<p><span class="math display">\[
PP(W) = \sqrt[n]{\prod _{i=1}^N \frac {1}{P(w_i|w_{i-1})}} \\
\]</span></p></li>
<li><p>The higher the probability, the lower the perplexity. Two
interpretations of perplexity:</p>
<ul>
<li><p>Weighted average branching factor: The branching factor refers to
the number of words that can follow any preceding text. It is obvious
that if our model has learned nothing, any word in the test set can
follow any preceding text, resulting in a high branching factor and high
perplexity; conversely, if our model has learned specific rules, words
are restricted to follow some specified preceding texts, and the
perplexity decreases. Perplexity uses probability-weighted branching
factor, and the size of the branching factor remains unchanged before
and after the model learns, so "morning" can still follow any preceding
text, but the probability of it following "good" increases, thus it is a
weighted branching factor.</p></li>
<li><p>Entropy: For a language sequence, we define the entropy of a
sequence as: <span class="math display">\[
H(w_1,w_2,…,w_n )=-\sum _{W_1^n \in L} p(W_1^n) \log ⁡p(W_1^n)
\]</span> which is the sum of the entropies of all prefix sub-sequences
within this sequence, and its mean is the entropy rate of the sequence.
To calculate the entropy of the entire language, assuming the language
is a stochastic process that generates word sequences, with the word
sequence being infinitely long, then its entropy rate is: <span
class="math display">\[
H(L)=\lim _{n \rightarrow \infty}⁡ \frac 1n H(w_1,w_2,…,w_n) =\lim _{n
\rightarrow \infty} -⁡\frac 1n \sum _{W \in L} p(W_1^n)  \log ⁡p(W_1^n)
\]</span> According to the Shannon-McMillan-Breiman theorem, as n
approaches infinity, if the language is both stationary and regular, the
entropy of the sum of these substrings can be replaced by the maximum
substring, where the replacement refers to the probability of the
maximum substring calculated after the log, while the probability before
the log remains the probability of each substring? If so, the logarithm
of the probability of the maximum substring is proposed, and the sum of
probabilities of all sub-strings is obtained: <span
class="math display">\[
H(L)=\lim _{n \rightarrow \infty} -⁡ \frac 1n \log ⁡p(w_1,w_2,…,w_n)
\]</span> Cross-entropy can measure the distance between the probability
distribution generated by our model and the specified probability
distribution, and we hope that the probability distribution generated by
the model is as close as possible to the true distribution, i.e., the
cross-entropy is small. Specifically, it measures the cross-entropy of
the probabilities of generating the same language sequence by the model
m trained and the ideal model p: <span class="math display">\[
H(p,m) = \lim _{n \rightarrow \infty}⁡ - \frac 1n \sum _{W \in L}
p(W_1^n) \log⁡ m(W_1^n)
\]</span> However, we do not know the ideal distribution p. At this
point, according to the previous Shannon-McMillan-Breiman theorem, we
obtain the cross-entropy of a sequence that only contains one
probability distribution (?): <span class="math display">\[
H(p,m)=\lim _{n \rightarrow \infty}⁡ - \frac 1n \log⁡ m(W_1^n)
\]</span> On the test data, since we do not have infinitely long
sequences, we approximate the cross-entropy of the infinitely long
sequence using the cross-entropy of finite-length sequences. Perplexity
is the exponential operation of this (approximate? Only containing one
probability distribution?) cross-entropy:</p>
<p><span class="math display">\[
Perplexity(W) = 2^{H(W)} \\
= P(w_1 w_2 ... w_N)^{\frac {-1}{N}} \\
= \sqrt[n]{\frac {1}{P(w_1 w_2 ... w_N)}} \\
= \sqrt[n]{\prod _{i=1}^N \frac {1}{P(w_i | w_1 ... w_{i-1})}} \\
\]</span></p></li>
</ul></li>
</ul>
<h2 id="smooth">Smooth</h2>
<ul>
<li>Because the N-gram model depends on corpus, generally speaking, the
higher the N in the N-gram model, the sparser the data provided by the
corpus. In this case, the N-gram model performs poorly in estimating
grammatical counts that are very small, and if a sentence in the test
set contains an N-gram that did not appear in the training set, we
cannot use perplexity for evaluation. Therefore, we use smoothing as an
improvement method to make the maximum likelihood estimation of the
N-gram model adaptable to these situations with 0 probability.</li>
<li>Next, two types of smoothing are introduced:
<ul>
<li>Laplace smoothing (add 1 smoothing)</li>
<li>Good-Turing discounting method</li>
</ul></li>
</ul>
<h3 id="laplace-smoothing">Laplace Smoothing</h3>
<ul>
<li><p>Add 1 smoothing is to add 1 to each count before calculating
probability normalization, correspondingly, the denominator in
normalization is increased by the size of the dictionary:</p>
<p><span class="math display">\[
P_{Laplace}(w_i) = \frac {c_i + 1}{N+V} \\
\]</span></p></li>
<li><p>To demonstrate the effect of smoothing, an adjusted count <span
class="math inline">\(c^{*}\)</span> is introduced, and the smoothed
probability is written in the same form as before the smoothing:</p>
<p><span class="math display">\[
P_{Laplace} (w_i) = \frac {(C_i^{*})}{N} \\
C_i^{*} = \frac {(C_i+1)N}{(N+V)} \\
\]</span></p></li>
<li><p>An approach to viewing smoothness is: discount each non-zero
count, allocate some probability to the zero count, and define relative
discount <span class="math inline">\(d_c\)</span> (defined on non-zero
counts)</p>
<p><span class="math display">\[
d_c = \frac {c^{*}} {c}
\]</span></p></li>
<li><p>The change in word count before and after the discount is
represented by <span class="math inline">\(d_c\)</span> . After
smoothing, for non-zero counts, the count increases when <span
class="math inline">\(C_i &lt; \frac NV\)</span> , otherwise it
decreases. The higher the count, the greater the discount, with less
increase (more decrease). When there are many zero counts, N/V is
smaller, and in this case, most non-zero counts will decrease, and by a
larger amount.</p></li>
<li><p>And the 0 count was not affected by the discount. Therefore,
after a round of growth at different levels, the normalized result is
that the non-zero counts shared some probability with the 0 count.
Written in the form of adjusted counts, it means that the non-zero
counts decrease in value, and the 0 count changes (usually decreases) in
value (but not the decrease is equal to the increase). The book provides
an example, and the figure below shows the counts after binary grammar
smoothing of a part of the corpus: <img data-src="https://s2.ax1x.com/2019/01/03/FoZNDg.png" alt="FoZNDg.png" /> If
the table is written in the form of adjusted counts: <img data-src="https://s2.ax1x.com/2019/01/03/FoZtKS.png"
alt="FoZtKS.png" /></p></li>
<li><p>It can be seen that the original 0 count (blue) increases from 0,
while others decrease, for example, "<i>" decreases from 827 to 527,
from 608 to 238.</i></p></li>
<li><p>When the count of 0s is many, the reduction in the count of
non-0s is significant, and a decimal less than 1, <span
class="math inline">\(\delta\)</span> , can be used in place of 1, i.e.,
adding <span class="math inline">\(\delta\)</span> for smoothing.
Typically, this <span class="math inline">\(\delta\)</span> varies
dynamically.</p></li>
</ul>
<h3 id="gt-discounting-method">GT Discounting Method</h3>
<ul>
<li><p>Similar to the Good-Turing discounting method, the Witten-Bell
discounting method, and the Kneyser-Ney smoothing method, their basic
motivation is to estimate the count of never-seen items using the count
of items that appear only once. Items that appear only once are called
singletons or hapax legomena. The Good-Turing discounting method uses
the frequency of singletons to estimate the 0-count bigram.</p></li>
<li><p>Define N_c as the total number of N-gram grammars that appear c
times (not the total number multiplied by c), and call it the frequency
c frequency. The maximum likelihood estimate of c in N_c is c. This is
equivalent to dividing the N-gram grammar into multiple buckets
according to their frequency of occurrence, and the GT discounting
method uses the maximum likelihood estimate of the probability of the
grammar in the c+1 bucket to reestimate the probability of the grammar
in the c bucket. Therefore, after the GT estimation, the c obtained from
the maximum likelihood estimate is replaced with:</p>
<p><span class="math display">\[
c^{*}=(c+1) \frac {N_{c+1}}{N_c}
\]</span></p></li>
<li><p>After calculating the probability of a certain N-gram:</p>
<ul>
<li>Never appeared: <span
class="math inline">\(P_{GT}^{*}=\frac{N_1}{N}\)</span> . Here, N
represents the total number of N-gram grammars <span
class="math inline">\((\sum _i N_i * i)\)</span> . Assuming that <span
class="math inline">\(N_0\)</span> is known, this expression indicates
that when calculating the probability of a specific unknown N-gram
grammar, it should also be divided by <span
class="math inline">\(N_0\)</span> .</li>
<li><span class="math inline">\(P_{GT}^{*} = \frac{c^{*}}{N}\)</span>
has appeared (known count):</li>
</ul></li>
<li><p>Thus calculated, some probabilities of <span
class="math inline">\(N_1\)</span> are transferred to <span
class="math inline">\(N_0\)</span> . The GT discounting method assumes
that all N-gram probability distributions satisfy the binomial
distribution, and assumes that we know <span
class="math inline">\(N_0\)</span> , taking the bigram grammar as an
example:</p>
<p><span class="math display">\[
N_0 = V^2 - \sum _{i&gt;0} N_i \\
\]</span></p></li>
<li><p>Other considerations:</p>
<ul>
<li><p>Some <span class="math inline">\(N_c\)</span> are 0, in which
case we cannot use these <span class="math inline">\(N_c\)</span> to
calculate the smoothed c. In this situation, we directly abandon
smoothing, let <span class="math inline">\(c^{*} = c\)</span> , and then
calculate a logarithmic linear mapping, <span
class="math inline">\(log⁡(N_c) = a + b \log(c)\)</span> , based on the
normal data. Substitute the abandoned smoothing c and use its inverse to
calculate the <span class="math inline">\(N_c\)</span> with a count of
0, so that these <span class="math inline">\(N_c\)</span> have values
and do not affect the calculation of higher-order c.</p></li>
<li><p>Smooth only the smaller c's <span
class="math inline">\(N_c\)</span> , consider the larger c's <span
class="math inline">\(N_c\)</span> sufficiently reliable, set a
threshold k, and calculate the <span class="math inline">\(N_c\)</span>
of <span class="math inline">\(c &lt; k\)</span></p>
<p><span class="math display">\[
c^{*} = \frac {(c+1) \frac {N_c+1}{N_c} - c \frac {(k+1) N_{k+1} }{N_1}
} {1- \frac {(k+1)N_{k+1}} {N_1}} \\
\]</span></p></li>
<li><p>When calculating for smaller c such as c=1, also treat it as the
case of c=0 for smoothing</p></li>
</ul></li>
<li><p>An example: <img data-src="https://s2.ax1x.com/2019/01/03/FoZGgf.png"
alt="FoZGgf.png" /></p></li>
</ul>
<h2 id="interpolation-and-regression">Interpolation and Regression</h2>
<ul>
<li>The above smoothing only considers how to transfer probabilities to
grammatical units with a count of 0. For conditional probability <span
class="math inline">\(p(w|h)\)</span> , we can also adopt a similar
idea: if there is no ternary grammar to help calculate <span
class="math inline">\(p(w_n |w_{n-1} w_{n-2})\)</span> , then a grammar
with a lower order, <span class="math inline">\(p(w_n |w_{n-1})\)</span>
, can be used to assist in the calculation. There are two options:
<ul>
<li>Regression: High-order grammar alternative to the 0-count of
low-order grammar</li>
<li>Interpolation: Weighted estimation of higher-order grammar using
lower-order grammar</li>
</ul></li>
<li>In the Katz back-off, we use GT discounting as part of the method:
GT discounting tells us how much probability can be extracted from the
known grammar, while Katz back-off tells us how to distribute these
extracted probabilities to the unknown grammar. In the previous GT
discounting method, we distributed the extracted probabilities evenly
among each unknown grammar, while Katz back-off relies on the
information of low-order grammar to distribute: <img data-src="https://s2.ax1x.com/2019/01/03/FoZJv8.png" alt="FoZJv8.png" /></li>
<li>The probability obtained after the discount is denoted as <span
class="math inline">\(P^{*}\)</span> ; \alpha is the normalization
coefficient, ensuring that the probability allocated is equal to the
probability obtained from the unknown grammar allocation.</li>
<li>Interpolation is obtained by weighted summation using low-order
grammatical probabilities to derive the unknown high-order grammatical
probabilities: <img data-src="https://s2.ax1x.com/2019/01/03/FoZUbQ.png"
alt="FoZUbQ.png" /></li>
<li>The weighted coefficients can also be dynamically calculated through
context. There are two methods for calculating specific coefficients:
<ul>
<li>Attempt various coefficient combinations, using the one that
performs best on the validation set</li>
<li>Viewing coefficients as latent variables in a probabilistic
generative model and inferring them using the EM algorithm</li>
</ul></li>
</ul>
<h2 id="actual-issue-tools-and-data-formats">Actual Issue: Tools and
Data Formats</h2>
<ul>
<li>In language model computation, logarithms of probabilities are
generally used for calculation, for two reasons: to prevent numerical
underflow; and because taking logarithms converts multiplicative
accumulation into additive accumulation, thereby speeding up the
computation.</li>
<li>The N-gram grammar model generally uses ARPA format. ARPA format
files consist of some header information and lists of various N-gram
grammars, which include all grammars, probabilities, and normalized
coefficients for the N-gram grammars. Only low-order grammars that can
be called high-order grammar prefixes can be utilized in the backoff and
have normalized coefficients.</li>
<li>Two toolkits for calculating N-gram grammar models: SRILM toolkit
and Cambridge-CMU toolkit</li>
</ul>
<h2 id="advanced-issues-in-language-modeling">Advanced Issues in
Language Modeling</h2>
<h3 id="advanced-smoothing-method-kneser-ney-smoothing">Advanced
Smoothing Method: Kneser-Ney Smoothing</h3>
<ul>
<li>Noticing that in the GT discounting method, the estimated c value
after discounting is approximately one constant d more than the c value
obtained from maximum likelihood estimation. The absolute discounting
method takes this into account, subtracting this d from each count: <img data-src="https://s2.ax1x.com/2019/01/03/FoZwUs.png" alt="FoZwUs.png" /></li>
<li>Kneser-Ney smoothing incorporates this perspective and also
considers continuity: words that appear in different contexts are more
likely to appear after new contexts, and when backtracking, we should
prioritize such words that appear in multiple context environments
rather than those that occur frequently but only in specific contexts.
<img data-src="https://s2.ax1x.com/2019/01/03/FoZdEj.png"
alt="FoZdEj.png" /></li>
<li>In Kneser-Ney, the interpolation method can achieve better results
than the back-off method: <img data-src="https://s2.ax1x.com/2019/01/03/FoZ05n.png" alt="FoZ05n.png" /></li>
</ul>
<h3 id="based-on-categorization-n-gram-grammar">Based on Categorization
N-gram Grammar</h3>
<ul>
<li><p>This method is designed to address the sparsity of training data.
For example, in IBM clustering, each word can only belong to one
category; for instance, in the case of bigram grammar, the calculation
of the conditional probability of a bigram grammar becomes the
conditional probability of a word given the category of the preceding
context, which can also be further decomposed into the conditional
probabilities of two categories multiplied by the conditional
probability of a word given its category.</p>
<p><span class="math display">\[
p(w_i│w_{i-1} ) \approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \cdot p(w_i
|c_i)
\]</span></p></li>
</ul>
<h3 id="language-model-adaptation-and-network-applications">Language
Model Adaptation and Network Applications</h3>
<ul>
<li>Adaptation refers to training language models on large, broad
corpora and further improving them on language models in small,
specialized domains. The web is an important source of large corpora. In
practical applications, it is impossible to search for every grammar and
count all grammatical occurrences on all pages retrieved. We use the
number of pages retrieved to approximate the count.</li>
</ul>
<h3 id="utilizing-longer-distance-contextual-information">Utilizing
longer-distance contextual information</h3>
<ul>
<li>Usually we use bigram and trigram grammar models, but larger N can
also be used to capture more contextual information. To capture
longer-distance contextual information, there are several methods:
<ul>
<li>N-gram Model Based on Caching Mechanism</li>
<li>Based on topic modeling, a N-gram grammar model is applied to
different topic modeling language models, followed by weighted
summation</li>
<li>Not necessarily using adjacent context information, such as skip
N-grams, or not necessarily using fixed-length context information, such
as variable-length N-grams</li>
</ul></li>
</ul>
<h1 id="chapter-16-the-complexity-of-language">Chapter 16: The
Complexity of Language</h1>
<h2 id="chomsky-hierarchy">Chomsky hierarchy</h2>
<ul>
<li>Chomsky's hierarchy reflects the implicational relationships between
grammars described by different formalization methods, with stronger
generative capacity or more complex grammars on the outer layers. From
the outside to the inside, the constraints added to the rewrite grammar
rules increase, and the generative capacity of the language gradually
diminishes. <img data-src="https://s2.ax1x.com/2019/01/03/FoZXad.png"
alt="FoZXad.png" /></li>
<li>Five grammatical rules and application examples corresponding to
each: <img data-src="https://s2.ax1x.com/2019/01/03/Foepxf.png"
alt="Foepxf.png" />
<ul>
<li>0-type grammar: By rule, there is only one restriction, that is, the
left-hand side of the rule cannot be an empty string. 0-type grammar
characterizes recursively enumerable languages.</li>
<li>Context-related grammar: The non-terminal symbol A between the
contexts \alpha and \beta can be rewritten as any non-empty symbol
string</li>
<li>Temperate context-related grammar</li>
<li>Context-free grammar: Any single non-terminal symbol can be
rewritten as a string consisting of terminal and non-terminal symbols,
or it can be rewritten as an empty string</li>
<li>Regular Grammar: It can be right-linear or left-linear. Taking
right-linear as an example, a non-terminal symbol can be rewritten as
another non-terminal symbol with several terminal symbols added to the
left, and right-linear continuously generates terminal symbols on the
left side of the string.</li>
</ul></li>
</ul>
<h2 id="is-natural-language-regular">Is Natural Language Regular</h2>
<ul>
<li>The ability to determine whether a language is regular allows us to
understand which level of grammar should be used to describe a language,
and this question can help us understand certain formal characteristics
of different aspects of natural language.</li>
<li>P pumping lemma: Used to prove that a language is not a regular
language.
<ul>
<li>If a language can be described by a finite state automaton, there
corresponds to the automaton a memory constraint quantity. This
constraint quantity does not increase significantly for different symbol
strings, as the number of states is fixed; longer symbol strings should
be produced through transitions between states rather than by increasing
the number of states. Therefore, this memory quantity does not
necessarily scale proportionally with the length of the input.</li>
<li>If a regular language can describe arbitrarily long symbol
sequences, more than the number of states in an automaton, then there
must be cycles in the automaton. <img data-src="https://s2.ax1x.com/2019/01/03/FoZxPI.png" alt="FoZxPI.png" /></li>
</ul></li>
<li>As shown in the figure of the automaton, it can express xyz, xyyz,
xyyyz ..., of course, the infinitely long y sequence in the middle can
also be "sucked out," expressing xz. The principle of suction is
described as follows:</li>
<li>Let L be a finite regular language, then there exist symbol strings
x, y, z such that for any n ≥ 0, y ≠ <span
class="math inline">\(\epsilon\)</span> , and xy^n z ∈ L</li>
<li>If a language is regular, there exists a string y that can be
appropriately "absorbed." This theorem is a necessary but not sufficient
condition for a language to be regular.</li>
<li>Some scholars have proven that English is not a regular language:
<ul>
<li>Sentences with mirror properties can be proven not to be regular
languages through the principle of suction, and a special subset in
English is isomorphic to such sentences with mirror properties.</li>
<li>Another proof is based on certain sentences with a central-nested
structure. Such sentences can be obtained by the intersection of English
and a certain type of simple regular expression, and it can be shown by
the pumping lemma that these sentences are not regular languages. If the
intersection of English and regular languages is not a regular language,
then English is not a regular language.</li>
</ul></li>
</ul>
<h2 id="is-natural-language-context-free">Is natural language
context-free</h2>
<ul>
<li>Since natural language is not a regular language, we then consider a
more lenient constraint: is natural language context-free?</li>
<li>Not...</li>
</ul>
<h2
id="computational-complexity-and-human-language-processing">Computational
Complexity and Human Language Processing</h2>
<ul>
<li>People find it difficult to process centrally nested sentences
because the stack memory used in analysis is limited, and memories at
different levels in the stack are prone to confusion.</li>
</ul>
<h1 id="chapter-5-part-of-speech-tagging">Chapter 5: Part-of-Speech
Tagging</h1>
<ul>
<li>Various expressions: POS (Part of Speech), word classes,
morphological classes, lexical tags.</li>
<li>The significance of POS lies in:
<ul>
<li>A wealth of information about words and their contexts can be
provided.</li>
<li>The same word has different pronunciations under different parts of
speech, so POS can also provide information for speech processing.</li>
<li>Perform stemming, assist information retrieval</li>
</ul></li>
<li>This chapter introduces three word tagging algorithms:
<ul>
<li>Rule-based algorithm</li>
<li>Probabilistic algorithms, Hidden Markov Models</li>
<li>Algorithm Based on Transformation</li>
</ul></li>
</ul>
<h2 id="general-word-classes">General Word Classes</h2>
<ul>
<li>POS is divided into closed sets and open sets, with closed sets
being relatively stable, such as prepositions, while the words in open
sets are continuously dynamically expanded, such as nouns and verbs. The
open sets of a specific speaker or a specific corpus may differ, but all
speakers of a language and various large-scale corpora may share the
same closed set. The words in closed sets are called function words
(functional words, function words), which are grammatical words,
generally short, and have a high frequency of occurrence.</li>
<li>Four open classes: nouns, verbs, adjectives, adverbs.</li>
<li>Nouns are defined functionally rather than semantically, therefore
nouns generally represent people, places, and things, but neither
sufficiently nor necessarily. Define nouns:
<ul>
<li>With the appearance of determiners</li>
<li>Can the subject be modified by pronouns</li>
<li>词以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词</li>
</ul></li>
<li>Verb, a word indicating action and process, including the forms of
third-person singular, non-third-person singular, present continuous,
and past participle</li>
<li>Adjectives, describing nature and quality</li>
<li>Adverb, used for modification; adverbs can modify verbs, verb
phrases, and other adverbs.</li>
<li>Some closed classes in English:
<ul>
<li>Prepositions: Appear before noun phrases, indicating
relationships</li>
<li>Determiners: related to definiteness Articles: related to
definiteness</li>
<li>Pronouns: A brief form of reference to certain noun phrases,
entities, or events</li>
<li>Conjunctions: Used for connection and complementation
(complementation)</li>
<li>Auxiliary verbs: Mark certain semantic features of the main verb,
including: tense, perfective aspect, polarity opposition, modality</li>
<li>Particles: Combine with verbs to form phrasal verbs</li>
<li>Numerals</li>
</ul></li>
</ul>
<h2 id="word-tagging">Word tagging</h2>
<ul>
<li>The input of annotation algorithms is a sequence of word symbols and
a set of tags, and the output requires each word to be annotated with a
single and optimal tag. If each word corresponds to only one part of
speech, then according to the existing set of tags, part-of-speech
tagging is a simple process of lookup and labeling. However, many words
have multiple parts of speech, such as "book," which can be both a noun
and a verb, thus requiring disambiguation. Part-of-speech tagging is an
important aspect of disambiguation.</li>
</ul>
<h2 id="rule-based-part-of-speech-tagging">Rule-based Part-of-Speech
Tagging</h2>
<ul>
<li>Presented the ENGTWOL system, constructed based on a double-layered
morphology, establishing separate entries for each word type, and
calculating without considering inflectional and derivative forms.</li>
<li>The first stage of the annotation algorithm involves using a
two-layer transducer to obtain all possible word categories for a given
word</li>
<li>Afterward, incorrect word classes are excluded by applying
constraint rules, which determine which word classes to exclude based on
the type of context.</li>
</ul>
<h2 id="word-classification-based-on-hidden-markov-models">Word
Classification Based on Hidden Markov Models</h2>
<ul>
<li><p>Using Hidden Markov Models for word segmentation is a type of
Bayesian inference, where word segmentation is viewed as a sequence
classification task. The observation is a word sequence (such as a
sentence), and the task is to assign a labeling sequence to this
sequence.</p></li>
<li><p>Given a sentence, Bayesian inference aims to select the best
sequence among all possible annotated sequences, that is</p>
<p><span class="math display">\[
{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)
\]</span></p></li>
<li><p>Using Bayes' theorem, it can be transformed into:</p>
<p><span class="math display">\[
{t_1^n} _{best}={argmax}
_{t_1^n}  \frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n}
P(w_1^n│t_1^n)P(t_1^n)
\]</span></p></li>
<li><p>The Hidden Markov Model makes two assumptions on this basis</p>
<ul>
<li>The probability of a word's occurrence is only related to the word's
part-of-speech tagging, and is unrelated to other words in the context
or other tags, thus decomposing the joint probability of the sequence
into the product of element probabilities, i.e., P(w_1<sup>n│t_1</sup>n)
≈ ∏_{i=1}^n P(w_i |t_i)</li>
<li>A labeled probability is only related to the previous label, similar
to the assumption of binary grammar: P(t_1^n) ≈ ∏_{i=1}^n P(t_i
|t_{i-1})</li>
</ul></li>
<li><p>Under two assumptions, the best annotated sequence expression
after simplification is:</p>
<p><span class="math display">\[
{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \approx {argmax}
_{t_1^n} \prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})
\]</span></p></li>
<li><p>The above probability expression actually decomposes the joint
probability of the HMM model into the product of individual transition
probabilities, specifically into label transition probabilities
(transitions between hidden variables) and word likelihood (transitions
from hidden variables to observable variables). Through maximum
likelihood estimation, we can calculate these two types of probabilities
using the classical probability type method from the annotated
corpus:</p>
<p><span class="math display">\[
P(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\
P(w_i│t_i ) = \frac{C(t_i,w_i)}{C(t_i)} \\
\]</span></p></li>
<li><p>An example: How the HMM model correctly identifies "race" as a
verb rather than a noun in the following sentence:</p></li>
<li><p>The Secretariat is expected to race tomorrow.</p></li>
<li><p>Sketch the HMM models for the two cases where "race" is
identified as both a verb and a noun, and you can see that only three
transition probabilities differ between the two models, which are marked
with bold lines: <img data-src="https://s2.ax1x.com/2019/01/03/FoZDCq.png"
alt="FoZDCq.png" /></p></li>
<li><p>The HMM word sense disambiguator operates in a global rather than
a local manner. We obtain these three transition probabilities by
counting in the corpus and then multiplying them together, resulting in
the probability of (a) being 843 times that of (b). It is obvious that
"race" should be tagged as a verb.</p></li>
</ul>
<h2 id="formalized-hidden-markov-model-annotator">Formalized Hidden
Markov Model Annotator</h2>
<ul>
<li><p>HMM model is an extension of finite automata, specifically a
weighted finite automaton, an extension of Markov chains, which allows
us to consider observed variables and hidden variables, and consider
probabilistic models that include hidden variables. The HMM includes the
following components:</p>
<ul>
<li>State set of size N</li>
<li>A: A transfer probability matrix of size N*N</li>
<li>Observation event set of size T</li>
<li>B: The observation likelihood sequence, also known as emission
probability, <span class="math inline">\(b_i (o_t)\)</span> describes
the probability of generating observation o_t from state i</li>
<li>Special initial and final states, without connected observation
quantities</li>
</ul></li>
<li><p>The probability in A corresponds to the prior <span
class="math inline">\(P(w_i│t_i )\)</span> and likelihood <span
class="math inline">\(P(t_i |t _{i-1})\)</span> probabilities in each
cumulative product term of the previous formula:</p>
<p><span class="math display">\[
{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \approx {argmax}
_{t_1^n} \prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})
\]</span></p></li>
</ul>
<h2 id="hidden-markov-model-hmm-viterbi-algorithm">Hidden Markov Model
(HMM) Viterbi Algorithm</h2>
<ul>
<li>In the HMM model, the task of inferring the hidden variables given
the transition probabilities and the observation sequence is called
decoding. One algorithm for decoding is the Viterbi algorithm, which is
essentially a dynamic programming algorithm, similar to the algorithm
previously used to find the minimum edit distance.</li>
<li>First, we calculate two matrices A and B from the corpus, that is,
the transition probabilities of the model are known. For a given
observation sequence, the Viterbi algorithm is executed according to the
following steps: <img data-src="https://s2.ax1x.com/2019/01/03/FoZyvT.png"
alt="FoZyvT.png" /></li>
<li>The algorithm maintains a Viterbi probability matrix <span
class="math inline">\((N+2)*T\)</span> with 2 representing the initial
and final states. Viterbi[s,t] represents the best path probability at
state s in step t, while backpointer[s,t] corresponds to the previous
state that led to the best path, used for backtracking to output the
entire best path.</li>
<li>The key transition lies in <span class="math inline">\(viterbi[s,t]
\leftarrow max _{s^{*}=1}^N⁡ viterbi[s^{*},t-1] * a_{s^{*},s} * b_s
(o_t)\)</span> that the optimal path at the current time step is
transferred from the optimal paths of various states in the previous
time step. The path with the maximum product of the probability of the
optimal path in the previous step and the transition probability is
chosen as the optimal path at the current time step. From the
perspective of dynamic programming, the optimal path of length t must be
selected from the optimal paths of length t-1, otherwise, it is certain
that a better solution can be obtained by transferring from another path
with a higher probability. This limits the possibilities of generating
the optimal path and reduces the computational amount.</li>
</ul>
<h2 id="extending-the-hmm-algorithm-to-trigram-grammar">Extending the
HMM algorithm to trigram grammar</h2>
<ul>
<li><p>Modern HMM annotators generally consider a longer history of the
preceding context in the annotation of transition probabilities:</p>
<p><span class="math display">\[
P(t_1^n ) \approx \prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})
\]</span></p></li>
<li><p>Such a case requires boundary handling at the beginning and end
of the sequence. One issue with using trigram grammar is data sparsity:
for example, if we have never seen the annotated sequence PRP VB TO in
the training set, we cannot compute P(TO|PRP,VB). One solution is linear
interpolation:</p>
<p><span class="math display">\[
P(t_i│t _{i-1} t _{i-2} ) = \lambda _1 P ̂(t_i│t _{i-1} t _{i-2}
)+\lambda _2 P ̂(t_i│t _{i-1} )+\lambda _3 P ̂(t_i)
\]</span></p></li>
<li><p>Determine the coefficient <span
class="math inline">\(\lambda\)</span> using the method of deletion
interpolation: <img data-src="https://s2.ax1x.com/2019/01/03/FoZr80.png"
alt="FoZr80.png" /></p></li>
</ul>
<h2 id="transformation-based-annotation">Transformation-Based
Annotation</h2>
<ul>
<li>The method based on transformation combines the advantages of
rule-based and probabilistic methods. The method based on transformation
still requires rules, but it summarizes rules from the data, which is a
supervised learning approach known as Transformation Based Learning
(TBL). In the TBL algorithm, the corpus is first annotated with
relatively broad rules, then slightly special rules are selected to
modify, followed by narrower rules to modify a smaller number of
annotations.</li>
</ul>
<h2 id="how-to-apply-the-tbl-rules">How to Apply the TBL Rules</h2>
<ul>
<li>Firstly, the most general rule is applied, which is to annotate each
word based on probability and select the word class with the highest
probability as the annotation. Then, transformation rules are applied,
meaning that if a certain condition is met, the previously annotated
word class is transformed (corrected) into the correct word class.
Subsequently, more stringent transformations are continuously applied,
making minor modifications based on the previous transformation.</li>
<li>How to Learn the TBL Rules
<ul>
<li>First, label each word with the most probable tag</li>
<li>Examine each possible transformation, select the transformation that
yields the greatest improvement in effect, and here it is necessary to
use the correct label for each word to measure the improvement brought
by the transformation, therefore it is supervised learning.</li>
<li>According to this selected transformation, re-label the data, repeat
step 2 until convergence (the improvement effect is less than a certain
threshold)</li>
</ul></li>
<li>The output of the above process is a sequence of ordered
transformations, used to form a labeling process and applied to new
corpus. Although all rules can be enumerated, the complexity is too
high, so we need to limit the size of the transformation set. The
solution is to design a small set of templates (abstract
transformations), where each allowed transformation is an instantiation
of one of the templates.</li>
</ul>
<h2 id="evaluation-and-error-analysis">Evaluation and Error
Analysis</h2>
<ul>
<li>Generally, it is divided into training set, validation set, and test
set, with ten-fold cross-validation performed within the training
set.</li>
<li>Comparing the computational accuracy with the gold standard of human
annotation as a measure.</li>
<li>The general human performance is used as a ceiling, and the result
with the highest probability marked by the one-way grammar is used as
the baseline.</li>
<li>Through confusion matrices or contingency tables to conduct error
analysis. In an N-classification task, the element in the ith row and
jth column of an N*N confusion matrix indicates the proportion of times
that the ith class is mistakenly classified as the jth class out of the
total number of misclassifications. Some common word types that are easy
to misclassify include:
<ul>
<li>Single nouns, proper nouns, adjectives</li>
<li>Adverbs, particles, prepositions</li>
<li>Verb past tense, verb past participle, adjective</li>
</ul></li>
</ul>
<h2 id="some-other-issues-in-part-of-speech-tagging">Some other issues
in part-of-speech tagging</h2>
<ul>
<li>Labeling uncertainty: A word has ambiguity between multiple parts of
speech, which is difficult to distinguish. In this case, some annotators
allow a word to be labeled with multiple part-of-speech tags. During
training and testing, there are three methods to address the issue of
multi-labeled words:
<ul>
<li>Select a label from these candidate labels in some way</li>
<li>Specify a word type during training, and consider the annotation
correct as long as any of the candidate word types are marked during
testing</li>
<li>View the entire set of uncertain parts of speech as a new complex
part of speech</li>
</ul></li>
<li>Multicomponent words: Before annotation, segmentation is required.
Whether some multicomponent words should be segmented into one part,
such as whether "New York City" should be divided into three parts or
treated as a whole, is also a consideration for various annotation
systems.</li>
<li>Unknown word: Words not found in dictionaries are called unknown
words. For unknown words, the training set cannot provide its likelihood
P(w_i | t_i), which can be addressed in the following ways:
<ul>
<li>Predicting POS based solely on contextual information</li>
<li>Estimating the distribution of unknown words using words that appear
only once, similar to the Good Turing discounting method</li>
<li>Utilizing spelling information of words with unknown words,
morphological information. For example, hyphenation, ed endings,
capitalized initials, etc. Subsequently, calculate the likelihood of
each feature in the training set, assuming independence among features,
and then multiply the likelihoods of features as the likelihood of the
unknown word: <span class="math inline">\(P(w_i│t_i )=p(unknown word│t_i
) * p(capital│t_i ) * p(endings/hyph|t_i)\)</span></li>
<li>Maximum Entropy Markov Model</li>
<li>Utilizing the log-linear model</li>
</ul></li>
</ul>
<h2 id="noise-channel-model">Noise Channel Model</h2>
<ul>
<li>Bayesian inference is used for annotation, which can be considered
an application of a noise channel model. This section introduces how to
use the noise channel model to complete the task of spelling correction.
Previously, non-word errors could be detected through dictionary lookup
and corrected based on the minimum edit distance, but this method is
ineffective for real word errors. The noise channel model can correct
both types of spelling errors.</li>
<li>The motivation of the noise channel model lies in treating a
misspelled word as a correctly spelled word that has been distorted and
interfered with after passing through a noise channel. We try all
possible correct words, input them into the channel, and then compare
the word after interference with the misspelled word; the input word
that corresponds to the most similar example is considered the correct
word. Such noise channel models, such as the previous HMM tagging model,
are a special case of Bayesian inference. We observe a misspelled word
and hope to find the latent variable (correctly spelled word) that
generates this observation, which is to find the maximum a
posteriori.</li>
<li>Applying the noise channel model to spelling correction: First,
assume various types of spelling errors, such as misspelling one,
misspelling two, omitting one, etc., then generate all possible
corrections, excluding those not existing in the dictionary, and
finally, calculate the posterior probabilities separately, selecting the
correction with the highest posterior probability. In this process,
likelihood needs to be calculated based on local contextual
features.</li>
<li>Another correction algorithm is a method that improves through
iteration: first, assume that the ambiguous matrix for spelling
correction is uniformly distributed, then run the correction algorithm
based on the ambiguous matrix, and update the ambiguous matrix according
to the corrected dataset, repeating the iteration. This iterative
algorithm is an EM algorithm.</li>
</ul>
<h2 id="contextual-spelling-correction">Contextual spelling
correction</h2>
<ul>
<li>Correction of actual word spelling errors. To address such tasks, it
is necessary to extend the noise channel model: when generating
candidate correction words, the word itself and homophones should be
included. Subsequently, the correct correction word is selected based on
the maximum likelihood of the entire sentence.</li>
</ul>
<h1
id="chapter-6-hidden-markov-models-and-maximum-entropy-models">Chapter
6: Hidden Markov Models and Maximum Entropy Models</h1>
<ul>
<li>Hidden Markov Model is used to solve sequence labeling (sequence
classification problem).</li>
<li>The maximum entropy method is a classification idea that, under
given conditions, the classification should satisfy the minimum
restrictions (maximum entropy) and comply with Ockham's Razor
principle.</li>
<li>The maximum entropy Markov model is an extension of the maximum
entropy method to sequence labeling tasks.</li>
</ul>
<h2 id="markov-chain">Markov Chain</h2>
<ul>
<li><p>Weighted finite automata are an extension of finite automata,
where each transition path is assigned a probability as a weight,
indicating the possibility of transition along that path. A Markov chain
is a special case of a weighted finite state automaton, where the input
sequence uniquely determines the sequence of states the automaton will
pass through. A Markov chain can only assign probabilities to
deterministic sequences.</p></li>
<li><p>We regard the Markov chain as a probabilistic graph model; a
Markov chain is determined by the following components:</p>
<p><span class="math display">\[
Q=q_1 q_2…q_N \\
A=a_{01} a_{02} … a_{n1} … a_{nn} \\
q_0,q_F \\
\]</span></p></li>
<li><p>Respectively</p>
<ul>
<li>State Set</li>
<li>Transfer probability matrix, where a_ij represents the probability
of transitioning from state i to state j <span
class="math inline">\(P(q_j |q_i)\)</span></li>
<li>Special starting and ending states</li>
</ul></li>
<li><p>Probability graphs represent states as points in a graph and
transitions as edges.</p></li>
<li><p>First-order Markov models make a strong assumption about
transitions: the probability of a state is only related to the previous
state:</p>
<p><span class="math display">\[
P(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})
\]</span></p></li>
<li><p>Another representation of Markov chains does not require a
starting and ending state:</p>
<p><span class="math display">\[
\pi = \pi _1,\pi _2 , … , \pi _N \\
QA={q_x,q_y…} \\
\]</span></p></li>
<li><p>Are:</p>
<ul>
<li>The initial probability distribution of the state, the Markov chain
starts from state i with probability <span class="math inline">\(\pi
_i\)</span></li>
<li>Set QA is a subset of Q, representing a legitimate acceptance
state</li>
</ul></li>
<li><p>Therefore, the probability of state 1 as the initial state can be
written as <span class="math inline">\(a_{01}\)</span> or as <span
class="math inline">\(\pi _1\)</span> .</p></li>
</ul>
<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<ul>
<li>When the Markov chain is known, we can use it to calculate the
probability of an observed sequence. However, the observed sequence may
depend on some unobserved hidden variables, and we may be interested in
inferring these hidden variables. The Hidden Markov Model allows us to
consider both observed variables and hidden variables
simultaneously.</li>
<li>As defined previously, the Hidden Markov Model:
<ul>
<li>State set of size N</li>
<li>A: A transfer probability matrix of size N*N</li>
<li>Observation event set of size T</li>
<li>B: The observation likelihood sequence, also known as emission
probability, <span class="math inline">\(b_i (o_t)\)</span> describes
the probability of generating observation <span
class="math inline">\(o_t\)</span></li>
<li>Special initial and final states, without connected observation
quantities</li>
</ul></li>
<li>Similarly, the Hidden Markov Model can also be represented in a way
that does not depend on the initial and final states. The Hidden Markov
Model also makes two assumptions, namely the first-order Markov property
between hidden states and from hidden states to observations.</li>
<li>For Hidden Markov Models, three types of problems need to be
addressed:
<ul>
<li>Likelihood Calculation: Given parameters and an observation
sequence, calculate the likelihood <span
class="math inline">\(P(O|\lambda)\)</span></li>
<li>Decoding: Given the known parameters and the observed sequence, to
find the hidden state sequence</li>
<li>Learning: Solving model parameters given the observed sequence and
the set of hidden states</li>
</ul></li>
</ul>
<h2 id="computing-likelihood-forward-algorithm">Computing Likelihood:
Forward Algorithm</h2>
<ul>
<li><p>For the Markov chain, as it lacks a transition probability matrix
from hidden states to observables, it can be considered as having
observables and hidden states being the same. In the Hidden Markov
Model, it is not possible to directly calculate the likelihood; we need
to know the hidden state sequence.</p></li>
<li><p>Assuming the hidden state sequence is known, the likelihood
calculation is:</p>
<p><span class="math display">\[
P(O│Q) = \prod _{i=1}^T P(o_i |q_i)
\]</span></p></li>
<li><p>According to the first-order Markov property of hidden state
transitions, the prior of the hidden state can be obtained, multiplied
by the likelihood to obtain the joint probability of the observed
sequence and the hidden state sequence:</p>
<p><span class="math display">\[
P(O,Q)=P(O│Q) * P(Q) = \prod _{i=1}^n P(o_i│q_i )  \prod _{i=1}^n P(q_i
|q _{i-1})
\]</span></p></li>
<li><p>For the joint probability integral over the hidden state
sequence, the likelihood of the observed probability can be
obtained:</p>
<p><span class="math display">\[
P(O) = \sum _Q P(O,Q) = \sum _Q P(O|Q)P(Q)
\]</span></p></li>
<li><p>This calculation is equivalent to considering all possible hidden
states and calculating the likelihood for each possibility from the
start to the end of the hidden state sequence. In fact, it is possible
to retain the intermediate states of each calculation to reduce
redundant computation, which is known as dynamic programming. The
dynamic programming algorithm used in the forward calculation of the HMM
observation likelihood is called the forward algorithm:</p>
<ul>
<li><p>Let <span class="math inline">\(\alpha _t (j)\)</span> represent
the probability that the latent variable is in state j at the current
moment after obtaining the first t observations, with λ being the model
parameter:</p>
<p><span class="math display">\[
\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\lambda)
\]</span></p></li>
<li><p>This probability value can be calculated based on the \alpha
value of the previous time step, thus avoiding recalculating from
scratch each time:</p>
<p><span class="math display">\[
\alpha _t (j) = \sum _{i=1}^N \alpha _{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>Initialization <span class="math inline">\(\alpha _1 (j)\)</span>
:</p>
<p><span class="math display">\[
\alpha _1 (j)=a_{0s} b_s (o_1)
\]</span></p></li>
<li><p>Termination state:</p>
<p><span class="math display">\[
P(O│\lambda) = \alpha _T (q_F) = \sum _{i=1}^N \alpha _T (i) \alpha
_{iF}
\]</span></p></li>
</ul></li>
</ul>
<h2 id="decoding-viterbi-algorithm">Decoding: Viterbi Algorithm</h2>
<ul>
<li>The decoding task is to infer the most likely hidden state sequence
based on the observed sequence and parameters. The most naive approach
is to calculate the likelihood of the observed sequence for each
possible hidden state sequence and take the hidden state sequence
corresponding to the maximum likelihood. However, this is just like the
naive calculation of likelihood methods, with a high time complexity.
Similarly, we use dynamic programming to reduce the scale of the
solution. A Viterbi algorithm is used during decoding.
<ul>
<li><p>Let <span class="math inline">\(v_t (j)\)</span> represent the
probability of the current latent state being j, given the known first t
observations (1t) and the known first t-1 latent states (0t-1)</p>
<p><span class="math display">\[
v_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 …
o_t,q_t=j|\lambda)
\]</span></p></li>
<li><p>Among which, we have known the maximum possible hidden state
sequence for the first t time steps, which is also obtained through
dynamic programming:</p>
<p><span class="math display">\[
v_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>To obtain the optimal hidden state sequence, it is also necessary
to record the best choice at each step for easy backtracking to obtain
the path:</p>
<p><span class="math display">\[
{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>Initialization:</p>
<p><span class="math display">\[
v_1 (j) = a_{0j} b_j (o_1) \ \  1 \leq j \leq N \\
{bt}_1 (j) = 0 \\
\]</span></p></li>
<li><p>Termination, separately obtaining the optimal hidden state
sequence (with the starting value for backtracking) and its likelihood
value:</p>
<p><span class="math display">\[
P * = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) * a_{i,F} \\
q_{T*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) * a_{i,F} \\
\]</span></p></li>
</ul></li>
<li>The reason the Viterbi algorithm reduces the time complexity is that
it does not compute all the hidden state paths but utilizes the
condition that the best path at each time step can only extend from the
best path at the previous time step, thereby reducing the number of path
candidates and avoiding many unnecessary path computations. Moreover,
using the result from the previous step also employs the idea of dynamic
programming to reduce the amount of computation.</li>
</ul>
<h2
id="training-hidden-markov-models-forward-backward-algorithm">Training
Hidden Markov Models: Forward-Backward Algorithm</h2>
<ul>
<li><p>Learning problems refer to the situation where the known observed
sequence and the set of hidden states are given, and the model
parameters are to be solved for.</p></li>
<li><p>Forward-backward algorithms, also known as the Baum-Welch
algorithm, are a special case of the EM algorithm used to estimate the
parameters of probabilistic generative models containing latent
variables. The algorithm updates the transition probabilities and
generation probabilities iteratively until convergence. The BW algorithm
uses the ratio of the counts as the latent variables, iteratively
updating the transition probability matrix and the generation
probability matrix together.</p></li>
<li><p>Consider the learning problem of Markov chains. Markov chains can
be regarded as degenerate hidden Markov models, where each hidden
variable generates only observations of itself, with a probability of 0
for generating other observations. Therefore, only the transition
probabilities need to be learned.</p></li>
<li><p>For Markov chains, the transition probabilities can be
statistically estimated through the classical probability model:</p>
<p><span class="math display">\[
a_{ij} = \frac {Count(i \rightarrow j)} {\sum _{q \in Q} Count(i
\rightarrow q)}
\]</span></p></li>
<li><p>We can directly calculate the probability because in a Markov
chain, we know the current state. For the Hidden Markov Model, we cannot
calculate it directly because the hidden state sequence cannot be
determined for a given input. The Badum-Welch algorithm uses two simple
intuitions to solve this problem:</p>
<ul>
<li>Iterative estimation, first assuming a transition probability and a
generation probability, and then deducing better probabilities based on
the assumed probabilities</li>
<li>Estimate the forward probability of a certain observation, and
distribute this probability across different paths, thereby estimating
the probability</li>
</ul></li>
<li><p>Firstly, similar to the forward probability, we define the
backward probability:</p>
<ul>
<li><p>Let <span class="math inline">\(\beta _t (i)\)</span> represent
the probability that the latent variable is in state i at the current
moment after receiving t observations, and <span
class="math inline">\(\lambda\)</span> are the model parameters:</p>
<p><span class="math display">\[
\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\lambda)
\]</span></p></li>
<li><p>Similar to inductive calculations of backward probability:</p>
<p>$$ \beta_t (i) = \sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \beta _{t+1}
(j), \ \ 1≤i≤N,1≤t</p></li>
<li><p>Initialization $$ :</p>
<p><span class="math display">\[
\alpha _1 (j)
\]</span></p></li>
<li><p>Termination state:</p>
<p><span class="math display">\[
\beta _T (i)=\alpha _(i,F)
\]</span></p></li>
</ul></li>
<li><p>Similarly, we hope the classical probability in the Markov chain
can help us estimate the transition probabilities:</p>
<p><span class="math display">\[
P(O│\lambda)=\alpha _t (q_F )=\beta_1 (0)= \sum _{i=1}^N a_{0j} b_j
(o_1) \beta _1 (j)
\]</span></p></li>
<li><p>How to estimate the count values: We convert the count values of
the entire sequence's transition paths into the sum of count values
between time steps, with the probability of a specific transition path
between time steps being:</p>
<p><span class="math display">\[
a_{ij}^{*} = \frac {the expected count of transitions from state i to
state j}{the expected count of transitions from state i to other states}
\]</span></p></li>
<li><p>First, consider the joint probability of all the observed
sequences and this transition path (conditioned on parameters <span
class="math inline">\(P(q_t=i,q_{t+1}=j)\)</span> is omitted):</p>
<p><span class="math display">\[
\lambda
\]</span></p></li>
<li><p>Observe the following probability graph: <img data-src="https://s2.ax1x.com/2019/01/03/FoZWVJ.png"
alt="FoZWVJ.png" /></p></li>
<li><p>It can be seen that this joint probability includes three
parts:</p>
<ul>
<li>T-moment hidden state i forward probability</li>
<li>Backward probability of the hidden state j at T+1 moment</li>
<li>Probability of state transition between T time and T+1 time, as well
as the generation probability of the corresponding observed
quantities</li>
</ul></li>
<li><p>Therefore, there is:</p>
<p><span class="math display">\[
P(q_t=i,q_{t+1}=j,O)
\]</span></p></li>
<li><p>To obtain the joint probability of transition paths from the
joint distribution given the known observation sequence, it is necessary
to calculate the probability of the observation sequence, which can be
obtained through forward probability or backward probability:</p>
<p><span class="math display">\[
P(q_t=i,q_{t+1}=j,O)=\alpha _t (i) a_{ij} b_j (o_{t+1} ) \beta _{t+1}
(j)
\]</span></p></li>
<li><p>Ultimately obtained</p>
<p><span class="math display">\[
P(O)=\alpha _t (N)=\beta _T (1) = \sum _{j=1}^N \alpha _t (j) \beta_t
(j)
\]</span></p></li>
<li><p>The sum of all time steps yields the expected count of
transitions from state i to state j, thereby further obtaining an
estimate of the transition probability:</p>
<p><span class="math display">\[
ξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \frac {(\alpha _t (i) a_{ij} b_j
(o_{t+1} ) \beta_{t+1} (j))}{(\alpha _t (N))}
\]</span></p></li>
<li><p>Similarly, we also hope to obtain an estimate of the generation
probability:</p>
<p><span class="math display">\[
a_{ij}^{*} = \frac {\sum _{t=1}^{T-1} ξ_t (i,j)}{\sum _{t=1}^{T-1} \sum
_{j=1}^{N-1} ξ_t (i,j)}
\]</span></p></li>
<li><p>Similarly, the probability of being in the hidden state j at time
t is obtained by first calculating the joint distribution and then the
conditional distribution:</p>
<p><span class="math display">\[
b_{j}^{*} (v_k) = \frac {the expected count of observations of symbol
v_k in state j}{the expected count of observations of all symbols in
state j}
\]</span></p></li>
<li><p>The joint probability includes two parts, namely the forward
probability and the backward probability of being in state j at time t,
thus:</p>
<p><span class="math display">\[
γ_t (j)=P(q_t=j│O) = \frac {P(q_t=j,O)}{P(O)}
\]</span></p></li>
<li><p>Similarly, by summing over all time steps, an estimate of the
generation probability is obtained:</p>
<p><span class="math display">\[
γ_t (j) = \frac {\alpha _t (j) \beta_t (j)}{\alpha _t (N)}
\]</span></p></li>
<li><p>These two formulas are calculated under the condition of known
forward and backward probabilities, and introduce the intermediate
variables (latent variables) (ξ,γ). The motivation for introducing
latent variables is to transform the ratio of the expected counts of the
estimates of a and b into a ratio of probabilities, and these two latent
variables can be represented by a and b. Then, the transition
probabilities and generation probabilities are calculated from the
latent variables, thus forming an iterative loop, which can be solved
using the EM algorithm.</p>
<p><span class="math display">\[
(\alpha,\beta)
\]</span></p></li>
<li><p>E-step:</p>
<p><span class="math display">\[
a,b→\alpha,\beta→ξ,γ→a,b
\]</span></p></li>
<li><p>M-step (What is the goal of maximization):</p>
<p><span class="math display">\[
γ_t (j) = (\alpha _t (j) \beta_t (j))/(\alpha _t (N)) ξ_t (i,j) \\
= (\alpha _t (i) a_{ij} b_j (o_{t+1} ) \beta_{t+1} (j))/(\alpha _t (N))
\\
\]</span></p></li>
<li><p>Iterative calculations need to be recalculated:</p>
<p><span class="math display">\[
a _{ij} = (\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\sum _{t=1}^{T-1} \sum
_{j=1}^{N-1}   ξ_t (i,j)  ) \\
b ̂_j(v_k) = (\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\sum
_{t=1}^T   γ_t (j) ) \\
\]</span></p></li>
<li><p>The initial state of the iteration is important for the EM
algorithm, often designed by introducing some external
information.</p></li>
</ul>
<h2 id="maximum-entropy-model-background">Maximum Entropy Model:
Background</h2>
<ul>
<li><p>Another widely known form of the maximum entropy model is
multi-logistic regression (Softmax?).</p></li>
<li><p>The maximum entropy model solves classification problems. As a
probabilistic classifier, the maximum entropy model can calculate the
probability of each sample belonging to every category based on the
features of the samples, and then perform classification.</p></li>
<li><p>The maximum entropy model belongs to the exponential family
(log-linear) classifier, which calculates classification probabilities
by taking the exponential of a linear combination of features:</p>
<p><span class="math display">\[
\alpha _t (j) = \sum _{i=1}^N   \alpha_{t-1} (i) a_ij b_j (o_t) \\
\beta_t (i) = \sum _{j=1}^N   a_ij b_j (o_{t+1} ) \beta_{t+1} (j)  \\
\]</span></p></li>
<li><p>Z is a normalization coefficient that makes the sum of the
generated probabilities equal to 1.</p></li>
</ul>
<h2 id="maximum-entropy-modeling">Maximum Entropy Modeling</h2>
<ul>
<li><p>Extending binary logistic regression to the multi-class problem
results in:</p>
<p><span class="math display">\[
p(c│x)=\frac 1Z exp⁡(\sum _i   weight_i feature_i)
\]</span></p></li>
<li><p>Features in speech and language processing are typically binary
(whether the feature is present), thus indicator functions are used to
represent features</p>
<p><span class="math display">\[
P(c│x) = \frac {exp⁡(\sum _(i=0)^N   w_ci f_i) } {\sum _{c^{*} in
C}   exp⁡(\sum _{i=0}^N   w_{c^{*} i} f_i)  }
\]</span></p></li>
<li><p>Noticed that in this model, each class has its independent linear
weight w_c. Compared to hard distribution, the maximum entropy model can
provide the probability of being assigned to each class, thus allowing
the calculation of the classification probability at each moment, and
then the overall classification probability can be obtained, leading to
the global optimal classification result. Noticed that unlike support
vector machines and other models, the maximum entropy model cannot
utilize the combination of features and must manually construct
combinations as new features.</p></li>
<li><p>通常使用加上了正则化的最大似然作为优化的目标函数：</p>
<p><span class="math display">\[
P(c│x) = \frac {exp⁡(\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\sum _{c^{*}
\in C}   exp⁡(\sum _{i=0}^N   w_{c^{*} i} f_i (c^{*},x))  }
\]</span></p></li>
<li><p>This regularization is equivalent to adding a zero-mean Gaussian
prior to the probability distribution of the weights, where the weights
deviate more from the mean, i.e., the larger the weights, the lower
their probability.</p></li>
<li><p>Why multi-class logistic regression is a maximum entropy model:
The maximum entropy model guarantees that the un 约束 part of the
classification should be equally probable under given constraints, for
example, under two constraints:</p>
<p><span class="math display">\[
w ̂={argmax} _w \sum _i   \log P(y^{(i)}│x^{(i) } ) - \alpha \sum
_{j=1}^N w_j^2
\]</span></p></li>
<li><p>Then, if these two constraints are satisfied, the probability
results allocated by the maximum entropy model are:</p>
<p><span class="math display">\[
P(NN)+P(JJ)+P(NNS)+P(VB)=1 \\
P(t_i=NN or t_i=NNS)=8/10 \\
\]</span></p></li>
<li><p>In the paper "The Equivalence of Logistic Regression and Maximum
Entropy Models," it is proven that under the constraint of the balance
condition in the generalized linear regression model, the nonlinear
activation function that satisfies the maximum entropy distribution is
the sigmoid, i.e., logistic regression.</p></li>
</ul>
<h2 id="maximum-entropy-markov-model">Maximum Entropy Markov Model</h2>
<ul>
<li><p>The maximum entropy model can only classify single observations,
while the maximum entropy Markov model can extend it to the problem of
sequence classification.</p></li>
<li><p>Where does the Maximum Entropy Markov model excel over the Hidden
Markov Model? The Hidden Markov Model depends on transition
probabilities and generation probabilities for the classification of
each observation. If we want to introduce external knowledge during the
labeling process, we need to encode this external knowledge into these
two types of probabilities, which is inconvenient. The Maximum Entropy
Markov model can introduce external knowledge more simply.</p></li>
<li><p>In the Hidden Markov Model, we optimize the likelihood and
multiply by the prior to estimate the posterior:</p>
<p><span class="math display">\[
p(NN)=4/10  \\
p(JJ)=1/10  \\
p(NNS)=4/10  \\
p(VB)=1/10 \\
\]</span></p></li>
<li><p>In the maximum entropy hidden Markov model, we directly compute
the posterior. Because we directly train the model for classification,
that is, the maximum entropy Markov model is a type of discriminative
model rather than a generative model:</p>
<p><span class="math display">\[
T ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )
\]</span></p></li>
<li><p>Therefore, in the maximum entropy hidden Markov model, there is
no separate modeling of likelihood and prior, but rather the posterior
is estimated through a single probability model. The difference between
the two is shown in the figure below: <img data-src="https://s2.ax1x.com/2019/01/03/FoZgrF.png"
alt="FoZgrF.png" /></p></li>
<li><p>Additional features can be more dependent and flexible in the
Maximum Entropy Markov Model, as shown in the following figure: <img data-src="https://s2.ax1x.com/2019/01/03/FoZcKU.png"
alt="FoZcKU.png" /></p></li>
<li><p>Express this difference in formula:</p>
<p><span class="math display">\[
T ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1})
\]</span></p></li>
<li><p>When estimating the single transition probability (from state q*
to state q, producing the observation o), we use the following maximum
entropy model:</p>
<p><span class="math display">\[
HMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\
MEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\
\]</span></p></li>
</ul>
<h2 id="decoding-inference-of-the-maximum-entropy-markov-model">Decoding
(Inference) of the Maximum Entropy Markov Model</h2>
<ul>
<li><p>MEMM also uses the Viterbi algorithm for decoding</p></li>
<li><p>The general framework for decoding using the Viterbi algorithm
is:</p>
<p><span class="math display">\[
P(q│q^{*},o)=\frac{1}{Z(o,q^{*})} exp⁡(\sum _i   w_i f_i (o,q))
\]</span></p></li>
<li><p>In the HMM model, this framework is specifically realized as:</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j)
\]</span></p></li>
<li><p>In MEMM, replace the likelihood and prior directly with the
posterior:</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t)
\]</span></p></li>
</ul>
<h2 id="training-of-maximum-entropy-markov-models">Training of Maximum
Entropy Markov Models</h2>
<ul>
<li>MEMM as an extension of the maximum entropy model employs the same
supervised algorithm for training. If there are missing label sequences
in the training data, semi-supervised learning can also be performed
using the EM algorithm.</li>
</ul>
<h1 id="chapter-12-the-formal-grammar-of-english">Chapter 12: The Formal
Grammar of English</h1>
<h2 id="compositionality">Compositionality</h2>
<ul>
<li>How are words in English composed into a phrase?</li>
<li>In other words, how do we determine that some word combinations form
a part? One possibility is that these combinations can all appear in
similar syntactic environments, for example, noun phrases can all appear
before a verb. Another possibility comes from the prepositional and
postpositional structures, for example, the prepositional phrase "on
September seventeenth" can be placed at the beginning, middle, or end of
a sentence, but the individual components of this phrase cannot be split
and placed in different positions in the sentence, so we judge that "on
September seventeenth" these three word combinations form a phrase.</li>
</ul>
<h2 id="context-free-grammar">Context-free grammar</h2>
<ul>
<li><p>Context-free grammar, abbreviated as CFG, also known as phrase
structure grammar, has a formalization method equivalent to the
Backus-Naur form. A context-free grammar consists of two parts: rules or
production, and a vocabulary.</p></li>
<li><p>For example, in describing noun phrases with context-free
grammar, one way is that a noun phrase can be composed of a proper noun,
or it can be composed of a determiner plus a nominal component, where
the nominal component can consist of one or more nouns. The rules of
this CFG are:</p>
<ul>
<li>NP→Determiner Nominal</li>
<li>NP→ProperNoun</li>
<li>Nominal→Noun|Noun Nominal</li>
</ul></li>
<li><p>CFG can be hierarchically nested, thus the above rules can be
combined with the rules below that represent lexical facts (vocabulary
list):</p>
<ul>
<li>Det→a</li>
<li>Det→the</li>
<li>Noun→flight</li>
</ul></li>
<li><p>Symbols are divided into two categories:</p>
<ul>
<li>Ultimate Symbol: A symbol corresponding to a word in reality; the
lexicon is a collection of rules for introducing ultimate symbols</li>
<li>Non 终极符号: Clustering or generalizing symbols representing
ultimate symbols</li>
</ul></li>
<li><p>In each rule, the right side of the arrow contains one or more
terminal symbols and non-terminal symbols, and the left side of the
arrow is a non-terminal symbol associated with each word and its
category (part of speech).</p></li>
<li><p>CFG can be regarded as a mechanism for generating sentences as
well as a mechanism for assigning structure to a sentence.</p></li>
<li><p>For example, taking the CFG mentioned earlier, an NP (noun
phrase) symbol string can be generated step by step:</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t)
\]</span></p></li>
<li><p>A flight is a derivation of NP, which is generally represented by
a parse tree: <img data-src="https://s2.ax1x.com/2019/01/03/FoZ5P1.png"
alt="FoZ5P1.png" /> A CFG defines a formal language, which is a set of
symbol strings. If a sentence derived by a grammar is within the formal
language defined by that grammar, the sentence is syntactically correct.
Using formal languages to simulate the grammar of natural languages is
known as generative grammar.</p></li>
<li><p>Formal definition of context-free grammar:</p>
<ul>
<li>Set of non-terminating symbols (or variables)</li>
<li>Sigma: Set of terminal symbols, disjoint from N</li>
<li>Rule set or set of productions</li>
<li>S: Specified start symbol</li>
</ul></li>
<li><p>Some conventions defined:</p>
<ul>
<li>Capital letters: Represent non-terminating symbols</li>
<li>S: Start Symbol</li>
<li>Lowercase Greek letters: a sequence of symbols extracted from the
conjunction of non-terminal and terminal symbols</li>
<li>Lowercase Roman letters: end-of-sequence symbol string</li>
</ul></li>
<li><p>Direct derivation definition: Formula to be supplemented</p></li>
<li><p>Exportation is a generalization of direct exportation.
Subsequently, we can formally define the language L generated by the
grammar G as a set of strings composed of terminal symbols, which can be
exported from the specified starting symbol S through the grammar G:
Formula to be supplemented</p></li>
<li><p>Mapping a sequence of words to its corresponding parse tree is
called syntactic parsing.</p></li>
</ul>
<h2 id="some-grammatical-rules-of-english">Some grammatical rules of
English</h2>
<ul>
<li>Four most common sentence structures in English:
<ul>
<li>Declarative structure: a noun phrase subject followed by a verb
phrase</li>
<li>Imperative structure: Typically begins with a verb phrase and lacks
a subject</li>
<li>Yes-no interrogative structure: commonly used for asking questions,
and begins with an auxiliary verb, followed by a subject NP, and then a
VP</li>
<li>Wh interrogative structure: contains a wh phrase element</li>
</ul></li>
<li>In the previous description, the beginning symbol was used to
generate an entire sentence independently, but S can also appear on the
right side of grammatical generation rules, embedded within a larger
sentence. Such an S is called a clause, which has a complete semantics.
Having complete semantics means that this S, in the overall syntactic
parse tree of the sentence, has the main verb in its subtree with all
the required arguments.</li>
</ul>
<h2 id="noun-phrase">Noun phrase</h2>
<ul>
<li>Determiner Det: Noun phrases can begin with some simple lexical
determiners, such as a, the, this, those, any, some, etc., and the
position of determiners can also be replaced by more complex
expressions, such as possessives. Such expressions can be recursively
defined, for example, possessives plus noun phrases can constitute
determiners of larger noun phrases. No determiners are needed before
plural nouns or mass nouns.</li>
<li>Nominal: Comprising some modifiers before or after nouns</li>
<li>Before nouns, after determiners: Some special word classes can
appear before nouns and after determiners, including cardinal numbers
Card, ordinal numbers Ord, and quantity modifiers Quant.</li>
<li>Adjective Phrase AP: An adverb can appear before an adjective
phrase</li>
<li>The rules for the attributive modifiers of noun phrases can be
regularized as follows (items in parentheses are optional):</li>
<li>Nominal</li>
<li>Post-modifiers mainly include three types:
<ul>
<li>Prepositional phrase PP: Nominal -&gt; Nominal PP(PP)(PP)</li>
<li>Non-restrictive relative clause: Gerund VP, Gerund VP -&gt; GerundV
NP | GerundV PP | GerundV | GerundV NP PP</li>
<li>Relative clause: Clause starting with a relative pronoun Nominal
-&gt;Nominal RelCaluse;RelCaluse -&gt; (who|that) VP</li>
</ul></li>
</ul>
<h2 id="consistency-relationship">Consistency relationship</h2>
<ul>
<li><p>When a verb has a noun as its subject, the phenomenon of
agreement occurs; any sentence where the subject and its verb do not
agree is ungrammatical, for example, the third-person singular verb
without the -s ending. A set of rules can be used to expand the original
grammar, making it capable of handling agreement. For example, the rule
for yes-no questions is</p>
<p><span class="math display">\[
NP→Det Nominal→Det Noun→a flight
\]</span></p></li>
<li><p>Two rules of the following form can be substituted:</p>
<p><span class="math display">\[
S \rightarrow Aux \ NP \ VP
\]</span></p></li>
<li><p>Specify the auxiliary verb forms for the third person singular
and non-third person singular separately. Such a method would lead to an
increase in grammatical scale.</p></li>
</ul>
<h2 id="verb-phrases-and-subcategorization">Verb phrases and
subcategorization</h2>
<ul>
<li>Verb phrases include combinations of verbs and other components,
such as NP and PP, as well as combinations of both. The entire embedded
sentence can also follow the verb, becoming a complement of the
sentence.</li>
<li>Another potential constituent of the verb phrase is another verb
phrase.</li>
<li>The verb can also be followed by a particle, which is similar to
"借以," but when combined with the verb, it forms a phrasal verb that is
inseparable from the verb.</li>
<li>Reclassification refers to subcategorization. Traditional grammar
subcategorizes verbs into transitive verbs and intransitive verbs, while
modern grammar has differentiated verbs into 100 subcategories.
Discussing the relationship between verbs and possible components
involves viewing verbs as predicates and components as the arguments of
this predicate.</li>
<li>For the relationship between verbs and their complements, we can
express the consistency features using context-free grammar and it is
necessary to differentiate the various subclasses of verbs.</li>
</ul>
<h2 id="auxiliary-verb">Auxiliary verb</h2>
<ul>
<li>Auxiliaries are a subclass of verbs with special syntactic
constraints. Auxiliaries include modal verbs, perfect auxiliary verbs,
progressive auxiliary verbs, and passive auxiliary verbs. Each auxiliary
imposes a constraint on the form of the verb that follows it, and they
must be combined in a certain order.</li>
<li>Four auxiliary verbs categorize the VP subcategory, with the central
verbs of the VP being a bare verb, a past participle form, a present
participle form, and a past participle form, respectively.</li>
<li>A sentence can use multiple auxiliary verbs, but they should be
arranged in the order of modal auxiliary verbs, perfect auxiliary verbs,
progressive auxiliary verbs, and passive auxiliary verbs.</li>
</ul>
<h2 id="tree-diagram-database">Tree Diagram Database</h2>
<ul>
<li><p>Context-free grammar can analyze a sentence into a syntactic
parse tree. If all sentences in a corpus are represented in the form of
a syntactic parse tree, such syntactically annotated corpus is called a
treebank.</p></li>
<li><p>The sentences in the treebank database implicitly constitute a
grammar of a language, from which we can extract CFG rules for each
syntactic parse tree. The CFG rules extracted from the Penn Treebank are
highly flattened, resulting in a large number of rules and long
rules.</p></li>
<li><p>A special expression is required for searching in a treebank,
which can represent constraints on nodes and connections to search for
specific patterns. For example, tgrep or TGrep2.</p></li>
<li><p>A pattern in tgrep, TGrep2 consists of a description of a node,
and a node description can be used to return a subtree rooted at this
node.</p></li>
<li><p>One can name a class of patterns using double slashes:</p>
<p><span class="math display">\[
S \rightarrow 3sgAux \ 3sgNP \ VP \\
S \rightarrow Non3sgAux \ Non3sgNP \ VP \\
\]</span></p></li>
<li><p>The benefits of the Tgrep/Tgrep2 pattern lie in its ability to
describe connected information. The less than symbol represents direct
dominance, the much less than symbol represents dominance, and the
decimal point represents linear order. This description of connections
is reflected in the relationships within the parse tree as follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZ2b4.png"
alt="FoZ2b4.png" /></p></li>
</ul>
<h2 id="central-word-and-central-word-search">Central word and central
word search</h2>
<ul>
<li>Syntactic components can be associated with a lexical center word.
In a simple lexical center word model, each context-free rule is
associated with a center word, which is passed to the parse tree, so
that each non-terminal symbol in the parse tree is labeled with a single
word, which is the center word of that non-terminal symbol. An example
is as follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZfa9.png"
alt="FoZfa9.png" /></li>
<li>To generate such a tree, each CFG rule must be expanded to recognize
a right-hand constituent as the center word child node. The center word
of a node is set to the center word of its child center words.</li>
<li>Another approach is to complete the search for the center word
through a computational system. In this method, the search for the
specified sentence is based on the context of the tree, thereby
dynamically identifying the center word. Once a sentence is parsed, the
tree is traversed and each node is decorated with the appropriate center
word.</li>
</ul>
<h2 id="grammar-equivalence-and-patterns">Grammar Equivalence and
Patterns</h2>
<ul>
<li>Grammar equivalence includes two types: strong equivalence, which
means that two grammars generate the same set of symbol strings and
assign the same phrase structure to each sentence; weak equivalence,
which means that two grammars generate the same set of symbol strings
but do not assign the same phrase structure to each sentence.</li>
<li>All grammars use a single paradigm, in which each production rule
employs a specific form. For example, a context-free grammar with five
senses is sigma-free, and if each production rule's form is A-&gt;BC or
A-&gt;a, it indicates that this context-free grammar conforms to the
Chomsky paradigm, abbreviated as CNF. All grammars in the Chomsky
paradigm have a binary tree structure. Any context-free grammar can be
transformed into a weakly equivalent Chomsky paradigm grammar.</li>
<li>Using a parse tree in binary tree form can produce a smaller
grammar. Rules of the form A-&gt;A B are called Chomsky
and-conjunctions.</li>
</ul>
<h2 id="finite-state-grammar-and-context-free-grammar">Finite State
Grammar and Context-Free Grammar</h2>
<ul>
<li>Complex grammatical models must represent compositionality, hence
they are not suitable for describing grammar using finite state
models.</li>
<li>When the expansion of a non-terminal symbol also includes this
non-terminal symbol, a grammatical recursion problem arises.</li>
<li>For example, using regular expressions to describe nominal-centered
noun phrases: (Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*</li>
<li>To complete this regular expression, it is only necessary to expand
PP in sequence, resulting in (P NP)*. This then leads to the ground
problem, because NP now appears, and NP appears in the regular
expression for NP.</li>
<li>A context-free grammar can be generated by a finite automaton if and
only if there exists a context-free grammar for the language L with no
central self-embedding recursion.</li>
</ul>
<h2 id="dependency-grammar">Dependency Grammar</h2>
<ul>
<li>Dependency grammar contrasts with context-free grammar, where the
syntactic structure is entirely described by the semantic or syntactic
relationships between words and between words. An example is as follows:
<img data-src="https://s2.ax1x.com/2019/01/03/FoZOVH.png"
alt="FoZOVH.png" /></li>
<li>There are no non-terminal symbols or phrase nodes; the connections
in the tree only link two words. The connections, or dependency
relations, represent grammatical functions or general semantic
relationships, such as syntactic subjects, direct objects, indirect
objects, temporal adverbials, and so on.</li>
<li>Dependency grammar has a strong predictive analytical ability and
performs better in handling languages with relatively free word
order.</li>
</ul>
<h1 id="chapter-13-analysis-based-on-context-free-grammar">Chapter 13:
Analysis Based on Context-Free Grammar</h1>
<h2 id="analysis-is-searching">Analysis is searching</h2>
<ul>
<li>In syntactic parsing, parsing can be seen as a search through all
possible parsing tree spaces of a sentence to discover the correct
parsing tree.</li>
<li>For a certain sentence (input symbol string), the goal of the
parsing search is to discover all parsing trees rooted at the initial
symbol S that exactly cover the entire input symbol string. The
constraints on the search algorithm come from two aspects:
<ul>
<li>Constraints from the data, i.e., the input sentence itself, should
result in the leaves of the parsed tree being all the words of the
original sentence.</li>
<li>From grammatical constraints, the parsed tree that is searched
should have a root, namely the initial symbol S</li>
</ul></li>
<li>Top-down, goal-directed search and bottom-up, data-directed search
strategies were generated based on these two constraints.</li>
<li>For top-down search, starting from the root, we continuously
generate all possible child nodes at each subsequent level, searching
through every possibility at each level, as shown in the figure (for the
sentence "book that flight"): <img data-src="https://s2.ax1x.com/2019/01/03/FoZh5R.png" alt="FoZh5R.png" /></li>
<li>For bottom-up parsing, the analysis starts from the input words,
using the grammatical rules each time to attempt to construct an
analysis tree from the bottom up. If the analysis tree successfully
constructs a tree rooted at the initial symbol S and covers the entire
input, then the parsing is successful. First, each word is connected to
its corresponding word class through the lexicon; if a word has more
than one word class, all possibilities need to be considered. Unlike
top-down parsing, when moving to the next level, bottom-up parsing needs
to consider whether the analyzed component matches the right-hand side
of some rule, whereas top-down parsing matches the left-hand side. If a
rule cannot be matched during the process, this branch is removed from
the search space, as shown in the figure below: <img data-src="https://s2.ax1x.com/2019/01/03/FoZI8x.png" alt="FoZI8x.png" /></li>
<li>Both compared:
<ul>
<li>Top-down search starts from S, therefore, it will not search those
subtrees that cannot be found in the tree rooted at S, while bottom-up
search will generate many impossible search trees</li>
<li>Correspondingly, top-down search is wasted on trees that cannot
produce the input word sequence</li>
<li>In summary, we need to combine top-down and bottom-up
approaches</li>
</ul></li>
</ul>
<h2 id="ambiguity">Ambiguity</h2>
<ul>
<li><p>A problem that needs to be addressed in syntactic analysis is
structural ambiguity, that is, grammar may yield multiple parsing
results for a single sentence.</p></li>
<li><p>The most common two ambiguities: attachment ambiguity and
coordination conjunction ambiguity.</p></li>
<li><p>If a particular element can attach to more than one position in
the parse tree, the sentence will exhibit attachment ambiguity. For
example, in the sentence "We saw the Eiffel Tower flying to Paris,"
"flying to Paris" can modify either the Eiffel Tower or "We."</p></li>
<li><p>In coordination ambiguity, there exist different phrases
connected by conjunctions such as "and." For example, "old men and
women" can refer to elderly men and elderly women, or elderly men and
ordinary women, i.e., whether the term "old" is simultaneously assigned
to both "men" and "women."</p></li>
<li><p>The above two ambiguities can be combined and nested to form more
complex ambiguities. If we do not resolve the ambiguities but simply
return all possibilities, leaving it to the user or manual judgment, the
number of possibilities may increase exponentially as the analysis of
sentence structure becomes more complex or as the number of analysis
rules increases. Specifically, the growth of the analysis of possible
sentences is similar to the arithmetic expression insertion of
parentheses problem, growing exponentially according to Catalan
numbers:</p>
<p><span class="math display">\[
/NNS?/    NN|NNS
\]</span></p></li>
<li><p>Two methods exist to escape this exponential explosion:</p>
<ul>
<li>Dynamic programming, studying the regularity of the search space, so
that common parts are derived only once, reducing the overhead related
to ambiguity</li>
<li>Employing exploratory methods to improve the search strategy of the
profiler</li>
</ul></li>
<li><p>Utilizing planned and backtracking search algorithms such as
depth-first search or breadth-first search is a common approach in
searching complex search spaces, however, the pervasive ambiguity in
complex grammatical spaces makes these search algorithms inefficient, as
there are many redundant search processes.</p></li>
</ul>
<h2 id="dynamic-programming-analysis-method">Dynamic Programming
Analysis Method</h2>
<ul>
<li>In dynamic programming, we maintain a table, systematically filling
in the solutions for subproblems, and using the already stored solutions
for subproblems to solve larger subproblems without having to
recalculate from scratch.</li>
<li>In the analysis, such a table is used to store the subtrees of
various parts of the input, which are stored in the table upon discovery
for later retrieval, thereby solving the problem of repeated analysis
(only subtrees need to be searched without the need for re-analysis) and
ambiguity issues (the analysis table implicitly stores all possible
analysis results).</li>
<li>The main three dynamic programming parsing methods are the CKY
algorithm, the Earley algorithm, and the table parsing algorithm.</li>
</ul>
<h3 id="cky-parsing">CKY Parsing</h3>
<ul>
<li>CKY parsing requires that the grammar must satisfy the Chomsky
paradigm, i.e., the right-hand side of a generating rule must either be
two non-terminals or one terminal symbol. If it is not in Chomsky 范式,
then a general CFG needs to be transformed into CNF:
<ul>
<li>Right-hand side has both terminal symbols and non-terminal symbols:
Create a separate non-terminal symbol for the right-hand terminal
symbol, for example: INF-VP → to VP, change to INF-VP → TO VP and TO →
to</li>
<li>There is only one non-terminal symbol on the right: This
non-terminal symbol is called a unit product, which will eventually
generate non-unit products, and the unit products are replaced by the
rules of the non-unit products generated in the end</li>
<li>Right side has more than 2 symbols: introducing new non-terminal
symbols to decompose the rules</li>
<li>Lexical rules remain unchanged, but new lexical rules may be
generated during the transformation process</li>
</ul></li>
<li>After all the rules are converted to CNF, the non-terminal symbols
in the table have two child nodes during parsing, and each entry in the
table represents an interval in the input. For example, for an entry
such as [0,3], it can be split into two parts, where one part is [0,2],
and the other part is [2,3]. The former is on the left side of [0,3],
and the latter is directly below [0,3], as shown in the following
figure: <img data-src="https://s2.ax1x.com/2019/01/03/FoZo26.png"
alt="FoZo26.png" /></li>
<li>The next step is how to fill out the table, and we analyze it
through a bottom-up approach. For each entry [i, j], the table cells
within the input interval from i to j contribute to this entry value,
that is, the cells to the left and below the entry [i, j]. The CKY
pseudo-algorithm diagram in the following table describes this process:
<img data-src="https://s2.ax1x.com/2019/01/03/FoZjIA.png"
alt="FoZjIA.png" /></li>
<li>Outer loop iterates over columns from left to right, inner loop
iterates over rows from bottom to top, and the innermost loop traverses
all possible binary substrings of [i, j]. The table stores the set of
non-terminal symbols that can represent the symbol string in the
interval [i, j]. Since it is a set, there will be no repeated
non-terminal symbols.</li>
<li>Now that we have completed the recognition task, the next step is to
analyze. Analysis involves finding a non-terminal symbol as the starting
symbol S that corresponds to the entire sentence within the interval [0,
N]. Firstly, we need to make two modifications to the algorithm:
<ul>
<li>The stored information in the table is not only non-terminal symbols
but also their corresponding pointers, which point to the table entry
that generates the non-terminal symbol</li>
<li>Permit different versions of the same non-terminal symbol to exist
within an entry</li>
</ul></li>
<li>After making these changes, the table contains all possible parsing
information for a given input. We can choose any non-terminal symbol
from the [0,N] entries as the starting symbol S, and then iteratively
extract the parsing information according to the pointer.</li>
<li>Of course, returning all possible decompositions would encounter an
exponential explosion problem, therefore, we apply the Viterbi algorithm
to the complete table, calculate the decomposition with the highest
probability, and return this decomposition result.</li>
</ul>
<h3 id="early-algorithms">Early Algorithms</h3>
<ul>
<li>Compared to CKY's bottom-up parsing, the Early algorithm employs a
top-down parsing approach and uses a one-dimensional table to save the
state, with each state containing three types of information:
<ul>
<li>Corresponding subtree for a single grammatical rule</li>
<li>Completion state of subtrees</li>
<li>Subtrees correspond to positions in the input</li>
</ul></li>
<li>Algorithm flowchart as follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZHKO.png" alt="FoZHKO.png" /></li>
<li>Algorithms have three operations on states:
<ul>
<li>Prediction: Create a new state to represent the top-down prediction
generated during the analysis process. When the state to be analyzed is
neither a terminal symbol nor a category of lexical types, a new state
is created for each different expansion of this non-terminal
symbol.</li>
<li>Scan: When the state to be analyzed is a lexical category, check the
input symbol string and add the state corresponding to the predicted
lexical category to the syntax diagram.</li>
<li>Completion: When all the state analyses on the right are completed,
the completion operation searches for the grammatical category at this
position in the input, discovers, and advances all the states created
earlier.</li>
</ul></li>
</ul>
<h3 id="table-analysis">Table analysis</h3>
<ul>
<li>Table decomposition allows for the dynamic determination of the
order of table processing, where the algorithm dynamically removes an
edge from the graph according to a plan, and the order of elements in
the plan is determined by rules. <img data-src="https://s2.ax1x.com/2019/01/03/FoZTxK.png" alt="FoZTxK.png" /></li>
</ul>
<hr />
<ul>
<li>Sometimes we only need to input partial syntactic analysis
information of a sentence</li>
<li>The task of partial analysis can be accomplished by cascading finite
state automata, which will produce a more "flat" analysis tree than the
previously mentioned methods.</li>
<li>Another effective method of partial parsing is segmentation. Using
the most widely covered grammar for part-of-speech tagging of sentences,
dividing them into sub-blocks with main part-of-speech tagging
information and no recursive structure, where the sub-blocks do not
overlap, is segmentation.</li>
<li>We enclose each block with square brackets, and some words may not
be enclosed, belonging to the blocks outside.</li>
<li>The most important aspect of partitioning is that the basic
partitioning cannot recursively contain the same type of
components.</li>
</ul>
<h3 id="rule-based-finite-state-partitioning">Rule-based Finite State
Partitioning</h3>
<ul>
<li>Utilizing a finite state method for segmentation requires manually
constructing rules for specific purposes, then finding the longest
matching segment from left to right, and proceeding to segment in order
from there. This is a greedy segmentation process that does not
guarantee the globally optimal solution.</li>
<li>The main limitation of these partitioning rules is that they cannot
contain recursion.</li>
<li>The advantage of using finite state segmentation lies in the ability
to utilize the output of the previous transducer as input to form a
cascade, in partial parsing, this method can effectively approximate the
true context-free parser.</li>
</ul>
<h3 id="block-based-machine-learning">Block-based Machine Learning</h3>
<ul>
<li>Chunking can be regarded as a sequence classification task, where
each position is classified as 1 (chunk) or 0 (not chunked). Machine
learning methods used for training sequence classifiers can all be
applied to chunking.</li>
<li>A highly effective method is to treat segmentation as a sequence
labeling task similar to part-of-speech tagging, encoding both
segmentation information and the labeling information of each block with
a small set of labeling symbols. This method is called IOB labeling,
with B representing the beginning of a block, I indicating within a
block, and O indicating outside a block. Both B and I are followed by
suffixes, representing the syntactic information of the block.</li>
<li>Machine learning requires training data, and it is difficult to
obtain segmented labeled data. One method is to use existing treebank
resources, such as the Penn Treebank.</li>
</ul>
<h3 id="evaluation-block-system">Evaluation Block System</h3>
<ul>
<li>Accuracy: The number of correctly segmented blocks given by the
model / The total number of blocks given by the model</li>
<li>Recall rate: Number of correctly segmented blocks given by the model
/ Total number of correctly segmented blocks in the text</li>
<li>F1 score: Harmonic mean of accuracy and recall</li>
</ul>
<h1 id="chapter-14-statistical-analysis">Chapter 14: Statistical
Analysis</h1>
<h2 id="probabilistic-context-free-grammar">Probabilistic Context-Free
Grammar</h2>
<ul>
<li>Probability Context-Free Grammar (PCFG) is a simple extension of
context-free grammar, also known as stochastic context-free grammar.
PCFG makes a slight change in definition:
<ul>
<li>N: Non-terminal symbol set</li>
<li>Σ: Set of termination symbols</li>
<li>R: Rule set, similar to the context-free grammar, but with an
additional probability p, representing the conditional probability of
executing a particular rule <span
class="math inline">\(C(n)=\frac{1}{1+n} C_{2n}^n\)</span></li>
<li>A specified starting symbol</li>
</ul></li>
<li>When the sum of the probabilities of all sentences in a language is
1, we say that the PCFG is consistent. Some recursive rules can lead to
an inconsistent PCFG.</li>
</ul>
<h2 id="pcfg-for-disambiguation">PCFG for Disambiguation</h2>
<ul>
<li>The probability of a specific parsing for a given sentence is the
product of all rule probabilities, which is both the probability of the
parsing and the joint probability of the parsing and the sentence. Thus,
for sentences with parsing ambiguities, the probabilities of different
parses are different, and ambiguity can be resolved by choosing the
parse with a higher probability.</li>
</ul>
<h2 id="pcfg-for-language-modeling">PCFG for Language Modeling</h2>
<ul>
<li>PCFG assigns a probability to a sentence (i.e., the probability of
parsing), thus it can be used for language modeling. Compared to n-gram
grammar models, PCFG considers the entire sentence when calculating the
conditional probability of generating each word, resulting in better
performance. For ambiguous sentences, the probability is the sum of the
probabilities of all possible parses.</li>
</ul>
<h2 id="probability-cky-parsing-of-pcfg">Probability CKY Parsing of
PCFG</h2>
<ul>
<li>PCFG probabilistic parsing problem: generating the most probable
parsing for a sentence</li>
<li>The probability CKY algorithm extends the CKY algorithm, encoding
each part of the CKY parsing tree into a matrix of <span
class="math inline">\(P(\beta|A)\)</span> . Each element of the matrix
contains a probability distribution over a set of non-terminal symbols,
which can be considered as each element also being V-dimensional, thus
the entire storage space is <span
class="math inline">\((n+1)*(n+1)\)</span> . Where [i,j,A] represents
the probability that the non-terminal symbol A can be used to represent
the segment from position i to j in the sentence.</li>
<li>Algorithm Pseudocode: <img data-src="https://s2.ax1x.com/2019/01/03/FoZbrD.png" alt="FoZbrD.png" /></li>
<li>It can be seen that the method also divides the interval [i, j]
using k, takes the combination with the highest probability as the
probability of the interval, and then expands the interval to the right
for dynamic programming.</li>
</ul>
<h2 id="learning-the-rule-probabilities-of-pcfg">Learning the rule
probabilities of PCFG</h2>
<ul>
<li><p>The pseudo-algorithm diagram above uses the probability of each
rule. How to obtain this probability? There are two methods, the first
being a naive approach: using classical probability type statistics on a
known treebank dataset:</p>
<p><span class="math display">\[
(n+1)*(n+1)*V
\]</span></p></li>
<li><p>If we do not have a treebank, we can use a non-probabilistic
parsing algorithm to parse a dataset and then calculate the
probabilities. However, non-probabilistic parsing algorithms require
calculating probabilities for each possible parsing when parsing
ambiguous sentences, but calculating probabilities requires a
probabilistic parsing algorithm, thus falling into a chicken-and-egg
cycle. One solution is to first use a uniform probability parsing
algorithm to parse the sentence, obtain the probability of each parsing,
then use probability-weighted statistics, and then re-estimate the
probability of parsing rules, continue parsing, and iteratively repeat
until convergence. This algorithm is called the inside-outside
algorithm, which is an extension of the forward-backward algorithm and
is also a special case of the EM algorithm.</p></li>
</ul>
<h2 id="pcfg-issue">PCFG issue</h2>
<ul>
<li>The independence assumption leads to poor modeling of the structural
dependency of the parse tree: each PCFG rule is assumed to be
independent of other rules, for example, statistical results indicate
that pronouns are more likely to be subjects than nouns, so when an NP
is expanded, if the NP is a subject, it is more likely to be expanded
into a pronoun—here, the position of NP in the sentence needs to be
considered, however, this probabilistic dependency relationship is not
allowed by PCFG</li>
<li>Lack of sensitivity to specific words leads to issues such as
subcategorization ambiguity, preposition attachment ambiguity, and
coordination structure ambiguity: for example, in the preposition
attachment issue, which part does a prepositional phrase like "into
Afghanistan" attach to, and in the calculation of PCFG, it is abstracted
as which part a prepositional phrase should attach to, while the
abstraction probability comes from the statistics of the corpus, which
does not consider specific words. For example, in the coordination
structure ambiguity, if two possible parse trees of a sentence use the
same rules but the rules are located differently in the trees, PCFG
calculates the same probability for the two parses: because PCFG assumes
that rules are independent, the joint probability is the product of
individual probabilities.</li>
</ul>
<h2 id="improving-pcfg-by-splitting-and-merging-non-terminals">Improving
PCFG by Splitting and Merging Non-Terminals</h2>
<ul>
<li>Address the issue of structural dependency first. It was previously
mentioned that we hope for different probability rules for NP as a
subject and object. One idea is to split NP into subject NP and object
NP. The method to achieve this split is parent node annotation, where
each node is annotated with its parent node. For subject NP, the parent
node is S, and for object NP, the parent node is VP, thus distinguishing
different NPs. In addition, the parsing tree can be enhanced by
splitting words according to their parts of speech.</li>
<li>Splitting leads to an increase in rules, with less data available to
train each rule, causing overfitting. Therefore, a handwritten rule or
an automatic algorithm should be used to merge some splits based on each
training set.</li>
</ul>
<h2 id="probabilistic-lexicalization-cfg">Probabilistic lexicalization
CFG</h2>
<ul>
<li><p>Probability CKY parsing modifies the grammatical rules, while the
probabilistic lexicalization model modifies the probability model
itself. For each rule, not only should the rule changes for the
constituents be generated, but also the headword and part of speech of
each constituent should be annotated, as shown in the following figure:
<img data-src="https://s2.ax1x.com/2019/01/03/FoeSRP.png"
alt="FoeSRP.png" /></p></li>
<li><p>To generate such an analysis tree, each rule on the right side of
a PCFG needs to select a constituent as a central word sub-node, using
the central word and part of speech of the sub-node as the central word
and part of speech of the node. Among them, the rules are divided into
two categories, internal rules and lexical rules; the latter is
deterministic, while the former requires us to estimate: <img data-src="https://s2.ax1x.com/2019/01/03/FoZqqe.png"
alt="FoZqqe.png" /></p></li>
<li><p>We can split the rules using the idea of similar parent node
annotation, with each part corresponding to a possible choice of a
central word. If we treat the CFG with probabilistic vocabulary as a
large CFG with many rules, we can estimate the probability using the
previous classical probability model. However, such an effect will not
be very good, because the rule division is too fine, and there is not
enough data to estimate the probability. Therefore, we need to make some
independence assumptions, decomposing the probability into smaller
probability products, which can be easily estimated from the
corpus.</p></li>
<li><p>Different statistical analyzers differ in the independence
assumptions they make.</p></li>
<li><p>Collins' analysis is shown as follows: <img data-src="https://s2.ax1x.com/2019/01/03/FoZzGt.png"
alt="FoZzGt.png" /></p></li>
<li><p>The probability is decomposed as follows:</p>
<p><span class="math display">\[
P(\alpha \rightarrow \beta | \alpha) = \frac{Count(\alpha \rightarrow
\beta)}{\sum _{\gamma} Count(\alpha \rightarrow \gamma)}
\]</span></p></li>
<li><p>After generating the left side of the generative expression, the
central word of the rule is first generated, followed by the generation
of the dependency of the central word one by one from the inside out.
Starting from the left side of the central word, generation continues
until the STOP symbol is encountered, after which the right side is
generated. After making a probability split as shown in the above
expression, each probability is easy to statistically calculate from a
smaller amount of data. The complete Collins parser is more complex,
taking into account word distance relationships, smoothing techniques,
unknown words, and so on.</p></li>
</ul>
<h2 id="evaluation-analyzer">Evaluation Analyzer</h2>
<ul>
<li>PARSEVAL measure is the standard method for analyzer evaluation, for
each sentence s:
<ul>
<li>Recall rate = (Count of correct components in s's candidate parsing)
/ (Count of correct components in s's treebank)</li>
<li>Accuracy of tagging = (Count of correct components in candidate
parsing of s) / (Count of all components in candidate parsing of s)</li>
</ul></li>
</ul>
<h2 id="discriminant-reordering">Discriminant reordering</h2>
<ul>
<li>PCFG parsing and Collins lexical parsing both belong to generative
parsers. The drawback of generative models is that it is difficult to
introduce arbitrary information, i.e., it is difficult to incorporate
features that are locally irrelevant to a particular PCFG rule. For
example, the feature that parsing trees tend to be right-generated is
not convenient to add to the generative model.</li>
<li>There are two types of discriminative models for syntactic parsing,
those based on dynamic programming and those based on discriminative
reordering.</li>
<li>Discriminant reordering consists of two stages. In the first stage,
we use a general statistical profiler to generate the top N most
probable profiles and their corresponding probability sequences. In the
second stage, we introduce a classifier, which takes a series of
sentences and the top N profile-probability pairs of each sentence as
input, extracts a large set of features, and selects the best profile
for each sentence. Features include: profile probability, CFG rules in
the profile tree, the number of parallel and coordinate structures, the
size of each component, the extent of right generation in the tree, the
binary grammar of adjacent non-terminal symbols, the frequency of
different parts of the tree, and so on.</li>
</ul>
<h2 id="language-modeling-based-on-analysis">Language Modeling Based on
Analysis</h2>
<ul>
<li>The simplest way to use a statistical parser for language modeling
is to employ the two-phase algorithm previously mentioned. In the first
phase, we run a standard speech recognition decoder or machine
translation decoder (based on ordinary N-gram grammar) to generate N
best candidate sentences; in the second phase, we run the statistical
parser and assign a probability to each candidate sentence, selecting
the one with the highest probability.</li>
</ul>
<h2 id="human-anatomy">Human anatomy</h2>
<ul>
<li>Humans also employ similar probabilistic parsing ideas in
recognizing sentences; two examples:
<ul>
<li>For frequently occurring binary grammatical structures, people spend
less time reading these binary grammatical structures</li>
<li>Some experiments indicate that humans tend to choose the analysis
with a higher statistical probability during disambiguation</li>
</ul></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="第二章正则表达式与自动机">第二章：正则表达式与自动机</h1>
<ul>
<li>正则表达式：一种用于查找符合特定模式的子串或者用于以标准形式定义语言的工具，本章主要讨论其用于查找子串的功能。正则表达式用代数的形式来表示一些字符串集合。</li>
<li>正则表达式接收一个模式，然后在整个语料中查找符合这个模式的子串，这个功能可以通过设计有限状态自动机实现。</li>
<li>字符串看成符号的序列，所有的字符，数字，空格，制表符，标点和空格均看成符号。</li>
</ul>
<h2 id="基本正则表达式模式">基本正则表达式模式</h2>
<ul>
<li>用双斜线表示正则表达式开始和结束（perl中的形式）
<ul>
<li>查找子串，大小写敏感：/woodchuck/-&gt; woodchuck</li>
<li>用方括号代表取其中一个，或：/[Ww]oodchuck/-&gt;woodchuck or
Woodchuck</li>
<li>方括号加减号，范围内取或：/[2-5]/-&gt;/[2345]</li>
<li>插入符号放在左方括号后，代表模式中不出现后接的所有符号，取非: /^Ss/
-&gt;既不是大写S也不是小写s</li>
<li>问号代表之前的符号出现一个或不出现：/colou?r/-&gt;color or
colour</li>
<li>星号代表之前的符号出现多个或不出现：/ba*/-&gt;b or ba or baa or
baaa......</li>
<li>加号代表之前的符号出现至少一次：/ba+/-&gt;ba or baa or
baaa.......</li>
<li>小数点代表通配符，与任何除了回车符之外的符号匹配：/beg.n/-&gt;begin
or begun or beg’n or .......</li>
<li>锚符号，用来表示特定位置的子串，插入符号代表行首，美元符号代表行尾，，，perl将单词的定义为数字、下划线、字母的序列，不在其中的符号便可以作为单词的分界。</li>
</ul></li>
</ul>
<h2 id="析取组合和优先">析取、组合和优先</h2>
<ul>
<li>用竖线代表析取，字符串之间的或：/cat|dog/-&gt;cat or dog</li>
<li>用圆括号代表部分析取（组合），圆括号内也可以用基本算符：/gupp(y|ies)/-&gt;guppy
or guppies</li>
<li>优先级：圆括号&gt;计数符&gt;序列与锚&gt;析取符</li>
</ul>
<h2 id="高级算符">高级算符</h2>
<ul>
<li>任何数字</li>
<li>：任何非数字字符</li>
<li>：任何字母、数字、空格</li>
<li>：与</li>
<li>：空白区域</li>
<li>：与</li>
<li>{n}：前面的模式出现n个</li>
<li>{n,m}：前面的模式出现n到m个</li>
<li>{n,}：前面的模式至少出现n个</li>
<li>：换行</li>
<li>表格符</li>
</ul>
<h2 id="替换寄存器">替换、寄存器</h2>
<ul>
<li>替换s/A/B/：A替换成B</li>
<li>s/(A)/&lt;\1&gt;/：用数字算符\1指代A，在A的两边加上尖括号</li>
<li>在查找中也可以用数字算符，指代圆括号内内容，可以多个算符指代多个圆括号内内容</li>
<li>这里数字算符起到了寄存器的作用</li>
</ul>
<h2 id="有限状态自动机">有限状态自动机</h2>
<ul>
<li>有限状态自动机和正则表达式彼此对称，正则表达式是刻画正则语言的一种方法。正则表达式、正则语法和自动状态机都是表达正则语言的形式。FSA用有向图表示，圆圈或点代表状态，箭头或者弧代表状态转移，用双圈表示最终状态，如下图表示识别/baa+!/的状态机图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoVj3V.png"
alt="FoVj3V.png" /></li>
<li>状态机从初始状态出发，依次读入符号，若满足条件，则进行状态转移，若读入的符号序列满足模式，则状态机可以到达最终状态；若符号序列不满足模式，或者自动机在某个非最终状态卡住，则称自动机拒绝了此次输入。</li>
<li>另一种表示方式是状态转移表： <img data-src="https://s2.ax1x.com/2019/01/03/FoVqNn.png" alt="FoVqNn.png" /></li>
<li>一个有限自动机可以用5个参数定义：
<ul>
<li><span class="math inline">\(Q\)</span>：状态{q_i}的有限集合</li>
<li>：有限的输入符号字母表</li>
<li><span class="math inline">\(q_0\)</span>：初始状态</li>
<li><span class="math inline">\(F\)</span>：终极状态集合</li>
<li><span class="math inline">\(\delta
(q,i)\)</span>：状态之间的转移函数或者转移矩阵，是从<span
class="math inline">\(Q × \Sigma\)</span>到<span
class="math inline">\(2^Q\)</span>的一个关系</li>
</ul></li>
<li>以上描述的自动机是确定性的，即DFSA，在已知的记录在状态转移表上的状态时，根据查表自动机总能知道如何进行状态转移。算法如下，给定输入和自动机模型，算法确定输入是否被状态机接受：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZpB4.png"
alt="FoZpB4.png" /></li>
<li>当出现了表中没有的状态时自动机就会出错，可以添加一个失败状态处理这些情况。</li>
</ul>
<h2 id="形式语言">形式语言</h2>
<ul>
<li>形式语言是一个模型，能且只能生成和识别一些满足形式语言定义的某一语言的符号串。形式语言是一种特殊的正则语言。通常使用形式语言来模拟自然语言的某些部分。以上例/baa+!/为例，设对应的自动机模型为m，输入符号表<span
class="math inline">\(\Sigma = {a,b,!}\)</span>，<span
class="math inline">\(L(m)\)</span>代表由m刻画的形式语言，是一个无限集合<span
class="math inline">\({baa!,baaa!,baaaa!,…}\)</span></li>
</ul>
<h2 id="非确定有限自动机">非确定有限自动机</h2>
<ul>
<li>非确定的有限自动机NFSA,把之前的例子稍微改动，自返圈移动到状态2，就形成了NFSA，因为此时在状态2，输入a，有两种转移可选，自动机无法确定转移路径：
<img data-src="https://s2.ax1x.com/2019/01/03/FoVLhq.png"
alt="FoVLhq.png" /></li>
<li>另一种NFSA的形式是引入<span
class="math inline">\(\epsilon\)</span>转移，即不需要输入符号也可以通过此<span
class="math inline">\(\epsilon\)</span>转移进行转移，如下图，在状态3时依然不确定如何进行转移：
<img data-src="https://s2.ax1x.com/2019/01/03/FoVX90.png"
alt="FoVX90.png" /></li>
<li>在NFSA时，面临转移选择时自动机可能做出错误的选择，此时存在三种解决方法：
<ul>
<li>回退：标记此时状态，当确定发生错误选择之后，回退到此状态</li>
<li>前瞻：在输入中向前看，帮助判定进行选择</li>
<li>并行：并行的进行所有可能的转移</li>
</ul></li>
<li>在自动机中，采用回退算法时需要标记的状态称为搜索状态，包括两部分：状态节点和输入位置。对于NFSA，其状态转移表也有相应改变，如图，添加了代表<span
class="math inline">\(\epsilon\)</span>转移的<span
class="math inline">\(\epsilon\)</span>列，且转移可以转移到多个状态：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZE36.png"
alt="FoZE36.png" /></li>
<li>采用回退策略的非确定自动机算法如下，是一种搜索算法： <img data-src="https://s2.ax1x.com/2019/01/03/FoZSuF.png" alt="FoZSuF.png" /></li>
<li>子函数GENERATE-NEW-STATES接受一个搜索状态，提取出状态节点和输入位置，查找这个状态节点上的所有状态转移可能，生成一个搜索状态列表作为返回值；</li>
<li>子函数ACCEPT-STATE接受一个搜索状态，判断是否接受，接受时的搜索状态应该是最终状态和输入结束位置的二元组。</li>
<li>算法使用进程表（agenda）记录所有的搜索状态，初始只包括初始的搜索状态，即自动机初始状态节点和输入起始。之后不断循环，从进程表中调出搜索状态，先调用ACCEPT-STATE判断是否搜索成功，之后再调用GENERATE-NEW-STATES生成新的搜索状态加入进程表。循环直到搜索成功或者进程表为空（所有可能转移均尝试且未成功）返回拒绝。</li>
<li>可以注意到NFSA算法就是一种状态空间搜索，可以通过改变搜索状态的顺序提升搜索效率，例如用栈实现进程表，进行深度优先搜索DFS；或者使用队列实现进程表，进行宽度优先搜索BFS。</li>
<li>对于任何NFSA，存在一个完全等价的DFSA。</li>
</ul>
<h2 id="正则语言和nfsa">正则语言和NFSA</h2>
<ul>
<li>定义字母表为所有输入符号集合；空符号串<span
class="math inline">\(\epsilon\)</span>，空符号串不包含再字母表中；空集∅。在上的正则语言的类（或者正则集）可以形式的定义如下：
<ul>
<li>∅是正则语言</li>
<li>∀a ∈ <span class="math inline">\(\sum\)</span> ∪<span
class="math inline">\(\epsilon\)</span>,{a}是形式语言</li>
<li>如果<span class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>是正则语言，那么：</li>
<li><span class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>的拼接是正则语言</li>
<li><span class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>的合取、析取也是正则语言</li>
<li><span class="math inline">\(L_1\)</span>^*，即<span
class="math inline">\(L_1\)</span>的Kleene闭包也是正则语言</li>
</ul></li>
<li>可见正则语言的三种基本算符：拼接、合取及析取、Kleene闭包。任何正则表达式可以写成只使用这三种基本算符的形式。</li>
<li>正则语言对以下运算也封闭（<span
class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>均为正则语言）：
<ul>
<li>交：<span class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>的符号串集合的交构成的语言也是正则语言</li>
<li>差：<span class="math inline">\(L_1\)</span>和<span
class="math inline">\(L_2\)</span>的符号串集合的差构成的语言也是正则语言</li>
<li>补：不在<span
class="math inline">\(L_1\)</span>的符号串集合中的集合构成的语言也是正则语言</li>
<li>逆：<span
class="math inline">\(L_1\)</span>所有符号串的逆构成的集合构成的语言也是正则语言</li>
</ul></li>
<li>可以证明正则表达式和自动机等价，一个证明任何正则表达式可以建立对应的自动机的方法是，根据正则语言的定义，构造基础自动机代表<span
class="math inline">\(\epsilon\)</span>、∅以及<span
class="math inline">\(\sum\)</span>中的单个符号a，然后将三种基本算符表示为自动机上的操作，归纳性的，在基础自动机上应用这些操作，得到新的基础自动机，这样就可以构造满足任何正则表达式的自动机，如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoVxjU.png" alt="FoVxjU.png" />
基础自动机 <img data-src="https://s2.ax1x.com/2019/01/03/FoZPE9.png"
alt="FoZPE9.png" /> 拼接算符 <img data-src="https://s2.ax1x.com/2019/01/03/FoZ9HJ.png" alt="FoZ9HJ.png" />
Kleene闭包算符 <img data-src="https://s2.ax1x.com/2019/01/03/FoZiNR.png"
alt="FoZiNR.png" /> 合取析取算符</li>
</ul>
<h1
id="第三章形态学与有限状态转录机">第三章：形态学与有限状态转录机</h1>
<ul>
<li>剖析：取一个输入并产生关于这个输入的各类结构</li>
</ul>
<h2 id="英语形态学概论">英语形态学概论</h2>
<ul>
<li>形态学研究词的构成，词可以进一步拆解为语素，语素可分为词干和词缀，词缀可分为前缀、中缀、后缀、位缀。</li>
<li>屈折形态学：英语中，名词只包括两种屈折变化：一个词缀表示复数，一个词缀表示领属：
<ul>
<li>复数：-s，-es，不规则复数形式</li>
<li>领属：-‘s，-s’</li>
</ul></li>
<li>动词的屈折变化包括规则动词和非规则动词的变化：
<ul>
<li>规则动词：主要动词和基础动词，-s，-ing，-ed，</li>
<li>非规则动词</li>
</ul></li>
<li>派生形态学：派生将词干和一个语法语素结合起来，形成新的单词
<ul>
<li>名词化：-ation，-ee，-er，-ness</li>
<li>派生出形容词：-al，-able，-less</li>
</ul></li>
</ul>
<h2 id="形态剖析">形态剖析</h2>
<ul>
<li>例子：我们希望建立一个形态剖析器，输入单词，输出其词干和有关的形态特征，如下表，我们的目标是产生第二列和第四列：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZA9x.png"
alt="FoZA9x.png" /></li>
<li>我们至少需要：
<ul>
<li>词表（lexicon）：词干和词缀表及其基本信息</li>
<li>形态顺序规则（morphotactics）：什么样的语素跟在什么样的语素之后</li>
<li>正词法规则（orthographic rule）：语素结合时拼写规则的变化</li>
</ul></li>
<li>一般不直接构造词表，而是根据形态顺序规则，设计FSA对词干进行屈折变化生成词语。例如一个名词复数化的简单自动机如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZmuD.png"
alt="FoZmuD.png" /></li>
<li>其中reg-noun代表规则名词，可以通过加s形成复数形式，并且忽略了非规则单数名词(irreg-sg-noun)和非规则复数名词(irreg-pl-noun)。另外一个模拟动词屈折变化的自动机如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZQUA.png"
alt="FoZQUA.png" /></li>
<li>使用FSA解决形态识别问题（判断输入符号串是否合法）的一种方法是，将状态转移细分到字母层次，但是这样仍然会存在一些问题：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZZjO.png"
alt="FoZZjO.png" /></li>
</ul>
<h2 id="有限状态转录机">有限状态转录机</h2>
<ul>
<li>双层形态学：将一个词表示为词汇层和表层，词汇层表示该词语素之间的简单毗连（拼接，concatenation），表层表示单词实际最终的拼写，有限状态转录机是一种有限状态自动机，但其实现的是转录，实现词汇层和表层之间的对应，它有两个输入，产生和识别字符串对，每一个状态转移的弧上有两个标签，代表两个输入。
<img data-src="https://s2.ax1x.com/2019/01/03/FoZVgK.png"
alt="FoZVgK.png" /></li>
<li>从四个途径看待FST：
<ul>
<li>作为识别器：FST接受一对字符串，作为输入，如果这对字符串在语言的字符串对中则输出接受否则拒绝</li>
<li>作为生成器：生成语言的字符串对</li>
<li>作为翻译器：读入一个字符串，输出另一个</li>
<li>作为关联器：计算两个集合之间的关系</li>
</ul></li>
<li>定义有限状态转录机：
<ul>
<li>Q：状态{q_i}的有限集合</li>
<li>：有限的输入符号字母表</li>
<li>∆：有限的输出符号字母表</li>
<li><span class="math inline">\(q_0 \in Q\)</span>：初始状态</li>
<li><span class="math inline">\(F⊆Q\)</span>：终极状态集合</li>
<li><span class="math inline">\(\delta
(q,w)\)</span>：状态之间的转移函数或者转移矩阵，是从Q×到2^Q的一个关系，q是状态，w是字符串，返回新状态集合</li>
<li><span class="math inline">\(\sigma
(q,w)\)</span>：输出函数，给定每一个状态和输入，返回可能输出字符串的集合，是从<span
class="math inline">\(Q × \Sigma\)</span>到<span
class="math inline">\(2^∆\)</span>的一个关系</li>
</ul></li>
<li>在FST中，字母表的元素不是单个符号，而是符号对，称为可行偶对。类比于FSA和正则语言，FST和正则关系同构，对于并运算封闭，一般对于差、补、交运算不封闭。</li>
<li>此外，FST，
<ul>
<li>关于逆反（逆的逆）闭包，逆反用于方便的实现作为剖析器的FST到作为生成器的FST的转换</li>
<li>关于组合（嵌套）闭包，用于将多个转录机用一个更复杂的转录机替换。</li>
</ul></li>
<li>转录机一般是非确定性的，如果用FSA的搜索算法会很慢，如果用非确定性到确定性的转换算法，则有些FST本身是不可以被转换为为确定的。</li>
<li>顺序转录机是一种输入确定的转录机，每个状态转移在给定状态和输入之后是确定的，不像上图中的FST，状态0在输入b时有两种状态转移（转移到相同的状态，但是输出不同）。顺序转录机可以使用<span
class="math inline">\(\epsilon\)</span>符号，但是只能加在输出字符串上，不能加在输入字符串上，如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZuHH.png"
alt="FoZuHH.png" /></li>
<li>顺序转录机输出不一定是序列的，即从同一状态发出的不同转移可能产生相同输出，因此顺序转录机的逆不一定是顺序转录机，所以在定义顺序转录机时需要定义方向，且转移函数和输出函数需要稍微修改，输出空间缩小为Q和∆。</li>
<li>顺序转录机的一种泛化形式是并发转录机，其在最终状态额外输出一个字符串，拼接到已经输出的字符串之后。顺序和并发转录机的效率高，且有有效的算法对其进行确定化和最小化，因此很重要。P并发转录机在此基础上可以解决歧义问题。</li>
</ul>
<h2 id="用有限状态转录机进行形态剖析">用有限状态转录机进行形态剖析</h2>
<ul>
<li>将单词看成词汇层和表层之间的关系，如下图： <img data-src="https://s2.ax1x.com/2019/01/03/FoZnDe.png" alt="FoZnDe.png" /></li>
<li>在之前双层形态学的基础定义上，定义自己到自己的映射为基本对，用一个字母表示；用^代表语素边界；用#代表单词边界，在任务中提到需要输出+SG之类的语素特征，这些特征在另一个输出上没有对应的输出符号，因此映射到空字符串或边界符号。我们把输入输出对用冒号连接，也可以写在弧的上下。一个抽象的表示英语名词复数屈折变化的转录机如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZl4I.png"
alt="FoZl4I.png" /></li>
<li>之后我们需要更新词表，使得非规则复数名词能够被剖析为正确的词干：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZMEd.png"
alt="FoZMEd.png" /></li>
<li>之后将抽象的转录机写成具体的，由字母组成转移弧的转录机，如下图，只展示了具体化部分非规则复数和单数名词之后的转录机：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZ3Ct.png"
alt="FoZ3Ct.png" /></li>
</ul>
<h2 id="转录机和正词法规则">转录机和正词法规则</h2>
<ul>
<li>用拼写规则，也就是正词法规则来处理英语中经常在语素边界发生拼写错误的问题。</li>
<li>以下是一些拼写规则实例：
<ul>
<li>辅音重叠：beg/beggin</li>
<li>E的删除：make/making</li>
<li>E的插入：watch/watches</li>
<li>Y的替换：try/tries</li>
<li>K的插入：panic/panicked</li>
</ul></li>
<li>为了实现拼写规则，我们在词汇层和表层之间加入中间层，以符合特定规则的语素毗连作为输入，以修改之后的正确的语素毗连作为输出，例如fox
+N +PL输入到中间层即第一次转录，得到fox ^ s
#，之后中间层到表层的第二次转录检测到特殊语素毗连：x^和s#，就在表层的x和s之间插入一个e，得到foxes。下面的转录机示意图展示了这个过程：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZ88P.png"
alt="FoZ88P.png" /></li>
<li>这个转录机只考虑x^和s#毗连需插入e这一正词法规则</li>
<li>其他的词能正常通过</li>
<li><span
class="math inline">\(Q_0\)</span>代表无关词通过，是接受状态</li>
<li><span
class="math inline">\(Q_1\)</span>代表看见了zsx，作为中间状态保存，一直保存的是最后的与语素毗连的z,s,x，如果出现了其他字母则返回到q0，其本身也可以作为接受态</li>
<li><span
class="math inline">\(Q_2\)</span>代表看见了与z,s,x毗连的语素，这之后有四种转移
<ul>
<li>接了<span class="math inline">\(x\)</span>,<span
class="math inline">\(z\)</span>，回到<span
class="math inline">\(q_1\)</span>，也就是认为重新接到了可能和语素毗连的x,z</li>
<li>接了<span
class="math inline">\(s\)</span>，分为两种情况，一种是正常需要插入e，这时通过<span
class="math inline">\(\epsilon\)</span>转移到<span
class="math inline">\(q_3\)</span>再到<span
class="math inline">\(q_4\)</span>；另一种是本来就需要插入<span
class="math inline">\(e\)</span>，这就到达<span
class="math inline">\(q_5\)</span>，之后视情况回退了<span
class="math inline">\(q_1\)</span>、<span
class="math inline">\(q_0\)</span>，或者<span
class="math inline">\(s\)</span>又毗连语素回到<span
class="math inline">\(q_2\)</span>。两种情况不确定，需要通过搜索解决</li>
<li>接单词边界和其他符号，回到<span
class="math inline">\(q_0\)</span></li>
<li><span class="math inline">\(q_2\)</span>本身也可以作为接受态</li>
</ul></li>
</ul>
<h2 id="结合">结合</h2>
<ul>
<li>现在可以通过三层结构，结合产生语素和进行正词法规则矫正的转录机。从词汇层到中间层用一个转录机产生语素，从中间层到表层可并行使用多个转录机进行正词法规则的矫正。</li>
<li>两类转录机叠加时可以改写成一类转录机，这时需要对两类状态机状态集合计算笛卡尔积，对新集合内每一个元素建立状态。</li>
<li>这种三层结构是可逆的，但是进行剖析时（从表层到词汇层）会出现歧义问题，即一个单词可能剖析出多种语素结合，这时单纯依靠转录机无法消歧，需要借助上下文。</li>
</ul>
<h2 id="其他应用简单介绍">其他应用（简单介绍）</h2>
<ul>
<li>不需要词表的FST，PORTER词干处理器：将层叠式重写规则用FST实现，提取出单词的词干。</li>
<li>分词和分句：一个简单的英文分词可以基于正则表达式实现，一个简单的中文分词可以通过maxmatch（一种基于最大长度匹配的贪婪搜索算法）实现。</li>
<li>拼写检查与矫正：使用了投影操作的FST可以完成非词错误的检测，然后基于最小编辑距离（使用动态规划算法实现）可以矫正。正常词错误检测和矫正需借助N元语法模型。</li>
</ul>
<h2 id="人如何进行形态处理">人如何进行形态处理</h2>
<ul>
<li>研究表明，人的心理词表存储了一部分形态机构，其他的结构不组合在心理词表中，而需要分别提取并组合。研究说明了两个问题：
<ul>
<li>形态尤其是屈折变化之类的能产性形态在人的心理词表中起作用，且人的语音词表和正词法词表可能具有相同结构。</li>
<li>例如形态这种语言处理的很多性质，可以应用于语言的理解和生成。</li>
</ul></li>
</ul>
<h1 id="第四章n元语法">第四章：N元语法</h1>
<ul>
<li>语言模型是关于单词序列的统计模型，N元语法模型是其中的一种，它根据之前N-1个单词推测第N个单词，且这样的条件概率可以组成整个单词序列（句子）的联合概率。</li>
</ul>
<h2 id="在语料库中统计单词">在语料库中统计单词</h2>
<ul>
<li><p>区别：word type或者叫 vocabulary size
V，代表语料中不同单词的个数，而tokens，不去重，代表语料的大小。有研究认为词典大小不低于tokens数目的平方根。
非平滑N元语法模型</p></li>
<li><p>任务：根据以前的单词推断下一个单词的概率：<span
class="math inline">\(P(w|h)\)</span>，以及计算整个句子的概率<span
class="math inline">\(P(W)\)</span>。</p></li>
<li><p>最朴素的做法是用古典概型，统计所有历史h和当前词w组成的片段在整个语料中出现的次数，并除以历史h片段在整个语料中出现的次数。句子的概率也用相似的方法产生。缺点：依赖大语料，且语言本身多变，这样的计算限制过于严格。</p></li>
<li><p>接下来引入N元语法模型，首先通过概率的链式法则，可以得到条件概率<span
class="math inline">\(P(w|h)\)</span>和整个句子的联合概率<span
class="math inline">\(P(W)\)</span>之间的关系：</p>
<p><span class="math display">\[
P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\
= \prod _{k=1}^n P(w_k|w_1^{k-1}) \\
\]</span></p></li>
<li><p>N元语法模型放松了条件概率的限制，做出一个马尔可夫假设：每个单词的概率只和它之前N-1个单词相关，例如二元语法模型，只和前一个单词相关，用这个条件概率去近似<span
class="math inline">\(P(w|h)\)</span>:</p>
<p><span class="math display">\[
P(w_n|w_1^{n-1}) \approx P(w_n|w_{n-1}) \\
\]</span></p></li>
<li><p>N元语法模型里的条件概率用最大似然估计来估算，统计语料中各种N元语法的个数，并归一化，其中可以简化的一点是：以二元语法为例，所有给定单词开头的二元语法总数必定等于该单词一元语法的计数：</p>
<p><span class="math display">\[
P(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\
\]</span></p></li>
<li><p>使用N元语法之后，句子概率的链式分解变得容易计算，我们可以通过计算各种句子的概率来判断句子是否包含错字，或者计算某些句子在给定上下文中出现的可能，因为N元语法能捕捉一些语言学上的特征，或者一些用语习惯。在语料充足的时候，我们可以使用三元语法模型获得更好的效果。</p></li>
</ul>
<h2 id="训练集和测试集">训练集和测试集</h2>
<ul>
<li>N元语法模型对训练集非常敏感。N元语法的N越大，依赖的上下文信息越多，利用N元语法模型生成的句子就越流畅，但这些未必“过于流畅”，其原因在于N元语法概率矩阵非常大且非常稀疏，在N较大例如四元语法中，一旦生成了第一个单词，之后可供的选择非常少，接着生成第二个单词之后选择更少了，往往只有一个选择，这样生成的就和原文中某一个四元语法一模一样。过于依赖训练集会使得模型的泛化能力变差。因此我们选择的训练集和测试集应来自同一细分领域。</li>
<li>有时候测试集中会出现训练集词典里没有的词，即出现未登录词（Out Of
Vocabulty,OOV）。在开放词典系统中，我们先固定词典大小，并将所有未登录词用特殊符号<UNK>代替，然后才进行训练。</li>
</ul>
<h2 id="评价n元语法模型困惑度">评价N元语法模型：困惑度</h2>
<ul>
<li><p>模型的评价分两种：外在评价和内在评价。外在评价是一种端到端的评价，看看某一模块的改进是否改进了整个模型的效果。内在评价的目的是快速衡量模块的潜在改进效果。内在评价的潜在改进效果不一定会使得端到端的外在评价提高，但是一般两者都存在某种正相关关系。</p></li>
<li><p>困惑度（Perplexsity,PP）是一种关于概率模型的内在评价方法。语言模型的在测试集上的困惑度是语言模型给测试集分配的概率的函数。以二元语法为例，测试集上的困惑度为：</p>
<p><span class="math display">\[
PP(W) = \sqrt[n]{\prod _{i=1}^N \frac {1}{P(w_i|w_{i-1})}} \\
\]</span></p></li>
<li><p>概率越高，困惑度越低。困惑度的两种解释：</p>
<ul>
<li><p>加权的平均分支因子：分支因子是指可能接在任何上文之后的单词的数目。显然，如果我们的模型啥也没学习到，那么测试集任何单词可以接在任何上文之后，分支因子很高，困惑度很高；相反，如果我们的模型学习到了具体的规则，那么单词被限制接在一些指定上文之后，困惑度变低。困惑度使用了概率加权分支因子，分支因子的大小在模型学习前后不变，”morning”仍然可以接到任何上文之后，但是它接到”good”之后的概率变大了，因此是加权的分支因子。</p></li>
<li><p>熵：对于语言序列，我们定义一个序列的熵为： <span
class="math display">\[H(w_1,w_2,…,w_n )=-\sum _{W_1^n \in L} p(W_1^n)
\log
⁡p(W_1^n)\]</span>也就是这个序列中所有前缀子序列的熵之和，其均值是序列的熵率。计算整个语言的熵，假设语言是一个产生单词序列的随机过程，单词序列无限长，则其熵率是：<span
class="math display">\[H(L)=\lim _{n \rightarrow \infty}⁡ \frac 1n
H(w_1,w_2,…,w_n) =\lim _{n \rightarrow \infty} -⁡\frac 1n \sum _{W \in L}
p(W_1^n)  \log
⁡p(W_1^n)\]</span>根据Shannon-McMillan-Breiman理论，在n趋于无穷的情况下，如果语言既是平稳又是正则的，上面这些子串的和的熵，可以用最大串代替每一个子串得到，这里的代替是指log后面求的是最大串的概率，log之前的概率依然是各个子串的概率？假如是这样的话提出最大串的概率对数，对所有子串概率求和得到：<span
class="math display">\[H(L)=\lim _{n \rightarrow \infty} -⁡ \frac 1n \log
⁡p(w_1,w_2,…,w_n)\]</span>交叉熵可以衡量我们的模型生成的概率分布到指定概率分布之间的距离，我们希望模型生成概率分布尽可能近似真实分布，即交叉熵小。具体衡量时是对相同的语言序列，计算训练得到的模型m和理想模型p在生成这个序列上的概率的交叉熵：<span
class="math display">\[H(p,m) = \lim _{n \rightarrow \infty}⁡ - \frac 1n
\sum _{W \in L} p(W_1^n) \log⁡
m(W_1^n)\]</span>但是我们不知道理想的分布p，这时根据之前的Shannon-McMillan-Breiman定理，得到了只包含一个概率分布的序列交叉熵（？）：<span
class="math display">\[H(p,m)=\lim _{n \rightarrow \infty}⁡ - \frac 1n
\log⁡
m(W_1^n)\]</span>在测试数据上我们没有无限长的序列，就用有限长的序列的交叉熵近似这个无限长序列的交叉熵。困惑度则是这个（近似的？只包含一个概率分布的？）交叉熵取指数运算：</p>
<p><span class="math display">\[
Perplexity(W) = 2^{H(W)} \\
= P(w_1 w_2 ... w_N)^{\frac {-1}{N}} \\
= \sqrt[n]{\frac {1}{P(w_1 w_2 ... w_N)}} \\
= \sqrt[n]{\prod _{i=1}^N \frac {1}{P(w_i | w_1 ... w_{i-1})}} \\
\]</span></p></li>
</ul></li>
</ul>
<h2 id="平滑">平滑</h2>
<ul>
<li>因为N元语法模型依赖语料，一般而言对于N越高的N元语法，语料提供的数据越稀疏。这种情况下N元语法对于那些计数很小的语法估计很差，且如果测试集中某一句包含了训练集中没有出现的N元语法时，我们无法使用困惑度进行评价。因此我们使用平滑作为一种改进方法，使得N元语法的最大似然估计能够适应这些存在0概率的情况。</li>
<li>接下来介绍了两种平滑：
<ul>
<li>拉普拉斯平滑（加1平滑）</li>
<li>Good-Turing 打折法</li>
</ul></li>
</ul>
<h3 id="拉普拉斯平滑">拉普拉斯平滑</h3>
<ul>
<li><p>加1平滑就是在计算概率归一化之前，给每个计数加1，对应的，归一化时分母整体加了一个词典大小:</p>
<p><span class="math display">\[
P_{Laplace}(w_i) = \frac {c_i + 1}{N+V} \\
\]</span></p></li>
<li><p>为了表现平滑的作用，引入调整计数<span
class="math inline">\(c^{*}\)</span>，将平滑后的概率写成和平滑之前一样的形式：</p>
<p><span class="math display">\[
P_{Laplace} (w_i) = \frac {(C_i^{*})}{N} \\
C_i^{*} = \frac {(C_i+1)N}{(N+V)} \\
\]</span></p></li>
<li><p>一种看待平滑的角度是：对每个非0计数打折，分一些概率给0计数，定义相对打折<span
class="math inline">\(d_c\)</span>（定义在非0计数上），</p>
<p><span class="math display">\[
d_c = \frac {c^{*}} {c}
\]</span></p></li>
<li><p><span
class="math inline">\(d_c\)</span>代表了打折前后单词计数的变化。平滑之后，对于非0计数，当<span
class="math inline">\(C_i &lt; \frac
NV\)</span>时，计数增加；否则计数减少。计数越大，打折越多，增加越少（减少越多）。当0计数很多时，N/V较小，这时大部分非0计数都会减少，且减少较多。</p></li>
<li><p>而0计数则没有收到打折的影响。因此在一轮不同程度的增长之后，再归一化的结果就是非0计数分享了一些概率给0计数。写成调整计数的形式，就是非0计数减少数值，0计数变化（一般是减少）数值（但不是减少的完全等于增加的）。
书中给出了一个例子，下图是一部分语料的二元语法平滑之后的计数，蓝色代表平滑加1之后的0计数：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZNDg.png" alt="FoZNDg.png" />
如果把表写成调整计数的形式： <img data-src="https://s2.ax1x.com/2019/01/03/FoZtKS.png"
alt="FoZtKS.png" /></p></li>
<li><p>可以看到，本来的0计数（蓝色）从0变大，而其他的计数减少，例如&lt;
i want&gt;，从827减少到527，<want to>从608减少到238。</p></li>
<li><p>当0计数很多时，非0计数减少的数值很多，可以使用一个小于1的小数<span
class="math inline">\(\delta\)</span>代替1，即加<span
class="math inline">\(\delta\)</span>平滑。通常这个<span
class="math inline">\(\delta\)</span>是动态变化的。</p></li>
</ul>
<h3 id="gt打折法">GT打折法</h3>
<ul>
<li><p>类似于Good-Turing打折法, Witten-Bell打折法， Kneyser-Ney
平滑一类的方法，它们的基本动机是用只出现一次的事物的计数来估计从未出现的事物的计数。只出现一次的语法称为单件（singleton）或者罕见语（hapax
legomena）。Good-Turing打折法用单件的频率来估计0计数二元语法。</p></li>
<li><p>定义N_c为出现c次的N元语法的总个数（不是总个数乘以c），并称之为频度c的频度。对N_c中的c的最大似然估计是c。这样相当于将N元语法按其出现次数分成了多个桶，GT打折法用c+1号桶里语法概率的最大似然估计来重新估计c号桶内语法的概率。因此GT估计之后最大似然估计得到的c被替换成：</p>
<p><span class="math display">\[
c^{*}=(c+1) \frac {N_{c+1}}{N_c}
\]</span></p></li>
<li><p>之后计算某N元语法的概率：</p>
<ul>
<li>从未出现：<span
class="math inline">\(P_{GT}^{*}=\frac{N_1}{N}\)</span>。其中N是所有N元语法数<span
class="math inline">\((\sum _i N_i *
i)\)</span>。这里假设了我们已知<span
class="math inline">\(N_0\)</span>，则此式表示某一具体未知计数N元语法概率时还应除以<span
class="math inline">\(N_0\)</span>。</li>
<li>已出现（已知计数）：<span class="math inline">\(P_{GT}^{*} =
\frac{c^{*}}{N}\)</span></li>
</ul></li>
<li><p>这样计算，<span
class="math inline">\(N_1\)</span>的一些概率转移到了<span
class="math inline">\(N_0\)</span>上。GT打折法假设所有的N元语法概率分布满足二项式分布，且假设我们已知<span
class="math inline">\(N_0\)</span>，以二元语法为例：</p>
<p><span class="math display">\[
N_0 = V^2 - \sum _{i&gt;0} N_i \\
\]</span></p></li>
<li><p>其他注意事项：</p>
<ul>
<li><p>有些<span
class="math inline">\(N_c\)</span>为0，这时我们无法用这些<span
class="math inline">\(N_c\)</span>来计算平滑后的c。这种情况下我们直接放弃平滑，令<span
class="math inline">\(c^{*} =
c\)</span>，再根据正常的数据计算出一个对数线性映射，<span
class="math inline">\(log⁡(N_c) = a + b
\log(c)\)</span>，代入放弃平滑的c并用其倒推计算计数为0的<span
class="math inline">\(N_c\)</span>，使得这些<span
class="math inline">\(N_c\)</span>有值，不会影响更高阶的c的计算。</p></li>
<li><p>只对较小c的<span
class="math inline">\(N_c\)</span>进行平滑，较大c的<span
class="math inline">\(N_c\)</span>认为足够可靠，设定一个阈值k，对<span
class="math inline">\(c &lt; k\)</span>的<span
class="math inline">\(N_c\)</span>计算：</p>
<p><span class="math display">\[
c^{*} = \frac {(c+1) \frac {N_c+1}{N_c} - c \frac {(k+1) N_{k+1} }{N_1}
} {1- \frac {(k+1)N_{k+1}} {N_1}} \\
\]</span></p></li>
<li><p>计算较小的c如c=1时，也看成c=0的情况进行平滑</p></li>
</ul></li>
<li><p>一个例子： <img data-src="https://s2.ax1x.com/2019/01/03/FoZGgf.png"
alt="FoZGgf.png" /></p></li>
</ul>
<h2 id="插值与回退">插值与回退</h2>
<ul>
<li>上述的平滑只考虑了如何转移概率到计数为0的语法上去，对于条件概率<span
class="math inline">\(p(w|h)\)</span>，我们也可以采用类似的思想，假如不存在某个三元语法帮助计算<span
class="math inline">\(p(w_n |w_{n-1}
w_{n-2})\)</span>，则可以用阶数较低的语法<span
class="math inline">\(p(w_n |w_{n-1})\)</span>帮助计算，有两种方案：
<ul>
<li>回退：用低阶数语法的替代0计数的高阶语法</li>
<li>插值：用低阶数语法的加权估计高阶语法</li>
</ul></li>
<li>在Katz回退中，我们使用GT打折作为方法的一部分：GT打折告诉我们有多少概率可以从已知语法中分出来，Katz回退告诉我们如何将这些分出来的概率分配给未知语法。在之前的GT打折法中，我们将分出的概率均匀分给每一个未知语法，而Katz回退则依靠低阶语法的信息来分配：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZJv8.png"
alt="FoZJv8.png" /></li>
<li>其中<span
class="math inline">\(P^{*}\)</span>是打折之后得到的概率；，保证分出去的概率等于未知语法分配得到的概率。</li>
<li>插值则是用低阶语法概率加权求和得到未知高阶语法概率： <img data-src="https://s2.ax1x.com/2019/01/03/FoZUbQ.png" alt="FoZUbQ.png" /></li>
<li>加权的系数还可以通过上下文动态计算。具体系数的计算有两种方法：
<ul>
<li>尝试各种系数，用在验证集上表现最好的系数组合</li>
<li>将系数看成是概率生成模型的隐变量，使用EM算法进行推断</li>
</ul></li>
</ul>
<h2 id="实际问题工具和数据格式">实际问题：工具和数据格式</h2>
<ul>
<li>在语言模型计算中，一般将概率取对数进行计算，原因有二：防止数值下溢；取对数能将累乘运算变成累加，加速计算。</li>
<li>回退N元语法模型一般采用ARPA格式。ARPA格式文件由一些头部信息和各类N元语法的列表组成，列表中包含了该类N元语法下所有语法，概率，和回退的归一化系数。只有能够称为高阶语法前缀的低阶语法才能在回退中被利用，并拥有归一化系数。</li>
<li>两种计算N元语法模型的工具包：SRILM toolkit 和Cambridge-CMU
toolkit</li>
</ul>
<h2 id="语言建模中的高级问题">语言建模中的高级问题</h2>
<h3 id="高级平滑方法kneser-ney平滑">高级平滑方法：Kneser-Ney平滑</h3>
<ul>
<li>注意到在GT打折法当中，打折之后估计的c值比最大似然估计得到的c值近似多出一个定值d。绝对打折法便考虑了这一点，在每个计数中减去这个d：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZwUs.png"
alt="FoZwUs.png" /></li>
<li>Kneser-Ney平滑吸收了这种观点，并且还考虑了连续性：在不同上文中出现的单词更有可能出现在新的上文之后，在回退时，我们应该优先考虑这种在多种上文环境里出现的词，而不是那些出现次数很多，但仅仅在特定上文中出现的词。
<img data-src="https://s2.ax1x.com/2019/01/03/FoZdEj.png"
alt="FoZdEj.png" /></li>
<li>在Kneser-Ney中，插值法能够比回退法取得更加好的效果： <img data-src="https://s2.ax1x.com/2019/01/03/FoZ05n.png" alt="FoZ05n.png" /></li>
</ul>
<h3 id="基于分类的n元语法">基于分类的N元语法</h3>
<ul>
<li><p>这种方法是为了解决训练数据的稀疏性。例如IBM聚类，每个单词只能属于一类，以二元语法为例，某个二元语法的条件概率的计算变为给定上文所在类，某个单词的条件概率，还可以进一步链式分解为两个类的条件概率乘以某个单词在给定其类条件下的条件概率：</p>
<p><span class="math display">\[
p(w_i│w_{i-1} ) \approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \cdot p(w_i
|c_i)
\]</span></p></li>
</ul>
<h3 id="语言模型适应和网络应用">语言模型适应和网络应用</h3>
<ul>
<li>适应是指在大型宽泛的语料库上训练语言模型，并在小的细分领域的语言模型上进一步改进。网络是大型语料库的一个重要来源。在实际应用时我们不可能搜索每一个语法并统计搜索得到所有页面上的所有语法，我们用搜索得到的页面数来近似计数。</li>
</ul>
<h3 id="利用更长距离的上文信息">利用更长距离的上文信息</h3>
<ul>
<li>通常我们使用二元和三元语法模型，但是更大的N能够带来更好的效果。为了捕捉更长距离的上文信息，有以下几种方法：
<ul>
<li>基于缓存机制的N元语法模型</li>
<li>基于主题建模的N元语法模型，对不同主题建模语言模型，再加权求和</li>
<li>不一定使用相邻的上文信息，例如skip
N-grams或者不一定使用定长的上文信息，例如变长N-grams</li>
</ul></li>
</ul>
<h1 id="第十六章语言的复杂性">第十六章：语言的复杂性</h1>
<h2 id="chomsky层级">Chomsky层级</h2>
<ul>
<li>Chomsky层级反映了不同形式化方法描述的语法之间的蕴含关系，较强生成能力或者说更复杂的语法在层级的外层。从外到内，加在可重写语法规则上的约束增加，语言的生成能力逐渐降低。
<img data-src="https://s2.ax1x.com/2019/01/03/FoZXad.png"
alt="FoZXad.png" /></li>
<li>五种语法对应的规则和应用实例： <img data-src="https://s2.ax1x.com/2019/01/03/Foepxf.png" alt="Foepxf.png" />
<ul>
<li>0型语法：规则上只有一个限制，即规则左侧不能为空字符串。0型语法刻画了递归可枚举语言</li>
<li>上下文相关语法：可以把上下文，</li>
<li>温和的上下文相关语法</li>
<li>上下文无关语法：可以把任何单独的非终极符号重写为由终极符号和非终极符号构成的字符串，也可以重写为空字符串</li>
<li>正则语法：可以是右线性也可以是左线性，以右线性为例，非终极符号可以重写为左边加了若干终极符号的另一个非终极符号，右线性不断地在字符串左侧生成终极符号。</li>
</ul></li>
</ul>
<h2 id="自然语言是否正则">自然语言是否正则</h2>
<ul>
<li>判断语言是否正则能够让我们了解应该用哪一层次的语法来描述一门语言，且这个问题能够帮助我们了解自然语言的不同方面的某些形式特性。</li>
<li>抽吸引理：用来证明一门语言不是正则语言。
<ul>
<li>如果一门语言可以被有限状态自动机来描述，则与自动机对应有一个记忆约束量。这个约束量对于不同的符号串不会增长的很大，因为其状态数目是固定的，更长的符号串应该是通过状态之间转移产生而不是增加状态数目。因此这个记忆量不一定和输入的长度成比例。</li>
<li>如果一个正则语言能够描述任意长的符号序列，比自动机的状态数目还多，则该语言的自动机中必然存在回路。
<img data-src="https://s2.ax1x.com/2019/01/03/FoZxPI.png"
alt="FoZxPI.png" /></li>
</ul></li>
<li>如图所示自动机，可以表述xyz,xyyz,xyyyz.....，当然也可以将中间无限长的y序列“抽吸掉”，表述xz。抽吸引理表述如下：</li>
<li>设L是一个有限的正则语言，那么必然存在符号串x,y,z,使得对于任意n≥0，y≠<span
class="math inline">\(\epsilon\)</span>，且xy^n z∈L</li>
<li>即假如一门语言是正则语言，则存在某一个符号串y，可以被适当的“抽吸”。这个定理是一门语言是正则语言的必要非充分条件。</li>
<li>有学者证明英语不是一门正则语言：
<ul>
<li>具有镜像性质的句子通过抽吸原理可以证明不是正则语言，而英语中一个特殊的子集合和这种镜像性质的句子是同态的。</li>
<li>另一种证明基于某些带有中心-嵌套结构的句子。这种句子可以由英语和某一类简单的正则表达式相交得到，通过抽吸原理可以得到这种句子不是正则语言。英语和正则语言的交不是正则语言，则英语不是正则语言。</li>
</ul></li>
</ul>
<h2 id="自然语言是否上下文无关">自然语言是否上下文无关</h2>
<ul>
<li>既然自然语言不是正则语言，我们接着考虑更宽松的限定，自然语言是否是上下文无关的？</li>
<li>不是......</li>
</ul>
<h2 id="计算复杂性和人的语言处理">计算复杂性和人的语言处理</h2>
<ul>
<li>人对中心嵌套句子处理很困难，因为人们剖析时利用的栈记忆有限，且栈中不同层次记忆容易混淆。</li>
</ul>
<h1 id="第五章词类标注">第五章：词类标注</h1>
<ul>
<li>各种表述：POS（Part Of Speech）、word classes（词类）、morphological
classes（形态类）、lexical tags（词汇标记）。</li>
<li>POS的意义在于：
<ul>
<li>能够提供关于单词及其上下文的大量信息。</li>
<li>同一单词在不同词类下发音不同，因此POS还能为语音处理提供信息。</li>
<li>进行词干分割（stemming），辅助信息检索</li>
</ul></li>
<li>本章介绍三种词类标注算法：
<ul>
<li>基于规则的算法</li>
<li>基于概率的算法，隐马尔科夫模型</li>
<li>基于变换的算法</li>
</ul></li>
</ul>
<h2 id="一般词类">一般词类</h2>
<ul>
<li>POS分为封闭集和开放集，封闭集集合相对稳定，例如介词，开放集的词语则不断动态扩充，例如名词和动词。特定某个说话人或者某个语料的开放集可能不同，但是所有说一种语言以及各种大规模语料库可能共享相同的封闭集。封闭集的单词称为虚词（功能词，function
word），这些词是语法词，一般很短，出现频次很高。</li>
<li>四大开放类：名词、动词、形容词、副词。</li>
<li>名词是从功能上定义的而不是从语义上定义的，因此名词一般表示人、地点、事物，但既不充分也不必要。定义名词：
<ul>
<li>与限定词同时出现</li>
<li>可以受主有代词修饰</li>
<li>大多数可以以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词</li>
</ul></li>
<li>动词，表示行为和过程的词，包括第三人称单数、非第三人称单数、进行时、过去分词几种形态</li>
<li>形容词，描述性质和质量</li>
<li>副词，用于修饰，副词可以修饰动词、动词短语、其它副词。</li>
<li>英语中的一些封闭类：
<ul>
<li>介词 prepositions：出现在名词短语之前，表示关系</li>
<li>限定词 determiners 冠词 articles：与有定性（definiteness）相关</li>
<li>代词 pronouns：简短的援引某些名词短语、实体、或事件的一种形式</li>
<li>连接词 conjunctions：用于连接和补足（complementation）</li>
<li>助动词 auxiliary
verbs：标志主要动词的某些语义特征，包括：时态、完成体、极性对立、情态</li>
<li>小品词 particles：与动词结合形成短语动词</li>
<li>数词 numerals</li>
</ul></li>
</ul>
<h2 id="词类标注">词类标注</h2>
<ul>
<li>标注算法的输入是单词的符号串和标记集，输出要让每一个单词标注上一个单独且最佳的标记。如果每个单词只对应一种词性，那么根据已有的标记集，词类标注就是一个简单的查表打标的过程，但是很多词存在多种词性，例如book既可以是名词也可以是动词，因此要进行消歧，词类标注是歧义消解的一个重要方面。</li>
</ul>
<h2 id="基于规则的词类标注">基于规则的词类标注</h2>
<ul>
<li>介绍了ENGTWOL系统，根据双层形态学构建，对于每一个词的每一种词类分别立条，计算时不计屈折形式和派生形式.</li>
<li>标注算法的第一阶段是将单词通过双层转录机，得到该单词的所有可能词类</li>
<li>之后通过施加约束规则排除不正确的词类。这些规则通过上下文的类型来决定排除哪些词类。</li>
</ul>
<h2 id="基于隐马尔科夫模型的词类标注">基于隐马尔科夫模型的词类标注</h2>
<ul>
<li><p>使用隐马尔科夫模型做词类标注是一类贝叶斯推断，这种方法将词类标注看成是序列分类任务。观察量为一个词序列（比如句子），任务是给这个序列分配一个标注序列。</p></li>
<li><p>给定一个句子，贝叶斯推断想要在所有标注序列可能中选择最好的一个序列，即</p>
<p><span class="math display">\[
{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)
\]</span></p></li>
<li><p>使用贝叶斯法则将其转化为：</p>
<p><span class="math display">\[
{t_1^n} _{best}={argmax}
_{t_1^n}  \frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n}
P(w_1^n│t_1^n)P(t_1^n)
\]</span></p></li>
<li><p>隐马尔科夫模型在此基础上做了两点假设</p>
<ul>
<li>一个词出现的概率只与该词的词类标注有关，与上下文其他词和其他标注无关，从而将序列的联合概率拆解为元素概率之积，即：P(w_1<sup>n│t_1</sup>n)
_{i=1}^n P(w_i |t_i)</li>
<li>一个标注出现的概率只与前一个标注相关，类似于二元语法的假设：P(t_1^n
) <em>{i=1}^n P(t_i |t</em>{i-1})</li>
</ul></li>
<li><p>在两种假设下简化后的最好标注序列表达式为：</p>
<p><span class="math display">\[
{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \approx {argmax}
_{t_1^n} \prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})
\]</span></p></li>
<li><p>上面这个概率表达式实际上将HMM模型的联合概率拆成了各个部分转移概率的乘积，具体而言分为标签转移概率（隐变量之间转移）和词似然（隐变量转移到可观察变量）。通过最大似然估计，我们可以通过古典概型的方法从已标注的语料中计算出这两类概率：</p>
<p><span class="math display">\[
P(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\
P(w_i│t_i ) = \frac{C(t_i,w_i)}{C(t_i)} \\
\]</span></p></li>
<li><p>一个例子：HMM模型如何正确的将下句中的race识别为动词而不是名词：</p></li>
<li><p>Secretariat is expected to race tomorrow.</p></li>
<li><p>画出上句中race被识别为动词和名词两种情况下的HMM模型，可以看到两个模型对比只有三个转移概率不同，用加粗线标出：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZDCq.png"
alt="FoZDCq.png" /></p></li>
<li><p>HMM词类标注器消歧的方式是全局的而不是局部的。我们在语料中统计得到这三种转移概率，再累乘，结果是(a)的概率是(b)概率的843倍。显然race应该被标注为动词。</p></li>
</ul>
<h2 id="形式化隐马尔科夫模型标注器">形式化隐马尔科夫模型标注器</h2>
<ul>
<li><p>HMM模型是有限自动机的扩展，具体而言是一种加权有限自动机，马尔可夫链的扩展，这种模型允许我们考虑观察量和隐变量，考虑包含隐变量的概率模型。HMM包含以下组件：</p>
<ul>
<li>Q：大小为N的状态集</li>
<li>A：大小为N*N的转移概率矩阵</li>
<li>O：大小为T的观察事件集</li>
<li>B：观察似然序列，又叫发射概率，<span class="math inline">\(b_i
(o_t)\)</span>描述了从状态i里生成观察o_t的概率</li>
<li><span
class="math inline">\(q_0，q_F\)</span>：特殊的起始状态和最终状态，没有相连接的观察量</li>
</ul></li>
<li><p>A中的概率和B中的概率对应着之前式子中每一个累乘项里的先验<span
class="math inline">\(P(w_i│t_i )\)</span>和似然<span
class="math inline">\(P(t_i |t _{i-1})\)</span>概率：</p>
<p><span class="math display">\[
{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \approx {argmax}
_{t_1^n} \prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})
\]</span></p></li>
</ul>
<h2 id="hmm标注的维特比算法">HMM标注的维特比算法</h2>
<ul>
<li>在HMM模型中，已知转移概率和观察序列，求隐变量的任务叫做解码。解码的一种算法即维特比算法，实质上是一种动态规划算法，与之前求最小编辑距离的算法类似。</li>
<li>首先我们从语料中计算得到A和B两个矩阵，即模型的转移概率已知，对于给定的观察序列，按照以下步骤执行维特比算法：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZyvT.png"
alt="FoZyvT.png" /></li>
<li>算法维护一个<span
class="math inline">\((N+2)*T\)</span>的概率矩阵viterbi，加了2代表初始状态和结束状态，viterbi[s,t]代表了在第t步状态为s时的最佳路径概率，而backpointer[s,t]对应着保存了该最佳路径的上一步是什么状态，用于回溯输出整个最佳路径。</li>
<li>关键的转移在于<span class="math inline">\(viterbi[s,t] \leftarrow
max _{s^{*}=1}^N⁡ viterbi[s^{*},t-1] * a_{s^{*},s} * b_s
(o_t)\)</span>即当前时间步最佳路径是由上一时间步各个状态的最佳路径转移过来的，选择上一步最佳路径概率与转移概率乘积最大的路径作为当前时间步的最佳路径。从动态规划的角度而言，即长度为t的最佳路径，必定是从长度为t-1的最佳路径里选择一条转移得到，否则肯定可以从另一条概率更大的路径转移获得更优解。这样就限制了最佳路径的生成可能，减少了计算量。</li>
</ul>
<h2 id="将hmm算法扩展到三元语法">将HMM算法扩展到三元语法</h2>
<ul>
<li><p>现代的HMM标注器一般在标注转移概率上考虑更长的上文历史：</p>
<p><span class="math display">\[
P(t_1^n ) \approx \prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})
\]</span></p></li>
<li><p>这样的话需要在序列开头和结尾做一些边界处理。使用三元语法的一个问题是数据稀疏：例如我们从没有在训练集中见过标注序列PRP
VB TO，则我们无法计算P(TO|PRP,VB)。一种解决办法是线性插值：</p>
<p><span class="math display">\[
P(t_i│t _{i-1} t _{i-2} ) = \lambda _1 P ̂(t_i│t _{i-1} t _{i-2}
)+\lambda _2 P ̂(t_i│t _{i-1} )+\lambda _3 P ̂(t_i)
\]</span></p></li>
<li><p>使用删除插值的办法确定系数<span
class="math inline">\(\lambda\)</span>： <img data-src="https://s2.ax1x.com/2019/01/03/FoZr80.png"
alt="FoZr80.png" /></p></li>
</ul>
<h2 id="基于变换的标注">基于变换的标注</h2>
<ul>
<li>基于变换的方法结合了基于规则和基于概率方法的优点。基于变换的方法依然需要规则，但是从数据中总结出规则，是一种监督学习方法，称为基于变换的学习（Transformation
Based
Learning，TBL）。在TBL算法中，语料库首先用比较宽的规则来标注，然后再选择稍微特殊的规则来修改，接着再使用更窄的规则来修改数量更少的标记。</li>
</ul>
<h2 id="如何应用tbl规则">如何应用TBL规则</h2>
<ul>
<li>首先应用最宽泛的规则，就是根据概率给每个词标注，选择概率最大的词类作为标注。之后应用变换规则，即如果满足某一条件，就将之前标注的某一词类变换（纠正）为正确的词类，之后不断应用更严格的变换，在上一次变换的基础上进行小部分的修改。</li>
<li>如何学习到TBL规则
<ul>
<li>首先给每个词打上最可能的标签</li>
<li>检查每一个可能的变换，选择效果提升最多的变换，此处需要直到每一个词正确的标签来衡量变换带来的提升效果，因此是监督学习。</li>
<li>根据这个被选择的变换给数据重新打标，重复步骤2，直到收敛（提升效果小于某一阈值）</li>
</ul></li>
<li>以上过程输出的结果是一有序变换序列，用来组成一个标注过程，在新语料上应用。虽然可以穷举所有的规则，但是那样复杂度太高，因此我们需要限制变换集合的大小。解决方案是设计一个小的模板集合（抽象变换）,每一个允许的变换都是其中一个模板的实例化。</li>
</ul>
<h2 id="评价和错误分析">评价和错误分析</h2>
<ul>
<li>一般分为训练集、验证集、测试集，在训练集内做十折交叉验证。</li>
<li>与人类标注的黄金标准比较计算准确率作为衡量指标。</li>
<li>一般用人类表现作为ceiling，用一元语法最大概率标注的结果作为baseline。</li>
<li>通过含混矩阵或者列联表来进行错误分析。在N分类任务中，一个N*N的含混矩阵的第i行第j列元素指示第i类被错分为第j类的次数在总分错次数中的占比。一些常见的容易分错的词性包括：
<ul>
<li>单数名词、专有名词、形容词</li>
<li>副词、小品词、介词</li>
<li>动词过去式、动词过去分词、形容词</li>
</ul></li>
</ul>
<h2 id="词性标注中的一些其他问题">词性标注中的一些其他问题</h2>
<ul>
<li>标注不确定性：一个词在多个词性之间存在歧义，很难区分。这种情况下有些标注器允许一个词被打上多个词性标注。在训练和测试的时候，有三种方式解决这种多标注词：
<ul>
<li>通过某种方式从这些候选标注中选择一个标注</li>
<li>训练时指定一个词性，测试时只要打上了候选词性中任意一个就认为标注正确</li>
<li>将整个不确定的词性集看成一个新的复杂词性</li>
</ul></li>
<li>多部分词：在标注之前需要先分词，一些多部分词是否应该被分为一部分，例如New
York City应该分成三部分还是一个整体，也是各个标注系统需要考虑的。</li>
<li>未知词：不在词典中的词称为未知词。对于未知词，训练集无法给出它的似然P(w_i
|t_i)，可以通过以下几种方式解决：
<ul>
<li>只依赖上下文的POS信息预测</li>
<li>用只出现一次的词来估计未知词的分布，类似于Good Turing打折法</li>
<li>使用未知词的单词拼写信息，正词法信息。例如连字符、ed结尾、首字母大写等特征。之后在训练集中计算每个特征的似然，并假设特征之间独立，然后累乘特征似然作为未知词的似然：<span
class="math inline">\(P(w_i│t_i )=p(unknown word│t_i ) * p(capital│t_i )
* p(endings/hyph|t_i)\)</span></li>
<li>使用最大熵马尔可夫模型</li>
<li>使用对数线性模型</li>
</ul></li>
</ul>
<h2 id="噪声信道模型">噪声信道模型</h2>
<ul>
<li>贝叶斯推断用于标注可以认为是一种噪声信道模型的应用，本节介绍如何用噪声信道模型来完成拼写纠正任务。
之前对于非单词错误，通过词典查找可以检测到错误，并根据最小编辑距离纠正错误，但这种方法对于真实单词错误无能为力。噪声信道模型可以纠正这两种类型的拼写错误。</li>
<li>噪声信道模型的动机在于将错误拼写的单词看成是一个正确拼写的单词经过一个噪声信道时受到干扰扭曲得到。我们尝试所有可能的正确的词，将其输入信道，最后得到的干扰之后的词与错误拼写的词比较，最相似的例子对应的输入词就认为是正确的词。这类噪声信道模型，比如之前的HMM标注模型，是贝叶斯推断的一种特例。我们看到一个观察两（错误拼写词）并希望找到生成这个观察量的隐变量（正确拼写词），也就是找最大后验。</li>
<li>将噪声信道模型应用于拼写纠正：首先假设各种拼写错误类型，错拼一个、错拼两个、漏拼一个等，然后产生所有可能的纠正，除去词典中不存在的，最后分别计算后验概率，选择后验概率最大的作为纠正。其中需要根据局部上下文特征来计算似然。</li>
<li>另一种纠正算法是通过迭代来改进的方法：先假设拼写纠正的含混矩阵是均匀分布的，之后根据含混矩阵运行纠正算法，根据纠正之后的数据集更新含混矩阵，反复迭代。这种迭代的算法是一种EM算法。</li>
</ul>
<h2 id="根据上下文进行拼写纠正">根据上下文进行拼写纠正</h2>
<ul>
<li>即真实单词拼写错误的纠正。为了解决这类任务需要对噪声信道模型进行扩展：在产生候选纠正词时，需要包括该单词本身以及同音异形词。之后根据整个句子的最大似然来选择正确的纠正词。</li>
</ul>
<h1
id="第六章隐马尔科夫模型和最大熵模型">第六章：隐马尔科夫模型和最大熵模型</h1>
<ul>
<li>隐马尔科夫模型用来解决序列标注（序列分类问题）。</li>
<li>最大熵方法是一种分类思想，在满足给定条件下分类应满足限制最小（熵最大），满足奥卡姆剃刀原理。</li>
<li>最大熵马尔可夫模型是最大熵方法在序列标注任务上的扩展。</li>
</ul>
<h2 id="马尔可夫链">马尔可夫链</h2>
<ul>
<li><p>加权有限自动状态机是对有限自动状态机的扩展，每条转移路径上加上了概率作为权重，说明从这条路径转移的可能性。马尔可夫链是加权有限状态自动机的一种特殊情况，其输入序列唯一确定了自动机会经过的状态序列。马尔可夫链只能对确定性序列分配概率。</p></li>
<li><p>我们将马尔可夫链看作一种概率图模型，一个马尔可夫链由下面的成分确定：</p>
<p><span class="math display">\[
Q=q_1 q_2…q_N \\
A=a_{01} a_{02} … a_{n1} … a_{nn} \\
q_0,q_F \\
\]</span></p></li>
<li><p>分别是</p>
<ul>
<li>状态集合</li>
<li>转移概率矩阵，其中a_ij代表了从状态i转移到状态j的概率<span
class="math inline">\(P(q_j |q_i)\)</span></li>
<li>特殊的开始状态和结束状态</li>
</ul></li>
<li><p>概率图表示将状态看成图中的点，将转移看成边。</p></li>
<li><p>一阶马尔可夫对转移做了很强的假设：某一状态的概率只与前一状态相关：</p>
<p><span class="math display">\[
P(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})
\]</span></p></li>
<li><p>马尔可夫链的另一种表示不需要开始和结束状态：</p>
<p><span class="math display">\[
\pi = \pi _1,\pi _2 , … , \pi _N \\
QA={q_x,q_y…} \\
\]</span></p></li>
<li><p>分别是：</p>
<ul>
<li>状态的初始概率分布，马尔可夫链以概率<span class="math inline">\(\pi
_i\)</span>从状态i开始</li>
<li>集合QA是Q的子集，代表合法的接受状态</li>
</ul></li>
<li><p>因此状态1作为初始状态的概率既可以写成<span
class="math inline">\(a_{01}\)</span>也可以写成<span
class="math inline">\(\pi _1\)</span>。</p></li>
</ul>
<h2 id="隐马尔科夫模型">隐马尔科夫模型</h2>
<ul>
<li>当马尔可夫链已知时，我们可以用其计算一个观测序列出现的概率。但是观测序列可能依赖于一些不可观测的隐变量，我们可能感兴趣的是推断出这些隐变量。隐马尔科夫模型允许我们同时考虑观测变量和隐变量。</li>
<li>如之前一样定义隐马尔科夫模型：
<ul>
<li>Q：大小为N的状态集</li>
<li>A：大小为N*N的转移概率矩阵</li>
<li>O：大小为T的观察事件集</li>
<li>B：观察似然序列，又叫发射概率，<span class="math inline">\(b_i
(o_t)\)</span>描述了从状态i里生成观察<span
class="math inline">\(o_t\)</span>的概率</li>
<li><span
class="math inline">\(q_0，q_F\)</span>：特殊的起始状态和最终状态，没有相连接的观察量</li>
</ul></li>
<li>同样的，隐马尔科夫也可以用另一种不依赖初始和结束状态的方式表示。隐马尔科夫模型也做了两个假设，分别是隐状态之间转移和隐状态到观察量转移的一阶马尔可夫性。</li>
<li>对于隐马尔科夫模型需要解决三类问题：
<ul>
<li>似然计算：已知参数和观测序列，求似然<span
class="math inline">\(P(O|\lambda)\)</span></li>
<li>解码：已知参数和观测序列，求隐状态序列</li>
<li>学习：已知观测序列和隐状态集合，求解模型参数</li>
</ul></li>
</ul>
<h2 id="计算似然前向算法">计算似然：前向算法</h2>
<ul>
<li><p>对于马尔可夫链，其没有隐状态到观测量的转移概率矩阵，可以看成观察量与隐状态相同。在隐马尔科夫模型中不能直接计算似然，我们需要直到隐状态序列。</p></li>
<li><p>先假设隐状态序列已知，则似然计算为：</p>
<p><span class="math display">\[
P(O│Q) = \prod _{i=1}^T P(o_i |q_i)
\]</span></p></li>
<li><p>根据隐状态转移的一阶马尔可夫性，可以求得隐状态的先验，乘以似然得到观测序列和隐状态序列的联合概率：</p>
<p><span class="math display">\[
P(O,Q)=P(O│Q) * P(Q) = \prod _{i=1}^n P(o_i│q_i )  \prod _{i=1}^n P(q_i
|q _{i-1})
\]</span></p></li>
<li><p>对于联合概率积分掉隐状态序列，就可以得到观测概率的似然：</p>
<p><span class="math display">\[
P(O) = \sum _Q P(O,Q) = \sum _Q P(O|Q)P(Q)
\]</span></p></li>
<li><p>这样计算相当于考虑了所有的隐状态可能，并对每一种可能从隐状态序列开始到结束计算一次似然，实际上可以保留每次计算的中间状态来减少重复计算，也就是动态规划。在前向计算HMM观测似然使用的动态规划算法称为前向算法：</p>
<ul>
<li><p>令<span class="math inline">\(\alpha _t
(j)\)</span>代表在得到前t个观测量之后当前时刻隐变量处于状态j的概率,：</p>
<p><span class="math display">\[
\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\lambda)
\]</span></p></li>
<li><p>这个概率值可以根据前一时间步的，避免了每次从头开始计算：</p>
<p><span class="math display">\[
\alpha _t (j) = \sum _{i=1}^N \alpha _{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>初始化<span class="math inline">\(\alpha _1 (j)\)</span>：</p>
<p><span class="math display">\[
\alpha _1 (j)=a_{0s} b_s (o_1)
\]</span></p></li>
<li><p>终止状态：</p>
<p><span class="math display">\[
P(O│\lambda) = \alpha _T (q_F) = \sum _{i=1}^N \alpha _T (i) \alpha
_{iF}
\]</span></p></li>
</ul></li>
</ul>
<h2 id="解码维特比算法">解码：维特比算法</h2>
<ul>
<li>解码任务是根据观测序列和参数推断出最有可能隐状态序列。最朴素的做法：对于每种可能的隐状态序列，计算观测序列的似然，取似然最大时对应的隐状态序列。但是这样做就如同朴素的计算似然方法一样，时间复杂度过高，同样的，我们使用动态规划来缩小求解的规模。在解码时使用了一种维特比算法。
<ul>
<li><p>令<span class="math inline">\(v_t
(j)\)</span>代表已知前t个观测量（1<sub>t）和已知前t个隐状态（0</sub>t-1）的条件下，当前时刻隐状态为j的概率：</p>
<p><span class="math display">\[
v_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 …
o_t,q_t=j|\lambda)
\]</span></p></li>
<li><p>其中我们已知了前t个时间步最大可能的隐状态序列，这些状态序列也是通过动态规划得到的：</p>
<p><span class="math display">\[
v_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>为了得到最佳的隐状态序列，还需要记录每一步的最佳选择，方便回溯得到路径：</p>
<p><span class="math display">\[
{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)
\]</span></p></li>
<li><p>初始化：</p>
<p><span class="math display">\[
v_1 (j) = a_{0j} b_j (o_1) \ \  1 \leq j \leq N \\
{bt}_1 (j) = 0 \\
\]</span></p></li>
<li><p>终止，分别得到最佳隐状态序列（回溯开始值）及其似然值：</p>
<p><span class="math display">\[
P * = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) * a_{i,F} \\
q_{T*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) * a_{i,F} \\
\]</span></p></li>
</ul></li>
<li>维特比算法减小时间复杂度的原因在于其并没有计算所有的隐状态路径，而是利用了每一时间步的最佳路径只能从上一时间步的最佳路径中延伸而来这一条件，减少了路径候选，避免了许多不必要的路径计算。并且每一步利用上一步的结果也是用了动态规划的思想减少了计算量。</li>
</ul>
<h2
id="训练隐马尔科夫模型前向后向算法">训练隐马尔科夫模型：前向后向算法</h2>
<ul>
<li><p>学习问题是指已知观测序列和隐状态集合，求解模型参数。</p></li>
<li><p>前向后向算法，又称Baum-Welch算法，是EM算法的一种特例，用来求解包含隐变量的概率生成模型的参数。该算法通过迭代的方式反复更新转移概率和生成概率，直到收敛。BW算法通过设计计数值之比作为隐变量，将转移概率矩阵和生成概率矩阵一起迭代更新。</p></li>
<li><p>先考虑马尔科夫链的学习问题。马尔科夫链可以看作是退化的隐马尔科夫模型，即每个隐变量只生成和自己一样的观测量，生成其他观测量的概率为0。因此只需学习转移概率。</p></li>
<li><p>对于马尔可夫链，可以通过古典概型统计出转移概率：</p>
<p><span class="math display">\[
a_{ij} = \frac {Count(i \rightarrow j)} {\sum _{q \in Q} Count(i
\rightarrow q)}
\]</span></p></li>
<li><p>我们可以这样直接计算概率是因为在马尔可夫链中我们知道当前所处的状态。对于隐马尔科夫模型我们无法这样直接计算是因为对于给定输入，隐状态序列无法确定。Badum-Welch算法使用了两种简洁的直觉来解决这一问题：</p>
<ul>
<li>迭代估计，先假设一种转移概率和生成概率，再根据假设的概率推出更好的概率</li>
<li>计算某一观测量的前向概率，并将这个概率分摊到不同的路径上，通过这种方式估计概率</li>
</ul></li>
<li><p>首先类似于前向概率，我们定义后向概率：</p>
<ul>
<li><p>令<span class="math inline">\(\beta _t
(i)\)</span>代表在得到后t个观测量之后当前时刻隐变量处于状态i的概率,<span
class="math inline">\(\lambda\)</span>为模型参数：</p>
<p><span class="math display">\[
\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\lambda)
\]</span></p></li>
<li><p>类似于后向概率的归纳计算：</p>
<p><span class="math display">\[
\beta_t (i) = \sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \beta _{t+1} (j),  \
\   1≤i≤N,1≤t&lt;T
\]</span></p></li>
<li><p>初始化<span class="math inline">\(\alpha _1 (j)\)</span>：</p>
<p><span class="math display">\[
\beta _T (i)=\alpha _(i,F)
\]</span></p></li>
<li><p>终止状态：</p>
<p><span class="math display">\[
P(O│\lambda)=\alpha _t (q_F )=\beta_1 (0)= \sum _{i=1}^N a_{0j} b_j
(o_1) \beta _1 (j)
\]</span></p></li>
</ul></li>
<li><p>类似的，我们希望马尔可夫链中的古典概率能帮助我们估计转移概率：</p>
<p><span class="math display">\[
a_{ij}^{*} =
\frac{从状态i转移到状态j的计数值期望}{从状态i转移出去的计数值期望}
\]</span></p></li>
<li><p>如何估计计数值：我们将整个序列的转移路径计数值转化为时间步之间转移路径计数值之和，时间步之间某一条转移路径的概率为：</p>
<p><span class="math display">\[
P(q_t=i,q_{t+1}=j)
\]</span></p></li>
<li><p>首先考虑所有的观测序列和这一转移路径的联合概率（省略了以参数<span
class="math inline">\(\lambda\)</span>为条件）：</p>
<p><span class="math display">\[
P(q_t=i,q_{t+1}=j,O)
\]</span></p></li>
<li><p>观察下面的概率图： <img data-src="https://s2.ax1x.com/2019/01/03/FoZWVJ.png"
alt="FoZWVJ.png" /></p></li>
<li><p>可以看到这一联合概率包含了三个部分：</p>
<ul>
<li>T时刻隐状态为i的前向概率</li>
<li>T+1时刻隐状态为j的后向概率</li>
<li>T时刻与T+1时刻的状态转移概率以及生成对应观测量的生成概率</li>
</ul></li>
<li><p>所以有：</p>
<p><span class="math display">\[
P(q_t=i,q_{t+1}=j,O)=\alpha _t (i) a_{ij} b_j (o_{t+1} ) \beta _{t+1}
(j)
\]</span></p></li>
<li><p>为了从联合分布中得到已知观测序列求转移路径的联合概率，需要计算观测序列的概率，可以通过前向概率或者后向概率求得：</p>
<p><span class="math display">\[
P(O)=\alpha _t (N)=\beta _T (1) = \sum _{j=1}^N \alpha _t (j) \beta_t
(j)
\]</span></p></li>
<li><p>最终得到</p>
<p><span class="math display">\[
ξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \frac {(\alpha _t (i) a_{ij} b_j
(o_{t+1} ) \beta_{t+1} (j))}{(\alpha _t (N))}
\]</span></p></li>
<li><p>最后，对所有时间步求和就可以得到从状态i转移到状态j的期望计数值，从而进一步得到转移概率的估计：</p>
<p><span class="math display">\[
a_{ij}^{*} = \frac {\sum _{t=1}^{T-1} ξ_t (i,j)}{\sum _{t=1}^{T-1} \sum
_{j=1}^{N-1} ξ_t (i,j)}
\]</span></p></li>
<li><p>同样的，我们还希望得到生成概率的估计：</p>
<p><span class="math display">\[
b_{j}^{*} (v_k) = \frac {在状态j观测到符号v_k
的计数值期望}{状态j观测到所有符号的计数值期望}
\]</span></p></li>
<li><p>类似的，通过先计算联合分布再计算条件分布的方式得到在t时刻处于隐状态j的概率：</p>
<p><span class="math display">\[
γ_t (j)=P(q_t=j│O) = \frac {P(q_t=j,O)}{P(O)}
\]</span></p></li>
<li><p>联合概率包含两个部分，即t时刻处于状态j的前向概率和后向概率，所以有：</p>
<p><span class="math display">\[
γ_t (j) = \frac {\alpha _t (j) \beta_t (j)}{\alpha _t (N)}
\]</span></p></li>
<li><p>类似的，对所有时间步累加，进而得到生成概率的估计：</p>
<p><span class="math display">\[
b_{j}^{*} (v_k) = \frac{\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) }{\sum
_{t=1}^T   γ_t (j) }
\]</span></p></li>
<li><p>这两个式子是在已知前向概率和后向概率<span
class="math inline">\((\alpha,\beta)\)</span>的情况下，计算出中间变量（隐变量）(ξ,γ),引入隐变量的动机是将a、b估计值的期望计数值之比转化为概率之比，且这两个隐变量可以用a,b表示。再由隐变量计算出转移概率和生成概率，因此形成了一个迭代的循环，可以用EM算法求解：</p>
<p><span class="math display">\[
a,b→\alpha,\beta→ξ,γ→a,b
\]</span></p></li>
<li><p>E-step:</p>
<p><span class="math display">\[
γ_t (j) = (\alpha _t (j) \beta_t (j))/(\alpha _t (N)) ξ_t (i,j) \\
= (\alpha _t (i) a_{ij} b_j (o_{t+1} ) \beta_{t+1} (j))/(\alpha _t (N))
\\
\]</span></p></li>
<li><p>M-step（最大化的目标是什么）:</p>
<p><span class="math display">\[
a _{ij} = (\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\sum _{t=1}^{T-1} \sum
_{j=1}^{N-1}   ξ_t (i,j)  ) \\
b ̂_j(v_k) = (\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\sum
_{t=1}^T   γ_t (j) ) \\
\]</span></p></li>
<li><p>迭代时需重新计算：</p>
<p><span class="math display">\[
\alpha _t (j) = \sum _{i=1}^N   \alpha_{t-1} (i) a_ij b_j (o_t) \\
\beta_t (i) = \sum _{j=1}^N   a_ij b_j (o_{t+1} ) \beta_{t+1} (j)  \\
\]</span></p></li>
<li><p>迭代的初始状态对于EM算法来说很重要，经常是通过引入一些外部信息来设计一个好的初始状态。</p></li>
</ul>
<h2 id="最大熵模型背景">最大熵模型：背景</h2>
<ul>
<li><p>最大熵模型另一种广为人知的形式是多项Logistic回归（Softmax?）。</p></li>
<li><p>最大熵模型解决分类问题，最大熵模型作为一种概率分类器，能够根据样本的特征求出样本属于每一个类别的概率，进而进行分类。</p></li>
<li><p>最大熵模型属于指数家族（对数线性）分类器，通过将特征线性组合，取指数得到分类概率：</p>
<p><span class="math display">\[
p(c│x)=\frac 1Z exp⁡(\sum _i   weight_i feature_i)
\]</span></p></li>
<li><p>Z是一个归一化系数，使得生成的概率之和为1。</p></li>
</ul>
<h2 id="最大熵建模">最大熵建模</h2>
<ul>
<li><p>将二分类Logistic回归推广到多分类问题就得到：</p>
<p><span class="math display">\[
P(c│x) = \frac {exp⁡(\sum _(i=0)^N   w_ci f_i) } {\sum _{c^{*} in
C}   exp⁡(\sum _{i=0}^N   w_{c^{*} i} f_i)  }
\]</span></p></li>
<li><p>语音和语言处理中的特征通常是二值的（是否有该特征），因此使用指示函数表示特征</p>
<p><span class="math display">\[
P(c│x) = \frac {exp⁡(\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\sum _{c^{*}
\in C}   exp⁡(\sum _{i=0}^N   w_{c^{*} i} f_i (c^{*},x))  }
\]</span></p></li>
<li><p>注意到在该模型中每一个类都有其独立的线性权重w_c。相比于硬分布，最大熵模型能够给出分到每一类的概率，因此可以求出每一时刻的分类概率进而求出整体分类概率，得到全局最优分类结果。注意到不同于支持向量机等模型，最大熵模型无法利用特征之间的组合，必须手动构造组合作为新的特征。</p></li>
<li><p>一般使用加了正则化的最大似然作为优化的目标函数：</p>
<p><span class="math display">\[
w ̂={argmax} _w \sum _i   \log P(y^{(i)}│x^{(i) } ) - \alpha \sum
_{j=1}^N w_j^2  
\]</span></p></li>
<li><p>这种正则化相当于给权重的概率分布加了一个零均值高斯先验，权重越偏离均值，即权重越大，其概率越低。</p></li>
<li><p>为什么多分类Logistic回归是最大熵模型：最大熵模型保证在满足给定约束下，无约束的部分分类应该是等概率分配，例如在两个约束下：</p>
<p><span class="math display">\[
P(NN)+P(JJ)+P(NNS)+P(VB)=1 \\
P(t_i=NN or t_i=NNS)=8/10 \\
\]</span></p></li>
<li><p>则满足这两个约束，最大熵模型分配的概率结果为：</p>
<p><span class="math display">\[
p(NN)=4/10  \\
p(JJ)=1/10  \\
p(NNS)=4/10  \\
p(VB)=1/10 \\
\]</span></p></li>
<li><p>在The equivalence of logistic regression and maximum entropy
models一文中证明了在广义线性回归模型的平衡条件约束下，满足最大熵分布的非线性激活函数就是sigmoid，即logistic回归。</p></li>
</ul>
<h2 id="最大熵马尔可夫模型">最大熵马尔可夫模型</h2>
<ul>
<li><p>最大熵模型只能对单一观测量分类，使用最大熵马尔可夫模型可以将其扩展到序列分类问题上。</p></li>
<li><p>最大熵马尔可夫比隐马尔科夫模型好在哪儿？隐马尔科夫模型对于每个观测量的分类依赖于转移概率和生成概率，假如我们想要在标注过程中引入外部知识，则需要将外部知识编码进这两类概率中，不方便。最大熵马尔可夫模型能够更简单的引入外部知识。</p></li>
<li><p>在隐马尔科夫模型中我们优化似然，并且乘以先验来估计后验：</p>
<p><span class="math display">\[
T ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )   
\]</span></p></li>
<li><p>在最大熵隐马尔科夫模型中，我们直接计算后验。因为我们直接训练模型来分类，即最大熵马尔可夫模型是一类判别模型，而不是生成模型：</p>
<p><span class="math display">\[
T ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1})
\]</span></p></li>
<li><p>因此在最大熵隐马尔科夫模型中没有分别对似然和先验建模，而是通过一个单一的概率模型来估计后验。两者的区别如下图所示：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZgrF.png"
alt="FoZgrF.png" /></p></li>
<li><p>另外最大熵马尔可夫模型可以依赖的特征更多，依赖方式更灵活，如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZcKU.png"
alt="FoZcKU.png" /></p></li>
<li><p>用公式表示这一差别：</p>
<p><span class="math display">\[
HMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\
MEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\
\]</span></p></li>
<li><p>当估计单一转移概率（从状态q^{*}转移到状态q，产生观测量o）时，我们使用以下的最大熵模型：</p>
<p><span class="math display">\[
P(q│q^{*},o)=\frac{1}{Z(o,q^{*})} exp⁡(\sum _i   w_i f_i (o,q))
\]</span></p></li>
</ul>
<h2
id="最大熵马尔可夫模型的解码推断">最大熵马尔可夫模型的解码（推断）</h2>
<ul>
<li><p>MEMM同样使用维特比算法进行解码</p></li>
<li><p>使用维特比算法解码的通用框架是：</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j)
\]</span></p></li>
<li><p>在HMM模型中这一框架具体化为：</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t)
\]</span></p></li>
<li><p>在MEMM中直接将似然和先验替换为后验：</p>
<p><span class="math display">\[
v_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t)
\]</span></p></li>
</ul>
<h2 id="最大熵马尔可夫模型的训练">最大熵马尔可夫模型的训练</h2>
<ul>
<li>MEMM作为最大熵模型的推广，训练过程使用和最大熵模型一样的监督算法。如果训练数据的标签序列存在缺失，也可以通过EM算法进行半监督学习。</li>
</ul>
<h1 id="第十二章英语的形式语法">第十二章：英语的形式语法</h1>
<h2 id="组成性">组成性</h2>
<ul>
<li>英语中的单词是如何组成一个词组的呢？</li>
<li>换句话说，我们如何判断一些单词组合成了一个部分？一种可能是这种组合都能在相似的句法环境中出现，例如名词词组都能在一个动词之前出现。另一种可能依据来自于前置和后置结构，例如前置短语on
September
seventeenth可以放在句子的前面，中间或者后面，但是组合成这个短语的各个部分不能拆出来放在句子的不同位置，因此我们判断on
September seventeenth这三个词组成了一个短语。</li>
</ul>
<h2 id="上下文无关法则">上下文无关法则</h2>
<ul>
<li><p>上下文无关语法，简称CFG，又称为短语结构语法，其形式化方法等价于Backus-Naur范式。一个上下文无关语法包含两个部分：规则或者产生式，词表。</p></li>
<li><p>例如，用上下文无关语法描述名词词组，一种描述方式是名词词组可以由一个专有名词构成，也可以由一个限定词加一个名词性成分构成，而名词性成分可以是一个或多个名词，此CFG的规则为：</p>
<ul>
<li>NP→Det Nominal</li>
<li>NP→ProperNoun</li>
<li>Nominal→Noun|Noun Nominal</li>
</ul></li>
<li><p>CFG可以层级嵌套，因此上面的规则可以与下面表示词汇事实的规则（词表）结合起来：</p>
<ul>
<li>Det→a</li>
<li>Det→the</li>
<li>Noun→flight</li>
</ul></li>
<li><p>符号分为两类：</p>
<ul>
<li>终极符号：与现实中单词对应的符号，词表是引入终极符号的规则的集合</li>
<li>非终极符号：表示终极符号的聚类或者概括性符号</li>
</ul></li>
<li><p>在每个规则里箭头右边包含一个或多个终极符号和非终极符号，箭头左边为一个非终极符号，与每个单词相关联的是其词类范畴（词类）。</p></li>
<li><p>CFG既可以看成是生成句子的一种机制，也可以看成是给一个句子分配结构的机制。</p></li>
<li><p>以之前提到的CFG为例，对一个符号串NP，可以逐步生成：</p>
<p><span class="math display">\[
NP→Det Nominal→Det Noun→a flight
\]</span></p></li>
<li><p>称 a flight是NP的一个推导，一般用一个剖析树表示一种推导： <img data-src="https://s2.ax1x.com/2019/01/03/FoZ5P1.png" alt="FoZ5P1.png" />
一个CFG定义了一个形式语言，形式语言是符号串的集合，如果有一个语法推导出的句子处于由该语法定义的形式语言中，这个句子就是合语法的。使用形式语言来模拟自然语言的语法成为生成式语法。</p></li>
<li><p>上下文无关语法的正式定义：</p>
<ul>
<li>N：非终止符号（或者变量）的集合</li>
<li>Sigma：终止符号的集合，与N不相交</li>
<li>R：规则或者产生式的集合</li>
<li>S：指定的开始符号</li>
</ul></li>
<li><p>一些约定定义：</p>
<ul>
<li>大写字母：代表非终止符号</li>
<li>S：开始符号</li>
<li>小写希腊字母：从非终止符号和终止符号的并集中抽取出来的符号串</li>
<li>小写罗马字母：终止符号串</li>
</ul></li>
<li><p>直接导出的定义： <strong>公式待补充</strong></p></li>
<li><p>导出是直接导出的泛化。之后我们可以正式定义由语法G生成的语言L是一个由终止符号组成的字符串集合，这些终止符号可以从指定的开始符号S通过语法G导出：
<strong>公式待补充</strong></p></li>
<li><p>将一个单词序列映射到其对应的剖析树成为句法剖析。</p></li>
</ul>
<h2 id="英语的一些语法规则">英语的一些语法规则</h2>
<ul>
<li>英语中最常用最重要的四种句子结构：
<ul>
<li>陈述式结构：主语名词短语加一个动词短语</li>
<li>命令式结构：通常以一个动词短语开头，并且没有主语</li>
<li>Yes-no疑问式结构：通常用于提问，并且以一个助动词开头，后面紧跟一个主语NP，再跟一个VP</li>
<li>Wh疑问式结构：包含一个wh短语成分</li>
</ul></li>
<li>在之前的描述中开始符号用于单独生成整个句子，但是S也可以出现在语法生成规则的右边，嵌入到更大的句子当中。这样的S称为从句，拥有完整的语义。拥有完整的语义是指这个S在整体句子的语法剖析树当中，其子树当中的主要动词拥有所需的所有论元。</li>
</ul>
<h2 id="名词短语">名词短语</h2>
<ul>
<li>限定词Det：名词短语可以以一些简单的词法限定词开始，例如a,the,this,those,any,some等等，限定词的位置也可以被更复杂的表示替代，例如所有格。这样的表示是可以递归定义的，例如所有格加名词短语可以构成更大的名词短语的限定词。在复数名词、物质名词之前不需要加限定词。</li>
<li>名词性词Nominal：包含一些名词前或者名词后修饰语</li>
<li>名词之前，限定词之后：一些特殊的词类可以出现在名词之前限定词之后，包括基数词Card、序数词Ord、数量修饰语Quant。</li>
<li>形容词短语AP：形容词短语之前可以出现副词</li>
<li>可以讲名词短语的前修饰语规则化如下（括号内代表可选）：</li>
<li>NP-&gt;(Det)(Card)(Ord)(Quant)(AP)Nominal</li>
<li>后修饰语主要包含三种：
<ul>
<li>介词短语PP：Nominal-&gt;Nominal PP(PP)(PP)</li>
<li>非限定从句：动名词后修饰语GerundVP,GerundVP-&gt;GerundV NP | GerundV
PP | GerundV | GerundV NP PP</li>
<li>关系从句：以关系代词开头的从句 Nominal -&gt;Nominal
RelCaluse;RelCaluse -&gt; (who|that) VP</li>
</ul></li>
</ul>
<h2 id="一致关系">一致关系</h2>
<ul>
<li><p>每当动词有一个名词作为它的主语时，就会发生一致关系的现象，凡是主语和他的动词不一致的句子都是不合语法的句子，例如第三人称单数动词没有加-s。可以使用多个规则的集合来扩充原有的语法，使得语法可以处理一致关系。例如yes-no疑问句的规则是</p>
<p><span class="math display">\[
S \rightarrow Aux \ NP \ VP
\]</span></p></li>
<li><p>可以用如下形式的两个规则来替代：</p>
<p><span class="math display">\[
S \rightarrow 3sgAux \ 3sgNP \ VP \\
S \rightarrow Non3sgAux \ Non3sgNP \ VP \\
\]</span></p></li>
<li><p>再分别指定第三人称单数和非第三人称单数的助动词形态。这样的方法会导致语法规模增加。</p></li>
</ul>
<h2 id="动词短语和次范畴化">动词短语和次范畴化</h2>
<ul>
<li>动词短语包括动词和其他一些成分的组合，包括NP和PP以及两者的组合。整个的嵌入句子也可以跟随在动词之后，成为句子补语。</li>
<li>动词短语的另一个潜在成分是另一个动词短语。</li>
<li>动词后面也可以跟随一个小品词，小品词类似于借此，但与动词组合在一起是构成一个短语动词，与动词不可分割。</li>
<li>次范畴化即再分类。传统语法把动词次范畴化为及物动词和不及物动词，而现代语法已经把动词区分为100个次范畴。讨论动词和可能的成分之间的关系是将动词看成一个谓词，而成分想象成这个谓词的论元(argument)。</li>
<li>对于动词和它的补语之间的关系，我们可以用上下文无关语法表示一致关系特征，且需要区分动词的各个次类。</li>
</ul>
<h2 id="助动词">助动词</h2>
<ul>
<li>助动词是动词的一个次类，具有特殊的句法约束。助动词包括情态动词、完成时助动词、进行时助动词、被动式助动词。每一个助动词都给他后面的动词形式一个约束，且需要按照一定的顺序进行结合。</li>
<li>四种助动词给VP次范畴化时，VP的中心动词分别是光杆动词、过去分词形式、现在分词形式、过去分词形式。</li>
<li>一个句子可以用多个助动词，但是要按照情态助动词、完成时助动词、进行式助动词、被动式助动词的顺序。</li>
</ul>
<h2 id="树图资料库">树图资料库</h2>
<ul>
<li><p>上下文无关语法可以将一个句子剖析成一个句法剖析树，如果一个语料中所有句子都以句法剖析树的形式表示，这样的句法标注了的语料就称为树图资料库(treebank)。</p></li>
<li><p>树图资料库中的句子隐含的组成了一种语言的语法，我们可以对于每一棵句法剖析树提取其中的CFG规则。从宾州树库中提取出来的CFG规则非常扁平化，使得规则数量很多且规则很长。</p></li>
<li><p>在树库中搜索需要一种特殊的表达式，能够表示关于节点和连接的约束，用来搜索特定的模式。例如tgrep或者TGrep2。</p></li>
<li><p>在tgrep、TGrep2中的一个模式由一个关于节点的描述组成，一个节点描述可以用来返回一个以此节点为根的子树。</p></li>
<li><p>可以使用双斜线对某一类模式命名：</p>
<p><span class="math display">\[
/NNS?/    NN|NNS
\]</span></p></li>
<li><p>Tgrep/Tgrep2模式的好处在于能够描述连接的信息。小于号代表直接支配，远小于符号代表支配，小数点代表线性次序。这种对于连接的描述反应在剖析树中的关系如下：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZ2b4.png"
alt="FoZ2b4.png" /></p></li>
</ul>
<h2 id="中心词和中心词查找">中心词和中心词查找</h2>
<ul>
<li>句法成分能够与一个词法中心词相关联。在一个简单的词法中心词模型中，每一个上下文无关规则与一个中心词相关联，中心词传递给剖析树，因此剖析树中每一个非终止符号都被一个单一单词所标注，这个单一单词就是这个非终止符号的中心词。一个例子如下：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZfa9.png"
alt="FoZfa9.png" /></li>
<li>为了生成这样一棵树，每一个CFG规则都必须扩充来识别一个右手方向的组成成分来作为中心词子女节点。一个节点的中心词词被设置为其子女中心词的中心词。</li>
<li>另一种方式是通过一个计算系统来完成中心词查找。在这种方式下是依据树的上下文来寻找指定的句子，从而动态的识别中心词。一旦一个句子被解析出来，树将会被遍历一遍并使用合适的中心词来装饰每一个节点。</li>
</ul>
<h2 id="语法等价与范式">语法等价与范式</h2>
<ul>
<li>语法等价包括两种：强等价，即两个语法生成相同的符号串集合，且他们对于每个句子都指派同样的短语结构；弱等价，即两个语法生成相同的符号串集合，但是不给每个句子指派相同的短语结构。</li>
<li>语法都使用一个范式，在范式中每个产生式都使用一个特定的形式。例如一个上下文五官与法是sigma自由的，并且如果他们的每个产生式的形式为A-&gt;BC或者是A-&gt;a，就说明这个上下文无关语法是符合Chomsky范式的，简称CNF。凡是Chomsky范式的语法都具有二叉树形式。任何上下文无关语法都可以转变成一个弱等价的Chomsky范式语法。</li>
<li>使用二叉树形式的剖析树能够产生更小的语法。形如A-&gt;A
B的规则称为Chomsky并连。</li>
</ul>
<h2 id="有限状态语法和上下文无关语法">有限状态语法和上下文无关语法</h2>
<ul>
<li>复杂的语法模型必须表示组成性，因而不适合用有限状态模型来描述语法。</li>
<li>当一个非终止符号的展开式中也包含了这个非终止符号时，就会产生语法的递归问题。</li>
<li>例如，使用正则表达式来描述以Nominal为中心的名词短语：
(Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*</li>
<li>为了完成这个正则表达式，只需要按顺序展开PP，展开结果为(P
NP)*，这样就出现了地柜问题，因为此时出现了NP，在NP的正则表达式中出现了NP。</li>
<li>一个上下文无关语法能够被有限自动机生成，当且仅当存在一个生成语言L的没有任何中心自嵌入递归的上下文无关语法。</li>
</ul>
<h2 id="依存语法">依存语法</h2>
<ul>
<li>依存语法与上下文无关语法相对，其句法结构完全由词、词与词之间的语义或句法关系描述。一个例子如下：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZOVH.png"
alt="FoZOVH.png" /></li>
<li>其中没有非终止符号或者短语节点，树中的连接只将两个词语相连。连接即依存关系，代表着语法功能或者一般的语义联系，例如句法主语、直接对象、间接宾语、时间状语等等。</li>
<li>依存语法具有很强的预测剖析能力，且在处理具有相对自由词序的语言时表现更好。</li>
</ul>
<h1
id="第十三章基于上下文无关语法的剖析">第十三章：基于上下文无关语法的剖析</h1>
<h2 id="剖析即搜索">剖析即搜索</h2>
<ul>
<li>在句法剖析中，剖析可以看成对一个句子搜索一切可能的剖析树空间并发现正确的剖析树。</li>
<li>对于某一个句子（输入符号串），剖析搜索的目标是发现以初始符号S为根并且恰好覆盖整个输入符号串的一切剖析树。搜索算法的约束来自两方面：
<ul>
<li>来自数据的约束，即输入句子本身，搜索出来的剖析树的叶子应该是原句的所有单词。</li>
<li>来自语法的约束，搜索出来的剖析树应该有一个根，即初始符号S</li>
</ul></li>
<li>根据这两种约束，产生了两种搜索策略：自顶向下，目标制导的搜索；自下而上，数据制导的搜索。</li>
<li>对于自顶向下的搜索，从根开始，我们通过生成式不断生成下一层的所有可能子节点，搜索每一层的每一种可能，如下图（对于句子book
that flight）： <img data-src="https://s2.ax1x.com/2019/01/03/FoZh5R.png"
alt="FoZh5R.png" /></li>
<li>对于自底向上的搜索，剖析从输入的单词开始，每次都使用语法中的规则，试图从底部的单词向上构造剖析树，如果剖析树成功的构造了以初始符号S为根的树，而且这个树覆盖了整个输入，那么就剖析成功。首先通过词表将每个单词连接到对应的词类，如果一个单词有不止一个词类，就需要考虑所有可能。与自顶向下相反，每次进入下一层时，自底向上需要考虑被剖析的成分是否与某个规则的右手边相匹配，而自顶向下是与左手边相匹配。中途如果无法匹配到规则则将这个树枝从搜索空间中删除，如下图所示：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZI8x.png"
alt="FoZI8x.png" /></li>
<li>两者对比：
<ul>
<li>自顶向下是从S开始搜索的，因此不会搜索那些在以S为根的树中找不到位置的子树，而自底向上会产生许多不可能的搜索树</li>
<li>相对应的，自顶向下把搜索浪费在了不可能产生输入单词序列的树上</li>
<li>综上，我们需要将自顶向下和自底向上相结合</li>
</ul></li>
</ul>
<h2 id="歧义">歧义</h2>
<ul>
<li><p>在句法剖析中需要解决的一个问题是结构歧义，即语法会给一个句子多种剖析结果可能。</p></li>
<li><p>最常见的两种歧义：附着歧义和并列连接歧义。</p></li>
<li><p>如果一个特定的成分可以附着在剖析树的一个以上的位置，句子就会出现附着歧义。例如We
saw the Eiffel Tower flying to Paris一句中,flying to Paris可以修饰Eiffel
Tower也可以修饰We。</p></li>
<li><p>在并列连接歧义中，存在着不同的短语，这些短语之间用and这样的连接词相连。例如old
men and
women可以是老年男性和老年女性，或者老年男性和普通女性，即old是否同时分配到men和women上。</p></li>
<li><p>以上两种歧义还能相互组合嵌套形成更复杂的歧义。假如我们不消歧，仅仅返回所有的可能，留给用户或者人工判断，则随着剖析句子结构变复杂或者剖析规则的增加，得到的可能是成指数级增长的，具体而言，这种剖析句子可能的增长数和算术表达式插入括号问题相同，以Catalan数按指数增长：</p>
<p><span class="math display">\[
C(n)=\frac{1}{1+n} C_{2n}^n
\]</span></p></li>
<li><p>摆脱这种指数爆炸的方法有两个：</p>
<ul>
<li>动态规划，研究搜索空间的规律性，使得常见的部分只推导一次，减少与歧义相关的开销</li>
<li>使用试探性的方法来改善剖析器的搜索策略</li>
</ul></li>
<li><p>使用例如深度优先搜索或者宽度优先搜索之类的有计划与回溯的搜索算法是在复杂搜索空间中搜索常用的算法，然而在复杂语法空间中无处不在的歧义使得这一类搜索算法效率低下，因为有许多重复的搜索过程。</p></li>
</ul>
<h2 id="动态规划剖析方法">动态规划剖析方法</h2>
<ul>
<li>在动态规划中，我们维护一个表，系统的将对于子问题的解填入表中，利用已经存储的子问题的解解决更大的子问题，而不用重复从头开始计算。</li>
<li>在剖析中，这样的表用来存储输入中各个部分的子树，当子树被发现时就存入表中，以便以后调用，就这样解决了重复剖析的问题（只需查找子树而不需要重新剖析）和歧义问题（剖析表隐含的存储着所有可能的剖析结果）。</li>
<li>主要的三种动态规划剖析方法有三种，CKY算法、Earley算法和表剖析算法。</li>
</ul>
<h3 id="cky剖析">CKY剖析</h3>
<ul>
<li>CKY剖析要求语法必须满足Chomsky范式，即生成式右边要么时两个非终止符号要么是一个终止符号。如果不是Chomsky范式，则需要把一个一般的CFG转换成CNF：
<ul>
<li>右边有终止符号也有非终止符号：给右边的终止符号单独建一个非终止符号，例如：INF-VP
→ to VP，改成INF-VP → TO VP和TO → to</li>
<li>右边只有一个非终止符号：这种非终止符号称为单元产物，它们最终会生成非单元产物，用最终生成的非单元产物规则来替换掉单元产物</li>
<li>右边不止2个符号：引入新的非终止符号将规则分解</li>
<li>词法规则保持不变，但是在转换的过程中可能会生成新的词法规则</li>
</ul></li>
<li>当所有的规则都转换成CNF之后，表中的非终止符号在剖析中有两个子节点，且表中每一个入口代表了输入中的某个区间，对于某个入口例如[0,3]，其可以被拆分成两部分，假如一部分为[0,2]，则另一部分为[2,3]，前者在[0,3]的左边，后者在[0,3]的正下方，如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZo26.png"
alt="FoZo26.png" /></li>
<li>接下来就是如何填表，我们通过自底向上的方法来剖析，对于每个入口[i,j]，包含了输入中i到j这一区间部分的表格单元都会对这个入口值做出贡献，即入口[i,j]左边的单元和下边的单元。下表中的CKY伪算法图描述了这一过程：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZjIA.png"
alt="FoZjIA.png" /></li>
<li>外层循环从左往右循环列，内层循环从下往上循环行，而最里面的循环式遍历串[i,j]的所有可能二分子串，表中存的是可以代表[i,j]区间符号串的非终止符号集合，因为是集合，所以不会出现重复的非终止符号。</li>
<li>现在我们完成了识别任务，接下来是剖析。剖析即在[0,N]入口，对应整个句子，找到一个非终止符号作为起始符号S。首先我们要对算法做两点更改：
<ul>
<li>存入表中的不仅仅是非终止符号，还有其对应的指针，指向生成这个非终止符号的表入口</li>
<li>允许一个入口中存在同一个非终止符号的不同版本</li>
</ul></li>
<li>做了这些改动之后，这张表就包含了一个给定输入的所有可能剖析信息。我们可以选择[0,N]入口中任意一个非终止符号作为起始符号S，然后根据指针迭代提取出剖析信息。</li>
<li>当然，返回所有的可能剖析会遇到指数爆炸的问题，因此我们在完整的表上应用维特比算法，计算概率最大的剖析并返回这个剖析结果。</li>
</ul>
<h3 id="early算法">Early算法</h3>
<ul>
<li>相比CKY自底向上的剖析，Early算法采用了自顶向下的剖析，而且只用了一维的表保存状态，每个状态包含三类信息：
<ul>
<li>对应某一单一语法规则的子树</li>
<li>子树的完成状态</li>
<li>子树对应于输入中的位置</li>
</ul></li>
<li>算法流程图如下： <img data-src="https://s2.ax1x.com/2019/01/03/FoZHKO.png" alt="FoZHKO.png" /></li>
<li>算法对于状态的操作有三种：
<ul>
<li>预测：造出一个新的状态来表示在剖析过程中生成的自顶向下的预测。当待剖析的状态为非终极符号但又不是词类范畴时，对于这个非终极符号的不同展开，预测操作都造出一个新的状态。</li>
<li>扫描：当待剖析的状态是词类范畴时，就检查输入符号串，并把对应于所预测的词类范畴的状态加入线图中。</li>
<li>完成：当右边所有状态剖析完成时，完成操作查找输入中在这个位置的语法范畴，发现并推进前面造出的所有状态。</li>
</ul></li>
</ul>
<h3 id="表剖析">表剖析</h3>
<ul>
<li>表剖析允许动态的决定表格处理的顺序，算法动态的依照计划依次删除图中的一条边，而计划中的元素排序是由规则决定的。
<img data-src="https://s2.ax1x.com/2019/01/03/FoZTxK.png"
alt="FoZTxK.png" /></li>
</ul>
<h2 id="部分剖析">部分剖析</h2>
<ul>
<li>有时我们只需要输入句子的部分剖析信息</li>
<li>可以用有限状态自动机级联的方式完成部分剖析，这样会产生比之前提到的方法更加“平”的剖析树。</li>
<li>另一种有效的部分剖析的方法是分块。使用最广泛覆盖的语法给句子做词类标注，将其分为有主要词类标注信息且不没有递归结构的子块，子块之间不重叠，就是分块。</li>
<li>我们用中括号将每一个分块框起来，有可能一些词并没有被框住，属于分块之外。</li>
<li>分块中最重要的是基本分块中不能递归包含相同类型的成分。</li>
</ul>
<h3 id="基于规则的有限状态分块">基于规则的有限状态分块</h3>
<ul>
<li>利用有限状态方式分块，需要为了特定目的手动构造规则，之后从左到右，找到最长匹配分块，并接着依次分块下去。这是一个贪心的分块过程，不保证全局最优解。</li>
<li>这些分块规则的主要限制是不能包含递归。</li>
<li>使用有限状态分块的优点在于可以利用之前转录机的输出作为输入来组成级联，在部分剖析中，这种方法能够有效近似真正的上下文无关剖析器。</li>
</ul>
<h3 id="基于机器学习的分块">基于机器学习的分块</h3>
<ul>
<li>分块可以看成序列分类任务，每个位置分类为1（分块）或者0（不分块）。用于训练序列分类器的机器学习方法都能应用于分块中。</li>
<li>一种卓有成效的方法是将分块看成类似于词类标注的序列标注任务，用一个小的标注符号集同时编码分块信息和每一个块的标注信息，这种方式称为IOB标注，用B表示分块开始，I表示块内，O表示块外。其中B和I接了后缀，代表该块的句法信息。</li>
<li>机器学习需要训练数据，而分块的已标数据很难获得，一种方法是使用已有的树图资料库，例如宾州树库。</li>
</ul>
<h3 id="评价分块系统">评价分块系统</h3>
<ul>
<li>准确率：模型给出的正确分块数/模型给出的总分块数</li>
<li>召回率：模型给出的正确分块数/文本中总的正确分块数</li>
<li>F1值：准确率和召回率的调和平均</li>
</ul>
<h1 id="第十四章统计剖析">第十四章：统计剖析</h1>
<h2 id="概率上下文无关语法">概率上下文无关语法</h2>
<ul>
<li>概率上下文无关语法PCFG是上下文无关语法的一种简单扩展，又称随机上下文无关语法。PCFG在定义上做出了一点改变：
<ul>
<li>N：非终止符号集合</li>
<li>Σ：终止符号集合</li>
<li>R：规则集合，与上下文无关语法相同，只不过多了一个概率p，代表某一项规则执行的条件概率<span
class="math inline">\(P(\beta|A)\)</span></li>
<li>S：一个指定的开始符号</li>
</ul></li>
<li>当某个语言中所有句子的概率和为1时，我们称这个PCFG时一致的。一些递归规则可能导致PCFG不一致。</li>
</ul>
<h2 id="用于消歧的pcfg">用于消歧的PCFG</h2>
<ul>
<li>对于一个给定句子，其某一特定剖析的概率是所有规则概率的乘积，这个乘积既是一个剖析的概率，也是剖析和句子的联合概率。这样，对于出现剖析歧义的句子，其不同剖析的概率不同，通过选择概率大的剖析可以消歧。</li>
</ul>
<h2 id="用于语言建模的pcfg">用于语言建模的PCFG</h2>
<ul>
<li>PCFG为一个句子分配了一个概率（即剖析的概率），因此可以用于语言建模。相比n元语法模型，PCFG在计算生成每一个词的条件概率时考虑了整个句子，效果更好。对于含歧义的句子，其概率是所有可能剖析的概率之和。</li>
</ul>
<h2 id="pcfg的概率cky剖析">PCFG的概率CKY剖析</h2>
<ul>
<li>PCFG的概率剖析问题：为一个句子产生概率最大的剖析</li>
<li>概率CKY算法扩展了CKY算法，CKY剖析树中的每一个部分被编码进一个<span
class="math inline">\((n+1)*(n+1)\)</span>的矩阵（只用上三角部分），矩阵中每一个元素包含一个非终止符号集合上的概率分布，可以看成每一个元素也是V维，因此整个存储空间为<span
class="math inline">\((n+1)*(n+1)*V\)</span>，其中[i,j,A]代表非终止符号A可以用来表示句子的i位置到j位置这一段的概率。</li>
<li>算法伪代码： <img data-src="https://s2.ax1x.com/2019/01/03/FoZbrD.png"
alt="FoZbrD.png" /></li>
<li>可以看到也是用k对某一区间[i,j]做分割遍历，取最大的概率组合作为该区间的概率，并向右扩展区间进行动态规划。</li>
</ul>
<h2 id="学习到pcfg的规则概率">学习到PCFG的规则概率</h2>
<ul>
<li><p>上面的伪算法图用到了每一个规则的概率。如何获取这个概率？两种方法，第一种朴素的方法是在一个已知的树库数据集上用古典概型统计出概率：</p>
<p><span class="math display">\[
P(\alpha \rightarrow \beta | \alpha) = \frac{Count(\alpha \rightarrow
\beta)}{\sum _{\gamma} Count(\alpha \rightarrow \gamma)}
\]</span></p></li>
<li><p>假如我们没有树库，则可以用非概率剖析算法来剖析一个数据集，再统计出概率。但是非概率剖析算法在剖析歧义句子时，需要对每一种可能剖析计算概率，但是计算概率需要概率剖析算法，这样就陷入了鸡生蛋蛋生鸡的死循环。一种解决方案是先用等概率的剖析算法，剖析句子，得出每一种剖析得概率，然后用概率加权统计量，然后重新估计剖析规则的概率，继续剖析，反复迭代直到收敛。这种算法称为inside-outside算法，是前向后向算法的扩展，同样也是EM算法的一种特例。</p></li>
</ul>
<h2 id="pcfg的问题">PCFG的问题</h2>
<ul>
<li>独立性假设导致不能很好的建模剖析树的结构性依存：每个PCFG规则被假定为与其他规则独立，例如，统计结果表明代词比名词更有可能称为主语，因此当NP被展开时，如果NP是主语，则展开为代词的可能性较高——这里需要考虑NP在句子种的位置，然而这种概率依存关系是PCFG所不允许的，</li>
<li>缺乏对特定单词的敏感，导致次范畴化歧义、介词附着、联合结构歧义的问题：例如在介词附着问题中，某一个介词短语into
Afghanistan附着于哪一个部分，在PCFG中计算时被抽象化为介词短语应该附着一个哪一个部分，而抽象化的概率来自于对语料的统计，这种统计不会考虑特定的单词。又例如联合结构歧义，假如一个句子的两种可能剖析树使用了相同的规则，而规则在树中的位置不同，则PCFG对两种剖析计算出相同的概率：因为PCFG假定规则之间是独立的，联合概率是各个概率的乘积。</li>
</ul>
<h2
id="通过拆分和合并非终止符号来改进pcfg">通过拆分和合并非终止符号来改进PCFG</h2>
<ul>
<li>先解决结构性依存的问题。之前提到了我们希望NP作为主语和宾语时有不同概率的规则，一种想法就是将NP拆分成主语NP和宾语NP。实现这种拆分的方法是父节点标注，及每个节点标注了其父节点，对于主语NP其父节点是S，对于宾语NP，其父节点是VP，因此不同的NP就得到了区分。除此之外，还可以通过词性拆分的方式增强剖析树。</li>
<li>拆分会导致规则增多，用来训练每一条规则的数据变少，引起过拟合。因此要通过一个手写规则或者自动算法来根据每个训练集合并一些拆分。</li>
</ul>
<h2 id="概率词汇化的cfg">概率词汇化的CFG</h2>
<ul>
<li><p>概率CKY剖析更改了语法规则，而概率词汇化模型更改了概率模型本身。对于每一条规则，不仅要产生成分的规则变化，还要在每个成分上标注其中心词和词性，如下图：
<img data-src="https://s2.ax1x.com/2019/01/03/FoeSRP.png"
alt="FoeSRP.png" /></p></li>
<li><p>为了产生这样的剖析树，每一条PCFG规则右侧需要选择一个成分作为中心词子节点，用子节点的中心词和词性作为该节点的中心词和词性。
其中，规则被分成了两类，内部规则和词法规则，后者是确定的，前者是需要我们估计的：
<img data-src="https://s2.ax1x.com/2019/01/03/FoZqqe.png"
alt="FoZqqe.png" /></p></li>
<li><p>我们可以用类似父节点标注的思想来拆分规则，拆分后每一部分都对应一种可能的中心词选择。假如我们将概率词汇话的CFG看成一个大的有很多规则CFG，则可以用之前的古典概型来估计概率。但是这样的效果不会很好，因为这样的规则划分太细了，没有足够的数据来估计概率。因此我们需要做出一些独立性假设，将概率分解为更小的概率乘积，这些更小的概率能容易从语料中估计出来。</p></li>
<li><p>不同的统计剖析器区别在于做出怎样的独立性假设。</p></li>
<li><p>Collins剖析如下图所示： <img data-src="https://s2.ax1x.com/2019/01/03/FoZzGt.png"
alt="FoZzGt.png" /></p></li>
<li><p>其概率拆解为：</p>
<p><span class="math display">\[
P(VP(dumped,VBD)→VBD(dumped,VBD)NP(sacks,NNS)PP(into,P))= \\
P_H (VBD│VP,dumped)* \\
P_L (STOP│VP,VBD,dumped)* \\
P_R (NP(sacks,NNS)│VP,VBD,dumped)* \\
P_R (PP(into,P)│VP,VBD,dumped)* \\
P_R (STOP|VP,VBD,dumped) \\
\]</span></p></li>
<li><p>给出生成式左边之后，首先生成规则的中心词，之后一个一个从里到外生成中心词的依赖。先从中心词左侧一直生成直到遇到STOP符号，之后生成右边。如上式做出概率拆分之后，每一个概率都很容易从较小的数据量中统计出来。完整的Collins剖析器更为复杂，还考虑了词的距离关系、平滑技术、未知词等等。</p></li>
</ul>
<h2 id="评价剖析器">评价剖析器</h2>
<ul>
<li>剖析器评价的标准方法叫做PARSEVAL测度，对于每一个句子s：
<ul>
<li>标记召回率=(Count(s的候选剖析中正确成分数）)/(Count(s的树库中正确成分数）)</li>
<li>标记准确率=(Count(s的候选剖析中正确成分数）)/(Count(s的候选剖析中全部成分数）)</li>
</ul></li>
</ul>
<h2 id="判别式重排序">判别式重排序</h2>
<ul>
<li>PCFG剖析和Collins词法剖析都属于生成式剖析器。生成式模型的缺点在于很难引入任意信息，即很难加入对某一个PCFG规则局部不相关的特征。例如剖析树倾向于右生成这一特征就不方便加入生成式模型当中。</li>
<li>对于句法剖析，有两类判别式模型，基于动态规划的和基于判别式重排序的。</li>
<li>判别式重排包含两个阶段，第一个阶段我们用一般的统计剖析器产生前N个最可能的剖析及其对应的概率序列。第二个阶段我们引入一个分类器，将一系列句子以及每个句子的前N个剖析-概率对作为输入，抽取一些特征的大集合并针对每一个句子选择最好的剖析。特征包括：剖析概率、剖析树中的CFG规则、平行并列结构的数量、每个成分的大小、树右生成的程度、相邻非终止符号的二元语法、树的不同部分出现的次数等等。</li>
</ul>
<h2 id="基于剖析的语言建模">基于剖析的语言建模</h2>
<ul>
<li>使用统计剖析器来进行语言建模的最简单方式就是利用之前提到的二阶段算法。第一阶段我们运行一个普通的语音识别解码器或者机器翻译解码器（基于普通的N元语法），产生N个最好的候选；第二阶段，我们运行统计剖析器并为每一个候选句分配一个概率，选择概率最佳的。</li>
</ul>
<h2 id="人类剖析">人类剖析</h2>
<ul>
<li>人类在识别句子时也用到了类似的概率剖析思想，两个例子：
<ul>
<li>对于出现频率高的二元语法，人们阅读这个二元语法所花的时间就更少</li>
<li>一些实验表明人类在消歧时倾向于选择统计概率大的剖析</li>
</ul></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Graph-based Summarization</title>
    <url>/2019/10/03/graph-summarization/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>Graph-based Automatic Summary Related Paper Selection Reading</p>
<ul>
<li>AMR Generative Summary</li>
<li>AMR Multi-document Summarization Two Papers</li>
<li>pagerank in encoder attention</li>
<li>Build a graph based on thematic modeling, use ILP for extractive
summarization</li>
<li>Multi-document Extractive Summary Based on GCN</li>
<li>STRUCTURED NEURAL SUMMARIZATION</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="toward-abstractive-summarization-using-semantic-representations">Toward
Abstractive Summarization Using Semantic Representations</h1>
<ul>
<li>Explored how to construct an AMR graph for summarization from the
original AMR graph, i.e., graph summarization</li>
<li>Three-step approach, source graph construction, subgraph prediction,
text generation</li>
</ul>
<h2 id="source-graph-construction">Source Graph Construction</h2>
<ul>
<li>This step involves merging multiple graphs into a source graph, and
some concept nodes need to be combined</li>
<li>Firstly, AMR does not repeat the modeling of the mentioned concept,
but supplements the frequency mentioned as a feature into the node
embedding</li>
<li>Node merging consists of two steps: merging the subtrees of some
nodes, followed by merging nodes with the same concepts</li>
<li>The node names after subtree merging contain information from all
subtree nodes, and only nodes that are completely identical can be
merged thereafter. Therefore, nodes that have undergone one subtree
merge are difficult to merge again (difficult to be completely
identical), and some coreference resolution work needs to be done
(future work)</li>
<li>Some nodes will have multiple edges, take the two edges with the
most occurrences, merge them, and discard the other edges</li>
<li>Directly merge the same concept nodes</li>
<li>Add a total root node to connect the root nodes of each
sentence</li>
<li>The source graph connected in this way has a low edge coverage for
the gold summary graph, so further post-processing is done on the source
graph, adding null edges between all nodes to improve coverage</li>
</ul>
<h2 id="subgraph-prediction">Subgraph Prediction</h2>
<ul>
<li><p>Subgraph prediction problem is a structured prediction
problem</p></li>
<li><p>The author constructs the subgraph scoring function for the model
parameters as a linear weighted combination of edge and node
features:</p>
<p><span class="math display">\[
\operatorname{score}\left(V^{\prime}, E^{\prime} ; \boldsymbol{\theta},
\boldsymbol{\psi}\right)=\sum_{v \in V^{\prime}}
\boldsymbol{\theta}^{\top} \mathbf{f}(v)+\sum_{e \in E^{\prime}}
\boldsymbol{\psi}^{\top} \mathbf{g}(e)
\]</span></p></li>
<li><p>decoding: Selects the subgraph with the highest score based on
ILP. The constraint is that the selected subgraph must be legal and
connected, which can be described by the indicator function v, e, and
the flow f. <span class="math inline">\(v\_i=1\)</span> means that the
i-th node is selected, <span class="math inline">\(e\_{i,j}=1\)</span>
means that the edge between nodes i and j is selected, and <span
class="math inline">\(f\_{i,j}\)</span> represents the flow from i to j.
Then, the legal condition is:</p>
<p><span class="math display">\[
v_{i}-e_{i, j} \geq 0, \quad v_{j}-e_{i, j} \geq 0, \quad \forall i, j
\leq N
\]</span></p></li>
<li><p>Union: The flow that originates from the root reaches each
selected conceptual node, each conceptual node consumes a flow, and the
flow can only pass through when the edges are selected. These three
constraints are described mathematically as:</p>
<p><span class="math display">\[
\begin{array}{r}{\sum_{i} f_{0, i}-\sum_{i} v_{i}=0} \\ {\sum_{i} f_{i,
j}-\sum_{k} f_{j, k}-v_{j}=0, \quad \forall j \leq N} \\ {N \cdot e_{i,
j}-f_{i, j} \geq 0, \quad \forall i, j \leq N}\end{array}
\]</span></p></li>
<li><p>The author only assumes that each concept has only one parent
node, i.e., constructed in the form of a tree</p>
<p><span class="math display">\[
\sum _j e_{i,j} \leq 1, \quad \forall i, j \leq N
\]</span></p></li>
<li><p>This form of ILP has appeared in sentence compression and
dependency parsing, and the author has completed optimization using
Gurobi's ILP algorithm</p></li>
<li><p>A constraint can be added to limit the length of the summary, for
example, the total number of selected edges not exceeding L</p></li>
<li><p>The above is decoding, i.e., selecting subgraphs, but the
selection of graphs is based on scores, and the scores are weighted by
parameters, thus also including an optimization of parameters. We need a
loss function to measure the gap between the decoded graph and the gold
summary graph, however, the gold summary graph may not be in the source
graph. The authors refer to ramp loss from machine translation,
comparing the perceptron loss used by the perceptron, the hinge loss in
structured SVM, and the ramp loss, where <span
class="math inline">\(G\)</span> is the source graph, and <span
class="math inline">\(G^{\*}\)</span> is the gold summary graph:</p>
<p><span class="math display">\[
\begin{array}{ll}{\text {perceptron loss: }} &amp; {-\text { score
}\left(G^{*}\right)+\max _{G} \text { score }(G)} \\ {\text {hinge loss:
}} &amp; {-\text { score(G^{*} ) }+\max _{G}\left(\text
{score}(G)+\operatorname{cost}\left(G ; G^{*}\right)\right)} \\ {\text
{ramp loss: }} &amp; {-\max _{G}\left(\text
{score}(G)-\operatorname{cost}\left(G ; G^{*}\right)\right)+\max
_{G}\left(\text {score}(G)+\operatorname{cost}\left(G ;
G^{*}\right)\right)}\end{array}
\]</span></p></li>
<li><p>cost penalty for redundant edges</p></li>
<li><p>Perceptron loss is very simple, it is to minimize the score gap
between the gold graph and the decoded graph</p></li>
<li><p>hinge loss is added to the ILP with a penalty for redundant
edges, making the score of the decoded graph as large as possible, not
just close to the gold graph, here the score of the decoded graph will
be slightly lower than the graph obtained by direct score
calculation</p></li>
<li><p>ramp loss compared to hinge loss is that an inverse penalty is
added to the former, while the actual ramp loss still narrows the score
gap between the two images, with one image having a slightly higher
score than the best decoded graph and the other slightly lower, relaxing
the conditions</p></li>
</ul>
<h2 id="generation">Generation</h2>
<ul>
<li>Authors currently only counted the text span corresponding to the
concept nodes in the decoded graph, and did not generate a readable
summary, therefore only ROUGE-1 was calculated</li>
</ul>
<h1
id="abstract-meaning-representation-for-multi-document-summarization">Abstract
Meaning Representation for Multi-Document Summarization</h1>
<ul>
<li>This is an extension of the previous one</li>
<li>Using AMR to construct a rooted directed acyclic graph, where nodes
are concepts and edges are semantic relationships:
<ul>
<li>Node: May be a frameset (frame) in PropBank, an ordinary English
word, a special category word, or a string,</li>
<li>Edge: It can be a predicate relationship from PropBank or a modified
relationship</li>
</ul></li>
<li>The entire system consists of three parts
<ul>
<li>source sentence selection: input a series of articles and then pick
out sentences from different aspects of a certain topic</li>
<li>Content planning: Input a series of sentences, output an abstract
diagram</li>
<li>Surface realization: Convert diagrams into readable summary
sentences</li>
</ul></li>
<li>Three components can be optimized with domain-specific small corpora
separately</li>
</ul>
<h2 id="source-sentence-selection">Source Sentence Selection</h2>
<ul>
<li>Because it is a multi-document summary, spectral clustering is
performed for each input example (multiple documents), and several
sentences are then selected from each cluster</li>
<li>There are multiple sentence groups, and just like the modified input
summary model, it is necessary to reconstruct training pairs. Here, it
is to construct the training pairs to be provided for content planning,
that is, the sentence groups and their corresponding gold summary AMR
graphs. For each sentence in the gold summary, calculate an average
similarity with the sentence group, and select the one with the highest
similarity as the sentence group in the training pair. The average
similarity includes:
<ul>
<li>LCS</li>
<li>VSM</li>
<li>Smatch method, referring to the paper Smatch: an evaluation metric
for semantic feature structures</li>
<li>Concept Coverage, i.e., the concept in the maximum coverage gold
summary AMR graph</li>
</ul></li>
<li>Four similarity measures have also been ablated</li>
</ul>
<h2 id="content-planning">Content Planning</h2>
<ul>
<li><p>Training is for the AMR graph of sentence pairs and summaries,
naturally this part is about learning this transformation
process</p></li>
<li><p>Firstly, the summary in the sentence group needs to be converted
into an AMR graph; the author tried two AMR Parsers, JAMR and
CAMR</p></li>
<li><p>After that, convert each sentence in the sentence group into an
AMR graph and then merge them (this part of the paper is not described
clearly)</p>
<ul>
<li>Are same concept nodes merged?</li>
<li>Perform coreference resolution, merge nodes with the same reference
concepts</li>
<li>Some special nodes require integrating subtrees into the node
information, called mega-nodes, which is essentially canceling
unnecessary expansion and directly writing the specific expansion
information into the node, for example, date entity: year 2002: month 1:
day 5. These mega-nodes can only be merged if they are completely
identical.</li>
<li>Generate a root node finally, and connect the root nodes of various
subgraphs</li>
<li>Through the last operation, does the merging of seemingly identical
concept nodes represent the merging of the same nodes within the same
subgraph?</li>
</ul></li>
<li><p>Next, design an algorithm to identify the summarized AMR graph
from the source AMR graph, which includes two parts</p>
<ul>
<li><p>Graph Decoding: Identifies an optimal summary graph through
Integer Linear Programming (ILP): First, construct a parameterized graph
scoring function, where each node feature and edge feature is weighted
by parameters and summed to obtain a score; here, the features are
manually constructed, referring to Fei Liu's series of AMR summarization
papers; next, perform an ILP to find a subgraph that maximizes the
score, with the constraint of L nodes and the subgraph being
connected.</p></li>
<li><p>Parameter update: Minimize the gap between the abstract graph
decoded by the system and the gold summary graph. This step optimizes
the feature weighting parameters in the scoring function from the
previous step. Construct a loss function to measure the gap between the
decoded graph and the gold graph. Sometimes, the gold graph cannot be
decoded from the source graph, in which case structured ramp loss is
used, considering not only the score but also the cost, i.e., the degree
of agreement between the gold graph and the decoded graph on whether to
include a certain node or edge in the summary.</p>
<p><span class="math display">\[
L_{ramp}(\theta, \phi) = max_G (score(G)+cost(G;G_{gold})) -
max_G(score(G) - cost(G;G_{gold}))
\]</span></p></li>
</ul></li>
</ul>
<h2 id="surface-realization">Surface Realization</h2>
<ul>
<li>Convert image to sentence</li>
<li>AMR graphs do not convert into sentences because the graphs do not
contain grammatical information; a graph may generate multiple sentences
that are not grammatically correct. The author takes a two-step
approach, first converting the AMR graph into PENMAN form, and then
using the existing AMR-to-text to convert PENMAN into sentences</li>
</ul>
<h1
id="towards-a-neural-network-approach-to-abstractive-multi-document-summarization">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</h1>
<ul>
<li><p>This paper is an extension of the previous paper, expanding from
single-document summarization to multi-document summarization, mainly
focusing on how to transfer pre-trained models on large-scale
single-document summarization datasets to the multi-document
summarization task</p></li>
<li><p>Compared to the single-document model, an additional
document-level encoding layer is added on the encoding side. There is no
dependency or sequential relationship between documents, so there is no
need to use RNN. The authors directly use linear weighting. It is worth
noting that the weights of this weighting should not be fixed or
directly learned, but should be determined based on the document itself.
Therefore, the authors add a dependency relationship learned from the
document itself and the relationship between the document set:</p>
<p><span class="math display">\[
w_{m}=\frac{\mathbf{q}^{T}\left[\mathbf{d}_{m} ;
\mathbf{d}_{\Sigma}\right]}{\sum_{m} \mathbf{q}^{T}\left[\mathbf{d}_{m}
; \mathbf{d}_{\Sigma}\right]}
\]</span></p></li>
<li><p>The mechanism of attention remains essentially unchanged, with
the decoder's initial state transitioning from single-document encoding
to multi-document encoding, and the attention weighting shifting from
the number of sentences in a single document to the number of sentences
in multiple documents. One issue that arises here is that the number of
sentences in multi-documents is too large, with many attentions being
distributed very evenly, resulting in an excessive amount of information
after weighting. Therefore, the authors truncate the global soft
attention, allowing only the top k sentences to be weighted, with the
rest of the sentences being discarded directly during encoding.</p></li>
<li><p>The migration from single-document to multi-document actually is
not the focus of the paper. The author trains the single-document model
part on CNN/DM and then trains the multi-document part on a small DUC
dataset, but these two datasets are quite consistent. Many works trained
on CNNDM and tested on DUC can achieve good results.</p></li>
<li><p>The paper's ablation is very detailed, comparing the effects
under various functional graph model methods, including Textrank,
Lexrank, Centroid</p></li>
<li><p>It is noteworthy that the author uses edit distance to measure
the abstractness of the abstract</p></li>
</ul>
<h1
id="abstractive-document-summarization-with-a-graph-based-attentional-neural-model">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</h1>
<ul>
<li><p>A paper by the team of Teacher Wan, with very good ideas, the
important parts are in two points:</p>
<ul>
<li>hierarchical encoder and decoder: Since encoding and decoding at the
sentence level are required to adapt to the graph scoring operation, a
hierarchical seq2seq is adopted, with both encoding and decoding at the
word-level and sentence-level</li>
<li>graph-attention: The graph used here is actually a fully connected
graph from pagerank, where similarity is directly measured by the inner
product of the hidden vectors of enc-dec, and then the topic-aware
pagerank is used to recalculate the sentence-level attention
weights.</li>
</ul></li>
<li><p>In the encoding-decoding stage, we use hidden layers to calculate
similarity, which is the same as the original attention, but the
original attention adds a parameter matrix (modern attention doesn't
even bother to add a parameter matrix), so this similarity can reflect
the attention weight (score). Then, graph-attention directly calculates
the Markov chain iteration of pagerank on this similarity, considering
the stable distribution <span class="math inline">\(f\)</span> of the
Markov chain to be the sentence score after re-ranking. There is
something the paper doesn't mention; the author makes an assumption that
the state obtained during encoding-decoding is already in a stable
state, rather than starting from scratch, so we can let <span
class="math inline">\(f(t+1)=f(t)=f\)</span> and directly calculate the
stable distribution:</p>
<p><span class="math display">\[
\mathbf{f}(t+1)=\lambda W D^{-1} \mathbf{f}(t)+(1-\lambda) \mathbf{y} \\
\mathbf{f}=(1-\lambda)\left(I-\lambda W D^{-1}\right)^{-1} \mathbf{y} \\
\]</span></p></li>
<li><p>The basic form is consistent with pagerank, part of which is
based on salience allocation from a similarity matrix, and the other
part supplements a uniform distribution y to ensure the convergence of
the Markov chain (here it seems to be abbreviated, writing the uniform
transition matrix multiplied by f directly as a uniform distribution).
It is noteworthy that this calculation is done on the encoding and
decoding hidden layer states at the sentence level, therefore it is the
graph attention score of various encoding sentences given a certain
decoding sentence. How to reflect this certain decoding sentence? That
is to use topic-aware pagerank, treat the decoding sentence as a topic,
add this topic sentence to the pagerank graph, and change y from a
uniform distribution to a one-hot distribution, which ensures the
influence of the decoding sentence in the graph and thereby influences
other sentences.</p></li>
<li><p>Afterwards, the distraction attention mechanism was adopted to
prevent repeated attention:</p>
<p><span class="math display">\[
\alpha_{i}^{j}=\frac{\max \left(f_{i}^{j}-f_{i}^{j-1},
0\right)}{\sum_{l}\left(\max \left(f_{l}^{j}-f_{l}^{j-1},
0\right)\right)}
\]</span></p></li>
<li><p>Some minor techniques have also been applied at the decoding end,
including:</p>
<ul>
<li>Handling OOV, use <span class="citation"
data-cites="entity+word">@entity+word</span> length as a label to
replace all entities that are prone to become OOV, and attempt to
restore the entity labels generated in the decoded sentence, searching
in the original text according to word length</li>
<li>hierarchical beam search: word-level beam search scoring considers
the original sentence of "attend to" and the bigram overlap of the
currently generated part, hoping for a larger overlap; sentence-level
beam search hopes that the original sentence attended to is different
for each generated sentence, this description is not very clear, it
should be that N different original sentences are attended to when
generating each sentence, producing N different decoded sentences</li>
</ul></li>
<li><p>The hierarchical decoding in this article actually plays a very
crucial role; the author did not use word-level attention all at once
but rather reordered based on the sentence relationship component
diagram and also fully utilized two levels of information in beam
search</p></li>
</ul>
<h1
id="topical-coherence-for-graph-based-extractive-summarization">Topical
Coherence for Graph-based Extractive Summarization</h1>
<ul>
<li>Build a graph based on thematic modeling, use ILP for extractive
summarization</li>
<li>The author used a bipartite graph, with one side being sentence
nodes and the other side being topic nodes, connected by edges. The
weight of the edges is the sum of the logarithms of the probabilities of
all words in a sentence under a certain topic, normalized by the length
of the sentence</li>
<li>Using HITS algorithm to calculate the importance of sentences on a
bipartite graph</li>
</ul>
<h1 id="graph-based-neural-multi-document-summarization">Graph-based
Neural Multi-Document Summarization</h1>
<ul>
<li>Using GCN for extractive summarization, here GCN plays a role of
feature supplementation. The original approach is a two-level GRU, where
documents are clustered to create embeddings, with each sentence having
an embedding. Then, similar to IR, a comparison is made between sentence
embeddings and document embeddings to calculate salience scores.
Afterward, a greedy method is used to extract sentences based on the
scores, with the overall framework still being the scoring-extraction
approach</li>
<li>GCN added two layers between the GRUs, i.e., the embedding of
sentences under a sentence relationship graph was performed with three
layers of GCN, followed by the generation of document embeddings by the
GRU at the document level</li>
<li>Here are two points to focus on: how to construct the sentence
relationship diagram</li>
<li>The author of the sentence relationship diagram tried three methods:
<ul>
<li>The most naive, TF-IDF cosine distance</li>
<li>ADG in the paper "Towards Coherent Multi-Document
Summarization"</li>
<li>Author's Improved PDG on ADG</li>
</ul></li>
<li>After that, simply apply GCN propagation</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="toward-abstractive-summarization-using-semantic-representations">Toward
Abstractive Summarization Using Semantic Representations</h1>
<ul>
<li>探讨了如何从原文的AMR图构建摘要的AMR图，即graph summarization</li>
<li>三步走，source graph construction, subgraph prediction, text
generation</li>
</ul>
<h2 id="source-graph-construction">Source Graph Construction</h2>
<ul>
<li>这一步是将多句graph合并为source graph，一些concept node需要合并</li>
<li>首先AMR不会重复建模提到的concept，而是将提到的频次作为特征补充进node
embedding当中</li>
<li>节点合并包含两步：把一些节点的子树合并，接着把相同的概念节点合并</li>
<li>子树合并之后的节点名称里包含了所有子树节点的信息，之后只有完全相同的节点才能合并，因此经过一次子树合并之后的节点很难再次合并（很难完全相同），这里需要做一些共指消解的工作(future
work)</li>
<li>一些节点之间会有多条边，取出现次数最多的两个边的label，合并，抛弃其他边</li>
<li>相同的概念节点直接合并</li>
<li>加一个总的root节点连接各个句子的root节点</li>
<li>这样连接出来的source graph对于gold summary
graph的边覆盖度不高，因此对于source
graph还后处理一下，将所有节点之间加入null 边，提高覆盖率</li>
</ul>
<h2 id="subgraph-prediction">Subgraph Prediction</h2>
<ul>
<li><p>子图预测问题是一个structured prediction problem</p></li>
<li><p>作者构建子图打分函数为模型参数线性加权边和节点的特征：</p>
<p><span class="math display">\[
\operatorname{score}\left(V^{\prime}, E^{\prime} ; \boldsymbol{\theta},
\boldsymbol{\psi}\right)=\sum_{v \in V^{\prime}}
\boldsymbol{\theta}^{\top} \mathbf{f}(v)+\sum_{e \in E^{\prime}}
\boldsymbol{\psi}^{\top} \mathbf{g}(e)
\]</span></p></li>
<li><p>decoding：基于ILP选出得分最大的子图。这里的约束条件是选出的子图必须合法且是联通的，可以通过指示函数v,e和流量f来描述。<span
class="math inline">\(v_i=1\)</span>即第i个节点被选中，<span
class="math inline">\(e_{i,j}=1\)</span>即i,j两个节点之间的边被选中，<span
class="math inline">\(f_{i,j}\)</span>代表从i流向j的流量，那么合法即：</p>
<p><span class="math display">\[
v_{i}-e_{i, j} \geq 0, \quad v_{j}-e_{i, j} \geq 0, \quad \forall i, j
\leq N
\]</span></p></li>
<li><p>联通即：从根流出的流量到达选中的每一个概念节点，每一个概念节点消耗一个流量，只有边被选中时流量才可能通过，这三个约束用数学描述为：</p>
<p><span class="math display">\[
\begin{array}{r}{\sum_{i} f_{0, i}-\sum_{i} v_{i}=0} \\ {\sum_{i} f_{i,
j}-\sum_{k} f_{j, k}-v_{j}=0, \quad \forall j \leq N} \\ {N \cdot e_{i,
j}-f_{i, j} \geq 0, \quad \forall i, j \leq N}\end{array}
\]</span></p></li>
<li><p>另外作者只假设了每一个概念只有一个父节点，即构建为树的形式</p>
<p><span class="math display">\[
\sum _j e_{i,j} \leq 1, \quad \forall i, j \leq N
\]</span></p></li>
<li><p>这种形式的ILP在sentence compression和dependency
parsing中都出现过，作者使用gurobi的ILP算法完成最优化</p></li>
<li><p>可以附加一个约束来限制摘要的长度，例如选中的边总数不大于L</p></li>
<li><p>以上是decoding，即选子图，但选图基于分数，而分数由参数加权，因此还包含了一个参数的优化。我们需要一个损失函数来衡量decoded
graph和gold summary graph之间的差距，然而gold summary
graph可能不在source graph当中，作者借鉴了机器翻译中的ramp
loss，作者对比了感知机所用的perceptron loss, structured SVM中的hinge
loss以及ramp loss，其中<span class="math inline">\(G\)</span>是source
graph，<span class="math inline">\(G^{*}\)</span> 是gold summary
graph：</p>
<p><span class="math display">\[
\begin{array}{ll}{\text {perceptron loss: }} &amp; {-\text { score
}\left(G^{*}\right)+\max _{G} \text { score }(G)} \\ {\text {hinge loss:
}} &amp; {-\text { score(G^{*} ) }+\max _{G}\left(\text
{score}(G)+\operatorname{cost}\left(G ; G^{*}\right)\right)} \\ {\text
{ramp loss: }} &amp; {-\max _{G}\left(\text
{score}(G)-\operatorname{cost}\left(G ; G^{*}\right)\right)+\max
_{G}\left(\text {score}(G)+\operatorname{cost}\left(G ;
G^{*}\right)\right)}\end{array}
\]</span></p></li>
<li><p>cost对多余的边惩罚</p></li>
<li><p>perceptron loss很简单，就是希望缩小gold graph与decoded
graph之间的分数差距</p></li>
<li><p>hinge loss在ILP中加入对多余边的惩罚，使得decoded
graph的分数尽可能大，而不仅仅是和gold
graph接近，这里decoded的graph会比直接计算分数得到的graph分值上差一点</p></li>
<li><p>ramp loss相比hinge loss就是在前面一项加了一个反向的惩罚，实际ramp
loss依然是在缩小两个图的分数差距，只不过一个图比best decoded
graph分值高一点，另一个比best decoded graph低一点，放宽松了条件</p></li>
</ul>
<h2 id="generation">Generation</h2>
<ul>
<li>作者目前只统计了decoded graph中概念节点对应的text
span，并没有生成可读的摘要，因此只计算了ROUGE-1</li>
</ul>
<h1
id="abstract-meaning-representation-for-multi-document-summarization">Abstract
Meaning Representation for Multi-Document Summarization</h1>
<ul>
<li>这是上一篇的扩展</li>
<li>用AMR构建有根有向无环图，节点是概念，边是语义关系:
<ul>
<li>节点：可能是PropBank里的一个frameset（命题），一个普通英语单词，一个特殊类别词，一个字符串，</li>
<li>边：可以是PropBank里的命题关系，或者魔改之后的关系</li>
</ul></li>
<li>整个系统三个部分
<ul>
<li>source sentence
selection：输入一系列文章，然后挑出关于某一主题不同方面的句子</li>
<li>content planning：输入一系列句子，输出摘要图</li>
<li>surface realization：将图转换为可读的摘要句</li>
</ul></li>
<li>三个组件可分别用领域内小语料优化</li>
</ul>
<h2 id="source-sentence-selection">Source Sentence Selection</h2>
<ul>
<li>因为是多文档摘要，因此对每一个输入样例（多篇文档），做谱聚类，每个簇再挑若干句子</li>
<li>这样就有多个句子组，之后和更改了输入的摘要模型一样，需要重新构造训练对，这里是要构造接下来提供给content
planning的训练对，即句子组和对应的gold summary的AMR graph。就对gold
summary里的每一句，和句子组算一个平均相似度，选相似度大的作为训练对里的句子组。平均相似度有：
<ul>
<li>LCS</li>
<li>VSM</li>
<li>Smatch方法，参考了论文Smatch: an evaluation metric for semantic
feature structures</li>
<li>Concept Coverage，即最大覆盖gold summary AMR graph里的concept</li>
</ul></li>
<li>四种相似度也做了ablation</li>
</ul>
<h2 id="content-planning">Content Planning</h2>
<ul>
<li>训练对是句子组和summary的AMR
graph，自然这个部分就是学习这个转换过程</li>
<li>首先要把句子组里的summary转成AMR graph，作者试用了两种AMR
Parser，JAMR和CAMR</li>
<li>之后把句子组里的每一句也转成AMR
graph，并且做合并（这一部分论文描述并不清楚）
<ul>
<li>相同概念节点合并？</li>
<li>做共指消解，把相同指代概念节点合并</li>
<li>一些特殊节点需要把子树整合进节点信息里，叫mega-node，其实就是取消不必要的展开，将展开的具体信息直接写进节点里，例如date
entity :year 2002 :month 1 :day
5。这些mega-node只有完全相同时才能合并</li>
<li>最后生成一个root节点，把各个子图的root节点连起来</li>
<li>通过最后一个操作貌似相同概念节点合并是同一子图内相同节点合并？</li>
</ul></li>
<li>接下来设计算法，从源AMR graph中识别出摘要的AMR graph，包含两部分
<ul>
<li><p>graph
decoding：通过整数线性规划(ILP)识别出一个最优摘要图：首先构造一个参数化的图打分函数，将每一个节点特征和边特征通过参数加权并累加得到分数，这里的特征是手工构造，参考Fei
Liu他的一系列AMR
summarization的论文；接下来做一个ILP，要求找一个子图，使得得分最大，限制为L个节点而且子图是连接的。</p></li>
<li><p>parameter update：最小化系统解码出的摘要图和gold
summary图之间的差距。这一步优化的是上一步打分函数中的特征加权参数。构造损失函数来衡量decoded
graph和gold graph之间的差距。有时gold graph不能从source
graph中解码出来，这时就采用structed ramp
loss，不仅仅考虑score，还考虑cost，即gold graph和decoded
graph就是否将某个节点或者边加入摘要达成一致的程度</p>
<p><span class="math display">\[
L_{ramp}(\theta, \phi) = max_G (score(G)+cost(G;G_{gold})) -
max_G(score(G) - cost(G;G_{gold}))
\]</span></p></li>
</ul></li>
</ul>
<h2 id="surface-realization">Surface Realization</h2>
<ul>
<li>将图转成句子</li>
<li>AMR图并不好转成句子，因为图并不包含语法信息，一个图可能生成多句不合法的句子，作者两步走，先将AMR图转成PENMAN形式，然后用现有的AMR-to-text来将PENMAN转成句子</li>
</ul>
<h1
id="towards-a-neural-network-approach-to-abstractive-multi-document-summarization">Towards
a Neural Network Approach to Abstractive Multi-Document
Summarization</h1>
<ul>
<li><p>这篇论文是上篇论文的扩展，从单文档摘要扩展到多文档摘要，主要是如何将大规模单文档摘要数据集上预训练好的模型迁移到多文档摘要任务上</p></li>
<li><p>相比单文档模型，编码端又加了一层文档级别的编码，文档之间并没有依存或者顺序关系，因此没必要用RNN，作者直接用了线性加权,值得注意的是这个加权的权重不应该是固定或者直接学习出来的，而应该根据文档本身决定，因此作者给权重加了一个依赖关系学习出来，依赖文档本身和文档集的关系：</p>
<p><span class="math display">\[
w_{m}=\frac{\mathbf{q}^{T}\left[\mathbf{d}_{m} ;
\mathbf{d}_{\Sigma}\right]}{\sum_{m} \mathbf{q}^{T}\left[\mathbf{d}_{m}
; \mathbf{d}_{\Sigma}\right]}
\]</span></p></li>
<li><p>注意力的机制基本不变，decoder的初始状态从单文档变成多文档编码，注意力加权从单篇文档句子数量到多篇文档句子数量。这里带来的一个问题是多文档的句子数量太大了，很多注意力被分散的很均匀，加权之后包含的信息量太大。因此作者将global
soft attention给截断了一下，只有top
k个句子可以用权重加权，其余的句子直接在编码中被抛弃</p></li>
<li><p>单文档到多文档的迁移其实并不是论文的重点，作者在CNN/DM上训练单文档的模型部分，之后在少量DUC数据集上训练多文档的部分，但是这两个数据集挺一致的，很多工作在CNNDM上训练在DUC上测试也能取得不错的效果。</p></li>
<li><p>论文的ablation做的非常详细，对比了多种功能图模型方法下的效果，包括Textrank,Lexrank,Centroid</p></li>
<li><p>值得注意的是作者使用了编辑距离来衡量文摘的抽象程度</p></li>
</ul>
<h1
id="abstractive-document-summarization-with-a-graph-based-attentional-neural-model">Abstractive
Document Summarization with a Graph-Based Attentional Neural Model</h1>
<ul>
<li><p>万老师团队的一篇论文，想法非常的好，重要的部分在两点：</p>
<ul>
<li>hierarchical encoder and
decoder：由于需要在句子级别上做编解码以适应图打分的操作，所以采用了分层的seq2seq，无论编码解码都是word-level加sentence-level</li>
<li>graph-attention：这里用的图是其实是pagerank里的全连接图，相似度直接用enc-dec的隐层向量内积来衡量，然后利用topic-aware
pagerank来重新计算句子级别注意力权重。</li>
</ul></li>
<li><p>在编解码阶段，我们利用隐层来计算相似度，这和原始的attention是一样的，只不过原始的attention加了一个参数矩阵（现代的attention连参数矩阵都懒得加了）使得这个相似度能够体现出注意力权重（分数），那么graph-attention就是在这个相似度上直接计算pagerank的markov链迭代，认为马氏链的稳定分布<span
class="math inline">\(f\)</span>就是重新rank之后的句子分数，这里有一点论文里没讲，作者做了一个假设，即编解码时拿到的已经是稳定状态，而不是从头迭代，因此可以令<span
class="math inline">\(f(t+1)=f(t)=f\)</span>，直接算出稳定分布：</p>
<p><span class="math display">\[
\mathbf{f}(t+1)=\lambda W D^{-1} \mathbf{f}(t)+(1-\lambda) \mathbf{y} \\
\mathbf{f}=(1-\lambda)\left(I-\lambda W D^{-1}\right)^{-1} \mathbf{y} \\
\]</span></p></li>
<li><p>基本形式与pagerank一致，一部分是基于相似矩阵的salience分配，另一部分补上一个均匀分布<span
class="math inline">\(y\)</span>保证马氏链收敛(这里感觉应该是简略了了，把均匀转移矩阵乘以f直接写成了均匀分布)，值得注意的是这是在sentence-level的编解码隐层状态做的计算，因此是计算给定某解码句下，各个编码句的graph
attention score，如何体现这个给定某解码句？那就是用topic-aware
pagerank，将解码句看成topic，把这个topic句加入pagerank的图里，并且y从均匀分布改成one-hot分布，即保证了解码句在graph中的影响力，并借此影响其他句子。</p></li>
<li><p>之后借鉴了distraction attention使得注意力不重复：</p>
<p><span class="math display">\[
\alpha_{i}^{j}=\frac{\max \left(f_{i}^{j}-f_{i}^{j-1},
0\right)}{\sum_{l}\left(\max \left(f_{l}^{j}-f_{l}^{j-1},
0\right)\right)}
\]</span></p></li>
<li><p>在解码端也做了一些小技巧，包括：</p>
<ul>
<li>OOV的处理，用@entity+单词长度来作为标签替换所有容易成为OOV的实体，并尝试把解码句中生成的实体标签还原，根据单词长度在原文中查找</li>
<li>hierarchical beam search：word-level的beam search打分考虑了attend
to的原文句子和当前生成部分的bigram
overlap，希望这个overlap越大越好；sentence-level的beam
search则希望生成每一句时attend
to的原文句子不相同，这一段描述不是很清楚，应该是生成每一句时会attend
N个不同的原文句产生N个不同的decoded sentence</li>
</ul></li>
<li><p>本文的层次编解码其实起到了很关键的作用，作者并没有一股脑用单词级别的注意力，还是根据句子关系构件图并重排序，在beam
search也充分利用了两个层次的信息</p></li>
<li><p>从ablation来看，graph attention和sentence
beam的效果其实不大，影响ROUGE分数最大的是考虑了bigram
overlap的word-level beam
search，这也暴露了ROUGE的问题，即我们之前工作中提到的OTR问题</p></li>
</ul>
<h1
id="topical-coherence-for-graph-based-extractive-summarization">Topical
Coherence for Graph-based Extractive Summarization</h1>
<ul>
<li>基于主题建模构建图，使用ILP做抽取式摘要</li>
<li>作者使用了二分图，一边是句子节点，一边是主题节点，两组节点之间用边连接，边的权值是句子中所有单词在某一主题下概率的对数和，除以句子长度做归一化</li>
<li>使用HITS算法在二分图上计算句子的重要程度</li>
</ul>
<h1 id="graph-based-neural-multi-document-summarization">Graph-based
Neural Multi-Document Summarization</h1>
<ul>
<li>用GCN做抽取式摘要，在这里GCN起到了一个特征补充的作用，原始的做法就是一个two-level
GRU，documents
cluster做一个embedding，其中每一个sentence有一个embedding，然后类似IR，拿sentence
embedding和documents embedding做一个比较算出salience
score，之后再用一个贪心的方法根据分数抽句子，大框架依然是打分-抽取的思路</li>
<li>GCN加进了两层GRU之间，即句子的embedding在一个句子关系图下做了三层GCN，之后再由documents层次的GRU生成documents
embedding</li>
<li>这里就关注两点：句子关系图如何构建</li>
<li>句子关系图作者试了三种：
<ul>
<li>最naive的，tfidf的cosine距离</li>
<li>Towards Coherent Multi-Document Summarization一文中的ADG</li>
<li>作者在ADG上改进的PDG</li>
</ul></li>
<li>之后直接套GCN传播就行了</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>graph neural network</tag>
        <tag>deep learning</tag>
        <tag>summarization</tag>
        <tag>natural language processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Heterogeneous Information Network</title>
    <url>/2019/10/30/heterogeneous/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<p>Record some recent processing of heterogeneous information
networks</p>
<ul>
<li>PathSim</li>
<li>HGNN</li>
<li>HGAN</li>
<li>HGAN for text classification</li>
<li>Attribute, Attributed Multiplex Heterogeneous Network</li>
<li>Meta-graph Guided Random Walks</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="pathsim-meta-path-based-top-k-similarity-search-in-heterogeneous-information-networks">PathSim:
Meta Path-Based Top-K Similarity Search in Heterogeneous Information
Networks</h1>
<ul>
<li><p>An early paper (with authors all being big shots) clearly defined
many concepts in meta paths and proposed a method for measuring node
similarity in heterogeneous information networks.</p></li>
<li><p>Traditional similarity measurement methods have biases, methods
based on path count statistics and random walk have biases, favoring
nodes with higher degree; pairwise random walk is biased towards nodes
with more outlier neighbors</p></li>
<li><p>The idea behind PathSim is that two similar nodes should not only
be strongly linked to each other but also share comparable
visibility</p></li>
<li><p>Under the given symmetric meta path <span
class="math inline">\(P\)</span> , the PathSim of two nodes of the same
type <span class="math inline">\(x,y\)</span> is defined as:</p>
<p><span class="math display">\[
s(x, y)=\frac{2 \times\left|\left\{p_{x \leadsto y}: p_{x \sim y} \in
\mathcal{P}\right\}\right|}{\left|\left\{p_{x \leadsto x}: p_{x
\hookrightarrow x} \in \mathcal{P}\right\}\right|+\left|\left\{p_{y
\leadsto y}: p_{y \leadsto y} \in \mathcal{P}\right\}\right|}
\]</span></p></li>
<li><p>The actual node regression probability of the denominator is
visibility, and the author divides the traditional pathcount by
visibility, with all paths obtained by multiplying the edge
weights.</p></li>
<li><p>Similar to the symmetric normalization of the degree of
nodes.</p></li>
</ul>
<h1 id="heterogeneous-graph-neural-network">Heterogeneous Graph Neural
Network</h1>
<ul>
<li>Task: Graph Representation Learning</li>
<li>Heterogeneous Types: Node Heterogeneity, Edge Heterogeneity, Node
Multi-Attribute</li>
<li>Solution:
<ul>
<li>Four-step approach: Heterogeneous neighbor sampling, multi-attribute
encoding, same-type neighbor aggregation, different-type
aggregation</li>
<li>Heterogeneous neighborhood sampling: Based on restart random walk,
first perform a random walk, and with a certain probability p return to
the initial node (restart), until a certain number of neighborhood nodes
have been sampled. There is an upper limit for each type of neighborhood
node to ensure that all types of neighbors can be sampled. Then, scale
down proportionally, for each type of neighborhood node <span
class="math inline">\(t\)</span> , only take <span
class="math inline">\(K_t\)</span> neighborhood nodes, and group them
accordingly.</li>
<li>Multi-attribute coding: Pre-encoded based on multimodal content,
such as text using paragraph2vec, images using CNN, and different
attribute information of the same neighboring nodes using BiLSTM
encoding</li>
<li>Aggregation of similar neighbors: Using BiLSTM to aggregate features
of multiple neighborhood nodes under the same type</li>
<li>Different types of aggregation: Use an attention mechanism to
aggregate different types of features</li>
</ul></li>
</ul>
<h1 id="heterogeneous-graph-attention-network">Heterogeneous Graph
Attention Network</h1>
<ul>
<li>Task: Graph Representation Learning</li>
<li>Heterogeneous Types: Node Heterogeneity</li>
<li>Solution:
<ul>
<li><p>Implementing double attention at the node level and metapath
level.</p></li>
<li><p>Need to add itself to all metapath neighbors, similar to the
inner loop in GCN.</p></li>
<li><p>node-level attention, applying attention weighting to different
nodes along a metapath. Since nodes of different types have feature
representations in different spaces, a feature transformation matrix is
assigned to each type, mapping different types of nodes to the same
space, and then attention calculation and weighting are performed on the
nodes through the self-attention mechanism (where <span
class="math inline">\(\phi\)</span> represents the metapath):</p>
<p><span class="math display">\[
h_i^{\prime} = M_{\phi _i} \cdot h_i \\
e^{\phi}_{ij} = attn_{node}(h^{\prime}_i,h^{\prime}_j;\phi) \\
\]</span></p>
<p>In calculating attention, a mask needs to be applied, only
calculating attention for neighboring nodes and performing softmax. It
is noteworthy that the asymmetry of self-attention is important for
heterogeneous graphs, as in a node pair, the neighborhoods of the two
nodes are different, and their mutual influence is not equal. Simply
put, for a certain node, calculate the attention weights of all
neighboring nodes under a certain type of metapath, with input being the
h of the two nodes and a parameter vector specific to the metapath,
outputting the attention weights, and then for a certain node,
weightedly sum the h of all neighboring nodes under a certain type of
metapath to obtain the representation of the node under a certain
metapath.</p>
<p><span class="math display">\[
\alpha_{i j}^{\Phi}=\operatorname{softmax}_{j}\left(e_{i
j}^{\Phi}\right)=\frac{\exp
\left(\sigma\left(\mathbf{a}_{\Phi}^{\mathrm{T}}
\cdot\left[\mathbf{h}_{i}^{\prime} \|
\mathbf{h}_{j}^{\prime}\right]\right)\right)}{\sum_{k \in
\mathcal{N}_{i}^{\mathrm{\Phi}}} \exp
\left(\sigma\left(\mathbf{a}_{\Phi}^{\mathrm{T}}
\cdot\left[\mathbf{h}_{i}^{\prime} \|
\mathbf{h}_{k}^{\prime}\right]\right)\right)} \\
\]</span></p>
<p>Calculate the attention after transforming the metapath neighborhood
node features:</p>
<p><span class="math display">\[
\mathbf{z}_{i}^{\Phi}=\sigma\left(\sum_{j \in \mathcal{N}_{i}^{\Phi}}
\alpha_{i j}^{\Phi} \cdot \mathbf{h}_{j}^{\prime}\right)
\]</span></p></li>
<li><p>The author referred to the multi-head approach during actual
weighted calculation, computed k attention-weighted features and
concatenated them together</p></li>
<li><p>The attention at the metapath level refers to the weighted sum of
all different class metapath embeddings for a certain node. First,
transform the embeddings of each metapath to the same latent space, then
parameterize to calculate the attention weights and apply softmax. It
should be noted that the attention logits before softmax are the average
of a certain type of metapath calculated over all nodes, with the
denominator being the number of nodes and the numerator being the sum of
the metapath embeddings of the nodes containing that type of metapath.
The node-level before this is for the average of all neighboring nodes
under a certain node and metapath:</p>
<p><span class="math display">\[
w_{\Phi_{i}}=\frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}}
\mathbf{q}^{\mathrm{T}} \cdot \tanh \left(\mathbf{W} \cdot
\mathbf{z}_{i}^{\Phi}+\mathbf{b}\right)
\]</span></p></li>
<li><p>Afterward, perform softmax on all metapath types to obtain
weights, and weight each node's different metapath embedding to get the
final embedding</p></li>
<li><p>The entire range of the logit and softmax in the double-layer
attention is a bit confusing, sometimes local and sometimes global,
requiring careful consideration.</p></li>
</ul></li>
<li>The entire process can be parallelized at the node level</li>
<li>The results show that the effectiveness has reached the SOTA, and
the visualization results show that the clustering effect of node
embeddings is better, and attention also brings a certain degree of
interpretability.</li>
</ul>
<h1
id="heterogeneous-graph-attention-networks-for-semi-supervised-short-text-classification">Heterogeneous
Graph Attention Networks for Semi-supervised Short Text
Classification</h1>
<ul>
<li>Task: Node Classification</li>
<li>Heterogeneous types: Node heterogeneity, including three types of
nodes, text, entity, and topic</li>
<li>Solution:
<ul>
<li><p>The simplest: Expand the feature space of the expansion nodes,
concatenate the feature vectors of the three types of nodes, and set the
positions of all feature vectors not included in a specific node to
0</p></li>
<li><p>Heterogeneous Graph Convolution: Separates subgraphs of the same
node type, performs convolution on each subgraph individually, projects
different subgraphs through a parameter transformation matrix to the
same latent space and sums the activations as the next layer.
Specifically, the original GCN is:</p>
<p><span class="math display">\[
H^{(l+1)}=\sigma\left(\tilde{A} \cdot H^{(l)} \cdot W^{(l)}\right)
\]</span></p>
<p>And the Heterogeneous GCN is:</p>
<p><span class="math display">\[
H^{(l+1)}=\sigma\left(\sum_{\tau \in \mathcal{T}} \tilde{A}_{\tau} \cdot
H_{\tau}^{(l)} \cdot W_{\tau}^{(l)}\right)
\]</span></p>
<p>The line <span class="math inline">\(\tilde{A}_{\tau}\)</span>
represents all nodes, and the columns represent all nodes of a certain
type, thus isolating isomorphic subgraphs. For each node, we separately
consider the nodes of type a in its neighborhood, aggregate information
to obtain encoding a, and then consider the nodes of type b in the
neighborhood, aggregate information to obtain encoding b. Encodings a
and b are transformed to the same latent space by their respective
transformation matrices and then summed. This design is logically
sound.</p></li>
<li><p>The author also considered the following situations: the
contributions of different types of neighboring nodes to a certain node
are not the same, and the contributions of different neighboring nodes
within the same type are also not the same. It is obvious that attention
is needed here. The author proposed dual attention (i.e., two-layer
attention), one at the type level and one at the node level. First, the
mean of the embedding of a certain type of neighboring node is used as
the type embedding, and then the type attention weight is calculated
based on the current node embedding and the type embedding. Similarly,
the node attention is obtained by using the specific neighboring node
embedding and the current node embedding, plus the type attention, and
the calculated node attention is used to replace the symmetric
normalized adjacency matrix in GCN.</p></li>
</ul></li>
</ul>
<h1
id="representation-learning-for-attributed-multiplex-heterogeneous-network">Representation
Learning for Attributed Multiplex Heterogeneous Network</h1>
<ul>
<li>Task: Graph Representation Learning</li>
<li>Heterogeneous Types: Node Heterogeneity, Edge Heterogeneity, Node
Multi-Attribute</li>
<li>Solution:
<ul>
<li>Consider that a node has different embeddings under different types
of edges, and decompose the node's total overall embedding into a base
embedding unrelated to the edges and an edge embedding related to the
edges</li>
<li>edge embedding is related to edge type, and the neighboring nodes
connected by edges of the same type are aggregated to form it. Here, the
aggreagator function can adopt the approach from GraphSage.</li>
<li>After k-level aggregation, each node obtains k types of edge
embedding, which are weighted and summed through self-attention,
multiplied by the ratio, and then added to the base embedding to obtain
the final overall embedding</li>
<li>This is a transductive model. For unobserved data, an inductive
method is required. The specific approach is quite simple: parameterize
the base embedding and edge embedding as functions of the node
attributes, rather than randomly initializing and then completely
learning from the existing graph. This way, even if there are nodes not
seen in the graph, as long as the nodes have attributes, overall
embedding extraction can still be performed.</li>
<li>The final step is to perform a random walk based on meta-path to
obtain training pairs, using skip-gram training and incorporating
negative sampling.</li>
</ul></li>
</ul>
<h1
id="semi-supervised-learning-over-heterogeneous-information-networks-by-ensemble-of-meta-graph-guided-random-walks">Semi-supervised
Learning over Heterogeneous Information Networks by Ensemble of
Meta-graph Guided Random Walks</h1>
<ul>
<li>Task: Node Classification</li>
<li>Heterogeneous types: Node heterogeneity, including three types of
nodes, text, entity, and topic</li>
<li>Solution: meta-path guided random walk</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="pathsim-meta-path-based-top-k-similarity-search-in-heterogeneous-information-networks">PathSim:
Meta Path-Based Top-K Similarity Search in Heterogeneous Information
Networks</h1>
<ul>
<li><p>较早的一篇论文（作者都是大神），定义清楚了meta
path中的很多概念，提出了衡量异构信息网络中节点相似度的一种方法。</p></li>
<li><p>传统的相似度衡量方法存在偏差，基于路径数统计和随机游走的方法存在偏差，偏向度数较多的节点；pair-wise的随机游走偏向具有较多离群点邻居的节点</p></li>
<li><p>PathSim的想法是，两个相似的节点不仅仅应该相互强链接，还需要share
comparable visibility</p></li>
<li><p>在给定对称的meta path <span
class="math inline">\(P\)</span>下，两个同类型节点<span
class="math inline">\(x,y\)</span>的PathSim定义为：</p>
<p><span class="math display">\[
s(x, y)=\frac{2 \times\left|\left\{p_{x \leadsto y}: p_{x \sim y} \in
\mathcal{P}\right\}\right|}{\left|\left\{p_{x \leadsto x}: p_{x
\hookrightarrow x} \in \mathcal{P}\right\}\right|+\left|\left\{p_{y
\leadsto y}: p_{y \leadsto y} \in \mathcal{P}\right\}\right|}
\]</span></p></li>
<li><p>实际分母的节点回归概率就是visibility，作者在传统的pathcount上除以visibility，所有的路径都由edge
weight累乘得到。</p></li>
<li><p>类似于对节点的度做了对称归一化。</p></li>
</ul>
<h1 id="heterogeneous-graph-neural-network">Heterogeneous Graph Neural
Network</h1>
<ul>
<li>任务：图表示学习</li>
<li>异构类型：节点异构、边异构、节点多属性</li>
<li>解决办法：
<ul>
<li>四步走：异构邻居采样、多属性编码、同类型邻居聚合、不同类型聚合</li>
<li>异构邻居采样：基于重启的随机游走，先随机游走，且有一定概率p返回初始节点（重启），直到采样了一定数量的邻域节点。每种类型的邻域节点有上限值以确保所有类型的邻居都能采样到。再等比例缩小，对每个邻域节点类型<span
class="math inline">\(t\)</span>，只取<span
class="math inline">\(K_t\)</span>个邻域节点，分好组。</li>
<li>多属性编码：根据多模态内容，预先编好码，例如文本用paragraph2vec，图像用CNN，同一邻域节点的不同属性信息用BiLSTM编码</li>
<li>同类型邻居聚合：用BiLSTM聚合同类型下多个邻域节点的特征</li>
<li>不同类型聚合：再用一个注意力机制聚合不同类型的特征</li>
</ul></li>
</ul>
<h1 id="heterogeneous-graph-attention-network">Heterogeneous Graph
Attention Network</h1>
<ul>
<li>任务：图表示学习</li>
<li>异构类型：节点异构</li>
<li>解决办法：
<ul>
<li><p>实现node-level和metapath-level的双层注意力。</p></li>
<li><p>需要在所有的metapath
neighbour里加上自身，类似于GCN里的内环。</p></li>
<li><p>node-level注意力，对一条metapath上的不同节点进行注意力加权。因为不同类型的节点特征表示空间不同，因此针对每一种类型对应一个特征转换矩阵，将不同类型节点映射到同一空间，之后通过自注意力机制对节点进行注意力的计算和加权(其中<span
class="math inline">\(\phi\)</span>代表metapath)：</p>
<p><span class="math display">\[
h_i^{\prime} = M_{\phi _i} \cdot h_i \\
e^{\phi}_{ij} = attn_{node}(h^{\prime}_i,h^{\prime}_j;\phi) \\
\]</span></p>
<p>在计算attention需要做mask，只对邻域节点计算attention并做softmax。值得注意的是这里的自注意力的非对称性对异构图来说很重要，因为在一个节点对里，两个节点的邻域不同，相互的影响不是等量的。简单来说，对某一个节点，计算其在某一类metapath下所有邻域节点的注意力权重,输入是两个节点的h以及metapath
specific的一个参数向量，输出注意力权重，然后对某一节点，加权求和其某一类metapath下所有邻域节点的h，得到该节点的某一metapath的表示。</p>
<p><span class="math display">\[
\alpha_{i j}^{\Phi}=\operatorname{softmax}_{j}\left(e_{i
j}^{\Phi}\right)=\frac{\exp
\left(\sigma\left(\mathbf{a}_{\Phi}^{\mathrm{T}}
\cdot\left[\mathbf{h}_{i}^{\prime} \|
\mathbf{h}_{j}^{\prime}\right]\right)\right)}{\sum_{k \in
\mathcal{N}_{i}^{\mathrm{\Phi}}} \exp
\left(\sigma\left(\mathbf{a}_{\Phi}^{\mathrm{T}}
\cdot\left[\mathbf{h}_{i}^{\prime} \|
\mathbf{h}_{k}^{\prime}\right]\right)\right)} \\
\]</span></p>
<p>计算出注意力之后对变换后的metapath邻域节点特征加权：</p>
<p><span class="math display">\[
\mathbf{z}_{i}^{\Phi}=\sigma\left(\sum_{j \in \mathcal{N}_{i}^{\Phi}}
\alpha_{i j}^{\Phi} \cdot \mathbf{h}_{j}^{\prime}\right)
\]</span></p></li>
<li><p>在实际加权的时候作者参考了multi-head的做法，计算了k个attention加权特征并拼接起来</p></li>
<li><p>metapath-level的attention即对某一节点所有不同类的metapath
embedding进行加权。先将每一条metapath的embedding变换到同一隐空间，然后参数化计算出注意力权重并softmax。需要注意的是softmax之前的attention
logit是在所有节点上计算某一类型metapath的平均，分母是节点数，分子是包含该类型metapath的节点的metapath
embedding累加，而之前的node-level是针对某一节点某一metapath下所有的邻域节点平均：</p>
<p><span class="math display">\[
w_{\Phi_{i}}=\frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}}
\mathbf{q}^{\mathrm{T}} \cdot \tanh \left(\mathbf{W} \cdot
\mathbf{z}_{i}^{\Phi}+\mathbf{b}\right)
\]</span></p></li>
<li><p>之后再在所有metapath类型上做softmax，得到权重，加权每个节点不同metapath
embedding得到最终embedding</p></li>
<li><p>整个双层attention的logit以及softmax的范围有点绕，时而局部时而全局，需要仔细考虑清楚。</p></li>
</ul></li>
<li>可以看到整个过程是可以在节点层次并行化计算的</li>
<li>从结果来看效果达到了SOTA，而且可视化的结果可以看到节点embedding的聚类效果更好，attention也带来了一定可解释性。</li>
</ul>
<h1
id="heterogeneous-graph-attention-networks-for-semi-supervised-short-text-classification">Heterogeneous
Graph Attention Networks for Semi-supervised Short Text
Classification</h1>
<ul>
<li>任务：节点分类</li>
<li>异构类型：节点异构，包含三类节点，文本、实体、主题</li>
<li>解决办法：
<ul>
<li><p>最朴素：扩充节点的特征空间，将三类节点的特征向量拼接起来，对于具体的某一节点，其不包含的特征向量位置全设为0</p></li>
<li><p>异构图卷积：将相同节点类型的子图分离，每个子图单独做卷积，不同的子图通过参数变换矩阵投影到相同隐空间并相加激活作为下一层，具体而言，原始GCN为：</p>
<p><span class="math display">\[
H^{(l+1)}=\sigma\left(\tilde{A} \cdot H^{(l)} \cdot W^{(l)}\right)
\]</span></p>
<p>而异构GCN为：</p>
<p><span class="math display">\[
H^{(l+1)}=\sigma\left(\sum_{\tau \in \mathcal{T}} \tilde{A}_{\tau} \cdot
H_{\tau}^{(l)} \cdot W_{\tau}^{(l)}\right)
\]</span></p>
<p>其中<span
class="math inline">\(\tilde{A}_{\tau}\)</span>的行是所有节点，列是某一类型的所有节点，这样就抽离出了同构的连接子图，即对于每个节点，我们分别考虑他的邻域里类型a的节点，做信息聚合得到编码a，再考虑邻域里类型b的节点，做信息聚合得到编码b，编码a和b通过各自的变换矩阵变换到同一隐空间再相加。这样的设计是符合逻辑的。</p></li>
<li><p>作者还考虑了以下情况：对于某一节点，不同类型的邻域节点的贡献不一样，同一类型下不同的邻域节点贡献也不一样。显然这里需要注意力。作者就提出了对偶注意力(即双层注意力)，一层是type
level的，一层是node level的，先用某一类型邻域节点embedding的均值作为type
embedding，然后根据当前节点embedding与type embedding 计算出type
attention
weight，同理用具体的邻域节点embedding和当前节点embedding再加上type
attention得到node attention，利用计算出的node
attention替换GCN里的对称归一化邻接矩阵。</p></li>
</ul></li>
</ul>
<h1
id="representation-learning-for-attributed-multiplex-heterogeneous-network">Representation
Learning for Attributed Multiplex Heterogeneous Network</h1>
<ul>
<li>任务：图表示学习</li>
<li>异构类型：节点异构、边异构、节点多属性</li>
<li>解决办法：
<ul>
<li>考虑某一节点在不同类型边连接下有不同的embedding，将节点总的overall
embedding拆成与边无关的base embedding和与边相关的edge embedding</li>
<li>edge
embedding与边类型相关，通过相同类型的边相连的邻域节点aggregate得到，这里的aggreagator
function可以采用GraphSage里的做法。</li>
<li>经过k层聚合之后，对于每个节点都得到了k种边类型的edge
embedding，通过self attention将这些edge
embedding加权求和，乘上比例再加上base embedding就得到了最终的overall
embedding</li>
<li>以上是直推式(transductive)模型，对于未观测数据，需要归纳式(inductive)的方法。具体做法很简单，将base
embedding和edge
embedding参数化为节点attribute的函数，而不是随机初始化之后完全根据已有的图学习。这样即便有图中没看见的节点，只要节点有属性，一样可以进行overall
embedding的提取</li>
<li>最后做基于meta-path的random walk得到训练对，使用skip
gram训练，加入了负采样。</li>
</ul></li>
</ul>
<h1
id="semi-supervised-learning-over-heterogeneous-information-networks-by-ensemble-of-meta-graph-guided-random-walks">Semi-supervised
Learning over Heterogeneous Information Networks by Ensemble of
Meta-graph Guided Random Walks</h1>
<ul>
<li>任务：节点分类</li>
<li>异构类型：节点异构，包含三类节点，文本、实体、主题</li>
<li>解决办法：meta-path guided random walk</li>
</ul>
</div>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>graph neural network</tag>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>heterogeneous information network</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Hierarchical Latent Dirichlet Allocation</title>
    <url>/2019/11/15/hlda/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/cbd846ecd88abb611db2c204930d896d.png" width="500"></p>
<p>Note for Hierarchical Latent Dirichlet Allocation</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li>Still mainly referred to Prof. Yida Xu’s
tutorials<sup class="refplus-num"><a href="#ref-yidaxu">[1]</a></sup>.</li>
</ul>
<h1 id="improvements-of-hlda">Improvements of hLDA</h1>
<ul>
<li><p>Improved two points</p>
<ul>
<li><p>Introduced Dirichlet Process</p></li>
<li><p>Introduced Hierarchical Structure</p></li>
</ul></li>
</ul>
<h1 id="dp">DP</h1>
<ul>
<li><p>The Dirichlet Process extends the concept of Dirichlet
Distribution to a random process. Typically, sampling by probability
yields a sample, a value, while sampling by random process yields a
function, a distribution. Given the DP's hyperparameter <span class="math inline">\(\alpha\)</span>, a metric space <span class="math inline">\(\theta\)</span>, and a measure <span class="math inline">\(H\)</span> on this metric space (called base
distribution), sampling from <span class="math inline">\(DP(\alpha,H)\)</span> generates an
infinite-dimensional discrete distribution <span class="math inline">\(G\)</span> on <span class="math inline">\(\theta\)</span>. For any partition <span class="math inline">\(A_1,...,A_n\)</span> of <span class="math inline">\(\theta\)</span>, the partitioned <span class="math inline">\(G\)</span> still follows the Dirichlet
Distribution corresponding to the hyperparameters:</p>
<p><span class="math display">\[
(G(A_1,...,A_n))  \sim  Dir(\alpha H(A_1),...,\alpha H(A_n))
\]</span></p>
<p><span class="math inline">\(G\)</span> is defined as a sample
path/function/realization of the Dirichlet Process, i.e., <span class="math inline">\(G=DP(t,w_0) \sim \ DP(\alpha,H)\)</span>. A
realization of the Dirichlet Process is a probability measure, a
function defined on the metric space <span class="math inline">\(\theta\)</span>, with its output being a
probability. Note that due to its infinite-dimensionality, <span class="math inline">\(\alpha\)</span> cannot be preset to a specific
dimension, but only set to be the same <span class="math inline">\(\alpha\)</span>. Compared to LDA, we can see that
DP's hyperparameter <span class="math inline">\(\alpha\)</span> is a
concentration parameter that can only control the certainty of G
distribution trending towards uniformity, while the specific
distribution trend is determined by the partition <span class="math inline">\(A\)</span>.</p></li>
<li><p>Here we can see the difference from LDA's use of Dir
Distribution: DP directly samples to generate a probability measure,
which can further generate a discrete probability distribution; while in
LDA, sampling from Dir Distribution only yields a sample, which serves
as a parameter for the multinomial distribution, determining a discrete
distribution.</p></li>
<li><p>DP can be used to describe mixture models in scenarios with an
uncertain number of components. In a GMM scenario, if there are n
samples but we don't know how many GMs generated these n samples, for
sample i, we assign it to a certain GM, and let the parameters of this
GM for sample i be <span class="math inline">\(\theta _i\)</span>. This
<span class="math inline">\(\theta\)</span> follows a base distribution
<span class="math inline">\(H(\theta)\)</span>. If <span class="math inline">\(H\)</span> is a continuous distribution, the
probability of two samples taking the same <span class="math inline">\(\theta\)</span> approaches zero, equivalent to n
samples corresponding to n GMs. We can discretize this <span class="math inline">\(H\)</span> into G, with the discretization method
being <span class="math inline">\(G \sim DP(\alpha,H)\)</span>. The
smaller <span class="math inline">\(\alpha\)</span> is, the more
discrete it becomes; the larger <span class="math inline">\(\alpha\)</span> is, the closer G is to H. Note
that <span class="math inline">\(H\)</span> can also be
discrete.</p></li>
<li><p>The two parameters of DP, <span class="math inline">\(H\)</span>
and <span class="math inline">\(\alpha\)</span>, where the former
determines the location of each discrete point of <span class="math inline">\(G\)</span>, i.e., the specific value of <span class="math inline">\(\theta _i\)</span>; the latter determines the
degree of discreteness, or how dispersed <span class="math inline">\(\theta\)</span> is, whether the probability
distribution is concentrated or dispersed, consistent with the <span class="math inline">\(\alpha\)</span> in Dirichlet
Distribution.</p></li>
<li><p>Since G satisfies Dirichlet Distribution, it has many good
properties, including conjugacy to the multinomial distribution,
collapsing and splitting, and renormalization.</p>
<ul>
<li><p><span class="math inline">\(E[G(A_i)]=H(A_i)\)</span></p></li>
<li><p><span class="math inline">\(Var[G(A_i)]=\frac
{H(A_i)[1-H(A_i)]}{\alpha + 1}\)</span></p></li>
<li><p>We can see that when <span class="math inline">\(\alpha\)</span>
takes extreme values, the variance degenerates to 0 or the variance of a
Bernoulli distribution, corresponding to the two extreme cases of G
discretizing H that we mentioned earlier.</p></li>
</ul></li>
<li><p>So what do we want to do with DP? Create a generative model: we
want to obtain a probability measure <span class="math inline">\(G \sim
\ DP(H,\alpha)\)</span>, obtain the group parameter for each sample
point i based on <span class="math inline">\(G\)</span>: <span class="math inline">\(x_i \sim \  G\)</span>, and then generate the
sample point i based on this parameter and function <span class="math inline">\(F\)</span>: <span class="math inline">\(p_i \sim \
F(x_i)\)</span></p></li>
<li><p>Next, we can use the Chinese Restaurant Process (CRP), Stick
Breaking Process, and Polya Urn Model to refine this <span class="math inline">\(x_i\)</span>, that is, split the group parameter
corresponding to the sample point i into the group assignment and group
parameters, written as <span class="math inline">\(x_i=\phi
_{g_i}\)</span>, where <span class="math inline">\(g\)</span> is the
group assignment of the sample point, and <span class="math inline">\(\phi\)</span> is the group parameter.</p></li>
<li><p>Next, using <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process">echen's
description</a> to describe how three models refine <span class="math inline">\(x_i\)</span>:</p></li>
<li><p>In the Chinese Restaurant Process:</p>
<ul>
<li><p>We generate table assignments <span class="math inline">\(g_1,
\ldots, g_n \sim CRP(\alpha)\)</span> according to a Chinese Restaurant
Process. (<span class="math inline">\(g_i\)</span> is the table assigned
to datapoint <span class="math inline">\(i\)</span>.)</p></li>
<li><p>We generate table parameters <span class="math inline">\(\phi_1,
\ldots, \phi_m \sim G_0\)</span> according to the base distribution
<span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>Given table assignments and table parameters, we generate each
datapoint <span class="math inline">\(p_i \sim F(\phi_{g_i})\)</span>
from a distribution <span class="math inline">\(F\)</span> with the
specified table parameters. (For example, <span class="math inline">\(F\)</span> could be a Gaussian, and <span class="math inline">\(\phi_i\)</span> could be a parameter vector
specifying the mean and standard deviation).</p></li>
</ul></li>
<li><p>In the Polya Urn Model:</p>
<ul>
<li><p>We generate colors <span class="math inline">\(\phi_1, \ldots,
\phi_n \sim Polya(G_0, \alpha)\)</span> according to a Polya Urn Model.
(<span class="math inline">\(\phi_i\)</span> is the color of the ith
ball.)</p></li>
<li><p>Given ball colors, we generate each datapoint <span class="math inline">\(p_i \sim F(\phi_i)\)</span>.</p></li>
</ul></li>
<li><p>In the Stick-Breaking Process:</p>
<ul>
<li><p>We generate group probabilities (stick lengths) <span class="math inline">\(w_1, \ldots, w_{\infty} \sim
Stick(\alpha)\)</span> according to a Stick-Breaking process.</p></li>
<li><p>We generate group parameters <span class="math inline">\(\phi_1,
\ldots, \phi_{\infty} \sim G_0\)</span> from <span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>We generate group assignments <span class="math inline">\(g_1,
\ldots, g_n \sim Multinomial(w_1, \ldots, w_{\infty})\)</span> for each
datapoint.</p></li>
<li><p>Given group assignments and group parameters, we generate each
datapoint <span class="math inline">\(p_i \sim
F(\phi_{g_i})\)</span>.</p></li>
</ul></li>
<li><p>In the Dirichlet Process:</p>
<ul>
<li><p>We generate a distribution <span class="math inline">\(G \sim
DP(G_0, \alpha)\)</span> from a Dirichlet Process with base distribution
<span class="math inline">\(G_0\)</span> and dispersion parameter <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>We generate group-level parameters <span class="math inline">\(x_i \sim G\)</span> from <span class="math inline">\(G\)</span>, where <span class="math inline">\(x_i\)</span> is the group parameter for the ith
datapoint. (Note: this is not the same as <span class="math inline">\(\phi_i\)</span>. <span class="math inline">\(x_i\)</span> is the parameter associated to the
group that the ith datapoint belongs to, whereas <span class="math inline">\(\phi_k\)</span> is the parameter of the kth
distinct group.)</p></li>
<li><p>Given group-level parameters <span class="math inline">\(x_i\)</span>, we generate each datapoint <span class="math inline">\(p_i \sim F(x_i)\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="stick-breaking-process">Stick-Breaking Process</h1>
<ul>
<li><p>The Stick-Breaking Process provides an infinite division on <span class="math inline">\(\theta\)</span>. Let the DP parameters be <span class="math inline">\(\alpha\)</span>, and the process is as
follows:</p>
<ul>
<li><p><span class="math inline">\(\beta _1 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_1 = \beta _1\)</span></p></li>
<li><p><span class="math inline">\(\beta _2 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_2 = (1-\pi _1) * \beta
_2\)</span></p></li>
</ul></li>
<li><p>This way, each time a division on [0,1] is obtained from the Beta
distribution, cutting the entire <span class="math inline">\(\theta\)</span> into two parts. The first part is
taken as the first division on <span class="math inline">\(\theta\)</span>, and the remaining part is seen as
the whole for the next stick-breaking. Then, cut it into two parts
again, with the first part taken as the second division on <span class="math inline">\(\theta\)</span>. It's like a stick being
continuously broken, each time breaking from the remaining part, and the
final segments are the divisions.</p></li>
</ul>
<h1 id="dp2crp">DP2CRP</h1>
<ul>
<li><p>Introduce an indicator function. If two sample points i and j are
assigned to the same component, their indicator function <span class="math inline">\(z\)</span> is the same, which represents which
component each sample belongs to, <span class="math inline">\(x_i \sim
Component(\theta _{z_i})\)</span></p></li>
<li><p>For a mixture distribution, such as GMM, we want to obtain the
predictive distribution, that is, given the component assignment of
known data, for a new unknown data point, we want to know which
component it belongs to:</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i})
\]</span></p></li>
<li><p>From the definition, we know this probability should be
independent of <span class="math inline">\(H\)</span>, because we don't
care about the specific value of <span class="math inline">\(\theta\)</span>, we only care which <span class="math inline">\(\theta\)</span> it is, so the predictive
distribution is closely related to <span class="math inline">\(\alpha\)</span>. Expanding it:</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i}) = \frac {p(z_i=m,z_{not \ i})}{p(z_{not \ i})} \\
\]</span></p></li>
<li><p>Since in DP, the number of categories can be infinite, we first
assume k categories, and then let k approach infinity</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \
i}|p_1...p_k)p(p_1...p_k)}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)p(p_1...p_k)}
\]</span></p></li>
<li><p>The probabilities of these k categories follow a Dirichlet
Distribution. Assuming the Base Distribution is uniform, then</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \ i}|p_1...p_k)Dir(\frac
{\alpha}{k} ... \frac {\alpha}{k})}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)Dir(\frac {\alpha}{k} ... \frac{\alpha}{k})}
\]</span></p></li>
<li><p>In both numerator and denominator, the integral is essentially a
multinomial distribution multiplied by a Dirichlet distribution. Due to
conjugacy, the posterior should still be a Dirichlet distribution. We
derive the integral of the multinomial distribution multiplied by the
Dirichlet distribution:</p>
<p><span class="math display">\[
\int _{p_1...p_k} p(n_1...n_k|p_1...p_k) p(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} Mul(n_1...n_k|p_1...p_k) Dir(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} (\frac {n!}{n_1!...n_k!} \prod _{i=1}^k p_i ^{n_i})
\frac {\Gamma(\sum \alpha _i)}{\prod \Gamma (\alpha _i)} \prod _{i=1}^k
p_i^{\alpha _i -1} \\
\]</span></p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \int _{p_1...p_k} \prod _{i=1}^k  p_i^{n_i+\alpha _i -1} \\
\]</span></p></li>
<li><p>The integral term is actually a Dirichlet Distribution <span class="math inline">\(Dir(\alpha _1 + n_1 ... \alpha _k + n_k)\)</span>
excluding the constant part, so the integral result is 1/constant,
i.e.:</p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \frac { \prod \Gamma (\alpha _i + n_i)}{\Gamma (n + \sum
\alpha _i)}
\]</span></p></li>
<li><p>This expression includes three parts. The first part with n's is
introduced by the multinomial distribution, representing that we only
look at the size of each set after division, not the specific content of
each set, which is different from our requirements, so we don't need
this constant. The second part is generated by the Dir distribution
prior, and in the predictive distribution, the distribution priors are
all the same, so they cancel out. We mainly focus on the third part,
substituting it back into the predictive distribution fraction.</p></li>
<li><p>First, define an auxiliary variable <span class="math inline">\(n_{l , not \ i} = Count(z_{not \ i} ==
l)\)</span>, then:</p>
<p><span class="math display">\[
n_1 = n_{1,not \ i} \\
\]</span></p>
<p><span class="math display">\[
... \\
\]</span></p>
<p><span class="math display">\[
n_k = n_{k,not \ i} \\
\]</span></p></li>
<li><p>Because we are seeking <span class="math inline">\(p(z_i=m,
z_{not \ i})\)</span>, the number of other categories is already
determined by samples other than the ith sample. What about the mth
category?</p>
<p><span class="math display">\[
n_m = n_{m,not \ i} + 1
\]</span></p></li>
<li><p>This completes the transformation from indicator function
representation to multinomial distribution. Substituting the third part
of the previous derivation into the numerator gives:</p>
<p><span class="math display">\[
\frac {\Gamma(n_{m,not \ i} + \frac {\alpha}{k} + 1) \prod _{l=1,l \neq
m}^k Gamma(n_{l,not \ i})}{\Gamma (\alpha + n)}
\]</span></p></li>
<li><p>Similarly calculating the numerator, the numerator doesn't need
to consider the ith sample assigned to the mth category, so the form is
simpler:</p>
<p><span class="math display">\[
\frac {\prod _{l=1}^k \Gamma(n_{l,not \ i})}{\Gamma(\alpha +n -1)}
\]</span></p></li>
<li><p>Dividing the two expressions and using the property of the Gamma
function <span class="math inline">\(\Gamma(x) = (x-1) \Gamma
(x-1)\)</span> to simplify, we get:</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i} + \frac {\alpha}{k}}{n + \alpha - 1}
\]</span></p></li>
<li><p>Letting k approach infinity, we get:</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i}}{n + \alpha - 1}
\]</span></p></li>
<li><p>However, the sum of this expression for all categories from 1 to
m is not 1, but <span class="math inline">\(\frac {n-1}{n + \alpha
-1}\)</span>. The remaining probability is set as the probability of
taking a new category, thus completing the predictive distribution.
Interestingly, this probability corresponds exactly to the Chinese
Restaurant Process.</p></li>
</ul>
<h1 id="crp">CRP</h1>
<ul>
<li><p>The classic description of the Chinese Restaurant Process is to
distribute n people to an uncertain number of tables, creating a
partition on an integer set. Assuming each element in the set is a
customer, when the nth customer enters a restaurant, they choose a table
according to the following probabilities:</p>
<p><span class="math display">\[
\begin{aligned} p(\text { occupied table } i | \text { previous
customers }) &amp;=\frac{n_{i}}{\alpha +n-1} \\ p(\text { next
unoccupied table } | \text { previous customers }) &amp;=\frac{\alpha
}{\alpha +n-1} \end{aligned}
\]</span></p></li>
<li><p>Where <span class="math inline">\(n_i\)</span> is the number of
people already at table i, and <span class="math inline">\(\alpha\)</span> is the hyperparameter. This way,
the assignment of people to tables corresponds to a partition on the
integer set.</p></li>
<li><p>Analyzing this, if choosing an occupied table, customers tend to
choose tables with more people; if torn between occupied tables and a
new table, it depends on the hyperparameter <span class="math inline">\(\alpha\)</span></p></li>
<li><p>According to the previous derivation, this <span class="math inline">\(\alpha\)</span> is actually the hyperparameter of
the Dirichlet Distribution, and the effect completely matches. Since we
choose a uniform base distribution in CRP, the corresponding Dirichlet
Distribution chooses symmetric hyperparameters with the same <span class="math inline">\(alpha _i\)</span>. The larger <span class="math inline">\(\alpha\)</span> is, the more likely it is to
obtain an equal probability for each item in the Dirichlet Distribution
as a prior for the multinomial distribution. In the Chinese Restaurant
Process, this corresponds to each customer wanting to choose a new
table, so each table has only one person and is equally distributed.
Conversely, the smaller <span class="math inline">\(\alpha\)</span> is,
the less certain, and in the Chinese Restaurant Process, the table
assignments are also less certain.</p></li>
<li><p>We can obtain that the expected number of tables after the mth
person chooses is <span class="math inline">\(E(K_m|\alpha ) = O(\alpha
\log m)\)</span>, specifically <span class="math inline">\(E(K_m|\alpha
) = \alpha (\Psi (\alpha + n) - \Psi (\alpha )) \approx \alpha \log (1 +
\frac{n}{\alpha })\)</span>, which means the increase in the number of
clusters is linearly related to the logarithm of the sample size. We can
estimate the hyperparameter <span class="math inline">\(\alpha\)</span>
based on the amount of data and the desired number of clusters.</p></li>
</ul>
<h1 id="ncrp">nCRP</h1>
<ul>
<li><p>The above only completes an uncertain number of clustering using
DP. We can consider each table in the restaurant as a topic, people as
words, and the topic model as assigning words to topics, or people to
tables, but this is the same as LDA, with no correlation between topics.
To establish a hierarchical relationship between topics, Blei proposed
the Nested Chinese Restaurant Process.</p></li>
<li><p>In the Nested Chinese Restaurant Process, we unify the concepts
of restaurants and tables: restaurants are tables, tables are
restaurants! Why do we say this? First, we set a root restaurant
(obviously, we're building a tree), then choose a table in the root
restaurant according to the Chinese Restaurant Process. Each table in
the restaurant has a note indicating which restaurant the customer
should go to the next day. So the next day, the customer arrives at this
restaurant and chooses a table according to CRP, while also knowing
which restaurant to go to on the third day. Thus, tables correspond to
restaurants, and the tables of the parent restaurant correspond to child
restaurants. Each day is a layer of the tree, establishing a
hierarchical structure of the Chinese Restaurant Process.</p></li>
</ul>
<h1 id="hlda">hLDA</h1>
<ul>
<li><p>Now we can describe hLDA in the framework of nCRP</p></li>
<li><p>Define symbols</p>
<ul>
<li><p><span class="math inline">\(z\)</span>: topics, assuming <span class="math inline">\(K\)</span> topics</p></li>
<li><p><span class="math inline">\(\beta\)</span>: parameters from
topics to word distribution, Dir prior parameters</p></li>
<li><p><span class="math inline">\(w\)</span>: words</p></li>
<li><p><span class="math inline">\(\theta\)</span>: document-to-topic
distribution</p></li>
<li><p><span class="math inline">\(\alpha\)</span>: parameters of
document-to-topic distribution, Dir prior parameters</p></li>
</ul></li>
<li><p>We can simply define LDA:</p>
<p><span class="math display">\[
p(w | \beta) \sim Dir(\beta) \\
p(\theta | \alpha) \sim Dir(\alpha) \\
\theta \sim p(\theta | \alpha) \\
w \sim p(w | \theta , \beta) = \sum _{i=1}^K \theta _i p(w|z=i, \beta
_i) \\
\]</span></p></li>
<li><p>hLDA process:</p>
<ul>
<li><p>Obtain a path from root to leaf of length <span class="math inline">\(L\)</span> according to nCRP</p></li>
<li><p>Sample a topic distribution on the path from a <span class="math inline">\(L\)</span>-dimensional Dirichlet</p></li>
<li><p>Generate a word by mixing these L topics</p></li>
</ul></li>
<li><p>Detailed description: <a href="https://imgchr.com/i/Mwfu34"><img data-src="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg" alt="Mwfu34.md.jpg"></a></p></li>
<li><p>Probability graph, where <span class="math inline">\(c\)</span>
is the restaurant, nCRP is separately drawn out here. Actually, <span class="math inline">\(c\)</span> determines the topic <span class="math inline">\(z\)</span>, and <span class="math inline">\(\gamma\)</span> is the concentration parameter of
CRP corresponding to DP: <a href="https://imgchr.com/i/Mwf3Hx"><img data-src="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg" alt="Mwf3Hx.md.jpg"></a></p></li>
</ul>
<h1 id="gibbs-sampling-in-hlda">Gibbs Sampling in hLDA</h1>
<ul>
<li><p>Define variables:</p>
<ul>
<li><p><span class="math inline">\(w_{m,n}\)</span>: the nth word in the
mth document</p></li>
<li><p><span class="math inline">\(c_{m,l}\)</span>: the restaurant
corresponding to the topic at the lth layer in the path of the mth
document, needs to be sampled and calculated</p></li>
<li><p><span class="math inline">\(z_{m,n}\)</span>: the topic assigned
to the nth word in the mth document, needs to be sampled and
calculated</p></li>
</ul></li>
<li><p>The sampling formula from the posterior distribution is divided
into two parts. The first part is obtaining the path, which will use the
previous predictive distribution; the second part is known the path,
which is similar to ordinary LDA. The final sampling formula is:</p>
<p><span class="math display">\[
p\left(\mathbf{w}_{m} | \mathbf{c}, \mathbf{w}_{-m},
\mathbf{z}\right)=\prod_{\ell=1}^{L}\left(\frac{\Gamma\left(n_{c_{m,
\ell},-m}^{(\cdot)}+W \eta\right)}{\prod_{w} \Gamma\left(n_{c_{m,
e},-m}^{(w)}+\eta\right)} \frac{\prod_{w} \Gamma\left(n_{c_{m,
\ell},-m}^{(w)}+n_{c_{m, \ell},
m}^{(w)}+\eta\right)}{\Gamma\left(n_{c_{m, \ell},-m}^{(\cdot)}+n_{c_{m,
\ell}, m}^{(\cdot)}+W \eta\right)}\right)
\]</span></p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li>依然主要参考了徐亦达老师的教程<sup class="refplus-num"><a href="#ref-yidaxu">[1]</a></sup>。</li>
</ul>
<h1 id="hlda改进了什么">hLDA改进了什么</h1>
<ul>
<li><p>改进了两点</p>
<ul>
<li><p>引入了Dirichlet Process</p></li>
<li><p>引入了层次结构</p></li>
</ul></li>
</ul>
<h1 id="dp">DP</h1>
<ul>
<li><p>Dirichlet Process将Dirichlet
Distribution的概念扩展到随机过程，一般依概率采样会得到一个样本，一个值，而依据随机过程采样得到的是一个函数，是一个分布。给定DP的超参<span class="math inline">\(\alpha\)</span>，给定度量空间<span class="math inline">\(\theta\)</span>，以及该度量空间上的一个测度<span class="math inline">\(H\)</span>（称为基分布, Base Distribution)，<span class="math inline">\(DP(\alpha,H)\)</span>中采样得到的就是一个在<span class="math inline">\(\theta\)</span>上的无限维离散分布<span class="math inline">\(G\)</span>，假如对这个无限维（无限个离散点）做<span class="math inline">\(\theta\)</span>上的任意一种划分<span class="math inline">\(A_1,...,A_n\)</span>，那么划分之后的<span class="math inline">\(G\)</span>分布依然满足对应Dirichlet
Distribution在超参上的划分：</p>
<p><span class="math display">\[
(G(A_1,...,A_n))  \sim  Dir(\alpha H(A_1),...,\alpha H(A_n))
\]</span></p>
<p><span class="math inline">\(G\)</span>定义为Dirichlet
Process的一个sample path/function/realization,即<span class="math inline">\(G=DP(t,w_0) \sim \
DP(\alpha,H)\)</span>。Dirichelt
Process的一个realization是一个概率测度，是一个函数，定义域在度量空间<span class="math inline">\(\theta\)</span>上，函数输出即概率。注意因为是无限维，因此不能预先设置<span class="math inline">\(\alpha\)</span>的维数，只能设置为一样的<span class="math inline">\(\alpha\)</span>，对比LDA，可以看到DP的超参<span class="math inline">\(\alpha\)</span>是一个concentration
parameter，只能控制G分布趋于均匀分布的确定性，而不能控制G分布趋于怎样的分布，趋于怎样的分布由划分<span class="math inline">\(A\)</span>决定。</p></li>
<li><p>这里可以看到和LDA使用Dir
Distribution的区别：DP是直接采样生成了一个概率测度，可以进而生成离散的概率分布；而LDA中对Dir
Distribution采样只能得到一个样本，但是这个样本作为了多项式分布的参数，确定了一个多项式分布（也是离散的）。</p></li>
<li><p>DP可以用于描述混合模型，在混合组件数量不确定的情况下，通过DP来构造一个组件分配。放在GMM的场景里，假如有n个样本，但我不知道有几个GM来生成这n个样本，那么对样本i，我将其分配给某一个GM，称这个样本i所在GM的参数为<span class="math inline">\(\theta _i\)</span>，那么这个<span class="math inline">\(\theta\)</span>服从一个基分布<span class="math inline">\(H(\theta)\)</span>，假如<span class="math inline">\(H\)</span>是连续分布，那么两个样本取到相同的<span class="math inline">\(\theta\)</span>的概率趋于零，相当于n个样本对应n个GM，那么我们可以把这个<span class="math inline">\(H\)</span>离散化为G，离散的方式为<span class="math inline">\(G \sim DP(\alpha,H)\)</span>，<span class="math inline">\(\alpha\)</span>越小越离散，越大则<span class="math inline">\(G\)</span>越趋近于<span class="math inline">\(H\)</span>。注意<span class="math inline">\(H\)</span>也可以是离散的。</p></li>
<li><p>DP的两个参数，<span class="math inline">\(H\)</span>和<span class="math inline">\(\alpha\)</span>，前者决定了<span class="math inline">\(G\)</span>的每一个离散点的位置，即<span class="math inline">\(\theta
_i\)</span>具体的值；后者决定了离散程度，或者理解为<span class="math inline">\(\theta\)</span>有多分散，有多不重复，即概率分布是集中的还是分散的，这个Dirichlet
Distribution里的<span class="math inline">\(\alpha\)</span>是一致的。</p></li>
<li><p>由于G满足Dirichlet
Distribution,因此有很多好的性质，包括对于多项式分布的conjugate，collapsing和splitting，以及renormalization。</p>
<ul>
<li><p><span class="math inline">\(E[G(A_i)]=H(A_i)\)</span></p></li>
<li><p><span class="math inline">\(Var[G(A_i)]=\frac
{H(A_i)[1-H(A_i)]}{\alpha + 1}\)</span></p></li>
<li><p>可以看到<span class="math inline">\(\alpha\)</span>取极端时，方差分别退化为0或者伯努利分布的方差，对应着之前我们说的G去离散化H的两种极端情况。</p></li>
</ul></li>
<li><p>那么我们想用DP做什么，做一个生成式模型：我们想得到一个概率测度<span class="math inline">\(G \sim \ DP(H,\alpha)\)</span>，根据<span class="math inline">\(G\)</span>得到每一个样本点i所属的组对应的参数(Group
Parameter)<span class="math inline">\(x_i \sim
\  G\)</span>，之后根据这个参数和函数<span class="math inline">\(F\)</span>生成样本点i：<span class="math inline">\(p_i \sim \ F(x_i)\)</span></p></li>
<li><p>接下来可以用中国餐馆过程(CRP)、折棒过程(Stick Breaking)和Polya
Urm模型来细化这个<span class="math inline">\(x_i\)</span>，即将和样本点i对应组的参数拆成样本点i对应的组和每组的参数，写成<span class="math inline">\(x_i=\phi _{g_i}\)</span>，其中<span class="math inline">\(g\)</span>是样本点的组分配，<span class="math inline">\(\phi\)</span>是组参数。</p></li>
<li><p>接下来套用<a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process">echen大佬的描述</a>来描述三个模型如何细化<span class="math inline">\(x_i\)</span>的：</p></li>
<li><p>In the Chinese Restaurant Process:</p>
<ul>
<li><p>We generate table assignments <span class="math inline">\(g_1,
\ldots, g_n \sim CRP(\alpha)\)</span> according to a Chinese Restaurant
Process. (<span class="math inline">\(g_i\)</span> is the table assigned
to datapoint <span class="math inline">\(i\)</span>.)</p></li>
<li><p>We generate table parameters <span class="math inline">\(\phi_1,
\ldots, \phi_m \sim G_0\)</span> according to the base distribution
<span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>Given table assignments and table parameters, we generate each
datapoint <span class="math inline">\(p_i \sim F(\phi_{g_i})\)</span>
from a distribution <span class="math inline">\(F\)</span> with the
specified table parameters. (For example, <span class="math inline">\(F\)</span> could be a Gaussian, and <span class="math inline">\(\phi_i\)</span> could be a parameter vector
specifying the mean and standard deviation).</p></li>
</ul></li>
<li><p>In the Polya Urn Model:</p>
<ul>
<li><p>We generate colors <span class="math inline">\(\phi_1, \ldots,
\phi_n \sim Polya(G_0, \alpha)\)</span> according to a Polya Urn Model.
(<span class="math inline">\(\phi_i\)</span> is the color of the ith
ball.)</p></li>
<li><p>Given ball colors, we generate each datapoint <span class="math inline">\(p_i \sim F(\phi_i)\)</span>.</p></li>
</ul></li>
<li><p>In the Stick-Breaking Process:</p>
<ul>
<li><p>We generate group probabilities (stick lengths) <span class="math inline">\(w_1, \ldots, w_{\infty} \sim
Stick(\alpha)\)</span> according to a Stick-Breaking process.</p></li>
<li><p>We generate group parameters <span class="math inline">\(\phi_1,
\ldots, \phi_{\infty} \sim G_0\)</span> from <span class="math inline">\(G_0\)</span>, where <span class="math inline">\(\phi_k\)</span> is the parameter for the kth
distinct group.</p></li>
<li><p>We generate group assignments <span class="math inline">\(g_1,
\ldots, g_n \sim Multinomial(w_1, \ldots, w_{\infty})\)</span> for each
datapoint.</p></li>
<li><p>Given group assignments and group parameters, we generate each
datapoint <span class="math inline">\(p_i \sim
F(\phi_{g_i})\)</span>.</p></li>
</ul></li>
<li><p>In the Dirichlet Process:</p>
<ul>
<li><p>We generate a distribution <span class="math inline">\(G \sim
DP(G_0, \alpha)\)</span> from a Dirichlet Process with base distribution
<span class="math inline">\(G_0\)</span> and dispersion parameter <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>We generate group-level parameters <span class="math inline">\(x_i \sim G\)</span> from <span class="math inline">\(G\)</span>, where <span class="math inline">\(x_i\)</span> is the group parameter for the ith
datapoint. (Note: this is not the same as <span class="math inline">\(\phi_i\)</span>. <span class="math inline">\(x_i\)</span> is the parameter associated to the
group that the ith datapoint belongs to, whereas <span class="math inline">\(\phi_k\)</span> is the parameter of the kth
distinct group.)</p></li>
<li><p>Given group-level parameters <span class="math inline">\(x_i\)</span>, we generate each datapoint <span class="math inline">\(p_i \sim F(x_i)\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="折棒过程">折棒过程</h1>
<ul>
<li><p>折棒过程提供了一种在<span class="math inline">\(\theta\)</span>上的无限划分，依然令DP的参数为<span class="math inline">\(\alpha\)</span>，折棒过程如下：</p>
<ul>
<li><p><span class="math inline">\(\beta _1 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_1 = \beta _1\)</span></p></li>
<li><p><span class="math inline">\(\beta _2 \sim
Beta(1,\alpha)\)</span></p></li>
<li><p><span class="math inline">\(A_2 = (1-\pi _1) * \beta
_2\)</span></p></li>
</ul></li>
<li><p>这样每次从Beta分布中得到[0,1]上的一个划分，将整个<span class="math inline">\(\theta\)</span>切成两部分，第一部分作为<span class="math inline">\(\theta\)</span>上的第一个划分，剩下的部分看成下一次折棒的整体，接着从上面切两部分，第一部分作为<span class="math inline">\(\theta\)</span>上的第二个划分，像一个棒不断被折断，每次从剩下的部分里折，最后折成的分段就是划分。</p></li>
</ul>
<h1 id="dp2crp">DP2CRP</h1>
<ul>
<li><p>引入一个示性函数，假如两个样本点i,j他们被分配的组件相同，则他们的示性函数<span class="math inline">\(z\)</span>相同，也就是表征每一个样本属于哪一个组件，<span class="math inline">\(x_i \sim Component(\theta
_{z_i})\)</span></p></li>
<li><p>那么对于混合分布，比如GMM，我们希望得到的是predictive
distribution，即已知数据的组件分配情况下，新来了一个未知数据，我想知道他属于哪个组件：</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i})
\]</span></p></li>
<li><p>结合定义可以知道这个概率应该是和<span class="math inline">\(H\)</span>无关的，因为我不在乎<span class="math inline">\(\theta\)</span>具体的值，我只在乎是哪一个<span class="math inline">\(\theta\)</span>，所以predictive
distribution与<span class="math inline">\(\alpha\)</span>密切相关。将其展开：</p>
<p><span class="math display">\[
p(z_i=m|z_{not \ i}) = \frac {p(z_i=m,z_{not \ i})}{p(z_{not \ i})} \\
\]</span></p></li>
<li><p>由于在DP里是划分的类别数可以到无穷多个，因此这里采用了一个小技巧，我们先假设有k类，之后在把k趋于无穷</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \
i}|p_1...p_k)p(p_1...p_k)}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)p(p_1...p_k)}
\]</span></p></li>
<li><p>这里的k个类的概率是符合Dirichlet Distribution的，假设这里的Base
Distribution是均匀分布，则</p>
<p><span class="math display">\[
= \frac {\int _{p_1...p_k} p(z_i=m, z_{not \ i}|p_1...p_k)Dir(\frac
{\alpha}{k} ... \frac {\alpha}{k})}{\int _{p_1...p_k} p(z_{not \
i}|p_1...p_k)Dir(\frac {\alpha}{k} ... \frac{\alpha}{k})}
\]</span></p></li>
<li><p>上面无论分子分母，积分内其实都是一个多项式分布乘以一个Dirichlet分布，根据共轭我们知道后验应该还是一个Dirichlet分布，我们推导一下多项式分布与Dirichlet分布相乘的积分：</p>
<p><span class="math display">\[
\int _{p_1...p_k} p(n_1...n_k|p_1...p_k) p(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} Mul(n_1...n_k|p_1...p_k) Dir(p_1...p_k|\alpha _1 ...
\alpha _k) \\
\]</span></p>
<p><span class="math display">\[
= \int _{p_1...p_k} (\frac {n!}{n_1!...n_k!} \prod _{i=1}^k p_i ^{n_i})
\frac {\Gamma(\sum \alpha _i)}{\prod \Gamma (\alpha _i)} \prod _{i=1}^k
p_i^{\alpha _i -1} \\
\]</span></p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \int _{p_1...p_k} \prod _{i=1}^k  p_i^{n_i+\alpha _i -1} \\
\]</span></p></li>
<li><p>其中积分式内实际上是一个Dirichelt Distribution<span class="math inline">\(Dir(\alpha _1 + n_1 ... \alpha _k +
n_k)\)</span>排除了常数部分，因此积分的结果就是1/常数，即：</p>
<p><span class="math display">\[
= \frac {n!}{n_1!...n_k!} \frac {\Gamma(\sum \alpha _i)}{\prod \Gamma
(\alpha _i)} \frac { \prod \Gamma (\alpha _i + n_i)}{\Gamma (n + \sum
\alpha _i)}
\]</span></p></li>
<li><p>上式包括了三个部分，第一部分的一堆n，它是由多项式分布引入的，代表我们只看划分后每个集合的大小，而不看划分之后每个集合具体的内容，这和我们的需求是不一样的，因此不需要这个常数；第二个部分，是由Dir分布先验产生的，而在predictive
distribution中，分布先验都相同，因此抵消了，所以我们主要关注第三部分，回代入predictive
distribution那个分式当中。</p></li>
<li><p>首先定义一个辅助变量<span class="math inline">\(n_{l , not \ i} =
Count(z_{not \ i} == l)\)</span>，那么：</p>
<p><span class="math display">\[
n_1 = n_{1,not \ i} \\
\]</span></p>
<p><span class="math display">\[
... \\
\]</span></p>
<p><span class="math display">\[
n_k = n_{k,not \ i} \\
\]</span></p></li>
<li><p>因为我们是是在求<span class="math inline">\(p(z_i=m, z_{not \
i})\)</span>，那么肯定除了第m类，其余类的数量早已由除了第i个样本以外的样本确定，那么第m类呢？</p>
<p><span class="math display">\[
n_m = n_{m,not \ i} + 1
\]</span></p></li>
<li><p>这样我们就完成了从指示函数表示的概率到多项式分布的转换，分子部分代入之前得到的第三部分有：</p>
<p><span class="math display">\[
\frac {\Gamma(n_{m,not \ i} + \frac {\alpha}{k} + 1) \prod _{l=1,l \neq
m}^k Gamma(n_{l,not \ i})}{\Gamma (\alpha + n)}
\]</span></p></li>
<li><p>同理计算分子，分子不用考虑第i个样本分给第m类，因此不用在累乘里单独拎出来第m项，形式要简单一些：</p>
<p><span class="math display">\[
\frac {\prod _{l=1}^k \Gamma(n_{l,not \ i})}{\Gamma(\alpha +n -1)}
\]</span></p></li>
<li><p>将上面两式相除，再利用Gamma函数<span class="math inline">\(\Gamma(x) = (x-1) \Gamma
(x-1)\)</span>的性质简化，得到：</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i} + \frac {\alpha}{k}}{n + \alpha - 1}
\]</span></p></li>
<li><p>再令k趋于无穷，得到：</p>
<p><span class="math display">\[
= \frac {n_{m,not \ i}}{n + \alpha - 1}
\]</span></p></li>
<li><p>但是上面这个式子对所有的类别从1到m求和并不为1，而是<span class="math inline">\(\frac {n-1}{n + \alpha
-1}\)</span>，剩下一部分概率就设为取一个新类别的概率，这样我们的predictive
distribution就算完成了，而且可以发现，这个概率，实际上就对应着中国餐馆过程。</p></li>
</ul>
<h1 id="crp">CRP</h1>
<ul>
<li><p>中国餐馆过程的经典描述就是把n个人，一个一个人来，分到不确定张数目的桌子上，做一个整数集合上的划分。假设集合每个元素是一位顾客，第n位顾客走进了一家参观，则他按照以下概率去选择某一张已经有人的桌子坐下，或者找一张没人的新桌子坐下：</p>
<p><span class="math display">\[
\begin{aligned} p(\text { occupied table } i | \text { previous
customers }) &amp;=\frac{n_{i}}{\alpha +n-1} \\ p(\text { next
unoccupied table } | \text { previous customers }) &amp;=\frac{\alpha
}{\alpha +n-1} \end{aligned}
\]</span></p></li>
<li><p>其中<span class="math inline">\(n_i\)</span>是第i张桌子上已经有的人数，$$是超参数。这样人到桌子的分配就对应了整数集合上的划分。</p></li>
<li><p>分析一下，若是选择已经有人的桌子，则顾客倾向于选择人多的桌子；若是在有人的桌子与新桌子之间纠结，则依赖于超参$$</p></li>
<li><p>那根据之前的推导，这个<span class="math inline">\(\alpha\)</span>其实就是Dirichlet
Distribution的超参数，且效果完全吻合。由于在CRP中我们base
distribution选的是均匀分布，那对应的Dirichlet
Distribution选择对称超参，各个<span class="math inline">\(alpha
_i\)</span>相同。那么<span class="math inline">\(\alpha\)</span>越大，以Dirichlet
Distritbuion为参数先验的多项式分布里，取得各个项等概率的可能就越大，在中国餐馆过程中对应着每个顾客进来都想选择一张新桌子，因此每个桌子都只有一个人，等量分配；反之<span class="math inline">\(\alpha\)</span>越小则越不确定，在中国餐馆过程中桌子的分配也不确定</p></li>
<li><p>可以得到第m个人选择之后，桌子数量的期望是<span class="math inline">\(E(K_m|\alpha ) = O(\alpha \log
m)\)</span>，具体而言是<span class="math inline">\(E(K_m|\alpha ) =
\alpha (\Psi (\alpha + n) - \Psi (\alpha )) \approx \alpha \log (1 +
\frac{n}{\alpha })\)</span>，
也就是聚类数的增加与样本数的对数成线性关系。我们可以根据数据量和想要聚类的数量来反估计超参<span class="math inline">\(\alpha\)</span>的设置。</p></li>
</ul>
<h1 id="ncrp">nCRP</h1>
<ul>
<li><p>以上仅仅完成了一个利用了DP的不确定数目聚类，我们可以认为餐馆里每个桌子是一个主题，人就是单词，主题模型就是把词分配到主题，把人分配到桌子，但是这样的话和LDA一样，主题之间没有关联。为了建立主题之间的层次关系，Blei提出了嵌套餐馆过程。</p></li>
<li><p>在嵌套餐馆过程中，我们统一了餐馆和桌子的概念，餐馆就是桌子，桌子就是餐馆！为什么这么说？首先我们设置一个餐馆作为root餐馆（显然我们要建立一棵树了），然后根据中国餐馆过程选择root餐馆里的一个桌子，餐馆里的每个桌子上都有一张纸条指示顾客第二天去某一个餐馆，因此第二天顾客来到这个餐馆，接着根据CRP选个桌子，同时知晓了自己第三天该去哪个参观。因此桌子对应着餐馆，父节点餐馆的桌子对应着子节点餐馆，每一天就是树的每一层，这样就建立了一个层次结构的中国餐馆过程。</p></li>
</ul>
<h1 id="hlda">hLDA</h1>
<ul>
<li><p>接下来我们可以在nCRP的框架上描述hLDA</p></li>
<li><p>定义符号</p>
<ul>
<li><p><span class="math inline">\(z\)</span>：主题，假设有<span class="math inline">\(K\)</span>个</p></li>
<li><p><span class="math inline">\(\beta\)</span>：主题到词分布的参数，Dir先验参数</p></li>
<li><p><span class="math inline">\(w\)</span>：词</p></li>
<li><p><span class="math inline">\(\theta\)</span>：文档到主题的分布</p></li>
<li><p><span class="math inline">\(\alpha\)</span>：文档到主题分布的参数，Dir先验参数</p></li>
</ul></li>
<li><p>那么可以简单定义LDA：</p>
<p><span class="math display">\[
p(w | \beta) \sim Dir(\beta) \\
p(\theta | \alpha) \sim Dir(\alpha) \\
\theta \sim p(\theta | \alpha) \\
w \sim p(w | \theta , \beta) = \sum _{i=1}^K \theta _i p(w|z=i, \beta
_i) \\
\]</span></p></li>
<li><p>hLDA流程如下：</p>
<ul>
<li><p>根据nCRP获得一条从root到leaf的长为<span class="math inline">\(L\)</span>的路径</p></li>
<li><p>根据一个<span class="math inline">\(L\)</span>维的Dirichlet采样一个在路径上的主题分布</p></li>
<li><p>根据这L个主题混合生成一个词</p></li>
</ul></li>
<li><p>详细描述如下： <a href="https://imgchr.com/i/Mwfu34"><img data-src="https://s2.ax1x.com/2019/11/16/Mwfu34.md.jpg" alt="Mwfu34.md.jpg"></a></p></li>
<li><p>概率图如下，其中<span class="math inline">\(c\)</span>是餐馆，这里把nCRP单独拎出来了，实际上<span class="math inline">\(c\)</span>决定了主题<span class="math inline">\(z\)</span>，另外<span class="math inline">\(\gamma\)</span>是nCRP中CRP对应DP的concentration
paramter： <a href="https://imgchr.com/i/Mwf3Hx"><img data-src="https://s2.ax1x.com/2019/11/16/Mwf3Hx.md.jpg" alt="Mwf3Hx.md.jpg"></a></p></li>
</ul>
<h1 id="gibbs-sampling-in-hlda">Gibbs Sampling in hLDA</h1>
<ul>
<li><p>定义变量：</p>
<ul>
<li><p><span class="math inline">\(w_{m,n}\)</span>：第m篇文档里的第n个词</p></li>
<li><p><span class="math inline">\(c_{m,l}\)</span>：第m篇文档里路径上第l层选择的主题对应的餐馆，需要采样计算</p></li>
<li><p><span class="math inline">\(z_{m,n}\)</span>：第m篇文档里第n个词分配的主题，需要采样计算</p></li>
</ul></li>
<li><p>从后验分布中采样的公式分为两部分，第一部分是得到路径，这一部分就会利用到之前的predictive
distribution；第二部分是已知路径，剩下的部分就是普通的LDA，最终采样公式为：</p>
<p><span class="math display">\[
p\left(\mathbf{w}_{m} | \mathbf{c}, \mathbf{w}_{-m},
\mathbf{z}\right)=\prod_{\ell=1}^{L}\left(\frac{\Gamma\left(n_{c_{m,
\ell},-m}^{(\cdot)}+W \eta\right)}{\prod_{w} \Gamma\left(n_{c_{m,
e},-m}^{(w)}+\eta\right)} \frac{\prod_{w} \Gamma\left(n_{c_{m,
\ell},-m}^{(w)}+n_{c_{m, \ell},
m}^{(w)}+\eta\right)}{\Gamma\left(n_{c_{m, \ell},-m}^{(\cdot)}+n_{c_{m,
\ell}, m}^{(\cdot)}+W \eta\right)}\right)
\]</span></p></li>
</ul>
</div>
<ul id="refplus"><li id="ref-yidaxu" data-num="1">[1]  徐亦达机器学习课程 Variational Inference for LDA https://www.youtube.com/watch?v=e1wr0xHbfYk</li></ul>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>topic model</tag>
        <tag>lda</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Linear Algebra 3</title>
    <url>/2017/01/22/LinearAlgebra3/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/fcab3101f40bfbdcf2ec3e30b6171a26.png" width="500"/></p>
<h1 id="lecture-17-determinants-and-their-properties">Lecture 17:
Determinants and Their Properties</h1>
<h2 id="determinant">Determinant</h2>
<ul>
<li><p>The determinant of matrix A is a number associated with the
matrix, denoted as <span
class="math inline">\(detA或者|A|\)</span></p></li>
<li><p>Properties of determinants</p>
<ul>
<li><p><span class="math inline">\(detI=1\)</span></p></li>
<li><p>The sign of the determinant value will be reversed when rows are
exchanged</p></li>
<li><p>The determinant of a permutation matrix is 1 or -1, depending on
the parity of the number of rows exchanged</p></li>
<li><p>Two rows being equal makes the determinant equal to 0 (which can
be directly deduced from property two)</p></li>
<li><p>Matrix elimination does not change its determinant (proof is
below)</p></li>
<li><p>A certain row is 0, the determinant is 0 (multiplying by 0 is
equivalent to a certain row being 0, resulting in 0)</p></li>
<li><p>When and only when A is a singular matrix</p></li>
<li><p><span class="math inline">\(det(A+B) \neq detA+detB \\
detAB=(detA)(detB)\)</span></p></li>
<li><p><span class="math inline">\(detA^{-1}detA=1\)</span></p></li>
<li><p><span class="math inline">\(detA^2=(detA)^2\)</span></p></li>
<li><p><span class="math inline">\(det2A=2^n detA\)</span></p></li>
<li><p><span class="math inline">\(detA^T=detA\)</span> (Proof see
below)</p></li>
</ul></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li><p>The determinant is linear by row, but the determinant itself is
not linear</p>
<p><span class="math display">\[
\begin{vmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{vmatrix}=1 \\
\begin{vmatrix}
0 &amp; 1 \\
1 &amp; 0 \\
\end{vmatrix}=-1 \\
\begin{vmatrix}
ta &amp; tb \\
c &amp; d \\
\end{vmatrix}=
t\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix} \\
\begin{vmatrix}
t+a &amp; t+b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
t &amp; t \\
c &amp; d \\
\end{vmatrix}
\]</span></p></li>
<li><p>Proof that elimination does not change the determinant</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; b \\
c-la &amp; d-lb \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}-l
\begin{vmatrix}
a &amp; b \\
a &amp; b \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}
\]</span></p></li>
<li><p>Proof that the transpose does not change the determinant</p>
<p><span class="math display">\[
A=LU \\
\]</span></p></li>
<li><p>Translation: <span class="math inline">\(|U^TL^T|=|LU|\)</span>
<span class="math display">\[
|U^T||L^T|=|L||U|
\]</span></p></li>
<li><p>The above four matrices are all triangular matrices, the
determinant equals the product of the diagonal elements, the transpose
has no effect, so they are equal</p></li>
</ul>
<h2 id="triangular-matrix-determinant">Triangular matrix
determinant</h2>
<ul>
<li>The determinant of the triangular matrix U is the product of the
elements on the diagonal (the pivot product)</li>
<li>Why do the other elements of the triangular matrix not work? Because
by elimination we can obtain a matrix with only diagonal elements, and
elimination does not change the determinant</li>
<li>Why is it the product of the diagonal elements? Because after
elimination, the diagonal elements can be successively extracted,
yielding <span class="math inline">\(d_1d_2d_3...d_nI\)</span> , where
the determinant of the unit matrix is 1</li>
<li>The determinant of a singular matrix is 0, and it has rows of all
zeros; the determinant of an invertible matrix is not 0, and it can be
reduced to a triangular matrix, with the determinant being the product
of the diagonal elements of the triangular matrix</li>
</ul>
<h2 id="a-little-more">A little more</h2>
<ul>
<li>The determinant obtained from odd-numbered permutations and
even-numbered permutations is definitely different (signs differ), which
means the matrices after odd-numbered and even-numbered permutations
will not be the same, i.e., permutations strictly distinguish between
odd and even</li>
</ul>
<h1
id="eighteenth-lecture-determinant-formulas-and-algebraic-cofactors">Eighteenth
Lecture: Determinant Formulas and Algebraic Cofactors</h1>
<h2 id="determinant-formula">Determinant formula</h2>
<ul>
<li><p>Derive the 2x2 determinant</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; 0 \\
c &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; 0 \\
c &amp; 0 \\
\end{vmatrix}+
\begin{vmatrix}
a &amp; 0 \\
0 &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
c &amp; 0 \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
0 &amp; d \\
\end{vmatrix} \\
=0+ad-bc+0
\]</span></p>
<p>We can find that this method involves taking one row at a time,
decomposing this row (determinants are linear by rows), extracting
factors, obtaining the unit matrix through row exchanges, and then
obtaining the answer through properties one and two</p></li>
<li><p>If expanded to a 3x3 matrix, the first row is decomposed into
three parts, each of which is further decomposed into three parts for
the second row, resulting in a total of 27 parts. The parts that are not
zero are those matrices where there are elements in each row and
column.</p></li>
<li><p>For example</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; 0 &amp; 0\\
0 &amp; 0 &amp; b\\
0 &amp; c &amp; 0\\
\end{vmatrix}
\]</span></p>
<p>Extract the factors to obtain <span
class="math inline">\(abc\)</span> , swap the second and third rows to
get the identity matrix, so the answer is <span
class="math inline">\(abc*detI=abc\)</span> , and since a row swap was
performed, the answer is negative, <span
class="math inline">\(-abc\)</span></p></li>
<li><p>A matrix of size n*n can be divided into <span
class="math inline">\(n!\)</span> parts, because the first row is
divided into n parts, the second row cannot be repeated, and n-1 rows
are chosen, each with one repetition, thus obtaining <span
class="math inline">\(n!\)</span> parts</p></li>
<li><p>The determinant formula is the sum of these <span
class="math inline">\(n!\)</span> parts</p></li>
</ul>
<h2 id="algebraic-cofactor">Algebraic cofactor</h2>
<ul>
<li><span
class="math inline">\(det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)\)</span></li>
<li>Extract a factor, the remainder formed by the remaining factors,
i.e., the content within the parentheses, is the minor determinant</li>
<li>From the matrix perspective, selecting an element, its algebraic
cofactor is the determinant of the matrix obtained by excluding the row
and column of this element</li>
<li>The algebraic cofactor of <span
class="math inline">\(a_{ij}\)</span> is denoted as <span
class="math inline">\(c_{ij}\)</span></li>
<li>Pay attention to the sign of the algebraic cofactor, which is
related to the parity of <span class="math inline">\(i+j\)</span> . Even
numbers take the positive sign, and odd numbers take the negative sign.
Here, the symbol refers to the sign in front of the determinant after
the normal calculation of the submatrix corresponding to the algebraic
cofactor</li>
<li><span
class="math inline">\(detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}\)</span></li>
</ul>
<h1 id="th-lecture-cramers-rule-inverse-matrix-volume">19th Lecture:
Cramer's Rule, Inverse Matrix, Volume</h1>
<h2 id="invertible-matrix">Invertible matrix</h2>
<ul>
<li><p>Only when the determinant is not zero is the matrix
invertible</p></li>
<li><p>Invertible matrix formula</p>
<p><span class="math display">\[
A^{-1}=\frac{1}{detA}C^T
\]</span></p>
<p>The algebraic cofactor of <span class="math inline">\(C_{ij}\)</span>
is <span class="math inline">\(A_{ij}\)</span></p></li>
<li><p>Proof: i.e., prove <span
class="math inline">\(AC^T=(detA)I\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} &amp; ... &amp; a_{1n} \\
a_{n1} &amp; ... &amp; a_{nn} \\
\end{bmatrix}
\begin{bmatrix}
c_{11} &amp; ... &amp; c_{n1} \\
c_{1n} &amp; ... &amp; c_{nn} \\
\end{bmatrix}=
\begin{bmatrix}
detA &amp; 0 &amp; 0 \\
0 &amp; detA &amp; 0 \\
0 &amp; 0 &amp; detA \\
\end{bmatrix}
\]</span></p>
<p>On the diagonal are determinants, because <span
class="math inline">\(det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)\)</span>
other positions are all 0, because the algebraic cofactor of row a
multiplied by row b is equivalent to calculating the determinant of a
matrix where row a and row b are equal, and the determinant is
0</p></li>
</ul>
<h2 id="kramers-rule">Kramer's Rule</h2>
<ul>
<li><p>Ax=b</p>
<p><span class="math display">\[
Ax=b \\
x=A^{-1}b \\
x=\frac{1}{detA}C^Tb \\
\\
x_1=\frac{detB_1}{detA} \\
x_3=\frac{detB_2}{detA} \\
... \\
\]</span></p></li>
<li><p>Kramer's rule states that the determinant of matrix <span
class="math inline">\(B_i\)</span> is obtained by replacing the ith
column of matrix <span class="math inline">\(A\)</span> with b, while
keeping the rest unchanged</p></li>
</ul>
<h2 id="volume">Volume</h2>
<ul>
<li><p>The determinant of A can represent a volume, for example, the
determinant of a 3x3 matrix represents a volume within a
three-dimensional space</p></li>
<li><p>Each row of the matrix represents one edge of a box (originating
from the same vertex), and the determinant is the volume of the box; the
sign of the determinant represents the left-hand or right-hand
system.</p></li>
<li><ol type="1">
<li>The unit matrix corresponds to the unit cube, with a volume of
1</li>
</ol></li>
<li><p>For the orthogonal matrix Q,</p>
<p><span class="math display">\[
QQ^T=I \\
|QQ^T|=|I| \\
|Q||Q^T|=1 \\
{|Q|}^2=1 \\
|Q|=1 \\
\]</span></p>
<p>The box corresponding to Q is the unit cube corresponding to the unit
matrix rotated by an angle in space</p></li>
<li><p>(3a) If a row of a matrix is doubled, i.e., one set of edges of
the box is doubled, the volume is also doubled. From the perspective of
determinants, the factor can be factored out, so the determinant is also
doubled</p></li>
<li><ol start="2" type="1">
<li>Swapping two rows of a permutation matrix does not change the volume
of the box</li>
</ol></li>
<li><p>(3b) A row of the matrix is split, and the box is also divided
into two parts accordingly</p></li>
<li><p>The above, the three properties of determinants (1, 2, 3a, 3b)
can all be verified in terms of volume</p></li>
</ul>
<h1 id="lecture-20-eigenvalues-and-eigenvectors">Lecture 20: Eigenvalues
and Eigenvectors</h1>
<h2 id="feature-vector">Feature vector</h2>
<ul>
<li>Given matrix A, matrix A can be regarded as a function acting on a
vector x, resulting in the vector Ax</li>
<li>When \( \mathbf{A} \) is parallel to \( \mathbf{x} \), i.e., \(
\frac{\partial}{\partial x} \), we call \( \mathbf{v} \) the eigenvector
and \( \lambda \) the eigenvalue</li>
<li>If A is a singular matrix, <span class="math inline">\(\lambda =
0\)</span> is an eigenvalue</li>
</ul>
<h2 id="several-examples">Several examples</h2>
<ul>
<li><p>If A is a projection matrix, it can be observed that its
eigenvectors are any vectors on the projection plane, because <span
class="math inline">\(Ax\)</span> represents the projection onto the
plane, and all vectors on the plane remain unchanged after projection,
thus being parallel. At the same time, the eigenvalues are 1. If a
vector is perpendicular to the plane, <span
class="math inline">\(Ax=0\)</span> , the eigenvalue is 0. Therefore,
the eigenvectors of the projection matrix A fall into two cases, with
eigenvalues of 1 or 0.</p></li>
<li><p>Another example</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
0 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix} \\
\lambda =1, x=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix} \\
\lambda =-1, x=
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>An n*n matrix has n eigenvalues</p></li>
<li><p>The sum of the eigenvalues equals the sum of the diagonal
elements, this sum being called the trace</p></li>
<li><p>How to solve <span class="math inline">\(Ax=\lambda
x\)</span></p>
<p><span class="math display">\[
(A-\lambda I)x=0 \\
\]</span></p></li>
<li><p>The visible equation has non-zero solutions, <span
class="math inline">\((A-\lambda I)\)</span> must be singular, i.e.:</p>
<p><span class="math display">\[
det(A-\lambda I)=0 \\
\]</span></p></li>
<li><p><span class="math display">\[
If \qquad Ax=\lambda x \\
Then \qquad (A+3I)x=(\lambda +3)x \\
\]</span></p></li>
<li><p>Because the unit matrix is added, the eigenvector remains
unchanged as x, and the eigenvalue is increased by the coefficient of
the unit matrix, i.e., <span class="math inline">\((\lambda
+3)\)</span></p></li>
<li><p>The eigenvalues of A+B are not necessarily the sum of the
eigenvalues of A and B, because their eigenvectors may not be the same.
Similarly, the eigenvalues of AB are not necessarily the product of
their eigenvalues.</p></li>
<li><p>For another example, consider the rotation matrix Q</p>
<p><span class="math display">\[
Q=
\begin{bmatrix}
0 &amp; -1 \\
1 &amp; 0 \\
\end{bmatrix} \\
trace=0=\lambda _1 +\lambda _2 \\
det=1=\lambda _1 \lambda _2 \\
\]</span></p></li>
<li><p>However, it can be seen that <span class="math inline">\(\lambda
_1，\lambda _2\)</span> has no real solutions</p></li>
<li><p>Consider an even worse case (the matrix is more asymmetric, and
it is even harder to obtain real eigenvalues)</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
3 &amp; 1 \\
0 &amp; 3 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
3-\lambda &amp; 1 \\
0 &amp; 3-\lambda \\
\end{vmatrix}
==(3-\lambda )^2=0 \\
\lambda _1=\lambda _2=3 \\
x_1=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
\]</span></p></li>
</ul>
<h1 id="st-lecture-diagonalization-and-powers-of-a">21st Lecture:
Diagonalization and Powers of A</h1>
<h2 id="diagonalization">Diagonalization</h2>
<ul>
<li><p>Assuming A has n linearly independent eigenvectors, arranged as
columns to form the matrix S, i.e., the eigenvector matrix</p></li>
<li><p>All discussions about matrix diagonalization presented here are
under the premise that S is invertible, i.e., the n eigenvectors are
linearly independent</p></li>
<li><p><span class="math display">\[
AS=A[x_1,x_2...x_n]=[\lambda _1 x_1,....\lambda _n x_n] \\
=[x_1,x_2,...x_n]
\begin{bmatrix}
\lambda _1 &amp; 0 &amp; ... &amp; 0 \\
0 &amp; \lambda _2 &amp; ... &amp; 0 \\
... &amp; ... &amp; ... &amp; ... \\
0 &amp; 0  &amp; 0 &amp; \lambda _n \\
\end{bmatrix} \\
=S \Lambda \\
\]</span></p></li>
<li><p>Assuming S is invertible, i.e., the n eigenvectors are linearly
independent, we can obtain</p>
<p><span class="math display">\[
S^{-1}AS=\Lambda \\
A=S\Lambda S^{-1} \\
\]</span></p></li>
<li><p><span class="math inline">\(\Lambda\)</span> is a diagonal
matrix, here we obtain a matrix decomposition other than <span
class="math inline">\(A=LU\)</span> and <span
class="math inline">\(A=QR\)</span></p></li>
<li><p><span class="math display">\[
if \qquad Ax=\lambda x \\
A^2 x=\lambda AX=\lambda ^2 x \\
A^2=S\Lambda S^{-1} S \Lambda S^{-1}=S \Lambda ^2 S^{-1} \\
\]</span></p></li>
<li><p>The two equations above regarding <span
class="math inline">\(A^2\)</span> indicate that the squared eigen
vectors remain unchanged, the eigenvalues are squared, and similarly for
the K-th power</p></li>
<li><p>Eigenvalues and eigenvectors help us understand matrix powers.
When calculating matrix powers, we can decompose the matrix into the
form of a matrix of eigenvectors multiplied by a diagonal matrix, where
K multiplications can cancel each other out, as shown in the above
formula</p></li>
<li><p>What kind of matrix's power tends to 0 (stable)</p>
<p><span class="math display">\[
A^K \rightarrow 0 \quad as \quad K \rightarrow \infty \\
if \quad all |\lambda _i|&lt;1 \\
\]</span></p></li>
<li><p>Which matrices can be diagonalized? If all eigenvalues are
different, then A can be diagonalized</p></li>
<li><p>If matrix A is already diagonal, then <span
class="math inline">\(\Lambda\)</span> is the same as A</p></li>
<li><p>The number of times an eigenvalue repeats is called the algebraic
multiplicity, for triangular matrices, such as</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
2 &amp; 1 \\
0 &amp; 2 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
2-\lambda &amp; 1 \\
0 &amp; 2-\lambda \\
\end{vmatrix}=0 \\
\lambda =2 \\
A-\lambda I=
\begin{bmatrix}
0 &amp; 1 \\
0 &amp; 0 \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>For <span class="math inline">\(A-\lambda I\)</span> , the
geometric multiplicity is 1, while the algebraic multiplicity of the
eigenvalue is 2</p></li>
<li><p>The eigenvector is only (1,0), therefore, for a triangular
matrix, it cannot be diagonalized, and there do not exist two linearly
independent eigenvectors.</p></li>
</ul>
<h2 id="as-power">A's power</h2>
<ul>
<li><p>Most matrices have a set of linearly independent eigenvalues that
can be diagonalized. If diagonalization is possible, we need to focus on
how to solve for the powers of A.</p></li>
<li><p><span class="math display">\[
give \quad u_0 \\
u_{k+1}=Au_k \\
u_k=A^ku_0 \\
how \quad to \quad solve \quad u_k \\
u_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\
Au_0=c_1 \lambda _1 x_1 + c_2 \lambda _2 x_2 +...+c_n \lambda _n x_n \\
A^{100}u_0=c_1 \lambda _1^{100} x_1 + c_2 \lambda _2^{100} x_2 +...+c_n
\lambda _n^{100} x_n \\
=S\Lambda ^{100} C \\
=u_{100} \\
\]</span></p></li>
<li><p>Because the n feature vectors are mutually linearly independent,
they can serve as a set of bases to cover the entire n-dimensional
space, and naturally, <span class="math inline">\(u_0\)</span> can be
represented as a linear combination of the feature vectors, with C being
the linear coefficient vector. The above formula has derived the method
for solving matrix powers, and the next example will be given using the
Fibonacci sequence.</p>
<p><span class="math display">\[
F_0=0 \\
F_1=1 \\
F_2=1 \\
F_3=2 \\
F_4=3 \\
F_5=5 \\
..... \\
F_{100}=? \\
\]</span></p></li>
<li><p>The growth rate of the Fibonacci sequence is how fast? Determined
by the eigenvalues, we attempt to construct vectors to find the matrix
relationship of the iterative Fibonacci sequence</p>
<p><span class="math display">\[
F_{k+2}=F_{k+1}+F_k \\
F_{k+1}=F_{k+1} \\
\]</span></p></li>
<li><p>Define vector</p>
<p><span class="math display">\[
u_k=
\begin{bmatrix}
F_{k+1} \\
F_k \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>Using this vector, the first two equations can be written in
matrix form</p>
<p><span class="math display">\[
u_{k+1}=
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix}
u_k \\
A=
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix} \\
\lambda =\frac {1 \pm \sqrt 5}2 \\
\]</span></p></li>
<li><p>Obtaining two eigenvalues, it is easy to obtain the corresponding
eigenvectors</p></li>
<li><p>Returning to the Fibonacci sequence, the growth rate of the
Fibonacci sequence is determined by the eigenvalues of the "sequence
update matrix" we construct, and as can be seen from <span
class="math inline">\(A^{100}u_0=c_1 \lambda _1^100 x_1 + c_2 \lambda
_2^100 x_2 +...+c_n \lambda _n^100 x_n\)</span> , the growth rate is
mainly determined by the larger eigenvalues, therefore <span
class="math inline">\(F_{100}\)</span> can be written in the following
form</p>
<p><span class="math display">\[
F_{100} \approx c_1 {\frac {1 + \sqrt 5}2}^{100} \\
\]</span></p></li>
<li><p>There are initial values</p>
<p><span class="math display">\[
u_0=
\begin{bmatrix}
F_1 \\
F_0 \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
=c_1x_1+c_2x_2
\]</span></p></li>
<li><p>Among them, <span class="math inline">\(x_1,x_2\)</span> are two
feature vectors, whose linear coefficients can be calculated, and by
substituting them into the formula, an approximate value of <span
class="math inline">\(F_{100}\)</span> can be obtained</p></li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li>We find that under the condition of A being invertible, A can be
decomposed into the form <span class="math inline">\(S\Lambda
S^{-1}\)</span></li>
<li>This form has a characteristic that facilitates the calculation of
the power of A, as it can be observed that the unit matrix of the
eigenvalues of A's power is the power of the unit matrix of A's
eigenvalues</li>
<li>We attempt to apply this feature in solving the Fibonacci sequence,
first converting the sequence update into a matrix form</li>
<li>Determine the eigenvalues and eigenvectors of the matrix</li>
<li>From the expansion of the power series of A, it can be seen that the
power of A is mainly determined by the larger eigenvalues, therefore
<span class="math inline">\(F_{100}\)</span> can be written in the form
of <span class="math inline">\(F_{100} \approx c_1 {(\frac {1 + \sqrt
5}2)}^{100}\)</span></li>
<li>By the initial value <span class="math inline">\(F_0\)</span> ,
calculate the linear coefficients, substitute them into the above
formula, and obtain the approximate value of <span
class="math inline">\(F_{100}\)</span></li>
<li>This is an example of a difference equation; the next section will
discuss differential equations</li>
</ul>
<h1 id="lecture-22-differential-equations-and-expat">Lecture 22:
Differential Equations and exp(At)</h1>
<h2 id="differential-equation">Differential Equation</h2>
<ul>
<li><p>The solutions to linear equations with constant coefficients are
in exponential form; if the solution to the differential equation is in
exponential form, one can find the solution by using linear algebra to
determine the exponents and coefficients</p></li>
<li><p>For example</p>
<p><span class="math display">\[
\frac{du_1}{dt}=-u_1+2u_2 \\
\frac{du_2}{dt}=u_1-2u_2 \\
u(0)=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>First, we list the coefficient matrix and find the eigenvalues
and eigenvectors of the matrix</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
-1 &amp; 2 \\
1 &amp; -2 \\
\end{bmatrix}
\]</span></p></li>
<li><p><span class="math inline">\(\lambda=0\)</span> is a solution of
this singular matrix, and from the trace it can be seen that the second
eigenvalue is <span class="math inline">\(\lambda=-3\)</span> , and two
eigenvectors are obtained</p>
<p><span class="math display">\[
x_1=
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix} \\
x_2=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>The general solution form of the differential equation will
be</p>
<p><span class="math display">\[
u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2
\]</span></p></li>
<li><p>Why?</p>
<p><span class="math display">\[
\frac{du}{dt} \\
=c_1 \lambda _1 e^{\lambda _1 t}x_1 \\
=A c_1 e^{\lambda _1 t}x_1 \\
because \quad A x_1=\lambda _1 x_1 \\
\]</span></p></li>
<li><p>In the differential equation <span
class="math inline">\(u_{k+1}=Au_k\)</span> , the form of the solution
is <span class="math inline">\(c_1\lambda _1 ^k x_1+c_2 \lambda _2 ^k
x_2\)</span></p></li>
<li><p>In the differential equation <span class="math inline">\(\frac
{du}{dt}=Au\)</span> , the form of the solution is <span
class="math inline">\(u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2
t}x_2\)</span></p></li>
<li><p>Solved from the initial values, i.e., the coefficient matrix C
multiplied by the eigenvector matrix S yields the initial
values</p></li>
<li><p>It can be seen that as t approaches infinity, the solution of the
example equation is reduced to only the steady-state part, i.e., <span
class="math inline">\((\frac 23,\frac 13)\)</span></p></li>
<li><p>When does the solution tend towards 0? There exist negative
eigenvalues because <span class="math inline">\(e^{\lambda t}\)</span>
needs to tend towards 0</p></li>
<li><p>If the eigenvalues are complex? The magnitude of the imaginary
part is 1, so if the real part of the complex number is negative, the
solution still tends towards 0</p></li>
<li><p>When does a steady state exist? Only 0 and negative eigenvalues
exist, as in the example above</p></li>
<li><p>When does the solution fail to converge? Any eigenvalue has a
real part greater than 0</p></li>
<li><p>The sign of the coefficient matrix changes, the eigenvalues also
change sign, the steady-state solution remains steady-state, and the
convergent solution will become divergent</p></li>
<li><p>How to directly determine if the solution converges from a
matrix? That is, do all the real parts of the eigenvalues have a value
less than 0?</p></li>
<li><p>The trace of the matrix should be less than 0, but the sum of the
diagonal elements being 0 does not necessarily converge, as</p>
<p><span class="math display">\[
\begin{bmatrix}
-2 &amp; 0 \\
0 &amp; 1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Therefore, another condition is required: the value of the
determinant is the product of the eigenvalues, so the value of the
determinant should be greater than 0</p></li>
</ul>
<h2 id="expat">exp(At)</h2>
<ul>
<li>Can the solution be expressed in the form of <span
class="math inline">\(S,\Lambda\)</span></li>
<li>Matrix A represents <span class="math inline">\(u_1,u_2\)</span>
coupling, first we need to diagonalize u to decouple</li>
<li><span class="math display">\[
\frac{du}{dt} = Au \\
set \quad u=Sv \\
S \frac{dv}{dt} = ASv \\
\frac{dv}{dt}=S^{-1}ASv=\Lambda v \\
v(t)=e^{\Lambda t}v(0) \\
u(t)=Se^{\Lambda t}S^{-1}u(0) \\
\]</span></li>
</ul>
<h1 id="st-lecture-markov-matrix-fourier-series">21st Lecture: Markov
Matrix; Fourier Series</h1>
<h2 id="markov-matrix">Markov matrix</h2>
<ul>
<li><p>A typical Markov matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
0.1 &amp; 0.01 &amp; 0.3 \\
0.2 &amp; 0.99 &amp; 0.3 \\
0.7 &amp; 0 &amp; 0.4 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Each element is greater than or equal to 0, the sum of each
column is 1, and the powers of the Markov matrix are all Markov
matrices</p></li>
<li><p><span class="math inline">\(\lambda=1\)</span> is an eigenvalue,
and the absolute values of the other eigenvalues are all less than
1</p></li>
<li><p>In the previous lecture, we discussed that the power of a matrix
can be decomposed into</p>
<p><span class="math display">\[
u_k=A^ku_0=c_1\lambda _1 ^kx_1+c_2\lambda _2 ^kx_2+.....
\]</span></p></li>
<li><p>When A is a Markov matrix, there is only one eigenvalue of 1, and
the other eigenvalues are less than 1. As k increases, the terms with
eigenvalues less than 1 tend to approach 0, retaining only the term with
the eigenvalue of 1, and the elements of the corresponding eigenvector
are all greater than 0</p></li>
<li><p>When the sum of each column is 1, there necessarily exists an
eigenvalue <span class="math inline">\(\lambda =1\)</span></p></li>
<li><p>Proof:</p>
<p><span class="math display">\[
A-I=
\begin{bmatrix}
-0.9 &amp; 0.01 &amp; 0.3 \\
0.2 &amp; -0.01 &amp; 0.3 \\
0.7 &amp; 0 &amp; -0.6 \\
\end{bmatrix}
\]</span></p></li>
<li><p>If 1 is an eigenvalue, then <span
class="math inline">\(A-I\)</span> should be singular. It can be seen
that the sum of each column of <span class="math inline">\(A-I\)</span>
is 0, indicating that the row vectors are linearly dependent, i.e., the
matrix is singular, and the all-ones vector lies in the left null
space.</p></li>
<li><p>For the Markov matrix A, we study <span
class="math inline">\(u_{k+1}=Au_k\)</span></p></li>
<li><p>An example, u is the population in Massachusetts and California,
A is the population mobility matrix</p>
<p><span class="math display">\[
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k+1}
=
\begin{bmatrix}
0.9 &amp; 0.2 \\
0.1 &amp; 0.8 \\
\end{bmatrix}
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k}
\]</span></p></li>
<li><p>It can be seen that each year (k), 80% of people stay in
Massachusetts, 20% move to California, and 10% from California also
relocate to Massachusetts</p></li>
<li><p>On the Markov matrix A</p>
<p><span class="math display">\[
\begin{bmatrix}
0.9 &amp; 0.2 \\
0.1 &amp; 0.8 \\
\end{bmatrix} \\
\lambda _1 =1 \\
\lambda _2 =0.7 \\
\]</span></p></li>
<li><p>For the eigenvalue of 1, the eigenvector is easily found as <span
class="math inline">\((2,1)\)</span> , and for the eigenvalue of 0.7,
the eigenvector is (-1,1).</p></li>
<li><p>Obtain the formula we are to study</p>
<p><span class="math display">\[
u_k=c_1*1^k*
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix}
+c_2*(0.7)^k*
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>Assuming there were initially 0 people in California and 1000
people in Massachusetts, i.e., <span class="math inline">\(u_0\)</span>
, substituting this into the formula yields <span
class="math inline">\(c_1,c_2\)</span> . It can be seen that after many
years, the populations of California and Massachusetts will stabilize,
each accounting for one-third and two-thirds of the total 1000 people,
respectively.</p></li>
<li><p>The vector with a sum of 1 is another way to define a Markov
matrix</p></li>
</ul>
<h2 id="fourier-series">Fourier Series</h2>
<ul>
<li><p>Discussion of projection problems with standard orthogonal
bases</p></li>
<li><p>If <span class="math inline">\(q_1....q_n\)</span> is a set of
standard orthogonal bases, any vector <span
class="math inline">\(v\)</span> is a linear combination of this set of
bases</p></li>
<li><p>We now need to determine the linear combination coefficients
<span class="math inline">\(x_1....x_n\)</span> , <span
class="math inline">\(v=x_1q_1+x_2q_2+...x_nq_n\)</span> . One method is
to take the inner product of <span class="math inline">\(v\)</span> and
<span class="math inline">\(q_i\)</span> , and calculate the
coefficients one by one</p>
<p><span class="math display">\[
q_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\
\]</span></p></li>
<li><p>Written in matrix form</p>
<p><span class="math display">\[
\begin{bmatrix}
q_1 &amp; q_2 &amp; ... &amp; q_n \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
... \\
x_n \\
\end{bmatrix}=
v \\
Qx=v \\
x=Q^{-1}v=Q^Tv \\
\]</span></p></li>
<li><p>Now discussing Fourier series</p></li>
<li><p>We hope to decompose the function</p>
<p><span class="math display">\[
f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......
\]</span></p></li>
<li><p>The key is that in this decomposition, <span
class="math inline">\(coskx,sinkx\)</span> constitutes an infinite
orthogonal basis for a set of function spaces, i.e., the inner products
of these functions are 0 (the inner product of vectors is a discrete
value summation, while the inner product of functions is a continuous
value integration).</p></li>
<li><p>How to calculate the Fourier coefficients?</p></li>
<li><p>Using the previous vector example to calculate</p></li>
<li><p>Sequentially compute the inner product of <span
class="math inline">\(f(x)\)</span> with each element of the orthogonal
basis, obtaining the corresponding coefficient multiplied by <span
class="math inline">\(\pi\)</span> , for example</p>
<p><span class="math display">\[
\int _0 ^{2\pi} f(x)cosx dx=0+ a_1 \int _0^{2\pi}(cosx)^2dx+0+0...+0=\pi
a_1 \\
\]</span></p></li>
</ul>
<h1 id="lecture-22-symmetric-matrices-and-positive-definiteness">Lecture
22: Symmetric Matrices and Positive Definiteness</h1>
<h2 id="symmetric-matrix">Symmetric matrix</h2>
<ul>
<li><p>The eigenvalues of a symmetric matrix are real numbers, and the
eigenvectors corresponding to distinct eigenvalues are mutually
orthogonal</p></li>
<li><p>For a general matrix <span class="math inline">\(A=S\Lambda
S^{-1}\)</span> , S is the matrix of eigenvectors</p></li>
<li><p>For the symmetric matrix <span class="math inline">\(A=Q\Lambda
Q^{-1}=Q\Lambda Q^T\)</span> , Q is the matrix of standard orthogonal
eigenvectors</p></li>
<li><p>Why are all eigenvalues real numbers?</p></li>
<li><p>Conjugate both left and right, as we are now only considering the
real matrix A, <span class="math inline">\(Ax^{*}=\lambda ^{*}
x^{*}\)</span></p></li>
<li><p><span class="math inline">\(\lambda\)</span> and its conjugate
are eigenvalues; now take the transpose of both sides of the equation,
<span class="math inline">\(x^{* T}A^T=x^{* T} \lambda ^{*
T}\)</span></p></li>
<li><p>In the above formula, <span class="math inline">\(A=A^T\)</span>
, and both sides are multiplied by <span
class="math inline">\(x\)</span> , comparing with <span
class="math inline">\(x^{* T}A\lambda x^{* T}x\)</span> yields <span
class="math inline">\(\lambda ^{*}=\lambda\)</span> , i.e., the
eigenvalues are real numbers</p></li>
<li><p>It is evident that for multiple matrices, the condition <span
class="math inline">\(A=A^{* T}\)</span> is required to satisfy
symmetry</p></li>
<li><p>For symmetric matrices</p>
<p><span class="math display">\[
A=Q\Lambda Q^{-1}=Q\Lambda Q^T \\
=\lambda _1 q_1 q_1^T+\lambda _2 q_2 q_2^T+.... \\
\]</span></p></li>
<li><p>So every symmetric matrix is a combination of some mutually
orthogonal projection matrices</p></li>
<li><p>For symmetric matrices, the number of positive principal minors
is equal to the number of positive eigenvalues, and the product of the
principal minors equals the product of the eigenvalues, which equals the
determinant of the matrix</p></li>
</ul>
<h2 id="positivity">Positivity</h2>
<ul>
<li>Positive definite matrices are symmetric matrices, a subclass of
symmetric matrices, whose all eigenvalues are positive, all leading
principal minors are positive, and all subdeterminants are positive</li>
<li>The sign of eigenvalues is related to stability</li>
<li>The eigenvalue, determinant, and main element are unified as one in
linear algebra</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li><p>行列式按行是线性的，但行列式本身不是线性的</p>
<p><span class="math display">\[
\begin{vmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{vmatrix}=1 \\
\begin{vmatrix}
0 &amp; 1 \\
1 &amp; 0 \\
\end{vmatrix}=-1 \\
\begin{vmatrix}
ta &amp; tb \\
c &amp; d \\
\end{vmatrix}=
t\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix} \\
\begin{vmatrix}
t+a &amp; t+b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
t &amp; t \\
c &amp; d \\
\end{vmatrix}
\]</span></p></li>
<li><p>证明消元不改变行列式</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; b \\
c-la &amp; d-lb \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}-l
\begin{vmatrix}
a &amp; b \\
a &amp; b \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}
\]</span></p></li>
<li><p>证明转置不改变行列式</p>
<p><span class="math display">\[
A=LU \\
\]</span></p></li>
<li><p>即证 <span class="math inline">\(|U^TL^T|=|LU|\)</span> <span
class="math display">\[
|U^T||L^T|=|L||U|
\]</span></p></li>
<li><p>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等</p></li>
</ul>
<h2 id="三角阵行列式">三角阵行列式</h2>
<ul>
<li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li>
<li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li>
<li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到<span
class="math inline">\(d_1d_2d_3...d_nI\)</span>，其中单位矩阵的行列式为1</li>
<li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li>
</ul>
<h2 id="a-little-more">A little more</h2>
<ul>
<li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li>
</ul>
<h1
id="第十八讲行列式公式和代数余子式">第十八讲：行列式公式和代数余子式</h1>
<h2 id="行列式公式">行列式公式</h2>
<ul>
<li><p>推导2*2行列式</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; 0 \\
c &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
c &amp; d \\
\end{vmatrix}=
\begin{vmatrix}
a &amp; 0 \\
c &amp; 0 \\
\end{vmatrix}+
\begin{vmatrix}
a &amp; 0 \\
0 &amp; d \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
c &amp; 0 \\
\end{vmatrix}+
\begin{vmatrix}
0 &amp; b \\
0 &amp; d \\
\end{vmatrix} \\
=0+ad-bc+0
\]</span></p>
<p>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</p></li>
<li><p>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</p></li>
<li><p>例如</p>
<p><span class="math display">\[
\begin{vmatrix}
a &amp; 0 &amp; 0\\
0 &amp; 0 &amp; b\\
0 &amp; c &amp; 0\\
\end{vmatrix}
\]</span></p>
<p>先提取出因子，得到<span
class="math inline">\(abc\)</span>，交换第二行第三行得到单位矩阵，于是答案就是<span
class="math inline">\(abc*detI=abc\)</span>，又因为进行了一次行交换，所以答案是负的，<span
class="math inline">\(-abc\)</span></p></li>
<li><p>n*n的矩阵可以分成<span
class="math inline">\(n!\)</span>个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到<span
class="math inline">\(n!\)</span>部分</p></li>
<li><p>行列式公式就是这<span
class="math inline">\(n!\)</span>个部分加起来</p></li>
</ul>
<h2 id="代数余子式">代数余子式</h2>
<ul>
<li><span
class="math inline">\(det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)\)</span></li>
<li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li>
<li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li>
<li><span class="math inline">\(a_{ij}\)</span>的代数余子式记作<span
class="math inline">\(c_{ij}\)</span></li>
<li>注意代数余子式的正负，与<span
class="math inline">\(i+j\)</span>的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li>
<li><span
class="math inline">\(detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}\)</span></li>
</ul>
<h1
id="第十九讲克拉默法则逆矩阵体积">第十九讲：克拉默法则，逆矩阵，体积</h1>
<h2 id="逆矩阵">逆矩阵</h2>
<ul>
<li><p>只有行列式不为0时，矩阵才是可逆的</p></li>
<li><p>逆矩阵公式</p>
<p><span class="math display">\[
A^{-1}=\frac{1}{detA}C^T
\]</span></p>
<p>其中<span class="math inline">\(C_{ij}\)</span>是<span
class="math inline">\(A_{ij}\)</span>的代数余子式</p></li>
<li><p>证明：即证<span class="math inline">\(AC^T=(detA)I\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} &amp; ... &amp; a_{1n} \\
a_{n1} &amp; ... &amp; a_{nn} \\
\end{bmatrix}
\begin{bmatrix}
c_{11} &amp; ... &amp; c_{n1} \\
c_{1n} &amp; ... &amp; c_{nn} \\
\end{bmatrix}=
\begin{bmatrix}
detA &amp; 0 &amp; 0 \\
0 &amp; detA &amp; 0 \\
0 &amp; 0 &amp; detA \\
\end{bmatrix}
\]</span></p>
<p>对角线上都是行列式，因为<span
class="math inline">\(det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)\)</span>
其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</p></li>
</ul>
<h2 id="克拉默法则">克拉默法则</h2>
<ul>
<li><p>解Ax=b</p>
<p><span class="math display">\[
Ax=b \\
x=A^{-1}b \\
x=\frac{1}{detA}C^Tb \\
\\
x_1=\frac{detB_1}{detA} \\
x_3=\frac{detB_2}{detA} \\
... \\
\]</span></p></li>
<li><p>克拉默法则即发现矩阵<span
class="math inline">\(B_i\)</span>就是矩阵<span
class="math inline">\(A\)</span>的第i列换成b，其余不变</p></li>
</ul>
<h2 id="体积">体积</h2>
<ul>
<li><p>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</p></li>
<li><p>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</p></li>
<li><p>(1)单位矩阵对应单位立方体，体积为1</p></li>
<li><p>对正交矩阵Q,</p>
<p><span class="math display">\[
QQ^T=I \\
|QQ^T|=|I| \\
|Q||Q^T|=1 \\
{|Q|}^2=1 \\
|Q|=1 \\
\]</span></p>
<p>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</p></li>
<li><p>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</p></li>
<li><p>(2)交换矩阵两行，盒子的体积不变</p></li>
<li><p>(3b)矩阵某一行拆分，盒子也相应切分为两部分</p></li>
<li><p>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</p></li>
</ul>
<h1 id="第二十讲特征值和特征向量">第二十讲：特征值和特征向量</h1>
<h2 id="特征向量">特征向量</h2>
<ul>
<li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li>
<li>当Ax平行于x时，即<span class="math inline">\(Ax=\lambda
x\)</span>，我们称<span
class="math inline">\(x\)</span>为特征向量，<span
class="math inline">\(\lambda\)</span>为特征值</li>
<li>如果A是奇异矩阵，<span class="math inline">\(\lambda =
0\)</span>是一个特征值</li>
</ul>
<h2 id="几个例子">几个例子</h2>
<ul>
<li><p>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为<span
class="math inline">\(Ax\)</span>即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，<span
class="math inline">\(Ax=0\)</span>，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</p></li>
<li><p>再举一例</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
0 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix} \\
\lambda =1, x=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix} \\
\lambda =-1, x=
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
Ax=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix} \\    
\]</span></p></li>
<li><p>n*n矩阵有n个特征值</p></li>
<li><p>特征值的和等于对角线元素和，这个和称为迹(trace)，</p></li>
<li><p>如何求解<span class="math inline">\(Ax=\lambda x\)</span></p>
<p><span class="math display">\[
(A-\lambda I)x=0 \\
\]</span></p></li>
<li><p>可见方程有非零解，<span class="math inline">\((A-\lambda
I)\)</span>必须是奇异的 即:</p>
<p><span class="math display">\[
det(A-\lambda I)=0 \\
\]</span></p></li>
<li><p><span class="math display">\[
If \qquad Ax=\lambda x \\
Then \qquad (A+3I)x=(\lambda +3)x \\
\]</span></p></li>
<li><p>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即<span
class="math inline">\((\lambda +3)\)</span></p></li>
<li><p>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</p></li>
<li><p>再举一例，对旋转矩阵Q</p>
<p><span class="math display">\[
Q=
\begin{bmatrix}
0 &amp; -1 \\
1 &amp; 0 \\
\end{bmatrix} \\
trace=0=\lambda _1 +\lambda _2 \\
det=1=\lambda _1 \lambda _2 \\
\]</span></p></li>
<li><p>但是可以看出 <span class="math inline">\(\lambda _1，\lambda
_2\)</span>无实数解</p></li>
<li><p>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
3 &amp; 1 \\
0 &amp; 3 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
3-\lambda &amp; 1 \\
0 &amp; 3-\lambda \\
\end{vmatrix}
==(3-\lambda )^2=0 \\
\lambda _1=\lambda _2=3 \\
x_1=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
\]</span></p></li>
</ul>
<h1 id="第二十一讲对角化和a的幂">第二十一讲：对角化和A的幂</h1>
<h2 id="对角化">对角化</h2>
<ul>
<li><p>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</p></li>
<li><p>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</p></li>
<li><p><span class="math display">\[
AS=A[x_1,x_2...x_n]=[\lambda _1 x_1,....\lambda _n x_n] \\
=[x_1,x_2,...x_n]
\begin{bmatrix}
\lambda _1 &amp; 0 &amp; ... &amp; 0 \\
0 &amp; \lambda _2 &amp; ... &amp; 0 \\
... &amp; ... &amp; ... &amp; ... \\
0 &amp; 0  &amp; 0 &amp; \lambda _n \\
\end{bmatrix} \\
=S \Lambda \\
\]</span></p></li>
<li><p>假设S可逆，即n个特征向量无关，此时可以得到</p>
<p><span class="math display">\[
S^{-1}AS=\Lambda \\
A=S\Lambda S^{-1} \\
\]</span></p></li>
<li><p><span
class="math inline">\(\Lambda\)</span>是对角矩阵，这里我们得到了除了<span
class="math inline">\(A=LU\)</span>和<span
class="math inline">\(A=QR\)</span>之外的一种矩阵分解</p></li>
<li><p><span class="math display">\[
if \qquad Ax=\lambda x \\
A^2 x=\lambda AX=\lambda ^2 x \\
A^2=S\Lambda S^{-1} S \Lambda S^{-1}=S \Lambda ^2 S^{-1} \\
\]</span></p></li>
<li><p>上面关于<span
class="math inline">\(A^2\)</span>的两式说明平方后特征向量不变，特征值平方，K次方同理</p></li>
<li><p>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</p></li>
<li><p>什么样的矩阵的幂趋向于0(稳定)</p>
<p><span class="math display">\[
A^K \rightarrow 0 \quad as \quad K \rightarrow \infty \\
if \quad all |\lambda _i|&lt;1 \\
\]</span></p></li>
<li><p>哪些矩阵可以对角化？ 如果所有特征值不同，则A可以对角化</p></li>
<li><p>如果矩阵A已经是对角阵，则<span
class="math inline">\(\Lambda\)</span>与A相同</p></li>
<li><p>特征值重复的次数称为代数重度，对三角阵，如</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
2 &amp; 1 \\
0 &amp; 2 \\
\end{bmatrix} \\
det(A-\lambda I)=
\begin{vmatrix}
2-\lambda &amp; 1 \\
0 &amp; 2-\lambda \\
\end{vmatrix}=0 \\
\lambda =2 \\
A-\lambda I=
\begin{bmatrix}
0 &amp; 1 \\
0 &amp; 0 \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>对<span class="math inline">\(A-\lambda
I\)</span>，几何重数是1，而特征值的代数重度是2</p></li>
<li><p>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</p></li>
</ul>
<h2 id="a的幂">A的幂</h2>
<ul>
<li><p>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</p></li>
<li><p><span class="math display">\[
give \quad u_0 \\
u_{k+1}=Au_k \\
u_k=A^ku_0 \\
how \quad to \quad solve \quad u_k \\
u_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\
Au_0=c_1 \lambda _1 x_1 + c_2 \lambda _2 x_2 +...+c_n \lambda _n x_n \\
A^{100}u_0=c_1 \lambda _1^{100} x_1 + c_2 \lambda _2^{100} x_2 +...+c_n
\lambda _n^{100} x_n \\
=S\Lambda ^{100} C \\
=u_{100} \\
\]</span></p></li>
<li><p>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然<span
class="math inline">\(u_0\)</span>可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例</p>
<p><span class="math display">\[
F_0=0 \\
F_1=1 \\
F_2=1 \\
F_3=2 \\
F_4=3 \\
F_5=5 \\
..... \\
F_{100}=? \\
\]</span></p></li>
<li><p>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系</p>
<p><span class="math display">\[
F_{k+2}=F_{k+1}+F_k \\
F_{k+1}=F_{k+1} \\
\]</span></p></li>
<li><p>定义向量</p>
<p><span class="math display">\[
u_k=
\begin{bmatrix}
F_{k+1} \\
F_k \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>利用这个向量可以将前两个等式写成矩阵形式</p>
<p><span class="math display">\[
u_{k+1}=
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix}
u_k \\
A=
\begin{bmatrix}
1 &amp; 1 \\
1 &amp; 0 \\
\end{bmatrix} \\
\lambda =\frac {1 \pm \sqrt 5}2 \\
\]</span></p></li>
<li><p>得到两个特征值，我们很容易得到特征向量</p></li>
<li><p>回到斐波那契数列，斐波那契数列的增长速率由我们构造的"数列更新矩阵"的特征值决定，而且由<span
class="math inline">\(A^{100}u_0=c_1 \lambda _1^100 x_1 + c_2 \lambda
_2^100 x_2 +...+c_n \lambda _n^100
x_n\)</span>可以看出增长率主要由由较大的特征值决定，因此<span
class="math inline">\(F_{100}\)</span>可以写成如下形式</p>
<p><span class="math display">\[
F_{100} \approx c_1 {\frac {1 + \sqrt 5}2}^{100} \\
\]</span></p></li>
<li><p>再有初始值有</p>
<p><span class="math display">\[
u_0=
\begin{bmatrix}
F_1 \\
F_0 \\
\end{bmatrix}=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}
=c_1x_1+c_2x_2
\]</span></p></li>
<li><p>其中<span
class="math inline">\(x_1,x_2\)</span>是两个特征向量，线性系数可求，代入公式可求<span
class="math inline">\(F_{100}\)</span>的近似值</p></li>
</ul>
<h2 id="总结">总结</h2>
<ul>
<li>我们发现在A可逆的情况下，A可以分解成<span
class="math inline">\(S\Lambda S^{-1}\)</span>的形式</li>
<li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li>
<li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li>
<li>求出矩阵的特征值，特征向量</li>
<li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此<span
class="math inline">\(F_{100}\)</span>可以写成<span
class="math inline">\(F_{100} \approx c_1 {(\frac {1 + \sqrt
5}2)}^{100}\)</span>的形式</li>
<li>由初始值<span
class="math inline">\(F_0\)</span>求出线性系数，代入上式，得到<span
class="math inline">\(F_{100}\)</span>的近似值</li>
<li>以上是差分方程的一个例子，下一节将讨论微分方程</li>
</ul>
<h1 id="第二十二讲微分方程和expat">第二十二讲：微分方程和exp(At)</h1>
<h2 id="微分方程">微分方程</h2>
<ul>
<li><p>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</p></li>
<li><p>举个例子</p>
<p><span class="math display">\[
\frac{du_1}{dt}=-u_1+2u_2 \\
\frac{du_2}{dt}=u_1-2u_2 \\
u(0)=
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix} \\
\]</span></p></li>
<li><p>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量</p>
<p><span class="math display">\[
A=
\begin{bmatrix}
-1 &amp; 2 \\
1 &amp; -2 \\
\end{bmatrix}
\]</span></p></li>
<li><p>易得<span
class="math inline">\(\lambda=0\)</span>是这个奇异矩阵的一个解，由迹可以看出第二个特征值是<span
class="math inline">\(\lambda=-3\)</span>，并得到两个特征向量</p>
<p><span class="math display">\[
x_1=
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix} \\
x_2=
\begin{bmatrix}
1 \\
-1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>微分方程解的通解形式将是</p>
<p><span class="math display">\[
u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2 t}x_2
\]</span></p></li>
<li><p>为什么？</p>
<p><span class="math display">\[
\frac{du}{dt} \\
=c_1 \lambda _1 e^{\lambda _1 t}x_1 \\
=A c_1 e^{\lambda _1 t}x_1 \\
because \quad A x_1=\lambda _1 x_1 \\
\]</span></p></li>
<li><p>在差分方程<span
class="math inline">\(u_{k+1}=Au_k\)</span>当中，解的形式是<span
class="math inline">\(c_1\lambda _1 ^k x_1+c_2 \lambda _2 ^k
x_2\)</span></p></li>
<li><p>在微分方程<span class="math inline">\(\frac
{du}{dt}=Au\)</span>当中，解的形式是<span
class="math inline">\(u(t)=c_1e^{\lambda _1 t}x_1+c_1e^{\lambda _2
t}x_2\)</span></p></li>
<li><p><span
class="math inline">\(c_1,c_2\)</span>由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</p></li>
<li><p>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即<span
class="math inline">\((\frac 23,\frac 13)\)</span></p></li>
<li><p>什么时候解趋向于0？存在负数特征值，因为<span
class="math inline">\(e^{\lambda t}\)</span>需要趋向于0</p></li>
<li><p>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</p></li>
<li><p>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</p></li>
<li><p>什么时候解无法收敛？任何特征值的实数部分大于0</p></li>
<li><p>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</p></li>
<li><p>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</p></li>
<li><p>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如</p>
<p><span class="math display">\[
\begin{bmatrix}
-2 &amp; 0 \\
0 &amp; 1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</p></li>
</ul>
<h2 id="expat">exp(At)</h2>
<ul>
<li>是否可以把解表示成<span
class="math inline">\(S,\Lambda\)</span>的形式</li>
<li>矩阵A表示<span
class="math inline">\(u_1,u_2\)</span>耦合，首先我们需要将u对角化，解耦</li>
<li><span class="math display">\[
\frac{du}{dt} = Au \\
set \quad u=Sv \\
S \frac{dv}{dt} = ASv \\
\frac{dv}{dt}=S^{-1}ASv=\Lambda v \\
v(t)=e^{\Lambda t}v(0) \\
u(t)=Se^{\Lambda t}S^{-1}u(0) \\
\]</span></li>
</ul>
<h1
id="第二十一讲马尔科夫矩阵傅立叶级数">第二十一讲：马尔科夫矩阵;傅立叶级数</h1>
<h2 id="马尔科夫矩阵">马尔科夫矩阵</h2>
<ul>
<li><p>一个典型的马尔科夫矩阵</p>
<p><span class="math display">\[
\begin{bmatrix}
0.1 &amp; 0.01 &amp; 0.3 \\
0.2 &amp; 0.99 &amp; 0.3 \\
0.7 &amp; 0 &amp; 0.4 \\
\end{bmatrix}
\]</span></p></li>
<li><p>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</p></li>
<li><p><span
class="math inline">\(\lambda=1\)</span>是一个特征值，其余的特征值的绝对值都小于1</p></li>
<li><p>在上一讲中我们谈到矩阵的幂可以分解为</p>
<p><span class="math display">\[
u_k=A^ku_0=c_1\lambda _1 ^kx_1+c_2\lambda _2 ^kx_2+.....
\]</span></p></li>
<li><p>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</p></li>
<li><p>当每一列和为1时，必然存在一个特征值<span
class="math inline">\(\lambda =1\)</span></p></li>
<li><p>证明：</p>
<p><span class="math display">\[
A-I=
\begin{bmatrix}
-0.9 &amp; 0.01 &amp; 0.3 \\
0.2 &amp; -0.01 &amp; 0.3 \\
0.7 &amp; 0 &amp; -0.6 \\
\end{bmatrix}
\]</span></p></li>
<li><p>若1是一个特征值，则<span
class="math inline">\(A-I\)</span>应该是奇异的，可以看到<span
class="math inline">\(A-I\)</span>每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</p></li>
<li><p>对于马尔科夫矩阵A，我们研究<span
class="math inline">\(u_{k+1}=Au_k\)</span></p></li>
<li><p>一个例子，u是麻省和加州的人数，A是人口流动矩阵</p>
<p><span class="math display">\[
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k+1}
=
\begin{bmatrix}
0.9 &amp; 0.2 \\
0.1 &amp; 0.8 \\
\end{bmatrix}
\begin{bmatrix}
u_{cal} \\
u_{mass} \\
\end{bmatrix}_{t=k}
\]</span></p></li>
<li><p>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</p></li>
<li><p>对马尔科夫矩阵A</p>
<p><span class="math display">\[
\begin{bmatrix}
0.9 &amp; 0.2 \\
0.1 &amp; 0.8 \\
\end{bmatrix} \\
\lambda _1 =1 \\
\lambda _2 =0.7 \\
\]</span></p></li>
<li><p>对特征值为1的项，容易求出特征向量为<span
class="math inline">\((2,1)\)</span>，对特征值为0.7的项，特征向量为(-1,1)</p></li>
<li><p>得到我们要研究的公式</p>
<p><span class="math display">\[
u_k=c_1*1^k*
\begin{bmatrix}
2 \\
1 \\
\end{bmatrix}
+c_2*(0.7)^k*
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}
\]</span></p></li>
<li><p>假设一开始加州有0人，麻省有1000人，即<span
class="math inline">\(u_0\)</span>，代入公式可以得到<span
class="math inline">\(c_1,c_2\)</span>，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</p></li>
<li><p>行向量为和为1是另外一种定义马尔科夫矩阵的方式</p></li>
</ul>
<h2 id="傅里叶级数">傅里叶级数</h2>
<ul>
<li><p>先讨论带有标准正交基的投影问题</p></li>
<li><p>假设<span
class="math inline">\(q_1....q_n\)</span>是一组标准正交基，任何向量<span
class="math inline">\(v\)</span>都是这组基的线性组合</p></li>
<li><p>现在我们要求出线性组合系数<span
class="math inline">\(x_1....x_n\)</span> <span
class="math inline">\(v=x_1q_1+x_2q_2+...x_nq_n\)</span>
一种方法是将<span class="math inline">\(v\)</span>与<span
class="math inline">\(q_i\)</span>做内积，逐一求出系数</p>
<p><span class="math display">\[
q_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\
\]</span></p></li>
<li><p>写成矩阵形式</p>
<p><span class="math display">\[
\begin{bmatrix}
q_1 &amp; q_2 &amp; ... &amp; q_n \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
... \\
x_n \\
\end{bmatrix}=
v \\
Qx=v \\
x=Q^{-1}v=Q^Tv \\
\]</span></p></li>
<li><p>现在讨论傅里叶级数</p></li>
<li><p>我们希望将函数分解</p>
<p><span class="math display">\[
f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......
\]</span></p></li>
<li><p>关键是，在这种分解中，<span
class="math inline">\(coskx,sinkx\)</span>构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</p></li>
<li><p>如何求出傅里叶系数？</p></li>
<li><p>利用之前的向量例子来求</p></li>
<li><p>将<span
class="math inline">\(f(x)\)</span>逐一与正交基元素内积，得到这个正交基元素对应的系数乘<span
class="math inline">\(\pi\)</span>，例如</p>
<p><span class="math display">\[
\int _0 ^{2\pi} f(x)cosx dx=0+ a_1 \int _0^{2\pi}(cosx)^2dx+0+0...+0=\pi
a_1 \\
\]</span></p></li>
</ul>
<h1
id="第二十二讲对称矩阵及其正定性">第二十二讲：对称矩阵及其正定性</h1>
<h2 id="对称矩阵">对称矩阵</h2>
<ul>
<li><p>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</p></li>
<li><p>对一般矩阵<span class="math inline">\(A=S\Lambda
S^{-1}\)</span>，S为特征向量矩阵</p></li>
<li><p>对对称矩阵<span class="math inline">\(A=Q\Lambda Q^{-1}=Q\Lambda
Q^T\)</span>，Q为标准正交的特征向量矩阵</p></li>
<li><p>为什么特征值都是实数？</p></li>
<li><p><span class="math inline">\(Ax=\lambda
x\)</span>对左右同时取共轭，因为我们现在只考虑实数矩阵A，<span
class="math inline">\(Ax^{*}=\lambda ^{*} x^{*}\)</span></p></li>
<li><p>即<span
class="math inline">\(\lambda\)</span>和它的共轭都是特征值，现在再对等式两边取转置，$x<sup>{*
T}A</sup>T=x^{* T} ^{* T} $</p></li>
<li><p>上式中<span
class="math inline">\(A=A^T\)</span>，且两边同乘以<span
class="math inline">\(x\)</span>，与<span class="math inline">\(x^{*
T}A\lambda x^{* T}x\)</span>对比可得<span class="math inline">\(\lambda
^{*}=\lambda\)</span>，即特征值是实数</p></li>
<li><p>可见，对于复数矩阵，需要<span class="math inline">\(A=A^{*
T}\)</span>才满足对称</p></li>
<li><p>对于对称矩阵</p>
<p><span class="math display">\[
A=Q\Lambda Q^{-1}=Q\Lambda Q^T \\
=\lambda _1 q_1 q_1^T+\lambda _2 q_2 q_2^T+.... \\
\]</span></p></li>
<li><p>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</p></li>
<li><p>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</p></li>
</ul>
<h2 id="正定性">正定性</h2>
<ul>
<li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li>
<li>特征值的符号与稳定性有关</li>
<li>主元、行列式、特征值三位一体，线性代数将其统一</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>linearalgebra</tag>
      </tags>
  </entry>
  <entry>
    <title>Incremental Decoding</title>
    <url>/2020/03/17/incremental-decoding/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/6fa1d7f38e6bc1445a5434a93f3a92ad.png" width="500"/></p>
<p>Record the incremental decoding processing of parallel decoding
models such as CNN seq2seq and Transformer in the inference phase in
Fairseq.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="fairseq-architecture">Fairseq Architecture</h1>
<ul>
<li>In Facebook's seq2seq library Fairseq, all models inherit the
FairseqEncoderDecoder class, all Encoders inherit the FairseqEncoder
class, all Decoders inherit the FairseqIncrementalDecoder class, and
FairseqIncrementalDecoder inherits from the FairseqDecoder class.</li>
<li>The FairseqEncoder class only defines forward, reorder_encoder_out,
max_positions, and upgrade_state_dict, with forward being the most
important method, defining the forward propagation process of encoding.
Reorder is actually more important in the decoder, but it is defined
here as well.</li>
<li>The FairseqDecoder class defines forward, extract_features,
output_layer, get_normalized_probs, max_positions, upgrade_state_dict,
and prepare_for_onnx_export_. forward = extract_features + output_layer,
which means forward defines the entire forward process of decoding a
sequence, while extract_features only defines obtaining the decoder's
state sequence.</li>
<li>The Incremental Decoder additionally defines
reorder_incremental_state and set_beam_size. Reorder is closely related
to incremental decoding and beam search, which will be detailed
later.</li>
</ul>
<h1 id="training-parallelization-and-inference-incrementation">Training
Parallelization and Inference Incrementation</h1>
<ul>
<li>Models like CNN seq2seq and Transformer break the sequentiality of
RNN models, enabling the encoder and decoder in the seq2seq architecture
to be trained in parallel during training.</li>
<li>Parallel training of the encoder is quite obvious, and the decoder
is essentially a language model that can be parallelized during training
because of teacher forcing, where the input at each time step is assumed
to be known. Thus, the entire decoder input of (Batch, Length, Hidden)
can be directly input into the model for training.</li>
<li>However, during testing (inference), the input at each time step is
determined by the output of the previous time step, which cannot be
parallelized. If the entire decoder is run repeatedly, it would run
Length times, and only the information of the first i positions is
useful in the i-th run, with the remaining calculations completely
wasted, significantly reducing inference efficiency.</li>
<li>At this point, incremental decoding becomes necessary. During the
inference phase, whether it's CNN or Transformer, decoding is done step
by step like an RNN, using information previously inferred at each step,
rather than starting from scratch.</li>
</ul>
<h1 id="cnn">CNN</h1>
<ul>
<li><p>For CNN, it can be observed that at each layer of the decoder,
the i-th position only needs information from the [i-k, i) positions,
where k is the window size of the one-dimensional convolution.
Therefore, by maintaining a queue of length k to save the states
calculated by each layer, the model can reuse information previously
inferred.</p></li>
<li><p>Each calculation only needs to decode the i-th position, i.e.,
operate on (Batch, 1, Hidden) data Length times.</p></li>
<li><p>In the code, FConvDecoder passes input x and incremental_state to
LinearizedConvolution, which is described as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;&quot;&quot;An optimized version of nn.Conv1d.</span><br><span class="line">At training time, this module uses ConvTBC, which is an optimized version</span><br><span class="line">of Conv1d. At inference time, it optimizes incremental generation (i.e.,</span><br><span class="line">one time step at a time) by replacing the convolutions with linear layers.</span><br><span class="line">Note that the input order changes from training to inference.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>During training, data is organized in a Time-First format for
convolution to fully utilize GPU parallel performance. During inference,
convolution layers are replaced with equivalent linear layers for
frame-by-frame inference</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if incremental_state is None:</span><br><span class="line">   output = super().forward(input) # Here, LinearizedConvolution&#x27;s parent class is ConvTBC, so if there&#x27;s no inference, the entire sequence is sent to ConvTBC</span><br><span class="line">   if self.kernel_size[0] &gt; 1 and self.padding[0] &gt; 0:</span><br><span class="line">       # remove future timesteps added by padding</span><br><span class="line">       output = output[:-self.padding[0], :, :]</span><br><span class="line">return output</span><br></pre></td></tr></table></figure></li>
<li><p>Otherwise, inference is done layer by layer using linear layers,
and the input buffer is updated to update incremental_state</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># reshape weight</span><br><span class="line">weight = self._get_linearized_weight()</span><br><span class="line">kw = self.kernel_size[0]</span><br><span class="line">bsz = input.size(0)  # input: bsz x len x dim</span><br><span class="line">if kw &gt; 1:</span><br><span class="line">   input = input.data</span><br><span class="line">   input_buffer = self._get_input_buffer(incremental_state)</span><br><span class="line">   if input_buffer is None:</span><br><span class="line">       input_buffer = input.new(bsz, kw, input.size(2)).zero_()</span><br><span class="line">       self._set_input_buffer(incremental_state, input_buffer)</span><br><span class="line">   else:</span><br><span class="line">       # shift buffer</span><br><span class="line">       input_buffer[:, :-1, :] = input_buffer[:, 1:, :].clone()</span><br><span class="line">   # append next input</span><br><span class="line">   input_buffer[:, -1, :] = input[:, -1, :]</span><br><span class="line">   input = input_buffer</span><br><span class="line">with torch.no_grad():</span><br><span class="line">   output = F.linear(input.view(bsz, -1), weight, self.bias)</span><br><span class="line">return output.view(bsz, 1, -1)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="transformer">Transformer</h1>
<ul>
<li><p>Similarly, let's look at how self-attention-based models maintain
an incremental state</p></li>
<li><p>Clearly, when inferring the token at the i-th position, it's not
just related to the history of a window size like CNN, but to the first
i-1 positions. However, note that the key and value computed for the
first i-1 positions remain unchanged and can be reused. The i-th
position only generates its own key, value, and query, and uses the
query to query itself and the reusable key and value of the first i-1
positions. Therefore, the incremental state should include key and value
information, maintaining not a window size, but the entire
sequence.</p></li>
<li><p>In the code, TransformerDecoder passes the current layer input
and encoder output to TransformerDecoderLayer, updating the buffer</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if prev_self_attn_state is not None:</span><br><span class="line">   prev_key, prev_value = prev_self_attn_state[:2]</span><br><span class="line">   saved_state: Dict[str, Optional[Tensor]] = &#123;</span><br><span class="line">       &quot;prev_key&quot;: prev_key,</span><br><span class="line">       &quot;prev_value&quot;: prev_value,</span><br><span class="line">   &#125;</span><br><span class="line">   if len(prev_self_attn_state) &gt;= 3:</span><br><span class="line">       saved_state[&quot;prev_key_padding_mask&quot;] = prev_self_attn_state[2]</span><br><span class="line">   assert incremental_state is not None</span><br><span class="line">   self.self_attn._set_input_buffer(incremental_state, saved_state)</span><br><span class="line">_self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)</span><br></pre></td></tr></table></figure></li>
<li><p>And in MultiHeadAttention, if incremental_state exists, set key
and value to None, and subsequent calculations will skip when they are
None</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if incremental_state is not None:</span><br><span class="line">   saved_state = self._get_input_buffer(incremental_state)</span><br><span class="line">   if saved_state is not None and &quot;prev_key&quot; in saved_state:</span><br><span class="line">       # previous time steps are cached - no need to recompute</span><br><span class="line">       # key and value if they are static</span><br><span class="line">       if static_kv:</span><br><span class="line">           assert self.encoder_decoder_attention and not self.self_attention</span><br><span class="line">           key = value = None</span><br></pre></td></tr></table></figure></li>
<li><p>Then read, calculate, and update, with detailed code</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if saved_state is not None:</span><br><span class="line">   # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)</span><br><span class="line">   if &quot;prev_key&quot; in saved_state:</span><br><span class="line">       _prev_key = saved_state[&quot;prev_key&quot;]</span><br><span class="line">       assert _prev_key is not None</span><br><span class="line">       prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)</span><br><span class="line">       if static_kv:</span><br><span class="line">           k = prev_key</span><br><span class="line">       else:</span><br><span class="line">           assert k is not None</span><br><span class="line">           k = torch.cat([prev_key, k], dim=1)</span><br><span class="line">   if &quot;prev_value&quot; in saved_state:</span><br><span class="line">       _prev_value = saved_state[&quot;prev_value&quot;]</span><br><span class="line">       assert _prev_value is not None</span><br><span class="line">       prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)</span><br><span class="line">       if static_kv:</span><br><span class="line">           v = prev_value</span><br><span class="line">       else:</span><br><span class="line">           assert v is not None</span><br><span class="line">           v = torch.cat([prev_value, v], dim=1)</span><br><span class="line">   prev_key_padding_mask: Optional[Tensor] = None</span><br><span class="line">   if &quot;prev_key_padding_mask&quot; in saved_state:</span><br><span class="line">       prev_key_padding_mask = saved_state[&quot;prev_key_padding_mask&quot;]</span><br><span class="line">   assert k is not None and v is not None</span><br><span class="line">   key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(</span><br><span class="line">       key_padding_mask=key_padding_mask,</span><br><span class="line">       prev_key_padding_mask=prev_key_padding_mask,</span><br><span class="line">       batch_size=bsz,</span><br><span class="line">       src_len=k.size(1),</span><br><span class="line">       static_kv=static_kv,</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line">   saved_state[&quot;prev_key&quot;] = k.view(bsz, self.num_heads, -1, self.head_dim)</span><br><span class="line">   saved_state[&quot;prev_value&quot;] = v.view(bsz, self.num_heads, -1, self.head_dim)</span><br><span class="line">   saved_state[&quot;prev_key_padding_mask&quot;] = key_padding_mask</span><br><span class="line">   # In this branch incremental_state is never None</span><br><span class="line">   assert incremental_state is not None</span><br><span class="line">   incremental_state = self._set_input_buffer(incremental_state, saved_state)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="generate">Generate</h1>
<ul>
<li><p>Fairseq's models define all forward processes, and which forward
process is used depends on whether it's training or inference. Inference
uses fairseq-generate.</p></li>
<li><p>To complete a seq2seq, you need to specify the task and model,
along with other learning hyperparameters. The task determines dataset
parameters, establishes evaluation metrics, vocabulary, data batches,
and model instantiation.</p></li>
<li><p>The most important parts are train_step and inference_step, let's
look at inference_step</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def inference_step(self, generator, models, sample, prefix_tokens=None):</span><br><span class="line">   with torch.no_grad():</span><br><span class="line">       return generator.generate(models, sample, prefix_tokens=prefix_tokens)</span><br></pre></td></tr></table></figure></li>
<li><p>Here, the generator is a sequence_generator object, with the
generation part</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for step in range(max_len + 1):  # one extra step for EOS marker</span><br><span class="line">   # reorder decoder internal states based on the prev choice of beams</span><br><span class="line">   if reorder_state is not None:</span><br><span class="line">       if batch_idxs is not None:</span><br><span class="line">           # update beam indices to take into account removed sentences</span><br><span class="line">           corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)</span><br><span class="line">           reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)</span><br><span class="line">       model.reorder_incremental_state(reorder_state)</span><br><span class="line">       encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)</span><br><span class="line"></span><br><span class="line">   lprobs, avg_attn_scores = model.forward_decoder(</span><br><span class="line">       tokens[:, :step + 1], encoder_outs, temperature=self.temperature,</span><br><span class="line">   )</span><br></pre></td></tr></table></figure></li>
<li><p>This wraps an ensemble model. If we only have one decoder model,
then forward_decoder actually executes</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def _decode_one(</span><br><span class="line">   self, tokens, model, encoder_out, incremental_states, log_probs,</span><br><span class="line">   temperature=1.,</span><br><span class="line">):</span><br><span class="line">   if self.incremental_states is not None:</span><br><span class="line">       decoder_out = list(model.forward_decoder(</span><br><span class="line">           tokens,</span><br><span class="line">           encoder_out=encoder_out,</span><br><span class="line">           incremental_state=self.incremental_states[model],</span><br><span class="line">       ))</span><br><span class="line">   else:</span><br><span class="line">       decoder_out = list(model.forward_decoder(tokens, encoder_out=encoder_out))</span><br></pre></td></tr></table></figure></li>
<li><p>Here you can see that incremental decoding is used to decode the
sentence step by step</p></li>
</ul>
<h1 id="reorder">Reorder</h1>
<ul>
<li>For more detailed information, refer to this blog post, which is
very well written and even officially endorsed by being added to the
code comments <a
href="http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/">understanding-incremental-decoding-in-fairseq</a></li>
<li>There's another point about reorder in the decoder, also mentioned
in this blog post.</li>
<li>During inference, unlike training, beam search is used. So we
maintain not just one cache queue, but beam_size number of queues.</li>
<li>When selecting the i-th word, the input token stored in the k-th
beam's cache queue might have come from the j-th beam's cache queue
during beam search at the i-1 position. Therefore, reordering is needed
to ensure consistency.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="fairseq架构">Fairseq架构</h1>
<ul>
<li>在Facebook推出的seq2seq库Fairseq当中，所有模型继承FairseqEncdoerDecoder类，所有的Encoder继承FairseqEncoder类，所有的Decoder继承FairseqIncrementalDecoder类，而FairseqIncrementalDecoder继承自FairseqDecoder类。</li>
<li>FairseqEncoder类只定义了forward，reorder_encoder_out,max_positions，upgrade_state_dict，最重要的就是forward，即定义编码的前向传播过程。reorder其实在decoder中更重要，但是这里也定义了。</li>
<li>FairseqDecoder类定义了forward，extract_features,output_layer，get_normalized_probs，max_positions，upgrade_state_dict，prepare_for_onnx_export_。forward=extract_features+output_layer，即forward定义了解码出序列的整个前向过程，而extract_features只定义到获得整个decoder的state
sequence。</li>
<li>Incremental
Decoder额外定义了reorder_incremental_state，set_beam_size。reorder是和incremental以及beam
search密切相关的，后文将详细介绍。</li>
</ul>
<h1 id="训练并行推理增量">训练并行，推理增量</h1>
<ul>
<li>像CNN seq2seq，
Transformer之类的模型打破了RNN模型的顺序性，使得seq2seq架构中的编码器和解码器在训练是都可以并行训练。</li>
<li>编码器并行训练非常显然，而解码器实际上是一个语言模型，之所以可以并行是因为在训练时采用了teacher
forcing，因此语言模型的每一时间步输入在训练时我们假设是已知的，就可以一整个(Batch,Length,Hidden)的decoder
input输入模型，直接训练。</li>
<li>但是在测试（推理）阶段，每一时间步的输入由上一时间步的输出决定，无法并行操作，如果反复运行整个decoder，那么就要运行Length次，且第i次只有前i个位置的信息是有用的，剩下部分的计算完全浪费掉了，推理的效率大大降低。</li>
<li>这个时候就需要incremental
decoding，即在推理阶段，无论是CNN还是Transformer，都想RNN一样一步一步解码，每一步使用之前推理得到的信息，而不是完全从头开始计算。</li>
</ul>
<h1 id="cnn">CNN</h1>
<ul>
<li><p>对于CNN，可以发现，decoder无论哪一层，第i个位置都只需要该层[i-k,i)位置上内的信息，其中k为一维卷积的窗长。因此，只需要维护一个长度为k的队列，保存各层计算出来的state，就可以复用模型之前推理得到的信息，之后再把当前的state更新到队列中。</p></li>
<li><p>每次计算时只需要对第i个位置进行decoding，即操作(Batch,1,Hidden)的数据Length次。</p></li>
<li><p>在代码里，FConvDecoder将输入x和incremental_state一起传给了LinearizedConvolution，这里的介绍是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;&quot;&quot;An optimized version of nn.Conv1d.</span><br><span class="line">At training time, this module uses ConvTBC, which is an optimized version</span><br><span class="line">of Conv1d. At inference time, it optimizes incremental generation (i.e.,</span><br><span class="line">one time step at a time) by replacing the convolutions with linear layers.</span><br><span class="line">Note that the input order changes from training to inference.</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>即训练时使用Time-First的形式组织数据进行卷积，充分利用GPU的并行性能，在推断时，将卷积层换成相同效果的线性层，逐帧进行推断</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if incremental_state is None:</span><br><span class="line">   output = super().forward(input) # 这里 LinearizedConvolution的父类是ConvTBC，即没有推断时，直接将整个序列送入ConvTBC</span><br><span class="line">   if self.kernel_size[0] &gt; 1 and self.padding[0] &gt; 0:</span><br><span class="line">       # remove future timesteps added by padding</span><br><span class="line">       output = output[:-self.padding[0], :, :]</span><br><span class="line">return output</span><br></pre></td></tr></table></figure></li>
<li><p>否则，就逐层用线性层推断，并更新input
buffer进而更新incremental_state</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># reshape weight</span><br><span class="line">weight = self._get_linearized_weight()</span><br><span class="line">kw = self.kernel_size[0]</span><br><span class="line">bsz = input.size(0)  # input: bsz x len x dim</span><br><span class="line">if kw &gt; 1:</span><br><span class="line">   input = input.data</span><br><span class="line">   input_buffer = self._get_input_buffer(incremental_state)</span><br><span class="line">   if input_buffer is None:</span><br><span class="line">       input_buffer = input.new(bsz, kw, input.size(2)).zero_()</span><br><span class="line">       self._set_input_buffer(incremental_state, input_buffer)</span><br><span class="line">   else:</span><br><span class="line">       # shift buffer</span><br><span class="line">       input_buffer[:, :-1, :] = input_buffer[:, 1:, :].clone()</span><br><span class="line">   # append next input</span><br><span class="line">   input_buffer[:, -1, :] = input[:, -1, :]</span><br><span class="line">   input = input_buffer</span><br><span class="line">with torch.no_grad():</span><br><span class="line">   output = F.linear(input.view(bsz, -1), weight, self.bias)</span><br><span class="line">return output.view(bsz, 1, -1)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="transformer">Transformer</h1>
<ul>
<li><p>同样的，我们看基于自注意力的模型如何去维护一个incremental
state</p></li>
<li><p>显然，在推断第i个位置的token时，不像CNN只与窗口大小的history相关，而是与前i-1个位置相关，但是注意，前i-1个位置计算出来的key和value是不变的，是可以复用的，第i位置只生成该位置的key，value以及query，并用query查询自己以及前i-1个位置复用的key,value，因此，incremental
state应该包含了key与value的信息，且维护的不是窗口大小，而是整个序列。</p></li>
<li><p>在代码里，TransformerDecoder将当前层输入和encoder输出传给TransformerDecoderLayer，更新buffer</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if prev_self_attn_state is not None:</span><br><span class="line">   prev_key, prev_value = prev_self_attn_state[:2]</span><br><span class="line">   saved_state: Dict[str, Optional[Tensor]] = &#123;</span><br><span class="line">       &quot;prev_key&quot;: prev_key,</span><br><span class="line">       &quot;prev_value&quot;: prev_value,</span><br><span class="line">   &#125;</span><br><span class="line">   if len(prev_self_attn_state) &gt;= 3:</span><br><span class="line">       saved_state[&quot;prev_key_padding_mask&quot;] = prev_self_attn_state[2]</span><br><span class="line">   assert incremental_state is not None</span><br><span class="line">   self.self_attn._set_input_buffer(incremental_state, saved_state)</span><br><span class="line">_self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)</span><br></pre></td></tr></table></figure></li>
<li><p>并在MultiHeadAttention里，假如incremental_state存在，将key和value设为None，后面的计算判断为None时就跳过计算</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if incremental_state is not None:</span><br><span class="line">   saved_state = self._get_input_buffer(incremental_state)</span><br><span class="line">   if saved_state is not None and &quot;prev_key&quot; in saved_state:</span><br><span class="line">       # previous time steps are cached - no need to recompute</span><br><span class="line">       # key and value if they are static</span><br><span class="line">       if static_kv:</span><br><span class="line">           assert self.encoder_decoder_attention and not self.self_attention</span><br><span class="line">           key = value = None</span><br></pre></td></tr></table></figure></li>
<li><p>之后读取、计算、更新，代码写的很详细。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if saved_state is not None:</span><br><span class="line">   # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)</span><br><span class="line">   if &quot;prev_key&quot; in saved_state:</span><br><span class="line">       _prev_key = saved_state[&quot;prev_key&quot;]</span><br><span class="line">       assert _prev_key is not None</span><br><span class="line">       prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)</span><br><span class="line">       if static_kv:</span><br><span class="line">           k = prev_key</span><br><span class="line">       else:</span><br><span class="line">           assert k is not None</span><br><span class="line">           k = torch.cat([prev_key, k], dim=1)</span><br><span class="line">   if &quot;prev_value&quot; in saved_state:</span><br><span class="line">       _prev_value = saved_state[&quot;prev_value&quot;]</span><br><span class="line">       assert _prev_value is not None</span><br><span class="line">       prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)</span><br><span class="line">       if static_kv:</span><br><span class="line">           v = prev_value</span><br><span class="line">       else:</span><br><span class="line">           assert v is not None</span><br><span class="line">           v = torch.cat([prev_value, v], dim=1)</span><br><span class="line">   prev_key_padding_mask: Optional[Tensor] = None</span><br><span class="line">   if &quot;prev_key_padding_mask&quot; in saved_state:</span><br><span class="line">       prev_key_padding_mask = saved_state[&quot;prev_key_padding_mask&quot;]</span><br><span class="line">   assert k is not None and v is not None</span><br><span class="line">   key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(</span><br><span class="line">       key_padding_mask=key_padding_mask,</span><br><span class="line">       prev_key_padding_mask=prev_key_padding_mask,</span><br><span class="line">       batch_size=bsz,</span><br><span class="line">       src_len=k.size(1),</span><br><span class="line">       static_kv=static_kv,</span><br><span class="line">   )</span><br><span class="line"></span><br><span class="line">   saved_state[&quot;prev_key&quot;] = k.view(bsz, self.num_heads, -1, self.head_dim)</span><br><span class="line">   saved_state[&quot;prev_value&quot;] = v.view(bsz, self.num_heads, -1, self.head_dim)</span><br><span class="line">   saved_state[&quot;prev_key_padding_mask&quot;] = key_padding_mask</span><br><span class="line">   # In this branch incremental_state is never None</span><br><span class="line">   assert incremental_state is not None</span><br><span class="line">   incremental_state = self._set_input_buffer(incremental_state, saved_state)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="generate">Generate</h1>
<ul>
<li><p>Fairseq的模型定义了所有前向过程，至于具体选择哪个前向过程则依据训练还是推断来决定。推断使用了fairseq-generate。</p></li>
<li><p>要完成一次seq2seq，需要指定task和model，以及其他学习超参数。其中task确定了数据集参数，建立评价指标、词典、data_batch、实例化模型等等。</p></li>
<li><p>其中最重要的就是train_step和inference_step，我们看看inference_step</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def inference_step(self, generator, models, sample, prefix_tokens=None):</span><br><span class="line">   with torch.no_grad():</span><br><span class="line">       return generator.generate(models, sample, prefix_tokens=prefix_tokens)</span><br></pre></td></tr></table></figure></li>
<li><p>这里的generator是一个sequence_generator对象，其中生成的部分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for step in range(max_len + 1):  # one extra step for EOS marker</span><br><span class="line">   # reorder decoder internal states based on the prev choice of beams</span><br><span class="line">   if reorder_state is not None:</span><br><span class="line">       if batch_idxs is not None:</span><br><span class="line">           # update beam indices to take into account removed sentences</span><br><span class="line">           corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)</span><br><span class="line">           reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)</span><br><span class="line">       model.reorder_incremental_state(reorder_state)</span><br><span class="line">       encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)</span><br><span class="line"></span><br><span class="line">   lprobs, avg_attn_scores = model.forward_decoder(</span><br><span class="line">       tokens[:, :step + 1], encoder_outs, temperature=self.temperature,</span><br><span class="line">   )</span><br></pre></td></tr></table></figure></li>
<li><p>这里做了一层emsemble
model的包装，假如我们只有一个decoder模型，那么实际上forward_decoder执行的是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def _decode_one(</span><br><span class="line">   self, tokens, model, encoder_out, incremental_states, log_probs,</span><br><span class="line">   temperature=1.,</span><br><span class="line">):</span><br><span class="line">   if self.incremental_states is not None:</span><br><span class="line">       decoder_out = list(model.forward_decoder(</span><br><span class="line">           tokens,</span><br><span class="line">           encoder_out=encoder_out,</span><br><span class="line">           incremental_state=self.incremental_states[model],</span><br><span class="line">       ))</span><br><span class="line">   else:</span><br><span class="line">       decoder_out = list(model.forward_decoder(tokens, encoder_out=encoder_out))</span><br></pre></td></tr></table></figure></li>
<li><p>这里可以看到是用incremental decoding逐步解码出句子</p></li>
</ul>
<h1 id="reorder">Reorder</h1>
<ul>
<li>更多的详细信息可以参考这篇博文，写的非常好，甚至被官方钦点加入了代码注释里<a
href="http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/">understanding-incremental-decoding-in-fairseq</a></li>
<li>还有一点，就是decoder中的reorder，在这篇博文里也有提到。</li>
<li>在推断时和训练不同的另一点就是beam
search。因此我们不止维护一个缓存队列，而是beam_size个队列。</li>
<li>那么在挑选第i个词的时候，第k个beam缓存队列的存的输入token可能是第i-1个位置时第j个beam缓存队列beam
search出来的，因此需要重新排序保证一致。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>inference</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for NLP with Graph-Structured Representations</title>
    <url>/2020/04/05/graph-doc-thesis/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/39f4cd2114b7e9c3f742909d3b176d01.png" width="500"/></p>
<p>Read Dr. Bang Liu’s paper Natural Language Processing and Text Mining
with Graph-Structured Representations from the University of Alberta and
take some notes.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="natural-language-processing-based-on-graph-structural-representation">Natural
Language Processing Based on Graph Structural Representation</h1>
<ul>
<li>The paper mainly encompasses work in four primary directions:
<ul>
<li>Event Extraction, Story Generation</li>
<li>Semantic Matching</li>
<li>Recommendation</li>
<li>Reading Comprehension</li>
</ul></li>
</ul>
<h1 id="related-fields-and-methodology">Related Fields and
Methodology</h1>
<ul>
<li>Graph construction related work in NLP</li>
<li>Words as nodes
<ul>
<li>Syntactic information as edges: Learning substructures of document
semantic graphs for document summarization</li>
<li>Co-occurrence information as edges: Graph-of-word and tw-idf: new
approach to ad hoc ir; Shortest-path graph kernels for document
similarity; Directional skip-gram: Explicitly distinguishing left and
right context for word embeddings;</li>
</ul></li>
<li>Sentences, paragraphs, documents as nodes
<ul>
<li>Word co-occurrence, position as edges: Textrank: Bringing order into
texts;</li>
<li>Similarity as edges: Evaluating text coherence based on semantic
similarity graph;</li>
<li>Links as edges: Pagerank</li>
<li>Hybrid: Graph methods for multilingual framenets</li>
</ul></li>
<li>Methodology
<ul>
<li>Clarify input and output, determine semantic granularity, graph
construction (define nodes and edges, extract node and edge features),
graph representation reconstruction, conduct experiments</li>
<li>The key to introducing graphs in NLP is introducing structural and
relational information.</li>
<li>Graph representation reconstruction problems, such as
<ul>
<li>Semantic matching: Tree or graph matching</li>
<li>Event discovery: Community detection</li>
<li>Phrase mining: Node classification and ranking</li>
<li>Ontology creation: Relationship identification</li>
<li>Question generation: Node selection <img data-src="https://s1.ax1x.com/2020/04/02/GG2kes.png" alt="GG2kes.png" /></li>
</ul></li>
</ul></li>
</ul>
<h1 id="event-extraction-and-story-generation">Event Extraction and
Story Generation</h1>
<h2 id="related-work">Related Work</h2>
<ul>
<li>Text Clustering
<ul>
<li>Similarity-based methods requiring specified cluster count</li>
<li>Density-based methods less suitable for high-dimensional sparse text
spaces</li>
<li>Non-negative matrix factorization methods (spectral clustering)</li>
<li>Probabilistic models, such as PLSA, LDA, GMM</li>
</ul></li>
<li>Story Structure Generation
<ul>
<li>Continuously categorizing new events into existing clusters</li>
<li>Generating story summaries for time-sequential events; traditional
summarization methods cannot continuously generate; currently using
Bayesian model methods, but Gibbs sampling is too time-consuming</li>
</ul></li>
<li>Authors proposed EventX method, constructed Story Forest system,
related to above two approaches, focusing on open-domain news document
event extraction</li>
</ul>
<h2
id="story-forest-extracting-events-and-telling-stories-from-breaking-newstkdd">Story
Forest: Extracting Events and Telling Stories from Breaking
News(TKDD)</h2>
<ul>
<li>Story Forest: Extracting events and generating stories from
news</li>
<li>Definition: A topic is equivalent to a story forest, containing
multiple story trees; each story tree's nodes are events, with events as
the minimal processing unit, essentially multiple news documents about a
single event. The system assumes each article reports only one
event</li>
<li>Overall Architecture: <img data-src="https://s1.ax1x.com/2020/04/05/GrQJun.png" alt="GrQJun.png" /></li>
<li>Primary focus on EventX's event clustering method, a two-layer
clustering
<ul>
<li><p>Construct a keyword co-occurrence graph, with keywords as nodes,
considering two points for edge creation: intra-document relevance
(co-occurrence exceeding threshold in same document); corpus relevance
(conditional probability exceeding threshold):</p>
<p><span class="math display">\[
\operatorname{Pr}\left\{w_{i} | w_{j}\right\}=\frac{D F_{i, j}}{D F_{j}}
\]</span></p></li>
<li><p>Perform community detection on this keyword graph, clustering
keywords, with each cluster considered to describe the same topic. Each
topic is a collection of keywords, equivalent to a document (bag of
words)</p></li>
<li><p>Calculate similarity between each document and each topic,
assigning documents to the topic with maximum similarity, completing the
first layer: document clustering by theme</p></li>
<li><p>After separating documents by topic, further subdivide events
within each topic, the second clustering layer. Event cluster sizes are
typically severely imbalanced; authors propose a supervised
learning-guided clustering method</p></li>
<li><p>Now viewing each document as a node, aiming to create edges
between documents discussing the same event. Unable to manually design
rules, they used supervised learning, training an SVM to judge whether
documents describe the same event. After obtaining the document graph,
perform community detection to complete the second clustering
layer</p></li>
</ul></li>
</ul>
<h1 id="semantic-matching">Semantic Matching</h1>
<h2 id="related-work-1">Related Work</h2>
<ul>
<li>Document-level semantic matching, related work: Text matching,
document graph structural representation</li>
<li>Text Matching
<ul>
<li>Interaction placed last, first extracting embeddings via Siamese
networks, then scoring matched embedding pairs</li>
<li>Early interaction, first extracting pair-wise interactions as
features, then aggregating interactions through neural networks, finally
scoring</li>
</ul></li>
<li>Document Graph Structural Representation
<ul>
<li>Word graph: Constructing graphs via syntactic parsing to obtain SPO
triples, potentially extended via Wordnet; window-based methods with
nodes representing unique terms and directed edges representing
co-occurrences within a fixed-size sliding window; using dependency
relationships as edges; hyperlink-based graph construction</li>
<li>Text graph: Nodes are sentences, paragraphs, documents; edges
established based on word-level similarity, position, co-occurrence</li>
<li>Concept graph: Based on knowledge graphs, extracting document
entities as nodes (e.g., DBpedia), then performing DFS within 2 hops to
find outgoing relations and entities; or based on Wordnet, Verbnet,
finding semantic roles, constructing edges with semantic/syntactic
relations</li>
<li>Hybrid graph: Heterogeneous graph with multiple node and edge types,
including lexical, tokens, syntactic structure nodes, part of speech
nodes</li>
</ul></li>
</ul>
<h2
id="matching-article-pairs-with-graphical-decomposition-and-convolutionsacl-2019">Matching
Article Pairs with Graphical Decomposition and Convolutions(ACL
2019)</h2>
<ul>
<li>Authors' work in text matching and document graph structural
representation involves proposing a document graph construction method,
using GCN to extract document features for document-level matching</li>
<li>This section can improve the previous work's method of determining
whether two documents discuss the same event</li>
<li>Overall process: <img data-src="https://s1.ax1x.com/2020/04/05/GrQUEV.png" alt="GrQUEV.png" /></li>
<li>Graph Construction: Concept Interaction Graph
<ul>
<li>First construct key graph, extracting keywords and entities from
articles as nodes, creating edges if two nodes appear in the same
sentence</li>
<li>Key graph nodes can be directly used as concepts, or overlapping
community detection can be performed to divide the key graph into
multiple intersecting subgraphs, with subgraphs serving as concept
nodes, intersections creating edges</li>
<li>Concepts and sentences are now bag-of-words, enabling similarity
calculation and sentence assignment to concepts</li>
<li>Concepts are now sentence collections, viewed as bag-of-words, with
edges between concepts based on sentence set TF-IDF vector
similarity</li>
<li>Critically, since matching is performed, input is sentence pairs,
transformed into graph pairs. Authors merge two CIGs into a large CIG,
placing sentence sets from two articles describing the same concept in
one concept node</li>
</ul></li>
<li>Constructing Matching Network with GCN
<ul>
<li><p>After obtaining a large CIG, matching between two documents
becomes matching between sentence sets from two documents within each
node</p></li>
<li><p>Construct an end-to-end process: use Siamese networks to extract
node features, use GCN for inter-node feature interaction, aggregate
features for prediction</p></li>
<li><p>Each node contains two sentence sets, concatenated into two long
sentences, input into Siamese networks, extracting features using BiLSTM
or CNN, then feature aggregation:</p>
<p><span class="math display">\[
\mathbf{m}_{A
B}(v)=\left(\left|\mathbf{c}_{A}(v)-\mathbf{c}_{B}(v)\right|,
\mathbf{c}_{A}(v) \circ \mathbf{c}_{B}(v)\right)
\]</span></p></li>
<li><p>Obtaining matching vector for each node, representing
similarity-related features between two documents at that concept.
Additionally, authors extracted traditional similarity features (TF-IDF,
BM25, Jaccard, Ochiai) and concatenated them with matching
vectors</p></li>
<li><p>Next, pass through GCN</p></li>
<li><p>Aggregate all node features using simple mean pooling, then pass
through a linear layer for classification (0, 1)</p></li>
</ul></li>
<li>On long news corpora (avg 734 tokens), graph matching significantly
outperforms traditional two-tower models (DUET, DSSM, ARC at 50-60 F1,
graph model reaching 70-80)</li>
</ul>
<h2
id="matching-natural-language-sentences-with-hierarchical-sentence-factorizationwww-2018">Matching
Natural Language Sentences with Hierarchical Sentence Factorization(WWW
2018)</h2>
<figure>
<img data-src="https://s1.ax1x.com/2020/04/05/GrQrv9.png" alt="GrQrv9.png" />
<figcaption aria-hidden="true">GrQrv9.png</figcaption>
</figure>
<ul>
<li>Sentence pair semantic matching task, primarily utilizing AMR
parsing to obtain sentence structure</li>
<li>Five steps
<ul>
<li>AMR parsing and alignment: AMR represents sentences as directed
acyclic graphs; by copying and splitting nodes with multiple parents,
the directed acyclic graph can be converted to a tree. AMR leaf nodes
represent concepts, others are relationships representing concept
connections or concept parameters. After obtaining AMR graph, alignment
is needed to connect specific tokens with concepts. Authors used
existing tool JAMR</li>
<li>AMR purification: A token might connect to multiple concepts;
authors select only the shallowest concept connection, remove
relationship content, retain edges without edge labels, obtaining
simplified AMR tree</li>
<li>Index mapping: Add root node, reset node coordinates</li>
<li>Node completion: Similar to padding, ensuring tree depths are
identical</li>
<li>Node traversal: Perform depth-first search, concatenating child tree
content to each non-leaf node</li>
</ul></li>
<li>The root node obtains a reorganized sentence representation, similar
to predicate-argument form. Authors suggest this can uniformly express
two sentences, avoiding semantic matching errors caused by word order
and less important words</li>
<li>Subsequently use Ordered Word Mover Distance. The formula represents
transportation cost (embedding distance), transportation volume (word
proportion), α, β represent word frequency vectors of both sentences,
with uniform distribution directly substituted. Traditional WMD didn't
consider word order; authors introduced two penalty terms, with T
primarily concentrated on diagonal when I is large. P is an ideal T
distribution, with row-column elements satisfying standard Gaussian
distribution distance from diagonal, hoping T becomes a diagonal
matrix</li>
<li>This method calculates OWMD distance for AMR representations of two
sentences, completing unsupervised text similarity calculation. Can also
utilize AMR Tree for supervised learning</li>
<li>Note that different AMR tree layers represent different semantic
granularities of the entire sentence. Authors selected first three
layers, inputting different granularity semantic units into context
layer, summing token embeddings within the same semantic unit, assuming
maximum k child nodes per layer, padding if insufficient <img data-src="https://s1.ax1x.com/2020/04/11/GbgQUA.png" alt="GbgQUA.png" /></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="基于图结构表示的自然语言处理">基于图结构表示的自然语言处理</h1>
<ul>
<li>论文主要囊括了主要包含四个方向的工作：
<ul>
<li>事件抽取、故事生成</li>
<li>语义匹配</li>
<li>推荐</li>
<li>阅读理解</li>
</ul></li>
</ul>
<h1 id="相关领域及方法论">相关领域及方法论</h1>
<ul>
<li>在NLP当中构图的相关工作</li>
<li>词作为节点
<ul>
<li>syntactic信息作为边：Learning substructures of document semantic
graphs for document summarization</li>
<li>共现信息作为边： Graph-of-word and tw-idf: new approach to ad hoc
ir； Shortest-path graph kernels for document similarity；Directional
skip-gram: Explicitly distinguishing left and right context for word
embeddings；</li>
</ul></li>
<li>句子、段落、文档作为节点
<ul>
<li>词共现、位置作为边：Textrank: Bringing order into texts；</li>
<li>相似度作为边：Evaluating text coherence based on semantic similarity
graph；</li>
<li>链接作为边：Pagerank</li>
<li>混合：Graph methods for multilingual framenets</li>
</ul></li>
<li>方法论
<ul>
<li>明确输入输出、决定语义的细粒度、构图（定义节点和边，抽取节点和边的特征）、基于图的表示重构问题、进行实验</li>
<li>在NLP中引入图的关键是引入结构信息和关系信息。</li>
<li>基于图的表示重构问题，例如
<ul>
<li>语义匹配：树或者图的匹配</li>
<li>事件发现：社区发现</li>
<li>phrase挖掘：节点分类和排序</li>
<li>ontology creation： 关系鉴别</li>
<li>问题生成：节点选择 <img data-src="https://s1.ax1x.com/2020/04/02/GG2kes.png" alt="GG2kes.png" /></li>
</ul></li>
</ul></li>
</ul>
<h1 id="事件抽取及故事生成">事件抽取及故事生成</h1>
<h2 id="相关工作">相关工作</h2>
<ul>
<li>文本聚类
<ul>
<li>基于相似度的方法，需要指定聚类数目</li>
<li>基于密度的方法，不太适合文本这样的高维稀疏空间</li>
<li>基于非负矩阵分解的方法（谱聚类）</li>
<li>基于概率模型，例如PLSA，LDA，GMM</li>
</ul></li>
<li>故事结构生成
<ul>
<li>持续的将新事件归类到已有聚类当中</li>
<li>想要为一系列时序事件生成故事总结，传统的基于summarization的方法不能持续生成，现在多采用基于贝叶斯模型的方法，但是gibbs
sampling太耗时</li>
</ul></li>
<li>作者提出了EventX方法，构建了Story
Forest系统，与以上两个路线相关，做的是开放域新闻文档事件抽取</li>
</ul>
<h2
id="story-forest-extracting-events-and-telling-stories-from-breaking-newstkdd">Story
Forest: Extracting Events and Telling Stories from Breaking
News(TKDD)</h2>
<ul>
<li>Story Forest：从新闻中抽取事件，生成故事</li>
<li>定义：一个topic相当于一个story forest，包含多个story tree；每个story
tree的节点是一个event，event作为最小处理单元，实际上是关于某一个event的多篇新闻文档。本系统假设每篇文章只报道一个event</li>
<li>整体架构： <img data-src="https://s1.ax1x.com/2020/04/05/GrQJun.png"
alt="GrQJun.png" /></li>
<li>主要关注EventX中如何做cluster events，是一个两层聚类
<ul>
<li><p>构建一个keyword co-occurrence
graph，节点是keyword，建边考虑两点：单篇文档内的相关性，即同一篇文档内共现次数超过阈值就建边；语料上的相关性，即条件概率超过阈值：</p>
<p><span class="math display">\[
\operatorname{Pr}\left\{w_{i} | w_{j}\right\}=\frac{D F_{i, j}}{D F_{j}}
\]</span></p></li>
<li><p>在这个keyword graph上做community
detection，将keyword做一次聚类，认为每个类的Keyword描述同一个topic。这样每个topic是一系列keywords的集合，相当于一个文档（bag
of words）</p></li>
<li><p>计算每篇文档和每个topic之间的相似度，将文档分配给具有最大相似度的topic，这样就完成了第一层：文档按主题聚类</p></li>
<li><p>将文档按topic分开之后，每个topic下还要细分event，这就是第二层聚类，event
cluster的大小通常严重不均衡，作者提出了一种基于监督学习指导的聚类方法</p></li>
<li><p>现在将每篇文档看成节点，希望谈论同一个event的文档之间建边，这里不太好人为设计规则，就使用了监督学习，训练了一个SVM来判断是否描述同一个event，获得document
graph之后接着做community detection，完成第二层聚类</p></li>
</ul></li>
</ul>
<h1 id="语义匹配">语义匹配</h1>
<h2 id="相关工作-1">相关工作</h2>
<ul>
<li>文档级别的语义匹配，相关工作：文本匹配、文档图结构表示</li>
<li>文本匹配
<ul>
<li>交互放在最后，先通过孪生网络提取embedding，然后待匹配的embedding
pair进行score</li>
<li>交互提前，先提取pair-wise的交互作为特征，然后通过神经网络聚合交互，最后score</li>
</ul></li>
<li>文档图结构表示
<ul>
<li>word
graph:通过句法剖析得到spo三元组构图，还可以通过wordnet扩展合并；基于窗口的方法，nodes
represent unique terms and directed edges represent co-occurrences
between the terms within a fixed-size sliding
window；将依存关系作为边；基于超链接的构图；</li>
<li>text
graph:节点是句子、段落、文档，边基于词级别的相似度、位置、共现建立。</li>
<li>concept
graph:基于知识图谱，提取文档中的实体作为节点，例如DBpedia，然后通过最多两跳在知识图谱中进行dfs，找出outgoing
relation and entity，构图；或者基于wordnet,verbnet，找出semantic
role，用semantic/syntactic relation建边</li>
<li>hybrid graph:即异构图，多种节点以及多种边，lexical，tokens,
syntactic structure nodes, part of speech nodes</li>
</ul></li>
</ul>
<h2
id="matching-article-pairs-with-graphical-decomposition-and-convolutionsacl-2019">Matching
Article Pairs with Graphical Decomposition and Convolutions(ACL
2019)</h2>
<ul>
<li>作者在文本匹配和文档图结构表示方向的工作是提出了一种文档建图的方式，然后用GCN提取文档特征，进行文档级别的匹配</li>
<li>这一部分可以改进上一个工作中两篇文档是否讨论同一个event的部分。</li>
<li>整体流程： <img data-src="https://s1.ax1x.com/2020/04/05/GrQUEV.png"
alt="GrQUEV.png" /></li>
<li>构图：Concept Interaction Graph
<ul>
<li>先构建key
graph，抽取文章中的keywords和实体作为节点，假如两个节点出现在同一句中就建边</li>
<li>可以将key graph中的节点直接作为concept，也可以在key
graph上做一个overlapping community detection,将Key
graph切分成多个相交的子图，子图作为concept节点，相交就建边</li>
<li>现在concept 和 sentence都是bag of
words，就可以计算相似度，将句子分配到concept</li>
<li>现在concept是句子的集合，将其看成bag of
words，concept之间就可以根据句子集合之间的tfidf vector
similarity建边</li>
<li>很关键的一点，由于是做matching，输入是句子对，在这一步变成了图对，作者将两个CIG合并成一个大CIG，将描述同一个concept的两篇文章的句子集合放在一个concept
节点中</li>
</ul></li>
<li>构建matching network with GCN
<ul>
<li><p>得到一个大的CIG之后，两篇文档之间的matching变成了大CIG当中每一个节点里来自两篇文档的sentence
set之间的matching</p></li>
<li><p>构建了一个端到端的流程：用孪生网络提取节点特征、用GCN做节点直接的特征交互、聚合所有特征做预测</p></li>
<li><p>这里每个节点包含了两个句子集，将其拼接成两个长句，进孪生网络，分别用bilstm或者cnn提取到特征，再进行一个特征的聚合：</p>
<p><span class="math display">\[
\mathbf{m}_{A
B}(v)=\left(\left|\mathbf{c}_{A}(v)-\mathbf{c}_{B}(v)\right|,
\mathbf{c}_{A}(v) \circ \mathbf{c}_{B}(v)\right)
\]</span></p></li>
<li><p>这样就得到每个节点上的matching
vector，可以理解为在该节点(concept)上两篇文档的相似度相关特征。此外作者还提取了一些传统特征相似度（tfidf,bm25,jaccard,Ochiai）的值拼接到matching
vector当中</p></li>
<li><p>接下来过GCN</p></li>
<li><p>聚合所有节点特征就是一个简单的mean
pooling，然后过一个线性层做分类（0，1）</p></li>
</ul></li>
<li>在长篇新闻语料上（avg 734 token)，graph
matching的效果远好于传统的双塔模型(DUET,DSSM,ARC都在50~60的f1，graph
model达到了70~80)</li>
</ul>
<h2
id="matching-natural-language-sentences-with-hierarchical-sentence-factorizationwww-2018">Matching
Natural Language Sentences with Hierarchical Sentence Factorization(WWW
2018)</h2>
<figure>
<img data-src="https://s1.ax1x.com/2020/04/05/GrQrv9.png" alt="GrQrv9.png" />
<figcaption aria-hidden="true">GrQrv9.png</figcaption>
</figure>
<ul>
<li><p>句子对语义匹配任务，主要利用的是AMR剖析得到的句子结构</p></li>
<li><p>五个步骤</p>
<ul>
<li>AMR parsing and
alignment：AMR将句子表示为有向无环图，如果将有多个父节点的节点复制拆分，则可以将有向无环图转换为树。AMR的叶子节点代表一个概念，其余的是关系，代表概念之间的关联或者某个概念是另外一个概念的参数。得到AMR图之后还需要对齐，将句子中具体的token和概念建立连接。作者使用了现有的工具JAMR</li>
<li>AMR
purification：一个token可能与多个概念建立连接，作者只选择最浅层的概念建立连接，之后将关系的内容删去，只保留边不保留边的label，得到简化之后的AMR
tree，如上图所示</li>
<li>Index mapping：加入root节点，重设节点坐标</li>
<li>Node completion：类似于padding，保证两个句子的树的深度一样</li>
<li>Node
traversal：做一次dfs,使得每个非叶子节点的内容拼接了其子树的内容</li>
</ul></li>
<li><p>这样root节点就得到原句子的一个重新组织方式，类似于predicate-argument的形式，即谓语接谓语操作的词，类似于最初始的AMR
purification得到的只保留概念的N叉树先做后序遍历再逆序，作者的意思大致是这样可以统一的表达两个句子，避免原文表示中各种因词序和其他不那么重要的词（助词介词）的现象导致接下来语义匹配中产生错误。</p></li>
<li><p>接下来使用Ordered Word Mover
Distance。如下式，D代表传输的cost(embedding距离），T代表传输量(词占比），<span
class="math inline">\(\alpha,\beta\)</span>代表两个句子中各个词的词频向量，这里作者直接用均匀分布替代。传统的WMD没有考虑词的顺序，作者引入了两个惩罚项，当T主要集中在对角线上时I较大，P是T的一个理想分布，其第i行第j列个元素满足i,j到对角线位置的距离的标准高斯分布，也是希望T是一个对角阵。对角阵的意义就是会在传输时考虑顺序，不发生相对距离较远的传输。</p>
<p><span class="math display">\[
\begin{array}{ll}
\underset{T \in \mathbb{R}_{+}^{M \mathrm{XN}}}{\operatorname{minimize}}
&amp; \sum_{i, j} T_{i j} D_{i j}-\lambda_{1} I(T)+\lambda_{2} K L(T \|
P) \\
\text { subject to } &amp; \sum_{i=1}^{M} T_{i j}=\beta_{j}^{\prime}
\quad 1 \leq j \leq N^{\prime} \\
&amp; \sum_{j=1}^{N} T_{i j}=\alpha_{i}^{\prime} \quad 1 \leq i \leq
M^{\prime}
\end{array} \\
\]</span></p>
<p><span class="math display">\[
I(T)=\sum_{i=1}^{M^{\prime}} \sum_{j=1}^{N^{\prime}} \frac{T_{i
j}}{\left(\frac{i}{M^{\prime}}-\frac{j}{N^{\prime}}\right)^{2}+1} \\
\]</span></p>
<p><span class="math display">\[
P_{i j}=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{l^{2}(i, j)}{2
\sigma^{2}}} \\
\]</span></p>
<p><span class="math display">\[
l(i, j)=\frac{\left|i / M^{\prime}-j / N^{\prime}\right|}{\sqrt{1 /
M^{\prime 2}+1 / N^{\prime 2}}} \\
\]</span></p></li>
<li><p>通过上述方法就能得到针对两个句子AMR表示的OWMD距离，完成无监督的文本相似度计算。也可以充分利用AMR
Tree来完成监督学习。</p></li>
<li><p>注意到在AMR树中，不同层的节点其实代表了整句不同粒度的语义切分，例如第0层是整句，第一层是短句"Jerry
little",第二层是单一的概念"Jerry"，作者选取前三层，将每一层不同粒度的语义单元之间输入context
layer，同一语义单元内的token
embedding相加作为单一embedding，并假设每一层的子节点最多k个，不足的padding。
<img data-src="https://s1.ax1x.com/2020/04/11/GbgQUA.png"
alt="GbgQUA.png" /></p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>gnn</tag>
        <tag>math</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic Regression and Maximum Entropy</title>
    <url>/2018/10/14/lr-and-me/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/50e93d01eefd2d64738c372694d4f1fd.png" width="500"/></p>
<p>Note for John Mount's "The Equivalence of Logistic Regression and
Maximum Entropy Models" and explains that this proof is a special case
of the general derivation proof of the maximum entropy model introduced
in statistical learning methods</p>
<p>Conclusion</p>
<ul>
<li>Maximum entropy model is softmax classification</li>
<li>Under the balanced conditions of the general linear model, the model
mapping function that satisfies the maximum entropy condition is the
softmax function</li>
<li>In the book on Statistical Machine Learning methods, a maximum
entropy model defined under the feature function is presented, which,
along with softmax regression, belongs to the class of log-linear
models</li>
<li>When the feature function extends from a binary function to the
feature value itself, the maximum entropy model becomes a softmax
regression model</li>
<li>The maximum entropy maximizes conditional entropy, not the entropy
of conditional probabilities, nor the entropy of joint
probabilities.</li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="define-symbols">Define symbols</h1>
<ul>
<li>n-dimensional features, m samples, <span
class="math inline">\(x(i)_j\)</span> denotes the j-th feature of the
i-th sample, discuss the multi-class scenario, the output classification
<span class="math inline">\(y(i)\)</span> has k classes, the mapping
probability function <span class="math inline">\(\pi\)</span> maps from
<span class="math inline">\(R^n\)</span> to <span
class="math inline">\(R^k\)</span> , we hope <span
class="math inline">\(\pi(x(i))_{y(i)}\)</span> to be as large as
possible.</li>
<li>Indicator function <span class="math inline">\(A(u,v)\)</span> ,
equals 1 when <span class="math inline">\(u==v\)</span> and 0
otherwise</li>
</ul>
<h1 id="logistic-regression">Logistic regression</h1>
<p><span class="math display">\[
\pi(x)_1 = \frac{e^{\lambda x}}{1+e^{\lambda x}} \\
\pi(x)_2 = 1 - \pi(x)_1\\
\]</span></p>
<ul>
<li>The parameter to be learned <span
class="math inline">\(\lambda\)</span> is <span
class="math inline">\(R^n\)</span></li>
</ul>
<h1 id="softmax-regression">Softmax regression</h1>
<p><span class="math display">\[
\pi(x)_v = \frac{e^{\lambda _v x}} {\sum _{u=1}^k e^{\lambda _u x}}
\]</span></p>
<ul>
<li>For <span class="math inline">\(R^{k * n}\)</span></li>
</ul>
<h1 id="solving-softmax">Solving softmax</h1>
<ul>
<li><p>When using softmax or logistic as nonlinear functions, they
possess a good property of differentiation, that is, the derivative
function can be expressed in terms of the original function</p></li>
<li><p>We can now define the objective function, which is to maximize
the correct category probability output by the <span
class="math inline">\(\pi\)</span> function (maximum likelihood), and
define the optimization obtained by <span
class="math inline">\(\lambda\)</span> :</p>
<p><span class="math display">\[
\lambda = argmax \sum _{i=1}^m log (\pi (x(i))_{y(i)}) \\
= argmax f(\lambda) \\
\]</span></p></li>
</ul>
<h1 id="balanced-equation">Balanced Equation</h1>
<ul>
<li><p>Derive the objective function above and set the derivative to
0:</p>
<p><span class="math display">\[
\frac {\partial f(\lambda)}{\partial \lambda _{u,j}} = \sum
_{i=1，y(i)=u}^m x(i)_j - \sum _{i=1}^m x(i)_j \pi (x(i))_u =0 \\
\]</span></p></li>
<li><p>Thus, we obtain an important balance equation:</p>
<p><span class="math display">\[
\ \  for \ all \ u,j \\
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p>Analyze this equation:</p>
<ul>
<li><p>Plain Language: We hope to obtain a mapping function <span
class="math inline">\(\pi\)</span> , such that for a certain dimension
(j) feature, the sum of the weighted feature values of all samples
mapped to the u class by the mapping function is equal to the sum of the
feature values of all samples within the u class. It is obvious that the
best case is that the elements within both summation expressions are
completely identical, only the samples of the u class are summed, and
the probability that the mapping function maps the u class samples to
the u class is 1, while the probability that samples of other classes
are mapped to the u class is 0.</p></li>
<li><p>However, this equation is very lenient, requiring only that the
two sums be equal, without demanding that each element be the same, and
the expression of the mapping function is not explicitly written out.
Any nonlinear mapping that satisfies this equation could be called a
mapping function.</p></li>
<li><p>In formulaic terms, it is expressed as</p>
<p><span class="math display">\[
\sum _{i=1}^m A(u,y(i)) x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\pi (x(i))_u \approx A(u,y(i)) \\
\]</span></p></li>
</ul></li>
</ul>
<h1 id="from-maximum-entropy-to-softmax">From Maximum Entropy to
Softmax</h1>
<ul>
<li><p>What was mentioned above is that the balanced equation does not
require the format of the mapping function, then why did we choose
softmax? In other words, under what conditions can the constraint of the
balanced equation lead to the conclusion that the nonlinear mapping is
softmax?</p></li>
<li><p>The answer is maximum entropy. Now let's review the conditions
that need to be met in <span class="math inline">\(\pi\)</span> .</p>
<ul>
<li><p>Balance equation (i.e., this <span
class="math inline">\(\pi\)</span> can fit the data):</p>
<p><span class="math display">\[
\ \  for \ all \ u,j \\
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p>The output of <span class="math inline">\(\pi\)</span> should be
a probability:</p>
<p><span class="math display">\[
\pi (x)_v \geq 0 \\
\sum _{v=1}^k \pi (x)_v = 1 \\
\]</span></p></li>
</ul></li>
<li><p>According to the maximum entropy principle, we hope that <span
class="math inline">\(\pi\)</span> can have the maximum entropy while
satisfying the aforementioned constraints:</p>
<p><span class="math display">\[
\pi = argmax \ Ent(\pi) \\
Ent(\pi) = - \sum_{v=1}^k \sum _{i=1}^m \pi (x(i))_v log (\pi (x(i))_v)
\\
\]</span></p></li>
<li><p>The maximum entropy can be understood from two perspectives:</p>
<ul>
<li>Maximum entropy, also known as maximum perplexity, refers to the low
risk of overfitting in the model, with low model complexity. According
to Ockham's Razor principle, among multiple models with the same effect,
the one with lower complexity has better generalization ability. Under
the satisfaction of constraint conditions, of course, we would hope for
a model with lower complexity, which is equivalent to
regularization.</li>
<li>The constraints are the parts of our model that are known to need to
be satisfied and need to be fitted; the remaining parts are the unknown
parts, with no rules or data to guide us in assigning probabilities.
What should we do in this unknown situation? In the case of the unknown,
probabilities should be uniformly distributed among all possibilities,
which corresponds to the maximum entropy situation.</li>
</ul></li>
<li><p>The problem has now been formulated as a constrained optimization
problem, which can be solved using the Lagrange multiplier method. There
is a trick; the original text states that it would be somewhat complex
to directly consider the probabilistic inequality conditions, and the
KKT conditions would need to be used, which we will not consider here.
If the <span class="math inline">\(\pi\)</span> obtained satisfies the
inequality conditions, we can skip it (which is indeed the
case).</p></li>
</ul>
<p><span class="math display">\[
L = \sum _{j=1}^n \sum _{v=1}^k \lambda _{v,j} (\sum _{i=1}^m \pi
(x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\
+ \sum _{v=1}^k \sum _{i=1}^m \beta _i (\pi (x(i))_v -1) \\
- \sum _{v=1}^k \sum _{i=1}^m \pi(x(i))_v log(\pi (x(i))_v) \\
\]</span></p>
<ul>
<li><p>Here is another trick, where we should differentiate all
parameters. Here, we first differentiate <span class="math inline">\(\pi
(x(i))_u\)</span> and set it to 0 to obtain:</p>
<p><span class="math display">\[
\pi (x(i))_u = e^{\lambda _u x(i) + \beta _i -1}
\]</span></p></li>
<li><p>Considering the equality constraint condition (the sum of
probabilities equals 1), it is not necessary to differentiate with
respect to <span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[
\sum _{v=1}^k e^{\lambda _v x(i) + \beta _i -1} = 1 \\
e^{\beta} = \frac {1}{\sum _{v=1}^k e^{\lambda _v x(i) - 1}} \\
\]</span></p></li>
<li><p>Re-substitution yields:</p>
<p><span class="math display">\[
\pi (x)_u = \frac {e^{\lambda _u}x}{\sum _{v=1}^k e^{\lambda _v}x}
\]</span></p></li>
</ul>
<h1 id="solving-parameters">Solving Parameters</h1>
<ul>
<li>From the time of introducing the balanced equation, it can be seen
that we need to solve <span class="math inline">\(n \* k\)</span>
equations to obtain <span class="math inline">\(n \* k\)</span>
parameters <span class="math inline">\(\lambda\)</span> , or take
partial derivatives of <span class="math inline">\(n \* k\)</span> <span
class="math inline">\(\lambda\)</span> in the Lagrange equation of
maximum entropy, because <span class="math inline">\(\pi\)</span> is a
non-linear function of <span class="math inline">\(\lambda\)</span> .
Both of these methods are relatively difficult, but we can calculate the
Jacobian equations (or the Hessian matrix of the objective function) of
these equations by differentiation, and then we can solve <span
class="math inline">\(\lambda\)</span> using some Newton method, Fisher
Scoring, or iterative method</li>
</ul>
<h1
id="connection-with-the-maximum-entropy-model-defined-by-characteristic-functions">Connection
with the Maximum Entropy Model Defined by Characteristic Functions</h1>
<ul>
<li><p>In this paper, the constraint is (omitted the constraint <span
class="math inline">\(\pi\)</span> must be a probability):</p>
<p><span class="math display">\[
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p>The maximum entropy is:</p>
<p><span class="math display">\[
Ent(\pi) = - \sum_{v=1}^k \sum _{i=1}^m \pi (x(i))_v log (\pi (x(i))_v)
\\
\]</span></p></li>
<li><p>The results obtained are:</p>
<p><span class="math display">\[
\pi (x)_u = \frac {e^{\lambda _u}x}{\sum _{v=1}^k e^{\lambda _v}x}
\]</span></p></li>
<li><p>In statistical learning methods, the constraints are (with the
probability constraints similarly omitted), where <span
class="math inline">\(P^{*}\)</span> represents the empirical
distribution:</p>
<p><span class="math display">\[
\sum _{x,y} P^{*} (x,y)f(x,y) = \sum _{x,y} P^{*} (x)P(y|x)f(x,y)
\]</span></p></li>
<li><p>The maximum entropy is:</p>
<p><span class="math display">\[
Ent(P) = - \sum _{x,y} P^{*}(x) P(y|x) log P(y|x)
\]</span></p></li>
<li><p>The results obtained are:</p>
<p><span class="math display">\[
P(y|x) = \frac{e^{\sum _i w_i f_i(x,y)}}{\sum _y e^{\sum _i w_i
f_i(x,y)}}
\]</span></p></li>
<li><p>It can be seen that there is a distinction in the representation
of the two; the former directly obtains the form of the softmax
function, but does not maximize the conditional entropy, whereas the
latter is the opposite</p></li>
<li><p>In fact, both are unified. Firstly, the parameters of the model
are all Lagrange multipliers, the former being <span
class="math inline">\(\lambda\)</span> , and the latter being <span
class="math inline">\(w\)</span> , with the relationship:</p>
<p><span class="math display">\[
\lambda = \{w_0,...,w_i,...\}
\]</span></p></li>
<li><p>When the characteristic function extends to the eigenvalue, the
model obtained by both is the same (softmax function):</p>
<p><span class="math display">\[
f_i(x_j,y) = x(j)_i
\]</span></p></li>
<li><p>The balance conditions of both are also consistent. Noticing that
<span class="math inline">\(P^{*}\)</span> is an empirical distribution,
which is statistically obtained through classical probability type on
the training set, in general, repeated data is not considered (with a
total sample size of N and a number of categories K), then:</p>
<p><span class="math display">\[
P^{*} (x) = \frac 1N \\
\sum _{x,y} P^{*} (x,y) = 1 \\
P^{*} (x,y) \in \{0,\frac 1N \} \\
\]</span></p></li>
<li><p>After substitution, it will be found that the balance conditions
of the two are consistent, while the calculation in the paper seems to
be entropy, but it is actually conditional entropy; it merely ignores
the constant condition <span class="math inline">\(P^{*} (x) = \frac
1N\)</span> from the argmax expression and writes it in the form of
entropy.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="明确符号">明确符号</h1>
<ul>
<li>n维特征，m个样本，<span
class="math inline">\(x(i)_j\)</span>表示第i个样本第j维特征，讨论多分类情况，输出分类<span
class="math inline">\(y(i)\)</span>有k类，映射概率函数<span
class="math inline">\(\pi\)</span>从<span
class="math inline">\(R^n\)</span>映射到<span
class="math inline">\(R^k\)</span>，我们希望<span
class="math inline">\(\pi(x(i))_{y(i)}\)</span>尽可能大。</li>
<li>指示函数<span class="math inline">\(A(u,v)\)</span>，当<span
class="math inline">\(u==v\)</span>时为1，否则为0</li>
</ul>
<h1 id="logistic回归">Logistic回归</h1>
<p><span class="math display">\[
\pi(x)_1 = \frac{e^{\lambda x}}{1+e^{\lambda x}} \\
\pi(x)_2 = 1 - \pi(x)_1\\
\]</span></p>
<ul>
<li>其中要学习到的参数<span
class="math inline">\(\lambda\)</span>为<span
class="math inline">\(R^n\)</span></li>
</ul>
<h1 id="softmax回归">Softmax回归</h1>
<p><span class="math display">\[
\pi(x)_v = \frac{e^{\lambda _v x}} {\sum _{u=1}^k e^{\lambda _u x}}
\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span>为<span
class="math inline">\(R^{k * n}\)</span></li>
</ul>
<h1 id="求解softmax">求解softmax</h1>
<ul>
<li><p>当使用softmax或者logistic作为非线性函数时，它们存在一个很好的求导的性质，即导函数可以用原函数表示
<span class="math display">\[
\frac {\partial \pi (x)_v}{\partial \lambda _{v,j}} = x_j  \pi (x)_v
(1-\pi (x)_v) \\
\frac {\partial \pi (x)_v}{\partial \lambda _{u,j}} = -x_j \pi (x)_v \pi
(x)_u \ where \  u \neq v \\
\]</span></p></li>
<li><p>现在我们可以定义目标函数，即希望<span
class="math inline">\(\pi\)</span>函数输出的正确类别概率最大（最大似然），并定义最优化得到的<span
class="math inline">\(\lambda\)</span>：</p>
<p><span class="math display">\[
\lambda = argmax \sum _{i=1}^m log (\pi (x(i))_{y(i)}) \\
= argmax f(\lambda) \\
\]</span></p></li>
</ul>
<h1 id="平衡等式">平衡等式</h1>
<ul>
<li><p>对上面的目标函数求导并令导函数为0：</p>
<p><span class="math display">\[
\frac {\partial f(\lambda)}{\partial \lambda _{u,j}} = \sum
_{i=1，y(i)=u}^m x(i)_j - \sum _{i=1}^m x(i)_j \pi (x(i))_u =0 \\
\]</span></p></li>
<li><p>这样我们就得到一个重要的平衡等式(Balance Equation)：</p>
<p><span class="math display">\[
\ \  for \ all \ u,j \\
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p>分析这个等式：</p>
<ul>
<li><p>大白话：我们希望得到这么一个映射函数<span
class="math inline">\(\pi\)</span>，对某一维(j)特征，用所有样本被映射函数归为第u类的概率加权所有样本的特征值之和，等于第u类内所有样本的特征值之和。显然，最好的情况就是左右两个累加式内的元素完全一样，只有第u类的样本被累加，且第u类样本被映射函数归为第u类的概率为1，其他类样本被归为第u类样本的概率为0.</p></li>
<li><p>但是，这个等式非常的宽松，它只要求两个和式相同，并不要求每一个元素相同，而且这个式子没有显示的写出映射函数的表达式，任何满足该式的非线性映射都有可能称为映射函数。</p></li>
<li><p>用公式表达，就是</p>
<p><span class="math display">\[
\sum _{i=1}^m A(u,y(i)) x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\pi (x(i))_u \approx A(u,y(i)) \\
\]</span></p></li>
</ul></li>
</ul>
<h1 id="由最大熵推出softmax">由最大熵推出softmax</h1>
<ul>
<li><p>上面说到了平衡等式并没有要求映射函数的格式，那么为什么我们选择了softmax？换句话，什么条件下能从平衡等式的约束推出非线性映射为softmax？</p></li>
<li><p>答案是最大熵。我们现在回顾一下<span
class="math inline">\(\pi\)</span>需要满足的条件：</p>
<ul>
<li><p>平衡等式（即这个<span
class="math inline">\(\pi\)</span>能拟合数据）：</p>
<p><span class="math display">\[
\ \  for \ all \ u,j \\
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p><span class="math inline">\(\pi\)</span>的输出得是一个概率：</p>
<p><span class="math display">\[
\pi (x)_v \geq 0 \\
\sum _{v=1}^k \pi (x)_v = 1 \\
\]</span></p></li>
</ul></li>
<li><p>根据最大熵原理，我们希望满足上述约束条件的<span
class="math inline">\(\pi\)</span>能够具有最大的熵:</p>
<p><span class="math display">\[
\pi = argmax \ Ent(\pi) \\
Ent(\pi) = - \sum_{v=1}^k \sum _{i=1}^m \pi (x(i))_v log (\pi (x(i))_v)
\\
\]</span></p></li>
<li><p>最大熵可以从两个角度理解：</p>
<ul>
<li>最大熵也就是最大困惑度，即模型过拟合的风险低，模型复杂程度低，根据奥卡姆剃刀原则，在多个具有相同效果的模型中复杂程度小的模型具有更好的泛化能力，在满足了约束条件的情况下，当然我们希望要一个复杂程度小的模型，相当于正则化。</li>
<li>约束条件是我们的模型已知的需要满足、需要拟合的部分，剩下的部分是未知的部分，没有规则或者数据指导我们分配概率，那该怎么办？在未知的情况下就应该均匀分配概率给所有可能，这正是对应了最大熵的情况。</li>
</ul></li>
<li><p>现在问题已经形式化带约束条件的最优化问题，利用拉格朗日乘子法求解即可。这里有一个trick，原文中说如果直接考虑概率的不等条件就有点复杂，需要使用KTT条件，这里先不考虑，之后如果求出的<span
class="math inline">\(\pi\)</span>满足不等式条件的话就可以跳过了（事实也正是如此）。</p></li>
</ul>
<p><span class="math display">\[
L = \sum _{j=1}^n \sum _{v=1}^k \lambda _{v,j} (\sum _{i=1}^m \pi
(x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\
+ \sum _{v=1}^k \sum _{i=1}^m \beta _i (\pi (x(i))_v -1) \\
- \sum _{v=1}^k \sum _{i=1}^m \pi(x(i))_v log(\pi (x(i))_v) \\
\]</span></p>
<ul>
<li><p>这里又有一个trick，本来应该对所有参数求导，这里我们先对<span
class="math inline">\(\pi (x(i))_u\)</span>求导令其为0可得：</p>
<p><span class="math display">\[
\pi (x(i))_u = e^{\lambda _u x(i) + \beta _i -1}
\]</span></p></li>
<li><p>再考虑等式约束条件（概率之和为1），这样就不用再对<span
class="math inline">\(\beta\)</span>求导：</p>
<p><span class="math display">\[
\sum _{v=1}^k e^{\lambda _v x(i) + \beta _i -1} = 1 \\
e^{\beta} = \frac {1}{\sum _{v=1}^k e^{\lambda _v x(i) - 1}} \\
\]</span></p></li>
<li><p>回代可得：</p>
<p><span class="math display">\[
\pi (x)_u = \frac {e^{\lambda _u}x}{\sum _{v=1}^k e^{\lambda _v}x}
\]</span></p></li>
</ul>
<h1 id="求解参数">求解参数</h1>
<ul>
<li>从推出平衡等式的时候可以看到，我们需要解<span
class="math inline">\(n \* k\)</span>个方程来得到<span
class="math inline">\(n \* k\)</span>个参数<span
class="math inline">\(\lambda\)</span>，或者在最大熵的拉格朗日方程里对<span
class="math inline">\(n \* k\)</span>个<span
class="math inline">\(\lambda\)</span>求偏导，因为<span
class="math inline">\(\pi\)</span>是<span
class="math inline">\(\lambda\)</span>的非线性函数，这两种求解方法比较困难，但是我们可以求导计算这些等式的雅各比方程（或者说是目标函数的Hessian矩阵），之后我们就可以用某种牛顿法、Fisher
Scoring或者迭代的方法求解<span
class="math inline">\(\lambda\)</span></li>
</ul>
<h1
id="与特征函数定义的最大熵模型的联系">与特征函数定义的最大熵模型的联系</h1>
<ul>
<li><p>在本文中，约束为（省略了<span
class="math inline">\(\pi\)</span>必须为概率的约束）：</p>
<p><span class="math display">\[
\sum _{i=1，y(i)=u}^m x(i)_j = \sum _{i=1}^m x(i)_j \pi (x(i))_u \\
\]</span></p></li>
<li><p>最大化的熵为：</p>
<p><span class="math display">\[
Ent(\pi) = - \sum_{v=1}^k \sum _{i=1}^m \pi (x(i))_v log (\pi (x(i))_v)
\\
\]</span></p></li>
<li><p>得到的结果为：</p>
<p><span class="math display">\[
\pi (x)_u = \frac {e^{\lambda _u}x}{\sum _{v=1}^k e^{\lambda _v}x}
\]</span></p></li>
<li><p>而在统计学习方法中，约束为（同样省略了概率约束），其中<span
class="math inline">\(P^{*}\)</span>代表经验分布：</p>
<p><span class="math display">\[
\sum _{x,y} P^{*} (x,y)f(x,y) = \sum _{x,y} P^{*} (x)P(y|x)f(x,y)
\]</span></p></li>
<li><p>最大化的熵为：</p>
<p><span class="math display">\[
Ent(P) = - \sum _{x,y} P^{*}(x) P(y|x) log P(y|x)
\]</span></p></li>
<li><p>得到的结果为：</p>
<p><span class="math display">\[
P(y|x) = \frac{e^{\sum _i w_i f_i(x,y)}}{\sum _y e^{\sum _i w_i
f_i(x,y)}}
\]</span></p></li>
<li><p>可以看到两者的表示有区别，前者直接得到了softmax函数的形式，但是最大化的不是条件熵，后者则相反</p></li>
<li><p>实际上两者是统一的。首先，模型的参数都是拉格朗日乘子，前者是<span
class="math inline">\(\lambda\)</span>，后者是<span
class="math inline">\(w\)</span>，两者的关系：</p>
<p><span class="math display">\[
\lambda = \{w_0,...,w_i,...\}
\]</span></p></li>
<li><p>当特征函数扩展到特征值时，两者得到的模型就是一样的（softmax函数）：</p>
<p><span class="math display">\[
f_i(x_j,y) = x(j)_i
\]</span></p></li>
<li><p>两者的平衡条件也是一致的，注意到<span
class="math inline">\(P^{*}\)</span>是经验分布，是在训练集上通过古典概型统计出来的，一般情况下不考虑重复数据（样本总数为N，类别数为K），则有：</p>
<p><span class="math display">\[
P^{*} (x) = \frac 1N \\
\sum _{x,y} P^{*} (x,y) = 1 \\
P^{*} (x,y) \in \{0,\frac 1N \} \\
\]</span></p></li>
<li><p>代入之后会发现两者的平衡条件一致，而论文中计算的貌似是熵，实际上是条件熵，只不过把$P^{*}
(x) = 1N $这一常量条件从argmax表达式中忽略了，写成了熵的形式。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Inference Algorithms in Probabilistic ML</title>
    <url>/2018/08/28/inference-algorithm/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/cbd846ecd88abb611db2c204930d896d.png" width="500"/></p>
<p>Record the principles and derivations of algorithms used for
inferring unknown variables in probabilistic machine learning, such as
Variational Inference, Expectation Maximization, and Markov Chain Monte
Carlo. Many contents and derivations, as well as images, come from the
online course and lecture notes of Professor Xu Yida at the University
of Technology Sydney. Professor Xu's series of videos on non-parametric
Bayesian methods are very good, and you can find the videos by searching
his name on Bilibili or Youku. The address of Professor Xu's course
notes is <a
href="https://github.com/roboticcam/machine-learning-notes">roboticcam/machine-learning-notes</a>.
Unless otherwise specified, some screenshots and code are from Professor
Xu's lecture notes. Other contents come from various books or tutorials,
and the references will be indicated in the text.</p>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/19/iwWPun.png" alt="iwWPun.png" />
<figcaption aria-hidden="true">iwWPun.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="bayesian-inference">Bayesian Inference</h1>
<ul>
<li>In Bayesian inference, it is necessary to distinguish observable
quantities (data) from unknown variables (which may be statistical
parameters, missing data, or latent variables)</li>
<li>Statistical parameters are regarded as random variables in the
Bayesian framework, and we need to make probabilistic estimates of the
model's parameters. In the frequentist framework, parameters are
determined and non-random quantities, mainly used for probabilistic
estimates of the data</li>
<li>In the frequentist framework, only the likelihood is focused on
<span class="math inline">\(p(x|\theta)\)</span> , while the Bayesian
school believes that the parameters <span
class="math inline">\(\theta\)</span> should be treated as variables,
making prior assumptions about the parameters <span
class="math inline">\(p(\theta)\)</span> before observing the data</li>
<li>Posterior proportionality to the product of likelihood and prior,
representing the parameter probability distribution we obtain after
adjusting the prior for the observed data</li>
<li>In the Bayesian framework, we are more concerned with precision,
which is the reciprocal of variance, for example, in the posterior of a
normal distribution, precision is the sum of the precision of the prior
and the data</li>
<li>The posterior is actually a balance between the maximum likelihood
estimation and the prior</li>
<li>As the amount of data increases, the posterior gradually ceases to
depend on the prior</li>
<li>Many times, we do not have prior knowledge, in which case a flat,
dispersed distribution is generally used as the prior distribution, such
as a uniform distribution with a wide range or a normal distribution
with a large variance</li>
<li>Sometimes we do not need to know the entire posterior distribution
but only make some point estimates or interval estimates</li>
</ul>
<h1 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h1>
<ul>
<li>MCMC, the first MC stands for how to sample so that the sampling
points satisfy the distribution, and the second MC stands for using
random sampling to estimate the parameters of the distribution</li>
<li>Maximum likelihood estimation and the EM algorithm are both point
estimations, while MCMC finds the complete posterior distribution
through sampling</li>
<li>Monte Carlo simple sampling is known for its distribution, but it is
not possible to directly calculate certain statistics of functions on
this distribution. Therefore, statistics are indirectly calculated by
generating samples through random sampling of this distribution and then
using the samples to compute the statistics</li>
<li>Monte Carlo inference involves an unknown distribution and known
samples (data), where the distribution is inferred from the samples
(question mark pending determination)</li>
</ul>
<h2 id="sampling">Sampling</h2>
<ul>
<li><p>It is difficult (or the distribution is unknown) to derive some
statistical quantities directly through the distribution function; we
can obtain the statistical quantities by generating a series of samples
that conform to this distribution and calculating the statistical
quantities through sample statistics, i.e., by random sampling</p></li>
<li><p>In parameter inference, we can randomly sample a series of
samples from the posterior distribution that satisfies the parameters,
thereby estimating the parameters based on the samples</p></li>
<li><p>The simplest sampling: inverse sampling from the cumulative
distribution function, which is first performing a uniform distribution
sampling from [0,1], and then using this value as the output of the cdf
function; the sampling value is the input to the cdf:</p>
<p><span class="math display">\[
u = U(0,1) \\
x= cdf ^{-1} (u) \\
\]</span></p></li>
</ul>
<h2 id="refuse-sampling">Refuse Sampling</h2>
<ul>
<li><p>Not all cumulative distribution functions of distributions are
easy to invert. Another sampling method is called rejection
sampling.</p></li>
<li><p>For a probability density function, we cannot sample it directly,
so we construct a distribution that is everywhere greater than the
probability density function, surrounding this function, as shown in the
figure where the red line encloses the green line <img data-src="https://s1.ax1x.com/2018/10/20/i0oFwd.jpg"
alt="i0oFwd.jpg" /></p></li>
<li><p>We calculate the distance of each point to the red line and the
green line, dividing it into acceptance and rejection regions. Thus, we
first sample from the red distribution to obtain samples, and then
perform a [0,1] uniform distribution sampling. If the sample falls
within the acceptance region, it is accepted; otherwise, it is
rejected</p></li>
<li><p>显然红色分布处处比绿色大是不可能的，积分不为
1，因此需要按比例放缩一下，乘以一个系数 M，算法如下：</p>
<pre><code>i=0
while i!= N
x(i)~q(x) and u~U(0,1)
if u&lt; p(x(i))/Mq(x(i)) then
   accept x(i)
   i=i+1
else
   reject x(i)
end
end</code></pre></li>
<li><p>rejection sampling efficiency is too low because if the red
distribution is not chosen well, and it cannot tightly enclose the green
distribution, the acceptance rate is too low, and most samples will be
rejected.</p></li>
</ul>
<h2 id="adaptive-rejection-sampling">Adaptive Rejection Sampling</h2>
<ul>
<li>When the distribution is log-concave, we can effectively construct
the envelope of the green distribution, which means the red distribution
is closer to the green distribution and has a higher acceptance
rate</li>
<li>The basic idea is to divide the green distribution to be sampled
into k regions, with the leftmost point in each region serving as the
starting point. If the green distribution at the starting point in each
region can be enveloped by its tangent line, we can then use the tangent
lines on these k regions to form the red region</li>
<li>However, this requires that the original distribution be concave in
each region, but for example, the probability density function of the
Gaussian distribution is not a concave function; however, the Gaussian
distribution becomes concave after taking the logarithm, which is what
is called log-concave. Therefore, we first take the logarithm, draw the
tangent, and then calculate the exponential to return to the original
distribution, obtaining the k-segment tangents of the original
distribution. <img data-src="https://s1.ax1x.com/2018/10/20/i0oEFI.jpg"
alt="i0oEFI.jpg" /></li>
</ul>
<h2 id="importance-sampling">Importance Sampling</h2>
<ul>
<li><p>The sampling algorithm mentioned above samples from a simple
distribution (proposed distribution), calculates the acceptance rate for
each sample through the relationship between the simple distribution and
the complex distribution, and rejects some samples to ensure that the
remaining samples satisfy the complex distribution</p></li>
<li><p>The idea of importance sampling is to weight the sample points
rather than simply rejecting or accepting them, thus fully utilizing
each sample point.</p></li>
<li><p>For example, we hope to obtain the expected value of a
distribution through sampling</p>
<p><span class="math display">\[
E_{p(x)}(f(x)) = \int _x f(x)p(x)dx \\
E_{p(x)}(f(x)) = \int _x f(x) \frac{p(x)}{q(x)} q(x) dx \\
E_{p(x)}(f(x)) = \int _x g(x)q(x)dx \\
\]</span></p></li>
<li><p>p(x) is difficult to sample, so we convert it to sampling from
q(x). Here, <span class="math inline">\(\frac{p(x)}{q(x)}\)</span>
represents the importance weight.</p></li>
<li><p>We thus eliminate the restriction that the red distribution must
envelop the green distribution, as long as we calculate the importance
weights and perform importance weighting on the sampled points, we can
obtain some statistical quantities under the green
distribution.</p></li>
</ul>
<h2 id="markov-monte-carlo-and-metropolis-hastings-algorithms">Markov
Monte Carlo and Metropolis-Hastings algorithms</h2>
<ul>
<li><p>MCMC is another sampling method, where the sample sequence is
regarded as a Markov chain, and the samples sampled by MCMC are not
independent; the probability distribution of the next sample is related
to the previous sample</p></li>
<li><p>Different from the concepts of general sampling acceptance or
rejection, MCMC calculates the probability distribution of the next
sample's position under the premise of the current sample after each
sample, which is the key transition probability.</p></li>
<li><p>After sampling a sample, we draw the next one according to the
transition probability, obtaining a series of samples that conform to
the given distribution. It is evident that the transition probability
needs to be related to the given distribution. We utilize the
convergence of the Markov chain, hoping that the distribution after
convergence, denoted as <span class="math inline">\(\pi\)</span> , is
the given distribution, assuming the transition probability is denoted
as <span class="math inline">\(k(x^{&#39;} | x)\)</span> , from sample
<span class="math inline">\(x\)</span> to sample <span
class="math inline">\(x^{&#39;}\)</span> .</p></li>
<li><p>In the Markov chain, there is the following Chapman-Kolmogorov
equation:</p>
<p><span class="math display">\[
\pi _t (x^{&#39;}) = \int _x \pi _{t-1}(x) k(x^{&#39;} | x) dx
\]</span></p></li>
<li><p>The significance of this formula is self-evident. We hope to
achieve the convergence of Markov chains. After convergence, regardless
of how the transition is made, the sequence of samples obtained should
satisfy the same given distribution, then the requirement is:</p>
<p><span class="math display">\[
\pi _t (x) = \pi _{t-1} (x)
\]</span></p></li>
<li><p>Actual use relies on another important formula, known as the
detailed balance condition:</p>
<p><span class="math display">\[
\pi (x) k(x^{&#39;} | x) = \pi (x^{&#39;}) k(x | x^{&#39;})
\]</span></p></li>
<li><p>From detailed balance, it can be deduced that the
Chapman-Kologronvo equation holds, but the converse is not necessarily
true.</p></li>
<li><p>When the detailed balance condition is satisfied, the Markov
chain is convergent</p></li>
<li><p>In the LDA blog, mh and Gibbs are introduced, and
Metropolis-Hasting is the result of the basic MCMC where the acceptance
rate on one side is raised to 1: <img data-src="https://s1.ax1x.com/2018/10/20/i0okTA.jpg"
alt="i0okTA.jpg" /></p></li>
<li><p>In the mh, we did not alter the transition matrix to adapt to the
given distribution, but instead used the given distribution to correct
the transition matrix, thus, the transition matrix is one that we
ourselves designed. Generally, the transition matrix (proposal
distribution) is designed as a Gaussian distribution centered around the
current state. For this Gaussian distribution, when the variance is
small, the probability is concentrated around the current sampling
point, so the position transferred to the next sampling point is
unlikely to change much, resulting in a high acceptance rate (since the
current sampling point is the one that passed the acceptance, it is
likely to be in a position with a high acceptance rate). However, this
will cause the random walk to be slow; if the variance is large, it will
wander everywhere, and the acceptance rate will decrease.</p></li>
<li><p>Despite one side's sample acceptance rate reaching 1, there is
always one side below 1. If it is rejected, the MCMC will repeat
sampling at the same location once and then continue.</p></li>
<li><p>And Gibbs raised both acceptance rates to 1, which shows that
Gibbs is a special case of MCMC. MCMC does not modify the transition
probability but adds the acceptance rate, linking the original
transition probability with the distribution to be sampled. However, it
is obvious that if we ourselves choose the transition probability and
make it more closely related to the original distribution, the effect
will be better, and Gibbs follows this approach.</p></li>
</ul>
<h2 id="hybrid-metropolis-hasting">Hybrid Metropolis-Hasting</h2>
<ul>
<li>To be supplemented</li>
</ul>
<h2 id="gibbs-sampling">Gibbs Sampling</h2>
<ul>
<li><p>A motivation for Gibbs sampling: It is difficult to sample
directly from the joint distribution of multiple parameters, but if
other parameters are fixed as conditions, sampling from the conditional
distribution of just one parameter becomes much simpler, and it can be
proven that the samples obtained after convergence satisfy the joint
distribution</p></li>
<li><p>Firstly, let's consider why the Gibbs sampling process does not
change the joint probability distribution through iterative conditional
sampling. Firstly, when excluding the i-th parameter to calculate the
conditional probability, the marginal distribution of the excluded n-1
variables is the same as the marginal distribution of the true joint
probability distribution for these n-1 variables, because their values
have not changed; the condition on which the conditional probability is
based is unchanged compared to the true distribution, so the conditional
probability distribution is also unchanged. Both the marginal
distribution and the conditional probability distribution are unchanged
(true), so the joint distribution obtained by multiplying them is
naturally unchanged, and therefore, in each iteration step, sampling is
done according to the true distribution and the iteration does not
change this distribution.</p></li>
<li><p>Gibbs sampling is a coordinate descent method similar to
variational inference, updating one component of the sample at a time,
based on the conditional probability of the current updating component's
dimension given the other components: <img data-src="https://s1.ax1x.com/2018/10/20/i0oVYt.jpg"
alt="i0oVYt.jpg" /></p></li>
<li><p>Industrial applications of Gibbs sampling are widespread due to
its speed. In fact, such an iterative algorithm cannot be parallelized,
but the collapsed Gibbs sampling can parallelize the iteration. The
principle is to treat several components as a whole, collapsing them
into one component. When other components are updated using this set of
components, they are considered independent (there is some doubt;
another way to describe collapse is to ignore some conditional
variables. Basic Gibbs sampling is essentially collapsed Gibbs sampling,
and the approach of treating several components as a whole is blocked
Gibbs sampling):</p>
<pre><code>u~p(u|x,y,z)
x,y,z~p(x,y,z|u)
=p(x|u)p(y|u)p(z|u)</code></pre></li>
<li><p>The three conditional probabilities concerning x, y, and z can be
computed in parallel.</p></li>
<li><p>Now we prove that Gibbs is a special case of Metropolis-Hastings
with an acceptance rate of 1, let's first look at the acceptance rate of
Metropolis-Hastings</p>
<p><span class="math display">\[
\alpha = min(1,\frac{\pi (x^{&#39;}),q(x| x^{&#39;})}{\pi (x)
q(x^{&#39;} | x)})
\]</span></p></li>
<li><p>In Gibbs</p>
<p><span class="math display">\[
q(x|x^{&#39;})=\pi (x_i | x_{¬i}^{&#39;}) \\
q(x^{&#39;}|x)=\pi (x_i ^{&#39;} | x_{¬i}) \\
\]</span></p></li>
<li><p>And in fact, from <span class="math inline">\(x_{¬i}\)</span> to
<span class="math inline">\(x_{¬i}^{&#39;}\)</span> , only the ith
component changes, with the other components remaining unchanged,
therefore</p>
<p><span class="math display">\[
x_{¬i}^{&#39;}=x_{¬i}
\]</span></p></li>
<li><p>Next, let's examine Gibbs' acceptance rate</p>
<p><span class="math display">\[
\alpha _{gibbs} =  min(1,\frac{\pi (x^{&#39;}) \pi (x_i |
x_{¬i}^{&#39;})}{\pi (x) (x_i ^{&#39;} | x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;}) \pi (x_i | x_{¬i})}{\pi (x) (x_i ^{&#39;}
| x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;} |  x_{¬i}^{&#39;}) \pi( x_{¬i}^{&#39;}) \pi
(x_i | x_{¬i})}{\pi (x_i | x_{¬i}) \pi( x_{¬i}) (x_i ^{&#39;} |
x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;} |  x_{¬i}) \pi( x_{¬i}) \pi (x_i |
x_{¬i})}{\pi (x_i | x_{¬i}) \pi( x_{¬i}) (x_i ^{&#39;} | x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,1) \\
\]</span></p>
<p><span class="math display">\[
= 1 \\
\]</span></p></li>
</ul>
<h1 id="expectation-maximization">Expectation Maximization</h1>
<h2 id="update">Update</h2>
<ul>
<li>deep|bayes2018 mentions using stochastic gradient descent for the
M-step in EM, as it is stochastic, the E-step only targets a portion of
the data, reducing overhead, and enabling inference of latent variable
models on large-scale datasets. At the time, it was applied to word2vec,
adding a qualitative latent variable for each word to indicate one of
its multiple meanings, aiming to resolve ambiguity issues, and even
parameterize the number of word meanings using the Chinese restaurant
process. Will look at it in detail when I have time.</li>
</ul>
<h2 id="formula">Formula</h2>
<ul>
<li>For simple distributions, we want to perform parameter inference,
which only requires maximum likelihood estimation, first calculating the
log-likelihood:</li>
</ul>
<p><span class="math display">\[
\theta=\mathop{argmax}_{\theta} L(X | \theta) \\
=\mathop{argmax}_{\theta} \log \prod p(x_i | \theta) \\
=\mathop{argmax}_{\theta} \sum \log p(x_i | \theta) \\
\]</span></p>
<ul>
<li><p>Afterward, differentiate the log-likelihood to calculate the
extrema; however, for complex distributions, it may not be convenient to
differentiate</p></li>
<li><p>We can use the EM algorithm to iteratively solve this. The EM
algorithm considers the latent variables in the probabilistic generative
model and assigns probabilities to them, updating their probability
distribution and the parameter <span
class="math inline">\(\theta\)</span> simultaneously with each
iteration. It can be proven that after each iteration, the obtained
<span class="math inline">\(\theta\)</span> will increase the
log-likelihood.</p></li>
<li><p>Each iteration is divided into two parts, E and M, which
correspond to seeking the expectation and maximization</p>
<ul>
<li>The expectation is the expectation of <span
class="math inline">\(\log p(x,z|\theta)\)</span> over the distribution
<span class="math inline">\(p(z|x,\theta ^{(t)})\)</span> , where <span
class="math inline">\(\theta ^{(t)}\)</span> is the parameter calculated
at the t-th iteration</li>
<li>Maximization, that is, seeking the <span
class="math inline">\(\theta\)</span> that maximizes this expectation,
as the result of the parameter update in this iteration</li>
</ul></li>
<li><p>The formula for the EM algorithm is obtained when combined:</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \int p(z|x,\theta ^{(t)})
\log p(x,z|\theta) dz
\]</span></p>
<h2 id="why-effective">Why Effective</h2></li>
<li><p>That is to prove, the maximum likelihood will increase after each
iteration</p></li>
<li><p>To prove:</p>
<p><span class="math display">\[
\log p(x|\theta ^{(t+1)}) \geq \log p(x|\theta ^{(t)})
\]</span></p></li>
<li><p>Reformulate the log-likelihood</p>
<p><span class="math display">\[
\log p(x|\theta) = \log p(x,z|\theta) - \log p(z|x,\theta) \\
\]</span></p></li>
<li><p>Both sides of the distribution <span
class="math inline">\(p(z|x,\theta ^{(t)})\)</span> calculate the
expectation, noting that the left side of the equation is independent of
z, therefore, after calculating the expectation, it remains
unchanged:</p>
<p><span class="math display">\[
\log p(x|\theta) = \int _z \log p(x,z|\theta) p(z|x,\theta ^{(t)}) dz -
\int _z \log p(z|x,\theta) p(z|x,\theta ^{(t)}) dz \\
=Q(\theta,\theta ^{(t)})-H(\theta,\theta ^{(t)}) \\
\]</span></p></li>
<li><p>The Q part is the E part of the EM algorithm, note that here
<span class="math inline">\(\theta\)</span> is a variable, <span
class="math inline">\(\theta ^{(t)}\)</span> is a constant</p></li>
<li><p>After the iteration, due to the role of the M part in the EM
algorithm, the Q part must have increased (greater than or equal to),
then what will the new <span class="math inline">\(\theta\)</span> after
this iteration that makes the Q part increase change when substituted
into the H part?</p></li>
<li><p>We first calculate, assuming that the <span
class="math inline">\(\theta\)</span> of section H remains unchanged,
directly using the previous <span class="math inline">\(\theta
^{(t)}\)</span> to input, that is, <span class="math inline">\(H(\theta
^{(t)},\theta ^{(t)})\)</span></p>
<p><span class="math display">\[
H(\theta ^{(t)},\theta ^{(t)})-H(\theta,\theta ^{(t)})= \\
\]</span></p>
<p><span class="math display">\[
\int _z \log p(z|x,\theta ^{(t)}) p(z|x,\theta ^{(t)}) dz - \int _z \log
p(z|x,\theta) p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= \int _z \log (\frac {p(z|x,\theta ^{(t)})} {p(z|x,\theta)} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= - \int _z \log (\frac {p(z|x,\theta)} {p(z|x,\theta ^{(t)})} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
\geq - \log \int _z  (\frac {p(z|x,\theta)} {p(z|x,\theta ^{(t)})} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= - \log 1 \\
\]</span></p>
<p><span class="math display">\[
= 0 \\
\]</span></p></li>
<li><p>The inequality in question utilizes the Jensen inequality. That
is, directly using the previous <span class="math inline">\(\theta
^{(t)}\)</span> as <span class="math inline">\(\theta\)</span> to
substitute into H is the maximum value of H! Then, regardless of how
much <span class="math inline">\(\theta ^{(t+1)}\)</span> is obtained
from the new argmax Q part, substituting it into H will cause the H part
to decrease (less than or equal to) ! The numerator becomes larger, and
the denominator smaller, so the result is that the log-likelihood is
definitely larger, which proves the effectiveness of the EM
algorithm.</p></li>
</ul>
<h2
id="understanding-from-the-perspective-of-the-evidence-lower-bound-elbo">Understanding
from the perspective of the Evidence Lower Bound (ELBO)</h2>
<ul>
<li><p>We can also derive the formula for the EM algorithm from the
perspective of ELBO (Evidence Lower Bound)</p></li>
<li><p>In the previous rewriting of the log-likelihood, we obtained two
expressions <span class="math inline">\(p(x,z|\theta)\)</span> and <span
class="math inline">\(p(z|x,\theta)\)</span> . We introduce a
distribution <span class="math inline">\(q(z)\)</span> of latent
variables, and by computing the KL divergence between these two
expressions and <span class="math inline">\(q(z)\)</span> , we can prove
that the log-likelihood is the difference between these two KL
divergences:</p>
<p><span class="math display">\[
KL(q(z)||p(z|x,\theta)) = \int q(z) [\log q(z) - \log p(z|x,\theta)] dz
\\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x|z,\theta) - \log (z|\theta) + \log
p(x|\theta)] dz \\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x|z,\theta) - \log (z|\theta)] dz + \log
p(x|\theta) \\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x,z|\theta)] dz + \log p(x|\theta) \\
\]</span></p>
<p><span class="math display">\[
= KL(q(z)||p(x,z|\theta)) + \log p(x|\theta) \\
\]</span></p></li>
<li><p>That is to say</p>
<p><span class="math display">\[
\log p(x|\theta) = - KL(q(z)||p(x,z|\theta)) + KL(q(z)||p(z|x,\theta))
\]</span></p></li>
<li><p>ELBO is the evidence lower bound, because of <span
class="math inline">\(KL(q(z)||p(z|x,\theta)) \geq 0\)</span> , thus
ELBO is a lower bound for the log-likelihood. We can maximize this lower
bound to maximize the log-likelihood.</p></li>
<li><p>It can be seen that the ELBO has two parameters, <span
class="math inline">\(q\)</span> and <span
class="math inline">\(\theta\)</span> . First, we fix <span
class="math inline">\(\theta ^{(t-1)}\)</span> , and find the <span
class="math inline">\(q^{(t)}\)</span> that maximizes the ELBO, which is
actually the E-step of the EM algorithm. Next, we fix <span
class="math inline">\(q^{(t)}\)</span> , and find the <span
class="math inline">\(\theta ^{(t)}\)</span> that maximizes the ELBO,
which corresponds to the M-step of the EM algorithm</p></li>
<li><p>We substitute <span class="math inline">\(\theta = \theta
^{(t-1)}\)</span> into the ELBO expression:</p>
<p><span class="math display">\[
ELBO=\log p(x|\theta ^{(t-1)}) - KL(q(z)||p(z|x,\theta ^{(t-1)}))
\]</span></p></li>
<li><p>What value of q maximizes the ELBO? It is obvious that when the
KL divergence is 0, the ELBO reaches its maximum value, which is when
the lower bound reaches the logarithmic likelihood itself, at which
point <span class="math inline">\(q(z)=p(z|x,\theta ^{(t-1)})\)</span> ,
next we fix <span class="math inline">\(q\)</span> , and seek the value
of <span class="math inline">\(\theta\)</span> that maximizes the ELBO,
first rewriting the definition of ELBO:</p>
<p><span class="math display">\[
ELBO = - KL(q(z)||p(x,z|\theta)) \\
\]</span></p>
<p><span class="math display">\[
= \int q^{(t)}(z) [ \log p(x,z|\theta) - \log q^{(t)}(z)] dz \\
\]</span></p>
<p><span class="math display">\[
= - \int q^{(t)}(z) \log p(x,z|\theta) - q^{(t)}(z) \log q^{(t)}(z) dz
\\
\]</span></p></li>
<li><p>The second item is unrelated to <span
class="math inline">\(\theta\)</span> , therefore:</p>
<p><span class="math display">\[
\theta ^{(t)} = \mathop{argmax} _{\theta} \int q^{(t)}(z) \log
p(x,z|\theta) dz \\
\]</span></p></li>
<li><p>Substitute the <span class="math inline">\(q(z)=p(z|x,\theta
^{(t-1)})\)</span> obtained in the previous step, and we get</p>
<p><span class="math display">\[
\theta ^{(t)} = \mathop{argmax} _{\theta} \int \log
p(x,z|\theta)p(z|x,\theta ^{(t-1)}) dz
\]</span></p></li>
<li><p>Similarly, the iterative formula of the EM algorithm is
obtained</p></li>
<li><p>The following two figures are extracted from Christopher M.
Bishop's Pattern Recognition and Machine Learning, illustrating what the
E-step and M-step actually do: The E-step raises the lower bound ELBO to
the log-likelihood, but at this point only the latent variables are
updated, so the log-likelihood does not change. When the updated latent
variables are used to update the parameters <span
class="math inline">\(\theta\)</span> , i.e., after the M-step is
executed, we continue to obtain a higher ELBO and its corresponding
log-likelihood. At this time, q does not change, but p changes, so KL is
not 0, and the log-likelihood must be greater than the ELBO, i.e., it
will increase. Intuitively, we increase the ELBO in both the E and M
steps; the E-step first raises the ELBO to the log-likelihood in one go,
and then the M-step can still increase the ELBO, but the log-likelihood
will definitely be greater than or equal to (in fact, greater than) the
ELBO at the M-step, so the log-likelihood is "pushed up" by the ELBO
increased by the M-step. <img data-src="https://s1.ax1x.com/2018/10/20/i0oZfP.png" alt="i0oZfP.png" /> <img data-src="https://s1.ax1x.com/2018/10/20/i0ou6S.png"
alt="i0ou6S.png" /></p></li>
<li><p>The remaining issue is how to select z and q; in the mixed model,
z can be introduced as an indicator function, while the other
probability models containing latent variables can directly introduce
the latent variables during design</p></li>
</ul>
<h2
id="from-the-perspective-of-assuming-latent-variables-to-be-observable">From
the perspective of assuming latent variables to be observable</h2>
<ul>
<li>This understanding comes from the tutorial by Chuong B Do &amp;
Serafim Batzoglou: What is the Expectation Maximization Algorithm?</li>
<li>EM is used for inference in probabilistic models with unobserved
latent variables. In fact, if we make the latent variables observable
from unobserved, and perform maximum likelihood estimation for each
possible value of the latent variables, we can still obtain results, but
the time cost is quite high.</li>
<li>EM then improves this naive algorithm. One understanding of the EM
algorithm is: The EM algorithm first guesses a probability distribution
of the hidden variables in each iteration, creates a weighted training
set considering all possible values of the hidden variables, and then
performs a modified version of maximum likelihood estimation on it.</li>
<li>Guessing the probability distribution of a hidden variable is the
E-step, but we do not need to know the specific probability
distribution; we only need to calculate the expectation of the
sufficient statistic on this distribution.</li>
<li>The EM algorithm is a natural generalization of maximum likelihood
estimation to data containing hidden variables (or data containing
partially unobserved samples).</li>
</ul>
<h2
id="from-the-perspective-of-missing-values-in-the-latent-variables">From
the perspective of missing values in the latent variables</h2>
<ul>
<li>How are missing values generally handled? Replaced with random
values, mean values, 0 values, cluster center values, etc</li>
<li>EM is equivalent to replacing missing values with the mean, i.e.,
the latent variable, but it utilizes more information: this mean is
obtained by taking the expectation over the known distribution of x</li>
<li>The EM iteration involves repeatedly processing missing values
(latent variables), then adjusting the distribution of x based on the
complete data, and finally treating the latent variables as missing
values for adjustment</li>
</ul>
<h2 id="em-algorithm-and-k-means">EM algorithm and K-means</h2>
<ul>
<li>K-means is a Hard-EM algorithm that, like the EM algorithm, makes
assumptions about various possible latent variables (the class to which
the sample belongs), but it does not calculate probabilities and
expectations on the class level. Instead, it is more rigid, specifying
only one class as the sample's class, with a probability of 1 for this
class and 0 for all others.</li>
</ul>
<h2 id="benefits-of-introducing-latent-variables">Benefits of
Introducing Latent Variables</h2>
<ul>
<li><p>In fact, it should be said the other way around: many times, we
design latent variables based on logic and then use the EM algorithm to
infer the latent variables, rather than deliberately designing latent
variables to simplify computation.</p></li>
<li><p>For GMM, one advantage of introducing latent variables is that it
simplifies the computation of maximum likelihood estimation (of course,
this is under the assumption that we know the latent variables), by
exchanging the logarithm with the summation operation, referring to the
blog of the great pluskid: On Clustering (Extra Chapter): Expectation
Maximization</p></li>
<li><p>Before introducing latent variables as indicator functions for
GMM, the maximum likelihood estimation is:</p>
<p><span class="math display">\[
\sum _{i=1}^N \log (\sum _{k=1}^K \pi _k N(x_i | \mu _k , \Sigma _k))
\]</span></p></li>
<li><p>After introducing latent variables, the indicator function
corresponding to the ith sample <span class="math inline">\(x_i\)</span>
is <span class="math inline">\(z_i\)</span> , which is a k-dimensional
one-hot vector representing which of the k Gaussian models the ith
sample belongs to. If it belongs to the mth model, then <span
class="math inline">\(z_i^m\)</span> equals 1, and the rest are 0. Now,
the maximum likelihood estimation is:</p>
<p><span class="math display">\[
\log \prod _{i=1}^N p(x_i,z_i) \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N p(z_i) \prod _{k=1}^K N(x_i | \mu _k , \Sigma
_k)^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N  \prod _{k=1}^K \pi _k ^{z_i^k} \prod _{k=1}^K
N(x_i | \mu _k , \Sigma _k)^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N  \prod _{k=1}^K ( \pi _k N(x_i | \mu _k , \Sigma
_k)) ^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \sum _{i=1}^N \sum _{k=1}^K z_i^k(\log \pi _k + \log N(x_i | \mu _k ,
\Sigma _k)) \\
\]</span></p></li>
</ul>
<h2
id="application-of-monte-carlo-method-in-the-em-algorithm">Application
of Monte Carlo Method in the EM Algorithm</h2>
<ul>
<li><p>When the E-step cannot parse the computation, the integral of the
M-step can be approximated using Monte Carlo methods:</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \int p(z|x,\theta ^{(t)})
\log p(x,z|\theta) dz
\]</span></p></li>
<li><p>We sample a finite number of <span
class="math inline">\(Z^l\)</span> based on the posterior estimate <span
class="math inline">\(p(z|x,\theta ^{(t)})\)</span> of the latent
variables obtained now, and then substitute these <span
class="math inline">\(Z^l\)</span> into <span class="math inline">\(\log
p(x,z|\theta)\)</span> to approximate the integral:</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \approx \frac 1L
\sum_{l=1}^L  \log p(x,Z^l|\theta)
\]</span></p></li>
<li><p>An extreme example of the Monte Carlo EM algorithm is the random
EM algorithm, which is equivalent to sampling only one sample point in
the E-step at each iteration. In the solution of mixed models, the
latent variables act as indicator functions, and sampling only one
latent variable implies hard assignment, with each sample point assigned
to a component with a probability of 1.</p></li>
<li><p>Monte Carlo EM algorithm extended to the Bayesian framework
results in the IP algorithm</p>
<ul>
<li><p>I steps:</p>
<p><span class="math display">\[
p(Z|X)=\int p(Z | \theta ,X)p(\theta | X)d\theta
\]</span></p>
<p>Sample from <span class="math inline">\(p(\theta | X)\)</span> , then
substitute into it, and subsequently sample from <span
class="math inline">\(p(Z | \theta ^l ,X)\)</span> into <span
class="math inline">\(Z^l\)</span> .</p></li>
<li><p>P-step: Sampling from the I-step obtained <span
class="math inline">\(Z^l\)</span> for estimating the posterior
parameters:</p>
<p><span class="math display">\[
p(\theta | X) = \int p(\theta | Z,X)p(Z|X) dZ  \\
\approx \frac 1L \sum _{l=1}^L p(\theta | Z^l,X) \\
\]</span></p></li>
</ul></li>
</ul>
<h2 id="generalized-em-algorithm">Generalized EM Algorithm</h2>
<ul>
<li>Will not chicken out</li>
</ul>
<h2 id="wake-sleep-algorithm">Wake-Sleep algorithm</h2>
<ul>
<li>Pigeon Ethics Philosophy</li>
</ul>
<h2 id="generalized-em-algorithm-and-gibbs-sampling">Generalized EM
Algorithm and Gibbs Sampling</h2>
<ul>
<li>When you think I won't chicken out and I do, it's also a form of not
chickening out</li>
</ul>
<h1 id="variational-inference">Variational Inference</h1>
<h2 id="elbo">ELBO</h2>
<ul>
<li><p>Next, we introduce variational inference, and it can be seen that
the EM algorithm can be generalized to variational inference</p></li>
<li><p>Reintroducing the relationship between ELBO and
log-likelihood:</p>
<p><span class="math display">\[
\log p(x) = \log p(x,z) - \log p(z|x) \\
= \log \frac{p(x,z)}{q(z)} - \log \frac{p(z|x)}{q(z)} \\
= \log p(x,z) - \log q(z) - \log \frac{p(z|x)}{q(z)} \\
\]</span></p></li>
<li><p>Seek the expectation of the hidden distribution <span
class="math inline">\(q(z)\)</span> on both sides</p>
<p><span class="math display">\[
\log p(x) = \\
[ \int _z q(z) \log p(x,z)dz - \int _z q(z) \log q(z)dz ] + [- \int _z
\log \frac{p(z|x)}{q(z)} q(z) dz ]\\
= ELBO+KL(q||p(z|x)) \\
\]</span></p></li>
<li><p>We hope to infer the posterior distribution of the latent
variable <span class="math inline">\(z\)</span> , for this purpose, we
introduce a distribution <span class="math inline">\(q(z)\)</span> to
approximate this posterior. Under the premise of the current
observations, i.e., the log-likelihood, the approximation of the
posterior is equivalent to minimizing the KL divergence between <span
class="math inline">\(q(z)\)</span> and <span
class="math inline">\(p(z|x)\)</span> . From the above formula, it can
be seen that when the ELBO is maximized, the KL divergence is
minimized.</p></li>
<li><p>Next is the discussion on how to maximize the ELBO</p></li>
</ul>
<h2 id="variational-inference-on-arbitrary-distributions">Variational
inference on arbitrary distributions</h2>
<ul>
<li><p>For any distribution, update one component of the latent variable
at a time, such as the jth component</p></li>
<li><p>Ourself-selected <span class="math inline">\(q(z)\)</span> is of
course simpler than the approximate distribution; here, it is assumed
that the distribution is independent, and the latent variable is <span
class="math inline">\(M\)</span> -dimensional:</p>
<p><span class="math display">\[
q(z)=\prod _{i=1}^M q_i(z_i)
\]</span></p></li>
<li><p>Therefore, the ELBO can be expressed in two parts</p>
<p><span class="math display">\[
ELBO=\int \prod q_i(z_i) \log p(x,z) dz - \int \prod q_j(z_j) \sum \log
q_j(z_j) dz \\
=part1-part2 \\
\]</span></p></li>
<li><p>The part1 can be expressed in the form of multiple integrals over
the various dimensions of the latent variables, and we select the jth
dimension to rewrite it as</p>
<p><span class="math display">\[
part1=\int \prod q_i(z_i) \log p(x,z) dz \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_1} \int _{z_2} ... \int _{z_M} \prod _{i=1}^M q_i(z_i) \log
p(x,z) d z_1 , d z_2 , ... ,d z_M \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j} q_j(z_j) ( \int _{z_{i \neq j}} \log (p(x,z)) \prod _{z_{i
\neq j}} q_i(z_i) d z_i) d z_j \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j}  q_j(z_j) [E_{i \neq j} [\log (p(x,z))]] d z_j \\
\]</span></p></li>
<li><p>In this context, we define a form of pseudo-distribution, which
is the pseudo-distribution of a distribution, obtained by integrating
the logarithm of the distribution and then exponentiating the
result:</p>
<p><span class="math display">\[
p_j(z_j) = \int _{i \neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\
\]</span></p>
<p><span class="math display">\[
p_j^{&#39;}(z_j) = exp \int _{i \neq j} \log p(z_1,...,z_i) d z_1 , d
z_2 ,..., d z_i \\
\]</span></p>
<p><span class="math display">\[
\log p_j^{&#39;}(z_j)  = \int _{i \neq j} \log p(z_1,...,z_i) d z_1 , d
z_2 ,..., d z_i \\
\]</span></p></li>
<li><p>This part 1 can be rewritten in the form of
pseudo-distribution</p>
<p><span class="math display">\[
part1= \int _{z_j} q_j(z_j) \log p_j^{&#39;}(x,z_j) \\
\]</span></p></li>
<li><p>In part 2, because the components of the latent variables are
independent, the sum of the function can be rewritten as the sum of the
expectations of each function over the marginal distributions, in which
we focus on the j-th variable, treating the rest as constants:</p>
<p><span class="math display">\[
part2=\int \prod q_j(z_j) \sum \log q_j(z_j) dz \\
\]</span></p>
<p><span class="math display">\[
= \sum ( \int q_i(z_i) \log (q_i(z_i)) d z_i ) \\
\]</span></p>
<p><span class="math display">\[
= \int q_j(z_j) \log (q_j(z_j)) d z_j + const \\
\]</span></p></li>
<li><p>Combine part 1 and part 2 to obtain the form of the ELBO for
component j:</p>
<p><span class="math display">\[
ELBO = \int _{z_j} \log \log p_j^{&#39;}(x,z_j) -  \int q_j(z_j) \log
(q_j(z_j)) d z_j + const \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j} q_j(z_j) \log \frac{p_j^{&#39;}(x,z_j)}{q_j(z_j)} + const
\\
\]</span></p>
<p><span class="math display">\[
= - KL(p_j^{&#39;}(x,z_j) || q_j(z_j)) + const\\
\]</span></p></li>
<li><p>The ELBO is written as the negative KL divergence between a
pseudo-distribution and an approximate distribution, maximizing the ELBO
is equivalent to minimizing this KL divergence</p></li>
<li><p>When is this KL divergence minimum? That is to say:</p>
<p><span class="math display">\[
q_j(z_j) = p_j^{&#39;}(x,z_j) \\
\log q_j(z_j) = E_{i \neq j} [\log (p(x,z))] \\
\]</span></p></li>
<li><p>We have obtained the iterative formula for the approximate
distribution of a single component of the latent variables under
variational inference. When calculating the probability of the jth
component, the expectation over all other components <span
class="math inline">\(q_i(z_i)\)</span> is used, and then this new
probability of the jth component participates in the next iteration,
calculating the probabilities of the other components.</p></li>
</ul>
<h2 id="exponential-family-distribution">Exponential family
distribution</h2>
<ul>
<li><p>Define the exponential family distribution:</p>
<p><span class="math display">\[
p(x | \theta)=h(x) exp(\eta (\theta) \cdot T(x)-A(\theta)) \\
\]</span></p></li>
<li><p>Amongst</p>
<ul>
<li>sufficient statistics</li>
<li><span class="math inline">\(\theta\)</span>:parameter of the
family</li>
<li><span class="math inline">\(\eta\)</span>:natural parameter</li>
<li>underlying measure</li>
<li><span class="math inline">\(A(\theta)\)</span> : log normalizer /
partition function</li>
</ul></li>
<li><p>Attention: The parameter of the family and the natural parameter
are both vectors. When the exponential family distribution is in the
form of scalar parameters, i.e., <span class="math inline">\(\eta _i
(\theta) = \theta _i\)</span> , the exponential family distribution can
be written as:</p>
<p><span class="math display">\[
p(x | \eta)=h(x) exp(\eta (T(x) ^T \eta - A(\eta))
\]</span></p></li>
<li><p>When we express the probability density function in the
exponential family form, we have:</p>
<p><span class="math display">\[
\eta = \mathop{argmax} _ {\eta} [\log p(X | \eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} [\log \prod p(x_i | \eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} [\log [\prod h(x_i) exp [(\sum T(x_i))^T \eta
- n A(\eta)]]] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} (\sum T(x_i))^T \eta - n A(\eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} L(\eta) \\
\]</span></p></li>
<li><p>Continuing to seek extrema, we can obtain a very important
property of the exponential family distribution regarding the log
normalizer and sufficient statistics:</p>
<p><span class="math display">\[
\frac{\partial L (\eta)}{\partial \eta} = \sum T(x_i) - n
A^{&#39;}(\eta) =0 \\
\]</span></p>
<p><span class="math display">\[
A^{&#39;}(\eta) = \sum \frac{T(x_i)}{n} \\
\]</span></p></li>
<li><p>For example, the Gaussian distribution is written in the form of
an exponential family distribution:</p>
<p><span class="math display">\[
p(x) = exp[- \frac{1}{2 \sigma ^2}x^2 + \frac{\mu}{\sigma ^2}x -
\frac{\mu ^2}{2 \sigma ^2} - \frac 12 \log(2 \pi \sigma ^2)] \\
\]</span></p>
<p><span class="math display">\[
=exp ( [x \ x^2] [\frac{\mu}{\sigma ^2} \ \frac{-1}{2 \sigma ^2}] ^T -
\frac{\mu ^2}{2 \sigma ^2} - \frac 12 \log(2 \pi \sigma ^2) )
\]</span></p></li>
<li><p>Using natural parameters to replace variance and mean, expressed
in the exponential family distribution form:</p>
<p><span class="math display">\[
p(x) = exp( [x \ x^2] [ \eta _1 \ \eta _2] ^T + \frac{\eta _1 ^2}{4 \eta
_2} + \frac 12 \log (-2 \eta _2 ) - \frac 12 \log (2 \pi))
\]</span></p></li>
<li><p>Wherein:</p>
<ul>
<li><span class="math inline">\(T(x)\)</span>:<span
class="math inline">\([x \ x^2]\)</span></li>
<li><span class="math inline">\(\eta\)</span>:<span
class="math inline">\([ \eta _1 \ \eta _2] ^T\)</span></li>
<li><span class="math inline">\(-A(\eta)\)</span>:<span
class="math inline">\(\frac{\eta _1 ^2}{4 \eta _2} + \frac 12 \log (-2
\eta _2 )\)</span></li>
</ul></li>
<li><p>Next, we utilize the properties of the exponential family to
quickly calculate the mean and variance</p>
<p><span class="math display">\[
A^{&#39;}(\eta) = \sum \frac{T(x_i)}{n} \\
\]</span></p>
<p><span class="math display">\[
[\frac{\partial A}{\eta _1} \ \frac{\partial A}{\eta _2}] = [\frac{-
\eta _1}{2 \eta _2} \ \frac{\eta _1 ^2 }{2 \eta _2}-\frac{1}{2 \eta _2}]
\\
\]</span></p>
<p><span class="math display">\[
= [\frac{\sum x_i}{n} \ \frac{\sum x_i^2}{n}] \\
\]</span></p>
<p><span class="math display">\[
= [\mu \ \mu ^2 + \sigma ^2] \\
\]</span></p></li>
<li><p>Why is <span class="math inline">\(A(\eta)\)</span> called log
normalizer? Because the integral of the exponential family distribution
of the probability density has:</p>
<p><span class="math display">\[
\int _x \frac{h(x)exp(T(x)^T \eta)}{exp(A(\eta))} = 1 \\
\]</span></p>
<p><span class="math display">\[
A(\eta) = \log \int _x h(x)exp(T(x)^T \eta) \\
\]</span></p></li>
<li><p>Below discusses the conjugate relationships of exponential family
distributions, assuming that both the likelihood and the prior are
exponential family distributions:</p>
<p><span class="math display">\[
p(\beta | x) ∝ p(x | \beta) p(\beta) \\
\]</span></p>
<p><span class="math display">\[
∝ h(x) exp(T(x) \beta ^T - A_l (\beta)) h(\beta) exp(T(\beta) \alpha ^T
- A(\alpha)) \\
\]</span></p></li>
<li><p>Rewritten in the form of a vector group:</p>
<p><span class="math display">\[
T(\beta) = [\beta \ -g(\beta)] \\
\]</span></p>
<p><span class="math display">\[
\alpha = [\alpha _1 \ \alpha _2] \\
\]</span></p></li>
<li><p>In the original expression, <span
class="math inline">\(\beta\)</span> , <span
class="math inline">\(h(x)\)</span> , and <span
class="math inline">\(A(\alpha)\)</span> are all constants, which are
eliminated from the proportional expression and then substituted into
the vector group:</p>
<p><span class="math display">\[
∝ h(\beta) exp(T(x) \beta - A_l(\beta) + \alpha _1 \beta - \alpha _2
g(\beta)) \\
\]</span></p></li>
<li><p>We note that if we let <span class="math inline">\(-g(\beta)=-A_l
(\beta)\)</span> , the original expression can be written as:</p>
<p><span class="math display">\[
∝ h(\beta) exp((T(x)+\alpha _1)\beta - (1+\alpha _2) A_l (\beta)) \\
\]</span></p>
<p><span class="math display">\[
∝ h(\beta) exp(\alpha _1 ^{&#39;} \beta - \alpha _2 ^{&#39;} A_l
(\beta)) \\
\]</span></p></li>
<li><p>The prior and posterior forms are consistent, that is,
conjugate</p></li>
<li><p>We thus write down the likelihood and prior in a unified form</p>
<p><span class="math display">\[
p(\beta | x, \alpha) ∝ p(x | \beta) p(\beta | \alpha) \\
\]</span></p>
<p><span class="math display">\[
∝ h(x)exp[T(x)^T\beta - A_l(\beta)] h(\beta) exp[T(\beta)^T\alpha -
A_l(\alpha)] \\
\]</span></p></li>
<li><p>Here we can calculate the derivative of the log normalizer with
respect to the parameters, note that this is a calculated result,
different from the properties of the log normalizer and sufficient
statistics obtained from the maximum likelihood estimation of the
exponential family distribution</p>
<p><span class="math display">\[
\frac{\partial A_l(\beta)}{\partial \beta}=\int _x T(x) p(x | \beta)dx
\\
\]</span></p>
<p><span class="math display">\[
= E_{p(x|\beta)} [T(x)] \\
\]</span></p></li>
<li><p>The above equation can be proven by integrating over the
exponential family distribution with the integral equal to 1, and taking
the derivative with respect to <span
class="math inline">\(\beta\)</span> yields 0, transforming this
equation to prove it.</p></li>
</ul>
<h2
id="variational-inference-under-exponential-family-distributions">Variational
Inference under Exponential Family Distributions</h2>
<ul>
<li><p>Next, we will express the parameter posterior in the ELBO in the
form of an exponential family distribution, and it can be seen that the
final iteration formula is quite concise</p></li>
<li><p>We assume that there are two parameters to be optimized, x and z,
and we use <span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(\phi\)</span> to approximate <span
class="math inline">\(\eta(z,x)\)</span> and <span
class="math inline">\(\eta(\beta ,x)\)</span> . The goal remains to
maximize the ELBO, at which point the adjusted parameter is <span
class="math inline">\(q(\lambda , \phi)\)</span> , which is actually
<span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(\phi\)</span></p></li>
<li><p>We adopt a method of fixing one parameter and optimizing another,
iteratively making the ELBO larger</p></li>
<li><p>First, we rewrite the ELBO, noting <span
class="math inline">\(q(z,\beta)=q(z)q(\beta)\)</span></p>
<p><span class="math display">\[
ELBO=E_{q(z,\beta)}[\log p(x,z,\beta)] - E_{q(z,\beta)}[\log p(z,\beta)]
\\
\]</span></p>
<p><span class="math display">\[
= E_{q(z,\beta)}[\log p(\beta | x,z) + \log p(z | x) + \log p(x)] -
E_{q(z,\beta)}[\log q(\beta)] - E_{q(z,\beta)}[\log q(z)] \\
\]</span></p></li>
<li><p>The posterior is distributed in the exponential family, and the
q-distribution is approximated using simple parameters <span
class="math inline">\(\lambda\)</span> and <span
class="math inline">\(\phi\)</span></p>
<p><span class="math display">\[
p(\beta | x,z) = h(\beta) exp [ T(\beta) ^T \eta (z,x) - A_g
(\eta(z,x))] \\
\]</span></p>
<p><span class="math display">\[
\approx q(\beta | \lambda) \\
\]</span></p>
<p><span class="math display">\[
= h(\beta) exp [ T(\beta) ^T \eta (\lambda - A_g (\eta(\lambda))] \\
\]</span></p>
<p><span class="math display">\[
p(z | x,\beta) = h(z) exp [ T(z) ^T \eta (\beta,x) - A_l
(\eta(\beta,x))] \\
\]</span></p>
<p><span class="math display">\[
\approx q(\beta | \phi) \\
\]</span></p>
<p><span class="math display">\[
= h(z) exp [ T(z) ^T \eta (\phi - A_l (\eta(\phi))] \\
\]</span></p></li>
<li><p>Now we fix <span class="math inline">\(\phi\)</span> , optimize
<span class="math inline">\(\lambda\)</span> , and remove irrelevant
constants from the ELBO, yielding:</p>
<p><span class="math display">\[
ELBO_{\lambda} = E_{q(z,\beta)}[\log p(\beta | x,z)] -
E_{q(z,\beta)}[\log q(\beta)] \\
\]</span></p></li>
<li><p>Substitute the exponential family distribution, eliminate the
irrelevant constant <span class="math inline">\(-
E_{q(z)}[A_g(\eta(x,z))]\)</span> , and simplify to obtain:</p>
<p><span class="math display">\[
ELBO_{\lambda} = E_{q(\beta)}[T(\beta)^T]
E_{q(z)}[\eta(z,x)]  -E_{q(\beta)} [T(\beta)^T \lambda] + A_g(\lambda)
\]</span></p></li>
<li><p>Using the conclusions from the previous log normalizer regarding
parameter differentiation, we have:</p>
<p><span class="math display">\[
ELBO_{\lambda} = A_g^{&#39;}(\lambda)^T[E_{q(z)}[\eta(z,x)]] - \lambda
A_g^{&#39;}(\lambda) ^T + A_g (\lambda)
\]</span></p></li>
<li><p>Differentiate the above equation, set it to 0, and we have:</p>
<p><span class="math display">\[
A_g^{&#39;&#39;}(\lambda)^T[E_{q(z)}[\eta(z,x)]] -
A_g^{&#39;}(\lambda)-\lambda A_g^{&#39;&#39;}(\lambda) ^T + A_g^{}
(\lambda) = 0 \\
\lambda = E_{q(z)}[\eta(z,x)] \\
\]</span></p></li>
<li><p>We have obtained the iterative <span
class="math inline">\(\lambda\)</span> ! Similarly, we can obtain:</p>
<p><span class="math display">\[
\phi = E_{q(\beta)}[\eta(\beta,x)] \\
\]</span></p></li>
<li><p>Should be written as:</p>
<p><span class="math display">\[
\lambda = E_{q(z | \phi)}[\eta(z,x)] \\
\phi = E_{q(\beta | \lambda)}[\eta(\beta,x)] \\
\]</span></p></li>
<li><p>The variable update paths for these two iterative processes
are:</p>
<p><span class="math display">\[
\lambda \rightarrow q(\beta | \lambda) \rightarrow \phi \rightarrow q(z
| \phi) \rightarrow \lambda
\]</span></p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="bayesian-inference">Bayesian Inference</h1>
<ul>
<li>在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）</li>
<li>统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计</li>
<li>在频率学派框架中只关注似然<span
class="math inline">\(p(x|\theta)\)</span>，而贝叶斯学派认为应将参数<span
class="math inline">\(\theta\)</span>作为变量，在观察到数据之前，对参数做出先验假设<span
class="math inline">\(p(\theta)\)</span></li>
<li>后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布</li>
<li>在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和</li>
<li>后验实际上是在最大似然估计和先验之间权衡</li>
<li>当数据非常多时，后验渐渐不再依赖于先验</li>
<li>很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布</li>
<li>有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计</li>
</ul>
<h1 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h1>
<ul>
<li>MCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数</li>
<li>最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布</li>
<li>蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量</li>
<li>蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）</li>
</ul>
<h2 id="采样">采样</h2>
<ul>
<li><p>直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量</p></li>
<li><p>在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数</p></li>
<li><p>最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：</p>
<p><span class="math display">\[
u = U(0,1) \\
x= cdf ^{-1} (u) \\
\]</span></p></li>
</ul>
<h2 id="拒绝采样">拒绝采样</h2>
<ul>
<li><p>但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection
sampling</p></li>
<li><p>对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线
<img data-src="https://s1.ax1x.com/2018/10/20/i0oFwd.jpg"
alt="i0oFwd.jpg" /></p></li>
<li><p>我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接</p></li>
<li><p>显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">i=0</span><br><span class="line">while i!= N</span><br><span class="line">x(i)~q(x) and u~U(0,1)</span><br><span class="line">if u&lt; p(x(i))/Mq(x(i)) then</span><br><span class="line">   accept x(i)</span><br><span class="line">   i=i+1</span><br><span class="line">else</span><br><span class="line">   reject x(i)</span><br><span class="line">end</span><br><span class="line">end</span><br></pre></td></tr></table></figure></li>
<li><p>rejection
sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。</p></li>
</ul>
<h2 id="适应性拒绝采样">适应性拒绝采样</h2>
<ul>
<li>当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高</li>
<li>基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域</li>
<li>但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oEFI.jpg"
alt="i0oEFI.jpg" /></li>
</ul>
<h2 id="重要性采样">重要性采样</h2>
<ul>
<li><p>上面提到的采样算法是从简单分布（提议分布）采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布</p></li>
<li><p>importance
sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。</p></li>
<li><p>例如我们希望通过采样得到某个分布的期望</p>
<p><span class="math display">\[
E_{p(x)}(f(x)) = \int _x f(x)p(x)dx \\
E_{p(x)}(f(x)) = \int _x f(x) \frac{p(x)}{q(x)} q(x) dx \\
E_{p(x)}(f(x)) = \int _x g(x)q(x)dx \\
\]</span></p></li>
<li><p>p(x)难以采样，我们就转化为从q(x)采样。其中<span
class="math inline">\(\frac{p(x)}{q(x)}\)</span>就是importance
weight。</p></li>
<li><p>这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。</p></li>
</ul>
<h2
id="马尔可夫蒙特卡洛和metropolis-hasting算法">马尔可夫蒙特卡洛和Metropolis-Hasting算法</h2>
<ul>
<li><p>mcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关</p></li>
<li><p>不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。</p></li>
<li><p>我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布<span
class="math inline">\(\pi\)</span>就是给定分布，假定转移概率为<span
class="math inline">\(k(x^{&#39;} | x)\)</span>，从样本<span
class="math inline">\(x\)</span>转移到样本<span
class="math inline">\(x^{&#39;}\)</span>。</p></li>
<li><p>在马尔可夫链中，有如下Chapman-Kologronvo等式：</p>
<p><span class="math display">\[
\pi _t (x^{&#39;}) = \int _x \pi _{t-1}(x) k(x^{&#39;} | x) dx
\]</span></p></li>
<li><p>这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：</p>
<p><span class="math display">\[
\pi _t (x) = \pi _{t-1} (x)
\]</span></p></li>
<li><p>实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the
detailed balance：</p>
<p><span class="math display">\[
\pi (x) k(x^{&#39;} | x) = \pi (x^{&#39;}) k(x | x^{&#39;})
\]</span></p></li>
<li><p>由detailed
balance可以推出Chapman-Kologronvo等式，反之不一定。</p></li>
<li><p>当满足细致平稳条件时，马氏链是收敛的</p></li>
<li><p>在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：
<img data-src="https://s1.ax1x.com/2018/10/20/i0okTA.jpg"
alt="i0okTA.jpg" /></p></li>
<li><p>在mh中，我们没有改变转移矩阵来适应给定分布，而是用给定分布来修正转移矩阵，因此，转移矩阵是我们自己设计的。一般将转移矩阵（提议分布）设计为以当前状态为中心的高斯分布，对于这个高斯分布，当方差很小时，概率集中在本次采样点附近，那么转移到下次采样时大概率位置不会变动很多，接受率高（因为本次采样点就是通过了接收得到的，大概率是处于高接受率的位置），但这会造成随机游走缓慢；如果方差很大，到处走，接受率就会降低。</p></li>
<li><p>尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。</p></li>
<li><p>而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。</p></li>
</ul>
<h2 id="hybrid-metropolis-hasting">Hybrid Metropolis-Hasting</h2>
<ul>
<li>待补充</li>
</ul>
<h2 id="吉布斯采样">吉布斯采样</h2>
<ul>
<li><p>吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布</p></li>
<li><p>先看直觉上为啥吉布斯采样通过条件概率迭代抽样的过程中不改变联合概率分布。首先在排除第i个参数计算条件概率时，这被排除的n-1个变量的边缘分布与真实联合概率分布针对这n-1个变量的边缘分布是一样的，因为它们的值没有改变；条件概率依据的条件相比真实分布是不变的，那条件概率分布也是不变的。边缘分布和条件概率分布都是不变（真实）的，那相乘得到的联合分布自然也是不变的，因此每一步迭代里都是按照真实分布采样且迭代不会改变这个分布。</p></li>
<li><p>吉本斯采样是类似变分推断的coordinate
descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：
<img data-src="https://s1.ax1x.com/2018/10/20/i0oVYt.jpg"
alt="i0oVYt.jpg" /></p></li>
<li><p>工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed
gibbs
sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的（存在疑问，另一种关于collapse的说法是忽略一些条件变量，基本的gibbs采样就是collapsed
gibbs sampling，而这种几个分量看成一个整体的做法是blocked gibbs
sampling）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">u~p(u|x,y,z)</span><br><span class="line">x,y,z~p(x,y,z|u)</span><br><span class="line">=p(x|u)p(y|u)p(z|u)</span><br></pre></td></tr></table></figure></li>
<li><p>上面关于x,y,z的三个条件概率可以并行计算。</p></li>
<li><p>现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率</p>
<p><span class="math display">\[
\alpha = min(1,\frac{\pi (x^{&#39;}),q(x| x^{&#39;})}{\pi (x)
q(x^{&#39;} | x)})
\]</span></p></li>
<li><p>在gibbs中</p>
<p><span class="math display">\[
q(x|x^{&#39;})=\pi (x_i | x_{¬i}^{&#39;}) \\
q(x^{&#39;}|x)=\pi (x_i ^{&#39;} | x_{¬i}) \\
\]</span></p></li>
<li><p>而且实际上从<span class="math inline">\(x_{¬i}\)</span>到<span
class="math inline">\(x_{¬i}^{&#39;}\)</span>，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此</p>
<p><span class="math display">\[
x_{¬i}^{&#39;}=x_{¬i}
\]</span></p></li>
<li><p>接下来看看gibbs的接受率</p>
<p><span class="math display">\[
\alpha _{gibbs} =  min(1,\frac{\pi (x^{&#39;}) \pi (x_i |
x_{¬i}^{&#39;})}{\pi (x) (x_i ^{&#39;} | x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;}) \pi (x_i | x_{¬i})}{\pi (x) (x_i ^{&#39;}
| x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;} |  x_{¬i}^{&#39;}) \pi( x_{¬i}^{&#39;}) \pi
(x_i | x_{¬i})}{\pi (x_i | x_{¬i}) \pi( x_{¬i}) (x_i ^{&#39;} |
x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,\frac{\pi (x^{&#39;} |  x_{¬i}) \pi( x_{¬i}) \pi (x_i |
x_{¬i})}{\pi (x_i | x_{¬i}) \pi( x_{¬i}) (x_i ^{&#39;} | x_{¬i})}) \\
\]</span></p>
<p><span class="math display">\[
= min(1,1) \\
\]</span></p>
<p><span class="math display">\[
= 1 \\
\]</span></p></li>
</ul>
<h1 id="expectation-maximization">Expectation Maximization</h1>
<h2 id="更新">更新</h2>
<ul>
<li>看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。</li>
</ul>
<h2 id="公式">公式</h2>
<ul>
<li>对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：</li>
</ul>
<p><span class="math display">\[
\theta=\mathop{argmax}_{\theta} L(X | \theta) \\
=\mathop{argmax}_{\theta} \log \prod p(x_i | \theta) \\
=\mathop{argmax}_{\theta} \sum \log p(x_i | \theta) \\
\]</span></p>
<ul>
<li><p>之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导</p></li>
<li><p>这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数<span
class="math inline">\(\theta\)</span>，可以证明，每一次迭代之后得到的<span
class="math inline">\(\theta\)</span>都会使对数似然增加。</p></li>
<li><p>每一次迭代分为两个部分，E和M，也就求期望和最大化</p>
<ul>
<li>求期望，是求<span class="math inline">\(\log
p(x,z|\theta)\)</span>在分布<span class="math inline">\(p(z|x,\theta
^{(t)})\)</span>上的期望，其中<span class="math inline">\(\theta
^{(t)}\)</span>是第t次迭代时计算出的参数</li>
<li>最大化，也就是求使这个期望最大的<span
class="math inline">\(\theta\)</span>，作为本次参数迭代更新的结果</li>
</ul></li>
<li><p>合起来就得到EM算法的公式：</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \int p(z|x,\theta ^{(t)})
\log p(x,z|\theta) dz
\]</span></p>
<h2 id="为何有效">为何有效</h2></li>
<li><p>也就是证明，每次迭代后最大似然会增加</p></li>
<li><p>要证明：</p>
<p><span class="math display">\[
\log p(x|\theta ^{(t+1)}) \geq \log p(x|\theta ^{(t)})
\]</span></p></li>
<li><p>先改写对数似然</p>
<p><span class="math display">\[
\log p(x|\theta) = \log p(x,z|\theta) - \log p(z|x,\theta) \\
\]</span></p></li>
<li><p>两边对分布<span class="math inline">\(p(z|x,\theta
^{(t)})\)</span>求期望，注意到等式左边与z无关，因此求期望之后不变：</p>
<p><span class="math display">\[
\log p(x|\theta) = \int _z \log p(x,z|\theta) p(z|x,\theta ^{(t)}) dz -
\int _z \log p(z|x,\theta) p(z|x,\theta ^{(t)}) dz \\
=Q(\theta,\theta ^{(t)})-H(\theta,\theta ^{(t)}) \\
\]</span></p></li>
<li><p>其中Q部分就是EM算法中的E部分，注意在这里<span
class="math inline">\(\theta\)</span>是变量，<span
class="math inline">\(\theta ^{(t)}\)</span>是常量</p></li>
<li><p>迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的<span
class="math inline">\(\theta\)</span>，代入H部分，H部分会怎么变化呢？</p></li>
<li><p>我们先计算，假如H部分的<span
class="math inline">\(\theta\)</span>不变，直接用上一次的<span
class="math inline">\(\theta ^{(t)}\)</span>带入，即<span
class="math inline">\(H(\theta ^{(t)},\theta ^{(t)})\)</span></p>
<p><span class="math display">\[
H(\theta ^{(t)},\theta ^{(t)})-H(\theta,\theta ^{(t)})= \\
\]</span></p>
<p><span class="math display">\[
\int _z \log p(z|x,\theta ^{(t)}) p(z|x,\theta ^{(t)}) dz - \int _z \log
p(z|x,\theta) p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= \int _z \log (\frac {p(z|x,\theta ^{(t)})} {p(z|x,\theta)} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= - \int _z \log (\frac {p(z|x,\theta)} {p(z|x,\theta ^{(t)})} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
\geq - \log \int _z  (\frac {p(z|x,\theta)} {p(z|x,\theta ^{(t)})} )
p(z|x,\theta ^{(t)}) dz \\
\]</span></p>
<p><span class="math display">\[
= - \log 1 \\
\]</span></p>
<p><span class="math display">\[
= 0 \\
\]</span></p></li>
<li><p>其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的<span
class="math inline">\(\theta ^{(t)}\)</span>作为<span
class="math inline">\(\theta\)</span>代入H，就是H的最大值!那么无论新的由argmax
Q部分得到的<span class="math inline">\(\theta
^{(t+1)}\)</span>是多少，带入
H,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性</p></li>
</ul>
<h2 id="从elbo的角度理解">从ELBO的角度理解</h2>
<ul>
<li><p>我们还可以从ELBO（Evidence Lower
Bound）的角度推出EM算法的公式</p></li>
<li><p>在之前改写对数似然时我们得到了两个式子<span
class="math inline">\(p(x,z|\theta)\)</span>和<span
class="math inline">\(p(z|x,\theta)\)</span>，我们引入隐变量的一个分布<span
class="math inline">\(q(z)\)</span>，对这个两个式子做其与<span
class="math inline">\(q(z)\)</span>之间的KL散度，可以证明对数似然是这两个KL散度之差：</p>
<p><span class="math display">\[
KL(q(z)||p(z|x,\theta)) = \int q(z) [\log q(z) - \log p(z|x,\theta)] dz
\\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x|z,\theta) - \log (z|\theta) + \log
p(x|\theta)] dz \\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x|z,\theta) - \log (z|\theta)] dz + \log
p(x|\theta) \\
\]</span></p>
<p><span class="math display">\[
= \int q(z) [\log q(z) - \log p(x,z|\theta)] dz + \log p(x|\theta) \\
\]</span></p>
<p><span class="math display">\[
= KL(q(z)||p(x,z|\theta)) + \log p(x|\theta) \\
\]</span></p></li>
<li><p>也就是</p>
<p><span class="math display">\[
\log p(x|\theta) = - KL(q(z)||p(x,z|\theta)) + KL(q(z)||p(z|x,\theta))
\]</span></p></li>
<li><p>其中<span class="math inline">\(-
KL(q(z)||p(x,z|\theta))\)</span>就是ELBO，因为$ KL(q(z)||p(z|x,))
$，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然</p></li>
<li><p>可以看到，ELBO有两个参数，<span
class="math inline">\(q\)</span>和<span
class="math inline">\(\theta\)</span>，首先我们固定<span
class="math inline">\(\theta ^{(t-1)}\)</span>，找到使ELBO最大化的<span
class="math inline">\(q^{(t)}\)</span>，这一步实际上是EM算法的E步骤，接下来固定<span
class="math inline">\(q^{(t)}\)</span>，找到使ELBO最大化的<span
class="math inline">\(\theta
^{(t)}\)</span>，这一步对应的就是EM算法的M步骤</p></li>
<li><p>我们把<span class="math inline">\(\theta = \theta
^{(t-1)}\)</span>带入ELBO的表达式：</p>
<p><span class="math display">\[
ELBO=\log p(x|\theta ^{(t-1)}) - KL(q(z)||p(z|x,\theta ^{(t-1)}))
\]</span></p></li>
<li><p>q取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时<span
class="math inline">\(q(z)=p(z|x,\theta
^{(t-1)})\)</span>，接下来我们固定<span
class="math inline">\(q\)</span>，求使ELBO最大的<span
class="math inline">\(\theta\)</span>，先把ELBO的定义式改写：</p>
<p><span class="math display">\[
ELBO = - KL(q(z)||p(x,z|\theta)) \\
\]</span></p>
<p><span class="math display">\[
= \int q^{(t)}(z) [ \log p(x,z|\theta) - \log q^{(t)}(z)] dz \\
\]</span></p>
<p><span class="math display">\[
= - \int q^{(t)}(z) \log p(x,z|\theta) - q^{(t)}(z) \log q^{(t)}(z) dz
\\
\]</span></p></li>
<li><p>其中第二项与<span
class="math inline">\(\theta\)</span>无关，因此：</p>
<p><span class="math display">\[
\theta ^{(t)} = \mathop{argmax} _{\theta} \int q^{(t)}(z) \log
p(x,z|\theta) dz \\
\]</span></p></li>
<li><p>代入上一步得到的<span class="math inline">\(q(z)=p(z|x,\theta
^{(t-1)})\)</span>，得到</p>
<p><span class="math display">\[
\theta ^{(t)} = \mathop{argmax} _{\theta} \int \log
p(x,z|\theta)p(z|x,\theta ^{(t-1)}) dz
\]</span></p></li>
<li><p>同样得到了EM算法的迭代公式</p></li>
<li><p>下面两张图截取自Christopher M. Bishop的Pattern Recognition and
Machine
Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数<span
class="math inline">\(\theta\)</span>，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oZfP.png" alt="i0oZfP.png" />
<img data-src="https://s1.ax1x.com/2018/10/20/i0ou6S.png"
alt="i0ou6S.png" /></p></li>
<li><p>剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入</p></li>
</ul>
<h2 id="从假设隐变量为可观察的角度">从假设隐变量为可观察的角度</h2>
<ul>
<li>这种理解来自Chuong B Do &amp; Serafim Batzoglou的tutorial:What is
the expectation maximization algorithm?</li>
<li>EM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。</li>
<li>EM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。</li>
<li>猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。</li>
<li>所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。</li>
</ul>
<h2 id="从假设隐变量为缺失值的角度">从假设隐变量为缺失值的角度</h2>
<ul>
<li>一般如何处理缺失值？用随机值、平均值、0值、聚类中心值代替等等</li>
<li>EM相当于用均值代替缺失值，也就是隐变量，但是利用了更多的信息：这个均值是在已知的x分布上求期望得到</li>
<li>EM的迭代就是反复处理缺失值（隐变量），然后基于完整的数据再调整x的分布，再将隐变量看成缺失值进行调整</li>
</ul>
<h2 id="em算法与k-means">EM算法与K-means</h2>
<ul>
<li>K-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。</li>
</ul>
<h2 id="隐变量引入的好处">隐变量引入的好处</h2>
<ul>
<li><p>其实应该反过来说，很多时候我们凭借逻辑设计了隐变量，然后利用EM算法推断隐变量，而不是刻意设计隐变量来简化运算。</p></li>
<li><p>对于GMM来说，引入隐变量的一个好处是化简了最大似然估计的计算（当然这是假设我们已知隐变量的情况下），将log与求和运算交换，参考了pluskid大神的博客：<a
href="http://blog.pluskid.org/?p=81">漫谈 Clustering (番外篇):
Expectation Maximization</a></p></li>
<li><p>对于GMM，引入隐变量作为示性函数之前，最大似然估计是：</p>
<p><span class="math display">\[
\sum _{i=1}^N \log (\sum _{k=1}^K \pi _k N(x_i | \mu _k , \Sigma _k))
\]</span></p></li>
<li><p>引入隐变量之后，令第i个样本<span
class="math inline">\(x_i\)</span>对应的示性函数为<span
class="math inline">\(z_i\)</span>，这是一个k维one-hot向量，代表第i个样本属于k个高斯模型中哪一个，假设属于第m个模型，则<span
class="math inline">\(z_i^m\)</span>等于1，其余等于0。现在最大似然估计是：</p>
<p><span class="math display">\[
\log \prod _{i=1}^N p(x_i,z_i) \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N p(z_i) \prod _{k=1}^K N(x_i | \mu _k , \Sigma
_k)^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N  \prod _{k=1}^K \pi _k ^{z_i^k} \prod _{k=1}^K
N(x_i | \mu _k , \Sigma _k)^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \log \prod _{i=1}^N  \prod _{k=1}^K ( \pi _k N(x_i | \mu _k , \Sigma
_k)) ^{z_i^k} \\
\]</span></p>
<p><span class="math display">\[
= \sum _{i=1}^N \sum _{k=1}^K z_i^k(\log \pi _k + \log N(x_i | \mu _k ,
\Sigma _k)) \\
\]</span></p></li>
</ul>
<h2 id="在em算法中应用蒙特卡罗方法">在EM算法中应用蒙特卡罗方法</h2>
<ul>
<li><p>当E步骤无法解析的计算时，可以使用蒙特卡洛近似M步骤的积分：</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \int p(z|x,\theta ^{(t)})
\log p(x,z|\theta) dz
\]</span></p></li>
<li><p>我们根据现在得到的隐变量后验估计<span
class="math inline">\(p(z|x,\theta ^{(t)})\)</span>来采样有限个<span
class="math inline">\(Z^l\)</span>，之后将这些<span
class="math inline">\(Z^l\)</span>代入<span class="math inline">\(\log
p(x,z|\theta)\)</span>来近似积分：</p>
<p><span class="math display">\[
\theta ^{(t+1)} = \mathop{argmax} _{\theta} \approx \frac 1L
\sum_{l=1}^L  \log p(x,Z^l|\theta)
\]</span></p></li>
<li><p>蒙特卡洛EM算法的一个极端的例子是随机EM算法，相当于每次迭代只在E步骤只采样一个样本点。在混合模型求解中，隐变量作为示性函数，只采样一个隐变量意味着hard
assignment，每个样本点以1概率分配到某个component，</p></li>
<li><p>蒙特卡洛EM算法推广到贝叶斯框架，就得到IP算法</p>
<ul>
<li><p>I步骤：</p>
<p><span class="math display">\[
p(Z|X)=\int p(Z | \theta ,X)p(\theta | X)d\theta
\]</span></p>
<p>先从<span class="math inline">\(p(\theta | X)\)</span>中采样<span
class="math inline">\(\theta ^l\)</span>，再将其代入，接着从<span
class="math inline">\(p(Z | \theta ^l ,X)\)</span>中采样<span
class="math inline">\(Z^l\)</span>。</p></li>
<li><p>P步骤： 从I步骤采样得到的<span
class="math inline">\(Z^l\)</span>用于估计参数后验：</p>
<p><span class="math display">\[
p(\theta | X) = \int p(\theta | Z,X)p(Z|X) dZ  \\
\approx \frac 1L \sum _{l=1}^L p(\theta | Z^l,X) \\
\]</span></p></li>
</ul></li>
</ul>
<h2 id="广义em算法">广义EM算法</h2>
<ul>
<li>不会鸽</li>
</ul>
<h2 id="wake-sleep算法">Wake-Sleep算法</h2>
<ul>
<li>鸽德哲学</li>
</ul>
<h2 id="广义em算法与吉布斯采样">广义EM算法与吉布斯采样</h2>
<ul>
<li>当你认为我不会鸽的时候鸽了，亦是一种不鸽</li>
</ul>
<h1 id="variational-inference">Variational Inference</h1>
<h2 id="elbo">ELBO</h2>
<ul>
<li><p>接下来介绍变分推断，可以看到，EM算法可以推广到变分推断</p></li>
<li><p>重新推出ELBO与对数似然的关系：</p>
<p><span class="math display">\[
\log p(x) = \log p(x,z) - \log p(z|x) \\
= \log \frac{p(x,z)}{q(z)} - \log \frac{p(z|x)}{q(z)} \\
= \log p(x,z) - \log q(z) - \log \frac{p(z|x)}{q(z)} \\
\]</span></p></li>
<li><p>两边对隐分布<span class="math inline">\(q(z)\)</span>求期望</p>
<p><span class="math display">\[
\log p(x) = \\
[ \int _z q(z) \log p(x,z)dz - \int _z q(z) \log q(z)dz ] + [- \int _z
\log \frac{p(z|x)}{q(z)} q(z) dz ]\\
= ELBO+KL(q||p(z|x)) \\
\]</span></p></li>
<li><p>我们希望推断隐变量<span
class="math inline">\(z\)</span>的后验分布<span
class="math inline">\(p(z|x)\)</span>，为此我们引入一个分布<span
class="math inline">\(q(z)\)</span>来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得<span
class="math inline">\(q(z)\)</span>和<span
class="math inline">\(p(z|x)\)</span>的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。</p></li>
<li><p>接下来就是讨论如何使得ELBO最大化</p></li>
</ul>
<h2 id="任意分布上的变分推断">任意分布上的变分推断</h2>
<ul>
<li><p>对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量</p></li>
<li><p>我们自己选取的<span
class="math inline">\(q(z)\)</span>当然要比近似的分布简单，这里假设分布是独立的，隐变量是<span
class="math inline">\(M\)</span>维的：</p>
<p><span class="math display">\[
q(z)=\prod _{i=1}^M q_i(z_i)
\]</span></p></li>
<li><p>因此ELBO可以写成两部分</p>
<p><span class="math display">\[
ELBO=\int \prod q_i(z_i) \log p(x,z) dz - \int \prod q_j(z_j) \sum \log
q_j(z_j) dz \\
=part1-part2 \\
\]</span></p></li>
<li><p>其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成</p>
<p><span class="math display">\[
part1=\int \prod q_i(z_i) \log p(x,z) dz \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_1} \int _{z_2} ... \int _{z_M} \prod _{i=1}^M q_i(z_i) \log
p(x,z) d z_1 , d z_2 , ... ,d z_M \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j} q_j(z_j) ( \int _{z_{i \neq j}} \log (p(x,z)) \prod _{z_{i
\neq j}} q_i(z_i) d z_i) d z_j \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j}  q_j(z_j) [E_{i \neq j} [\log (p(x,z))]] d z_j \\
\]</span></p></li>
<li><p>在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：</p>
<p><span class="math display">\[
p_j(z_j) = \int _{i \neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\
\]</span></p>
<p><span class="math display">\[
p_j^{&#39;}(z_j) = exp \int _{i \neq j} \log p(z_1,...,z_i) d z_1 , d
z_2 ,..., d z_i \\
\]</span></p>
<p><span class="math display">\[
\log p_j^{&#39;}(z_j)  = \int _{i \neq j} \log p(z_1,...,z_i) d z_1 , d
z_2 ,..., d z_i \\
\]</span></p></li>
<li><p>这样part1用伪分布的形式可以改写成</p>
<p><span class="math display">\[
part1= \int _{z_j} q_j(z_j) \log p_j^{&#39;}(x,z_j) \\
\]</span></p></li>
<li><p>part2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：</p>
<p><span class="math display">\[
part2=\int \prod q_j(z_j) \sum \log q_j(z_j) dz \\
\]</span></p>
<p><span class="math display">\[
= \sum ( \int q_i(z_i) \log (q_i(z_i)) d z_i ) \\
\]</span></p>
<p><span class="math display">\[
= \int q_j(z_j) \log (q_j(z_j)) d z_j + const \\
\]</span></p></li>
<li><p>再把part1和part2合起来，得到ELBO关于分量j的形式：</p>
<p><span class="math display">\[
ELBO = \int _{z_j} \log \log p_j^{&#39;}(x,z_j) -  \int q_j(z_j) \log
(q_j(z_j)) d z_j + const \\
\]</span></p>
<p><span class="math display">\[
= \int _{z_j} q_j(z_j) \log \frac{p_j^{&#39;}(x,z_j)}{q_j(z_j)} + const
\\
\]</span></p>
<p><span class="math display">\[
= - KL(p_j^{&#39;}(x,z_j) || q_j(z_j)) + const\\
\]</span></p></li>
<li><p>也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度</p></li>
<li><p>何时这个KL散度最小？也就是：</p>
<p><span class="math display">\[
q_j(z_j) = p_j^{&#39;}(x,z_j) \\
\log q_j(z_j) = E_{i \neq j} [\log (p(x,z))] \\
\]</span></p></li>
<li><p>到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了<span
class="math inline">\(\log (p(x,z))\)</span>在其他所有分量<span
class="math inline">\(q_i(z_i)\)</span>上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。</p></li>
</ul>
<h2 id="指数家族分布">指数家族分布</h2>
<ul>
<li><p>定义指数家族分布：</p>
<p><span class="math display">\[
p(x | \theta)=h(x) exp(\eta (\theta) \cdot T(x)-A(\theta)) \\
\]</span></p></li>
<li><p>其中</p>
<ul>
<li><span class="math inline">\(T(x)\)</span>:sufficient statistics</li>
<li><span class="math inline">\(\theta\)</span>:parameter of the
family</li>
<li><span class="math inline">\(\eta\)</span>:natural parameter</li>
<li><span class="math inline">\(h(x)\)</span>:underlying measure</li>
<li><span class="math inline">\(A(\theta)\)</span>:log normalizer /
partition function</li>
</ul></li>
<li><p>注意parameter of the family和natural
parameter都是向量，当指数家族分布处于标量化参数形式，即<span
class="math inline">\(\eta _i (\theta) = \theta
_i\)</span>的时候，指数家族分布可以写成：</p>
<p><span class="math display">\[
p(x | \eta)=h(x) exp(\eta (T(x) ^T \eta - A(\eta))
\]</span></p></li>
<li><p>当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：</p>
<p><span class="math display">\[
\eta = \mathop{argmax} _ {\eta} [\log p(X | \eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} [\log \prod p(x_i | \eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} [\log [\prod h(x_i) exp [(\sum T(x_i))^T \eta
- n A(\eta)]]] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} (\sum T(x_i))^T \eta - n A(\eta)] \\
\]</span></p>
<p><span class="math display">\[
= \mathop{argmax} _ {\eta} L(\eta) \\
\]</span></p></li>
<li><p>继续求极值，我们就可以得到指数家族分布关于log
normalizer和sufficient statistics的很重要的一个性质：</p>
<p><span class="math display">\[
\frac{\partial L (\eta)}{\partial \eta} = \sum T(x_i) - n
A^{&#39;}(\eta) =0 \\
\]</span></p>
<p><span class="math display">\[
A^{&#39;}(\eta) = \sum \frac{T(x_i)}{n} \\
\]</span></p></li>
<li><p>举个例子，高斯分布写成指数家族分布形式：</p>
<p><span class="math display">\[
p(x) = exp[- \frac{1}{2 \sigma ^2}x^2 + \frac{\mu}{\sigma ^2}x -
\frac{\mu ^2}{2 \sigma ^2} - \frac 12 \log(2 \pi \sigma ^2)] \\
\]</span></p>
<p><span class="math display">\[
=exp ( [x \ x^2] [\frac{\mu}{\sigma ^2} \ \frac{-1}{2 \sigma ^2}] ^T -
\frac{\mu ^2}{2 \sigma ^2} - \frac 12 \log(2 \pi \sigma ^2) )
\]</span></p></li>
<li><p>用自然参数去替代方差和均值，写成指数家族分布形式：</p>
<p><span class="math display">\[
p(x) = exp( [x \ x^2] [ \eta _1 \ \eta _2] ^T + \frac{\eta _1 ^2}{4 \eta
_2} + \frac 12 \log (-2 \eta _2 ) - \frac 12 \log (2 \pi))
\]</span></p></li>
<li><p>其中：</p>
<ul>
<li><span class="math inline">\(T(x)\)</span>:<span
class="math inline">\([x \ x^2]\)</span></li>
<li><span class="math inline">\(\eta\)</span>:<span
class="math inline">\([ \eta _1 \ \eta _2] ^T\)</span></li>
<li><span class="math inline">\(-A(\eta)\)</span>:<span
class="math inline">\(\frac{\eta _1 ^2}{4 \eta _2} + \frac 12 \log (-2
\eta _2 )\)</span></li>
</ul></li>
<li><p>接下来我们利用指数家族的性质来快速计算均值和方差</p>
<p><span class="math display">\[
A^{&#39;}(\eta) = \sum \frac{T(x_i)}{n} \\
\]</span></p>
<p><span class="math display">\[
[\frac{\partial A}{\eta _1} \ \frac{\partial A}{\eta _2}] = [\frac{-
\eta _1}{2 \eta _2} \ \frac{\eta _1 ^2 }{2 \eta _2}-\frac{1}{2 \eta _2}]
\\
\]</span></p>
<p><span class="math display">\[
= [\frac{\sum x_i}{n} \ \frac{\sum x_i^2}{n}] \\
\]</span></p>
<p><span class="math display">\[
= [\mu \ \mu ^2 + \sigma ^2] \\
\]</span></p></li>
<li><p>为什么<span class="math inline">\(A(\eta)\)</span>叫做log
normalizer？因为把概率密度的指数族分布积分有：</p>
<p><span class="math display">\[
\int _x \frac{h(x)exp(T(x)^T \eta)}{exp(A(\eta))} = 1 \\
\]</span></p>
<p><span class="math display">\[
A(\eta) = \log \int _x h(x)exp(T(x)^T \eta) \\
\]</span></p></li>
<li><p>下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：</p>
<p><span class="math display">\[
p(\beta | x) ∝ p(x | \beta) p(\beta) \\
\]</span></p>
<p><span class="math display">\[
∝ h(x) exp(T(x) \beta ^T - A_l (\beta)) h(\beta) exp(T(\beta) \alpha ^T
- A(\alpha)) \\
\]</span></p></li>
<li><p>用向量组的方式改写：</p>
<p><span class="math display">\[
T(\beta) = [\beta \ -g(\beta)] \\
\]</span></p>
<p><span class="math display">\[
\alpha = [\alpha _1 \ \alpha _2] \\
\]</span></p></li>
<li><p>原式中关于<span class="math inline">\(\beta\)</span>，<span
class="math inline">\(h(x)\)</span>和<span
class="math inline">\(A(\alpha)\)</span>都是常数，从正比式中消去，带入向量组有：</p>
<p><span class="math display">\[
∝ h(\beta) exp(T(x) \beta - A_l(\beta) + \alpha _1 \beta - \alpha _2
g(\beta)) \\
\]</span></p></li>
<li><p>我们注意到，如果令<span class="math inline">\(-g(\beta)=-A_l
(\beta)\)</span>，原式就可以写成：</p>
<p><span class="math display">\[
∝ h(\beta) exp((T(x)+\alpha _1)\beta - (1+\alpha _2) A_l (\beta)) \\
\]</span></p>
<p><span class="math display">\[
∝ h(\beta) exp(\alpha _1 ^{&#39;} \beta - \alpha _2 ^{&#39;} A_l
(\beta)) \\
\]</span></p></li>
<li><p>这样先验和后验形式一致，也就是共轭</p></li>
<li><p>这样我们用统一的形式写下似然和先验</p>
<p><span class="math display">\[
p(\beta | x, \alpha) ∝ p(x | \beta) p(\beta | \alpha) \\
\]</span></p>
<p><span class="math display">\[
∝ h(x)exp[T(x)^T\beta - A_l(\beta)] h(\beta) exp[T(\beta)^T\alpha -
A_l(\alpha)] \\
\]</span></p></li>
<li><p>这里我们可以计算log
normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log
normalizer和sufficient statistics的性质：</p>
<p><span class="math display">\[
\frac{\partial A_l(\beta)}{\partial \beta}=\int _x T(x) p(x | \beta)dx
\\
\]</span></p>
<p><span class="math display">\[
= E_{p(x|\beta)} [T(x)] \\
\]</span></p></li>
<li><p>上式可以通过指数族分布积分为1，积分对<span
class="math inline">\(\beta\)</span>求导为0，将这个等式变换证明。</p></li>
</ul>
<h2 id="指数族分布下的变分推断">指数族分布下的变分推断</h2>
<ul>
<li><p>接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁</p></li>
<li><p>我们假定要优化的参数有两个，x和z，我们用<span
class="math inline">\(\lambda\)</span>和<span
class="math inline">\(\phi\)</span>来近似<span
class="math inline">\(\eta(z,x)\)</span>和<span
class="math inline">\(\eta(\beta
,x)\)</span>，依然是要使ELBO最大，这时调整的参数是<span
class="math inline">\(q(\lambda , \phi)\)</span>，实际上是<span
class="math inline">\(\lambda\)</span>和<span
class="math inline">\(\phi\)</span></p></li>
<li><p>我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大</p></li>
<li><p>首先我们改写ELBO，注意<span
class="math inline">\(q(z,\beta)=q(z)q(\beta)\)</span>：</p>
<p><span class="math display">\[
ELBO=E_{q(z,\beta)}[\log p(x,z,\beta)] - E_{q(z,\beta)}[\log p(z,\beta)]
\\
\]</span></p>
<p><span class="math display">\[
= E_{q(z,\beta)}[\log p(\beta | x,z) + \log p(z | x) + \log p(x)] -
E_{q(z,\beta)}[\log q(\beta)] - E_{q(z,\beta)}[\log q(z)] \\
\]</span></p></li>
<li><p>其中后验为指数家族分布，且q分布用简单的参数<span
class="math inline">\(\lambda\)</span>和<span
class="math inline">\(\phi\)</span>去近似：</p>
<p><span class="math display">\[
p(\beta | x,z) = h(\beta) exp [ T(\beta) ^T \eta (z,x) - A_g
(\eta(z,x))] \\
\]</span></p>
<p><span class="math display">\[
\approx q(\beta | \lambda) \\
\]</span></p>
<p><span class="math display">\[
= h(\beta) exp [ T(\beta) ^T \eta (\lambda - A_g (\eta(\lambda))] \\
\]</span></p>
<p><span class="math display">\[
p(z | x,\beta) = h(z) exp [ T(z) ^T \eta (\beta,x) - A_l
(\eta(\beta,x))] \\
\]</span></p>
<p><span class="math display">\[
\approx q(\beta | \phi) \\
\]</span></p>
<p><span class="math display">\[
= h(z) exp [ T(z) ^T \eta (\phi - A_l (\eta(\phi))] \\
\]</span></p></li>
<li><p>现在我们固定<span class="math inline">\(\phi\)</span>，优化<span
class="math inline">\(\lambda\)</span>，将ELBO中无关常量除去，有：</p>
<p><span class="math display">\[
ELBO_{\lambda} = E_{q(z,\beta)}[\log p(\beta | x,z)] -
E_{q(z,\beta)}[\log q(\beta)] \\
\]</span></p></li>
<li><p>代入指数家族分布，消去无关常量<span class="math inline">\(-
E_{q(z)}[A_g(\eta(x,z))]\)</span>，化简得到：</p>
<p><span class="math display">\[
ELBO_{\lambda} = E_{q(\beta)}[T(\beta)^T]
E_{q(z)}[\eta(z,x)]  -E_{q(\beta)} [T(\beta)^T \lambda] + A_g(\lambda)
\]</span></p></li>
<li><p>利用之前log normalizer关于参数求导的结论，有:</p>
<p><span class="math display">\[
ELBO_{\lambda} = A_g^{&#39;}(\lambda)^T[E_{q(z)}[\eta(z,x)]] - \lambda
A_g^{&#39;}(\lambda) ^T + A_g (\lambda)
\]</span></p></li>
<li><p>对上式求导，令其为0，有：</p>
<p><span class="math display">\[
A_g^{&#39;&#39;}(\lambda)^T[E_{q(z)}[\eta(z,x)]] -
A_g^{&#39;}(\lambda)-\lambda A_g^{&#39;&#39;}(\lambda) ^T + A_g^{}
(\lambda) = 0 \\
\lambda = E_{q(z)}[\eta(z,x)] \\
\]</span></p></li>
<li><p>我们就得到了<span
class="math inline">\(\lambda\)</span>的迭代式！同理可以得到：</p>
<p><span class="math display">\[
\phi = E_{q(\beta)}[\eta(\beta,x)] \\
\]</span></p></li>
<li><p>写完整应该是：</p>
<p><span class="math display">\[
\lambda = E_{q(z | \phi)}[\eta(z,x)] \\
\phi = E_{q(\beta | \lambda)}[\eta(\beta,x)] \\
\]</span></p></li>
<li><p>观察这两个迭代式，变量更新的路径是:</p>
<p><span class="math display">\[
\lambda \rightarrow q(\beta | \lambda) \rightarrow \phi \rightarrow q(z
| \phi) \rightarrow \lambda
\]</span></p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>inference</tag>
        <tag>mcmc</tag>
        <tag>variational inference</tag>
        <tag>em</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper reading on Knowledge Graphs</title>
    <url>/2019/11/13/kg/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li>Knowledge Graph Special Collection
<ul>
<li>Entity Alignment in Cross-lingual Knowledge Graphs</li>
<li>Knowledge Graph Language Model</li>
<li>Dynamic Knowledge Graph Dialogue Generation</li>
<li>Graph2Seq</li>
<li>Graph Matching Network</li>
<li>Dynamic Knowledge Graph Update</li>
<li>Attention-based Embeddings for Relation Prediction</li>
</ul></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</h1>
<ul>
<li><p>Research: Entity Alignment in Cross-lingual Knowledge
Graphs</p></li>
<li><p>Generally, the approach involves projecting entities into
low-dimensional vectors in their respective language knowledge graphs
and then learning a similarity calculation function</p></li>
<li><p>The problem is that this approach relies on the assumption that
the neighborhood structures of the same entities are identical in
different language knowledge graphs, but this assumption is not always
true. Therefore, traditional methods are not friendly to entities with
few aligned neighborhood nodes or few neighborhood nodes</p></li>
<li><p>The authors propose a topic entity graph to encode the context
information of entity nodes, transforming the node embedding match into
a graph match between topic entity graphs</p></li>
<li><p>Here, the topic entity refers to the entities that need to be
aligned. The topic entity graph is a subgraph constructed by the one-hop
neighborhood entities and the entity itself. If these two entities are
not directly connected in the original knowledge graph, an edge is added
to the knowledge graph</p></li>
<li><p>After obtaining the topic entity graphs, a four-layer network
calculates the similarity between two topic entity graphs:</p>
<ul>
<li><p>Input representation layer: Learn embeddings for each entity in
the topic entity graph. First, use word-level LSTM to learn initial
embeddings, then because it's a directed graph, distinguish between
input and output neighbors, perform aggregation separately (FFN + mean
pooling), concatenate with the previous entity embedding and update
(FFN), iterate K times</p></li>
<li><p>Node-level local matching layer: Mutually match all entity nodes
between the two graphs using attention-based matching. First, calculate
the cosine distance between a node i in topic entity graph 1 and all
nodes in topic entity graph 2 as attention weights, use these weights to
weight all nodes in topic entity graph 2 to obtain graph 2's graph
embedding, then calculate a multi-perspective cosine distance between
this graph embedding and graph 1's query entity embedding. The
multi-perspective means l perspectives represented by l weighted vectors
(d-dimensional, same as embedding dimension). The cosine distance of one
perspective is calculated by element-wise multiplication of a weighted
vector and then computing the cosine distance. The l perspectives
together form a matrix <span class="math inline">\(W \in
R^{l*d}\)</span>, as follows:</p>
<p><span class="math display">\[
score_{perspective_k} = cosine(W_k \cdot embedding_1, W_k \cdot
embedding_2)
\]</span></p></li>
<li><p>Global matching layer: The local matching layer still has the
previously mentioned problem of not being friendly to nodes with few
co-occurring neighbors. Here, a global matching is needed. Specifically,
use a GCN to propagate the local match embedding (the vector obtained
from the previous layer's multi-perspective cosine score) to the entire
topic entity graph, then perform an FFN + max/mean pooling on the entire
graph to obtain the graph matching vector for the two graphs</p></li>
<li><p>Prediction layer: Concatenate the graph matching vectors of the
two topic entity graphs and send them to a softmax for
prediction</p></li>
</ul></li>
<li><p>During training, 20 negative samples were heuristically generated
for each positive sample, 10 in each direction. Here, word-level average
embedding was directly used as the feature vector for entities, matching
the 10 most similar entities.</p></li>
</ul>
<h1
id="baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling">Barack's
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</h1>
<ul>
<li><p>The authors want to maintain a local knowledge graph in the
language model to manage detected facts and use this graph to query
unknown facts for text generation, called KGLM (Knowledge Graph Language
Model)</p></li>
<li><p>Assuming the entity set is <span
class="math inline">\(\xi\)</span>, KGLM predicts:</p>
<p><span class="math display">\[
p(x_t,\xi _t|x_{1,t-1},\xi_ {1,t-1})
\]</span></p></li>
<li><p>The process of generating the next word can be split as
follows:</p>
<ul>
<li><p>Next word is not an entity: Calculate probability in the normal
vocabulary range</p></li>
<li><p>Next word is a completely new entity: Calculate probability
across both normal vocabulary and all entities</p></li>
<li><p>Next word is an entity related to an already seen entity: First
select a previously seen entity as the parent node, then select a child
node, and then calculate probability across the normal vocabulary and
all aliases of the child node</p></li>
</ul></li>
<li><p>The authors use LSTM as the base model for the language model.
All selections - selecting new entities, selecting parent nodes,
selecting child nodes - are based on the LSTM's hidden state (divided
into three parts), with pre-trained embeddings of entities and relations
as input dependencies, followed by probability calculation via
softmax</p></li>
<li><p>To implement such a model, the dataset should provide entity
information. The authors proposed the Linked WikiText2 dataset, with the
following construction process:</p>
<ul>
<li><p>Create entity links based on Wikipedia links, use neural-el to
identify additional links in the Wikidata database, and use Stanford
CoreNLP for co-reference resolution</p></li>
<li><p>Construct local knowledge graph: Next, build parent relations
between entities. For each entity a, add all related entities {b} in
Wikidata as matching candidates. If a related entity b appears in
subsequent paragraphs, set entity a as the parent node of entity
b</p></li>
<li><p>The above method only creates an initial set and needs continuous
expansion. The authors also created alias tables for dates, quantifiers,
etc.</p></li>
<li><p>Below is a representation of a sentence in Linked WikiText2.
Compared to the API query method of WikiText, Linked WikiText2 directly
operates on the original HTML, preserving more link information: <img data-src="https://s2.ax1x.com/2019/11/14/MY7je0.jpg"
alt="MY7je0.jpg" /></p></li>
</ul></li>
<li><p>Train and Inference: First, use TransE algorithm for pre-training
entities and relations. Given a triple (p,r,e), the objective is to
minimize the distance:</p>
<p><span class="math display">\[
\delta\left(\mathbf{v}_{p}, \mathbf{v}_{r},
\mathbf{v}_{e}\right)=\left\|\mathbf{v}_{p}+\mathbf{v}_{r}-\mathbf{v}_{e}\right\|^{2}
\]</span></p>
<p>Use Hinge Loss to ensure the score difference between positive and
negative samples does not exceed <span
class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}=\max \left(0, \gamma+\delta\left(\mathbf{v}_{p},
\mathbf{v}_{r},
\mathbf{v}_{e}\right)-\delta\left(\mathbf{v}_{p}^{\prime},
\mathbf{v}_{r}, \mathbf{v}_{e}^{\prime}\right)\right)
\]</span></p></li>
<li><p>Although the entire process is generative, all variables are
visible, so it can be trained end-to-end. For entity nodes with multiple
parent nodes, probability needs to be normalized</p></li>
<li><p>During inference, we don't have annotation information. We want
to calculate the marginal probability of word <span
class="math inline">\(x\)</span> summed over entities <span
class="math inline">\(\xi\)</span>, not the joint probability (we only
want to get words, with entity information marginalized), but there are
too many entities to calculate joint probabilities and sum them.
Therefore, the authors use importance sampling:</p>
<p><span class="math display">\[
\begin{aligned} p(\mathbf{x}) &amp;=\sum_{\mathcal{E}} p(\mathbf{x},
\mathcal{E})=\sum_{\mathcal{E}} \frac{p(\mathbf{x},
\mathcal{E})}{q(\mathcal{E} | \mathbf{x})} q(\mathcal{E} | \mathbf{x})
\\ &amp; \approx \frac{1}{N} \sum_{\mathcal{E} \sim q}
\frac{p(\mathbf{x}, \mathcal{E})}{q(\mathcal{E} | \mathbf{x})}
\end{aligned}
\]</span></p>
<p>Where the proposed distribution is obtained by the discriminative
KGLM, i.e., training another KGLM to judge the annotation of the current
token</p></li>
<li><p>The results are impressive. KGLM, using only LSTM with a small
number of parameters, shows a significant advantage in entity word
prediction compared to the large-scale GPT-2 model.</p></li>
</ul>
<h1
id="dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</h1>
<ul>
<li><p>The authors propose a new task of dialogue generation based on
dynamic knowledge graphs, aiming to capture relationships and generalize
knowledge graph-based dialogue generation to zero-shot
scenarios</p></li>
<li><p>The task description is divided into two steps:</p>
<ul>
<li><p>For each dialogue turn t, given input x and graph K, generate the
correct answer y that includes the correct knowledge graph
entities</p></li>
<li><p>When the knowledge graph is updated (only relations and
recipients can be updated), the answer y should be correspondingly
modified</p></li>
</ul></li>
<li><p>To effectively measure the quality of dynamic knowledge graph
dialogue, the authors propose two types of metrics:</p>
<ul>
<li><p>Knowledge Entity Modeling: including accuracy of predicting known
entities, entity word hit rate; true positive/false negative/false
positive rates for distinguishing predicted entities from generic words;
true positive/false negative/false positive rates for all entities in
the knowledge graph</p></li>
<li><p>Graph Adaptability: The authors propose three methods of changing
the graph, including shuffling and randomly replacing entities,
observing whether the generated sequence is replaced and replaced
correctly</p></li>
</ul></li>
<li><p>The authors create a parallel corpus containing dialogue data
from two TV series in Chinese and English, with detailed
processing</p></li>
<li><p>The proposed Qadpt model modifies the seq2seq approach. First,
the decoder's current state <span class="math inline">\(d_t\)</span>
generates a controller <span class="math inline">\(c_t\)</span> to
decide whether to select entities from the KG or generate general
vocabulary. Similar to the copy mechanism, this selection is not hard
but calculates probabilities separately, then concatenates two
vocabulary lists and selects based on probability:</p>
<p><span class="math display">\[
\begin{aligned} P\left(\{K B, \mathcal{W}\} | y_{1} y_{2} \ldots
y_{t-1}, \mathbf{e}(x)\right)
\\=\operatorname{softmax}\left(\phi\left(\mathbf{d}_{t}\right)\right) \\
\mathbf{w}_{t}=P\left(\mathcal{W} | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ c_{t}=P\left(K B | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ \mathbf{o}_{t}=\left\{c_{t} \mathbf{k}_{t} ;
\mathbf{w}_{t}\right\} \end{aligned}
\]</span></p></li>
<li><p>Regarding how to generate entity candidate lists, they perform
reasoning on the knowledge graph. Unlike typical attention-based graph
embedding approaches, the authors use multi-hop reasoning</p>
<ul>
<li>First, combine path matrix and adjacency matrix into a transition
matrix, where the path matrix represents the probability of selecting
each relation for each entity learned from <span
class="math inline">\(d_t\)</span>, then select recipient nodes based on
probability:</li>
</ul>
<p><span class="math display">\[
\begin{aligned} \mathbf{R}_{t}
&amp;=\operatorname{softmax}\left(\theta\left(\mathbf{d}_{t}\right)\right)
\\ \mathbf{A}_{i, j, \gamma} &amp;=\left\{\begin{array}{ll}{1,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \in \mathcal{K}} \\ {0,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \notin
\mathcal{K}}\end{array}\right.\\ \mathbf{T}_{t}=\mathbf{R}_{t}
\mathbf{A} \end{aligned}
\]</span></p>
<ul>
<li>Then take an initial vector <span class="math inline">\(s\)</span>
(uniformly distributed?), transform n times using the transition matrix
to obtain the probability of each entity appearing, and provide it to
the controller for calculation. Here, cross-entropy is calculated using
one-hot ground truth as an auxiliary loss</li>
</ul></li>
</ul>
<h1
id="graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</h1>
<ul>
<li><p>As the name suggests, the input is graph-structured data,
generating a sequence</p></li>
<li><p>Previous approaches encoded graphs into fixed-length sequences
for Seq2Seq, which the authors believe leads to information loss.
Encoding graph to encoder sequence introduces an additional layer of
information loss</p></li>
<li><p>A more natural approach would be for the decoder to perform
attention on the encoded graph nodes, directly utilizing graph
information</p></li>
<li><p>First, the graph encoder, referencing GraphSage's approach.
Notably, the authors handle directed graphs by distinguishing neighbor
nodes in two directions, performing Aggregate and Update operations, and
concatenating after k hops</p></li>
<li><p>The authors tried Mean, LSTM, and Pooling methods. Since
neighbors are unordered, LSTM has no temporal effect, so the authors
randomly arrange neighbors and use LSTM Aggregate</p></li>
<li><p>The authors believe that in addition to node embedding, graph
embedding should be passed to the decoder. They adopted two methods to
obtain graph embedding</p>
<ul>
<li><p>Pooling-based: First pass all node embeddings through a fully
connected layer, then perform element-wise max, min, average pooling.
The authors found the three methods performed similarly and used max
pooling as the default pooling method</p></li>
<li><p>Node-based: Add a super node to the graph, connected to all other
nodes, using the embedding of this node after graph encoding as the
graph embedding</p></li>
</ul></li>
<li><p>Attention-based decoder: Graph embedding is input as the initial
state of the decoder, and at each step, the decoder generates attention
on all node embeddings and uses this as the decoder's hidden state for
that time step</p></li>
<li><p>Focusing on NLG tasks, the authors tested SQL2Text task, first
building a graph from SQL Query, then using Graph2Seq. The effect was
significantly better than Seq2seq from SQL Query to Text</p></li>
<li><p>In the Aggregate comparison experiment, they found Mean Pooling
performed best, and for Graph Embedding, Pooling-based significantly
outperformed Node-based</p></li>
</ul>
<h1
id="graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</h1>
<ul>
<li>Google-produced, experimental and visualization results are as rich
as ever</li>
<li>Two contributions:
<ul>
<li>Proved that GNN can generate graph embeddings for similarity
calculation</li>
<li>Proposed attention-based Graph Matching Networks, surpassing
baselines</li>
</ul></li>
</ul>
<h2 id="graph-embedding-model">Graph Embedding Model</h2>
<ul>
<li><p>Baseline: Graph Embedding Model, a simple
encode-propagation-aggregate model</p></li>
<li><p>Encode: Encode point and edge features through MLP to get
embeddings</p></li>
<li><p>Propagation: Transmit center point, adjacent point, and adjacent
edge embeddings to the next layer's center point embedding</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right) \\ \mathbf{h}_{i}^{(t+1)} &amp;=f_{\text {node
}}\left(\mathbf{h}_{i}^{(t)}, \sum_{j:(j, i) \in E} \mathbf{m}_{j
\rightarrow i}\right) \end{aligned}
\]</span></p></li>
<li><p>Aggregate: The author uses a gating method to weight and sum the
embeddings of each node to obtain the final graph embedding</p>
<p><span class="math display">\[
\mathbf{h}_{G}=\operatorname{MLP}_{G}\left(\sum_{i \in V}
\sigma\left(\operatorname{MLP}_{\operatorname{gate}}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\odot \operatorname{MLP}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\]</span></p></li>
</ul>
<h2 id="graph-matching-networks">Graph Matching Networks</h2>
<ul>
<li><p>GMN does not separately generate embeddings for two graphs and
then match them, but directly accepts two graphs as input to output a
similarity score</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right), \forall(i, j) \in E_{1} \cup E_{2} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=f_{\text {match }}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right) \\ \forall i \in V_{1}, j &amp; \in V_{2},
\text { or } i \in V_{2}, j \in V_{1} \\ \mathbf{h}_{i}^{(t+1)}
&amp;=f_{\text {node }}\left(\mathbf{h}_{i}^{(t)}, \sum_{j}
\mathbf{m}_{j \rightarrow i}, \sum_{j^{\prime}} \mu_{j^{\prime}
\rightarrow i}\right) \\ \mathbf{h}_{G_{1}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{1}}\right) \\ \mathbf{h}_{G_{2}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{2}}\right) \\ s &amp;=f_{s}\left(\mathbf{h}_{G_{1}},
\mathbf{h}_{G_{2}}\right) \end{aligned}
\]</span></p></li>
<li><p>From the above formula, GMN made two modifications in the
propagation phase</p>
<ul>
<li><p>Since a pair of graphs is input at once, the first step of
neighborhood nodes is found from the range of two graphs. However,
generally, there are no node connections between two graphs, unless the
same nodes in both graphs share neighborhoods?</p></li>
<li><p>In addition to neighborhood information transmission, the authors
also calculate the match between two graphs, using a simple attention
mechanism, weighting the difference between two node embeddings by their
distance:</p>
<p><span class="math display">\[
\begin{aligned} a_{j \rightarrow i} &amp;=\frac{\exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right)\right)}{\sum_{j^{\prime}} \exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j^{\prime}}^{(t)}\right)\right)} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right) \end{aligned}
\]</span></p></li>
<li><p>When updating to the next layer of node embeddings, the match
part actually calculates the weighted distance of a graph's node with
all nodes of b graph:</p>
<p><span class="math display">\[
\sum_{j} \boldsymbol{\mu}_{j \rightarrow i}=\sum_{j} a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right)=\mathbf{h}_{i}^{(t)}-\sum_{j}
a_{j \rightarrow i} \mathbf{h}_{j}^{(t)}
\]</span></p></li>
<li><p>This calculation increases complexity to <span
class="math inline">\(O(V(G_1)V(G_2))\)</span>, but this point-by-point
comparison can distinguish subtle changes and is more interpretable. So
the algorithm's use case should be small graphs with high precision
requirements</p></li>
</ul></li>
<li><p>For such matching problems, pair or triplet loss can be used,
with the former comparing similarity and dissimilarity, and the latter
comparing which is more similar to two candidates. The authors provide
margin loss in both forms:</p>
<p><span class="math display">\[
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2}, t\right)}\left[\max
\left\{0, \gamma-t\left(1-d\left(G_{1},
G_{2}\right)\right)\right\}\right] \\
L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\max \left\{0, d\left(G_{1},
G_{2}\right)-d\left(G_{1}, G_{3}\right)+\gamma\right\}\right] \\
\]</span></p></li>
<li><p>The authors specifically mention that to accelerate computation,
graph embeddings can be binarized, using Hamming distance instead,
sacrificing some Euclidean space. The specific method is to pass the
entire vector through tanh and use average inner product as the graph
similarity during training, designing a loss that pushes the Hamming
distance of positive sample pairs towards 1 and negative sample pairs
towards -1. This loss design is more stable than margin loss when used
for retrieval:</p>
<p><span class="math display">\[
s\left(G_{1}, G_{2}\right)=\frac{1}{H} \sum_{i=1}^{H} \tanh
\left(h_{G_{1} i}\right) \cdot \tanh \left(h_{G_{2} i}\right) \\
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2},
t\right)}\left[\left(t-s\left(G_{1}, G_{2}\right)\right)^{2}\right] / 4
\\
\begin{aligned} L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\left(s\left(G_{1},
G_{2}\right)-1\right)^{2}+\right.\\\left.\left(s\left(G_{1},
G_{3}\right)+1\right)^{2}\right] / 8 \end{aligned} \\
\]</span></p>
<p>Dividing by 4 or 8 is to constrain the loss range to the [0,1]
interval.</p></li>
</ul>
<h1 id="learning-to-update-knowledge-graphs-by-reading-news">Learning to
Update Knowledge Graphs by Reading News</h1>
<ul>
<li>An EMNLP 2019 work, the author is definitely a basketball fan, using
a very appropriate NBA player transfer example to illustrate the problem
this paper aims to solve: knowledge graph updating</li>
<li>For example, when a player transfers clubs, the player graphs of the
two related clubs will change. The authors highlight two key points, as
shown in Figure 1:
<ul>
<li>Knowledge graph updates only occur in the text subgraph, not the
1-hop subgraph</li>
<li>Traditional methods cannot extract hidden knowledge graph update
information from text, such as a player's teammates changing after a
transfer, which is not explicitly mentioned in the text but can be
inferred</li>
</ul></li>
<li>The overall structure is an encoder based on R-GCN and GAT, and a
decoder based on DistMult, essentially modifying RGCN to an
attention-based approach, with the decoder remaining unchanged,
performing link prediction tasks</li>
</ul>
<h2 id="encoder">Encoder</h2>
<ul>
<li><p><strong>Encoder: RGCN+GAT = RGAT.</strong> In RGCN, the forward
propagation process is as follows:</p>
<p><span class="math display">\[
\mathbf{H}^{l+1} = \sigma\left(\sum_{r \in \mathbf{R}}
\hat{\mathbf{A}}_{r}^{l} \mathbf{H}^{l} \mathbf{W}_{r}^{l}\right)
\]</span></p></li>
<li><p>This means assigning a parameter matrix to each type of
heterogeneous edge, calculating independently, summing the results, and
then applying an activation function. By replacing the adjacency matrix
with an attention matrix, the attention calculation becomes:</p>
<p><span class="math display">\[
a_{ij}^{lr} = \left\{
\begin{array}{ll}
\frac{\exp \left(\text{att}^{lr}\left(\mathbf{h}_i^l,
\mathbf{h}_j^l\right)\right)}{\sum_{k \in \mathcal{N}_i^r} \exp
\left(\text{att}^{lr}\left(\mathbf{h}_i^l,
\mathbf{h}_k^l\right)\right)}, &amp; j \in \mathcal{N}_i^r \\
0, &amp; \text{otherwise}
\end{array}
\right.
\]</span></p></li>
<li><p>The attention function <span
class="math inline">\(\text{attn}\)</span> is computed based on the
text:</p>
<ul>
<li><p>First, a bidirectional GRU encodes the sequence as <span
class="math inline">\(u\)</span>.</p></li>
<li><p>Sequence attention is then used to obtain the contextual
representation:</p>
<p><span class="math display">\[
b_t^{lr} = \frac{\exp \left(\mathbf{u}_t^T
\mathbf{g}_{\text{text}}^{lr}\right)}{\sum_{k=1}^{|S|} \exp
\left(\mathbf{u}_k^T \mathbf{g}_{\text{text}}^{lr}\right)} \\
\mathbf{c}^{lr} = \sum_{t=1}^{|S|} b_t^{lr} \mathbf{u}_t
\]</span></p></li>
<li><p>When applying attention, a trainable guidance vector <span
class="math inline">\(g\)</span> incorporates this contextual
representation through simple linear interpolation:</p>
<p><span class="math display">\[
\mathbf{g}_{\text{fin}}^{lr} = \alpha^{lr}
\mathbf{g}_{\text{graph}}^{lr} + \left(1-\alpha^{lr}\right)
\mathbf{U}^{lr} \mathbf{c}^{lr} \\
\text{att}^{lr}(\mathbf{h}_i^l, \mathbf{h}_j^l) =
\mathbf{g}_{\text{fin}}^{lr}[\mathbf{h}_i^l \| \mathbf{h}_j^l]
\]</span></p></li>
</ul></li>
<li><p>In practical applications, aimed at the task of KG updates, the
authors incorporated several techniques:</p>
<ul>
<li>The number of parameters in RGCN/RGAT increases linearly with the
number of edge (relation) types. To reduce parameters, the authors used
<strong>basis decomposition</strong>, where <span
class="math inline">\(k\)</span> relation types share <span
class="math inline">\(b\)</span> parameter sets (<span
class="math inline">\(b &lt; k\)</span>), and these <span
class="math inline">\(k\)</span> parameters are linear combinations of
the <span class="math inline">\(b\)</span> sets.</li>
<li>In sparse relational datasets, messages cannot aggregate effectively
in one or two layers of RGAT. Thus, the authors added an artificial
"SHORTCUT" relation between all entities in the graph. Using an
information extraction tool, the "SHORTCUT" relation was refined into
<strong>add</strong>, <strong>delete</strong>, and
<strong>other</strong> categories to preliminarily capture transfer
relationships (e.g., an individual leaving one club and joining
another).</li>
</ul></li>
</ul>
<h2 id="decoder">Decoder</h2>
<ul>
<li><p>The paper <em>Embedding Entities and Relations for Learning and
Inference in Knowledge Bases</em> summarized the task of learning
relation embeddings in KGs. The method involves combining different
linear/bilinear parameter matrices with various scoring functions to
compute the <strong>margin triplet loss</strong>:</p>
<figure>
<img data-src="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg"
alt="Knowledge Graph Embedding" />
<figcaption aria-hidden="true">Knowledge Graph Embedding</figcaption>
</figure></li>
<li><p><strong>DistMult</strong> is the simplest approach, where
bilinear parameter matrices are replaced with diagonal matrices. The
final score is obtained by element-wise multiplication of two entity
embeddings, weighted by a relation-specific parameter. The formula used
is:</p>
<p><span class="math display">\[
P(y) = \operatorname{sigmoid}\left(\mathbf{h}_i^T \left(\mathbf{r}_k
\circ \mathbf{h}_j\right)\right)
\]</span></p></li>
</ul>
<h2 id="results">Results</h2>
<ul>
<li>Comparing several baselines (RGCN, PCNN), the authors used a
GRU-based network to extract semantic similarities, which is
data-intensive. While the dataset size was small, the final results were
impressive. Notably, RGAT doubled the accuracy of RGCN in small-sample
classes like <strong>add</strong> and <strong>delete</strong>.</li>
<li>A highlight of the paper was how it framed the link prediction
problem, emphasizing continual learning and updates. By simplifying the
task to link prediction, the model achieved high performance without
extensive modifications.</li>
</ul>
<h1
id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</h1>
<ul>
<li><p>This study also focused on <strong>link prediction</strong>,
again using GAT.</p></li>
<li><p>The authors emphasized the importance of relations in KGs, but
direct feature assignment to edges is challenging. To address this, they
incorporated edge features into node features indirectly.</p></li>
<li><p>A diagram explains this approach effectively:</p>
<figure>
<img data-src="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png"
alt="Attention-based Embeddings" />
<figcaption aria-hidden="true">Attention-based Embeddings</figcaption>
</figure></li>
<li><p>From left to right:</p>
<ul>
<li><p><strong>Input:</strong> Node features are input into GAT. Each
node feature is derived via self-attention over triplet features, which
are created by concatenating node and relation features. Green indicates
relation features, and yellow indicates node features:</p>
<p><span class="math display">\[
c_{ijk} = \mathbf{W}_1\left[\vec{h}_i \| \vec{h}_j \| \vec{g}_k\right]
\\
\begin{aligned}
\alpha_{ijk} &amp;= \operatorname{softmax}_{jk}\left(b_{ijk}\right) \\
&amp;= \frac{\exp \left(b_{ijk}\right)}{\sum_{n \in \mathcal{N}_i}
\sum_{r \in \mathcal{R}_{in}} \exp \left(b_{inr}\right)}
\end{aligned} \\
\overrightarrow{h_i&#39;} = \|_{m=1}^M \sigma\left(\sum_{j \in
\mathcal{N}_i} \alpha_{ijk}^m c_{ijk}^m\right)
\]</span></p></li>
<li><p><strong>Intermediate Layer:</strong> GAT computes multi-head
attention, concatenating the results. Features (6-dimensional gray) are
transformed and concatenated with relation features (green) to form
triplet representations for each node.</p></li>
<li><p><strong>Final Layer:</strong> Average pooling is applied instead
of concatenation. Input node features are incorporated again,
concatenated with relation features, and loss is calculated based on the
triplet distance: subject + predicate - object. Negative sampling
involves randomly replacing the subject or object.</p></li>
</ul></li>
<li><p><strong>Decoder:</strong> The decoder uses
<strong>ConvKB</strong>:</p>
<p><span class="math display">\[
f\left(t_{ij}^k\right) = \left(\prod_{m=1}^\Omega
\operatorname{ReLU}\left(\left[\vec{h}_i, \vec{g}_k, \vec{h}_j\right] *
\omega^m\right)\right) \mathbf{. W}
\]</span></p></li>
<li><p><strong>Loss:</strong> Soft-margin loss is used (although the
values for 1 and -1 seem reversed):</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L} &amp;= \sum_{t_{ij}^k \in \{S \cup S&#39;\}} \log \left(1 +
\exp \left(l_{t_{ij}^k} * f\left(t_{ij}^k\right)\right)\right) +
\frac{\lambda}{2}\|\mathbf{W}\|_2^2 \\
\text{where } l_{t_{ij}^k} &amp;=
\begin{cases}
1 &amp; \text{for } t_{ij}^k \in S \\
-1 &amp; \text{for } t_{ij}^k \in S&#39;
\end{cases}
\end{aligned}
\]</span></p></li>
<li><p>Additional improvements included adding edges between 2-hop
nodes.</p></li>
<li><p>The results were excellent, achieving SOTA performance on
FB15K-237 and WN18RR. The authors avoided integrating edge features
directly into GAT's message passing, focusing instead on encoding these
features effectively and ensuring that initial features influenced every
model layer for robust gradient propagation.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="cross-lingual-knowledge-graph-alignment-via-graph-matching-neural-network">Cross-lingual
Knowledge Graph Alignment via Graph Matching Neural Network</h1>
<ul>
<li><p>研究：跨语言知识图谱中实体的对齐</p></li>
<li><p>一般的做法，在各自语言的知识图谱中投影到低维向量，然后学习到一个相似度计算函数</p></li>
<li><p>问题在于，上面的做法依赖于相同实体在不同语言知识图谱下的邻域结构相同这一假设，但这一假设并不总是成立，因此传统的做法对少对齐邻域节点/少邻域节点的实体不友好</p></li>
<li><p>作者提出主题实体图来编码实体节点的上下文信息，将节点embedding之间的Match转换为主题实体图之间的图Match</p></li>
<li><p>这里的主题实体指的是需要对齐的实体，主题实体图即主题实体周围一跳邻域实体与本体一同构建的子图，假如在原知识图谱里这两个实体没有直接相连的话就在知识图谱里添加一条边</p></li>
<li><p>得到主题实体图之后，经过四层网络计算出两个主题实体图之间的相似度：</p>
<ul>
<li><p>输入表示层：对主题实体图中的每一个实体学习到embedding。首先用word-level
lstm学习到初始embedding，然后因为是有向图，因此需要将邻域节点区分为输入和输出邻居，分别做聚合（FFN+mean
pooling），拼接到上一次得到的实体embedding并更新（FFN），迭代K次</p></li>
<li><p>节点级别的局部匹配层：两个图互相匹配所有实体节点，这里用了基于注意力的Matching，即先用cosine距离得到主题实体图1里某个节点i和主题实体图2所有节点之间的相似度，用这个相似度作为注意力权重去加权主题实体图2所有节点得到图2的graph
embedding，然后用这个graph embedding和图1的query entity
embedding之间计算一个多角度cosine距离作为local match
score，这个多角度是指l个角度用l个加权向量(d维，同embedding维度）表示。一个角度下的cosine距离就用一个加权向量（逐元素相乘）加权两个embedding再计算cosine距离，l个角度合在一起就是一个<span
class="math inline">\(W \in R^{l*d}\)</span>的矩阵，如下：</p>
<p><span class="math display">\[
score_{perspective_k} = cosine(W_k \cdot embedding_1, W_k \cdot
embedding_2)
\]</span></p></li>
<li><p>全局匹配层：局部匹配层存在着之前提到的对少共现邻居节点不友好的问题，这里就需要全局匹配。具体做法是，再用一个GCN将局部的match
embedding（上一层多角度cosine
score得到的向量）传递到整个主题实体图，之后在整个图上做一个FFN +
max/mean pooling，得到两个图的graph matching vector</p></li>
<li><p>预测层:将两个主题实体图的graph matching
vector拼接送入一个softmax预测</p></li>
</ul></li>
<li><p>训练时，对每一个正样本启发式的生成了20个负样本，两个方向各十个，这里直接基于word-level
average embedding作为实体的特征向量，匹配十个最相似的实体</p></li>
</ul>
<h1
id="baracks-wife-hillary-using-knowledge-graphs-for-fact-aware-language-modeling">Barack’s
Wife Hillary: Using Knowledge Graphs for Fact-Aware Language
Modeling</h1>
<ul>
<li><p>作者希望在语言模型当中维持一个local knowledge
graph来管理已经探测到的事实，并利用该图谱来查询未知的事实用于文本生成，称之为KGLM(Knowledge
Graph Language Model)</p></li>
<li><p>假设实体集合为<span
class="math inline">\(\xi\)</span>，则KGLM要预测的是</p>
<p><span class="math display">\[
p(x_t,\xi _t|x_{1,t-1},\xi_ {1,t-1})
\]</span></p></li>
<li><p>生成下一个词的流程可以拆分如下：</p>
<ul>
<li><p>下一个词不是实体：那就正常词典范围上计算概率</p></li>
<li><p>下一个词是全新实体：在正常词典以及所有实体范围上计算概率</p></li>
<li><p>下一个词是与已经看见的实体相关的实体：先挑出一个已经看见的实体作为父节点，再挑出一个子节点，之后在正常词典以及该子节点的所有别名上计算概率</p></li>
</ul></li>
<li><p>作者使用LSTM作为LM的基础模型，所有的挑选：挑新实体、挑父节点、挑子节点，均利用LSTM的隐状态（切分为三部分），并加上实体和关系的预训练embedding作为输入依赖，之后通过softmax计算概率</p></li>
<li><p>为了实现这样的模型数据集应该提供实体信息，作者提出了Linked
WikiText2数据集，该数据集的构建流程如下：</p>
<ul>
<li><p>根据维基百科上的链接创造实体之间的链接，借助neural-el来识别wikidata数据库中额外的链接，使用stanford
corenlp来做共指消解。</p></li>
<li><p>构造local knowledge graph：接下来需要建立实体之间的parents
relation，每遇到一个实体a，将其在wikidata中相关联的所有实体{b}加入matching的候选，加入相关联的某一实体b在之后的文段中出现了，则将实体a作为实体b的父节点</p></li>
<li><p>以上的做法只是构建了初始集合，需要不断扩展，作者还对日期、量词等做了alias
table</p></li>
<li><p>以下是一句话在Linked WikiText2中的表示，相比WikiText使用api
query构造的方法，Linked
WikiText2直接对原始html操作，保留了更多链接信息： <img data-src="https://s2.ax1x.com/2019/11/14/MY7je0.jpg"
alt="MY7je0.jpg" /></p></li>
</ul></li>
<li><p>Train and
Inference：首先对实体和关系使用TransE算法做一个Pretraining，给定三元组(p,r,e)，目标是最小化距离：</p>
<p><span class="math display">\[
\delta\left(\mathbf{v}_{p}, \mathbf{v}_{r},
\mathbf{v}_{e}\right)=\left\|\mathbf{v}_{p}+\mathbf{v}_{r}-\mathbf{v}_{e}\right\|^{2}
\]</span></p>
<p>采用Hinge Loss，使得正负样本得分之差不超过<span
class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}=\max \left(0, \gamma+\delta\left(\mathbf{v}_{p},
\mathbf{v}_{r},
\mathbf{v}_{e}\right)-\delta\left(\mathbf{v}_{p}^{\prime},
\mathbf{v}_{r}, \mathbf{v}_{e}^{\prime}\right)\right)
\]</span></p></li>
<li><p>虽然整个过程是生成式的，但是所有变量均可见，因此可以端到端的直接训练，对于有多个父节点的实体节点，需要对概率做归一化</p></li>
<li><p>在推理过程中，我们没有标注信息，我们希望计算的也是单词<span
class="math inline">\(x\)</span>在实体<span
class="math inline">\(\xi\)</span>求和得到的边缘概率，而不是联合概率（我们只希望得到词，词的实体信息被marginalize了），然而实体太多，不可能对所有实体计算联合概率再求和，因此作者采用了重要性采样:</p>
<p><span class="math display">\[
\begin{aligned} p(\mathbf{x}) &amp;=\sum_{\mathcal{E}} p(\mathbf{x},
\mathcal{E})=\sum_{\mathcal{E}} \frac{p(\mathbf{x},
\mathcal{E})}{q(\mathcal{E} | \mathbf{x})} q(\mathcal{E} | \mathbf{x})
\\ &amp; \approx \frac{1}{N} \sum_{\mathcal{E} \sim q}
\frac{p(\mathbf{x}, \mathcal{E})}{q(\mathcal{E} | \mathbf{x})}
\end{aligned}
\]</span></p>
<p>其中proposed
distribution使用判别式的KGLM得到，即另外训练一个KGLM判断当前token的annotation</p></li>
<li><p>结果非常漂亮，KGLM仅仅用了LSTM，参数量也不大，和超大规模的GPT-2模型相比，在实体词的预测上有着明显优势。</p></li>
</ul>
<h1
id="dykgchat-benchmarking-dialogue-generation-grounding-on-dynamic-knowledge-graphs">DyKgChat:
Benchmarking Dialogue Generation Grounding on Dynamic Knowledge
Graphs</h1>
<ul>
<li><p>本文作者提出了一个新的任务，动态知识图谱对话生成，也就是希望抓住图谱中的关系，来将基于知识图谱的对话生成推广到zero-shot</p></li>
<li><p>任务的详细描述分为两步：</p>
<ul>
<li><p>每轮对话t，给定输入x和图谱K，希望生成正确的回答y，而且包含正确的知识图谱实体</p></li>
<li><p>当知识图谱更新之后（这里只可能更新关系和受体），回答y能够相应更改回答。</p></li>
</ul></li>
<li><p>为了有效衡量动态知识图谱对话的质量，作者提出了两类指标：</p>
<ul>
<li><p>知识实体建模：包括已知要预测实体，实体词命中的准确率；判别要预测实体还是通用词的TP;整个知识图谱所有实体的TP</p></li>
<li><p>图自适应：作者提出了三种改变图的方法，包括shuffle和随即替换实体，观察生成的序列是否替换且替换正确</p></li>
</ul></li>
<li><p>作者提出了一个平行语料库，包含中英两个电视剧的语料，并做了详细的处理</p></li>
<li><p>作者提出的模型Qadpt在seq2seq的基础上修改，首先将decoder的当前状态<span
class="math inline">\(d_t\)</span>生成一个controller<span
class="math inline">\(c_t\)</span>来决定是从KG里挑实体还是从generic
vocab里生成一般词汇。和copy
mechanism一样这个选择不是hard，而是分别计算概率，最后将两部分词表拼到一起，最后依概率选择：</p>
<p><span class="math display">\[
\begin{aligned} P\left(\{K B, \mathcal{W}\} | y_{1} y_{2} \ldots
y_{t-1}, \mathbf{e}(x)\right)
\\=\operatorname{softmax}\left(\phi\left(\mathbf{d}_{t}\right)\right) \\
\mathbf{w}_{t}=P\left(\mathcal{W} | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ c_{t}=P\left(K B | y_{1} y_{2} \ldots y_{t-1},
\mathbf{e}(x)\right) \\ \mathbf{o}_{t}=\left\{c_{t} \mathbf{k}_{t} ;
\mathbf{w}_{t}\right\} \end{aligned}
\]</span></p></li>
<li><p>至于如何产生实体候选列表，就是在知识图谱上做reasoning，不同于一般的attention
based graph embedding的做法，作者采用了multi-hop reasoning</p>
<ul>
<li>首先将path matrix和adjacency matrix合成transition matrix，其中的path
matrix是指用<span
class="math inline">\(d_t\)</span>学习到的每个实体选择每一种关系的概率，之后依概率选择受体节点：</li>
</ul>
<p><span class="math display">\[
\begin{aligned} \mathbf{R}_{t}
&amp;=\operatorname{softmax}\left(\theta\left(\mathbf{d}_{t}\right)\right)
\\ \mathbf{A}_{i, j, \gamma} &amp;=\left\{\begin{array}{ll}{1,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \in \mathcal{K}} \\ {0,} &amp;
{\left(h_{i}, r_{j}, t_{\gamma}\right) \notin
\mathcal{K}}\end{array}\right.\\ \mathbf{T}_{t}=\mathbf{R}_{t}
\mathbf{A} \end{aligned}
\]</span></p>
<ul>
<li>之后取一个初始向量<span
class="math inline">\(s\)</span>（均匀分布？），用transition
matrix做n次transform，得到每个实体出现的概率并提供给controller计算，这里会使用one
hot ground truth计算一个交叉熵作为辅助损失</li>
</ul></li>
</ul>
<h1
id="graph2seq-graph-to-sequence-learning-with-attention-based-neural-networks">Graph2Seq:
Graph to Sequence Learning with Attention-based Neural Networks</h1>
<ul>
<li><p>顾名思义，输入为图结构组织的数据，生成的是序列</p></li>
<li><p>以往的做法，将图编码成固定长度的序列，再用Seq2Seq，作者认为这样存在信息丢失，本身Enc
Seq 2 Dec Seq就存在信息丢失，现在Graph 2 Enc
Seq会再丢失一层信息</p></li>
<li><p>因此比较自然的做法应该是，解码器在编码的图节点上做attention，直接利用图的信息</p></li>
<li><p>首先是图编码器，参考了GraphSage的做法，值得注意的是作者处理的是有向图，因此将邻居节点按两个方向做了区分，分别做Aggregate和Update的操作，做了k跳之后再拼接回来</p></li>
<li><p>作者试了Mean、LSTM、Pooling三种，由于邻居是无序的，因此LSTM没有时序上的效果，作者直接随机排列邻居用LSTM
Aggregate</p></li>
<li><p>作者认为传给解码器的不只node embedding还需要graph
embedding。作者采用了两种方法获取graph embedding</p>
<ul>
<li><p>Pooling-based：先将所有的node
embedding经过一个全连接层，然后逐元素做max、min、average
pooling，作者发现三种方法的实际效果相差不大，就使用max
pooling作为默认的池化方法</p></li>
<li><p>Node-based：在图中加入一个超级节点，该节点与图中其他所有节点相连，用该节点经过图编码之后的embedding作为Graph
embedding</p></li>
</ul></li>
<li><p>基于注意力的解码器：graph
embedding作为解码器的初始状态输入，之后decoder每一步生成在所有node
embedding上的attention并加权作为该时间步decoder的隐状态</p></li>
<li><p>重点关注NLG task，作者测试了SQL2Text任务，首先将SQL
Query建图，然后使用Graph2Seq。效果显著好于SQL
Query到Text的Seq2seq</p></li>
<li><p>另外在Aggregate的比对实验中发现，Mean
Pooling的效果最好，对于Graph Embedding，Pooling based的效果显著好于Node
based</p></li>
</ul>
<h1
id="graph-matching-networks-for-learning-the-similarity-of-graph-structured-objects">Graph
Matching Networks for Learning the Similarity of Graph Structured
Objects</h1>
<ul>
<li>google出品，实验和可视化结果一如既往的丰富</li>
<li>两点贡献：
<ul>
<li>证明了GNN可以产生用于相似度计算的graph embedding</li>
<li>提出了attention based的Graph Matching
Networks，并超越了baseline</li>
</ul></li>
</ul>
<h2 id="graph-embedding-model">Graph Embedding Model</h2>
<ul>
<li><p>baseline:Graph Embedding
Model，一个简单的encode-propagation-aggregate模型</p></li>
<li><p>encode：将点和边的特征通过MLP编码得到embedding</p></li>
<li><p>proprgation：将中心点，邻接点，邻接边的embedding传递到下一层的中心点embedding</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right) \\ \mathbf{h}_{i}^{(t+1)} &amp;=f_{\text {node
}}\left(\mathbf{h}_{i}^{(t)}, \sum_{j:(j, i) \in E} \mathbf{m}_{j
\rightarrow i}\right) \end{aligned}
\]</span></p></li>
<li><p>aggregate：作者用门控的方式将各个节点的embedding加权求和得到最后的graph
embedding</p>
<p><span class="math display">\[
\mathbf{h}_{G}=\operatorname{MLP}_{G}\left(\sum_{i \in V}
\sigma\left(\operatorname{MLP}_{\operatorname{gate}}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\odot \operatorname{MLP}\left(\mathbf{h}_{i}^{(T)}\right)\right)
\]</span></p></li>
</ul>
<h2 id="graph-matching-networks">Graph Matching Networks</h2>
<ul>
<li><p>GMN并不像Baseline一样分别对两个图先生成embedding再match，而是接受两个图作为输入直接输出similarity
score。</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{m}_{j \rightarrow i} &amp;=f_{\text {message
}}\left(\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}, \mathbf{e}_{i
j}\right), \forall(i, j) \in E_{1} \cup E_{2} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=f_{\text {match }}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right) \\ \forall i \in V_{1}, j &amp; \in V_{2},
\text { or } i \in V_{2}, j \in V_{1} \\ \mathbf{h}_{i}^{(t+1)}
&amp;=f_{\text {node }}\left(\mathbf{h}_{i}^{(t)}, \sum_{j}
\mathbf{m}_{j \rightarrow i}, \sum_{j^{\prime}} \mu_{j^{\prime}
\rightarrow i}\right) \\ \mathbf{h}_{G_{1}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{1}}\right) \\ \mathbf{h}_{G_{2}}
&amp;=f_{G}\left(\left\{\mathbf{h}_{i}^{(T)}\right\}_{i \in
V_{2}}\right) \\ s &amp;=f_{s}\left(\mathbf{h}_{G_{1}},
\mathbf{h}_{G_{2}}\right) \end{aligned}
\]</span></p></li>
<li><p>从上面的公式可以看到，在propagation阶段，GMN做出了两点改动</p>
<ul>
<li><p>因为一次性输入一对图，因此第一步的邻域节点是从两张图的范围内找。但是一般而言两张图之间是没有节点连接的，除非两张图里的相同节点共享邻域?</p></li>
<li><p>除了邻域信息的传递之外，作者还计算了两张图之间的match，这里用了一个最简单的attention机制，用待匹配两个节点embedding的距离加权两个节点embedding之间的差：</p>
<p><span class="math display">\[
\begin{aligned} a_{j \rightarrow i} &amp;=\frac{\exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j}^{(t)}\right)\right)}{\sum_{j^{\prime}} \exp
\left(s_{h}\left(\mathbf{h}_{i}^{(t)},
\mathbf{h}_{j^{\prime}}^{(t)}\right)\right)} \\ \boldsymbol{\mu}_{j
\rightarrow i} &amp;=a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right) \end{aligned}
\]</span></p></li>
<li><p>这样在update到下一层节点embedding时，match的那部分实际上计算了a图某一结点与b图所有节点的加权距离：</p>
<p><span class="math display">\[
\sum_{j} \boldsymbol{\mu}_{j \rightarrow i}=\sum_{j} a_{j \rightarrow
i}\left(\mathbf{h}_{i}^{(t)}-\mathbf{h}_{j}^{(t)}\right)=\mathbf{h}_{i}^{(t)}-\sum_{j}
a_{j \rightarrow i} \mathbf{h}_{j}^{(t)}
\]</span></p></li>
<li><p>这样计算的复杂度就升到了<span
class="math inline">\(O(V(G_1)V(G_2))\)</span>，但正是这逐点的比较能够区分那些细微的变化，而且可视化更加具有可解释性。所以该算法的使用场景应该是小图且对区分精度要求高</p></li>
</ul></li>
<li><p>对于这样的匹配问题可以用pair 或者triplet
loss，前者比较相似不相似，后者比较和两个候选相比跟哪个更相似，作者分别给出了两种形式下的margin
loss：</p>
<p><span class="math display">\[
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2}, t\right)}\left[\max
\left\{0, \gamma-t\left(1-d\left(G_{1},
G_{2}\right)\right)\right\}\right] \\
L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\max \left\{0, d\left(G_{1},
G_{2}\right)-d\left(G_{1}, G_{3}\right)+\gamma\right\}\right] \\
\]</span></p></li>
<li><p>作者还特意提到，为了加速运算，可以对Graph
Embedding做二值化处理，这样在衡量距离的时候就是用汉明距离，牺牲掉了一些欧式空间的部分，具体做法是将整个向量过tanh并作平均内积用作训练时的图相似度，并设计损失将正样本对的汉明距离推向1，负样本对的汉明距离推向-1，假如在推断检索下使用汉明距离进行检索，这样的损失设计比margin
loss更加稳定：</p>
<p><span class="math display">\[
s\left(G_{1}, G_{2}\right)=\frac{1}{H} \sum_{i=1}^{H} \tanh
\left(h_{G_{1} i}\right) \cdot \tanh \left(h_{G_{2} i}\right) \\
L_{\text {pair }}=\mathbb{E}_{\left(G_{1}, G_{2},
t\right)}\left[\left(t-s\left(G_{1}, G_{2}\right)\right)^{2}\right] / 4
\\
\begin{aligned} L_{\text {triplet }}=\mathbb{E}_{\left(G_{1}, G_{2},
G_{3}\right)}\left[\left(s\left(G_{1},
G_{2}\right)-1\right)^{2}+\right.\\\left.\left(s\left(G_{1},
G_{3}\right)+1\right)^{2}\right] / 8 \end{aligned} \\
\]</span></p>
<p>其中除以4或者除以8是为了约束损失的范围在[0,1]区间内。</p></li>
</ul>
<h1 id="learning-to-update-knowledge-graphs-by-reading-news">Learning to
Update Knowledge Graphs by Reading News</h1>
<ul>
<li>EMNLP2019的一项工作，作者肯定是个篮球迷，举了一个很恰当的NBA转会的例子来说明本文要解决的问题：知识图谱更新</li>
<li>比如发生了球员转俱乐部，则相关的两个俱乐部的球员图谱就会发生变化，作者提出了两个重点，如文中图1所示：
<ul>
<li>知识图谱的更新只发生在text subgraph而不是1-hop subgraph</li>
<li>传统方法不能从文本中获取隐藏的图谱更新信息，例如球员转会之后，这个球员的队友就会发生变化，这是文中没提但是可以推断出来的</li>
</ul></li>
<li>整体的结构是一个基于R-GCN和GAT的encoder和一个基于DistMult的decoder，基本上就是把RGCN改成了attention
based，decoder依然不变，做链接预测任务</li>
</ul>
<h2 id="encoder">encoder</h2>
<ul>
<li><p>encoder:RGCN+GAT=RGAT，在RGCN中前向过程为：</p>
<p><span class="math display">\[
\mathbf{H}^{l+1}=\sigma\left(\sum_{r \in \mathbf{R}}
\hat{\mathbf{A}}_{r}^{l} \mathbf{H}^{l} \mathbf{W}_{r}^{l}\right)
\]</span></p></li>
<li><p>即对异构的边分给予一个参数矩阵，独立的计算之后求和再激活。
将邻接矩阵改为注意力矩阵，注意力计算为：</p>
<p><span class="math display">\[
a_{i j}^{l r}=\left\{\begin{array}{ll}{\frac{\exp \left(a t t^{l
r}\left(\mathbf{h}_{i}^{l}, \mathbf{h}_{j}^{l}\right)\right)}{\sum_{k
\in \mathcal{N}_{i}^{r}} \exp \left(a t t^{l} r\left(\mathbf{h}_{i}^{l},
\mathbf{h}_{k}^{l}\right)\right)}} &amp; {, j \in \mathcal{N}_{i}^{r}}
\\ {0} &amp; {, \text { otherwise }}\end{array}\right.
\]</span></p></li>
<li><p>其中注意力函数<span
class="math inline">\(attn\)</span>基于文本计算</p>
<ul>
<li><p>首先用双向GRU对序列编码<span
class="math inline">\(u\)</span></p></li>
<li><p>再利用序列注意力得到上下文表示</p>
<p><span class="math display">\[
b_{t}^{l r}=\frac{\exp \left(\mathbf{u}_{t}^{T} \mathbf{g}_{t e x t}^{l
r}\right)}{\sum_{k=1}^{|S|} \exp \left(\mathbf{u}_{k}^{T} \mathbf{g}_{t
e x t}^{l r}\right)} \\
\mathbf{c}^{l r}=\sum_{t=1}^{|S|} b_{t}^{l r} \mathbf{u}_{t} \\
\]</span></p></li>
<li><p>之后利用注意力的时候，trainable guidance vector<span
class="math inline">\(g\)</span>就利用了这个上下文表示，利用一个简单的线性插值引入</p>
<p><span class="math display">\[
\mathbf{g}_{f i n}^{l r}=\alpha^{l r} \mathbf{g}_{g r a p h}^{l
r}+\left(1-\alpha^{l r}\right) \mathbf{U}^{l r} \mathbf{c}^{l r} \\
a t t^{l r}(\mathbf{h}_{i}^{l}, \mathbf{h}_{j}^{l}) =\mathbf{g}_{f i
n}^{l r}[\mathbf{h}_{i}^{l} | \mathbf{h}_{j}^{l}]  \\
\]</span></p></li>
</ul></li>
<li><p>在实际应用到作者想要完成的kg
update任务中，作者还引入了几个小技巧</p>
<ul>
<li>RGCN/RGAT中的参数量随着边(关系)的类别数量成线性增长，为了减少参数量，作者利用了basis-decomposition，也就是k类关系，存在k套参数，这k套参数用b套参数线性组合而成，而b小于k，这样来减少参数</li>
<li>实际数据集里实体之间的关系很稀疏，一两层的RGAT聚合不到消息，因此在构造数据集时首先对图中所有实体之间人为添加一个叫SHORTCUT的关系，并使用现成的信息抽取工具将SHORTCUT细化为add,delete和other，用来初步的判定人员的转会（从一个俱乐部delete，add到另一个俱乐部）关系</li>
</ul></li>
</ul>
<h2 id="decoder">decoder</h2>
<ul>
<li><p>在EMBEDDING ENTITIES AND RELATIONS FOR LEARNING AND INFERENCE IN
KNOWLEDGE
BASES一文中总结了知识图谱中的关系embedding学习问题，可以归结为不同的线性/双线性参数矩阵搭配不同的打分函数，计算margin
triplet loss： <a href="https://imgchr.com/i/lzxTht"><img data-src="https://s2.ax1x.com/2020/01/17/lzxTht.md.jpg"
alt="lzxTht.md.jpg" /></a></p></li>
<li><p>DistMult即最简单的，将双线性参数矩阵换成对角阵，即最后的分数是两个实体embedding逐元素相乘并加权求和得到，权重与关系相关，在本文中的具体实现为：</p>
<p><span class="math display">\[
P(y)=\operatorname{sigmoid}\left(\mathbf{h}_{i}^{T}\left(\mathbf{r}_{k}
\circ \mathbf{h}_{j}\right)\right)
\]</span></p></li>
</ul>
<h2 id="result">result</h2>
<ul>
<li>对比几个Baseline：RGCN,PCNN，感觉作者使用了GRU这样data-hungry的网络提取语义计算相似度，数据集偏小，当然最后结果还是很好看，可以看到数据集明显不平衡，但是在add和delete这些小样本类上RGAT比RGCN提升了一倍的准确率。</li>
<li>值得称赞的是论文将链接预测问题包装的很好，一个update突出了持续学习持续更新的想法，最后简化问题为链接预测，模型没有太多改进，但是效果达到了。</li>
</ul>
<h1
id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">Learning
Attention-based Embeddings for Relation Prediction in Knowledge
Graphs</h1>
<ul>
<li><p>依然是做链接预测，依然是基于GAT</p></li>
<li><p>作者认为在KG当中关系非常重要，但是又不好给边加特征，因此就曲线救国，将边的特征融入到节点的特征当中</p></li>
<li><p>一图胜千言 <a href="https://imgchr.com/i/1SLmi6"><img data-src="https://s2.ax1x.com/2020/01/18/1SLmi6.md.png"
alt="1SLmi6.md.png" /></a></p></li>
<li><p>从左到右</p>
<ul>
<li><p>输入，依然是节点特征输入GAT，只不过每个节点特征是与其相关的三元组特征做self
attention得到，而三元组特征由节点和关系特征拼接得到，绿色为输入关系特征，黄色为输入节点特征：</p>
<p><span class="math display">\[
c_{i j k}=\mathbf{W}_{1}\left[\vec{h}_{i}\left\|\vec{h}_{j}\right\|
\vec{g}_{k}\right] \\
\begin{aligned} \alpha_{i j k} &amp;=\operatorname{softmax}_{j
k}\left(b_{i j k}\right) \\ &amp;=\frac{\exp \left(b_{i j
k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}}
\exp \left(b_{i n r}\right)} \end{aligned} \\
\overrightarrow{h_{i}^{\prime}}=\|_{m=1}^{M} \sigma\left(\sum_{j \in
\mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right) \\
\]</span></p></li>
<li><p>之后经过GAT，得到灰色的中间层节点特征，两个3维灰色拼接是指GAT里multi-head
attention的拼接，之后两个6维灰色与绿色做变换之后拼接是指依然用三元组表示每个节点</p></li>
<li><p>最后一层，不做拼接了，做average
pooling，并且加入了输入节点特征，再拼接上关系特征，计算损失</p></li>
<li><p>损失依然用三元组距离，即subject+predicate-object，margin triplet
loss，负采样时随机替换subject或者object</p></li>
</ul></li>
<li><p>以上是encoder部分，decoder用ConvKB</p>
<p><span class="math display">\[
f\left(t_{i j}^{k}\right)=\left(\prod_{m=1}^{\Omega}
\operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k},
\vec{h}_{j}\right] * \omega^{m}\right)\right) \mathbf{. W} \\
\]</span></p></li>
<li><p>损失为soft-margin loss（好像1和-1写反了？）</p>
<p><span class="math display">\[
\begin{array}{l}{\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup
S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k}} *
f\left(t_{i
j}^{k}\right)\right)\right)+\frac{\lambda}{2}\|\mathbf{W}\|_{2}^{2}} \\
{\text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}{1} &amp;
{\text { for } t_{i j}^{k} \in S} \\ {-1} &amp; {\text { for } t_{i
j}^{k} \in S^{\prime}}\end{array}\right.}\end{array}
\]</span></p></li>
<li><p>另外作者还为2跳距离的节点之间加入了边</p></li>
<li><p>结果非常好，在FB15K-237和WN18RR上取得了SOTA。作者并没有试图将边的特征直接整合进GAT的message
passing，而是就把特征当成待训练的输入，用encoder专注于训练特征，并且在模型的每一层都直接输入的初始特征来保证梯度能够传递到原始输入。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>machine learning</tag>
        <tag>knowledge graph</tag>
      </tags>
  </entry>
  <entry>
    <title>[Some Questions asking Myself 2025.5]</title>
    <url>/2025/05/21/next-on-llm-2/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/0de1613151fa41695f480a9e134dc3f2.png" width="500"></p>
<p>The second post on my "some very-personal questions to myself"
series. It's been over a year since last post and many progress on LLM
have been made from academic/industry, which partially solves my
questions. I will introduce these works and ask myself some new
questions. This post is about Pretrain Ceiling, Second Half, Scaling the
Environment.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="questions-from-a-year-ago">Questions from a Year Ago</h1>
<h2 id="can-compression-solve-everything">Can Compression Solve
Everything?</h2>
<ul>
<li>One year later, it appears that mainstream AI research still adheres
to the LLM compression paradigm: using pretraining to compress world
knowledge, then relying on post-training to extract it.</li>
<li>As for whether LLMs can discover entirely new knowledge, research
has now largely shifted to the AI4Science domain.</li>
<li>Regarding the example from the previous blog post involving
mathematicians and biologists:
<ul>
<li>In foundational fields like mathematics, new breakthroughs can often
be achieved via interpolation within existing research ideas and
paradigms. For example, DeepMind’s <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a><sup class="refplus-num"><a href="#ref-alphaevolve">[2]</a></sup>
combines evolutionary algorithms with LLMs to discover new fast matrix
multiplication algorithms. The foundational algorithmic knowledge
required was already encoded in the model via compression. Through
prompt engineering and evolutionary iteration, the system uncovered
“low-hanging fruit” that humans had yet to explore.</li>
<li>In empirical sciences like biology, which rely heavily on large
amounts of new observations, an agentic approach can allow LLMs to
interact with the real world using tools to synthesize new knowledge. In
this paradigm, the LLM acts more like a scientist’s tool than a
replacement. Another path is to bypass the reasoning abilities of LLMs
altogether and build domain-specific models directly from field
data—like <a href="https://news.stanford.edu/stories/2025/02/generative-ai-tool-marks-a-milestone-in-biology-and-accelerates-the-future-of-life-sciences">Evo2</a><sup class="refplus-num"><a href="#ref-evo2">[3]</a></sup>,
which trains on genome sequences. For naturally sequential data (like
genomes), retraining domain-specific models makes sense; for
non-sequential data, one can structure it as text, using language models
for knowledge organization and reasoning.</li>
</ul></li>
</ul>
<h2 id="world-models-data-driven">World Models: Data-Driven?</h2>
<ul>
<li>There has been no substantial breakthrough in world modeling so
far.</li>
<li>Researchers have found that using LLMs to simulate both the world
and the agent requires <a href="https://arxiv.org/abs/2407.02446">different
capabilities</a><sup class="refplus-num"><a href="#ref-tradeoff_world_agent">[1]</a></sup>.</li>
<li>More practical progress lies in the domain of LLM Agents: using
models to construct interactive environments, such as video generation
models or 3D space models—collectively referred to as world models. This
reflects another extension trend in agent research: scaling the
environment.</li>
<li>In <em>Advances and Challenges in Foundation Agents</em>, scholars
provided a comprehensive overview of the <a href="https://arxiv.org/abs/2504.01990">current state of world model
research</a><sup class="refplus-num"><a href="#ref-foundation_agents">[6]</a></sup>
from the agent perspective; most current approaches rely on models or
external simulators and treat world models as single-task modeling
problems that ultimately reduce to traditional single-step
prediction.</li>
</ul>
<h2 id="the-bitter-lesson-of-agents">The "Bitter Lesson" of Agents?</h2>
<ul>
<li>The “bitter lesson” still holds true. For instance, <a href="https://arxiv.org/abs/2505.20286">Alita: Generalist
Agent</a><sup class="refplus-num"><a href="#ref-alita">[5]</a></sup>
minimizes prior design and maximizes freedom, autonomously building and
invoking MCP tools and achieving impressive results on the GAIA
platform.</li>
<li>Minimal priors and maximal freedom mean the agent’s capabilities are
internalized within the base model, requiring no additional framework or
scaffolding. We have yet to see truly “agent-native” application
scenarios.</li>
<li>Since the release of OpenAI’s o1 and DeepSeek, the industry
consensus is that even the most basic LLM responses can shift from
System 1 to System 2 reasoning.</li>
</ul>
<h2 id="alignment-and-feedback">Alignment and Feedback</h2>
<ul>
<li>As mentioned a year ago, post-training is essentially about steering
LLMs: traditional alignment sacrifices some capability to guide models
toward safer behavior; similarly, we can steer models toward more
intelligent yet more hallucination-prone behavior, as demonstrated by
DeepSeek R1’s "reasoning" capabilities.</li>
<li>Our understanding of post-training continues to deepen. Looking at
the <a href="https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge/">life
cycle of
LLMs</a><sup class="refplus-num"><a href="#ref-llm_knowledge_lifecycle">[7]</a></sup>,
we find that knowledge is hard to truly add, remove, or edit during the
post-training phase—retrieval is the most viable option. Hence, the
“incentive, not memorize” principle seems reasonable.</li>
<li>Pretraining has already endowed models with powerful knowledge.
Traditional supervised-learning-based post-training can only introduce
limited new knowledge. If we wish to add knowledge and capability, it’s
better to stick with next-token-prediction rather than simple
input-output pairs. So far, reinforcement learning appears to be a more
suitable direction for post-training, enabling models to explore
autonomously and truly grow in ability, rather than merely memorizing
narrowly defined tasks.</li>
<li>From a reward perspective, just as alignment uses human data to
train reward models and inject values, we can also design rule-based
rewards to encode natural laws into models.</li>
<li>Recent discussions about the <a href="https://arxiv.org/abs/2504.13837">limits of post-trained models
not exceeding pretraining
ceilings</a><sup class="refplus-num"><a href="#ref-reasoning_limit">[8]</a></sup>
show that if post-training is used only for steering without new
knowledge input, it aligns with entropy-reduction expectations. In RL
processes, the policy rolls out and only strengthens the good parts; if
the correct solution never appears, pretraining ceilings can’t be
breached. Traditional alignment data flywheels may face similar
bottlenecks, though their knowledge scope is so broad it’s hard to
investigate.</li>
</ul>
<h2 id="beyond-language">Beyond Language</h2>
<ul>
<li>The multimodal field has surged forward over the past year. Many
advances are not in foundational techniques but in improved user
experiences—combining multiple modalities better appeals to the senses.
For example, <a href="https://deepmind.google/models/veo/">Google
VEO3</a><sup class="refplus-num"><a href="#ref-google_veo3">[9]</a></sup>
can generate both video and audio simultaneously.</li>
<li>At the same time, we’ve seen fascinating new pretraining paradigms
such as <a href="https://arxiv.org/abs/2502.17437">fractal neural
networks</a><sup class="refplus-num"><a href="#ref-fractal_neural_net">[10]</a></sup>
and <a href="https://arxiv.org/abs/2502.09992">diffusion language
models</a><sup class="refplus-num"><a href="#ref-diffusion_lm">[11]</a></sup>.</li>
<li>What do additional modalities mean for reasoning capabilities? Most
researchers respond: using text reasoning capabilities for visual
reasoning. This isn’t about introducing new modalities for reasoning,
but feeding other modalities into text-based reasoning. What I hope to
see is “purely visual chain-of-thought” reasoning. Some recent efforts
like <a href="https://arxiv.org/abs/2505.11409">Visual Planning: Let’s
Think Only with
Images</a><sup class="refplus-num"><a href="#ref-visual_planning">[4]</a></sup>
attempt image-only reasoning. But such tasks are often limited to
navigation, maps, or Frozen Lake scenarios, and still require
interpreting intermediate images into action commands. To achieve true
“image-to-image” reasoning, clearer problem definitions and task setups
may be needed.</li>
</ul>
<hr>
<h1 id="new-questions">New Questions</h1>
<h2 id="beyond-pretraining-and-post-training">Beyond Pretraining and
Post-training</h2>
<ul>
<li>The field continues to explore scaling laws, revealing from
perspectives like <a href="https://arxiv.org/abs/2411.00660v2">communication</a><sup class="refplus-num"><a href="#ref-communication_scaling">[12]</a></sup>
and <a href="https://physics.allen-zhu.com/">physics</a><sup class="refplus-num"><a href="#ref-physics_llm">[13]</a></sup>
that knowledge can be transferred effectively to models via
next-token-prediction and that model capacity can be expressed
predictably.</li>
<li>Is it possible to go beyond the pretraining and post-training
paradigm to truly enable adding, deleting, updating, and querying
knowledge and capabilities? This is crucial for personalized LLMs.</li>
<li>Existing knowledge editing methods remain too simplistic and
intrusive.</li>
</ul>
<h2 id="self-evolution">Self-Evolution</h2>
<ul>
<li>Recently, research on "model self-evolution" has grown rapidly.
Nearly all unsupervised, weakly supervised, and self-supervised
post-training approaches claim self-evolution capabilities. But is this
truly self-evolution? Just as AlphaGo evolved through self-play, can
LLMs under RLVR paradigms achieve genuine self-evolution? Or is this
still just “self-entertainment” within the boundaries of
pretraining?</li>
</ul>
<h2 id="what-am-i-overlooking">What Am I Overlooking?</h2>
<ul>
<li>Both academia and industry are being driven by a blind arms race:
<ul>
<li>When industry makes a breakthrough with large-scale models or
reduces research costs, academia quickly follows to harvest the
benefits. Academia becomes the tester for breakthroughs made by
industry. From “Can LLM do sth?” benchmark-style papers to “XXPO”
studies applying DeepSeek GRPO to various domains, researchers now
sometimes don’t even test other domains—just overfit some math
benchmarks.</li>
<li>Industry too faces competitive pressure, like smartphone vendors
releasing new iPhones, LLM companies roll out new versions monthly. If
one company introduces a new model feature, competitors often replicate
it by the next release cycle. If a competitor can replicate it quickly,
it means the breakthrough falls within a foreseeable range of the
scaling law.</li>
<li>This causes researchers to be constrained by low-hanging fruit and
predictable problems, overlooking broader questions. Problems can remain
unsolved, but critical thinking should never stop. In this LLM era, what
overlooked domains are still worth examining?</li>
</ul></li>
<li>Don’t underestimate applications. Applications are the final link of
science serving society, and they can also reverse-inspire new research
trends. ChatGPT is a classic example: by combining a simple chat
interface with post-training, it brought the value of LLMs into the
homes of everyday users—and in turn spurred academic interest in LLM
research.</li>
<li>Why did I once overlook large models? And what am I overlooking
now?</li>
</ul>
<h2 id="if-llms-are-agi">If LLMs Are AGI</h2>
<ul>
<li>Then should we use LLMs to build real AGI applications? Suppose AGI
has arrived, and we can operate an entity equivalent to an ordinary
person or even superhuman. What valuable things could we attempt?</li>
<li>“Operate” sounds very negative, but operating a human being is
actually simple—it doesn’t require science. Many industries throughout
history have relied on humans functioning as operated entities to keep
running.</li>
<li>Our first thought is workers—blue-collar, white-collar, industry
employees—can they be replaced by AI? But there’s also a different
angle, like <a href="https://www.pnas.org/doi/10.1073/pnas.2407639121">recruiting
ancient human
subjects</a><sup class="refplus-num"><a href="#ref-ancient_human_subjects">[14]</a></sup>.
This line of thinking doesn’t just ask which jobs can be replaced, but
explores what things require humans yet are impossible for humans to
achieve—perhaps AI can step in.</li>
</ul>
<h2 id="the-second-half">“The Second Half”</h2>
<ul>
<li>Recent works such as <a href="https://ysymyth.github.io/The-Second-Half/">The Second
Half</a><sup class="refplus-num"><a href="#ref-the_second_half">[15]</a></sup>
and <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">Welcome
to the Era of
Experience</a><sup class="refplus-num"><a href="#ref-era_of_experience">[16]</a></sup>
suggest that research should shift from “how to solve problems” to “how
to define problems.”</li>
<li>I strongly agree, but I see this as a terminology update following
the rise of powerful LLMs—the paradigm itself hasn’t changed: we still
define tasks and solve them with models. What’s changed is that we’ve
moved from constructing datasets to designing new environments, and from
proposing new models to enabling models to learn online in environments
and outperform others. We’re not scaling datasets—we’re scaling
environments.</li>
<li>In what directions should we “scale the environments”?
<ul>
<li>A quality environment should generate infinite data; data volume
should no longer be the only axis of expansion.</li>
<li>The environment’s difficulty should become the focus of
expansion.</li>
<li>Beyond digital environments, real-world environments may be an
important milestone—for instance, agents physically building cities on
Earth.</li>
<li>Scientific environments may offer even higher ceilings.</li>
</ul></li>
<li>In this “second half,” is RL the only thing we need?</li>
</ul>
<h1 id="citation">Citation</h1>
<p>If you find this blog post interesting and wish to cite it, you may
use the following bibtex:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_on_llm_2025_5,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {[Some Questions asking Myself 2025.5] Pretrain Ceiling, Second Half, Scaling the Environment},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {5},</span><br><span class="line">  url = {https://thinkwee.top/2025/05/21/next-on-llm-2/},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="关于一年前的疑问">关于一年前的疑问</h1>
<h2 id="压缩能否解决一切">压缩能否解决一切？</h2>
<ul>
<li>时隔一年，目前看来，主流的 AI 研究依然未能突破 LLM
压缩理论的范式：依靠预训练来压缩世界知识，再通过后训练来提取这些知识。</li>
<li>关于 LLM 是否能够发现全新的知识，研究已聚焦于 AI4Science 领域。</li>
<li>针对上一篇博客中数学家与生物学家的例子：
<ul>
<li>数学等基础学科，往往可以基于现有研究思路与范式进行插值，从而取得新成果。例如
DeepMind 推出的 <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a><sup class="refplus-num"><a href="#ref-alphaevolve">[2]</a></sup>，通过结合进化算法与
LLM，发现了新的快速矩阵乘法算法。该方法所需的基础算法知识，已经以压缩形式编码于模型，通过
prompt engineering
与进化算法迭代，便可挖掘人类尚未充分探索的“漏网之鱼”。</li>
<li>对于生物学这类高度依赖大量新观测才能得出结论的经验科学，则可通过
agentic
方式，让大模型借助工具与真实世界交互，从而总结新知识。在此范式中，LLM
更像科学家的工具，而非替代者。另一种途径是摒弃语言大模型的推理能力，直接基于领域数据构建专用大模型，例如
<a href="https://news.stanford.edu/stories/2025/02/generative-ai-tool-marks-a-milestone-in-biology-and-accelerates-the-future-of-life-sciences">Evo2</a><sup class="refplus-num"><a href="#ref-evo2">[3]</a></sup>，以基因组序列训练模型。对于天然呈序列形式的数据（如基因组），适合重训领域专用模型；而对于非序列数据，则可将其整理为文本，借助语言模型进行知识整理与推理。</li>
</ul></li>
</ul>
<h2 id="世界模型以数据驱动">世界模型：以数据驱动？</h2>
<ul>
<li>世界模型方面尚未出现实质性进展。</li>
<li>研究者发现，用 LLM 同时模拟世界与 agent 需要<a href="https://arxiv.org/abs/2407.02446">不同的能力</a><sup class="refplus-num"><a href="#ref-tradeoff_world_agent">[1]</a></sup>。</li>
<li>更为实用的进展体现在 LLM Agent
领域：利用模型来构建与之交互的环境，如视频生成模型、3D
空间生成模型，将此类模型称为世界模型。这体现了 Agent
研究的另一种扩展趋势——scale the environment。</li>
<li>在《Advances and Challenges in Foundation Agents》中，学者们从 Agent
视角梳理了<a href="https://arxiv.org/abs/2504.01990">世界模型研究现状</a><sup class="refplus-num"><a href="#ref-foundation_agents">[6]</a></sup>；当前大多依赖模型或外部
simulator，从不同的 (state, observation, action)
拓扑结构出发，将世界模型视作单一任务的建模，而终归回到传统的单步预测。</li>
</ul>
<h2 id="智能体的苦涩教训">智能体的“苦涩教训”？</h2>
<ul>
<li>“苦涩教训”依然有效，例如 <a href="https://arxiv.org/abs/2505.20286">Alita: Generalist
Agent</a><sup class="refplus-num"><a href="#ref-alita">[5]</a></sup>，以最少的先验设计赋予模型最大自由度，自建并调用
MCP 工具，在 GAIA 平台上取得亮眼成果。</li>
<li>最少先验、最大自由度，意味着 Agent
能力被基础模型内化，无需额外框架或脚手架。迄今尚未出现真正“Agent
原生”（Agent-Native）的应用场景。</li>
<li>自 OpenAI o1 与 DeepSeek 以来，业界共识是：最朴素的 LLM
响应，能够从系统 1 过渡到系统 2。</li>
</ul>
<h2 id="对齐与反馈">对齐与反馈</h2>
<ul>
<li>正如一年前所述，post-training 本质上是为语言模型定向：传统的
alignment
用以牺牲部分能力，将模型引导至更安全的方向；同理，也可将模型引向更高智能却更易幻觉的方向，DeepSeek
R1 已展现此种“推理”能力。</li>
<li>对 post-training 的理解愈发深化。观察整个 LLM 的<a href="https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge/">生命周期</a><sup class="refplus-num"><a href="#ref-llm_knowledge_lifecycle">[7]</a></sup>，我们发现“知识”在后训练阶段难以真正增删改，仅适合检索。因此，“incentive,
not memorize” 的理念显得合理。</li>
<li>预训练已赋予模型强大知识，传统 supervised-learning-based 的
post-training
一方面能引入的知识有限，另一方面若要添加知识与能力，更应沿用
next-token-prediction，而非简单的 input-output
对；当前来看，强化学习是更适宜的 post-training
方向，通过目标奖励让模型自主探索，促进真正能力的增长，而非狭隘的记忆。</li>
<li>从奖励角度看，正如 alignment
可通过人类数据训练奖励模型注入价值观，我们亦可设计基于客观规则的
reward，将自然法则注入模型。</li>
<li>近期关于 post-trained model 无法超越 pre-trained <a href="https://arxiv.org/abs/2504.13837">上限的讨论</a><sup class="refplus-num"><a href="#ref-reasoning_limit">[8]</a></sup>亦表明：若后训练仅用于
steering 而不引入新知识，则符合熵减预期。RL 流程中，我们让 policy 自行
roll
out，再强化好的部分；因此若正确解法从未出现，则难以突破预训练上限。传统
alignment
数据飞轮或许也存在类似瓶颈，只是其涵盖的知识过于广泛，不易探查。</li>
</ul>
<h2 id="超越语言">超越语言</h2>
<ul>
<li>多模态领域在过去一年突飞猛进，许多进展并非底层技术突破，而是改善用户体验——融合多种模态更易打动感官，例如
<a href="https://deepmind.google/models/veo/">Google
VEO3</a><sup class="refplus-num"><a href="#ref-google_veo3">[9]</a></sup>，可同时生成视频与声音。</li>
<li>同时，出现了诸多有趣的新预训练范式，如<a href="https://arxiv.org/abs/2502.17437">分形神经网络</a><sup class="refplus-num"><a href="#ref-fractal_neural_net">[10]</a></sup>及<a href="https://arxiv.org/abs/2502.09992">扩散语言模型</a><sup class="refplus-num"><a href="#ref-diffusion_lm">[11]</a></sup>。</li>
<li>更多模态对推理能力有何意义？大多数研究者的回答是：将文本推理能力用于视觉推理。这并非为推理引入新模态，而是为文本推理输入其他模态。我期待的路径是“纯视觉链式推理”，近期已有工作如
<a href="https://arxiv.org/abs/2505.11409">Visual Planning: Let’s Think
Only with
Images</a><sup class="refplus-num"><a href="#ref-visual_planning">[4]</a></sup>，尝试通过纯图像进行推理。但此类任务多局限于导航、地图、Frozen
Lake
等场景，且依旧需从中间生成的图像解析行动指令。要实现纯粹的“图像到图像”推理，或需更明确的问题与任务定义。</li>
</ul>
<hr>
<h1 id="新的问题">新的问题</h1>
<h2 id="超越预训练与后训练">超越预训练与后训练</h2>
<ul>
<li>业界不断在 scaling law 方面探索，从<a href="https://arxiv.org/abs/2411.00660v2">通信</a><sup class="refplus-num"><a href="#ref-communication_scaling">[12]</a></sup>与<a href="https://physics.allen-zhu.com/">物理学</a><sup class="refplus-num"><a href="#ref-physics_llm">[13]</a></sup>等角度揭示：知识可通过
next-token-prediction 有效迁移至模型，并对容量进行可预测性表达。</li>
<li>有没有可能超越预训练和后训练范式，实现对知识与能力的真正增删改查？这对个性化
LLM 至关重要。</li>
<li>现有的知识编辑方法仍过于简单且侵入性强。</li>
</ul>
<h2 id="自我进化">自我进化</h2>
<ul>
<li>近期“模型自我进化”研究日益增多，几乎所有无监督、弱监督、自监督的
post-training 均声称能自我进化。它们是真正的自进化吗？如同 AlphaGo
的自对弈，LLM 在 RLVR
范式下能否实现同样的自进化？还是说，这不过还是在预训练范围内的“自娱自乐”？</li>
</ul>
<h2 id="我忽视了什么">我忽视了什么</h2>
<ul>
<li>学术界与工业界均被盲目的军备竞赛所牵引：
<ul>
<li>一旦工业界在大规模模型上取得新突破，或在研究成本上作出有效降低，学术界便会紧随其后，啃食红利。学术界沦为工业界突破后，在各领域的测试者。从“Can
LLM do sth?” 类型的 Benchmark 文章，到将 DeepSeek GRPO
应用于各领域的“XXPO”研究，甚至近期都不愿意在其他领域做一个测试者，而是仅仅过拟合一些数学领域的benchmark。</li>
<li>工业界亦面临竞争压力，如同手机厂商不断推出新
iPhone，大模型厂商以月为单位发布新版本的模型。一家厂商模型推出的新功能，往往在下一发布季度就被友商复现。问题在于，若能被友商迅速复现，说明其技术突破在可预见的
scaling law 射程范围内。</li>
<li>这导致研究者被低悬果实与可预测问题局限，忽视更宽广的议题。问题可暂未解决，但思考不可放弃：在当下
LLM 时代，还有哪些值得审视的被忽略领域？</li>
</ul></li>
<li>别轻视应用。应用是科学服务社会的最终环节，也可反向引领研究新潮流。ChatGPT
即是经典案例，通过简易对话界面与 post-training，将 LLM
的价值以普罗大众可接受的方式推向千家万户，反过来促进学术界对 LLM
的研究。</li>
<li>我自己为何曾忽视大模型？而今又在忽视什么？</li>
</ul>
<h2 id="如果-llm-是-agi">如果 LLM 是 AGI</h2>
<ul>
<li>那么，我们是否该用 LLM 构建真正的 AGI
应用？设想AGI已经出现，我们可以操纵一个普通人乃至超人类的实体，我们能做何有益尝试？</li>
<li>操纵这个词听起来非常的负面，但其实操纵一个人类非常简单，无需科学。自古以来很多产业也需要人类以一个被操作的实体来工作，来运转。</li>
<li>我们第一想到的是工人，是白领，是行业里的打工人，能否被AI替代。但其实也有完全不同的角度，例如<a href="https://www.pnas.org/doi/10.1073/pnas.2407639121">招募古代人类被试</a><sup class="refplus-num"><a href="#ref-ancient_human_subjects">[14]</a></sup>。这个角度所代表的方向不是仅仅考虑哪些岗位能被替代，而是考虑用AI做那些需要人类才能做好，但无法由人类完成的事。</li>
</ul>
<h2 id="下半场">“下半场”</h2>
<ul>
<li>近期诸如<a href="https://ysymyth.github.io/The-Second-Half/">The
Second
Half</a><sup class="refplus-num"><a href="#ref-the_second_half">[15]</a></sup>及<a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">Welcome
to the Era of
Experience</a><sup class="refplus-num"><a href="#ref-era_of_experience">[16]</a></sup>等文章指出：研究应从“如何解决问题”转向“如何定义问题”。</li>
<li>我非常赞同，但我觉得这是当 LLM
足够强大后，研究术语的更新，范式仍旧未变：提出任务并通过模型解决。不同的是，从构造数据集转向设计新环境，从提出新模型转向让模型在环境中在线学习并胜出。我们要scale的不是dataset，而是environment</li>
<li>我们应朝哪些方向“scale the environments”？
<ul>
<li>优质环境应能生成无限数据，数据量不应再是唯一的扩展维度。</li>
<li>环境的难度更应成为扩展目标。</li>
<li>超越数字环境，现实环境或是重要里程碑，例如让 agent
在地球上实际建立城市。</li>
<li>而科学环境或许有更高上限。</li>
</ul></li>
<li>在这“下半场”，RL 是否为唯一所需？</li>
</ul>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_on_llm_2025_5,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {[Some Questions asking Myself 2025.5] Pretrain Ceiling, Second Half, Scaling the Environment},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {5},</span><br><span class="line">  url = {https://thinkwee.top/2025/05/21/next-on-llm-2/},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>
<ul id="refplus"><li id="ref-tradeoff_world_agent" data-num="1">[1]  Li, M., Shi, W., Pagnoni, A., West, P., &amp; Holtzman, A. (2024). Predicting vs. Acting: A Trade-off Between World Modeling &amp; Agent Modeling. In arXiv: Vol. abs/2407.02446. https://doi.org/10.48550/ARXIV.2407.02446</li><li id="ref-alphaevolve" data-num="2">[2]  Google DeepMind. (2025). AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms. https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/</li><li id="ref-evo2" data-num="3">[3]  Stanford University. (2025). Evo2: Generative AI tool marks a milestone in biology and accelerates the future of life sciences. https://news.stanford.edu/stories/2025/02/generative-ai-tool-marks-a-milestone-in-biology-and-accelerates-the-future-of-life-sciences</li><li id="ref-visual_planning" data-num="4">[4]  Xu, K., Wang, Y., Zhang, R., &amp; Chen, X. (2025). Visual Planning: Let's Think Only with Images. In arXiv: Vol. abs/2505.11409. https://arxiv.org/abs/2505.11409</li><li id="ref-alita" data-num="5">[5]  Qiu, H., Xiao, C., Yang, Y., Wang, H., &amp; Zheng, L. (2025). Alita: Generalist Agent with Minimal Predefinition and Maximal Self-Evolution. In arXiv: Vol. abs/2505.20286. https://arxiv.org/abs/2505.20286</li><li id="ref-foundation_agents" data-num="6">[6]  Liu, M., Tian, Y., Yang, S., Chen, B., &amp; Zhou, B. (2025). Advances and Challenges in Foundation Agents. In arXiv: Vol. abs/2504.01990. https://arxiv.org/abs/2504.01990</li><li id="ref-llm_knowledge_lifecycle" data-num="7">[7]  Zhang, S., Chen, J., &amp; Wang, W. Y. (2025). The LLM Knowledge Lifecycle: An AAAI 2025 Tutorial. https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge/</li><li id="ref-reasoning_limit" data-num="8">[8]  Wei, J., Chen, X., &amp; Bubeck, S. (2025). Can reasoning emerge from large language models? Investigating the limits of reasoning capabilities in pre-trained and fine-tuned models. In arXiv: Vol. abs/2504.13837. https://arxiv.org/abs/2504.13837</li><li id="ref-google_veo3" data-num="9">[9]  Google DeepMind. (2025). Veo3: Advancing text-to-video generation with synchronized audio. https://deepmind.google/models/veo/</li><li id="ref-fractal_neural_net" data-num="10">[10]  Chen, L., Dai, Y., &amp; He, K. (2025). Fractal Neural Networks: Scaling Deep Learning Beyond Linear Paradigms. In arXiv: Vol. abs/2502.17437. https://arxiv.org/abs/2502.17437</li><li id="ref-diffusion_lm" data-num="11">[11]  Yang, M., Tian, Y., &amp; Lin, Z. (2025). Diffusion Language Models: Toward Controllable Text Generation with Guided Diffusion. In arXiv: Vol. abs/2502.09992. https://arxiv.org/abs/2502.09992</li><li id="ref-communication_scaling" data-num="12">[12]  Rao, S., Knight, W., &amp; Sutskever, I. (2024). Scaling Laws from an Information-Theoretic Perspective. In arXiv: Vol. abs/2411.00660. https://arxiv.org/abs/2411.00660v2</li><li id="ref-physics_llm" data-num="13">[13]  Allen-Zhu, Z., &amp; Li, Y. (2025). On the Connection between Physical Laws and Neural Scaling Laws. https://physics.allen-zhu.com/</li><li id="ref-ancient_human_subjects" data-num="14">[14]  Jiang, L., Cohen, J., &amp; Griffiths, T. L. (2024). Recruiting ancient human subjects with large language models. Proceedings of the National Academy of Sciences, 121(21), e2407639121. https://www.pnas.org/doi/10.1073/pnas.2407639121</li><li id="ref-the_second_half" data-num="15">[15]  Chen, K., Liu, H., &amp; Zhang, D. (2025). The Second Half: From Solving Problems to Defining Problems. https://ysymyth.github.io/The-Second-Half/</li><li id="ref-era_of_experience" data-num="16">[16]  DeepMind. (2025). Welcome to the Era of Experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf</li><li id="ref-previous_post" data-num="17">[17]  Liu, W. (2024). [Some Questions asking Myself 2024.4] Compression, World Model, Agent and Alignment. https://thinkwee.top/2024/04/23/next-on-llm/</li></ul>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>MyQuestion</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>llm</tag>
        <tag>agent</tag>
        <tag>inference</tag>
        <tag>questions</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>What is the Next Step for Scaling in the Era of RL for LLM?</title>
    <url>/2025/07/15/next-scaling-202507/</url>
    <content><![CDATA[<p><a href="https://imgse.com/i/pV1R7X8"><img data-src="https://s21.ax1x.com/2025/07/16/pV1R7X8.md.webp" alt="pV1R7X8.md.webp"></a></p>
<p>When the redundant designs we added in the pre-LLM era have been
deleted by the bitter lesson, we are ready to scale up. In the era of RL
for LLM, what should be the next scaling up?</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="towards-general-reasoning">Towards General Reasoning</h1>
<ul>
<li>I've been following recent developments on Agents and the line of
work on Incentive Training for Reasoning sparked by DeepSeek. A few days
ago, the release of Kimi K2 caught my attention, particularly its
section on general reasoning:</li>
</ul>
<blockquote>
<p>
Going beyond verifiable rewards, our general RL system uses a
self-judging mechanism where the model acts as its own critic, providing
scalable, rubric-based feedback for non-verifiable tasks.
</p>
<footer>
<strong>[<a href="https://moonshotai.github.io/Kimi-K2/">Kimi
K2</a>]</strong>
</footer>
</blockquote>
<ul>
<li>It was at this moment that I realized the dots from the recent
papers I had been reading started to connect—just as Steve Jobs once
said, “You can't connect the dots looking forward; you can only connect
them looking backwards. So you have to trust that the dots will somehow
connect in your future.”</li>
<li>This post organizes those thoughts, as illustrated in the diagram
above: the scaling of large models has never stopped—we have continued
to scale up the knowledge and capabilities learned from next token
prediction, in various ways and from different angles.</li>
</ul>
<h1 id="next-token-prediction">Next Token Prediction</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/5c71b7331c275cbc784bd315744eb000.webp" width="800"></p>
<ul>
<li>At the top, <em>Next token prediction is all you need</em>. A
general consensus seems to be that knowledge is primarily acquired
during pretraining. Post-training mainly serves to steer/incentivize
capabilities. Continuing pretraining remains the preferred way to encode
knowledge into the model. Trying to add/edit/delete knowledge during
post-training is extremely challenging, and research on knowledge
editing has remained relatively toy-level.
<blockquote>
<p>
Knowledge Is Stored Messily in LLMs
</p>
<footer>
<strong>[<a href="https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge/">Life
Cycle of LLM Knowledge</a>]</strong>
</footer>
</blockquote></li>
<li>Once we clarify the roles of pretraining and post-training, we find
that both are worth scaling. When the scaling of pretraining hits a
bottleneck, OpenAI proposed <em>inference-time scaling</em>—which
actually relies on post-training to elicit capabilities. In this sense,
it is really scaling up post-training.</li>
</ul>
<h1 id="scaling-up-human-feedback-with-reward-models">Scaling Up Human
Feedback with Reward Models</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/4bdc98917e64f45c74bb2272c5225fae.webp" width="800"></p>
<ul>
<li>Moving to the lower left is the RLHF path proposed by OpenAI.
Besides highlighting the importance of human feedback (beyond objective
correctness, there's also the distinction between hard-to-verify good vs
bad), I believe it importantly demonstrates how reward models can be
used to scale up human feedback. It's unrealistic to have humans
annotate massive amounts of model rollouts, but we can use a small
amount of high-quality human data to train a reward model, which can
then provide feedback at scale for policy model rollouts. This is
essentially a tradeoff: sacrificing precision (human-labeled quality)
for quantity (reward model scalability). A reward model trained on a
small dataset is sufficient to guide a strong policy—because posing a
question is easier than solving it (Discriminator-Generator Gap). Once
this cold start is done, OpenAI productized it (ChatGPT), enabling
continuous data collection and spinning up a data flywheel.</li>
</ul>
<h1 id="verifiable-rewards">Verifiable Rewards</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/983b3a2577db47a72f6f30615fff2430.webp" width="800"></p>
<ul>
<li>Moving rightward, we see RLVR—DeepSeek’s exploration of end-to-end
RL with outcome rewards, undertaken in the broader effort to replicate
OpenAI’s o1 model. DeepSeek’s work has offered me three insights:
<ul>
<li>From DeepSeekMath to DeepSeekR1, it showed that when the pretrained
checkpoint is strong enough, RL still has enormous potential—not only
can it solve math proofs, it can also elicit general reasoning
capabilities;</li>
<li>Simple, rule-based rewards can directly be used to train language
models. This expands the range of possible environments for RL with
LLMs, enabling them to serve as general-purpose models across
tasks;</li>
<li>With GRPO/Rule Reward, DeepSeek removed the need for a critic model
and a reward model, making the approach extremely simple. The early
media narrative focused on lower costs, but I believe the greater
potential lies in higher efficiency scaling.</li>
</ul></li>
</ul>
<h1 id="llms-implicitly-contain-reward-models">LLMs Implicitly Contain
Reward Models</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/51c2cec1e457b7dbdfea261bacda5363.webp" width="800"></p>
<ul>
<li>Below RLHF, we see the realization that LLM-based policy models
inherently contain reward models:
<ul>
<li>The intuition behind DPO is straightforward: if I follow a pipeline
of [train reward model on high-quality human preference data → train
policy model using reward model], then surely there exists a way to
directly train the policy model using high-quality preference data. DPO
mathematically proves this. Although it overlooks on-policy vs
off-policy distinctions, DPO offers a key insight: in RLHF
post-training, an LLM policy model might also be a reward model;</li>
<li>There's a paper not included in the diagram—PRIME: Process
Reinforcement through Implicit Rewards by Tsinghua—which extends the
implicit reward concept from DPO into outcome-reward tasks, extracting
process reward signals. While PRIME is not central to this post, it’s
very interesting, and a future combining outcome + process rewards could
be promising;</li>
<li>Finally, we have <em>Generalist Reward Models: Endogenous
Reward</em>. Next token prediction on massive human-approved corpora is
itself a way of learning a reward function. This continues DPO’s idea:
LLMs are both policy and reward models, not just in post-training, but
even during pretraining.</li>
</ul></li>
</ul>
<h1 id="llms-implicitly-contain-verifier-models">LLMs Implicitly Contain
Verifier Models</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/41910a45eba55627de00243f82207fdc.webp" width="800"></p>
<ul>
<li>Further right, mirroring the idea of Secret Reward Model under RLHF,
is the RLVR counterpart: in RLVR, the LLM itself serves as a verifier
model. This includes our recent work NOVER and several related papers.
The motivation is straightforward: RLVR depends on verifiable tasks, but
what if we only have freeform ground truth, which can’t be rule-based
verified? A natural idea is to use the model's perplexity on those
ground truth samples as reward. For incentive training, we can condition
on the reasoning trajectory and compute perplexity. The idea is simple,
but echoing Secret Reward Model, it supports a broader claim: whether
RLHF or RLVR, alignment or incentive training, LLMs themselves are
sufficient feedback signal extractors. In RL terms, LLMs are good enough
as both policy and reward.</li>
</ul>
<h1 id="scaling-up-reinforcement-learning">Scaling Up Reinforcement
Learning</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/3f4d3619d0bf4112fbc0386139871937.webp" width="800"></p>
<ul>
<li>At the bottom, we see the community’s recent efforts in scaling RL
for LLMs:
<ul>
<li>DeepSeek-GRM emphasizes that reward models are worth scaling
up;</li>
<li>ProRL suggests that RL training itself is worth scaling, with
potential to surpass the pretrained ceiling;</li>
<li>POLAR argues that reward models should not only be scaled up, but
done so with pretraining-level scale.</li>
</ul></li>
</ul>
<h1 id="converging-to-a-single-point">Converging to a Single Point</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/298e8cda183da68a8d7f44739a1cde8e.webp" width="800"></p>
<ul>
<li>Looking back at the road we've traveled, we see everything converges
to one point:
<ul>
<li>Reward models should be scaled up</li>
<li>Post-training should be scaled up</li>
<li>LLMs themselves are reward models</li>
</ul></li>
<li>--&gt; We only need to scale up the LLM itself! It is both policy
and reward, and RL enhances its capabilities in both roles. A stronger
reward provides better guidance for policies tackling harder problems.
The simplest form accommodates the widest range of data and compute.
Once tasks and data are well-prepared, everything clicks into gear,
spinning faster and leveraging bigger levers. This is the insight
conveyed by Kimi K2’s section on general reasoning. As Hyung Won Chung’s
slide suggests—less structure, more performance. We began by adding
various models and structures, and now we’re removing them one by
one:</li>
</ul>
<p><img data-src="https://i.mji.rip/2025/07/16/23a1a1ebeb4308046cb91071b0afa865.webp" width="800"></p>
<blockquote>
<p>
As a community we love adding structures but a lot less for removing
them. We need to do more cleanup.
</p>
<footer>
<strong>[<a href="https://x.com/hwchung27/status/180067631291665659">Hyung Won
Chung's tweet on Less Structure</a>]</strong>
</footer>
</blockquote>
<h1 id="the-next-step">The Next Step</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/ecb64f8e8fd05a37beaa567f2e1c4fff.webp" width="400"></p>
<ul>
<li>So what should we scale next? And how?</li>
<li>One seemingly obvious answer is: from training to inference, to
agentic, to multiagent—the next step is scaling up multiagent. (Grok 4
Heavy may currently be the strongest multiagent model/framework;
multiagent has also become one of 2025’s hottest AI terms.)</li>
</ul>
<blockquote>
<p>
We have made further progress on parallel test-time compute, which
allows Grok to consider multiple hypotheses at once. We call this model
Grok 4 Heavy, and it sets a new standard for performance and
reliability.
</p>
<footer>
<strong>[<a href="https://x.ai/news/grok-4">Grok 4</a>]</strong>
</footer>
</blockquote>
<ul>
<li>But why? Just because the terms are newer? My view is: we’re not
scaling the terms or paradigms themselves. When new paradigms emerge,
we’re keen to build around them—but ultimately, the benefits are
internalized by the model itself. Scaling laws remain faithful to data
and compute, only now they come in different forms.</li>
<li>When identifying the next direction for scaling, it's not just about
agentic or multiagent formats—but where the data comes from to support
such scaling. Some scattered thoughts:
<ul>
<li>Synthetic data is inevitable, but I suspect it will show patterns
similar to the Discriminator-Generator Gap, enabling tradeoffs to
continually produce harder and better data;</li>
<li>In RL contexts, data may also appear as environments. A well-defined
environment can theoretically generate near-infinite data, meaning that
the next phase is not just scaling <em>amount</em>, but scaling
<em>difficulty</em>—an environment that assigns harder goals to stronger
agents.
<blockquote>
<p>
Why you should stop working on RL research and instead work on product
// The technology that unlocked the big scaling shift in AI is the
internet, not transformers
</p>
<footer>
<strong>[<a href="https://x.com/_kevinlu/status/1942977315031687460">Kevin's tweet
on Stop Working on RL</a>]</strong>
</footer>
</blockquote></li>
<li>I agree with the training → inference → agentic → multiagent scaling
roadmap, not just because those terms aim for higher intelligence, but
because that path makes LLMs increasingly <em>useful</em>. And
usefulness brings a key benefit: people are willing to provide more and
diverse data to use it.</li>
<li>For multiagent, I'm particularly interested in heterogeneous,
personal multiagent systems. LLMs have nearly exhausted all knowledge on
the internet, which reflects an average of humanity’s collective
information. But for individuals, each person’s micro-world continues to
generate virtually infinite data and environments. Assigning each person
an agent, and allowing society to mirror itself with a society of agents
evolving through this data, may be how multiagent scaling becomes
possible.</li>
</ul></li>
</ul>
<h1 id="citation">Citation</h1>
<p>If you found the topics in this blog post interesting and would like
to cite it, you may use the following BibTeX entry: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_scaling_202507,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {What is the Next Step for Scaling in the Era of RL for LLM?},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {7},</span><br><span class="line">  url = {https://thinkwee.top/2025/07/15/next-scaling-202507/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="迈向通用推理">迈向通用推理</h1>
<ul>
<li>最近一直在关注Agent以及由DeepSeek引发的一系列Incentive Training for
Reasoning工作。直到前几天Kimi K2的发布，瞅了一眼其general
reasoning的报告部分，如下：</li>
</ul>
<blockquote>
<p>
Going beyond verifiable rewards, our general RL system uses a
self-judging mechanism where the model acts as its own critic, providing
scalable, rubric-based feedback for non-verifiable tasks.
</p>
<footer>
<strong>[<a href="https://moonshotai.github.io/Kimi-K2/">Kimi
K2</a>]</strong>
</footer>
</blockquote>
<ul>
<li>这时察觉近期阅读的一系列研究串了起来，就像乔布斯说的，“You can't
connect the dots looking forward; you can only connect them looking
backwards. So you have to trust that the dots will somehow connect in
your future.”</li>
<li>本文将其整理了一下，就像上图所示：大模型的scaling从未停止，我们一直以不同的方式，从不同角度继续scale
up从next token prediction里学习到的知识与能力。</li>
</ul>
<h1 id="下一个token预测">下一个token预测</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/5c71b7331c275cbc784bd315744eb000.webp" width="800"></p>
<ul>
<li>最上方，Next token prediction is all you
need。应该是大家的一点共识是，知识主要从预训练获取。后训练更多的是对能力的steering/incentivize。将知识训进模型优先的选择还是continue
pretraining。如果在后训练去做知识的增删改查会非常困难，知识编辑之类的研究也一直处于比较toy的状态。
<blockquote>
<p>
Knowledge Is Stored Messily in LLMs
</p>
<footer>
<strong>[<a href="https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge/">Life
Cycle of LLM Knowledge</a>]</strong>
</footer>
</blockquote></li>
<li>在分清楚预训练和后训练的角色之后，我们发现两部分都值得scale
up。预训练的scaling遇到瓶颈之后，OpenAI提出来inference time
scaling，但其实inference time
scaling依赖后训练对能力的激发，这实际上是在scale up后训练。</li>
</ul>
<h1 id="用奖励模型scale-up人类反馈">用奖励模型scale up人类反馈</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/4bdc98917e64f45c74bb2272c5225fae.webp" width="800"></p>
<ul>
<li>往左下，即OpenAI提出的RLHF路线。这条路线除了证明human
feedback的重要性（在客观正确之上，还有hard-to-verify好坏的差别）之外，我个人认为还有一点很重要，即用reward
model去scale up human feedback。让人类打标海量的model
rollouts是不现实的，但是我们可以用少量的高质量人工数据训练一个奖励模型，然后奖励模型可以给海量的policy
model
rollouts提供反馈。这实际上是一种tradeoff，牺牲精度（人类标注的质量）换取数量（reward
model标注的scalability）。用少量数据训练的reward
model足以指导强大的policy，因为提出问题总比解决问题简单（Discriminator-Generator
Gap）。当这个冷启动完成之后，OpenAI将其产品化（ChatGPT），从而可以不断地收集新的用户数据，完成数据飞轮。</li>
</ul>
<h1 id="可验证奖励">可验证奖励</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/983b3a2577db47a72f6f30615fff2430.webp" width="800"></p>
<ul>
<li>往右，是RLVR，即DeepSeek在全员复现OpenAI
o1的背景下，探索出的一条路径：借助端到端的RL with outcome
reward，可以将推理能力内生化到模型当中。DeepSeek给我带来的启发有三：
<ul>
<li>从DeepSeekMath到DeepSeekR1，证明了pretrained
checkpoint足够好的情况下，RL的挖掘空间还很大，不仅仅是可以做数学证明，还能激发出通用推理能力；</li>
<li>基于规则的简单奖励也可以直接用来训练语言模型，这一下拓宽了RL for
LLM环境的范围，使得LLM作为一个通用模型能够借助强化学习胜任各种任务；</li>
<li>借助GRPO/Rule Reward，DeepSeek一下子拿掉了critic model和reward
model，变得极度简洁。极度简洁带来的好处早期媒体都报道为降低成本，但我想更大的潜力在于能够以更高的效率scale
up。</li>
</ul></li>
</ul>
<h1 id="llm隐含了奖励模型">LLM隐含了奖励模型</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/51c2cec1e457b7dbdfea261bacda5363.webp" width="800"></p>
<ul>
<li>RLHF往下，是人们发现基于LLM的policy model本身就隐含了奖励模型。
<ul>
<li>DPO的直觉非常简单，我使用[高质量人类标注偏好数据训练奖励模型--&gt;用奖励模型训练策略模型]的pipeline，那么肯定能用一种方式将这个pipeline直接表达为用高质量人类标注偏好数据训练policy
model。DPO数学上证明了这一点。虽然其忽视了on-policy/off-policy的区别，但DPO提供了一种思想，在RLHF的后训练中，一个LLM
policy model可能本身也是一个reward model；</li>
<li>中间有一篇我没有在图中标出的paper，即清华的PRIME: Process
Reinforcement through Implicit Rewards。基于DPO提出的implicit
reward思想，我们可以将其在outcome reward任务中训练，然后提取出process
reward
signals。虽然PRIME不在本文讨论的主线里，但是非常有意思，未来outcome+process
reward的结合可能也是一条路径；</li>
<li>最后就是最近的Generalist Reward Models：Endogenous
Reward。在海量人类（认为正确的）语料上进行next token
prediction本身就是在学习一个reward
function。这延续了DPO的想法，不仅仅在后训练，在预训练上LLM就可能同时是policy
&amp; reward model。</li>
</ul></li>
</ul>
<h1 id="llm隐含了验证模型">LLM隐含了验证模型</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/41910a45eba55627de00243f82207fdc.webp" width="800"></p>
<ul>
<li>再往右，是RLVR下方的和Secret Reward
Model互为镜像的观点：对于RLHF，LLM本身就是一个reward
model；对于RLVR，LLM本身就是一个verifier
model。这里包括了我们最近提出的工作NOVER，以及一系列相关工作。这一系列paper的动机和观察很简单：RLVR依赖可验证的任务，如果我只有freeform
ground truth，无法rule-based
verify，那怎么办呢？一个很直觉的想法就是用policy model在这些ground
truth上的ppl作为奖励，如果是incentive training，那就用conditioned on
reasoning trajectory的ppl作为奖励。想法很简单，但这和Secret Reward
Model遥相呼应证明了一个观点：无论是RLHF还是RLVR，无论是alignment还是incentive
training，LLM本身就足以成为一个足够好的反馈信号提取器。在强化学习的语境下，LLM本身就是足够好的policy
&amp; reward。</li>
</ul>
<h1 id="scale-up强化学习">Scale up强化学习</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/3f4d3619d0bf4112fbc0386139871937.webp" width="800"></p>
<ul>
<li>最后下方，是大家最近在RL for LLM里发力的一件事，也就是如何scaling
RL:
<ul>
<li>DeepSeek-GRM强调Reward Model是值得scale up的；</li>
<li>ProRL认为RL training是值得scale up的，并且有希望突破pretrained
ceiling；</li>
<li>POLAR说Reward Model不仅值得scale up，而且值得用预训练的规格scale
up</li>
</ul></li>
</ul>
<h1 id="汇聚一点">汇聚一点</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/298e8cda183da68a8d7f44739a1cde8e.webp" width="800"></p>
<ul>
<li>这时我们回过头看走来的路，就会发现一切回归到了一个点：
<ul>
<li>Reward Model应该scale up</li>
<li>后训练应该scale up</li>
<li>LLM本身就是Reward Model</li>
</ul></li>
<li>--&gt;我们只需要scale up
LLM本身！它既是policy，也是reward，而RL能够增强它作为两者的能力，越强的reward也能在更难的问题上给予policy更好的指导。最简洁的形态能够实现对数据和算力的最大包容。只要将任务、数据整理恰当，一切就如同齿轮一下能够转起来，越转越快，撬动更大的杠杠。这就是上图里Kimi
K2关于general reasoning部分的insight。正如Hyung Won
Chung那张ppt所言，less structure, more
performance，我们最开始添加了各种模型，各种架构，然后我们再一一删掉：</li>
</ul>
<p><img data-src="https://i.mji.rip/2025/07/16/23a1a1ebeb4308046cb91071b0afa865.webp" width="800"></p>
<blockquote>
<p>
As a community we love adding structures but a lot less for removing
them. We need to do more cleanup.
</p>
<footer>
<strong>[<a href="https://x.com/hwchung27/status/180067631291665659">Hyung Won
Chung's tweet on Less Structure</a>]</strong>
</footer>
</blockquote>
<h1 id="下一步">下一步</h1>
<p><img data-src="https://i.mji.rip/2025/07/16/ecb64f8e8fd05a37beaa567f2e1c4fff.webp" width="400"></p>
<ul>
<li>那下一步，我们该scale什么？以何种方式scale up？</li>
<li>一个看似显然的观点是，从training到inference，再到agentic，再到multiagent，下一步我们应该scaling
up multiagent（Grok 4
Heavy应该是最近最强的multiagent模型/框架，multiagent也是2025
AI最火热的词之一）。</li>
</ul>
<blockquote>
<p>
We have made further progress on parallel test-time compute, which
allows Grok to consider multiple hypotheses at once. We call this model
Grok 4 Heavy, and it sets a new standard for performance and
reliability.
</p>
<footer>
<strong>[<a href="https://x.ai/news/grok-4">Grok 4</a>]</strong>
</footer>
</blockquote>
<ul>
<li>但是为什么？因为这些名词一个比一个新吗？我的想法是，scale
up的不是这些名词，这些范式。新的范式兴起时，我们热衷于为其添砖加瓦，但最终都被模型所内化，让模型受益于scaling。Scaling
law依然忠实于数据和算力，只不过数据和算力以不同的形式呈现。</li>
<li>在关注下一个scaling的方向时，我们关注它是以agentic/multiagent的形式出现，更要关注支持这些形式来做scaling的数据从哪来。个人的一些零碎的想法是：
<ul>
<li>合成数据不可避免，但我猜想合成数据也会出现类似Discriminator-Generator
Gap的情况，让我们能够利用一些tradeoff来不断产生更难、更好的数据。</li>
<li>在强化学习的语境下，数据还可能以环境的形式出现。良好定义的环境理论上能产生近似无限的数据，因此下一阶段不仅仅是scale数量，更要scale难度，例如一个能够对越优秀的agent分配更难目标的环境。
<blockquote>
<p>
Why you should stop working on RL research and instead work on product
// The technology that unlocked the big scaling shift in AI is the
internet, not transformers
</p>
<footer>
<strong>[<a href="https://x.com/_kevinlu/status/1942977315031687460">Kevin's tweet
on Stop Working on RL</a>]</strong>
</footer>
</blockquote></li>
<li>我赞同training--&gt;inference--&gt;agentic--&gt;multiagent的scaling路线，不仅仅因为这些名词被创造时就aim更高的智能，而是因为这个路线代表LLM能够越来越有用。有用的一个好处在于，人们愿意为LLM提供更多的、更不同的数据来使用它。</li>
<li>对于MultiAgent，我感兴趣的是heterogeneous的，personal的multiagent系统。LLM几乎耗光了互联网上所有知识，但这些知识是大众的，是人类全体信息的期望。对于个人而言，每个人所处的小世界都在不断地产生近乎无限的数据和环境。为每个人分配一个agent，让人类社会拥有一个agent社会镜像，在无限的人类活动和个人环境下进行进化，是我认为multiagent实现下一个scaling的可能性。</li>
</ul></li>
</ul>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:
</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_scaling_202507,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {What is the Next Step for Scaling in the Era of RL for LLM?},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {7},</span><br><span class="line">  url = {https://thinkwee.top/2025/07/15/next-scaling-202507/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>rl</tag>
        <tag>scaling</tag>
        <tag>llm</tag>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>[Some Questions asking Myself 2024.4]</title>
    <url>/2024/04/23/next-on-llm/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/122eedc8f2f91dc50d74b7b244718f69.png" width="500"></p>
<p>Some very-personal questions, assumptions and predictions on the
future after the large model era. I hope to keep it a habit for writing
such future-ask post for every half year to keep me thinking about the
"next token" in the AI era. This post is about Compression, World Model,
Agent and Alignment.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="is-compression-our-only-path-to-general-intelligence">Is
Compression Our Only Path to General Intelligence?</h1>
<h2 id="is-compression-all-we-need">Is compression all we need?</h2>
<ul>
<li>The first question is about compression.<br>
</li>
<li>Large models compress all the textual data in the world into the
parameters of a single model, enabling everyone to "extract" information
through natural language interaction. This process undoubtedly
alleviates knowledge or information asymmetry. For example, a dentist
can query an LLM to write code, while a programmer can enhance their
paper writing with the assistance of an LLM. Extracting pre-encoded
knowledge from LLMs is always beneficial. However, our aspirations go
beyond this simple query-based knowledge retrieval. We wonder:
<ul>
<li><strong>Can new discoveries emerge from the compressed
information/knowledge in these models?</strong> For instance, could a
physicist uncover a new law from an LLM? Or could an LLM predict the
content of this post? The answer is uncertain: it could be yes or no.
<ul>
<li>On the affirmative side, mathematicians provide an example—many
discoveries in pure theoretical research arise solely from scientists'
cognitive processes and prior knowledge. Compression-based large models
excel at leveraging past knowledge. If they can effectively simulate the
cognitive process of scientists, they might achieve groundbreaking
discoveries.<br>
</li>
<li>On the negative side, some discoveries require empirical
observation. They are "discovered" because someone observes them, such
as the identification of new species in biology, which cannot be
inferred merely from known information.<br>
</li>
<li>Another question worth pondering is whether new discoveries are even
necessary. After all, perhaps 99.999% of the world's activities in the
next second follow established patterns. A tool that efficiently
extracts and applies these patterns can still profoundly impact
humanity. While this is true, our pursuit of AGI compels us to strive
for more than this pragmatic goal.</li>
</ul></li>
<li>The core question hinges on <strong>"Is compression all we
need?"</strong><sup class="refplus-num"><a href="#ref-compression_for_agi">[1]</a></sup>
If I could compress all the world's myriad and diverse data into a
model, could it predict the future? If the model could accurately
simulate the entire world, the answer would be yes—fast-forwarding the
simulation would reveal glimpses of the future. But does compression
combined with conditional extraction truly equate to simulation?</li>
<li>Elon Musk once remarked that the focus should be on the
transformation between energy and intelligence. <strong>Is compression
the best method for such transformation?</strong> Perhaps it serves as
an efficient intermediary between energy and compressed knowledge
(instead of intelligence).<br>
</li>
<li>Related to this "compression question" is another: <strong>"Is
predicting the next token all we need?"</strong> This question probes
the limits of procedural and causal knowledge representation.</li>
</ul></li>
</ul>
<h2 id="world-model-a-data-driven-approach">World Model: A Data-Driven
Approach?</h2>
<ul>
<li>Regarding world models, a popular concept posits that intelligence
comprises several interconnected subsystems (e.g., cognition, memory,
perception, and world models), informed by human cognitive priors. The
world model specifically refers to our brain's simulation of the world,
enabling decision-making without waiting for real-world
interaction.<br>
</li>
<li>The aspiration is to model these subsystems individually. However,
most of our data is either unsupervised or end-to-end (holistic rather
than divided into subsystems). Unsupervised data poses challenges in
enabling all subsystem functionalities (e.g., language model pretraining
struggles with instruction-following). End-to-end data might not train
all subsystems effectively.<br>
</li>
<li>If we could segment and organize data to correspond to these
subsystems, could we achieve a world model in the form of multi-agent or
multi-LM systems?</li>
</ul>
<h2 id="agents">Agents</h2>
<ul>
<li><p>Could OpenAI's <em>Bitter Lesson</em> overshadow many aspects of
research on large models? Will agent-based research meet a similar fate?
In other words, even after scaling up large models, will the research
focus on agents remain irreplaceable? This might depend on whether the
most rudimentary outputs of LLMs can transition from "System 1"
(intuitive responses) to "System 2" (deliberative
reasoning)<sup class="refplus-num"><a href="#ref-system2">[2]</a></sup><sup class="refplus-num"><a href="#ref-consciousness_prior">[3]</a></sup>.</p></li>
<li><p>If an agent possesses all the actions and information of a human,
can we consider it equivalent to a human?</p></li>
</ul>
<h2 id="alignment-and-feedback">Alignment and Feedback</h2>
<ul>
<li>Everything revolves around the <strong>data flywheel</strong>. The
objective is to achieve better signals with each update by aligning the
model.<br>
</li>
<li>Alignment demonstrates the importance of improving positive samples
rather than focusing on negative samples, distinguishing it
significantly from contrastive learning.<br>
</li>
<li>Alignment<sup class="refplus-num"><a href="#ref-rlfh">[4]</a></sup>
can be beneficial or detrimental, depending on the goal to which the
model is aligned.<br>
</li>
<li>Some interesting questions are:
<ul>
<li>How can we integrate various forms of feedback (human/non-human,
textual/other modalities, social/physical)?<br>
</li>
<li>By connecting all these feedback types, we might align models with
more powerful goals. Moreover, the laws governing this integration could
reveal fundamental rules of the world.<br>
</li>
<li>Reward models exemplify the energy hidden in tradeoffs: by
sacrificing some precision, we gain scalable training, rewarding, and
labeling. This tradeoff results in stable improvements. Can we uncover
more such "energy" within these processes?
<ul>
<li>For example, could cascading reward models (like interlocking gears)
amplify the reward knowledge encoded by human annotations across
datasets?<br>
</li>
</ul></li>
<li>Similarly, the <strong>alignment
tax</strong><sup class="refplus-num"><a href="#ref-alignment_tax">[5]</a></sup>
represents another tradeoff. Is there latent "energy" in these
tradeoffs, where sacrificing A for B leads to overall intelligence
gains?</li>
</ul></li>
</ul>
<h2 id="beyond-language">Beyond Language</h2>
<ul>
<li>Language is more intricate, reasoned, and abstract than other
modalities because it is fundamentally "unnatural"—a construct of human
invention.<br>
</li>
<li>Nonetheless, researchers have identified an elegant objective for
language: <strong>predicting the next token</strong>, a goal reflecting
the entire history of computational linguistics.<br>
</li>
<li>Other modalities, like images, videos, and sounds, are "natural," as
they convey raw information from the physical world. Could these
modalities have objectives as intuitive or powerful as predicting the
next token?<br>
</li>
<li>What implications do multimodal capabilities have for the reasoning
abilities of large models?</li>
</ul>
<h2 id="cite-this-post">Cite This Post</h2>
<p>If you find this post helpful or interesting, you can cite it as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_on_llm_2024,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {[Some Questions asking Myself 2024.4] Compression, World Model, Agent and Alignment},</span><br><span class="line">  year = {2024},</span><br><span class="line">  month = {4},</span><br><span class="line">  url = {https://thinkwee.top/2024/04/23/next-on-llm/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="压缩是我们通往通用智能的唯一可能吗">压缩是我们通往通用智能的唯一可能吗？</h1>
<h2 id="压缩能解决一切吗">压缩能解决一切吗?</h2>
<ul>
<li>这是第一个问题，关于压缩。</li>
<li>大模型将世界上所有的的语料压缩到一个模型的参数中，每个人都可以通过自然语言交互来"提取"信息。这个过程无疑缓解了信息或知识的不对称性。例如，一个牙医可以通过查询LLM来编写程序，而程序员也可以通过LLM的协助来提升工作效率。从LLM中提取已编码的现有知识总是有益的，但我们想要肯定不仅仅是这种简单的知识查询，而是更多的可能：
<ul>
<li>是否可能用现有的压缩信息/知识发现新事物？例如，物理学家能否从LLM中发现新定律？或者LLM能否预测这篇文章？目前没有答案，可能是，也可能不是。
<ul>
<li>积极的一面是，数学家已经证明了这一点，因为许多纯理论研究发现仅源于科学家的大脑和过去的知识。基于压缩的大模型擅长利用过去的知识，如果它们能有效地模拟科学家的认知过程，就可能发现新的发现。</li>
<li>消极的一面是，一些新发现往往来自经验观察（他们"发现"某物是因为他们看到了它），比如在生物学中，新的物种不可能仅仅从已知物种信息的推理中得出。</li>
</ul></li>
<li>另一个值得思考的问题是，新的发现是否真的必要。可能下一秒钟的世界，99.999%的活动都遵循着既定模式。一个能够高效提取和应用这些模式的工具仍然可以深刻影响人类。即便如此，我们追求
AGI（通用人工智能）的理想肯定远大于仅仅挖掘实用模式。</li>
<li>核心问题在于“压缩是否就是我们需要的全部？”<sup class="refplus-num"><a href="#ref-compression_for_agi">[1]</a></sup>如果我能将世界上千变万化的数据压缩进一个模型，它能否预测未来？如果模型能够准确模拟整个世界，答案就是肯定的——快进模拟将揭示未来的片段。但是，压缩与条件提取的结合真的等同于模拟吗？</li>
<li>埃隆·马斯克曾表示，应关注能量与智能之间的转化。压缩是否是这种转化的最佳方法？或许它充当了能量与压缩知识之间的高效中介，而不是能量与智能的中介。</li>
<li>与此“压缩问题”相关的是另一个问题：“预测下一个标记是否就是我们需要的全部？”这个问题探讨了程序和因果知识表示的极限。</li>
</ul></li>
</ul>
<h2 id="世界模型以数据驱动的方式">世界模型，以数据驱动的方式？</h2>
<ul>
<li>对于世界模型，一种流行的说法是基于人类认知的先验将智能分割成几个相互连接的子系统（包含认知/记忆/感知/世界模型）。世界模型包含了我们脑中对于世界的模拟，这样我们不用等到与真实世界交互之后再进行决策。</li>
<li>我们期望用模型分别建模这几个子系统，但我们拥有的大多数数据都是无监督的或端到端的（整体而不是分成多个子部分的数据），而无监督的建模难以实现所有部分的功能（例如预训练阶段的语言模型无法实现指令跟随），端到端的数据则不确定能否训练好所有子系统。</li>
<li>如果数据可以为所有这些子系统分割和组织，我们能否以多智能体或多LM系统的形式实现世界模型？</li>
</ul>
<h2 id="智能体">智能体</h2>
<ul>
<li>OpenAI的"痛苦教训"可能会掩盖许多关于大模型的研究。智能体是否会面临类似的命运？</li>
<li>换句话说，即使在扩大大模型规模后，智能体研究的内容是否仍然不可为LLM所替代？这可能取决于LLM最朴素的响应是否能从系统1过渡到系统2<sup class="refplus-num"><a href="#ref-system2">[2]</a></sup><sup class="refplus-num"><a href="#ref-consciousness_prior">[3]</a></sup>。</li>
<li>如果一个智能体拥有一个人的所有行为和信息，我们能说它就是一个人吗？</li>
</ul>
<h2 id="对齐反馈">对齐/反馈</h2>
<ul>
<li>一切都与数据飞轮有关。目标是在每次更新中对齐模型后获得更好的信号。</li>
<li>对齐<sup class="refplus-num"><a href="#ref-rlfh">[4]</a></sup>证明我们需要更好的正样本，而不是构建负样本。这是与对比学习的一个显著区别。</li>
<li>对齐可以是好的也可以是坏的，这取决于模型对齐的目标。</li>
<li>我们如何连接各种反馈，包括人类/非人类、文本/其他模态、人类社会/物理世界。</li>
<li>通过链接所有这些反馈我们可以与更强大的目标对齐。更重要的是，连接这些反馈的规律可能揭示世界的规则。</li>
<li>RLFH使用人类标注训练奖励模型，然后用奖励模型扩大规模训练语言模型，实现了人类标注和模型标注的tradeoff，牺牲了一点奖励的精确度，换来了奖励样本的scale
up。RLFH挖掘了这种tradeoff中蕴含着的能量，即牺牲多少质量，换来多少数量，可以带来最终效果的稳定提升。我们是否可能挖掘更多这种能量的可能性？比如级联的reward
model，例如齿轮一般，将人类标注的奖励知识不断放大到样本中。</li>
<li>类似的，alignment
tax<sup class="refplus-num"><a href="#ref-alignment_tax">[5]</a></sup>也是一种tradeoff，这种tradeoff是否也有牺牲A换来B但最终提升了整体智能的能量？</li>
</ul>
<h2 id="超越语言">超越语言</h2>
<ul>
<li>语言比其他模态更复杂、更具推理性、更抽象，因为它实际上是"非自然的"，是人类创造的。</li>
<li>但研究人员为语言找到了一个令人惊叹的目标，它反映了计算语言学的整个历史，即语言模型，也就是预测下一个词。</li>
<li>图像/视频/声音等其他模态是自然的，因为它们传达了物理世界的原始信息。这些信息能否有类似或更朴素的目标？</li>
<li>更多模态对大模型的推理能力意味着什么？</li>
</ul>
<h2 id="引用">引用</h2>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{next_on_llm_2024_4,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {[Some Questions asking Myself 2024.4] Compression, World Model, Agent and Alignment},</span><br><span class="line">  year = {2024},</span><br><span class="line">  month = {4},</span><br><span class="line">  url = {https://thinkwee.top/2024/04/23/next-on-llm/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>
<ul id="refplus"><li id="ref-compression_for_agi" data-num="1">[1]  Compression for AGI - Jack Rae | Stanford MLSys #76 https://www.youtube.com/watch?v=dO4TPJkeaaU</li><li id="ref-system2" data-num="2">[2]  LeCun, Y. (2022). A path towards autonomous machine intelligence. version 0.9. 2, 2022-06-27. Open Review, 62(1), 1-62.</li><li id="ref-consciousness_prior" data-num="3">[3]  Bengio, Y. (2017). The consciousness prior. arXiv preprint arXiv:1709.08568.</li><li id="ref-rlfh" data-num="4">[4]  Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.</li><li id="ref-alignment_tax" data-num="5">[5]  Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., ... &amp; Kaplan, J. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.</li></ul>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>MyQuestion</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>llm</tag>
        <tag>agent</tag>
        <tag>inference</tag>
        <tag>questions</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Latent Dirichlet Allocation</title>
    <url>/2018/07/23/lda/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/cbd846ecd88abb611db2c204930d896d.png" width="500"/></p>
<p>Latent Dirichlet Allocation Document Topic Generation Model Study
Notes This article mainly summarizes from "Mathematical Curiosities of
LDA(LDA数学八卦)," which is written very beautifully (recommend reading
the original first). There are many places that spark further thought,
and this article sorts out the steps to derive LDA, removes some
irrelevant extensions, and summarizes LDA in plain language.</p>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oNlT.jpg" alt="i0oNlT.jpg" />
<figcaption aria-hidden="true">i0oNlT.jpg</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="what-is-lda-used-for">What is LDA used for</h1>
<ul>
<li>LDA is a topic model; the question is actually what is a topic model
used for? It is used to represent documents. Here, documents are viewed
as a bag of words.</li>
<li>If each word in the dictionary is considered a feature and the tfidf
value is used as the magnitude of the feature to represent a document,
then the feature vector of the document is too sparse and has too high a
dimensionality</li>
<li>The solution of LSI is to perform singular value decomposition on
the document-word matrix, reduce dimensions, but the reduced dimension
space obtained, i.e., the latent variables between words and documents,
is unexplainable; a purely mathematical method, too forceful</li>
<li>PLSA proposes that latent variables should be topics, which can
represent documents as topic vectors, and define topics as a certain
polynomial distribution on the dictionary. This way, PLSA contains two
layers of polynomial distributions: the polynomial distribution from
documents to topics (the mixing proportion of various topics in a
document, i.e., the feature vector of the document), and the polynomial
distribution from topics to words (the probability distribution over the
entire dictionary, representing the probability of each word appearing
under different topics)</li>
<li>LDA specifies Dirichlet priors for the parameters of these two
polynomial distributions, introducing a Bayesian framework for PLSA</li>
</ul>
<h1 id="bayesian-model">Bayesian model</h1>
<ul>
<li><p>LDA is a Bayesian model</p></li>
<li><p>Given the training data, how does a Bayesian model learn
parameters (the distribution of parameters): Bayesian estimation</p>
<ul>
<li><p>Prior distribution for the parameter <span
class="math inline">\(p(\theta)\)</span></p></li>
<li><p>Given the data, calculate the likelihood <span
class="math inline">\(p(X|\theta)\)</span> and evidence <span
class="math inline">\(P(X)\)</span> , and then compute the posterior
distribution of the parameters according to Bayes' formula</p></li>
<li><p>Posterior distribution is the distribution of the learned
parameters</p>
<p><span class="math display">\[
p(\vartheta | X)=\frac{p(X | \vartheta) \cdot
p(\vartheta)}{p(\mathcal{X})}
\]</span></p></li>
</ul></li>
<li><p>The data may be too much, so update in batches; the posterior
obtained after the last update serves as the prior for the next update,
similar to the idea in stochastic gradient descent</p></li>
<li><p>For the likelihood of new data, unlike maximum likelihood
estimation or maximum a posteriori estimation, both of which are point
estimates, directly calculating <span
class="math inline">\(p(x_{new}|\theta _{ML})\)</span> , <span
class="math inline">\(p(x_{new}|\theta _{MAP})\)</span> , Bayesian
estimation requires integrating over the parameter distribution:</p>
<p><span class="math display">\[
\begin{aligned}
p(\tilde{x} | X) &amp;=\int_{\vartheta \in \Theta} p(\tilde{x} |
\vartheta) p(\vartheta | X) \mathrm{d} \vartheta \\
&amp;=\int_{\vartheta \in \Theta} p(\tilde{x} | \vartheta) \frac{p(X |
\vartheta) p(\vartheta)}{p(X)} \mathrm{d} \vartheta
\end{aligned}
\]</span></p></li>
<li><p>Two issues often arise in Bayesian estimation</p>
<ul>
<li>With prior knowledge and likelihood, the calculated posterior is
complex</li>
<li>evidence is hard to calculate</li>
</ul></li>
<li><p>Therefore, conjugate and Gibbs sampling respectively solve these
two problems</p>
<ul>
<li>Conjugate: Given the likelihood, find a prior such that the
posterior has the same form as the prior</li>
<li>gibbs sampling: Using the idea of MCMC to approximate the posterior
without explicit computation of the evidence</li>
</ul></li>
<li><p>If it is a Bayesian model with latent variables, then there
is</p>
<p><span class="math display">\[
p(\vartheta|x) = \int_{z} p(\vartheta|z)p(z|x)
\]</span></p></li>
<li><p>In LDA, the latent variable z represents the topic to which a
word belongs (the word's topic assignment), so <span
class="math inline">\(p(\vartheta|z)\)</span> is naturally easy to
calculate, and then the remaining <span
class="math inline">\(p(z|x)\)</span> can be calculated using the
formula for Bayesian inference:</p>
<p><span class="math display">\[
p(z | X)=\frac{p(X | z) \cdot p(z)}{p(\mathcal{X})}
\]</span></p></li>
<li><p>Before introducing LDA, two other models are introduced: the
Bayesian unigram and PLSA. The former can be considered as LDA without a
topic layer, and the latter as LDA without Bayesian
regularization</p></li>
<li><p>Next, we will separately introduce conjugate, Gibbs sampling,
Bayesian unigram/PLSA, and finally, LDA</p></li>
</ul>
<h1 id="conjugation">Conjugation</h1>
<h2 id="gamma-function">Gamma function</h2>
<ul>
<li><p>Definition</p>
<p><span class="math display">\[
\Gamma (x) = \int _0 ^{\infty} t^{x-1} e^{-t} dt
\]</span></p></li>
<li><p>Due to its recursive nature <span
class="math inline">\(\Gamma(x+1)=x\Gamma(x)\)</span> , the definition
of factorial can be extended to the real number domain, thereby
extending the definition of the derivative of a function to the real
set, for example, calculating the second derivative at 1/2</p></li>
<li><p>Bohr-Mullerup theorem: If <span
class="math inline">\(f:(0,\infty) \rightarrow (0,\infty)\)</span> and
satisfy</p>
<ul>
<li><span class="math inline">\(f(1)=1\)</span></li>
<li><span class="math inline">\(f(x+1)=xf(x)\)</span></li>
<li>If <span class="math inline">\(log f(x)\)</span> is a convex
function, then <span class="math inline">\(f(x)=\Gamma(x)\)</span></li>
</ul></li>
<li><p>Digamma function</p>
<p><span class="math display">\[
\psi (x)=\frac{d log \Gamma(x)}{dx}
\]</span></p>
<p>It has the following properties</p>
<p><span class="math display">\[
\psi (x+1)=\psi (x)+\frac 1x
\]</span></p></li>
<li><p>The result of inferring LDA using variational inference is in the
form of the digamma function</p></li>
</ul>
<h2 id="gamma-distribution">Gamma distribution</h2>
<ul>
<li><p>Transform the above equation</p>
<p><span class="math display">\[
\int _0 ^{\infty} \frac{x^{\alpha -1}e^{-x}}{\Gamma(\alpha)}dx = 1
\]</span></p>
<p>Therefore, it is advisable to take the function in the integral as
the probability density, obtaining the density function of the simplest
form of the Gamma distribution:</p>
<p><span class="math display">\[
Gamma_{\alpha}(x)=\frac{x^{\alpha -1}e^{-x}}{\Gamma(\alpha)}
\]</span></p></li>
<li><p>Exponential distribution and <span class="math inline">\(\chi
^2\)</span> distribution are both special cases of the Gamma
distribution, and they are very useful as prior distributions, widely
applied in Bayesian analysis.</p></li>
<li><p>Gamma distribution and Poisson distribution have a formal
consistency, and in their usual representations, there is only a
difference of 1 in the parameters. As previously mentioned, factorials
can be represented by the Gamma function, therefore, it can be
intuitively considered that the Gamma distribution is a continuous
version of the Poisson distribution over the set of positive real
numbers.</p>
<p><span class="math display">\[
Poisson(X=k|\lambda)=\frac{\lambda ^k e^{-\lambda}}{k!}
\]</span></p></li>
<li><p>The limit distribution of the binomial distribution with <span
class="math inline">\(np=\lambda\)</span> is the Poisson distribution as
n approaches infinity and p approaches 0. A common example used to
explain the Poisson distribution is the reception of calls by a switch.
Assuming the entire time is divided into several time intervals, with at
most one call per interval, the probability being p, the total number of
calls follows a binomial distribution. When np is a constant <span
class="math inline">\(\lambda\)</span> , dividing time into an infinite
number of segments, which are almost continuous, substituting <span
class="math inline">\(p=\frac{\lambda}{n}\)</span> into the binomial
distribution, taking the limit yields the Poisson distribution. On this
basis, by making the distribution values continuous (i.e., replacing the
factorial of k in the Poisson distribution with the Gamma function) we
obtain the Gamma distribution.</p></li>
</ul>
<h2 id="beta-distribution">Beta distribution</h2>
<ul>
<li>Background:
<ul>
<li><p>How to obtain the distribution of the kth order statistic <span
class="math inline">\(p=x_k\)</span> when n random variables uniformly
distributed in the interval [0,1] are sorted in ascending
order?</p></li>
<li><p>To obtain the distribution, by employing the idea of limits, we
calculate the probability that this variable falls within a small
interval <span class="math inline">\(x \leq X_{k} \leq x+\Delta
x\)</span></p></li>
<li><p>Divide the entire interval into three parts: before the small
interval, the small interval, and after the small interval. If only the
k-th largest number is present in the small interval, then there should
be k-1 numbers before the small interval, and n-k numbers after the
small interval. The probability of this situation is</p>
<p><span class="math display">\[
\begin{aligned}
P(E)&amp;=x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\
&amp;=x^{k-1}(1-x)^{n-k}\Delta x+\omicron (\Delta x) \\
\end{aligned}
\]</span></p></li>
<li><p>If there are two or more numbers within a small interval, the
probability of this situation is <span class="math inline">\(\omicron
(\Delta x)\)</span> , therefore, only consider the case where there is
only the k-th largest number within the small interval. In this case,
let <span class="math inline">\(\Delta x\)</span> tend towards 0, then
we obtain the probability density function of the k-th largest number
(note that the coefficient of event E should be <span
class="math inline">\(nC_n^k\)</span> ):</p>
<p><span class="math display">\[
f(x)=\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \quad x \in [0,1]
\]</span></p>
<p>Expressed in terms of the Gamma function (let <span
class="math inline">\(\alpha =k,\beta = n-k+1\)</span> ), we obtain:</p>
<p><span class="math display">\[
\begin{aligned}
f(x)&amp;=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\\
&amp;=Beta(p|k,n-k+1) \\
&amp;=Beta(\alpha,\beta) \\
\end{aligned}
\]</span></p>
<p>This is the Beta distribution; we can take the point of the maximum
probability distribution as the predicted value for the kth largest
number.</p></li>
</ul></li>
<li>Beta distribution is actually a prediction of a distribution, that
is, the distribution of distributions. In the background, we are looking
for the probability distribution of the kth largest order statistic, and
we denote this kth largest order statistic as p. Now, we know there are
n values uniformly distributed in the interval [0,1], and n and k
establish a relative position within the interval [0,1], which is the
role of <span class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> (because <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> are calculated from n and k),
representing where I tend to believe p is within [0,1]. Since the n
statistics are uniformly distributed and p is the kth largest among
them, p tends to be at the position <span class="math inline">\(\frac
kn\)</span> .</li>
<li>Therefore, the parameters of the Beta distribution ( <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> ) are clearly meaningful,
representing my prior belief about the possible location of p, even when
I know nothing, that is, the prior. The result of the Beta distribution
is the distribution of p calculated under this prior influence, and the
actual meaning of this p is the parameter of the binomial distribution.
Therefore, the Beta distribution can serve as a prior for the parameters
of the binomial distribution.</li>
</ul>
<h2 id="beta-binomial-conjugate">Beta-Binomial conjugate</h2>
<ul>
<li>We can establish a Beta distribution prior for the kth largest
number. If we now know there are m numbers uniformly distributed between
[0,1], and some of them are larger than the kth largest number while
others are smaller, we can incorporate this as additional data knowledge
to form the posterior distribution.</li>
<li>Assuming the number to be guessed is <span
class="math inline">\(p=X_k\)</span> , now we know that there are <span
class="math inline">\(m_1\)</span> numbers smaller than <span
class="math inline">\(p\)</span> , and <span
class="math inline">\(m_2\)</span> numbers larger than <span
class="math inline">\(p\)</span> , it is obvious that the probability
density function of <span class="math inline">\(p\)</span> becomes <span
class="math inline">\(Beta(p|k+m_1,n-k+1+m_2)\)</span></li>
<li>The knowledge of m numbers added into the data is equivalent to
conducting m Bernoulli experiments, because the range we are discussing
is on [0,1], and the m numbers only care whether they are larger or
smaller than <span class="math inline">\(p\)</span> , the value of <span
class="math inline">\(p=X_k\)</span> can represent this probability,
therefore <span class="math inline">\(m_1\)</span> follows a binomial
distribution <span class="math inline">\(B(m,p)\)</span></li>
</ul>
<table data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3">
<thead data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3">
<tr data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3">
<th align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">A
priori</font></font></font>
</th>
<th align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">Data
Knowledge</font></font></font>
</th>
<th align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">Posterior</font></font></font>
</th>
</tr>
</thead>
<tbody data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3">
<tr data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3">
<td align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">Beta
distribution</font></font></font>
</td>
<td align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">Binomial
distribution</font></font></font>
</td>
<td align="center" data-immersive-translate-walked="8b4bb475-eb27-486e-9bfc-357b87ff2cd3" data-immersive-translate-paragraph="1">
<font class="notranslate immersive-translate-target-wrapper" data-immersive-translate-translation-element-mark="1" lang="en"><br><font class="notranslate immersive-translate-target-translation-theme-dashed immersive-translate-target-translation-block-wrapper-theme-dashed immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-dashed-inner" data-immersive-translate-translation-element-mark="1">Beta
distribution</font></font></font>
</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Therefore, we can obtain the Beta-Binomial conjugate</p>
<p><span class="math display">\[
Beta(p|\alpha,\beta)+BinomCount(m_1,m_2)=Beta(p|\alpha+m_1,\beta+m_2)
\]</span></p>
<p>When the data conforms to a binomial distribution, both the prior
distribution and the posterior distribution of the parameters can
maintain the form of a Beta distribution. We can assign a clear physical
meaning to the parameters in the prior distribution and extend this
interpretation to the posterior distribution, therefore, the parameters
<span class="math inline">\(\alpha,\beta\)</span> in the Beta
distribution are generally referred to as pseudo-counts, representing
physical counts.</p></li>
<li><p>It can be verified that when both parameters of the Beta
distribution are 1, it becomes a uniform distribution. At this point,
the conjugate relationship can be considered as follows: Initially, the
unevenness of the coin is unknown, and it is assumed that the
probability of the coin landing on heads is uniformly distributed. After
tossing the coin m times, data is obtained with <span
class="math inline">\(m_1\)</span> tosses landing on heads and the rest
landing on tails. By calculating the posterior probability using Bayes'
formula, the probability of the coin landing on heads can be determined
to be exactly <span
class="math inline">\(Beta(p|m_1+1,m_2+1)\)</span></p></li>
<li><p>Through this conjugation, we can derive an important formula
concerning the binomial distribution:</p>
<p><span class="math display">\[
P(C \leq k) = \frac{n!}{k!(n-k-1)!}\int _p ^1 t^k(1-t)^{n-k-1}dt \quad C
\sim B(n,p)
\]</span></p>
<p>The following can now be proven:</p>
<ul>
<li><p>The left side of the formula is the probability cumulative of the
binomial distribution, and the right side is the probability integral of
the <span class="math inline">\(Beta(t|k+1,n-k)\)</span>
distribution</p></li>
<li><p>Select n random variables uniformly distributed in [0,1], for the
binomial distribution <span class="math inline">\(B(n,p)\)</span> , if
the number is less than <span class="math inline">\(p\)</span> , it is
considered successful; otherwise, it is a failure. The number of random
variables less than <span class="math inline">\(p\)</span> is C and
follows the binomial distribution <span
class="math inline">\(B(n,p)\)</span></p></li>
<li><p>At this point, we obtain <span class="math inline">\(P(C \leq
k)=P(X_{k+1}&gt;p)\)</span> , which means that there are <span
class="math inline">\(k\)</span> variables less than <span
class="math inline">\(p\)</span> after n random variables are arranged
in order</p></li>
<li><p>At this point, utilizing our probability density of the k-th
largest number, we calculate it to be a Beta distribution, and then
substitute it into the equation</p>
<p><span class="math display">\[
\begin{aligned}
P(C \leq k) &amp;=P(X_{k+1} &gt; p) \\
  &amp;=\int _p ^1 Beta(t|k+1,n-k)dt \\
  &amp;=\frac{n!}{k!(n-k-1)!}\int _p ^1 t^k(1-t)^{n-k-1}dt \\
\end{aligned}
\]</span></p>
<p>Proof by construction</p></li>
</ul></li>
<li><p>By taking the limit of n to infinity in this formula and
converting it to a Poisson distribution, the Gamma distribution can be
derived.</p></li>
<li><p>In this section, we introduced other information to the prior,
that is, there are several numbers larger and smaller than the k-th
largest number. This information is equivalent to telling me: I can
modify the prior of p; the position I previously tended to believe in
has shifted. If I know several numbers are larger than p, then the prior
position of p should be shifted to the right. If I know several numbers
are smaller than p, then the prior position of p should be shifted to
the left. What if I simultaneously know 100 numbers are larger than p
and 100 numbers are smaller than p? The position of p remains unchanged,
but I am more confident that the true position of p is now this prior
position. Therefore, the Beta distribution is more concentrated at this
prior position. This point will be seen again in the subsequent analysis
of the meaning of the Dirichlet parameters. After adding prior knowledge
and data, we form the posterior, which is the basic content of Bayes'
formula.</p></li>
</ul>
<h2 id="dirichlet-multinomial-conjugacy">Dirichlet-Multinomial
Conjugacy</h2>
<ul>
<li><p>How should we calculate the joint distribution of guessing two
numbers <span class="math inline">\(x_{k_1},x_{k_1+k_2}\)</span>
?</p></li>
<li><p>Similarly, we set two extremely small intervals <span
class="math inline">\(\Delta x\)</span> , dividing the entire interval
into five parts, with the extremely small intervals being <span
class="math inline">\(x_1,x_2,x_3\)</span> , after which the calculation
can be obtained</p>
<p><span class="math display">\[
f(x_1,x_2,x_3)=\frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}
\]</span></p></li>
<li><p>Organize it and it can be written as</p>
<p><span class="math display">\[
f(x_1,x_2,x_3)=\frac{\Gamma(\alpha _1+\alpha _2+\alpha
_3)}{\Gamma(\alpha _1)\Gamma(\alpha _2)\Gamma(\alpha _3)}x_1^{\alpha
_1-1}x_2^{\alpha _2-1}x_3^{\alpha _3-1}
\]</span></p>
<p>This is the 3D form of the Dirichlet distribution. The <span
class="math inline">\(x_1,x_2,x_3\)</span> (which actually only has two
variables) determines the joint distribution of two order statistics,
and <span class="math inline">\(f\)</span> represents the probability
density.</p></li>
<li><p>Observing that under the condition of <span
class="math inline">\(\alpha\)</span> being determined, the preceding
series of gamma functions are actually the denominator of the
probability normalization, in the following text we will express the
Dirichlet distribution in a more general form:</p>
<p><span class="math display">\[
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})=\frac{1}{\int
\prod_{k=1}^V p_k^{\alpha _k -1}d\mathop{p}^{\rightarrow}} \prod_{k=1}^V
p_k^{\alpha _k -1}
\]</span></p></li>
<li><p>The normalization denominator, that is, the entire set of gammas,
is made to be:</p>
<p><span class="math display">\[
\Delta(\mathop{\alpha}^{\rightarrow})=\int \prod _{k=1}^V p_k^{\alpha _k
-1}d\mathop{p}^{\rightarrow}
\]</span></p></li>
<li><p>参数</p>
<p><span class="math display">\[
Dir(p|\alpha)+MultCount(m)=Dir(p|\alpha+m)
\]</span></p>
<p>The parameters in the above formula are all vectors, corresponding to
the multi-dimensional case.</p></li>
<li><p>Whether it is the Beta distribution or the Dirichlet
distribution, they both possess an important property, that is, their
mean can be represented by the ratio of the parameters, for example, for
the Beta distribution, <span
class="math inline">\(E(p)=\frac{\alpha}{\alpha+\beta}\)</span> , and
for the Dirichlet distribution, the mean is a vector corresponding to
the vector composed of the ratios of the parameters.</p></li>
</ul>
<h2 id="dirichlet-analysis">Dirichlet analysis</h2>
<ul>
<li>According to the properties of Dirichlet, the ratio of its
parameters represents a partition on [0,1], determining the locations of
high probability in the Dirichlet distribution, and the magnitude of the
parameters determines the proportion of high probability (steepness), as
shown in the following figure, the polynomial distribution has three
terms with parameters <span class="math inline">\(p_1,p_2,p_3\)</span> ,
their sum is 1 and each is greater than zero, and in three-dimensional
space it forms a triangular face, with each point on the surface
representing a polynomial distribution; the red area has a high
probability, and the blue area has a low probability:</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0orkR.png" alt="i0orkR.png" />
<figcaption aria-hidden="true">i0orkR.png</figcaption>
</figure>
<ul>
<li>Controlled the mean shape and sparsity of polynomial distribution
parameters.</li>
<li>On the far left, the three parameters <span
class="math inline">\(\alpha _1,\alpha _2,\alpha _3\)</span> of
Dirichlet are equal, indicating that the red region is centered, and the
values of the three <span class="math inline">\(\alpha\)</span> are all
relatively large, with a smaller red region. If the heatmap is viewed as
a contour plot, this would represent a steeper red region, suggesting
that Dirichlet is very confident that the parameters of the polynomial
distribution will be centered. For the parameters <span
class="math inline">\(p_1,p_2,p_3\)</span> of the three polynomial
distributions, a larger possibility is that all three p values are
equal.</li>
<li>The middle figure, with three <span
class="math inline">\(\alpha\)</span> not equal, one <span
class="math inline">\(\alpha\)</span> being larger, causing the red area
to lean towards one corner of the triangular face, leading to a larger p
value for one and a higher probability of smaller p values for the other
two. At this point, the parameter prior acts as a concentration
parameter, focusing the probability attention on certain terms.</li>
<li>On the far right, like the far left, the three <span
class="math inline">\(\alpha\)</span> are equal, the red area is
centered, but the values of <span class="math inline">\(\alpha\)</span>
are all smaller, causing the red area to diverge, that is, Dirichlet
believes that the three p values should be in the center, but is not
very certain. The result is that there is a difference between the three
p values, but the difference will not be too great (still near the
center), and there will not be a very steep situation (the steepest is
the infinite height at the center, with a probability of 1, so the three
p values must be the same).</li>
<li>The ratio of <span class="math inline">\(\alpha\)</span> determines
the location of high probability in the polynomial distribution, which
mainly determines the proportions of various <span
class="math inline">\(p\)</span> , setting the concentration, while the
size of <span class="math inline">\(\alpha\)</span> determines the
concentration of this location; the smaller <span
class="math inline">\(\alpha\)</span> , the more concentrated the
location, the more certain the distribution of p, whereas the
distribution of p is roughly determined by the position of the red area,
but with a larger range of variation.</li>
<li>When <span class="math inline">\(\alpha\)</span> is much smaller
than 1, the Dirichlet distribution approaches another extreme. In the
example of the triangular face, the red region remains steep, but the
steepness is at the three corners of the triangle. It can be imagined
that <span class="math inline">\(alpha\)</span> changes from large to 1
and then decreases, with the high-probability density region gradually
spreading from the center to the entire face and then concentrating at
the three corners.</li>
<li>Intuitive Interpretations of the Parameters The concentration
parameter Dirichlet distributions are very often used as prior
distributions in Bayesian inference. The simplest and perhaps most
common type of Dirichlet prior is the symmetric Dirichlet distribution,
where all parameters are equal. This corresponds to the case where you
have no prior information to favor one component over any other. As
described above, the single value α to which all parameters are set is
called the concentration parameter. If the sample space of the Dirichlet
distribution is interpreted as a discrete probability distribution, then
intuitively the concentration parameter can be thought of as determining
how "concentrated" the probability mass of a sample from a Dirichlet
distribution is likely to be. With a value much less than 1, the mass
will be highly concentrated in a few components, and all the rest will
have almost no mass. With a value much greater than 1, the mass will be
dispersed almost equally among all the components. See the article on
the concentration parameter for further discussion.</li>
<li>When <span class="math inline">\(\alpha\)</span> is much smaller
than 1, the probability density will mainly accumulate on one or a few
terms, that is, the red area is concentrated at the three corners. In
this case, the polynomial distribution obtained from Dirichlet
distribution sampling is likely to be at the corners, that is, the
probability density accumulates on one term, and the probabilities of
the other two terms are approximately 0. When <span
class="math inline">\(\alpha\)</span> is much larger than 1, the
probability density will be dispersed to various parts, which
corresponds to the leftmost figure in the three figures, and the
possibility of the probabilities of the three terms being relatively
similar is greater.</li>
</ul>
<h2 id="role-in-lda">Role in LDA</h2>
<ul>
<li>Summarizing conjugacy: Given a distribution A, the parameter
distribution of A (or the distribution of the distribution) is B. If the
posterior of B, obtained after knowledge of A's data, belongs to the
same class of distribution as the prior, then A and B are conjugate, and
B is called the parameter conjugate distribution of A (or the prior
conjugate). In the example mentioned above, the Beta distribution is the
parameter conjugate distribution of the binomial distribution, and the
Dirichlet distribution is the parameter conjugate distribution of the
multinomial distribution.</li>
<li>LDA actually models text generation as a probabilistic generative
model, specifically a three-layer Bayesian model, and assumes that the
parameters of both the document-topic and topic-word multinomial
distributions are prior Dirichlet distributions, using
Dirichlet-Multinomial conjugacy to update its posterior with data
knowledge.</li>
<li>To introduce the Dirichlet-Multinomial conjugacy, it is first
necessary to introduce the Gamma function and its distribution. The
Gamma function extends the factorial to the real number domain. After
that, the Beta function, which can estimate the distribution, is
introduced. With the introduction of the Gamma function, the parameters
of the Beta distribution can be extended to the real number domain.
Then, the Beta-Binomial conjugacy is introduced, which brings the
benefit that the form of the posterior distribution is known when
correcting with data. By extending this conjugacy relationship to higher
dimensions (estimating multiple distributions), the
Dirichlet-Multinomial conjugacy is obtained.</li>
<li>The benefit of adding Dirichlet distribution as a prior for the
document-to-topic and topic-to-word polynomial distributions is that by
treating polynomial parameters as variables, prior information guides
the range of parameter variation rather than specific values, making the
model's generalization ability stronger within small training
samples.</li>
<li>The size of <span class="math inline">\(\alpha\)</span> in the
Dirichlet Process reflects the degree of granularity when fitting the
Base Measure with the Dirichlet distribution; the larger <span
class="math inline">\(\alpha\)</span> , the less granular, and each term
receives a fairly similar probability.</li>
<li>The degree of dispersion corresponds to the LDA model, representing
whether the document is concentrated on a few topics or distributed
relatively evenly across all topics, or whether the topics are
concentrated on a few words or distributed relatively evenly across the
entire vocabulary.</li>
</ul>
<h1 id="gibbs-sampling">Gibbs Sampling</h1>
<h2 id="random-simulation">Random Simulation</h2>
<ul>
<li>Monte Carlo method, used for known distributions, involves
generating a series of random samples that satisfy this distribution and
using the statistics of these samples to estimate some parameters of the
original distribution that are not easily analytically computed.</li>
<li>Markov refers to a method of generating random samples where the
process depends on the properties of Markov chains. By constructing the
transition matrix of the Markov chain, it is possible to produce a
sequence of samples that satisfy a given distribution when the Markov
chain converges.</li>
<li>A method of MCMC is random sampling, which calculates the acceptance
rate using the known distribution and only accepts a portion of the
samples. Gibbs sampling is an MCMC method with an acceptance rate of 1.
It improves the acceptance rate but extends the sampling process.</li>
</ul>
<h2 id="mars-chain">Mars Chain</h2>
<ul>
<li>Markov chain refers to a state transition where the probability
depends only on the previous state</li>
<li>Because the transition probability only depends on the previous
state, the state transition probabilities can be written as a transition
probability matrix. The probability distribution of each state after n
transitions is the result obtained by multiplying the initial state
probability distribution vector by the nth power of the matrix</li>
<li>The matrix power remains constant after a certain number of
iterations, meaning each row converges to the same probability
distribution, and the initial probability transition also converges to
the same distribution after sufficient iterations</li>
<li>On the Definition of the Convergence of Markov Chains
<ul>
<li>If a non-periodic Markov chain has a transition probability matrix P
(which may have infinitely many states), and any two states are
connected (any two states can be reached through a finite number of
transitions), then <span class="math inline">\(lim_{n \rightarrow
\infty}P_{ij}^n\)</span> exists and is independent of <span
class="math inline">\(i\)</span> , and this convergent matrix is denoted
as <span class="math inline">\(\pi(j)\)</span></li>
<li><span class="math inline">\(\pi (j)=\sum _{i=0}^{\infty} \pi (i)
P_{ij}\)</span></li>
<li><span class="math inline">\(\pi\)</span> is the unique non-negative
solution of equation <span class="math inline">\(\pi P =
\pi\)</span></li>
<li>Stable Distribution Called the Markov Chain</li>
</ul></li>
</ul>
<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>
<ul>
<li><p>Returning to random generation, for a given probability
distribution, we hope to generate the corresponding samples. An idea is
to construct a Markov chain whose stationary distribution is exactly
this probability distribution: because when the Markov chain converges,
its stationary distribution is the probability distribution of each
state. After convergence, regardless of how long a state sequence is
generated through state transitions, the distribution of states in this
sequence maintains the stationary distribution. If the stationary
distribution is the probability distribution to be generated, then the
state sequence is a random sample sequence under the given probability
distribution.</p></li>
<li><p>Therefore, the issue is how to construct the state transition
matrix of a Markov chain given a stationary distribution. It mainly
utilizes the detailed stationary condition of non-periodic Markov
chains: if <span class="math inline">\(\pi(i)P_{ij}=\pi(j)P_{ij} \quad
for \quad all \quad i,j\)</span> , then <span
class="math inline">\(\pi(x)\)</span> is the stationary distribution of
the Markov chain. A physical interpretation of this theorem is: if the
probability mass of state i is stable, then the probability mass of the
change from state i to state j is exactly complementary to the
probability mass of the change from state j to state i.</p></li>
<li><p>The probability of transitioning from state i to state j is
denoted as <span class="math inline">\(q(i,j)\)</span> . If <span
class="math inline">\(p(i)q(i,j)=p(j)q(j,i)\)</span> , then <span
class="math inline">\(p(x)\)</span> is the stationary distribution of
this Markov chain, and the transition matrix does not need to be
changed. However, in general, your luck is not that good. In the case of
knowing <span class="math inline">\(p(x)\)</span> , we need to modify
<span class="math inline">\(q\)</span> . To do this, we multiply by an
acceptance rate <span class="math inline">\(\alpha\)</span> , so
that:</p>
<p><span class="math display">\[
p(i)q(i,j)\alpha (i,j)=p(j)q(j,i)\alpha (j,i)
\]</span></p>
<p>Why is it called the acceptance rate? Because it can be understood
that this <span class="math inline">\(\alpha\)</span> is multiplied by a
probability after the original state transition, representing whether
this transition is accepted.</p></li>
<li><p>How to determine the acceptance rate? In fact, it is self-evident
<span class="math inline">\(\alpha (i,j)=p(j)q(j,i)\)</span> , symmetric
construction will suffice.</p></li>
<li><p>Therefore, after each transition, we sample a variable from a
uniform distribution, and if the variable is less than the acceptance
rate, we transition according to the original transition matrix;
otherwise, we do not transition.</p></li>
<li><p>Such MCMC sampling algorithms have a problem: we actually did not
modify the original transition probability matrix q, but calculated the
acceptance rate based on q to ensure convergence to p. However, the
acceptance rate may be calculated to be very small, leading to the state
remaining stationary for a long time and slow convergence. In fact,
multiplying both sides of the formula <span
class="math inline">\(p(i)q(i,j)\alpha
(i,j)=p(j)q(j,i)\alpha(j,i)\)</span> by a multiple does not break the
detailed balance condition, but the acceptance rate is improved.
Therefore, it is sufficient to multiply both sides of the acceptance
rate by a multiple and ensure that the two acceptance rates, after
doubling, do not exceed 1. The general practice is to multiply the
larger acceptance rate by 1. At this point, the most common MCMC method
is obtained: the Metropolis-Hastings algorithm.</p></li>
</ul>
<h2 id="gibbs-sampling-1">Gibbs Sampling</h2>
<ul>
<li><p>The previous discussion mentioned that MCMC actually does not
alter the transition probability matrix, thus requiring an acceptance
rate supplement. Even after scaling, there is always an acceptance rate
less than 1, which reduces the convergence efficiency. Gibbs sampling
aims to find a transition matrix Q such that the acceptance rate equals
1.</p></li>
<li><p>For the two-dimensional probability distribution <span
class="math inline">\(p(x,y)\)</span> , it is easy to obtain</p>
<p><span class="math display">\[
\begin{aligned}
p(x_1,y_1)p(y_2|x_1) &amp; =p(x_1)p(y_1|x_1)p(y_2|x_1) \\
&amp; =p(x_1)p(y_2|x_1)p(y_1|x_1) \\
&amp; =p(x_1,y_2)p(y_1|x_1) \\
\end{aligned}
\]</span></p></li>
<li><p>From the left-hand side to the final form, this type is very
similar to the detailed balance condition! In fact, if <span
class="math inline">\(x=x_1\)</span> is fixed, then <span
class="math inline">\(p(y|x_1)\)</span> can serve as the transition
probability between any two different y-values on the line <span
class="math inline">\(x=x_1\)</span> . And this transition satisfies the
detailed balance condition. Fixing y yields the same conclusion,
therefore, between any two points in this two-dimensional plane, we can
construct a transition probability matrix:</p>
<ul>
<li>If two points are on the vertical line <span
class="math inline">\(x=x_1\)</span> , then <span
class="math inline">\(Q=p(y|x_1)\)</span></li>
<li>If two points are on the horizontal line <span
class="math inline">\(y=y_1\)</span> , then <span
class="math inline">\(Q=p(x|y_1)\)</span></li>
<li>If the line connecting two points is neither vertical nor
horizontal, then applying the transition matrix Q to any two points on
the plane in such a way satisfies the detailed balance condition; the
Markov chain on this two-dimensional plane will converge to <span
class="math inline">\(p(x,y)\)</span> .</li>
</ul></li>
<li><p>After Gibbs sampling yields a new x dimension, the calculation of
the new y dimension depends on the new x dimension because it is based
on the previously selected coordinate axis transformation; otherwise, it
cannot jump to the new state <span
class="math inline">\((x_2,y_2)\)</span> . What you actually get is
<span class="math inline">\((x_1,y_2)\)</span> and <span
class="math inline">\((x_2,y_1)\)</span> .</p></li>
<li><p>Therefore, given a two-dimensional probability distribution, the
transition probabilities along the horizontal or vertical directions
satisfying the detailed balance condition on this plane can be
calculated. Starting from any state on the plane, a single transition
only changes the abscissa or ordinate, that is, it moves horizontally or
vertically. From the formula of the detailed balance condition, it can
be seen that this balance is transitive; if the transition in one
dimension satisfies the balance condition and is followed by another
dimension, then the equivalent single transition of the two transitions
is also balanced.</p></li>
<li><p>After all dimensions have been transferred once, a new sample is
obtained. The sample sequence formed after the Markov chain converges is
the random sample sequence we need. The state transition can be a cyclic
transformation of the coordinate axes, i.e., this time horizontal
transformation, next time vertical transformation, or it can be randomly
selected each time. Although randomly selecting the coordinate axis each
time will result in different new dimension values calculated in the
middle, the stationary condition is not broken, and it can eventually
converge to the same given distribution.</p></li>
<li><p>Similarly, the aforementioned algorithm can be generalized to
multi-dimensional spaces. When generalized to multi-dimensional spaces,
the transition probability constructed on the <span
class="math inline">\(x\)</span> axis is <span
class="math inline">\(Q=p(x|¬ x)\)</span> . It is noteworthy that the
sampling samples obtained from the above method are not mutually
independent, but only conform to the given probability
distribution.</p></li>
</ul>
<h2 id="role-in-lda-1">Role in LDA</h2>
<ul>
<li>Firstly, it is clear that the MCMC method is used to generate
samples from a known distribution, but Gibbs sampling only requires the
use of complete conditional probabilities, generating samples that
satisfy the joint distribution, unlike general sampling methods that
directly sample from the joint distribution</li>
<li>Gibbs sampling has this characteristic that allows it to infer
parameters without knowing the joint probability, and further derive the
joint probability distribution</li>
<li>However, in LDA, Gibbs sampling is not used to directly infer
parameters, but rather to approximate the posterior, completing the step
of updating the prior with data knowledge. Moreover, since LDA has the
hidden variable of topics, the joint distribution of Gibbs sampling is
not the topic distribution of documents or the word distribution of
topics, and is not directly linked to the parameters of the LDA model.
Gibbs sampling in LDA samples the topic assignment of tokens, i.e., the
distribution of the hidden variables.</li>
<li>However, after all token themes are assigned, the parameters of the
LDA model are determined, and two multinomial distributions (parameters)
can be obtained through classical probability (maximum likelihood
estimation), and the corresponding Dirichlet distribution (posterior
distribution of parameters) is also updated. Moreover, since the
document-word matrix is decomposed into themes, in fact, we do not need
to maintain the <span class="math inline">\(Document \* word\)</span>
matrix, but rather maintain the <span class="math inline">\(Document \*
topic + topic \* word\)</span> matrix.</li>
<li>Gibbs sampling word topic assignment is actually calculating the
posterior distribution of latent variables, thereby obtaining the
posterior distribution of parameters.</li>
<li>In Gibbs sampling, parameters are continuously updated, for
instance, if the update in this iteration is <span
class="math inline">\(p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)\)</span> , the
next iteration becomes <span
class="math inline">\(p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)\)</span> ,
using the updated <span class="math inline">\(x_1^{t+1}\)</span> for
calculation. In LDA, this process is achieved by updating the topic of
the sampled word. In Bayesian inference, data is divided into batches,
and the posterior updates the prior in an iterative manner, which is
further refined to each coordinate update in Gibbs sampling.</li>
<li>The Gibbs sampling formula can be seen below, which can be
interpreted as determining one's own theme distribution based on the
thematic distribution of other words, and iteratively updating the
thematic distribution of all words; how to determine it includes two
parts, which are similar to the information provided by tf and idf.</li>
<li>When calculating the posterior distribution of topic assignments
based on the sampling formula, we do not directly obtain the posterior
distribution of the parameters. However, after sampling new topics based
on the posterior distribution of topic assignments and updating the
statistics, since the Gibbs sampling formula itself includes the
statistics, this is equivalent to completing the calculation of the
posterior and updating the prior with a single step. Alternatively, it
can also be understood that in LDA, we are always performing Bayesian
inference on the distribution of topic assignments (latent variables),
i.e., <span class="math inline">\(p(topic|word,doc)\)</span> , and after
completing this, performing a maximum likelihood estimation (classical
type) based on the topic assignments allows us to obtain the parameters
of the model.</li>
</ul>
<h1 id="text-modeling">Text Modeling</h1>
<ul>
<li>Next, we discuss how to perform probabilistic modeling on text. The
basic idea is that we assume all words in a document are generated
according to a pre-set probability distribution, and we aim to find this
probability distribution. Specifically, this is divided into the
following two tasks:
<ul>
<li>What is the model like?</li>
<li>What is the generation probability of each word or the model
parameters?</li>
</ul></li>
</ul>
<h2 id="unigram-model">Unigram model</h2>
<ul>
<li><p>What is the model like? The traditional unigram model, also known
as the one-gram model, assumes that the generation of words is
independent of each other; documents and words are independently
exchangeable, regardless of order, as if all words are placed in a bag
and a word is taken out each time according to a probability
distribution, hence also known as the bag-of-words (BoW) model. The
parameters of the bag-of-words model are the generation probabilities of
each word, and the frequentist view holds that generation probabilities
can be determined through word frequency statistics.</p></li>
<li><p>Here introduces a Bayesian framework for unigrams, laying the
groundwork for the derivation of the two-layer Bayesian framework for
LDA in the following text. The Bayesian school believes that the
generation of words is not only one layer: there are many kinds of
probability distributions for word probabilities, and the probability
distribution itself also follows a probability distribution, as if God
has many dice, he picks one die and throws it again, generating a
word.</p></li>
<li><p>That is, the next document of the Unigram model is just a bag of
words, with the generation of these words following a distribution,
denoted as <span class="math inline">\(\mathop{p}^{\rightarrow}\)</span>
, and the distribution of word generation itself also follows a
distribution, denoted as <span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span> . Translated
into mathematical formulas, the above two distributions represent the
generation probability of a document:</p>
<p><span class="math display">\[
p(W)=\int
p(W|\mathop{p}^{\rightarrow})p(\mathop{p}^{\rightarrow})d\mathop{p}^{\rightarrow}
\]</span></p></li>
<li><p>In the view of the Bayesian school, we should first assume a
prior distribution, and then correct it using training data. Here, we
need to assume <span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span> , which is
the prior of the distribution, and what is the training data? It is the
word frequency distribution extracted from the corpus. Assuming <span
class="math inline">\(\mathop{n}^{\rightarrow}\)</span> is the word
frequency sequence of all words, then this sequence satisfies the
multinomial distribution:</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{n}^{\rightarrow}) &amp;=
Mult(\mathop{n}^{\rightarrow}|\mathop{p}^{\rightarrow},N) \\
&amp;= C_N ^{\mathop{n}^{\rightarrow}} \prod_{k=1}^V p_k^{n_k} \\
\end{aligned}
\]</span></p></li>
<li><p>Since the training data satisfies multiple distributions, we
naturally want to utilize the Dirichlet-Multinomial conjugacy, thus
assuming the prior distribution of <span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span> to be the
Dirichlet distribution:</p>
<p><span class="math display">\[
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})=\frac{1}{\int
\prod_{k=1}^V p_k^{\alpha _k -1}d\mathop{p}^{\rightarrow}} \prod_{k=1}^V
p_k^{\alpha _k -1}
\]</span></p>
<p><span class="math inline">\(V\)</span> is the size of the corpus
dictionary, and the parameters <span
class="math inline">\(\alpha\)</span> of the Dirichlet distribution need
to be set manually. After that, the posterior distribution of <span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span> is obtained
based on the conjugate after data correction:</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{p}^{\rightarrow}|W,\mathop{\alpha}^{\rightarrow}) &amp;=
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})+MultCount(\mathop{n}^{\rightarrow})
\\
&amp;=
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow}+\mathop{n}^{\rightarrow})
\\
\end{aligned}
\]</span></p></li>
<li><p>After obtaining the posterior, one can use maximum likelihood
estimation or mean estimation to calculate <span
class="math inline">\(\mathop{p}^{\rightarrow}\)</span> , here we use
the mean of the Dirichlet distribution from the posterior to estimate,
combining the previously mentioned properties of the Dirichlet
distribution, we have:</p>
<p><span class="math display">\[
\mathop{p_i}^{~}=\frac{n_i+\alpha _i}{\sum _{i=1}^{V} (n_i+\alpha _i)}
\]</span></p>
<p>The physical interpretation of this formula: Unlike the general use
of word frequency as an estimate, we first assume the word frequency
(i.e., the prior pseudo-count <span class="math inline">\(\alpha
_i\)</span> ), then add the word frequency given by the data <span
class="math inline">\(n_i\)</span> , and normalize it as a
probability.</p></li>
<li><p>Now that the probability distribution of word generation <span
class="math inline">\(\mathop{p}^{\rightarrow}\)</span> has been
obtained, the generation probability of documents under this
distribution is obviously:</p>
<p><span class="math display">\[
p(W|\mathop{p}^{\rightarrow})=\prod _{k=1}^V p_k^{n_k}
\]</span></p>
<p>The distribution of word generation probabilities, by bringing the
conditional generation probability of documents under the word
generation distribution into the previously mentioned document
probability integral formula, yields the generation probability of the
document under all distribution scenarios. Substituting and simplifying
it results in a very nice formula.</p>
<p><span class="math display">\[
p(W|\mathop{\alpha}^{\rightarrow})=\frac{\Delta(\mathop{\alpha}^{\rightarrow}+\mathop{n}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\]</span></p>
<p><span class="math inline">\(\Delta\)</span> is the normalization
factor:</p>
<p><span class="math display">\[
\Delta(\mathop{\alpha}^{\rightarrow})=\int \prod _{k=1}^V p_k^{\alpha _k
-1}d\mathop{p}^{\rightarrow}
\]</span></p></li>
</ul>
<h2 id="plsa-model">PLSA model</h2>
<ul>
<li>PLSA, or Probabilistic Latent Semantic Analysis model, posits that
there exists an implicit thematic hierarchy between documents and words.
Documents contain multiple themes, each corresponding to a distribution
of words. When generating words, the model first selects a theme and
then selects words from within that theme to generate them (in actual
computation, it is the probability summation of each theme).</li>
<li>Compared to the unigram model without Bayes, PLSA adds an additional
layer of topics between documents and words.</li>
<li>PLSA does not introduce Bayesian, it is just a model containing
latent variables, performing maximum likelihood estimation, so the
parameters can be iteratively learned using the EM algorithm, and the
specific calculations are omitted here.</li>
</ul>
<h2 id="role-in-lda-2">Role in LDA</h2>
<ul>
<li>Now, let's organize, the Unigram model mainly includes two parts
<ul>
<li>Word Generation Probability Distribution</li>
<li>Parameter Distribution of Word Generation Probability
Distribution</li>
</ul></li>
<li>PLSA model mainly consists of two parts
<ul>
<li>Word Generation Probability Distribution</li>
<li>Subject Generation Probability Distribution</li>
</ul></li>
<li>The Unigram model exhibits the distribution of the distribution,
which is the significance of introducing parameter priors for the word
distribution: making the word distribution a variable, turning the
selection of a word from rolling a die into first selecting a die, and
then rolling it to choose a word. Whether the introduction of priors is
actually useful is a topic of contention between the Bayesian school and
the frequentist school.</li>
<li>PLSA model provides a very intuitive modeling for human language
generation, introducing topics as implicit semantics, and defining the
distribution of topic representative words, viewing articles as a
mixture of topics.</li>
</ul>
<h1 id="lda-text-modeling">LDA text modeling</h1>
<h2 id="model-overview">Model Overview</h2>
<ul>
<li>LDA integrates the advantages of Unigram and PLSA, adding Dirichlet
prior assumptions for the word and topic dice respectively
<ul>
<li>Word Generation Probability Distribution (Temporarily Noted as
A)</li>
<li>Parameter Distribution of Word Generation Probability Distribution
(temporarily noted as B)</li>
<li>Subject Generation Probability Distribution (Temporarily Noted as
C)</li>
<li>Parameter Distribution of the Probability Distribution of Topic
Generation (temporarily denoted as D)</li>
</ul></li>
<li>It is easy for beginners to confuse that the topic generation
probability distribution is not the parameter distribution of the word
generation probability distribution; one must distinguish the
hierarchical relationship in the LDA model from the conjugate
relationships within each level. Additionally, the relationship between
topics and words is not one-to-many; they are many-to-many. In fact, in
the LDA model, a document is generated in this way (assuming there are K
topics):
<ul>
<li>Sample K from the A distribution under the condition of the B
distribution</li>
<li>For each document, a C distribution is obtained by drawing under the
condition of the D distribution, and the following process is repeated
to generate words:
<ul>
<li>From a C distribution, a topic z is sampled</li>
<li>From the z-th A distribution, a word is sampled</li>
</ul></li>
</ul></li>
<li>Assuming there are <span class="math inline">\(m\)</span> documents,
<span class="math inline">\(n\)</span> words, and <span
class="math inline">\(k\)</span> topics, then <span
class="math inline">\(D+C\)</span> is <span
class="math inline">\(m\)</span> independent Dirichlet-Multinomial
conjugates, and <span class="math inline">\(B+A\)</span> is <span
class="math inline">\(k\)</span> independent Dirichlet-Multinomial
conjugates. The two Dirichlet parameters are a k-dimensional vector (
<span class="math inline">\(\alpha\)</span> ) and an n-dimensional
vector ( <span class="math inline">\(\beta\)</span> ). Now we can
understand the illustration at the beginning of this paper, where we
express the symbols in their actual meanings, corresponding to the title
illustration. This illustration actually describes these <span
class="math inline">\(m+k\)</span> independent Dirichlet-Multinomial
conjugates in LDA:</li>
</ul>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0oGYq.png"
alt="i0oGYq.png" /> <img data-src="https://s1.ax1x.com/2018/10/20/i0oJf0.jpg"
alt="i0oJf0.jpg" /></p>
<h2 id="establish-distribution">Establish distribution</h2>
<ul>
<li><p>Now we can model the LDA topic model with <span
class="math inline">\(m+k\)</span> Dirichlet-Multinomial conjugate
pairs, drawing on the final document generation distribution derived
from the derivation of the Unigram model, we can separately
calculate:</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow}) = \prod
_{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\\
p(\mathop{w}^{\rightarrow}|\mathop{z}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\\
\end{aligned}
\]</span></p></li>
<li><p>The final result is the joint distribution of words and
topics:</p>
<p><span class="math display">\[
p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\]</span></p></li>
</ul>
<h2 id="sampling">Sampling</h2>
<ul>
<li><p>We mentioned earlier that we estimate model parameters within the
framework of Bayesian inference, which requires us to obtain the
posterior probability of topic assignments; here, Gibbs sampling is
needed to help approximate the posterior</p></li>
<li><p>According to the definition of Gibbs sampling, we need to sample
using the fully conditional probability of topic assignments <span
class="math inline">\(p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})\)</span> , to approximate
<span class="math inline">\(p(z_i=k|\mathop{w}^{\rightarrow})\)</span> ,
<span class="math inline">\(z_i\)</span> , which represent the topic of
the ith word (here the subscript i represents the nth word in the mth
document), while the vector w represents all the words we are currently
observing.</p></li>
<li><p>After establishing the entire probability model, we trained it
using the following methods: we set the hyperparameters, randomly
initialized various word frequency statistics (including the number of
words of topic k in article m, the number of words of vocabulary t
belonging to topic k, the total number of words in article m, and the
total number of words in topic k), then sequentially performed Gibbs
sampling on all words in the corpus, sampled their topics, and assigned
this topic to the word, and updated the four word frequencies (i.e.,
using conjugate posterior updates), cyclically sampled until
convergence, that is, the topic distribution after sampling basically
conforms to the topic distribution generated by the model under the
posterior probability, and the data can no longer provide more knowledge
to the model (no longer updated).</p>
<ul>
<li>Gibbs sampling requires restricting a certain dimension, sampling
according to the conditional probability of the other dimensions, in
text topic modeling, the dimension is the word, and calculating the
conditional probability of the other dimensions is to exclude the count
of the current word and its topic from the four word frequencies.</li>
<li>After sampling the topic, assign this topic to words, increase the
frequency counts of four words, if it has converged, then the topic
before and after sampling is the same, and if the word frequencies have
not changed, it is equivalent to the posterior not being updated with
knowledge from the data.</li>
</ul></li>
<li><p>Formula Derivation: The following section introduces two
derivation methods, the first being the derivation based on the
conjugate relationship as presented in the article "LDA Mathematical
Mysteries," and the second being the derivation based on the joint
distribution as described in the article "Parameter Estimation for Text
Analysis."</p></li>
<li><p>Based on the conjugation relationship, the following derivation
is obtained:</p></li>
<li><p>The object of the sample is the theme corresponding to the word,
with a probability of:</p>
<p><span class="math display">\[
p(z_i=k|\mathop{w}^{\rightarrow})
\]</span></p></li>
<li><p>Using Gibbs sampling to sample the topic of a word requires
calculating conditional probabilities with the topics of other words as
conditions:</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})
\]</span></p></li>
<li><p>By Bayes' theorem, this conditional probability is proportional
to (sampling, we can scale up the probabilities of each sample
proportionally):</p>
<p><span class="math display">\[
p(z_i=k,w_i=t|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})
\]</span></p></li>
<li><p>Expand this formula according to thematic distribution and word
distribution:</p>
<p><span class="math display">\[
\int p(z_i=k,w_i=t,\mathop{\vartheta _m}^{\rightarrow},\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} d\mathop{\varphi
_k}^{\rightarrow}
\]</span></p></li>
<li><p>Since all the conjugates are independent, the above expression
can be written as:</p>
<p><span class="math display">\[
\int p(z_i=k,\mathop{\vartheta _m}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})p(w_i=t,\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} d\mathop{\varphi
_k}^{\rightarrow}
\]</span></p></li>
<li><p>The probability chain is decomposed, and since the two
expressions are respectively related to the topic distribution and the
word distribution, they can be written as the product of two
integrals:</p>
<p><span class="math display">\[
\int p(z_i=k|\mathop{\vartheta _m}^{\rightarrow})p(\mathop{\vartheta
_m}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} \cdot \int
p(w_i=t|\mathop{\varphi _k}^{\rightarrow})p(\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\varphi _k}^{\rightarrow}
\]</span></p></li>
<li><p>Given the topic distribution of the mth document and the
distribution of the kth topic word, we seek the probability of the ith
word being t and the probability that the ith word corresponds to the
kth topic, then it is obvious that:</p>
<p><span class="math display">\[
p(z_i=k|\mathop{\vartheta _m}^{\rightarrow})=\mathop{\vartheta _{mk}} \\
p(w_i=t|\mathop{\varphi _k}^{\rightarrow})=\mathop{\varphi _{kt}} \\
\]</span></p></li>
<li><p>And according to the conjugate relationship, there is</p>
<p><span class="math display">\[
p(\mathop{\vartheta _m}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})=Dir(\mathop{\vartheta
_m}^{\rightarrow}|\mathop{n_{m,¬
i}}^{\rightarrow}+\mathop{\alpha}^{\rightarrow}) \\
p(\mathop{\varphi _k}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})=Dir(\mathop{\varphi
_k}^{\rightarrow}|\mathop{n_{k,¬
i}}^{\rightarrow}+\mathop{\beta}^{\rightarrow}) \\
\]</span></p></li>
<li><p>Therefore, the entire expression can be regarded as the product
of the k-th and t-th terms of the expected vectors of two Dirichlet
distributions. According to the properties of Dirichlet previously
mentioned, it is easy to obtain that these expectations are fractional
values obtained in proportion to the Dirichlet parameters, so the final
probability calculation is (note that it is proportional to):</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝\frac{n_{m,¬ i}^{(k)}+\alpha
_k}{\sum _{k=1}^K (n_{m,¬ i}^{(k)}+\alpha _k)} \cdot \frac{n_{k,¬
i}^{(t)}+\beta _t}{\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>This probability can be understood as (excluding the current i-th
token):</p>
<p><span class="math display">\[
(文档m中主题k所占的比例) * (主题k中词t所占的比例）
\]</span></p></li>
<li><p>Observed that the denominator of the first term is the sum of the
subject, which is actually unrelated to k, therefore it can be written
as:</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝
(n_{m,¬ i}^{(k)}+\alpha _k) \cdot \frac{n_{k,¬ i}^{(t)}+\beta _t}{\sum
_{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>We take another look at how to derive based on the joint
distribution</p></li>
<li><p>We have previously obtained the joint distribution of words and
topics:</p>
<p><span class="math display">\[
p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\]</span></p></li>
<li><p>According to Bayes' formula, there is</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})=\frac{p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow})}{p(\mathop{w}^{\rightarrow},\mathop{z_{¬
i}}^{\rightarrow})} \\
=\frac{p(\mathop{w}^{\rightarrow}|\mathop{z}^{\rightarrow})}
{p(\mathop{w_{¬ i}}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow})p(w_i)}
\cdot \frac{p(\mathop{z}^{\rightarrow})} {\mathop{p(z_{¬
i})}^{\rightarrow}} \\
\]</span></p></li>
<li><p>Since <span class="math inline">\(p(w_i)\)</span> is an
observable variable, we omit it, obtaining an expression proportional to
it. We then express this expression in the form of <span
class="math inline">\(\Delta\)</span> (fraction divided by fraction, the
denominators cancel each other out):</p>
<p><span class="math display">\[
∝
\frac{\Delta(\mathop{n_{z}}^{\rightarrow})+\mathop{\beta}^{\rightarrow}}{\Delta(\mathop{n_{z,¬
i}}^{\rightarrow})+\mathop{\beta}^{\rightarrow}} \cdot
\frac{\Delta(\mathop{n_{m}}^{\rightarrow})+\mathop{\alpha}^{\rightarrow}}{\Delta(\mathop{n_{m,¬
i}}^{\rightarrow})+\mathop{\alpha}^{\rightarrow}}
\]</span></p></li>
<li><p>By substituting the expression of <span
class="math inline">\(\Delta\)</span> into the calculation, we can also
obtain:</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝
(n_{m,¬ i}^{(k)}+\alpha _k) \cdot \frac{n_{k,¬ i}^{(t)}+\beta _t}{\sum
_{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>Text Analysis: A Pseudo-Algorithm Diagram of Gibbs Sampling for
Parameter Estimation</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oU6U.png" alt="i0oU6U.png" />
<figcaption aria-hidden="true">i0oU6U.png</figcaption>
</figure>
<ul>
<li><p>It can be seen that the conditional probability is mainly
calculated by recording four n values (two matrices and two vectors),
and the topic is also updated by updating four n values for incremental
updates. The algorithm first assigns initial values by random uniform
sampling, and then updates the topic according to the sampling formula
(first subtracting the old topic distribution and then adding the new
topic distribution), where formula 78 is the <span
class="math inline">\(p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})\)</span> we calculated
previously, and formulas 81 and 82 are the <span
class="math inline">\(\mathop{\vartheta _{mk}},\mathop{\varphi
_{kt}}\)</span> . We can directly obtain them from the four n values,
without considering the <span class="math inline">\(¬ i\)</span>
condition during sampling, specifically:</p>
<p><span class="math display">\[
\mathop{\vartheta _{mk}} = \frac{n_{m}^{(k)}+\alpha _k}{\sum _{k=1}^K
(n_{m}^{(t)}+\alpha _k)} \\
\mathop{\varphi _{kt}} = \frac{n_{k}^{(t)}+\beta _t}{\sum _{t=1}^V
(n_{k}^{(t)}+\beta _t)}
\]</span></p></li>
</ul>
<h2 id="training-and-testing">Training and Testing</h2>
<ul>
<li>Next, we train the LDA model, first randomly initializing the
Dirichlet parameters (prior), then using text to supplement data
knowledge, and finally obtaining the correct posterior. The training
process is iterative:
<ul>
<li>Iterate what? Sample and update the themes corresponding to the
words</li>
<li>What iteration? The complete conditional probability of Gibbs
sampling</li>
<li>Iterative effects? Changes in topic allocation, changes in
statistics, and changes in the fully conditional probability for the
next Gibbs sampling</li>
<li>When to iterate? Gibbs sampling converges, i.e., the distribution of
topics stabilizes and remains unchanged over a certain time interval, or
the degree of model convergence is measured according to perplexity and
other indicators.</li>
</ul></li>
<li>The difference between training and testing lies in that training
involves sampling and updating the entire document collection, with both
document-to-topic and topic-to-word distributions being updated, whereas
testing retains the topic-to-word distribution unchanged, only sampling
the current test document to convergence to obtain the topic
distribution of that document.</li>
<li>In fact, the two hyperparameters <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span> have undergone many iterations of
posterior substitution for prior after training, with the parameter
values becoming very large. <span class="math inline">\(\alpha\)</span>
discarded the final posterior result, and re-used the initial prior
values when generating the topic distribution for new documents. This
means that the document-to-topic distributions obtained during the
training set are actually not usable when testing new documents. We
utilize the topic-to-word distribution: because only the set of topics
is for the entire document space (training set and test set), the
topic-to-word distribution is also established on the dictionary of the
entire document space. We retain the final posterior results for these k
<span class="math inline">\(\beta\)</span> vectors because this
posterior has absorbed the likelihood knowledge of the data, resulting
in large parameter values and small uncertainty. Essentially, each <span
class="math inline">\(\beta\)</span> vector is equivalent to determining
a polynomial distribution from a topic to words, which is to say
determining a topic. We use these determined topics to test the
distribution of a new document across various topics. Therefore, when
testing new documents, the parameter <span
class="math inline">\(\alpha\)</span> is generally set to be symmetric,
i.e., with equal components (without prior preference for any particular
topic), and with small values (i.e., high uncertainty; otherwise, the
generated topic distribution would be uniform), which is analogous to
the idea of maximum entropy. The test is conducted by using known fixed
topics to obtain the document-to-topic distribution.</li>
<li>LDA training is actually an unparameterized Bayesian inference,
which can be implemented using MCMC and non-MCMC methods. Among the MCMC
methods, Gibbs sampling is often used, while non-MCMC methods can employ
variational inference and other iterative approaches to obtain
parameters.</li>
</ul>
<h1 id="lda-in-gensim">LDA in Gensim</h1>
<ul>
<li>Gensim's LDA provides several parameters, among which the default
value for <span class="math inline">\(\alpha\)</span> is as follows:
<blockquote>
<p>
alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of
length equal to the number of expected topics that expresses our
a-priori belief for the each topics’ probability. Alternatively, default
prior selecting strategies can be employed by supplying a string:
'asymmetric': Uses a fixed normalized asymmetric prior of 1.0 / topicno.
'default': Learns an asymmetric prior from the corpus.
</p>
<footer>
<strong>Gensim</strong><cite><a href="https://radimrehurek.com/gensim/models/ldamodel.html">models.ldamodel
– Latent Dirichlet Allocation</a></cite>
</footer>
</blockquote></li>
<li>gensim does not expose <span class="math inline">\(\beta\)</span> to
users; users can only set <span class="math inline">\(\alpha\)</span> ,
which can be customized and can also be set as symmetric or asymmetric.
The symmetric setting means all are 1, while the asymmetric setting fits
the Zipf law (?), and it is possible that <span
class="math inline">\(\beta\)</span> has an asymmetric default
setting.</li>
</ul>
<h1 id="more">More</h1>
<ul>
<li><p>Parameter estimation for text analysis points out that hidden
topics actually come from high-order co-occurrence relationships between
words</p></li>
<li><p>LDA is used for document query, where LDA is trained on
candidates, and a test is conducted each time a new query is
received</p>
<ul>
<li><p>Based on a similarity ranking method, the similarity between the
topic distribution of candidates and queries is calculated using JS
distance or KL divergence, and then sorted</p></li>
<li><p>Based on the Predictive likelihood ranking method, calculate the
probability of each candidate appearing for a given query, based on
thematic decomposition of topic z:</p>
<p><span class="math display">\[
\begin{aligned}
p\left(\vec{w}_{m} | \tilde{\vec{w}}_{\tilde{m}}\right)
&amp;=\sum_{k=1}^{K} p\left(\vec{w}_{m} | z=k\right) p\left(z=k |
\tilde{\vec{w}}_{\tilde{m}}\right) \\
&amp;=\sum_{k=1}^{K} \frac{p\left(z=k | \vec{w}_{m}\right)
p\left(\vec{w}_{m}\right)}{p(z=k)} p\left(z=k |
\tilde{\vec{w}}_{\tilde{m}}\right) \\
&amp;=\sum_{k=1}^{K} \vartheta_{m, k} \frac{n_{m}}{n_{k}}
\vartheta_{\tilde{m}, k}
\end{aligned}
\]</span></p></li>
</ul></li>
<li><p>LDA for clustering</p>
<ul>
<li>In fact, the distribution of topics is a soft clustering division of
documents. If each document is assigned to the topic with the highest
probability, that would be a hard division.</li>
<li>Or use topic distribution as the feature vector of the document and
then further cluster using various clustering algorithms</li>
<li>Evaluation of clustering results can utilize a known clustering
partitioning result as a reference, and assess using the Variation of
Information distance</li>
</ul></li>
<li><p>LDA evaluation metrics, perplexity, defined as the reciprocal
geometric mean of the likelihood measured on the validation set:</p>
<p><span class="math display">\[
\mathrm{P}(\tilde{\boldsymbol{W}} | \boldsymbol{M})=\prod_{m=1}^{M}
p\left(\tilde{\vec{w}}_{\tilde{m}} |
\mathcal{M}\right)^{-\frac{1}{N}}=\exp -\frac{\sum_{m=1}^{M} \log
p\left(\tilde{\bar{w}}_{\tilde{m}} | \mathcal{M}\right)}{\sum_{m=1}^{M}
N_{m}}
\]</span></p></li>
<li><p>Assuming the distribution of the validation set and the training
set is consistent, a high perplexity of LDA on the validation set
indicates a larger entropy, greater uncertainty, and that the model has
not yet learned a stable parameter.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="lda用来做什么">LDA用来做什么</h1>
<ul>
<li>LDA是一种主题模型，问题实际上是主题模型是用来做什么？用来表示文档
。在这里将文档看成一个词袋。</li>
<li>如果将词典里每一个词看成一个特征，tfidf值作为特征值大小来表示文档，则文档的特征向量太过稀疏，且维度太高</li>
<li>LSI的解决办法是，将文档-词的矩阵进行奇异值分解，降维，但是这样得到的降维空间，即词到文档之间的隐变量无法解释，纯数学的方法，太暴力</li>
<li>PLSA提出了隐变量应该是主题，可以把文档表示为主题向量，而主题定义为在词典上的某一种多项式分布，这样PLSA中包含了两层多项式分布：文档到主题的多项式分布（文档中各个主题的混合比例，即文档的特征向量），主题到词的多项式分布（在整个词典上的概率分布，表示不同主题下各个词出现的概率）</li>
<li>LDA则对这两个多项式分布的参数制定了迪利克雷先验，为PLSA引入贝叶斯框架</li>
</ul>
<h1 id="贝叶斯模型">贝叶斯模型</h1>
<ul>
<li><p>LDA是一种贝叶斯模型</p></li>
<li><p>给定训练数据，贝叶斯模型怎么学习参数(参数的分布）：贝叶斯估计</p>
<ul>
<li><p>先给参数一个先验分布<span
class="math inline">\(p(\theta)\)</span></p></li>
<li><p>给定数据，计算似然<span
class="math inline">\(p(X|\theta)\)</span>和evidence<span
class="math inline">\(P(X)\)</span>，根据贝叶斯公式计算参数的后验分布</p></li>
<li><p>后验分布就是学习到的参数分布</p>
<p><span class="math display">\[
p(\vartheta | X)=\frac{p(X | \vartheta) \cdot
p(\vartheta)}{p(\mathcal{X})}
\]</span></p></li>
</ul></li>
<li><p>可能数据太多，那就分批更新，上一次更新后得到的后验作为下一次更新的先验，类似于随机梯度下降中的思想</p></li>
<li><p>对于新数据的似然，不像最大似然估计或者最大后验估计，两者是点估计，直接算<span
class="math inline">\(p(x_{new}|\theta _{ML})\)</span>,<span
class="math inline">\(p(x_{new}|\theta
_{MAP})\)</span>，贝叶斯估计需要对参数分布求积分：</p>
<p><span class="math display">\[
\begin{aligned}
p(\tilde{x} | X) &amp;=\int_{\vartheta \in \Theta} p(\tilde{x} |
\vartheta) p(\vartheta | X) \mathrm{d} \vartheta \\
&amp;=\int_{\vartheta \in \Theta} p(\tilde{x} | \vartheta) \frac{p(X |
\vartheta) p(\vartheta)}{p(X)} \mathrm{d} \vartheta
\end{aligned}
\]</span></p></li>
<li><p>贝叶斯估计里常常出现两个问题</p>
<ul>
<li>有了先验，有了似然，计算出的后验很复杂</li>
<li>evidence很难算</li>
</ul></li>
<li><p>因此共轭和gibbs sampling分别解决这两个问题</p>
<ul>
<li>共轭：给定似然，找一个先验，使得后验的形式和先验一致</li>
<li>gibbs
sampling：利用MCMC的思路去近似后验，而不用显式的计算evidence</li>
</ul></li>
<li><p>假如是带隐变量的贝叶斯模型，那么有</p>
<p><span class="math display">\[
p(\vartheta|x) = \int_{z} p(\vartheta|z)p(z|x)
\]</span></p></li>
<li><p>在LDA当中，隐变量z就是词所属的主题（词的主题分配），那么<span
class="math inline">\(p(\vartheta|z)\)</span>自然是很好求的，那么剩下的<span
class="math inline">\(p(z|x)\)</span>，就再套上面贝叶斯推断的公式:</p>
<p><span class="math display">\[
p(z | X)=\frac{p(X | z) \cdot p(z)}{p(\mathcal{X})}
\]</span></p></li>
<li><p>在介绍LDA之前，介绍了其他两个模型，带贝叶斯的unigram以及plsa，前者可以看成是没有主题层的LDA，后者可以看成是没有贝叶斯的LDA</p></li>
<li><p>接下来就分别介绍共轭、gibbs
sampling、带贝叶斯的unigram/plsa，最后介绍LDA</p></li>
</ul>
<h1 id="共轭">共轭</h1>
<h2 id="gamma函数">Gamma函数</h2>
<ul>
<li><p>定义</p>
<p><span class="math display">\[
\Gamma (x) = \int _0 ^{\infty} t^{x-1} e^{-t} dt
\]</span></p></li>
<li><p>因为其递归性质<span
class="math inline">\(\Gamma(x+1)=x\Gamma(x)\)</span>，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数</p></li>
<li><p>Bohr-Mullerup定理：如果<span class="math inline">\(f:(0,\infty)
\rightarrow (0,\infty)\)</span>，且满足</p>
<ul>
<li><span class="math inline">\(f(1)=1\)</span></li>
<li><span class="math inline">\(f(x+1)=xf(x)\)</span></li>
<li><span class="math inline">\(log f(x)\)</span>是凸函数 那么<span
class="math inline">\(f(x)=\Gamma(x)\)</span></li>
</ul></li>
<li><p>Digamma函数</p>
<p><span class="math display">\[
\psi (x)=\frac{d log \Gamma(x)}{dx}
\]</span></p>
<p>其具有以下性质</p>
<p><span class="math display">\[
\psi (x+1)=\psi (x)+\frac 1x
\]</span></p></li>
<li><p>在用变分推断对LDA进行推断时结果就是digamma函数的形式</p></li>
</ul>
<h2 id="gamma分布">Gamma分布</h2>
<ul>
<li><p>将上式变换</p>
<p><span class="math display">\[
\int _0 ^{\infty} \frac{x^{\alpha -1}e^{-x}}{\Gamma(\alpha)}dx = 1
\]</span></p>
<p>因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：</p>
<p><span class="math display">\[
Gamma_{\alpha}(x)=\frac{x^{\alpha -1}e^{-x}}{\Gamma(\alpha)}
\]</span></p></li>
<li><p>指数分布和<span class="math inline">\(\chi
^2\)</span>分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。</p></li>
<li><p>Gamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。</p>
<p><span class="math display">\[
Poisson(X=k|\lambda)=\frac{\lambda ^k e^{-\lambda}}{k!}
\]</span></p></li>
<li><p>令二项分布中<span
class="math inline">\(np=\lambda\)</span>，当n趋向于无穷且p趋向于0时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值<span
class="math inline">\(\lambda\)</span>时，将时间分为无穷个段，几乎是连续的，取<span
class="math inline">\(p=\frac{\lambda}{n}\)</span>带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。</p></li>
</ul>
<h2 id="beta分布">Beta分布</h2>
<ul>
<li>背景：
<ul>
<li><p>现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量<span
class="math inline">\(p=x_k\)</span>的分布？</p></li>
<li><p>为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率<span
class="math inline">\(x \leq X_{k} \leq x+\Delta x\)</span></p></li>
<li><p>将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为</p>
<p><span class="math display">\[
\begin{aligned}
P(E)&amp;=x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\
&amp;=x^{k-1}(1-x)^{n-k}\Delta x+\omicron (\Delta x) \\
\end{aligned}
\]</span></p></li>
<li><p>若小区间内有两个及两个以上的数，计算可得这种情况的概率是<span
class="math inline">\(\omicron (\Delta
x)\)</span>，因此只考虑小区间内只有第k大的数，此时令<span
class="math inline">\(\Delta
x\)</span>趋向于0，则得到第k大数的概率密度函数（注意事件E的系数应该是<span
class="math inline">\(nC_n^k\)</span>）：</p>
<p><span class="math display">\[
f(x)=\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \quad x \in [0,1]
\]</span></p>
<p>用Gamma函数表示（令<span class="math inline">\(\alpha =k,\beta =
n-k+1\)</span>）得到:</p>
<p><span class="math display">\[
\begin{aligned}
f(x)&amp;=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\\
&amp;=Beta(p|k,n-k+1) \\
&amp;=Beta(\alpha,\beta) \\
\end{aligned}
\]</span></p>
<p>这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。</p></li>
</ul></li>
<li>Beta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是<span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>的作用（因为<span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在<span
class="math inline">\(\frac kn\)</span>这个位置。</li>
<li>因此Beta分布的参数（<span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。</li>
</ul>
<h2 id="beta-binomial共轭">Beta-Binomial共轭</h2>
<ul>
<li>这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。</li>
<li>假设要猜的数<span
class="math inline">\(p=X_k\)</span>，现在我们知道了有<span
class="math inline">\(m_1\)</span>个数比<span
class="math inline">\(p\)</span>小，<span
class="math inline">\(m_2\)</span>个数比<span
class="math inline">\(p\)</span>大，显然此时<span
class="math inline">\(p\)</span>的概率密度函数就变成了<span
class="math inline">\(Beta(p|k+m_1,n-k+1+m_2)\)</span></li>
<li>补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比<span
class="math inline">\(p\)</span>大还是小，<span
class="math inline">\(p=X_k\)</span>的数值便可以代表这一概率，因此<span
class="math inline">\(m_1\)</span>服从二项分布<span
class="math inline">\(B(m,p)\)</span></li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">先验</th>
<th style="text-align: center;">数据知识</th>
<th style="text-align: center;">后验</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Beta分布</td>
<td style="text-align: center;">二项分布</td>
<td style="text-align: center;">Beta分布</td>
</tr>
</tbody>
</table>
<ul>
<li><p>因此我们可以得到Beta-Binomial共轭</p>
<p><span class="math display">\[
Beta(p|\alpha,\beta)+BinomCount(m_1,m_2)=Beta(p|\alpha+m_1,\beta+m_2)
\]</span></p>
<p>即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数<span
class="math inline">\(\alpha,\beta\)</span>称为伪计数，表示物理计数。</p></li>
<li><p>可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了<span
class="math inline">\(m_1\)</span>次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是<span
class="math inline">\(Beta(p|m_1+1,m_2+1)\)</span></p></li>
<li><p>通过这个共轭，我们可以推出关于二项分布的一个重要公式：</p>
<p><span class="math display">\[
P(C \leq k) = \frac{n!}{k!(n-k-1)!}\int _p ^1 t^k(1-t)^{n-k-1}dt \quad C
\sim B(n,p)
\]</span></p>
<p>现在可以证明如下：</p>
<ul>
<li><p>式子左边是二项分布的概率累积，式子右边时是<span
class="math inline">\(Beta(t|k+1,n-k)\)</span>分布的概率积分</p></li>
<li><p>取n个随机变量，均匀分布于[0,1]，对于二项分布<span
class="math inline">\(B(n,p)\)</span>，若数小于<span
class="math inline">\(p\)</span>则是成功，否则失败，则n个随机变量小于<span
class="math inline">\(p\)</span>的个数C符合二项分布<span
class="math inline">\(B(n,p)\)</span></p></li>
<li><p>此时可以得到<span class="math inline">\(P(C \leq
k)=P(X_{k+1}&gt;p)\)</span>，即n个随机变量按顺序排好后，小于<span
class="math inline">\(p\)</span>的有<span
class="math inline">\(k\)</span>个</p></li>
<li><p>这时利用我们对第k大数的概率密度计算出为Beta分布，带入有</p>
<p><span class="math display">\[
\begin{aligned}
P(C \leq k) &amp;=P(X_{k+1} &gt; p) \\
&amp;=\int _p ^1 Beta(t|k+1,n-k)dt \\
&amp;=\frac{n!}{k!(n-k-1)!}\int _p ^1 t^k(1-t)^{n-k-1}dt \\
\end{aligned}
\]</span></p>
<p>即证</p></li>
</ul></li>
<li><p>通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。</p></li>
<li><p>在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。</p></li>
</ul>
<h2 id="dirichlet-multinomial共轭">Dirichlet-Multinomial共轭</h2>
<ul>
<li><p>假设我们不仅要猜一个数，还要猜两个数<span
class="math inline">\(x_{k_1},x_{k_1+k_2}\)</span>的联合分布，该如何计算？</p></li>
<li><p>同理，我们设置两个极小区间<span class="math inline">\(\Delta
x\)</span>，将整个区间分为五块，极小区间之间分别为<span
class="math inline">\(x_1,x_2,x_3\)</span>计算之后可以得到</p>
<p><span class="math display">\[
f(x_1,x_2,x_3)=\frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}
\]</span></p></li>
<li><p>整理一下可以写成</p>
<p><span class="math display">\[
f(x_1,x_2,x_3)=\frac{\Gamma(\alpha _1+\alpha _2+\alpha
_3)}{\Gamma(\alpha _1)\Gamma(\alpha _2)\Gamma(\alpha _3)}x_1^{\alpha
_1-1}x_2^{\alpha _2-1}x_3^{\alpha _3-1}
\]</span></p>
<p>这就是3维形式的Dirichlet分布。其中<span
class="math inline">\(x_1,x_2,x_3\)</span>（实际上只有两个变量）确定了两个顺序数联合分布，<span
class="math inline">\(f\)</span>代表概率密度。</p></li>
<li><p>注意到在<span
class="math inline">\(\alpha\)</span>确定的情况下，前面的一堆gamma函数其实是概率归一化的分母，后文我们将Dirichlet分布更一般的写成：</p>
<p><span class="math display">\[
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})=\frac{1}{\int
\prod_{k=1}^V p_k^{\alpha _k -1}d\mathop{p}^{\rightarrow}} \prod_{k=1}^V
p_k^{\alpha _k -1}
\]</span></p></li>
<li><p>其中归一化分母，也即那一堆gamma，令其为：</p>
<p><span class="math display">\[
\Delta(\mathop{\alpha}^{\rightarrow})=\int \prod _{k=1}^V p_k^{\alpha _k
-1}d\mathop{p}^{\rightarrow}
\]</span></p></li>
<li><p>同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布</p>
<p><span class="math display">\[
Dir(p|\alpha)+MultCount(m)=Dir(p|\alpha+m)
\]</span></p>
<p>上式中的参数均是向量，对应多维情况。</p></li>
<li><p>无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，<span
class="math inline">\(E(p)=\frac{\alpha}{\alpha+\beta}\)</span>，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。</p></li>
</ul>
<h2 id="dirichlet分析">Dirichlet分析</h2>
<ul>
<li>根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为<span
class="math inline">\(p_1,p_2,p_3\)</span>，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0orkR.png" alt="i0orkR.png" />
<figcaption aria-hidden="true">i0orkR.png</figcaption>
</figure>
<ul>
<li><span
class="math inline">\(\alpha\)</span>控制了多项式分布参数的mean
shape和sparsity。</li>
<li>最左边，Dirichlet的三个参数<span class="math inline">\(\alpha
_1,\alpha _2,\alpha _3\)</span>相等，代表其红色区域位置居中，且三个<span
class="math inline">\(\alpha\)</span>的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数<span
class="math inline">\(p_1,p_2,p_3\)</span>来说，较大可能取到三个p等值的情况。</li>
<li>中间的图，三个<span
class="math inline">\(\alpha\)</span>不相等，某一个<span
class="math inline">\(\alpha\)</span>偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。</li>
<li>最右边，同最左边，三个<span
class="math inline">\(\alpha\)</span>相等，红色区域居中，但是<span
class="math inline">\(\alpha\)</span>的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）</li>
<li>因此可以看出，<span
class="math inline">\(\alpha\)</span>的比例决定了多项式分布高概率的位置，也就是主要确定了各个<span
class="math inline">\(p\)</span>的比例，定好concentration，而<span
class="math inline">\(\alpha\)</span>的大小决定了这个位置的集中情况，<span
class="math inline">\(\alpha\)</span>越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。</li>
<li>当<span
class="math inline">\(\alpha\)</span>远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成<span
class="math inline">\(alpha\)</span>从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。</li>
<li>再来看看维基百科中关于Dirichlet参数<span
class="math inline">\(\alpha\)</span>的描述：
<blockquote>
<p>
The concentration parameter Dirichlet distributions are very often used
as prior distributions in Bayesian inference. The simplest and perhaps
most common type of Dirichlet prior is the symmetric Dirichlet
distribution, where all parameters are equal. This corresponds to the
case where you have no prior information to favor one component over any
other. As described above, the single value α to which all parameters
are set is called the concentration parameter. If the sample space of
the Dirichlet distribution is interpreted as a discrete probability
distribution, then intuitively the concentration parameter can be
thought of as determining how "concentrated" the probability mass of a
sample from a Dirichlet distribution is likely to be. With a value much
less than 1, the mass will be highly concentrated in a few components,
and all the rest will have almost no mass. With a value much greater
than 1, the mass will be dispersed almost equally among all the
components. See the article on the concentration parameter for further
discussion.
</p>
<footer>
<strong>Dirichlet_distribution</strong><cite><a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters">Intuitive
interpretations of the parameters</a></cite>
</footer>
</blockquote></li>
<li>当<span
class="math inline">\(\alpha\)</span>远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。<span
class="math inline">\(\alpha\)</span>远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。</li>
</ul>
<h2 id="role-in-lda">Role in LDA</h2>
<ul>
<li>总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭）
，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。</li>
<li>LDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题和主题-词语两个multinomial分布都假设其参数先验为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验。</li>
<li>为了介绍Dirichlet-Multinomial共轭，先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。</li>
<li>为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。</li>
<li>在Dirichlet Process中<span
class="math inline">\(\alpha\)</span>的大小体现了Dirichlet分布拟合Base
Measure时的离散程度，<span
class="math inline">\(\alpha\)</span>越大，越不离散，各个项均能得到差不多的概率。</li>
<li>对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。</li>
</ul>
<h1 id="吉步斯采样">吉步斯采样</h1>
<h2 id="随机模拟">随机模拟</h2>
<ul>
<li>即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本，并用这些样本的统计量来估计原分布一些不好直接解析计算的参数。</li>
<li>马尔可夫是指产生随机样本的方法依赖于马氏链的性质，通过构造马氏链当中的转移矩阵，使得马氏链收敛时能够产生满足给定分布的样本序列。</li>
<li>MCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。</li>
</ul>
<h2 id="马氏链">马氏链</h2>
<ul>
<li>马氏链即状态转移的概率只依赖于前一个状态</li>
<li>因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果</li>
<li>矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布</li>
<li>关于马氏链收敛的定义
<ul>
<li>如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则<span
class="math inline">\(lim_{n \rightarrow
\infty}P_{ij}^n\)</span>存在且与<span
class="math inline">\(i\)</span>无关，记这个收敛的矩阵为<span
class="math inline">\(\pi(j)\)</span></li>
<li><span class="math inline">\(\pi (j)=\sum _{i=0}^{\infty} \pi (i)
P_{ij}\)</span></li>
<li><span class="math inline">\(\pi\)</span>是方程$P =
$的唯一非负解</li>
<li><span class="math inline">\(\pi\)</span>称为马氏链的平稳分布</li>
</ul></li>
</ul>
<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>
<ul>
<li><p>回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。</p></li>
<li><p>因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若<span
class="math inline">\(\pi(i)P_{ij}=\pi(j)P_{ij} \quad for \quad all
\quad i,j\)</span>，则<span
class="math inline">\(\pi(x)\)</span>是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。</p></li>
<li><p>设从状态i转移到状态j的概率为<span
class="math inline">\(q(i,j)\)</span>，若<span
class="math inline">\(p(i)q(i,j)=p(j)q(j,i)\)</span>，那么此时<span
class="math inline">\(p(x)\)</span>就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知<span
class="math inline">\(p(x)\)</span>的情况下，我们需要对<span
class="math inline">\(q\)</span>进行改造，为此我们乘上一个接受率<span
class="math inline">\(\alpha\)</span>，使得：</p>
<p><span class="math display">\[
p(i)q(i,j)\alpha (i,j)=p(j)q(j,i)\alpha (j,i)
\]</span></p>
<p>为什么叫接受率？因为可以理解为这个<span
class="math inline">\(\alpha\)</span>是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。</p></li>
<li><p>如何确定接受率？其实显而易见<span class="math inline">\(\alpha
(i,j)=p(j)q(j,i)\)</span>，对称构造即可。</p></li>
<li><p>因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。</p></li>
<li><p>这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式<span
class="math inline">\(p(i)q(i,j)\alpha
(i,j)=p(j)q(j,i)\alpha(j,i)\)</span>两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。</p></li>
</ul>
<h2 id="gibbs-sampling">Gibbs Sampling</h2>
<ul>
<li><p>之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。</p></li>
<li><p>对二维概率分布<span
class="math inline">\(p(x,y)\)</span>，易得到</p>
<p><span class="math display">\[
\begin{aligned}
p(x_1,y_1)p(y_2|x_1) &amp; =p(x_1)p(y_1|x_1)p(y_2|x_1) \\
&amp; =p(x_1)p(y_2|x_1)p(y_1|x_1) \\
&amp; =p(x_1,y_2)p(y_1|x_1) \\
\end{aligned}
\]</span></p></li>
<li><p>从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定<span
class="math inline">\(x=x_1\)</span>，则<span
class="math inline">\(p(y|x_1)\)</span>可以作为直线<span
class="math inline">\(x=x_1\)</span>上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：</p>
<ul>
<li>若两点在垂直线<span class="math inline">\(x=x_1\)</span>上，则<span
class="math inline">\(Q=p(y|x_1)\)</span></li>
<li>若两点在水平线<span class="math inline">\(y=y_1\)</span>上，则<span
class="math inline">\(Q=p(x|y_1)\)</span></li>
<li>若两点连线既不垂直也水平，则<span class="math inline">\(Q=0\)</span>
这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二维平面上的马氏链将收敛到<span
class="math inline">\(p(x,y)\)</span>。</li>
</ul></li>
<li><p>gibbs采样得到新的x维度之后，在计算新的y维度时是依赖了新的x维度，因为是在之前选定坐标轴转换的基础上再进行转移，不然无法跳转到新状态<span
class="math inline">\((x_2,y_2)\)</span>，你得到的实际上是<span
class="math inline">\((x_1,y_2)\)</span>和<span
class="math inline">\((x_2,y_1)\)</span>。</p></li>
<li><p>因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，从细致平稳条件的公式可以看到这个平稳是可以传递的，如果从某一个维度的转移满足平稳条件，之后接着另一个维度，那么两次转移所等效的一次转移也是平稳的。</p></li>
<li><p>等到所有维度都转移了一次，就得到了一个新的样本。等到马氏链收敛之后形成的样本序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。虽然每次随机选择坐标轴会导致中途计算出来的新的维度值不一样，但是平稳条件没有打破，最终能够收敛到一样的给定分布。</p></li>
<li><p>同样，上述算法也可以推广到多维。扩展到多维时，在<span
class="math inline">\(x\)</span>轴上构建的的转移概率就是<span
class="math inline">\(Q=p(x|¬
x)\)</span>。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。</p></li>
</ul>
<h2 id="role-in-lda-1">Role in LDA</h2>
<ul>
<li>首先明确，MCMC方法是产生已知分布的样本，但是gibbs采样只需要使用完全条件概率，产生了满足联合分布的样本，而不像一般采样方法直接从联合分布中采样</li>
<li>gibbs采样这种特性就使其可以在不知道联合概率的情况下去推断参数，进一步推出联合概率分布</li>
<li>但是在LDA中，并没有用gibbs
sampling去直接推断参数，而是用其去近似后验，完成用数据知识更新先验这一步。而且由于LDA存在着主题这一隐变量，gibbs采样的联合分布并不是文档的主题分布或者主题的词分布，和LDA模型的参数没有直接挂钩。Gibbs
sampling在LDA中采样的是token的主题分配，即隐变量的分布。</li>
<li>但是所有token主题分配确定之后，LDA模型的参数就确定了，通过古典概型（最大似然估计）就可以得到两个multinomial分布（参数），对应的dirichelt分布（参数后验分布）也得到更新。且由于引入了主题分解了文档-单词矩阵，实际上我们不需要维护<span
class="math inline">\(Document \* word\)</span>矩阵，而是维护<span
class="math inline">\(Document \* topic + topic \*
word\)</span>即可。</li>
<li>gibbs采样词的主题分配，实际上是在计算隐变量分布的后验，进而得到参数分布的后验。</li>
<li>gibbs采样中在不断更新参数，例如本次迭代更新<span
class="math inline">\(p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)\)</span>，则下一次迭代为<span
class="math inline">\(p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)\)</span>，即使用更新之后的<span
class="math inline">\(x_1^{t+1}\)</span>来计算。在LDA中，这一过程通过更新被采样单词的主题实现。贝叶斯推断中将数据分批，用后验更新先验的迭代，在这里被进一步细化到了gibbs
sampling的每一次坐标更新。</li>
<li>下文可以看到gibbs采样公式，可以解释为根据其他词的主题分配情况决定自己的主题分配，迭代更新所有词的主题分配；具体如何决定，包含了两个部分，这两个部分类似于tf和idf提供的信息。</li>
<li>当根据采样公式计算出主题分配的后验时，我们并没有直接得到参数的后验分布，但是当根据主题分配后验采样出新主题，更新了统计量之后，由于gibbs
sampling公式里本身包含了统计量，这里相当于计算后验和用后验更新先验一步完成。或者也可以理解成，LDA里一直做的是主题分配分布（隐变量）的贝叶斯推断，即<span
class="math inline">\(p(topic|word,doc)\)</span>，做完之后根据主题分配，做一次最大似然估计（古典概型）就能得到模型的参数。</li>
</ul>
<h1 id="文本建模">文本建模</h1>
<ul>
<li>接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：
<ul>
<li>模型是怎样的？</li>
<li>各个词的生成概率或者说模型参数是多少？</li>
</ul></li>
</ul>
<h2 id="unigram模型">Unigram模型</h2>
<ul>
<li><p>模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率。</p></li>
<li><p>这里为unigram引入一层贝叶斯框架，为后文LDA两层贝叶斯框架的推导铺垫。贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。</p></li>
<li><p>也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为<span
class="math inline">\(\mathop{p}^{\rightarrow}\)</span>，同时这个词生成分布也遵循一个分布，设为<span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span>。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：</p>
<p><span class="math display">\[
p(W)=\int
p(W|\mathop{p}^{\rightarrow})p(\mathop{p}^{\rightarrow})d\mathop{p}^{\rightarrow}
\]</span></p></li>
<li><p>按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设<span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span>，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设<span
class="math inline">\(\mathop{n}^{\rightarrow}\)</span>是所有词的词频序列，则这个序列满足多项分布：</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{n}^{\rightarrow}) &amp;=
Mult(\mathop{n}^{\rightarrow}|\mathop{p}^{\rightarrow},N) \\
&amp;= C_N ^{\mathop{n}^{\rightarrow}} \prod_{k=1}^V p_k^{n_k} \\
\end{aligned}
\]</span></p></li>
<li><p>既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设<span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span>的先验分布为Dirichlet分布：</p>
<p><span class="math display">\[
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})=\frac{1}{\int
\prod_{k=1}^V p_k^{\alpha _k -1}d\mathop{p}^{\rightarrow}} \prod_{k=1}^V
p_k^{\alpha _k -1}
\]</span></p>
<p>其中<span
class="math inline">\(V\)</span>是语料词典大小，Dirichlet分布的参数<span
class="math inline">\(\alpha\)</span>需要自己设置。之后根据共轭得到数据修正之后<span
class="math inline">\(p(\mathop{p}^{\rightarrow})\)</span>的后验分布：</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{p}^{\rightarrow}|W,\mathop{\alpha}^{\rightarrow}) &amp;=
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow})+MultCount(\mathop{n}^{\rightarrow})
\\
&amp;=
Dir(\mathop{p}^{\rightarrow}|\mathop{\alpha}^{\rightarrow}+\mathop{n}^{\rightarrow})
\\
\end{aligned}
\]</span></p></li>
<li><p>得到后验之后，可以使用极大似然估计或者均值估计来计算<span
class="math inline">\(\mathop{p}^{\rightarrow}\)</span>，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：</p>
<p><span class="math display">\[
\mathop{p_i}^{~}=\frac{n_i+\alpha _i}{\sum _{i=1}^{V} (n_i+\alpha _i)}
\]</span></p>
<p>这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数<span
class="math inline">\(\alpha _i\)</span>），然后加上数据给出的词频<span
class="math inline">\(n_i\)</span>，再归一化作为概率。</p></li>
<li><p>现在得到了词语的生成概率分布<span
class="math inline">\(\mathop{p}^{\rightarrow}\)</span>，那么在此分布下的文档的生成概率显然为：</p>
<p><span class="math display">\[
p(W|\mathop{p}^{\rightarrow})=\prod _{k=1}^V p_k^{n_k}
\]</span></p>
<p>将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。</p>
<p><span class="math display">\[
p(W|\mathop{\alpha}^{\rightarrow})=\frac{\Delta(\mathop{\alpha}^{\rightarrow}+\mathop{n}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\]</span></p>
<p>其中<span class="math inline">\(\Delta\)</span>是归一化因子：</p>
<p><span class="math display">\[
\Delta(\mathop{\alpha}^{\rightarrow})=\int \prod _{k=1}^V p_k^{\alpha _k
-1}d\mathop{p}^{\rightarrow}
\]</span></p></li>
</ul>
<h2 id="plsa模型">PLSA模型</h2>
<ul>
<li>PLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。</li>
<li>与没有贝叶斯的unigram模型相比，plsa在文档和词之间加了一层，主题。</li>
<li>PLSA没有引入贝叶斯，只是一个包含隐变量的模型，做最大似然估计，那么可以用EM算法迭代学习到参数，具体的计算在这里就略过了。</li>
</ul>
<h2 id="role-in-lda-2">Role in LDA</h2>
<ul>
<li>现在整理一下，Unigram模型中主要包含两部分
<ul>
<li>词生成概率分布</li>
<li>词生成概率分布的参数分布</li>
</ul></li>
<li>PLSA模型主要包含两部分
<ul>
<li>词生成概率分布</li>
<li>主题生成概率分布</li>
</ul></li>
<li>Unigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了。</li>
<li>PLSA模型为人类语言生成提供了一个很直观的建模，引入了主题作为隐含语义，并定义了主题代表词的分布，将文章看成是主题的混合。</li>
</ul>
<h1 id="lda文本建模">LDA文本建模</h1>
<h2 id="模型概述">模型概述</h2>
<ul>
<li>LDA整合了Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设
<ul>
<li>词生成概率分布（暂记A）</li>
<li>词生成概率分布的参数分布（暂记B）</li>
<li>主题生成概率分布（暂记C）</li>
<li>主题生成概率分布的参数分布（暂记D）</li>
</ul></li>
<li>这里初学容易混淆的是，主题生成概率分布并不是词生成概率分布的参数分布，要区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：
<ul>
<li>先在B分布条件下抽样得到K个A分布</li>
<li>对每一篇文档，在符合D分布条件下抽取得到一个C分布，重复如下过程生成词：
<ul>
<li>从C分布中抽样得到一个主题z</li>
<li>从第z个A分布中抽样得到一个单词</li>
</ul></li>
</ul></li>
<li>假设有<span class="math inline">\(m\)</span>篇文档，<span
class="math inline">\(n\)</span>个词，<span
class="math inline">\(k\)</span>个主题，则<span
class="math inline">\(D+C\)</span>是<span
class="math inline">\(m\)</span>个独立的Dirichlet-Multinomial共轭，<span
class="math inline">\(B+A\)</span>是<span
class="math inline">\(k\)</span>个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量(<span
class="math inline">\(\alpha\)</span>)和1个n维向量(<span
class="math inline">\(\beta\)</span>)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这<span
class="math inline">\(m+k\)</span>个独立的Dirichlet-Multinomial共轭：</li>
</ul>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0oGYq.png"
alt="i0oGYq.png" /> <img data-src="https://s1.ax1x.com/2018/10/20/i0oJf0.jpg"
alt="i0oJf0.jpg" /></p>
<h2 id="建立分布">建立分布</h2>
<ul>
<li><p>现在我们可以用<span
class="math inline">\(m+k\)</span>个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：</p>
<p><span class="math display">\[
\begin{aligned}
p(\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow}) = \prod
_{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\\
p(\mathop{w}^{\rightarrow}|\mathop{z}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\\
\end{aligned}
\]</span></p></li>
<li><p>最终得到词与主题的联合分布:</p>
<p><span class="math display">\[
p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\]</span></p></li>
</ul>
<h2 id="采样">采样</h2>
<ul>
<li><p>在前面说了，我们按照贝叶斯推断的框架估计模型参数，需要直到主题分配的后验概率，这里就需要gibbs
sampling来帮忙对后验做一个近似</p></li>
<li><p>按照gibbs sampling的定义，我们需要主题分配的完全条件概率<span
class="math inline">\(p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})\)</span>去采样，进而近似<span
class="math inline">\(p(z_i=k|\mathop{w}^{\rightarrow})\)</span>，<span
class="math inline">\(z_i\)</span>代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。</p></li>
<li><p>在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。</p>
<ul>
<li>其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。</li>
<li>采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。</li>
</ul></li>
<li><p>公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter
estimation for text analysis 一文中基于联合分布做出的推导</p></li>
<li><p>基于共轭关系推导如下：</p></li>
<li><p>采样的对象是词所对应的主题，概率为：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{w}^{\rightarrow})
\]</span></p></li>
<li><p>使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})
\]</span></p></li>
<li><p>由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：</p>
<p><span class="math display">\[
p(z_i=k,w_i=t|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})
\]</span></p></li>
<li><p>把这个公式按主题分布和词分布展开：</p>
<p><span class="math display">\[
\int p(z_i=k,w_i=t,\mathop{\vartheta _m}^{\rightarrow},\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} d\mathop{\varphi
_k}^{\rightarrow}
\]</span></p></li>
<li><p>由于所有的共轭都是独立的，上式可以写成：</p>
<p><span class="math display">\[
\int p(z_i=k,\mathop{\vartheta _m}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})p(w_i=t,\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} d\mathop{\varphi
_k}^{\rightarrow}
\]</span></p></li>
<li><p>把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：</p>
<p><span class="math display">\[
\int p(z_i=k|\mathop{\vartheta _m}^{\rightarrow})p(\mathop{\vartheta
_m}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\vartheta _m}^{\rightarrow} \cdot \int
p(w_i=t|\mathop{\varphi _k}^{\rightarrow})p(\mathop{\varphi
_k}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w_{¬
i}}^{\rightarrow})d\mathop{\varphi _k}^{\rightarrow}
\]</span></p></li>
<li><p>已知第m篇文档的主题分布和第k个主题词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{\vartheta _m}^{\rightarrow})=\mathop{\vartheta _{mk}} \\
p(w_i=t|\mathop{\varphi _k}^{\rightarrow})=\mathop{\varphi _{kt}} \\
\]</span></p></li>
<li><p>而根据共轭关系，有</p>
<p><span class="math display">\[
p(\mathop{\vartheta _m}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})=Dir(\mathop{\vartheta
_m}^{\rightarrow}|\mathop{n_{m,¬
i}}^{\rightarrow}+\mathop{\alpha}^{\rightarrow}) \\
p(\mathop{\varphi _k}^{\rightarrow}|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w_{¬ i}}^{\rightarrow})=Dir(\mathop{\varphi
_k}^{\rightarrow}|\mathop{n_{k,¬
i}}^{\rightarrow}+\mathop{\beta}^{\rightarrow}) \\
\]</span></p></li>
<li><p>因此整个式子可以看作是两个Dirichlet分布的期望向量的第k项和第t项相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，因此最后的概率计算出来就是（注意是正比于）：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝\frac{n_{m,¬ i}^{(k)}+\alpha
_k}{\sum _{k=1}^K (n_{m,¬ i}^{(k)}+\alpha _k)} \cdot \frac{n_{k,¬
i}^{(t)}+\beta _t}{\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>这个概率可以理解为（排除当前这第i个token以外）：</p>
<p><span class="math display">\[
(文档m中主题k所占的比例) * (主题k中词t所占的比例）
\]</span></p></li>
<li><p>注意到第一项的分母是对主题求和，实际上和k无关，因此可以写成：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝
(n_{m,¬ i}^{(k)}+\alpha _k) \cdot \frac{n_{k,¬ i}^{(t)}+\beta _t}{\sum
_{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>我们再看看基于联合分布如何推导</p></li>
<li><p>之前我们已经得到词和主题的联合分布：</p>
<p><span class="math display">\[
p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow}|\mathop{\alpha}^{\rightarrow},\mathop{\beta}^{\rightarrow})
= \prod _{m=1}^M
\frac{\Delta(\mathop{n_m}^{\rightarrow}+\mathop{\alpha}^{\rightarrow})}{\Delta(\mathop{\alpha}^{\rightarrow})}
\prod _{k=1}^K
\frac{\Delta(\mathop{n_k}^{\rightarrow}+\mathop{\beta}^{\rightarrow})}{\Delta(\mathop{\beta}^{\rightarrow})}
\]</span></p></li>
<li><p>根据贝叶斯公式有</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})=\frac{p(\mathop{w}^{\rightarrow},\mathop{z}^{\rightarrow})}{p(\mathop{w}^{\rightarrow},\mathop{z_{¬
i}}^{\rightarrow})} \\
=\frac{p(\mathop{w}^{\rightarrow}|\mathop{z}^{\rightarrow})}
{p(\mathop{w_{¬ i}}^{\rightarrow}|\mathop{z_{¬ i}}^{\rightarrow})p(w_i)}
\cdot \frac{p(\mathop{z}^{\rightarrow})} {\mathop{p(z_{¬
i})}^{\rightarrow}} \\
\]</span></p></li>
<li><p>因为<span
class="math inline">\(p(w_i)\)</span>是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的<span
class="math inline">\(\Delta\)</span>形式表示（分式除以分式，分母相同抵消了）：</p>
<p><span class="math display">\[
∝
\frac{\Delta(\mathop{n_{z}}^{\rightarrow})+\mathop{\beta}^{\rightarrow}}{\Delta(\mathop{n_{z,¬
i}}^{\rightarrow})+\mathop{\beta}^{\rightarrow}} \cdot
\frac{\Delta(\mathop{n_{m}}^{\rightarrow})+\mathop{\alpha}^{\rightarrow}}{\Delta(\mathop{n_{m,¬
i}}^{\rightarrow})+\mathop{\alpha}^{\rightarrow}}
\]</span></p></li>
<li><p>将<span
class="math inline">\(\Delta\)</span>的表达式带入计算，也可以得到：</p>
<p><span class="math display">\[
p(z_i=k|\mathop{z_{¬ i}}^{\rightarrow},\mathop{w}^{\rightarrow})∝
(n_{m,¬ i}^{(k)}+\alpha _k) \cdot \frac{n_{k,¬ i}^{(t)}+\beta _t}{\sum
_{t=1}^V (n_{k,¬ i}^{(t)}+\beta _t)}
\]</span></p></li>
<li><p>最后附上Parameter estimation for text
analysis一文中吉布斯采样的伪算法图：</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oU6U.png" alt="i0oU6U.png" />
<figcaption aria-hidden="true">i0oU6U.png</figcaption>
</figure>
<ul>
<li><p>可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的<span
class="math inline">\(p(z_i=k|\mathop{z_{¬
i}}^{\rightarrow},\mathop{w}^{\rightarrow})\)</span>，公式81和82分别为<span
class="math inline">\(\mathop{\vartheta _{mk}},\mathop{\varphi
_{kt}}\)</span>，我们可以直接通过四个n值得到，不用考虑采样时的<span
class="math inline">\(¬ i\)</span>条件了，具体是：</p>
<p><span class="math display">\[
\mathop{\vartheta _{mk}} = \frac{n_{m}^{(k)}+\alpha _k}{\sum _{k=1}^K
(n_{m}^{(t)}+\alpha _k)} \\
\mathop{\varphi _{kt}} = \frac{n_{k}^{(t)}+\beta _t}{\sum _{t=1}^V
(n_{k}^{(t)}+\beta _t)}
\]</span></p></li>
</ul>
<h2 id="训练与测试">训练与测试</h2>
<ul>
<li>接下来我们来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：
<ul>
<li>迭代什么？采样并更新词对应的主题</li>
<li>根据什么迭代？gibbs采样的完全条件概率</li>
<li>迭代之后的效果？主题分配改变、统计量改变、下一次gibbs采样的完全条件概率改变</li>
<li>迭代到什么时候为止？Gibbs采样收敛，即采样一段时间区间内主题分布稳定不变，或者根据困惑度等指标来衡量模型收敛的程度。</li>
</ul></li>
<li>训练和测试的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而测试则保留主题到词的分布不变，只针对当前测试的文档采样到收敛，得到该文档的主题分布。</li>
<li>事实上两个超参<span class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>在训练完之后是经过了很多次后验替换先验的迭代，参数值很大了，<span
class="math inline">\(\alpha\)</span>就抛弃了这最后的后验结果，在对新文档产生主题分布时重新用最开始的先验值，这样的话中途得到的训练集上的文档到主题的分布在测试新文档时实际上是用不上的，我们利用的是主题到词的分布：因为只有主题集合是针对整个文档空间的（训练集和测试集），主题到词的分布也是建立在整个文档空间的词典上的，这一部分的k个<span
class="math inline">\(\beta\)</span>向量我们保留最后的后验结果，因为这个后验吸收了数据的似然知识后参数值很大，不确定度很小了，基本上每个<span
class="math inline">\(\beta\)</span>向量就等同于确定了一个主题到词的多项式分布，也就是确定了一个主题。我们利用这确定的主题，来测试一篇新文档在各个主题上的分布。因此在测试新文档时参数<span
class="math inline">\(\alpha\)</span>一般设置为对称的，即各个分量相同（没有先验偏好那个主题），且值很小（即不确定度大，否则生成的主题分布是均匀分布），这里类似于最大熵的思想。测试是利用已知的固定的主题去得到文档到主题的分布。</li>
<li>LDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。</li>
</ul>
<h1 id="lda-in-gensim">LDA in Gensim</h1>
<ul>
<li>Gensim中的LDA提供了几个参数，其中<span
class="math inline">\(\alpha\)</span>的默认值如下：
<blockquote>
<p>
alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of
length equal to the number of expected topics that expresses our
a-priori belief for the each topics’ probability. Alternatively default
prior selecting strategies can be employed by supplying a string:
’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.
’default’: Learns an asymmetric prior from the corpus.
</p>
<footer>
<strong>Gensim</strong><cite><a href="https://radimrehurek.com/gensim/models/ldamodel.html">models.ldamodel
– Latent Dirichlet Allocation</a></cite>
</footer>
</blockquote></li>
<li>gensim中没有暴露<span
class="math inline">\(\beta\)</span>给用户，用户只能设置<span
class="math inline">\(\alpha\)</span>，可以自定义，也可以设置对称或者不对称。其中对称设置即全为1，不对称设置则拟合了zipf
law（？），可能<span
class="math inline">\(\beta\)</span>的默认设置就是不对称。</li>
</ul>
<h1 id="more">More</h1>
<ul>
<li><p>Parameter estimation for text analysis
一文指出了隐主题实际上来自词与词之间的高阶共现关系</p></li>
<li><p>LDA用于document
query，其中LDA在candidates上训练，新来一个query就进行一次测试</p>
<ul>
<li><p>基于similarity
ranking的方法，使用JS距离或者KL散度计算candidate与query之间topic
distribution的相似度，并排序</p></li>
<li><p>基于Predictive likelihood
ranking的方法，计算给定query，每个candidate出现的概率，基于主题z分解：</p>
<p><span class="math display">\[
\begin{aligned}
p\left(\vec{w}_{m} | \tilde{\vec{w}}_{\tilde{m}}\right)
&amp;=\sum_{k=1}^{K} p\left(\vec{w}_{m} | z=k\right) p\left(z=k |
\tilde{\vec{w}}_{\tilde{m}}\right) \\
&amp;=\sum_{k=1}^{K} \frac{p\left(z=k | \vec{w}_{m}\right)
p\left(\vec{w}_{m}\right)}{p(z=k)} p\left(z=k |
\tilde{\vec{w}}_{\tilde{m}}\right) \\
&amp;=\sum_{k=1}^{K} \vartheta_{m, k} \frac{n_{m}}{n_{k}}
\vartheta_{\tilde{m}, k}
\end{aligned}
\]</span></p></li>
</ul></li>
<li><p>LDA用于聚类</p>
<ul>
<li>事实上主题分布就是对文档的一种软聚类划分，假如把每篇文档划分到拥有最大概率的主题上的话，那就是一种硬划分。</li>
<li>或者利用topic
distribution作为文档的特征向量，再进一步使用各种聚类算法聚类</li>
<li>聚类结果的评估，可以利用一个已知聚类划分的结果作为参考，利用Variation
of Information distance进行评估</li>
</ul></li>
<li><p>LDA的评价指标，困惑度，其定义为模型在验证集上测出的似然的reciprocal
geometric mean：</p>
<p><span class="math display">\[
\mathrm{P}(\tilde{\boldsymbol{W}} | \boldsymbol{M})=\prod_{m=1}^{M}
p\left(\tilde{\vec{w}}_{\tilde{m}} |
\mathcal{M}\right)^{-\frac{1}{N}}=\exp -\frac{\sum_{m=1}^{M} \log
p\left(\tilde{\bar{w}}_{\tilde{m}} | \mathcal{M}\right)}{\sum_{m=1}^{M}
N_{m}}
\]</span></p></li>
<li><p>假定验证集和训练集分布一致，那么LDA在验证集上的困惑度高，代表熵越大，不确定性大，模型还没有学到一个稳定的参数。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>topic model</tag>
        <tag>lda</tag>
        <tag>mcmc</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM Reasoning in 2025</title>
    <url>/2025/05/30/reasoning-2025-1/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/bf9ca47310c9ee5c25c72925061f7d4b.png" width="500"></p>
<p>What LLM Reasoning be like in the first half of 2025.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><ul>
<li>Since DeepSeek R1, researchers have been overfitting in the field of
mathematical reasoning for large language models</li>
<li>Clearly, every month brings new “breakthroughs” in the
technology</li>
<li>To boost mathematical reasoning performance by tens of points:
<ul>
<li>Even a model one-tenth the size can achieve it</li>
<li>A few hundred high-quality examples suffice</li>
<li>Supervised fine-tuning (SFT) can do it</li>
<li>Distillation can also do it</li>
<li>A single data point can do it</li>
<li>No data at all can do it</li>
<li>Self-generated data can do it</li>
<li>Randomly assigned rewards, or even deliberately incorrect rewards,
can do it</li>
</ul></li>
<li>This teaches us not to test on the training set</li>
<li>It also inspires us that if we do test on the training set, what
tricks can effectively extract the already-ingested samples and produce
the correct answers</li>
<li>Beyond the RLVR veneer, this is also an interesting direction on how
large models write in and extract knowledge</li>
<li>After the first half-year carnival, which advances will stand the
test of time, and which will meet a violent end?</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><ul>
<li>自DeepSeek
R1以来，研究者们就一直在大语言模型的数学推理领域过拟合</li>
<li>肉眼可见，每个月技术都在不断“突破”</li>
<li>要提升几十点的数学推理能力
<ul>
<li>十分之一大小的小模型也可以做到</li>
<li>几百条高质量数据就可以做到</li>
<li>SFT也可以做到</li>
<li>蒸馏也可以做到</li>
<li>一条数据也可以做到</li>
<li>没有数据也可以做到</li>
<li>自我生成数据也可以做到</li>
<li>奖励随机设置，甚至刻意设置错误奖励也可以做到</li>
</ul></li>
<li>这告诉我们，不要在训练集上测试</li>
<li>这也启发我们，如果在训练集上测试的话，怎样的trick才能有效提取出明明已经训进去的样本，得到正确答案</li>
<li>抛开RLVR的外壳，这也是关于大模型知识写入和提取的一个有意思的方向</li>
<li>在上半年的狂欢过后，哪些是大浪淘沙，哪些会迎来violent ends？</li>
</ul>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>MyQuestion</category>
      </categories>
      <tags>
        <tag>llm</tag>
        <tag>inference</tag>
        <tag>reasoning</tag>
        <tag>questions</tag>
      </tags>
  </entry>
  <entry>
    <title>Reformer - Paper Reading</title>
    <url>/2020/02/07/reformer/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/fe446d582e4167ad3d7cc0318571c810.png" width="500"/></p>
<p>Reading note for reformer.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="efficiently-and-economically">Efficiently and Economically</h1>
<ul>
<li>The author primarily proposes two methods to reduce memory usage of
Transformers, especially when processing extremely long sequences,
significantly reducing computational load and improving speed.</li>
</ul>
<h1 id="lsh-attention">LSH Attention</h1>
<p><a href="https://imgchr.com/i/39Y8pt"><img data-src="https://s2.ax1x.com/2020/02/16/39Y8pt.png"
alt="39Y8pt.png" /></a></p>
<ul>
<li><p>The original idea is that in Transformer's self-attention, each
token as a query needs to calculate attention with all tokens in the
sequence, and then weight them to obtain a representation of the current
token. However, we know that attention is generally very sparse, with
weights concentrated on just a few tokens. So why not calculate weights
and apply weighting only on those few tokens, thereby greatly reducing
the <span class="math inline">\(O(N^2)\)</span> computational and memory
overhead in self-attention?</p></li>
<li><p>How can we know which few tokens these are? If we could only
determine this by calculating attention, how could we possibly know
which tokens have high weights before computing attention? It's
impossible. But in self-attention, computing weights between query and
key is simply an inner product, where keys similar to the query have
higher weights. The model learns attention by learning to generate
correct query and key representations, and only needs to compare query
and key when calculating attention.</p></li>
<li><p>So the problem transforms into finding a few keys similar to each
query for attention calculation. How? Certainly not by calculating all
and taking the top k, as that would contradict our initial goal of
reducing computational complexity. Here, the author uses Local Sensitive
Hashing (LSH), which means that similar vectors are more likely to be
mapped to the same hash value, with multiple similar vectors essentially
placed in the same "bucket". We only need to calculate self-attention
within each bucket. More specifically, for two vectors <span
class="math inline">\(q_1, q_2\)</span>, an LSH hash function <span
class="math inline">\(h\)</span> can achieve:</p>
<p><span class="math display">\[
for \ dis(q_1,q_2) &lt;= d_1 , \ p(h(q_1)==h(q_2)) &gt;= p_1 \\
for \ dis(q_1,q_2) &gt;= d_2 , \ p(h(q_1)==h(q_2)) &lt;= p_2 \\
\]</span></p></li>
<li><p>Existing research in related fields has various hash functions
<span class="math inline">\(h\)</span> for different distance metrics
<span class="math inline">\(dis\)</span>. Evidently, our distance metric
here is cosine distance, corresponding to spherical projection LSH,
which projects vectors onto a b-dimensional hypersphere divided into
<span class="math inline">\(n_{buckets}\)</span> quadrants. Vectors
projected into the same quadrant are in the same bucket. The specific
projection hash is:</p>
<p><span class="math display">\[
h(x) = argmax[xR;-xR] \\
\]</span></p>
<p>Where <span class="math inline">\(R\)</span> is a random projection
matrix of <span class="math inline">\([d_k,b/2]\)</span></p></li>
<li><p>The next challenge is that the number of queries and keys in a
bucket might not be equal, and many queries might lack keys. So the
author simply shares QK by making queries and keys emerge from the same
linear transformation, with keys just normalized: <span
class="math inline">\(k_{j}=\frac{q_{j}}{\left\|q_{j}\right\|}\)</span></p></li>
<li><p>Chunk Operation: Instead of performing self-attention separately
in each bucket, the author segments them, rearranging bucket contents
into a sequence, cutting it into equal-length segments, performing
self-attention within segments, and also performing attention between
adjacent segments. There's some doubt here: the paper's diagram looks
ideal, with buckets of almost equal size that can be compensated by
adjacent segment attention. But the actual bucket sizes are unknown.
Perhaps by artificially setting this, the author is imposing a prior
constraint on attention learning, suggesting bucket sizes tend to be
equal and match segment length.</p></li>
<li><p>Multi-round LSH: LSH involves probability and thus error. The
author devised a clever experiment to verify LSH's restoration of
original attention, finding single-round performance unsatisfactory.
Therefore, multiple hash rounds are used to ensure probability, taking
the union of multiple hash rounds to ensure similar vectors land in the
same bucket. Taking the union instead of intersection is likely because
with many buckets, hashing becomes sparse, and the probability of
dissimilar vectors landing in the same bucket is far lower than similar
vectors landing in different buckets. Some details here remain to be
elaborated.</p></li>
<li><p>Causal Masking: Normal transformers do temporal masking at the
decoder, but LSH scrambles sequence order, so corresponding processing
is needed to ensure temporal mask correctness.</p></li>
<li><p>Notably, most self-attention implementations include the self in
value, but in LSH, this can't be done because key and value share
values, and the self is always the most similar.</p></li>
</ul>
<h1 id="reversible-transformer">Reversible Transformer</h1>
<ul>
<li><p>This section's idea references the paper: "The Reversible
Residual Network: Backpropagation Without Storing Activations".</p></li>
<li><p>The basic idea is to modify the residual structure into a
reversible residual structure to save GPU memory. During
backpropagation, networks need to store activation values for each layer
to conduct automatic differentiation, calculate each layer's
derivatives, and chain-rule differentiate. Storing these activation
values consumes significant GPU memory. The reversible residual idea is
to split channels into two paths with mutual residuals, modifying the
computational graph's topology so that path activations can be
calculated from the previous layer's activations, as shown in the
image:<br />
<img data-src="https://s2.ax1x.com/2020/02/16/39l0tP.png"
alt="39l0tP.png" /></p></li>
<li><p>Forward propagation process:</p>
<p><span class="math display">\[
\begin{array}{l}{y_{1}=x_{1}+\mathcal{F}\left(x_{2}\right)} \\
{y_{2}=x_{2}+\mathcal{G}\left(y_{1}\right)}\end{array}
\]</span></p></li>
<li><p>Backward propagation:</p>
<p><span class="math display">\[
\begin{array}{l}{x_{2}=y_{2}-\mathcal{G}\left(y_{1}\right)} \\
{x_{1}=y_{1}-\mathcal{F}\left(x_{2}\right)}\end{array}
\]</span></p></li>
<li><p>Note that calculating <span class="math inline">\(x_2\)</span>
only uses previous layer activations <span
class="math inline">\(y_1,y_2\)</span>, and calculating <span
class="math inline">\(x_1\)</span> uses the previously computed <span
class="math inline">\(x_1\)</span>, thus avoiding activation value
storage. Although space is saved, activation functions must be
recalculated, essentially trading time for space.</p></li>
<li><p>The original paper applied this to ResNet, saving GPU memory to
enable larger batch sizes. In transformers, it can be used to train
longer sequences.</p></li>
<li><p>In Reformer, functions <span
class="math inline">\(\mathcal{F}\)</span> and <span
class="math inline">\(\mathcal{G}\)</span> are respectively changed to
self-attention and fully connected layers, corresponding to the
transformer's reversible structure.</p></li>
<li><p>While the reversible structure eliminates layer-count impact on
space complexity, the feed-forward network (FFN) in transformers, which
consumes the most memory, is still influenced by sequence length. To
reduce FFN memory usage, the author again employs chunking, as FFN lacks
sequence dependencies and can be computed in segments. Correspondingly,
reversible structure inputs and outputs are also computed in segments.
For scenarios with large vocabularies, loss log-probabilities are also
computed segmentally.</p></li>
<li><p>The author additionally notes that this saves intermediate
variables during backpropagation gradient computation, not model
parameters. Saving parameter memory can be achieved by transferring to
CPU memory, typically uneconomical due to high data transfer overhead
between CPU and GPU. However, since Reformer can process more data in
each transformation, this becomes more feasible.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="多快好省">多快好省</h1>
<ul>
<li>作者主要提出了两点操作来降低Transformer，尤其是在处理超长序列时的内存占用，减少了大量运算，提升了速度。</li>
</ul>
<h1 id="lsh-attention">LSH Attention</h1>
<p><a href="https://imgchr.com/i/39Y8pt"><img data-src="https://s2.ax1x.com/2020/02/16/39Y8pt.png"
alt="39Y8pt.png" /></a></p>
<ul>
<li><p>这一部分最原始的想法就是，Transformer当中的self
attention，每一个token作为query时，要把序列中所有token当成key去计算注意力，再在所有token上加权得到当前token的一个表示，但我们知道注意力一般是非常稀疏的，权重就集中于少数几个token上，那不如只在这几个token上计算权重并加权，这样就大大减少了self
attention里<span
class="math inline">\(O(N^2)\)</span>的计算量和内存占用量。</p></li>
<li><p>那么怎么才知道那少数几个token是哪几个？假如要完全靠注意力计算出来才能得到的话，怎么可能在计算注意力之前就知道哪几个token权重大？是不可能，但是在self
attention里，query和key计算权重，就是简单的内积，和query相似的key权重大。模型学习到注意力，是指学习到生成正确的query以及key的表示，在计算注意力时只需要比对query和key就可以了。</p></li>
<li><p>所以问题转换成，对每一个query，我先找到相近的几个key计算注意力就好了。怎么找？当然不是全部算一遍取top
k，那就与我们减少计算量的初衷相悖，在这里作者用到了Local Sensitive
Hashing(LSH)，局部敏感哈希，大意就是相近的向量，映射到同一哈希值的概率较大，多个相近的、映射到同一哈希值的向量相当于装进了同一个桶里(bucket)，那么我们只需要对每个桶里的向量计算self
attention。详细一点的描述是，两个向量<span
class="math inline">\(q_1,q_2\)</span>，满足LSH的哈希函数<span
class="math inline">\(h\)</span>能做到</p>
<p><span class="math display">\[
for \ dis(q_1,q_2) &lt;= d_1 , \ p(h(q_1)==h(q_2)) &gt;= p_1 \\
for \ dis(q_1,q_2) &gt;= d_2 , \ p(h(q_1)==h(q_2)) &lt;= p_2 \\
\]</span></p></li>
<li><p>相关领域已经有很多研究，对于不同的距离度量<span
class="math inline">\(dis\)</span>，有不同的<span
class="math inline">\(h\)</span>满足LSH。显然在这里我们的距离度量是cosine距离，对应的LSH哈希是球形投影，即将向量投影到一个b维超球面上，该球面被分成了<span
class="math inline">\(n_{buckets}\)</span>个象限，投影到同一象限的向量即在同一个桶中，该投影哈希具体写出来是：</p>
<p><span class="math display">\[
h(x) = argmax[xR;-xR] \\
\]</span></p>
<p><span class="math inline">\(R\)</span>是一个<span
class="math inline">\([d_k,b/2]\)</span>的随机投影矩阵</p></li>
<li><p>接下来的一个问题是，一个桶里面，query和key的数量不一定相等，而且有可能一个桶里许多query，没有key。于是作者干脆share
QK，即令query和key相同，都是embedding从同一个线性变换出来的，只不过key做了归一化操作<span
class="math inline">\(k_{j}=\frac{q_{j}}{\left\|q_{j}\right\|}\)</span></p></li>
<li><p>chunk操作：接下来作者并不是让每个桶里分别做self
attention，而是做了分段，即把同一个桶里的放在一起，重新排成一个序列，然后等长切成若干个段，段内做self
attention，相邻的段也做一次attention。这里其实有点疑问，论文的图画的非常理想，每个桶的大小差不多，可能差了一两个可以通过相邻段做attention来弥补，但是实际情况并不知道每个桶的大小。也许是因为attention本身也是学习出来的，作者这么人为设置，是不是相当于给了一个每个桶大小都趋于相同且等于段长的先验限制了attention的学习。</p></li>
<li><p>Multi-round
lsh：lsh是讲概率的，有概率就有误差，作者构造了一个巧妙的实验来验证lsh对原始attention的还原度，发现单轮的效果并不好。因此就多次hash来保证概率，取多轮hash的并集来保证相似的向量能落到同一个桶里。这里取并集而不是交集，个人理解是桶一多，hash其实很稀疏，不相似的向量落在同一个桶的概率远小于相似的向量落在不同桶的概率。这里还有一些细节待补充</p></li>
<li><p>casual
masking：正常的transformer在decoder端是要做时序掩码的，这里lsh把序列顺序打乱了，因此也要做对应的处理，保证时序掩码的正确性。</p></li>
<li><p>值得一提的是大部分self
attention的实现，value包括了自身，但是在lsh里不能包含自身，因为key和value共享值，自身永远是最相似的。</p></li>
</ul>
<h1 id="reversible-transformer">Reversible Transformer</h1>
<ul>
<li><p>这一部分的想法参照了论文：The Reversible Residual Network:
Backpropagation Without Storing Activations。</p></li>
<li><p>基本思想就是，将残差结构改为可逆残差结构，从而节省了显存。网络在做方向传播的时候，需要存储每一层的激活值，带入自动微分计算每一层的导数，再链式求导，其中存储每一层的激活值占了很大的显存。可逆残差的思想就是，通过将channel一分为二，做成两路，互相残差，更改计算图的拓扑结构，使得两路的激活值能够通过上一层的激活值计算出来，如图：<br />
<img data-src="https://s2.ax1x.com/2020/02/16/39l0tP.png"
alt="39l0tP.png" /></p></li>
<li><p>前向传播过程为：</p>
<p><span class="math display">\[
\begin{array}{l}{y_{1}=x_{1}+\mathcal{F}\left(x_{2}\right)} \\
{y_{2}=x_{2}+\mathcal{G}\left(y_{1}\right)}\end{array}
\]</span></p></li>
<li><p>反向传播为：</p>
<p><span class="math display">\[
\begin{array}{l}{x_{2}=y_{2}-\mathcal{G}\left(y_{1}\right)} \\
{x_{1}=y_{1}-\mathcal{F}\left(x_{2}\right)}\end{array}
\]</span></p></li>
<li><p>可以看到计算<span
class="math inline">\(x_2\)</span>时只用了上一层的激活值<span
class="math inline">\(y_1,y_2\)</span>，计算<span
class="math inline">\(x_1\)</span>时用了上一步计算出来的<span
class="math inline">\(x_1\)</span>，因此不需要存储这两个激活值。虽然节省了空间，但是激活函数需要重新算一遍，相当于用时间换空间。</p></li>
<li><p>原始论文用在resnet里，节约显存可以换得更大的batch_size，在transformer中就可以用来训练更长的sequence</p></li>
<li><p>reformer中把两个函数<span
class="math inline">\(\mathcal{F}\)</span>和<span
class="math inline">\(\mathcal{G}\)</span>分别改成了自注意力层和全连接层，这样就对应了transformer的可逆结构</p></li>
<li><p>可逆结构虽然消除了层数对于空间复杂度的影响，但是transformer里占显存最大的FFN，其依然受序列长度影响，为了减少这一部分显存占用，作者有一次采用了chunking，因为FFN这里是不存在序列依赖的，完全可以拆成几段计算，相应的，可逆结构的输入输出也拆成几段计算，又一次用时间换空间。此外，对于词典较大的应用场景，作者在计算损失log-probabilities时也是分段的。</p></li>
<li><p>作者还额外提到，这样节省的是反向传播计算梯度时用到的中间临时变量，并不会节省参数量，节省参数量在GPU的消耗可以通过将其转到CPU内存来解决，通常这样的操作得不偿失，因为在CPU和GPU之间传输数据非常耗时，但是由于reformer在每次转换时可以处理更多的数据，就“得能尝失”了。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>local sensitive hashing</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-agent Reinforcement Learning Notes</title>
    <url>/2023/07/20/marl/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/598fac8fa369cf85729973005cc0b37a.png" width="500"/></p>
<p>A simple note on the RL used in single-agent and multi-agent.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="sequential-decision-making">Sequential Decision Making</h1>
<ul>
<li>Modeling agents in sequential decision making problems, rather than
just single-step or one-shot decision problems.</li>
<li>Discuss a sequential decision problem in:
<ul>
<li>A fully observable stochastic environment</li>
<li>With a Markov transition model</li>
<li>Additive rewards Typically includes a set of states, a set of
actions, a transition model, and a reward function. The solution to the
problem is the policy.</li>
</ul></li>
<li>If the sequence has no time limit, and the optimal policy only
depends on the state, independent of time, it is called a stationary
policy.</li>
</ul>
<h2 id="bellman-equation-u-and-q-functions">Bellman Equation, U and Q
Functions</h2>
<ul>
<li><p>The value function represents the cumulative reward of a
state/action sequence, such as U(s0,a0,s1,a0....), starting from the
current state and action s0,a0.</p></li>
<li><p>The value is defined by an additive discount, where future
rewards are discounted by gamma:</p>
<p><span class="math display">\[
U\left(\,\left[s_{0},\,s_{1},\,s_{2},\,\cdots\,\right]\,\right)=R(s_{0})+\gamma
R(s_{1})+\gamma ^2 R(s_{2})+\,\cdots
\]</span></p>
<ul>
<li>Because recent rewards are more important.</li>
<li>If rewards can be invested, earlier rewards have higher value.</li>
<li>This is equivalent to each transition having a <span
class="math inline">\(1-\gamma\)</span> chance of unexpected
termination.</li>
<li>Satisfies stationarity, meaning the best action at time t+1 is the
same as the best action at time t in the future.</li>
<li>Prevents infinite sequence transitions.</li>
</ul></li>
<li><p>Based on the value function, the best action can be selected at
each state, i.e., the action that maximizes the current value
(instantaneous reward + expected discounted future value):</p>
<ul>
<li><p>There is an expectation here, as each action can lead to
different future states (with probability), so the value of an action is
the sum of its expected value over all possible future states.</p></li>
<li><p>The value function is only a function of the state, as it has
already accumulated the expectation over all actions:</p>
<p><span class="math display">\[
\pi^*(s) = \underset{a \in A(s)}{\text{argmax}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p></li>
</ul></li>
<li><p>Similarly, the value function is only a function of the state,
and has already accumulated the expectation over all actions. This is
essentially the same as the previous formula, but without the argmax to
select actions; instead, it's a max, as the agent assumes the best
action is taken. The interpretation is: if the agent selects the best
action, the state value is the expected reward of the next transition
plus the discounted value of the next state:</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p>
<ul>
<li>Note that these two formulas are essentially the same; both involve
selecting actions and summing over all possible states, but this is
based on an estimate of the future expectation. The actual execution is
choosing an action and transitioning to a state.</li>
<li>This equation is the <strong>Bellman Equation</strong>.</li>
</ul></li>
<li><p>Introduce the Q-function, which is a function of both action and
state, while U is just a function of state. The relationship between
them is as follows:</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} Q(s,a)
\]</span></p></li>
<li><p>Similarly, the Bellman equation can be written for the
Q-function:</p>
<p><span class="math display">\[
Q(s,a) = \sum_{s^{&#39;}} P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;})]
\]</span></p></li>
<li><p>Note that the above discussion focuses on the optimal value
function and optimal Q-function, both using max, meaning they compute
the return under the optimal policy, as opposed to on-policy value
functions and Q-functions, which calculate the expected return.</p></li>
</ul>
<h2 id="reward-shaping">Reward Shaping</h2>
<ul>
<li><p>The reward function R can be modified (without changing the
optimal policy) to stabilize the reinforcement learning process.</p>
<ul>
<li>Constraints: Avoid agent exploitation of reward.</li>
<li>Exploration: Encourage exploration.</li>
<li>Acceleration: Improve sparse reward situations by breaking tasks
down into smaller sub-tasks, making it easier for the agent to
learn.</li>
</ul></li>
<li><p>A common modification is the introduction of a potential
function.</p></li>
<li><p>A potential function is a state-only function <span
class="math inline">\(\Phi(s)\)</span> (unlike the value function, it is
independent of action/state sequences and does not result from removing
actions).</p></li>
<li><p>The potential function encodes the objective environmental
factors that influence the rewards.</p></li>
<li><p>It can be proven that a potential function can be any arbitrary
function of state s, and when added to the immediate reward, the optimal
policy derived from the Bellman equation remains unchanged.
Specifically, when the reward function is modified as:</p>
<p><span class="math display">\[
R^{&#39;}(s,a,s^{&#39;}) = R(s,a,s^{&#39;}) + \gamma \Phi(s^{&#39;}) -
\Phi(s)
\]</span></p>
<p>The optimal policy remains unchanged, <span
class="math inline">\(Q(s,a)=Q^{&#39;}(s,a)\)</span>.</p></li>
</ul>
<h2 id="solving-mdps">Solving MDPs</h2>
<h3 id="value-iteration">Value Iteration</h3>
<ul>
<li><p>With n states, there are n equations and n unknowns in the
Bellman equation. The analytical solution of nonlinear equations is
difficult, but an iterative method can be used. Starting with random
initial values, each state's value is updated based on neighboring
states' values until equilibrium is reached.</p></li>
<li><p>Introduce an iteration timestep i, the Bellman update
(<strong>Bellman Update</strong>) is as follows:</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>It can be proven that infinite iterations will guarantee
convergence to the optimal solution (assuming the immediate rewards are
correct).</p></li>
</ul>
<h3 id="policy-iteration">Policy Iteration</h3>
<ul>
<li><p>Sometimes we do not need to compute the exact value function,
just the action that yields the maximum value. This leads to the idea of
directly iterating and optimizing the policy.</p></li>
<li><p>Starting with an initial policy, the following two steps are
alternated:</p>
<ul>
<li>Policy Evaluation: Given a policy, compute the value of each state
under the policy at a particular timestep.</li>
<li>Policy Improvement: Calculate a new policy based on the value
function (using the Bellman equation) for all states.</li>
</ul></li>
<li><p>The process continues until policy improvement does not result in
a significant change in the value function.</p></li>
<li><p>Policy evaluation is also based on the Bellman equation, but we
do not need to traverse actions since they are determined by the policy.
By fixing the current policy <span class="math inline">\(\pi_i\)</span>
at timestep i, we obtain n equations, which can be solved:</p>
<p><span class="math display">\[
U_{i}(s) = \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>When the state space is large, solving it exactly becomes
difficult. In this case, a modified policy iteration can be used for
policy evaluation, where the value function for the next timestep is
computed directly from the current policy and iterated repeatedly:</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>The above method is synchronous, meaning all states are updated
at each iteration. In fact, we can update only a subset of states, which
is called asynchronous policy iteration.</p>
<ul>
<li>The advantage is that we can focus on updating strategies for
certain effective states, as some states may not reach an optimal
solution regardless of the action taken.</li>
</ul></li>
</ul>
<h3 id="linear-programming">Linear Programming</h3>
<ul>
<li>TBD</li>
</ul>
<h3 id="online-algorithms">Online Algorithms</h3>
<ul>
<li>Value iteration and policy iteration are offline methods: given all
conditions/rewards, the optimal solution is computed, and the agent
executes it.</li>
<li>Online algorithms: The agent does not receive an offline solution
and execute it. Instead, it computes decisions in real-time at each
decision point.</li>
</ul>
<h2 id="slot-machine-problem">Slot Machine Problem</h2>
<ul>
<li>TBD</li>
</ul>
<h2 id="pomdp">POMDP</h2>
<ul>
<li>Partially Observable Markov Decision Process</li>
<li>Since the agent is uncertain about its current state (the definition
of partial observability), a belief state is introduced, and the agent’s
decision-making cycle has an additional step:
<ul>
<li>Act based on the belief state.</li>
<li>Perceive observations (evidence).</li>
<li>Update the belief state based on the perception, action, and
previous belief state through some updating mechanism.</li>
</ul></li>
<li>In physical state space, solving POMDP can be simplified to solving
the MDP in the belief state space.</li>
<li>Value iteration for POMDPs.</li>
</ul>
<h1 id="single-agent-rl">Single Agent RL</h1>
<ul>
<li>The agent is in an MDP but does not know the transition model or
reward function and needs to take actions to learn more
information.</li>
<li>The sequential decision-making problem above assumes a known
environment and optimal policy derivation. However, in general
reinforcement learning, the environment is unknown, and the agent learns
the optimal policy through interaction with the environment.</li>
<li>Model-based Methods:
<ul>
<li>The environment provides a transition model or initially an unknown
model that needs to be learned.</li>
<li>Typically, a value function is learned, defined as the total reward
accumulated from state s.</li>
<li>The sequential decision-making problems discussed above</li>
</ul></li>
</ul>
<p>are often solved through value iteration or policy iteration in a
model-based manner. - Model-free Methods: - The environment is not known
beforehand and needs to be learned. Instead of computing and using a
model, the agent computes and learns the value function or policy. -
Model-free approaches are often simpler to implement than model-based
methods but may require more interactions with the environment.
Examples: - Q-learning - SARSA - Monte Carlo methods.</p>
<h2 id="passive-reinforcement-learning">Passive Reinforcement
Learning</h2>
<ul>
<li>The policy is fixed, and the value function is learned.</li>
<li>The policy is fixed, for example, a greedy approach that selects the
action with the maximum value. In this case, the Q-function only needs
to be learned, and the optimal action under the fixed policy will
emerge.</li>
<li>This is similar to policy evaluation (where, given a policy, the
value of each state at a particular time step is computed), but the
agent doesn't know the transition probabilities between states or the
immediate rewards after taking an action.</li>
</ul>
<h3 id="direct-value-estimation">Direct Value Estimation</h3>
<ul>
<li>The value of a state is defined as the expected total reward
(reward-to-go) from that state.</li>
<li>Each trial will leave a sample of the value for the states it passes
through (multiple visits to the same state will provide multiple
samples).</li>
<li>This way, samples are collected, and supervised learning can be used
to map states to values.</li>
<li>However, this method ignores an important constraint: the value of a
state should satisfy the Bellman equation for the fixed policy, i.e.,
the value of the state is related to the reward and expected value of
the successor states, not just its own value.</li>
<li>Ignoring this will lead to a larger search space and slow
convergence.</li>
</ul>
<h3 id="adaptive-dynamic-programming-adp">Adaptive Dynamic Programming
(ADP)</h3>
<ul>
<li>The agent learns the transition model between states and uses
dynamic programming to solve the MDP.</li>
<li>In a fully observable or deterministic environment, the agent
continually runs trials to gather data, then trains a supervised model
that takes the current state and action as inputs and outputs the
transition probabilities (the transition model).</li>
<li>After obtaining the transition model, the agent can solve the MDP
using sequence decision methods, correcting the policy iteratively.</li>
<li>ADP requires the agent to trial continuously, gather historical data
with reward signals, and then learn the environment's transition model,
which transforms the problem into a known sequence decision
problem.</li>
<li>ADP can be seen as an extension of policy iteration in the passive
reinforcement learning setting.</li>
</ul>
<h3 id="temporal-difference-learning">Temporal Difference Learning</h3>
<ul>
<li><p>In the passive reinforcement learning setting, where a policy
<span class="math inline">\(\pi\)</span> is given, if the agent takes
action <span class="math inline">\(\pi(s)\)</span> from state <span
class="math inline">\(s\)</span> and transitions to state <span
class="math inline">\(s^{&#39;}\)</span>, the value function is updated
using the temporal difference equation:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow U^{\pi}(s) + \alpha (R(s,\pi(s),s^{&#39;}) +
\gamma U^{\pi}(s^{&#39;}) - U^{\pi}(s))
\]</span></p></li>
<li><p>Here, <span class="math inline">\(\alpha\)</span> is the learning
rate. Compared to Bellman, temporal difference updates the value based
on the observed difference between the value of state <span
class="math inline">\(s\)</span> and the reward plus discounted future
value of state <span class="math inline">\(s^{&#39;}\)</span>:</p>
<ul>
<li><p>The difference term provides error information, and the update
reduces this error.</p></li>
<li><p>The modified formula shows that the value of the state is updated
using interpolation between the current value and the reward + future
discounted value:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow (1-\alpha)U^{\pi}(s) + \alpha
(R(s,\pi(s),s^{&#39;}) + \gamma U^{\pi}(s^{&#39;}))
\]</span></p></li>
</ul></li>
<li><p>Connection and difference with adaptive dynamic programming:</p>
<ul>
<li>Both adjust the current value based on future estimates, but ADP
uses a weighted sum over all possible successor states, while temporal
difference only uses the observed successor state.</li>
<li>ADP aims for as many adjustments as possible to ensure consistency
between the value estimate and the transition model, while TD makes a
single adjustment based on the observed transition.</li>
<li>TD can be seen as an approximation of ADP:
<ul>
<li>TD can use the transition model to generate multiple
pseudo-experiences, rather than relying only on the actual observed
transition, leading to value estimates that are closer to ADP.</li>
<li>Prioritized sweeping updates states that are highly probable and
recently had large adjustments in their successor states.</li>
<li>One advantage of TD as an approximation of ADP is that early on, the
transition model may not be accurate, so learning an exact value
function to match the transition model is less meaningful.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="active-reinforcement-learning">Active Reinforcement
Learning</h2>
<ul>
<li>The policy needs to be learned.</li>
<li>A complete transition model needs to be learned, taking into account
all possible actions since the policy is not fixed (unknown).</li>
<li>Consider whether, after learning the optimal policy, simply
executing it is always the right action.</li>
</ul>
<h3 id="introducing-exploration">Introducing Exploration</h3>
<ul>
<li>Adaptive dynamic programming is greedy, so exploration needs to be
introduced.</li>
<li>A broad design would introduce an exploration function <span
class="math inline">\(f(u, n)\)</span>, where a higher value <span
class="math inline">\(u\)</span> encourages greediness, and fewer trials
<span class="math inline">\(n\)</span> encourage exploration.</li>
</ul>
<h3 id="td-q-learning">TD Q-learning</h3>
<ul>
<li><p>A temporal difference method for active reinforcement
learning.</p></li>
<li><p>No model of the transition probabilities is required (model-free
method).</p></li>
<li><p>The agent learns the action-value function to avoid needing the
transition model itself:</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
\max_{a^{&#39;}}Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>No transition model <span
class="math inline">\(P(s&#39;|s,a)\)</span> is needed.</p></li>
<li><p>Since no policy is provided, we need to take the max over all
possible actions.</p></li>
<li><p>Learning is difficult when rewards are sparse.</p></li>
</ul>
<h3 id="sarsa">Sarsa</h3>
<ul>
<li><p>Sarsa stands for state, action, reward, state, action, and
represents the update rule for this five-tuple:</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>Compared to TD Q-learning, it does not take the max over all
possible actions, but instead updates based on the action actually
taken.</p></li>
<li><p>If the agent is greedy and always selects the action with the
highest Q-value, then Sarsa and Q-learning are equivalent. If not
greedy, Sarsa penalizes actions that encounter negative rewards during
exploration.</p></li>
<li><p>On/Off Policy:</p>
<ul>
<li>Sarsa is on-policy: "If I stick to my policy, what is the value of
this action at this state?"</li>
<li>Q-learning is off-policy: "If I stop following my current policy and
instead use an estimated optimal policy, what is the value of this
action at this state?"</li>
</ul></li>
</ul>
<h2 id="generalization-in-reinforcement-learning">Generalization in
Reinforcement Learning</h2>
<ul>
<li>Both the value function and the Q-function are stored in table form,
and the state space is large.</li>
<li>If they can be parameterized, the number of parameters to be learned
can be greatly reduced.</li>
<li>In passive reinforcement learning, supervised learning can be used
to learn the value function based on trials, and functions or neural
networks can be used to parameterize this.</li>
<li>In temporal difference learning, the difference term can be
parameterized and learned through gradient descent.</li>
<li>There are several issues with parameterizing and approximating the
value or Q-function:
<ul>
<li>Difficulty in convergence.</li>
<li>Catastrophic forgetting: This can be mitigated by experience replay,
where trajectories are saved and replayed to ensure that value functions
for states not currently visited remain accurate.</li>
</ul></li>
<li>Reward function design, how to address sparse rewards?
<ul>
<li>Issue: credit assignment, which action should be credited for the
final positive or negative reward?</li>
<li>Reward shaping can help by providing intermediate rewards; potential
functions are one example, reflecting progress towards partial goals or
measurable distances from the final desired state.</li>
</ul></li>
<li>Another approach is hierarchical reinforcement learning, TBD.</li>
</ul>
<h2 id="policy-search">Policy Search</h2>
<ul>
<li><p>Adjust the policy as long as there is improvement in
performance.</p></li>
<li><p>A policy is a function mapping states to actions.</p></li>
<li><p>If the policy is parameterized, it can be optimized. However,
optimizing the Q-function doesn't necessarily lead to the optimal value
estimate or Q-function because policy search only cares whether the
policy is optimal.</p></li>
<li><p>Directly learning Q-values and then using argmax to derive the
policy can lead to discrete, non-differentiable issues. In this case,
Q-values are treated as logits, and softmax is used to represent action
probabilities, with techniques like Gumbel-Softmax ensuring the policy
is continuously differentiable.</p></li>
<li><p>If the expected reward from executing the policy can be written
as a parameterized expression, policy gradient methods can be used for
direct optimization. Otherwise, the expression can be computed by
observing accumulated rewards during policy execution and optimized
using experience gradients.</p></li>
<li><p>For the simplest case where only one action is taken, the policy
gradient can be written as:</p>
<p><span class="math display">\[
\triangledown_{\theta}\sum_a R(s_0,a,s_0)\pi_{\theta}(s_0,a)
\]</span></p></li>
<li><p>This sum can be approximated using samples generated from the
policy’s probability distribution, and extended to sequential states,
resulting in the REINFORCE algorithm. Here, the policy probability
weighted sum is approximated over N trials, and the single-step reward
is extended to a value function, with states extended to the entire
state space of the environment:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{j=1}^N \frac{u_j(s)
\triangledown_{\theta}\pi_{\theta}(s,a_j)}{\pi_{\theta}(s,a_j)}
\]</span></p></li>
</ul>
<h1 id="marl-multi-agent-rl">MARL (Multi-Agent Rl)</h1>
<ul>
<li>TBD</li>
<li><img data-src="/img/marl1.png" width="1200"></li>
<li><img data-src="/img/marl2.png" width="1200"></li>
<li><img data-src="/img/marl3.png" width="1200"></li>
<li><img data-src="/img/marl4.png" width="1200"></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="序列决策">序列决策</h1>
<ul>
<li>建模序列决策下的智能体，而不仅仅是单一回合或者一次性决策问题</li>
<li>讨论一个
<ul>
<li>完全可观测的随机环境</li>
<li>具有马尔科夫转移模型</li>
<li>加性奖励
的序列决策问题，通常包含状态集合、动作集合、转移模型、奖励函数。问题的解即策略。</li>
</ul></li>
<li>假如序列没有时间限制，最优策略只与状态有关，与时间无关，则称最优策略是平稳的。</li>
</ul>
<h2 id="bellman方程u和q函数">bellman方程，U和Q函数</h2>
<ul>
<li><p>价值函数（价值函数），代表某一状态/行为序列的奖励综合，U(s0,a0,s1,a0....)，从当前状态和动作s0,a0开始</p></li>
<li><p>用加性折扣定义价值，未来的奖励乘gamma递减:</p>
<p><span class="math display">\[
U\left(\,\left[s_{0},\,s_{1},\,s_{2},\,\cdots\,\right]\,\right)=R(s_{0})+\gamma
R(s_{1})+\gamma ^2 R(s_{2})+\,\cdots
\]</span></p>
<ul>
<li>因为看重近期奖励</li>
<li>如果奖励可以投资，则越早的奖励价值越大</li>
<li>等价于每次转移有<span
class="math inline">\(1-\gamma\)</span>的意外终止</li>
<li>满足平稳性，t+1的最佳选择未来也是t的最佳选择未来</li>
<li>避免无穷的序列转移</li>
</ul></li>
<li><p>基于价值函数，可以选出当前最佳动作，即在当前状态下，使得当前价值最大的动作（转移的即时奖励+后续的期望折扣价值）</p>
<ul>
<li><p>这里存在一个期望，因为每个动作都可能到每个状态（概率），因此一个动作的价值是在所有可能的未来状态下累加。</p></li>
<li><p>价值函数只是状态的函数，已经对所有动作累加求期望</p>
<p><span class="math display">\[
\pi^*(s) = \underset{a \in A(s)}{\text{argmax}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p></li>
</ul></li>
<li><p>同理，价值函数只是状态的函数，已经对所有动作累加求期望，其实就是上式，只不过不是argmax选动作，而是max，这里的解释是：假设agent选择了最佳动作，状态价值是下一次转移的期望奖励加上下一个状态的折扣价值</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U(s^{&#39;})]
\]</span></p>
<ul>
<li>注意两个式子本质是一样的，都是挑动作，都是在所有可能的状态下累加，但这是基于对未来期望的估计，实际执行就是选择一个动作，转移到一个状态。</li>
<li>该式即<strong>bellman方程</strong></li>
</ul></li>
<li><p>引入Q函数，Q是动作和状态的函数，U仅仅是状态的函数，两者的转换关系如下</p>
<p><span class="math display">\[
U(s) = \underset{a \in A(s)}{\text{max}} Q(s,a)
\]</span></p></li>
<li><p>同理也可以写成bellman方程的形式</p>
<p><span class="math display">\[
Q(s,a) = \sum_{s^{&#39;}} P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;})]
\]</span></p></li>
<li><p>注意以上讨论的都是最优价值函数和最优q函数，都是取max，即计算最优策略下的return，区别于on-policy价值函数和q函数，计算的是期望</p></li>
</ul>
<h2 id="reward-shaping">reward shaping</h2>
<ul>
<li><p>可以通过修改奖励函数R（而不改变最优策略）来使强化学习过程更加稳定</p>
<ul>
<li>约束：避免一些agent套路reward的情况</li>
<li>探索：鼓励explore</li>
<li>加速：改善奖励稀疏的情况，将任务分解成更小的子任务，从而使得智能体更容易学习</li>
</ul></li>
<li><p>一种常见的修改方式是引入势函数</p></li>
<li><p>势函数是一个仅与状态相关的函数<span
class="math inline">\(\Phi(s)\)</span>（不同于价值函数，与动作状态序列无关，不是消掉动作得到的）</p></li>
<li><p>势函数编码了环境本身客观存在的因素，影响了奖励</p></li>
<li><p>可以证明，势函数可以为状态s的任意函数，且加入及时奖励时，bellman方程得到的最优策略不变，即当奖励函数改成</p>
<p><span class="math display">\[
R^{&#39;}(s,a,s^{&#39;}) = R(s,a,s^{&#39;}) + \gamma \Phi(s^{&#39;}) -
\Phi(s)
\]</span></p>
<p>时，最优策略不变，<span
class="math inline">\(Q(s,a)=Q^{&#39;}(s,a)\)</span></p></li>
</ul>
<h2 id="求解mdp">求解MDP</h2>
<h3 id="价值迭代">价值迭代</h3>
<ul>
<li><p>n个状态，bellman方程就有n个方程n个未知数，非线性方程的解析解很难得到，可以通过迭代的方法，随机初始值，再根据邻居的价值更新每个状态的价值，重复直至平衡</p></li>
<li><p>引入迭代的timestep i，bellman更新(<strong>Bellman
Update</strong>)如下</p>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \underset{a \in A(s)}{\text{max}} \sum_{s^{&#39;}}
P(s^{&#39;}|s,a)[R(s,a,s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p></li>
<li><p>可以证明：无限次迭代可以保证达到平衡，得到最优策略(前提是即时奖励是正确的)。</p></li>
</ul>
<h3 id="策略迭代">策略迭代</h3>
<ul>
<li>有些时候我们并不需要得到精确的价值函数，只要知道哪个动作带来的价值最大即可，这就引出了直接对策略进行迭代优化的思想
.</li>
<li>从某个初始策略开始，交替进行以下两个步骤
<ul>
<li>策略评估：给定策略，计算执行策略后某一时间步每个状态的价值</li>
<li>策略改进：基于所有状态价值的一步前瞻（即价值函数bellman方程）来计算新的策略</li>
</ul></li>
<li>直到策略改进不对价值产生（足够大）改变</li>
<li>策略评估也是基于bellman方程，只不过不用遍历动作，因为动作已经由策略决定，然后固定当前时间步i策略<span
class="math inline">\(\pi_i\)</span>，我们可以得到n个方程，求解即可</li>
</ul>
<p><span class="math display">\[
U_{i}(s) = \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p>
<ul>
<li>在状态空间比较大的时候，精确求解比较困难，这时候可以使用修正策略迭代来进行策略评估，即下一时间步的价值函数直接由当前策略计算出，然后反复迭代</li>
</ul>
<p><span class="math display">\[
U_{i+1}(s) \leftarrow \sum_{s^{&#39;}}
P(s^{&#39;}|s,\pi_i(s))[R(s,\pi_i(s),s^{&#39;}) + \gamma U_i(s^{&#39;})]
\]</span></p>
<ul>
<li>以上都是同步的形式，即每次迭代更新所有状态。事实上可以只更新部分状态，即异步策略迭代
<ul>
<li>好处是可以只专注为某些有效的状态更新策略，有些状态可能无论什么动作都达不到最优解</li>
</ul></li>
</ul>
<h3 id="线性规划">线性规划</h3>
<ul>
<li>TBD</li>
</ul>
<h3 id="在线算法">在线算法</h3>
<ul>
<li>价值迭代和策略迭代都是离线的：给定了所有条件/奖励，先生成最优解，然后agent执行</li>
<li>在线算法：agent不是拿到离线解之后再执行，而是在每个决策点即时计算决策。</li>
</ul>
<h2 id="老虎机问题">老虎机问题</h2>
<ul>
<li>TBD</li>
</ul>
<h2 id="pomdp">POMDP</h2>
<ul>
<li>部分可观测环境的马尔科夫决策过程</li>
<li>因为agent对自己所处的状态不确定（这是部分可观测的定义），所以需要引入一个信念状态，然后agent的决策周期增加了一个环节
<ul>
<li>根据信念状态，执行动作</li>
<li>观测感知（证据）</li>
<li>基于感知、动作、之前的信念状态，通过某种更新机制得到新的信念</li>
</ul></li>
<li>在物理空间状态上求解POMDP可以简化为在信念状态空间上求解MDP</li>
<li>POMDP的价值迭代</li>
</ul>
<h1 id="single-agent-rl">Single Agent RL</h1>
<ul>
<li>Agent处在MDP当中，不知道转移模型和奖励函数，需要通过采取行动了解更多信息</li>
<li>上文的序列决策是在已知环境下，如何得到一个最优策略，其实不需要agent的互动。一般而言的强化学习是指环境未知，需要agent在与环境的交互中来得到数据，从而确定最优策略。</li>
<li>基于模型的方法
<ul>
<li>环境提供了转移模型，或者一开始未知环境模型，但是需要去学习</li>
<li>通常会学习一个价值函数，定义为状态s之后的奖励总和</li>
<li>上文的序列决策都是在基于模型的前提下阐述的</li>
</ul></li>
<li>无模型的方法
<ul>
<li>不知道环境的转移模型，而且也不会学习它</li>
<li>agent直接学习策略，一般通过两种方式来在无模型的前提下学习策略
<ul>
<li>学习Q函数，即学习处于状态s下采取动作a得到的奖励</li>
<li>直接学习策略<span
class="math inline">\(\pi\)</span>，即学习状态到动作的映射</li>
</ul></li>
</ul></li>
</ul>
<h2 id="被动强化学习">被动强化学习</h2>
<ul>
<li>策略固定，学习价值函数</li>
<li>策略固定，比如说贪心的取价值最大的动作，这时候只需要将Q函数学好，策略固定的情况下具体的最优动作也就出来了。</li>
<li>类似于策略评估（给定策略，计算执行策略后某一时间步每个状态的价值），但agent不知道采取动作后到各个状态的转移概率，也不知道即时奖励</li>
</ul>
<h3 id="直接价值估计">直接价值估计</h3>
<ul>
<li>一个状态的价值定义为从该状态出发的期望总奖励（reward-to-go）</li>
<li>每次trial都会在其经过的状态上留下一个价值的数值样本(多次经过一个状态就提供多个样本)</li>
<li>这样就收集了样本，可以使用监督学习学到状态到价值的映射</li>
<li>但是该方法忽略了一个重要约束：状态价值应满足固定策略的bellman方程，即状态的价值和后继状态的奖励和期望价值相关，而不是只取决于自己</li>
<li>这种忽略将导致搜索空间变大，收敛缓慢</li>
</ul>
<h3 id="自适应动态规划adp">自适应动态规划(ADP)</h3>
<ul>
<li>agent学习状态之间转移模型，并用dp解决MDP</li>
<li>在环境确定/可观测的情况下，不断的trial，得到数据，训练一个监督模型，输入当前状态和动作，输出后续状态概率，即转移模型</li>
<li>得到转移模型后，之后按照序列决策的方法，通过修正策略迭代求解MDP</li>
<li>可以看到ADP是需要agent先不断的trial，在环境中得到一系列包含奖励信号的历史数据，然后用这些数据学习到环境的转移模型，将其转化为环境已知的序列决策问题。</li>
<li>自适应动态规划可以看成是策略迭代在被动强化学习设置下的扩展</li>
</ul>
<h3 id="时序差分学习">时序差分学习</h3>
<ul>
<li><p>在被动强化学习的setting下，即给定策略<span
class="math inline">\(\pi\)</span>，假如在状态s下采取动作<span
class="math inline">\(\pi(s)\)</span>转移到了状态<span
class="math inline">\(s^{&#39;}\)</span>，则通过时序差分方程更新价值函数:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow U^{\pi}(s) + \alpha (R(s,\pi(s),s^{&#39;}) +
\gamma U^{\pi}(s^{&#39;}) - U^{\pi}(s))
\]</span></p></li>
<li><p>其中<span
class="math inline">\(\alpha\)</span>是学习率。对比bellman，时序差分是在观测到在状态s下采取动作a到达了状态s'，就根据这个相继状态之间价值的差分更新价值:</p>
<ul>
<li><p>差分项是关于误差的信息，更新是为了减少这个误差</p></li>
<li><p>公式变化后可以看出来是当前价值和奖励+未来折扣价值之间做插值:</p>
<p><span class="math display">\[
U^{\pi}(s) \leftarrow (1-\alpha)U^{\pi}(s) + \alpha
(R(s,\pi(s),s^{&#39;}) + \gamma U^{\pi}(s^{&#39;}))
\]</span></p></li>
</ul></li>
<li><p>与自适应动态规划的联系与区别：</p>
<ul>
<li>都是根据未来调整当前价值，自适应的未来是在所有可能后继状态上加权求和，而时间差分的未来是观测到的后继</li>
<li>自适应尽可能进行多的调整，以保证价值估计和转移模型的一致性；差分对观测到的转移只做单次调整</li>
<li>TD可以看成一种近似ADP
<ul>
<li>可以用转移模型生成多个pseudo
experience，而不是仅仅只看真实发生的一次转移，这样TD的价值估计会接近ADP</li>
<li>prioritized sweeping，对哪些高概率
后继状态刚刚经过大调整的状态进行更新</li>
<li>TD作为近似ADP的一个优点是，训练刚开始时，转移模型往往学不正确，因此像ADP一样学习一个精确的价值函数来匹配这个转移模型意义不大。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="主动强化学习">主动强化学习</h2>
<ul>
<li>需要学习策略</li>
<li>需要学习一个完整的转移模型，需要考虑所有的动作，因为策略不固定（未知）</li>
<li>需要考虑，得到最优策略后，简单的执行这个策略就是正确的吗？</li>
</ul>
<h3 id="引入explore">引入explore</h3>
<ul>
<li>自适应动态规划是greedy的，需要引入exploration</li>
<li>一个宏观的设计，是引入探索函数f(u,n)，选择价值u较高的即贪心，选择尝试次数n少的即探索</li>
</ul>
<h3 id="td-q-learning">TD Q-learning</h3>
<ul>
<li><p>一种主动强化学习下的时序差分方法</p></li>
<li><p>无需一个学习转移概率的模型，无模型的方法</p></li>
<li><p>通过学习动作-价值函数来避免对转移模型本身的需求</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
max_{a^{&#39;}}Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>不需要转移模型P(s'|s,a)</p></li>
<li><p>注意因为没有给定策略，这里需要对所有可能动作取max</p></li>
<li><p>奖励稀疏时难以学习</p></li>
</ul>
<h3 id="sarsa">Sarsa</h3>
<ul>
<li><p>即state,action,reward,state,action，sarsa的缩写代表了更新的五元组</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s,a,s^{&#39;}) + \gamma
Q(s^{&#39;},a^{&#39;}) - Q(s,a))
\]</span></p></li>
<li><p>相比TD
Q-learning，不是对所有可能动作取max，而是先执行动作，再根据这个动作更新</p></li>
<li><p>如果agent是贪心的，总是执行q-value最大的动作，则sarse和q-learning等价；如果不是贪心，sarsa会惩罚探索时遇到的负面奖励动作</p></li>
<li><p>on/off policy</p>
<ul>
<li>sarsa是on-policy：“假设我坚持我自己的策略，那么这个动作在该状态下的价值是多少？”</li>
<li>q-learning是off-policy：“假设我停止使用我正在使用的任何策略，并依据估计选择最佳动作的策略开始行动，那么这个动作在改状态下的价值是多少？”</li>
</ul></li>
</ul>
<h2 id="强化学习中的泛化">强化学习中的泛化</h2>
<ul>
<li>价值函数和Q函数都用表格的形式记录，状态空间巨大</li>
<li>要是能参数化，需要学习的参数值可以减少很多</li>
<li>对于被动强化学习，需要根据trials使用监督学习价值函数，这里可以用函数或者NN来参数化。</li>
<li>对于时序差分学习，可以将差分项参数化，通过梯度下降学习</li>
<li>参数化来近似学习价值或者q函数存在几个问题
<ul>
<li>难以收敛</li>
<li>灾难性遗忘：可以通过experience
replay，保存轨迹进行回放，确保agent不再访问的那部分状态空间上的价值函数仍然准确</li>
</ul></li>
<li>奖励函数设计，如何解决稀疏奖励？
<ul>
<li>问题：credit
assignment，最后的正面或者负面奖励应该归因到哪次动作上</li>
<li>可以通过修改奖励函数（reward
shaping）来提供一些中间奖励，势函数就是一个例子，势反映了我们所希望的部分状态（某个子目标的实现、离最终希望的终止状态的某种可度量的距离）</li>
</ul></li>
<li>另一种方案是分层强化学习，TBD</li>
</ul>
<h2 id="策略搜索">策略搜索</h2>
<ul>
<li><p>只要策略的表现有所改进，就继续调整策略</p></li>
<li><p>策略是一个状态到动作的映射函数</p></li>
<li><p>将策略参数化表达，尽管可以通过优化q函数得到，但并不一定得到最优的q函数或者价值估计，因为策略搜索只在乎策略是否最优</p></li>
<li><p>直接学习Q值，然后argmax
Q值得到策略会存在离散不可导问题，这时将Q值作为logits，用softmax表示动作概率，用类似gumbel-softmax使得策略连续可导</p></li>
<li><p>假如执行策略所得到的期望奖励可以写成关于参数的表达式，则可以使用策略梯度直接优化；否则可以通过执行策略观测累计的奖励来计算表达式，通过经验梯度优化</p></li>
<li><p>考虑最简单的只有一次动作的情况，策略梯度可以写成下式，即对各个动作的奖励按其策略概率加权求和，并对策略参数求导。</p>
<p><span class="math display">\[
\triangledown_{\theta}\sum_aR(s_0,a,s_0)\pi_{\theta}(s_0,a)
\]</span></p></li>
<li><p>将这个求和用策略所定义的概率分布生成的样本来近似，并且扩展到时序状态，就得到了REINFORCE算法，这里用N次trial近似策略概率加权求和，并且将单步奖励扩展到价值函数，状态也扩展到整个环境的状态集合：</p>
<p><span class="math display">\[
\frac1N
\sum_{j=1}^N\frac{u_j(s)\triangledown_{\theta}\pi_{\theta}(s,a_j)}{\pi_{\theta}(s,a_j)}
\]</span></p></li>
</ul>
<h1 id="marl-multi-agent-rl">MARL (Multi-Agent Rl)</h1>
<ul>
<li>先挖坑</li>
<li><img data-src="/img/marl1.png" width="1200"></li>
<li><img data-src="/img/marl2.png" width="1200"></li>
<li><img data-src="/img/marl3.png" width="1200"></li>
<li><img data-src="/img/marl4.png" width="1200"></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>RL</category>
      </categories>
      <tags>
        <tag>reinforcement learning</tag>
        <tag>multi-agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Prompt - Task Reformulation in NLP</title>
    <url>/2021/05/13/pet/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/ea5f8ed907cd529450d9e9793f9aa94a.png" width="500"/></p>
<ul>
<li>Record of recent task reconstruction methods based on templates, a
particularly interesting direction since the appearance of GPT-3. These
methods typically design prompts for tasks, converting samples and tasks
into natural language templates, which are then directly input into
pre-trained language models to generate text, thereby indirectly
completing the tasks. The construction of prompts standardizes the form
of downstream tasks and pre-trained tasks (language models), achieving
good results in few-shot learning. Key papers to read include the
following nine:
<ul>
<li>Early work that converts questions into natural language and uses
pre-trained language models for answers:
<ul>
<li>(Harvard) Commonsense Knowledge Mining from Pretrained Models</li>
<li>(Heidelberg) Argumentative Relation Classification as Plausibility
Ranking</li>
<li>(NVIDIA) Zero-shot Text Classification With Generative Language
Models</li>
</ul></li>
<li>The PET approach, Pattern Exploiting Training:
<ul>
<li>(LMU) Exploiting Cloze Questions for Few Shot Text Classification
and Natural Language Inference</li>
<li>(LMU) It’s Not Just Size That Matters: Small Language Models Are
Also Few-Shot Learners</li>
<li>(UNC) Improving and Simplifying Pattern Exploiting Training</li>
</ul></li>
<li>Automatically constructing prompts, Automatically Searching Prompts:
<ul>
<li>(UCI, UCB) AUTOPROMPT: Eliciting Knowledge from Language Models with
Automatically Generated Prompts</li>
<li>(Princeton, MIT) Making Pre-trained Language Models Better Few-shot
Learners</li>
<li>(THU) GPT Understands, Too <span id="more"></span></li>
</ul></li>
</ul></li>
</ul>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="commonsense-knowledge-mining-from-pretrained-models">Commonsense
Knowledge Mining from Pretrained Models</h1>
<ul>
<li>The authors aim to mine commonsense knowledge from data with unknown
distributions, whereas traditional supervised learning methods are
easily influenced by the data distribution in the training set, leading
to biased results.</li>
<li>They convert relation triples into masked sentences and input them
into BERT. The mutual information between the predicted results from
BERT and the triples is used to rank the credibility of the
triples.</li>
<li>The task involves scoring a given triple to determine its likelihood
of representing real-world knowledge. This is divided into two steps:
<ul>
<li><p>Converting the triple into a masked sentence: multiple templates
were manually designed for each relationship, and a set of rules was
designed to ensure grammatical correctness (singular/plural, inserting
articles, modifying nouns, etc.). All combinations of these templates
and rules form a series of candidate sentences, which are then input
into a pre-trained unidirectional language model to compute the
log-likelihood score of each sentence being grammatically
correct.</p></li>
<li><p>Inputting the generated sentences into BERT for scoring: here,
the authors use conditional pointwise mutual information (PMI), where
the mutual information between the head and tail entities, conditioned
on the relation r, serves as the score:</p>
<p><span class="math display">\[
PMI(tail, head | relation) = \log p(tail | head, relation) - \log p(tail
| relation)
\]</span></p>
<p>In the language model, this effectively means masking the tail and
predicting it. The first term in the equation masks only the tail, while
the second term also masks the head (no prediction for the head).
Additionally, if entities are composed of multiple words, a greedy
approximation is used. Initially, all words are masked, and the
highest-probability word is unmasked, followed by iterative predictions
for the remaining words, where each time the highest-probability word is
restored. The product of these probabilities gives the conditional
probability of the entire word. The equation is asymmetric, so the
authors also compute the head entity probability based on the
relationship and tail entity, and average the two PMI values as the
final result.</p></li>
</ul></li>
<li>The final results, although not as good as supervised learning,
achieved the best results in unsupervised learning.</li>
<li>This is an early attempt to use pre-trained models for Mask Predict,
where the task is framed as a Cloze task. The patterns here are still
manually designed (with a set of rules designed for each relation).</li>
</ul>
<h1
id="argumentative-relation-classification-as-plausibility-ranking">Argumentative
Relation Classification as Plausibility Ranking</h1>
<ul>
<li>The task in this paper is Argumentative Relation Classification,
i.e., text pair classification, where the goal is to distinguish whether
a pair of texts supports or contradicts a given (or implicit)
conclusion. In positive text pairs, both texts support the conclusion,
while in negative pairs, one supports and the other contradicts the
conclusion.</li>
<li>For this interesting task, the authors propose a similarly
interesting approach: using a Siamese network for ranking, where the
ranking is based on the plausibility (credibility) of the constructed
text. And what is this constructed text? Quite simple: the two sentences
to be classified are connected by a conjunction to form the constructed
text:
<ul>
<li>Positive example: Text A, and Text B</li>
<li>Negative example: Text A, however, Text B</li>
</ul></li>
<li>If Text A and Text B are contradictory, the credibility of the
negative example is high. If Text A and Text B support each other, the
credibility of the positive example is high.</li>
<li>The next step involves using a pre-trained language model as the
encoder in the Siamese network for ranking.</li>
<li>The core idea here is to transform the task into natural language
and use the language model, which has learned general knowledge about
natural text, to perform the task and make predictions indirectly.</li>
<li>Similar to the previous paper, the key here is to convert the task
into natural language (a template) and cleverly use pre-trained language
models to indirectly complete the task (by completing the constructed
task).</li>
</ul>
<h1
id="zero-shot-text-classification-with-generative-language-models">Zero-shot
Text Classification With Generative Language Models</h1>
<ul>
<li>The authors use GPT to transform the text classification task into a
natural language question by combining the original text with the
category, and indirectly determine the category through text
generation.</li>
<li>The main advantage, as highlighted in the title, is zero-shot
learning, which allows the model to generalize to categories that do not
exist in the training data.</li>
<li>Specifically, the text classification problem is turned into a
multiple-choice QA task, where the options are formulated into a
question: "Which category does this text belong to? A; B; C; D..." and
then the text to be classified is appended. The goal is to train the
language model to directly generate the correct category as text.</li>
<li>To minimize the gap between pre-training and fine-tuning, the
authors introduce a pre-training task, called title prediction
pretraining, where all candidate titles are concatenated with the main
text and the correct title is generated.</li>
<li>This is a very intuitive, indirect, and bold use of language models
for classification tasks, where the language model directly generates
category labels. <img data-src="https://z3.ax1x.com/2021/05/17/gW98oV.png"
alt="gW98oV.png" /></li>
<li>The final zero-shot results, while not as good as fine-tuning or
state-of-the-art models, show a strong generalization ability,
outperforming the random and majority baseline models. The key takeaway
is how the language model is used creatively to solve the classification
task by designing multiple-choice questions.</li>
</ul>
<h1
id="exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inference">Exploiting
Cloze Questions for Few Shot Text Classification and Natural Language
Inference</h1>
<ul>
<li>This paper formally introduces the concept of PET:
Pattern-Exploiting Training.</li>
<li>In the previous three papers, we see that many NLP tasks can be
completed unsupervised or indirectly by providing natural language
descriptions of the tasks through language models. However, these
methods still fall short compared to supervised learning methods.</li>
<li>PET offers a semi-supervised learning approach that successfully
outperforms supervised models in low-resource settings.</li>
<li>The principle of PET is explained in a single diagram: <img data-src="https://z3.ax1x.com/2021/05/17/gWivmd.png" alt="gWivmd.png" />
<ul>
<li>The authors introduce two concepts: pattern, which transforms the
input text into a masked Cloze text based on the task, and verbalizer,
which maps the predicted masked words from the language model to labels.
Each pattern corresponds to a verbalizer, forming a PvP
(pattern-verbalizer pair).</li>
<li>The PET process is divided into three steps:
<ul>
<li>First, use PvP to fine-tune the pre-trained language model on a
small training set.</li>
<li>Second, for each task, multiple PvPs can be designed to create
different models through fine-tuning, and then a soft label is assigned
to unannotated data using these models.</li>
<li>Third, a classifier is trained on the labeled data to complete
supervised learning.</li>
</ul></li>
</ul></li>
<li>In the second step, there are two small details: ensemble learning
with multiple classifiers (adding the predicted label distributions,
which can be equally weighted or weighted based on zero-shot performance
in the training set) and using soft labels (probability distributions)
when labeling the data, with softmax applied at temperature T=2. These
two techniques help better leverage the knowledge from the language
model, one through ensemble robustness and the other through knowledge
distillation.</li>
<li>The authors also introduce iPET, a traditional semi-supervised
learning approach that iterates between labeling and training, using
increasing amounts of data and different generations of models to
improve performance.</li>
<li>The advantage of this semi-supervised framework is that the final
operation is still supervised learning, achieving high accuracy, while
reducing the uncertainty introduced by the language model through
knowledge distillation (soft labels).</li>
</ul>
<h1
id="its-not-just-size-that-matters-small-language-models-are-also-few-shot-learners">It's
Not Just Size That Matters: Small Language Models Are Also Few-Shot
Learners</h1>
<ul>
<li>The original PET team has another paper where the motivation is that
small models can also achieve results comparable to large models like
GPT-3 in few-shot learning when using PET, promoting environmental
sustainability.</li>
<li>In this paper, the authors extend the prediction of masked words in
PvP to multiple masks, inserting a fixed maximum number of masks during
training and then performing post-processing during prediction.</li>
<li>They provide more extensive experimental results, which, while still
in preprint form (not yet published in conferences...), later won the
NAACL 2021 Outstanding Paper Award.</li>
</ul>
<h1 id="improving-and-simplifying-pattern-exploiting-training">Improving
and Simplifying Pattern Exploiting Training</h1>
<ul>
<li><p><a href="https://imgtu.com/i/gIih5j"><img data-src="https://z3.ax1x.com/2021/05/19/gIih5j.png"
alt="gIih5j.png" /></a></p></li>
<li><p>This paper improves PET by further simplifying the design of the
pattern-verbalizer pair and reducing the number of patterns needed to
achieve few-shot learning in a broad set of tasks.</p></li>
<li><p>The simplification helps lower the entry barrier for researchers
and developers by making it easier to implement this framework with
minimal effort.</p></li>
</ul>
<h1
id="autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-prompts">AUTOPROMPT:
Eliciting Knowledge from Language Models with Automatically Generated
Prompts</h1>
<ul>
<li><p>From the work introduced above, it can be seen that constructing
effective text to trigger language models to generate results is
crucial, which means constructing the prompt. Currently, all prompts are
manually constructed, but later a series of works emerged attempting to
automatically construct prompts.</p></li>
<li><p>This work cannot really be considered as prompts; a more accurate
term would be "trigger words sequence," because it essentially applies a
method for generating adversarial text samples to the task of
constructing prompts.</p></li>
<li><p>Specifically, it draws on two papers: <em>HotFlip: White-box
adversarial examples for text classification</em> and <em>Universal
Adversarial Triggers for Attacking and Analyzing NLP</em>. The idea is
to concatenate a sequence of trigger words into the sample, which can
lead the model to make incorrect predictions. The search for trigger
words in the model primarily uses the HotFlip method:</p>
<ul>
<li><p>Initialize the trigger word <span
class="math display">\[\mathbf{e}_{adv}\]</span> (e.g., words like the,
a, an), then pass the model forward to obtain the gradient of the loss
with respect to the trigger word embedding <span
class="math display">\[\nabla_{\mathbf{e}_{adv}} \mathcal{L}\]</span>.
Note that the label used for loss calculation should be the incorrect
label that the model is intended to be fooled into predicting (i.e., the
label after fooling the model).</p></li>
<li><p>We aim to replace the <span class="math inline">\(i\)</span>-th
trigger word with a word <span
class="math display">\[\mathbf{e}_{i}\]</span> such that the loss is
minimized most significantly after replacement, meaning the model is
most likely to predict the wrong label. Therefore, the word we are
looking for is <span class="math display">\[
\underset{\mathbf{e}_{i}^{\prime} \in \mathcal{V}}{\arg \min }
\mathcal{L}(\mathbf{e}_{i}^{\prime}) \]</span>, where a first-order
Taylor expansion is used for approximation. We need to compute the
gradient of the loss with respect to the token. Since token embedding
lookup is not differentiable, we need to compute the gradient of the
embedding of a specific token.</p></li>
</ul>
<p><span class="math display">\[
\mathcal{L}(\mathbf{e}_{i}^{\prime})    =  \mathcal{L}(\mathbf{e}_{adv_{i}})
+ \left[\mathbf{e}_{i}^{\prime}-\mathbf{e}_{adv_{i}}\right]^{\top}
\nabla_{\mathbf{e}_{adv_{i}}} \mathcal{L}
\]</span></p>
<p><span class="math display">\[
\propto \left[\mathbf{e}_{i}^{\prime}-\mathbf{e}_{adv_{i}}\right]^{\top}
\nabla_{\mathbf{e}_{adv_{i}}} \mathcal{L}
\]</span></p>
<ul>
<li>This results in the first trigger word in the first round of
iteration. Then, through beam search, the remaining trigger words are
generated, iterating multiple times to eventually obtain a sequence of
trigger words that can be used to attack the model.</li>
</ul></li>
<li><p>The above describes the HotFlip method for text adversarial
attacks. Its essence is to generate trigger words and append them to the
sample to make the model predict an incorrect label. The idea of
autoprompt is to generate trigger words to make the model predict a
specified label.</p>
<p><a href="https://imgtu.com/i/ghFDuF"><img data-src="https://z3.ax1x.com/2021/05/18/ghFDuF.md.png"
alt="ghFDuF.md.png" /></a></p></li>
<li><p>Now it becomes simpler. The authors first used the HotFlip method
to generate trigger words for each task in the training set, then used a
template to transform the sample into a sentence. As shown in the
figure, the sentence is concatenated with the trigger word sequence
([T]) and the mask position ([P]) that the PLM is to predict. The model
then predicts the word, and the label is obtained through
post-processing. The specific post-processing operation involves summing
the probabilities of the predicted words for each label, and finally
normalizing these sums to get the probability for each label.</p></li>
<li><p>The above only explains the automatic prompt construction method
in PvP. As for the verbalizer, i.e., the mapping from predicted words to
labels, the authors also propose an automatic search method:</p>
<ul>
<li>After encoding the PLM and obtaining the embedding of the mask token
containing contextual information, this embedding is used as a feature
input, and the label is used as the output to train a logistic
classifier. Then, the PLM-encoded embedding of each word is fed into
this classifier to obtain a score for each word on each label. The top k
words with the highest scores are chosen for each label as the mapped
word set. Essentially, this compares the mask token's encoded embedding
(needed for predicting the label) with the embeddings of each word,
selecting the top k closest words, but using a logistic classifier to
apply class-related feature weighting. This is not merely based on the
semantic similarity of the PLM encoding but is a very clever
method.</li>
</ul></li>
</ul>
<h1
id="making-pre-trained-language-models-better-few-shot-learners">Making
Pre-trained Language Models Better Few-shot Learners</h1>
<ul>
<li><p>The title of this paper is essentially the title of GPT-3 with
"better" added, emphasizing how to better use prompts for few-shot
learning.</p></li>
<li><p>A training framework is proposed: prompt-based fine-tuning +
automatic prompt generation + dynamically and selectively integrating
task descriptions into prompts, all of which are strongly task-agnostic.
Let's now go through these three improvements in detail.</p></li>
<li><p><a href="https://imgtu.com/i/g58yOe"><img data-src="https://z3.ax1x.com/2021/05/19/g58yOe.png"
alt="g58yOe.png" /></a></p></li>
<li><p>The image above clearly demonstrates the first improvement:
prompt-based fine-tuning. As we can see, compared to previous
prompt-based methods, in addition to the input and prompt, the input is
also concatenated with a description for each label.</p></li>
<li><p>As for automatic prompt generation, it is divided into two
parts:</p>
<ul>
<li>How to automatically generate the mapping from target words to
labels given a template. Here, the author iterates over the results of a
pre-trained language model (PLM). First, for each class, all the
training samples of this class are identified, and the probability
distribution of the masked word is inferred using the PLM. The top-k
words are selected by accumulating the probability distributions of all
samples to get the word-to-label mapping for that category. Since the
model parameters change during the fine-tuning process, the result may
shift, so the mapping needs to be re-ranked and adjusted after each
training round.</li>
<li>Given a category and its target word, how to generate a template.
The author uses the T5 model because its mask span seq2seq pretraining
task aligns well with the template generation task. This can be
explained in a single diagram: <img data-src="https://z3.ax1x.com/2021/05/19/g5YjfI.png" alt="g5YjfI.png" /> The
generated prompt takes into account both the context of the training
samples and the semantic context of the label word. The author uses a
wide beam width to beam search a set of prompt candidates (100+), then
fine-tunes each sample on a small training set, selecting the one with
the highest performance on the validation set (or using top-k ensemble)
as the final prompt.</li>
<li>Dynamic selective integration of tasks, which is more complicated.
After obtaining the prompt, the question is how to construct the input
sample. As shown in the first image, for each category, a sample is
randomly selected and converted into a prompt to serve as the
description for that category. All category descriptions are
concatenated with the input sample (the one to be trained). During
sampling, sentence-BERT is used to obtain the semantic embeddings of
each sample, and only the top 50% of samples with the highest semantic
similarity to the input sample are selected.</li>
</ul></li>
<li><p>The design of this prompt is somewhat similar to a semantic
similarity task, where the input is "x is a mask example? y is positive;
z is negative." This essentially compares the semantic similarity
between x and yz, propagating the label through this
comparison.</p></li>
</ul>
<h1 id="gpt-understands-too">GPT Understands, Too</h1>
<ul>
<li><a href="https://imgtu.com/i/g52dDH"><img data-src="https://z3.ax1x.com/2021/05/19/g52dDH.png"
alt="g52dDH.png" /></a></li>
<li>This paper introduces P-tuning, which is not about finding discrete
prompts (specific texts), but rather continuous ones (embeddings).</li>
<li>Let's review the entire prompt-based method. It essentially
transforms data and tasks into a form suitable for language model tasks,
bringing them closer to pretraining objectives and enabling better
utilization of the pretrained model's knowledge. In practice, this
involves adding some prompt-generated templates to the input, and the
output becomes target words related to category labels. The author
reflects on whether these prompt-generated templates necessarily need to
be human-understandable text. After all, what the model actually
processes are embeddings. So, when searching for prompts, why not
directly optimize the embeddings instead? Therefore, the author proposes
using some unused symbols from word tables (such as the "unused" tokens
in BERT) as pseudo-template tokens. These tokens are fixed, and rather
than searching for new tokens, we directly optimize the embeddings
corresponding to these tokens.</li>
<li>To make these pseudo-tokens resemble natural language more closely,
rather than just being independent symbols, the author also uses a
bidirectional LSTM for encoding, which serves as the prompt encoder.
However, the motivation for this approach isn't fully clear. Why not
directly model the relationship within the PLM itself?</li>
<li>From this perspective, the approach is essentially about
concatenating a few embeddings to the input and optimizing them. The
output and post-processing adopt the PET (Prompt-based Elicitation of
Task) form, which feels like adding a layer for fine-tuning (hence the
name <strong>P</strong>rompt fine<strong>tuning</strong>?). In my view,
both layer-based fine-tuning and P-tuning introduce a small number of
parameters to adapt the PLM to downstream tasks, but P-tuning changes
the format of the downstream task to better align with pretraining
objectives, making the fine-tuning structural priors more reasonable. It
also offers a higher-level summary of prompt-based work.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="commonsense-knowledge-mining-from-pretrained-models">Commonsense
Knowledge Mining from Pretrained Models</h1>
<ul>
<li>作者想要做到挖掘未知分布数据中的常识，而传统的监督学习方法容易受到训练集中的数据分布影响，导致结果有偏差</li>
<li>将关系三元组转换为masked
sentence送给BERT，通过BERT的预测结果计算互信息来对三元组的可信度排序</li>
<li>任务，给定一个三元组为其打分，确定这个三元组代表了真实世界知识的可能性，作者将其分为两步：
<ul>
<li><p>将三元组转化为mask过后的句子：对每个关系手工设计了多个模板，同时还设计了一系列规则来确保语法正确性（单复数、插入冠词、改动名词等等），这样所有模板和规则的组合得到了一系列候选句子，然后通过预训练单向语言模型来计算每个句子是正常句子的得分log-likelihood</p></li>
<li><p>将生成的句子输入BERT打分：这里作者用条件点互信息计算，即在关系r的条件下，头尾实体之间的互信息大小作为分数：</p>
<p><span class="math display">\[
PMI(tail,head|relation) = \log p(tail|head, relation) - \log
p(tail|realtion) \\
\]</span></p>
<p>放在语言模型中，实际上就是将tail
mask掉然后预测，只不过上式右边第一项是只mask
tail,第二项则还mask掉了head（只mask,不预测）。另外可能出现实体由多个词组成的情况，这里作者采用了一种贪心近似的方法，先把词全部mask掉然后预测，拿到概率最高的词unmask，再反复迭代预测剩下的词，每次还原概率最高的词，之后累乘这一系列概率就可以得到整个词的条件概率。上式并不是对称的，因此作者还反过来计算了基于关系和尾实体的头实体概率，最后平均两个PMI值作为结果。</p></li>
</ul></li>
<li>最终结果虽然比不上监督学习，但是在无监督学习中取得了最佳效果</li>
<li>这是较早尝试利用预训练模型的Mask
Predict，将任务设计为完形填空来完成，可以看到这里的Pattern还是手工设计（针对每个关系设计一系列规则）。</li>
</ul>
<h1
id="argumentative-relation-classification-as-plausibility-ranking">Argumentative
Relation Classification as Plausibility Ranking</h1>
<ul>
<li>这篇论文做的任务为Argumentative relation
classification，即文本对分类，给定（或者不显式给出）结论，区分一对文本是支持还是反对。正例文本对里，两个文本都支持结论；负例文本对里，一个支持结论而另一个不支持，互相反驳。</li>
<li>对于这个很有意思的任务，作者采用了一个同样很有意思的做法：使用孪生网络做ranking，rank的是一个构造文本的plausibility，即可信度。而这个构造文本是什么？很简单，将要判别的两个句子用一个连接词连接起来，得到构造文本的正负例：
<ul>
<li>正例：文本A，而且，文本B</li>
<li>负例：文本A，然而，文本B</li>
</ul></li>
<li>假如文本A和文本B是反对的关系，那么显然负例这么一段文本的可信度高；为文本A和文本B互相支持，那么正例构造文本的可信度高。</li>
<li>接下来就用预训练语言模型作为孪生网络的编码器，然后做ranking。</li>
<li>本质思想是构造了文本和任务，将任务用正常的自然语言表示，这样就可以利用学习到正常文本知识的语言模型来做学习和预测。</li>
<li>和上一篇论文一样，核心都是将任务转为自然语言（模板），巧用预训练语言模型间接的完成任务（完成构造任务）</li>
</ul>
<h1
id="zero-shot-text-classification-with-generative-language-models">Zero-shot
Text Classification With Generative Language Models</h1>
<ul>
<li>作者使用GPT，将文本分类问题转化为给定包含原文本和类别的自然语言，通过文本生成间接判断类别</li>
<li>这样做的一个好处即标题提到的zero-shot，可以泛化到训练集中不存在的类别</li>
<li>具体而言，将文本分类问题转为一个选择QA任务，即所有的选项拼成了问题：该文本属于下面哪一类？A;B;C;D.....，之后再拼接上待分类文本，目标是训练语言模型，直接生成正确的类别的文本。</li>
<li>另外为了减少预训练和finetune之间的gap，作者还加入了一个前置的预训练任务，叫title
prediction
pretraining，即将所有候选标题和正文拼接起来，然后生成正确的标题。</li>
<li>这是一篇非常直观、间接且大胆的利用语言模型分类任务的工作，直接让语言模型生成类别文字。
<a href="https://imgtu.com/i/gW98oV"><img data-src="https://z3.ax1x.com/2021/05/17/gW98oV.png"
alt="gW98oV.png" /></a></li>
<li>最终的zero-shot结果，虽然依然比不上finetune和sota，但是相比random和majority两个baseline可以比较出模型还是学到了相当强的泛化能力。最主要的还是把语言模型玩出了花，提供了这么一种直接设计多项选择疑问句来完成分类任务的思路。</li>
</ul>
<h1
id="exploiting-cloze-questions-for-few-shot-text-classification-and-naturallanguage-inference">Exploiting
Cloze Questions for Few Shot Text Classification and NaturalLanguage
Inference</h1>
<ul>
<li>该论文正式引入了PET的概念：Pattern-Exploiting Training。</li>
<li>在上面三篇论文中我们可以看到，很多NLP任务可以通过提供自然语言任务描述的方式，通过语言模型来无监督的或者间接的完成。但是这类方法终究还是比不过监督学习方法。</li>
<li>PET提供了一种半监督学习方式，在低资源场景下成功超过了监督学习模型的结果。</li>
<li>一张图就能说明PET的原理： <a href="https://imgtu.com/i/gWivmd"><img data-src="https://z3.ax1x.com/2021/05/17/gWivmd.png" alt="gWivmd.png" /></a>
<ul>
<li>作者引入了两个名词，pattern负责把输入文本根据任务改造成一个带mask的完形填空文本，verbalizer负责把语言模型预测的mask词映射到label上。这样一个pattern对应一个verbalizer，称为PvP。。。（pattern
verbalizer pair）</li>
<li>整个PET过程分三步：
<ul>
<li>第一步用PvP，在小训练集上微调预训练语言模型</li>
<li>第二步，每一个任务可以设计多个PvP，这样得到多个第一步训练出的语言模型，集成，在大量未标注数据上打标软标签</li>
<li>第三步，用一个分类器在打标后的数据上完成监督学习</li>
</ul></li>
</ul></li>
<li>第二步中有两个小细节：多分类器集成，即多个预测标签分布相加，这里可以等权重相加，也可以根据PvP直接在训练集上zero-shot的表现作为先验权重（实验结果这样做好些）；打标时打的是软标签即概率分布，softmax时取T=2做了温度处理。这两个处理都是为了能够更好的学习到语言模型的知识，一个在于集成更加鲁棒，另一个则相当于知识蒸馏。</li>
<li>另外作者还提出了iPET，其实就是传统的半监督学习，训练打标之间迭代，用越来越多的数据训练出不同代模型然后集成。</li>
<li>这样的半监督框架好处在于，最终实际操作依然是监督学习，准确率较高，而语言模型带来的不确定性在知识蒸馏（软化标签）的时候降低了。</li>
</ul>
<h1
id="its-not-just-size-that-matters-small-language-models-are-also-few-shot-learners">It's
Not Just Size That Matters: Small Language Models Are Also Few-Shot
Learners</h1>
<ul>
<li>还是PET原版人马，又水了一篇，换了个动机，说用PET的话，小模型也能在few-shot上取得与GPT-3这样的大模型接近的结果，环保</li>
<li>将PvP中要预测的词从单个mask扩展为多个mask，训练的时候插入固定最大数量的mask，预测时再做后处理</li>
<li>给了更丰富的实验结果（不过好像还是arxiv挂着，没中会议。。。）（更新：惊了，拿到了NAACL
2021 杰出论文）</li>
</ul>
<h1 id="improving-and-simplifying-pattern-exploiting-training">Improving
and Simplifying Pattern Exploiting Training</h1>
<ul>
<li><a href="https://imgtu.com/i/gIih5j"><img data-src="https://z3.ax1x.com/2021/05/19/gIih5j.png"
alt="gIih5j.png" /></a></li>
<li>PET依然需要大量领域未标注数据来做半监督学习，本文提出了ADAPET，不用未标注数据也能取得更好效果</li>
<li>作者通过修改任务目标来达成这一目的。当我们使用PET时，浪费了两类信息：
<ul>
<li>mask位置上预测的词，仅仅在与类别label有映射关系的target word
vocab上做softmax计算交叉熵，其余词没有计算损失</li>
<li>仅仅预测了mask位置，其他所有位置的embedding没有计算损失</li>
</ul></li>
<li>因此作者就想充分利用这两个信息，修改任务目标
<ul>
<li>将损失从交叉熵改为两个二元交叉熵，一个依然是在label相关target词上算损失，另一部分损失则负责优化降低其他所有不相关词的概率</li>
<li>将mask替换为正确或者错误的target
word，然后对输入剩下部分做MLM,要是target
word对的话MLM就应该预测对，反之就应该预测错</li>
<li>分别对应图中左右两类损失</li>
</ul></li>
<li>ADAPET增加了目标函数，对参数做了更充分的训练，对比PET结果也确实不错，不使用未标注数据还在很多任务上超过了PET</li>
</ul>
<h1
id="autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-prompts">AUTOPROMPT:
Eliciting Knowledge from Language Models with Automatically Generated
Prompts</h1>
<ul>
<li><p>由上面介绍的工作可以发现，构建有效的文本来触发语言模型得到结果至关重要，即构建prompt。目前看到的都是手工构建的，后来也出现了一批工作尝试自动构建prompts</p></li>
<li><p>这个工作其实不能算是prompts，更准确的说法是trigger words
sequence，因为它其实是把文本对抗样本生成的一套方法拿到了prompt构建当中。</p></li>
<li><p>具体而言，其借鉴了HotFlip: White-box adversarial examples for
text classification 和 Universal Adversarial Triggers for Attacking and
Analyzing
NLP两篇论文，即在样本中拼接一系列触发词，即可使得模型的预测结果错误，而模型的触发词搜索主要使用的是hotflip方法：</p>
<ul>
<li>初始化触发词 <span class="math display">\[\mathbf{e}_{a d
v}\]</span>（比如the，a，an等），前向过一遍模型得到损失关于触发词embedding的梯度
<span class="math display">\[\nabla_{\mathbf{e}_{a d v}}
\mathcal{L}\]</span>
，注意这里用于计算损失所用的label应该是想要攻击得到的错误label，即fool
model之后的label</li>
<li>我们希望替换第i个触发词为词 <span
class="math display">\[\mathbf{e}_{i}\]</span>，使得替换之后损失下降的最多，模型最容易预测出错误的标签，所以我们要找的词是
<span class="math display">\[ \underset{\mathbf{e}_{i}^{\prime} \in
\mathcal{V}}{\arg \min } \mathcal{L}(\mathbf{e}_{i}^{\prime})
\]</span>。这里通过泰勒一阶展开来近似，需要求到损失关于token的导数，由于token
embedding lookup不可导，所以才需要求到某个token的embedding的导数</li>
</ul>
<p><span class="math display">\[
\mathcal{L}(\mathbf{e}_{i}^{\prime})    =  \mathcal{L}(\mathbf{e}_{a d
v_{i}}) + \left[\mathbf{e}_{i}^{\prime}-\mathbf{e}_{a d
v_{i}}\right]^{\top} \nabla_{\mathbf{e}_{a d v_{i}}} \mathcal{L}
\]</span></p>
<p><span class="math display">\[
\propto \left[\mathbf{e}_{i}^{\prime}-\mathbf{e}_{a d
v_{i}}\right]^{\top} \nabla_{\mathbf{e}_{a d v_{i}}} \mathcal{L}
\]</span></p>
<ul>
<li>这样就得到了第一轮迭代中的第一个触发词，之后通过beam
search得到剩下的触发词，并迭代多次，最终得到可以用于攻击模型的触发词序列。</li>
</ul></li>
<li><p>以上是文本对抗攻击中的hotflip方法，其本质就是生成一些触发词，拼接到样本上，使得模型预测出错的label。autoprompt的思想就是生成触发词，使得模型预测出指定label。
<a href="https://imgtu.com/i/ghFDuF"><img data-src="https://z3.ax1x.com/2021/05/18/ghFDuF.md.png"
alt="ghFDuF.md.png" /></a></p></li>
<li><p>接下来就简单了。作者首先在训练集上用hotflip方法为每个任务生成了触发词，然后用模板将样本变为一个句子，如图所示，句子拼接上触发词序列（[T]）和PLM要预测的mask位置([P])，让模型预测出词之后再后处理得到label。具体的后处理操作是，将每个label对应的预测词集合得到的概率累加，最后归一化，作为标签的概率。</p></li>
<li><p>上面只说了PvP中的prompt自动构建方法，而verbalizer，即预测词到标签的映射作者也给出了一个自动搜索的方法：</p>
<ul>
<li>将PLM编码之后包含上下文信息的mask
token的embedding作为特征输入，标签作为输出来训练一个logistic分类器，之后将所有词的PLM编码之后的embedding依次输入这个分类器，得到每个词在每个标签上的评分，根据评分top
k来为每个标签类别选择词作为映射集合。这么做实际上是将预测标签所需的mask
token编码embedding和每个词的编码embedding比较，取最相近的top
k，只不过利用logistic分类器做了一个类别相关的特征加权，不仅仅是取PLM编码之后的语义相似度，非常巧妙。</li>
</ul></li>
</ul>
<h1
id="making-pre-trained-language-models-better-few-shot-learners">Making
Pre-trained Language Models Better Few-shot Learners</h1>
<ul>
<li>这篇论文标题就是GPT3的标题加了个better，强调如何更好的利用prompt做few
shot learning。</li>
<li>提出了一个训练体系：基于prompt的微调+prompt自动生成+动态选择性融入任务说明到prompt中，且这一切都是strong
task-agnostic。接下来分别说这三点改进。</li>
<li><a href="https://imgtu.com/i/g58yOe"><img data-src="https://z3.ax1x.com/2021/05/19/g58yOe.png"
alt="g58yOe.png" /></a></li>
<li>上图清晰的展示了第一点改进：基于prompt的微调。可以看到，和以往prompt方法相比，除了输入、prompt之外，输入还拼接上了每个label的说明</li>
<li>至于prompt自动生成，分为两部分：
<ul>
<li>如何在给定模板的情况下，自动生成目标词到标签的映射。这里作者也是用PLM的结果不断迭代。首先对每个类，找出这个类的所有训练样本，通过PLM推断得到mask词的概率分布，累加所有样本的概率分布取topk就得到了词到该类别标签的映射。由于接下来训练微调时模型参数变化，结果可能有改变，所以需要每轮训练后重新rerank调整一下映射关系。</li>
<li>给定类别和这个类别的目标词，如何生成模板。作者采用了T5模型，因为其mask
span seq2seq预训练的目标和模板生成任务很符合。一张图就可以解释清楚： <a
href="https://imgtu.com/i/g5YjfI"><img data-src="https://z3.ax1x.com/2021/05/19/g5YjfI.png" alt="g5YjfI.png" /></a>
这样生成的prompt考虑了训练样本上下文和标签词的语境。作者使用wide beam
width来beam
search出一堆prompt候选（100+），然后在一个小训练集上微调每个样本，取验证集最高的（或者topk集成）作为最终prompt</li>
<li>动态选择性融入任务，这里做的比较麻烦，即得到prompt后如何构造输入样本，也是如第一张图所示，对每个类别，采样一个样本转化为prompt当做这个类别的说明，将所有类别说明和输入样本（待训练样本）拼接。采样时，使用sentence-BERT得到每个样本的语义embedding，然后只取和输入样本语义相似度前50%的样本进行采样。</li>
</ul></li>
<li>这种prompt的设计有点像是在做语义相似度任务，输入x，已知y为正例，z为负例，构造了输入为“x是mask例？y为正例；z为负例”，相当于比较x与yz的语义相似度，做一个标签的传播</li>
</ul>
<h1 id="gpt-understands-too">GPT Understands, Too</h1>
<ul>
<li><a href="https://imgtu.com/i/g52dDH"><img data-src="https://z3.ax1x.com/2021/05/19/g52dDH.png"
alt="g52dDH.png" /></a></li>
<li>本文提出了P-tuning，即不是找离散的prompt（具体文本），而是找连续的（embedding）</li>
<li>回顾一下整个prompt based
methods，都是把数据和任务转化为语言模型任务的形式，使其更加贴近预训练目标，能够更好的利用预训练模型的知识。实际操作时，就是把输入添加一些prompt
generated templates，输出变成与类别label相关的target
words，作者反思，这些prompt generated templates
本质上就是一些词，一定要是人类能够理解的文本吗？这些文本输入到模型的实际上是embedding，那么搜索prompt的时候为什么不直接优化embedding呢？所以作者提出就用几个词表中没用的符号（例如BERT中的unused）来作为pseudo
template
token，固定这些token，不去搜索新的token，而是直接优化token对应的embedding。</li>
<li>为了让这些pseudo
token更像是自然语言，而不是独立的几个符号，作者还用了双向LSTM来做编码，即prompt
encoder，这里感觉动机阐释的不是很清楚，为什么不能放在PLM里直接建模之间关系？</li>
<li>这么看来整体就相当于输入拼接上几个embedding然后去优化，只不过输出和后处理采用了PET的形式，很像自己加了某个层去微调（所以叫<strong>P</strong>rompt
fine<strong>tuning</strong>？）。我感觉加层微调和P-tuning都是引入少量参数把PLM用到自己的下游任务上，只不过P-tuning转换了下游任务形式，使其跟贴近预训练目标，算是微调结构先验更合理吧，同时也算是从另一个高度总结了prompt一类的工作。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>nlp</tag>
        <tag>pretrained language model</tag>
        <tag>pet</tag>
        <tag>few shot</tag>
      </tags>
  </entry>
  <entry>
    <title>OJ</title>
    <url>/2017/03/27/oj/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/86c202d0b5712f432526beca2939166e.png" width="500"/></p>
<p>算法刷题目录，方便自己查找回忆复习
之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧</p>
<span id="more"></span>
<p>！<a
href="http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg">mark</a></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="leetcode">Leetcode</h1>
<ul>
<li><a href="https://leetcode.com/problemset/algorithms/">LeetCode
Algorithm List</a></li>
<li>Directly search for the serial number of each question</li>
</ul>
<h2 id="sorting">Sorting</h2>
<ul>
<li>75: Given a sequence containing only 0, 1, and 2, sort it to form
the form [0,0,...,0,1,...,1,2,...,2], and modify it on the original
sequence. Drawing inspiration from the Lomuto partition algorithm,
maintain three intervals [0,i], [i,j], [j,k] for the ranges of 0, 1, and
2, respectively. According to priority, if the current number is 2 or 1
or 0, first set nums[k]=2, k++, if the current number is 1 or 0, then
set nums[j]=1, j++, and if the current number is 0, set nums[i]=0, i++.
The further back it is, the more it can cover the previous ones, with
higher priority.</li>
<li>Given an unordered sequence, find the maximum difference between
adjacent elements after sorting the sequence in O(n) time complexity.
Using bucket sort, let the maximum and minimum values in the sequence be
max and min, respectively. It can be easily deduced that the maximum
difference between adjacent elements must be greater than (max - min) /
(n - 1), and let this value be gap, where n is the length of the
sequence. Divide the value range [min, max] of the sequence into nums
buckets with an interval of (max - min) / (n - 1), where nums is
int((max - min) / gap + 1). Distribute the elements of the sequence into
these buckets, storing only the maximum and minimum elements
corresponding to each interval. The maximum interval must be at the
boundaries between adjacent buckets (the maximum value of the previous
bucket and the minimum value of the next bucket), because the maximum
interval within a bucket is less than gap. Finally, traverse the buckets
to find the result.</li>
<li>179: Water problem, given a sequence of numbers, requiring the
combination of the numbers into the largest possible number. String
sorting</li>
</ul>
<h2 id="stack-queue">Stack, Queue</h2>
<ul>
<li>Merge K sorted linked lists into one sorted linked list, using a
min-heap to store all the head nodes of the lists, and then inserting
them in order into the new list</li>
<li>Arithmetic Expression, Stack, Python's Switch Syntax</li>
<li>224: Arithmetic expressions with parentheses, stack, an ingenious
method for handling parentheses, if the sign outside the parentheses is
negative, multiply by -1; otherwise, multiply by 1</li>
<li>373: Find the k groups of number pairs with the minimum sum in 2
ordered arrays, priority queue</li>
</ul>
<h2 id="combinatorial-mathematics">Combinatorial Mathematics</h2>
<ul>
<li>47: Generate all combinations of n numbers, with repetitions, and
sequentially insert numbers into the k+1 spaces of the already generated
k-number combinations, use sets for deduplication directly, and in the
Top Solution, deduplication is done by checking if the next number to be
inserted is a repetition</li>
</ul>
<h2 id="search-and-find">Search and Find</h2>
<ul>
<li>In a matrix with a specific order, find using two binary
searches</li>
<li>78: Generate subsets, recursively or iteratively, deciding for each
number whether to include it in the subset or not for recursion</li>
<li>In a letter matrix, find a word, depth-first search</li>
<li>89: Generate Gray code, iterative, adding 1 or 0 at the front each
time, Gray code is symmetrically above and below</li>
<li>140: Convert a string into a sentence by concatenating with a given
dictionary and spaces, memoized search</li>
<li>153: Split an ordered array into two parts and concatenate them,
then search, a special binary search</li>
<li>154:153 If a transformation of repeated digits is added, with a
condition that the binary start and end are the adjacent non-repeated
first digits</li>
<li>240:74 revised version, each row and column is ordered, count, and
separately perform binary search on rows and columns, the number must be
at the intersection of rows and columns that meet the conditions, and
there are other solutions</li>
</ul>
<h2 id="树">树</h2>
<ul>
<li>98: Determine if a binary tree is a search tree, i.e., the left
subtree is all less than the node, and the right subtree is all greater
than the node; an in-order traversal will show if it is an increasing
sequence, using recursion or a stack</li>
<li>101: Determine if a binary tree is symmetric, using recursion, note
that the recursion is (left.left, right.right) and (left.right,
right.left), i.e., the inner and outer pairs of child nodes, and an
iterative version can also be implemented using a stack</li>
<li>106: Given the inorder and postorder traversals, find the preorder
traversal. The last node in the postorder traversal is the root node,
find this root node in the inorder traversal and remove it, the nodes to
the left of the root node in the inorder traversal are the left subtree,
and the nodes to the right are the right subtree; at this point, the
last node in the postorder traversal is the root node of the left
subtree, and recursion can be used.</li>
<li>107: Output the nodes of each level of the tree from bottom to top
and from left to right, recording the answers in a two-dimensional
array. If the one-dimensional length of the two-dimensional array, which
represents the level, is less than the current level of the node being
traversed, a new level is created to store the answers. It can be
implemented directly using depth-first search (DFS), or by replacing
recursion with a stack and a queue; if using a stack, it is DFS, and if
using a queue, it is BFS.</li>
<li>Construct a binary search tree, recursively</li>
<li>114: Give a tree, compress it by preorder traversal to become a tree
with only right nodes. The operation of compressing a node is defined
as, setting the left subtree to empty, making the right subtree the
original left subtree, and appending the original right subtree to the
end of the original left subtree, with the tail node of the original
left subtree being the last point reached during the preorder traversal
of this subtree. Recursively, first compress the left node, then
compress the right node, and then backtrack to compress the parent
node.</li>
<li>144: Obtain the preorder traversal of a tree using a stack, pop a
node to record its value, and then push the right node first, followed
by the left node</li>
<li>Find the maximum value at each level of a binary tree, BFS</li>
</ul>
<h2 id="graph-theory">Graph Theory</h2>
<ul>
<li>130: Water problem, replace all white pieces surrounded by black
pieces in a diagram with black pieces, directly find all the white
pieces on the edge and store them, then paint the entire board black and
restore the white pieces. The stored is the top solution, the writing
style is very pythonic.</li>
</ul>
<h2 id="math-problem">Math problem</h2>
<ul>
<li>Given two sorted sequences, find the median of the merged sequence
with a time complexity of O(log(m+n)), so it cannot be done by comparing
one by one. The significance of the median is to split the sequence into
two parts of equal length, where the smallest number in the larger part
is greater than the largest number in the smaller part. Based on this
principle, split the two sequences into two parts, with sequence 1 split
at position i and sequence 2 split at position j. It needs to be
ensured: 1: If the lengths are the same, then <span
class="math inline">\(i+j=m-i+n-j\)</span> ; 2: The union of the smaller
parts of the two sequences has any number smaller than the union of the
larger parts of the two sequences, because the split parts are still
ordered, so this condition is <span class="math inline">\(B[j-1] &lt;=
A[i] and A[i-1] &lt;= B[j]\)</span> . Once the position i is determined,
the position j is also determined, so it is only necessary to perform a
binary search to find the position i. Note the parity and boundary
conditions.</li>
<li>57: Given a set of intervals, insert a new interval and merge.
Simulation</li>
<li>122: Water, one line of code, find the maximum adjacent element
difference in a sequence</li>
<li>142: Given a linked list, find the starting point of the loop in the
list. It still uses the two-pointer technique, with one fast and one
slow pointer starting from the beginning of the list. When they meet for
the first time, it indicates that there is a loop and the difference in
their steps is the length of the loop. It can also be deduced that the
distance from the starting point of the list to the starting point of
the loop is equal to the distance from the meeting point to the starting
point of the loop, thus allowing the starting point of the loop to be
found.</li>
<li>166: Write the quotient and divisor, including the display of
repeating decimals, a mathematical analysis problem, continuously
multiply the decimal by 10 and divide by the divisor, update the
remainder, and when the remainder repeats, the decimal repeats</li>
<li>172: Find the number of zeros in n!, a problem in mathematical
analysis. The zeros come from 5*2, so it's about how many 5s and their
multiples are in n</li>
<li>202: Two-pointer technique, a method for finding a loop</li>
<li>263: Mathematical Problem</li>
<li>264: Mathematical Problem</li>
<li>313: Mathematical Problem</li>
</ul>
<h2 id="string">String</h2>
<ul>
<li>Split a string into palindromic substrings; using Python's [i::-1]
is very convenient and can be done in one line of code</li>
<li>242: Water topic, usage of Python dictionaries</li>
<li>Given a string, delete the minimum number of parentheses to make all
parentheses in the string match, and output all possible cases. Note two
points: do it in both normal and reverse order, as there are two schemes
for deleting left and right parentheses; output all cases, and store
them in a set.</li>
<li>451: Frequency statistics of letters, hash, Python dictionary</li>
<li>541: Partial string reversal, simulation, note the use of Python
slicing</li>
</ul>
<h2 id="greed">Greed</h2>
<ul>
<li>134: Gas stations are arranged in a circle, given the amount of fuel
each station can add and the fuel consumed between stations, determine
from which station one can complete a full circle. The data guarantees
that the answer is unique. Greedy approach: if it's not possible to
complete the circle with the first i stations, then set the starting
point to station i+1</li>
<li>402: Remove k digits from a large number to make the new number
smallest, stack, greedy</li>
<li>Overlap Interval Problem, Greedy</li>
</ul>
<h2 id="dynamic-programming">Dynamic Programming</h2>
<ul>
<li><p>Classic problem climbing ladder, Fibonacci sequence, dp</p></li>
<li><p>96: Given a sequence of numbers from 1 to n, ask how many
different BSTs can be constructed. Let ans(i) be the number of different
BSTs that can be constructed from a sequence of length i. We need to
find the relationship between ans(i) and ans(i-1). Introduce the
intermediate variable f(i,n), which represents the number of different
BSTs that can be constructed with the i-th number as the root and a
sequence of length n. Here, a recursive relationship can be found. The
left subsequence of the root constructs the left subtree of the root,
and there are left different left subtrees. The right subsequence right
is the same. Then f(i,n) should be left * right, i.e., f(i,n) = ans(i-1)
* ans(n-i). At the same time, with different i as the root, there will
be completely different partitions. Therefore, <span
class="math inline">\(ans(n)=\sum _{i=1}^n f(i,n)\)</span> , merging
gives ans(i) = ans(0) * ans(n-1) + ans(1) * ans(n-2) + ... + ans(n-1) *
ans(0), with boundary ans(0) = ans(1) = 1.</p></li>
<li><p>139: Provide a dictionary and a string, and determine if the
string can be completely composed of words from the dictionary. Define
f[i] as true if the first i characters of the string can be completely
composed of words. Then, traverse each word, with length k, if f[i-k] is
true, then f[i] is true. This can also be converted into a graph and
solved using depth-first search (DFS). An edge between i and j indicates
that s[i:j] is a word, and we need to find a path from 0 to
len(s).</p></li>
<li><p>174: The matrix contains positive and negative numbers, and the
minimum path is sought without the intermediate value being 0. Dynamic
programming, from the end to the beginning.</p></li>
<li><p>312: Given a sequence of numbers, ask how to sequentially
eliminate numbers to get the maximum number of coins. The rule for
getting coins is: eliminating a number yields the sum of the product of
this number and its two adjacent numbers in coins. Interval dp (divide
and conquer), the maximum number of coins that can be obtained in a
segment is f[x,y], which depends on the last elimination position at i,
the coins obtained being f[x,i-1] + num[i-1] * num[i] * num[i+1] +
f[i+1,y]. Enumerate the position i within this interval, where the
positions of the two subintervals on both sides are known, so the
interval is enumerated from small to large. There are three layers of
loops in total: the outer loop for the length of the interval, the
middle loop for the starting position of the interval in the entire
sequence, and the inner loop for enumerating the position i within the
interval.</p></li>
<li><p>Determine the number of 1s in the binary representation of each
number in [1, num], and by analyzing several numbers, it is found that
for even number n, its binary representation is the binary of n/2
shifted one bit to the left, with the number of 1s unchanged. For odd
number n, its binary representation is the binary of n/2 shifted one bit
to the left and a 1 added at the end, which means there is one more 1.
It is obvious that the state transition equation</p>
<p><span class="math display">\[
f(x)=
\begin{cases}
f(n) &amp; x=2n \\
f(n)+1 &amp; x=2n+1 \\
\end{cases}
\]</span></p></li>
<li><p>397: If a number is even, divide it by 2; if it is odd, change it
to the adjacent number. Ask how many times it takes to become 1.
According to the problem, the state transition equation can be written
as:</p>
<p><span class="math display">\[
f(x)=
\begin{cases}
f(n)+1 &amp; x=2n \\
min(f(2n+2)+1,f(2n)+1) &amp; x=2n+1 \\
\end{cases}
\]</span></p>
<p>Odd cases can be simplified to <span
class="math inline">\(min(f(2n)+1,f(n+1)+2)\)</span> , so it can be
solved by dynamic programming from 1 to n, but it may exceed time limit.
The equation can be further simplified: If n % 4 = 3 and n != 3, then
f(n) = f(n + 1) + 1. If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.
Proof is here.</p></li>
<li><p>472: It is a variant of 139, still determining whether a string
can be composed of words, but both the words and the string to be judged
are in a dictionary. Each word needs to be checked if it can be
completely composed of other words. Since there are no repeated words in
the dictionary, the words are first sorted by length, and each word can
only be composed of the words that come before it. The problem then
transforms into 139, where the ith word is the string to be queried, and
the first i-1 words form the dictionary required for the query. Dynamic
programming is applied in the same way. The top solution utilizes a trie
dictionary tree to accelerate, see the implementation of the dictionary
tree in Python.</p></li>
</ul>
<h2 id="divide-and-conquer">Divide and Conquer</h2>
<ul>
<li>247: Provide an expression without parentheses, and ask how many
possible solutions there are when parentheses are added (without
requiring duplicates), divide and conquer, take the i-th operation
symbol, and recursively solve the partial solutions on the left and
right sides of the symbol, usage of map in Python</li>
</ul>
<h1 id="poj">Poj</h1>
<ul>
<li><a href="https://github.com/thinkwee/Poj_Test">C++ source
code</a></li>
<li>Dijkstra</li>
<li>Simulation</li>
<li>1094: Topological Sorting</li>
<li>1328: Greedy, switch to solve variables</li>
<li>1753: Enumeration, Bitwise Operations</li>
<li>1789: Prim, priority queue</li>
<li>1860: Bellman-Ford</li>
<li>2109: Greedy, High-Precision Multiplication</li>
<li>2965: Enumeration, Bitwise Operations</li>
<li>Modeling, Bellman-Ford</li>
<li>3295: Simulation, Stack</li>
</ul>
<h1 id="intra-school-competition">Intra-school competition</h1>
<ul>
<li><a href="http://code.bupt.edu.cn/contest/650/">2017pre</a></li>
<li>Finding three ordered numbers in a set that sum to 0, and then
calculating the sum of their squares, original problem from
LeetCode</li>
<li>D, Find the sum of consecutive prime numbers and factorization, POJ
original problem, data is large, so there is no need to use a table, and
directly judge each input</li>
<li>F, the title is misleading, no backtracking is needed, the
characteristic equation can be written and solved using the recursive
formula</li>
<li>Find the largest subset of a set with binding rules, where selecting
a number requires selecting several other numbers, bfs</li>
<li>H, given the character transformation rules and cost, find the
minimum cost to make two strings identical, flyod, note that two letters
can be transformed into a third letter, not necessarily mutually, and
the two strings may not be of equal length; if they are not of equal
length, output -1 directly</li>
<li>I, in high school physics, seek the acceleration due to gravity, the
equation is difficult to solve, use bisection method to approximate the
answer</li>
</ul>
<h1 id="hiho">hiho</h1>
<ul>
<li>hiho</li>
<li>1505: Given a set, ask how many index quadruples (i, j, p, q)
satisfy that i, j, p, q are all different, and i &lt; j, p &lt; q, Ai +
Aj = Ap + Aq. The 2sum problem, with a small data range, can directly
open a hash array, where sum[i] records how many groups of two numbers
sum to i, and use the principle of inclusion-exclusion.</li>
<li>1506: The probability of getting heads m times out of n coin tosses,
with different probabilities for each toss to land heads up. Dynamic
programming, let dp[i][j] be the probability of getting heads up j times
after i tosses, with the state transition equation: dp[i][j] =
dp[i-1][j-1] * a[i] + dp[i-1][j]; a[i] is the probability of getting
heads up on the i-th toss, and special treatment is required for
j=0.</li>
<li>1507: Incorrect record, given a tree, given the root node number,
add an error edge in the middle, and find all possible error edges. Note
that since only one error edge is added, the situation can be divided
into two cases: 1. This error edge connects to the root node, output
directly. 2. If the root node is normal, then this error edge must be
connected to a node with an in-degree of 2, and there is only one such
node. Find this node, remove the two edges connected to it, and start a
depth-first search (dfs) from the root node. If the dfs traversal count
is n, it means that the tree still holds after removing this edge, which
is the error edge. If the count is less than n, it means that the tree
is broken and forms a forest after removing this edge, and this edge is
not an error edge. Note the case of repeated edges, at this time only
one edge is removed, but both edges are output as error edges
separately. Since the number of error edges is less than or equal to 2,
the dfs count is less than or equal to 2, and the time complexity,
including the construction of the graph with removed edges, is
O(n).</li>
<li>1515: Given some score relationships between classmates (how many
points higher or lower A is compared to B), answer q queries in the end.
With weighted union-find, each set maintains the score difference
between classmates and a certain root student in this set. Each time a
relationship is input, merge the union-find sets of the two classmates,
and update each union-find set once, where y is merged into x, the
relationship value between x and y is s, and the update formula is
d[root of y] = d[x] - d[y] - s. In the next iteration, update the values
of the entire union-find set. Finally, perform the direct query.</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="leetcode">Leetcode</h1>
<ul>
<li><a
href="https://leetcode.com/problemset/algorithms/">Leetcode算法列表</a></li>
<li>直接搜每道题的序号即可</li>
</ul>
<h2 id="排序">排序</h2>
<ul>
<li>75:给出一个只包含0，1，2的数列，排序形成[0,0,...,0,1,...,1,2,...,2]的形式，在原数列上修改。借鉴Lomuto
partition
algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。</li>
<li>164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max
- min) / gap +
1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。</li>
<li>179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序</li>
</ul>
<h2 id="堆栈队列">堆、栈、队列</h2>
<ul>
<li>23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中</li>
<li>150:算术表达式，栈，python的switch写法</li>
<li>224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1</li>
<li>373:在2对有序数组中找出有最小和的k组数对，优先队列</li>
</ul>
<h2 id="组合数学">组合数学</h2>
<ul>
<li>47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top
Solution中用插入处后一个数字是否重复来排重</li>
</ul>
<h2 id="搜索与查找">搜索与查找</h2>
<ul>
<li>74:在一个有有特定顺序的矩阵中查找，两次二分查找</li>
<li>78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归</li>
<li>79:在一个字母矩阵中查找单词，深搜</li>
<li>89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称</li>
<li>140:将一个字符串按给定词典加空格成句子，记忆化搜索</li>
<li>153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找</li>
<li>154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个</li>
<li>240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法</li>
</ul>
<h2 id="树">树</h2>
<ul>
<li>98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈</li>
<li>101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本</li>
<li>106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可</li>
<li>107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs</li>
<li>108:构造一棵二叉查找树，递归</li>
<li>114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点</li>
<li>144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点</li>
<li>515:求二叉树每一层的最大值，BFS</li>
</ul>
<h2 id="图论">图论</h2>
<ul>
<li>130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top
solution，写法很pythonic</li>
</ul>
<h2 id="数学题">数学题</h2>
<ul>
<li>4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则<span
class="math inline">\(i+j=m-i+n-j\)</span>；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是<span
class="math inline">\(B[j-1] &lt;= A[i] and A[i-1] &lt;=
B[j]\)</span>，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。</li>
<li>57：给出一组区间，插入一个新区间并合并。模拟</li>
<li>122:水，一行代码，找一个序列中的最大相邻元素差</li>
<li>142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点</li>
<li>166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复</li>
<li>172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数</li>
<li>202:快慢指针，求循环的一种方法</li>
<li>263:数学题</li>
<li>264:数学题</li>
<li>313:数学题</li>
</ul>
<h2 id="字符串">字符串</h2>
<ul>
<li>131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码</li>
<li>242:水题，python字典的使用</li>
<li>301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况</li>
<li>451:按频率统计字母，哈希，python字典</li>
<li>541:部分字符串翻转，模拟，注意python切片的使用</li>
</ul>
<h2 id="贪心">贪心</h2>
<ul>
<li>134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1</li>
<li>402:从一个大数中移除k个数字，使得新的数最小，栈，贪心</li>
<li>452:重叠区间问题，贪心</li>
</ul>
<h2 id="动态规划">动态规划</h2>
<ul>
<li><p>70:经典问题爬梯子，斐波那契数列，dp</p></li>
<li><p>96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left*right，即f(i,n)=ans(i-1)*ans(n-i)，同时以不同的i为根会有完全不同的划分，则<span
class="math inline">\(ans(n)=\sum _{i=1}^n
f(i,n)\)</span>，合并可以得到ans(i)=ans(0) * ans(n-1) + ans(1) *
ans(n-2) + … + ans(n-1) * ans(0)，边界ans(0)=ans(1)=1。</p></li>
<li><p>139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)</p></li>
<li><p>174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前</p></li>
<li><p>312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]*num[i]*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。</p></li>
<li><p>338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程</p>
<p><span class="math display">\[
f(x)=
\begin{cases}
f(n) &amp; x=2n \\
f(n)+1 &amp; x=2n+1 \\
\end{cases}
\]</span></p></li>
<li><p>397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:</p>
<p><span class="math display">\[
f(x)=
\begin{cases}
f(n)+1 &amp; x=2n \\
min(f(2n+2)+1,f(2n)+1) &amp; x=2n+1 \\
\end{cases}
\]</span></p>
<p>奇数情况可以化简为<span
class="math inline">\(min(f(2n)+1,f(n+1)+2)\)</span>，这样就可以从1到n动规了，但是会超时
可以将方程进一步化简 If n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.
If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1. <a
href="https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof">证明在这里</a><br />
</p></li>
<li><p>472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top
solution中利用了trie字典树加速，见<a
href="http://thinkwee.top/2017/05/02/trie/#more">Python中实现字典树</a></p></li>
</ul>
<h2 id="分治">分治</h2>
<ul>
<li>247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法</li>
</ul>
<h1 id="poj">Poj</h1>
<ul>
<li><a href="https://github.com/thinkwee/Poj_Test">C++源码</a></li>
<li>1062:Dijkstra</li>
<li>1068:模拟</li>
<li>1094:拓扑排序</li>
<li>1328:贪心，换求解变量</li>
<li>1753:枚举，位运算</li>
<li>1789:Prim，优先队列</li>
<li>1860:bellman-ford</li>
<li>2109:贪心，高精度乘法</li>
<li>2965:枚举，位运算</li>
<li>3259:建模，bellman-ford</li>
<li>3295:模拟，栈</li>
</ul>
<h1 id="校内赛">校内赛</h1>
<ul>
<li><a href="http://code.bupt.edu.cn/contest/650/">2017pre</a></li>
<li>A,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题</li>
<li>D,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断</li>
<li>F,题目骗人，不需要回溯，递推公式可以写出特征方程求解</li>
<li>G,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs</li>
<li>H,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1</li>
<li>I,高中物理，求重力加速度，方程不好解，二分法逼近答案</li>
</ul>
<h1 id="hiho">hiho</h1>
<ul>
<li>hiho</li>
<li>1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p,
q两两不同，并且i &lt; j, p &lt; q, Ai + Aj = Ap +
Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。</li>
<li>1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1][j](1.0-a[i]);a[i]为第i次正面朝上的概率，注意对j=0进行特判。</li>
<li>1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。</li>
<li>1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root
of
y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>python</tag>
        <tag>c++</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Seq2seq based Summarization</title>
    <url>/2018/07/04/seq2seq-summarization/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/2f67b4f7fae34b1e08aa1c7cc17df6b8.png" width="500"/></p>
<p>A bachelor's graduation project involves developing a short sentence
summarization model based on seq2seq and designing an emotional fusion
mechanism. Now, let's provide a brief summary of the entire model</p>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TGHH.png" alt="i0TGHH.png" />
<figcaption aria-hidden="true">i0TGHH.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="task">Task</h1>
<ul>
<li>Automatic text summarization is a type of natural language
processing (NLP) task. For a longer text, it generates a short text that
covers the core meaning of the original text, which is the summary of
the original text. Automatic summarization technology refers to
constructing mathematical models on computers, inputting long texts into
the model, and then automatically generating short summaries through
computation. According to the scale of the corpus needed for generating
summaries and the scale of the summaries, summaries can be divided into
multi-document summaries, long document summaries, and short document
summaries. This paper mainly studies short document summarization: for a
sentence or a few sentences of text, generate a short summary that
summarizes the key information of the original text, and is fluent and
readable, trying to reach the level of summaries written by human
authors.</li>
<li>Automatic text summarization is divided into extraction-based and
generation-based methods, the former being the extraction of original
sentences to form the summary, and the latter being the generation of
the summary through a deep learning model, character by character. This
paper mainly focuses on the generation of summaries and abstracts the
problem into generating a short sentence of an average length of 8 words
from a long sentence with an average length of 28 words.</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TYEd.jpg" alt="i0TYEd.jpg" />
<figcaption aria-hidden="true">i0TYEd.jpg</figcaption>
</figure>
<h1 id="preparatory-knowledge">Preparatory Knowledge</h1>
<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>
<ul>
<li><p>Recurrent Neural Network (RNN), a variant of neural networks, is
capable of effectively processing sequential data. All its hidden layers
share parameters, with each hidden layer not only depending on the
current moment's input but also on the state of the previous hidden
layer. The data flow is not propagated between network layers as in
traditional neural networks, but rather circulates as a state within its
own network.</p></li>
<li><figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TtUA.jpg" alt="i0TtUA.jpg" />
<figcaption aria-hidden="true">i0TtUA.jpg</figcaption>
</figure></li>
<li><p>After time step expansion: <img data-src="https://s1.ax1x.com/2018/10/20/i0TN4I.jpg"
alt="i0TN4I.jpg" /></p></li>
</ul>
<h2 id="lstm-and-gru">LSTM and GRU</h2>
<ul>
<li><p>Recurrent neural networks can effectively capture the sequential
information of sequence data and can construct very deep neural networks
without generating a large number of parameters to be learned; however,
due to parameter sharing, when gradients are chain-derived through time
steps, it is equivalent to matrix power operations. If the eigenvalues
of the parameter matrix are too small, it will cause gradient diffusion,
and if the eigenvalues are too large, it will cause gradient explosion,
affecting the backpropagation process, i.e., the long-term dependency
problem of RNNs. When dealing with long sequence data, the long-term
dependency problem can lead to the loss of long-term memory information
in the sequence. Although people have tried to alleviate this problem by
introducing gradient truncation and skip connection techniques, the
effect is not significant until the long short-term memory neural
networks and gated recurrent neural networks, as extended forms of RNNs,
effectively solve this problem.</p></li>
<li><p>LSTM stands for Long Short-Term Memory, a type of neural network
that introduces gate units in its nodes to capture long-term memory
information. These gate units, as part of the network parameters,
participate in training and control the extent to which the current
hidden layer node memory (forgetting) past information and accepts new
memory input. <img data-src="https://s1.ax1x.com/2018/10/20/i0TaCt.jpg"
alt="i0TaCt.jpg" /></p></li>
<li><p>GRU stands for Gated Recurrent Unit, which differs from LSTM in
that GRU integrates the forget gate and input gate into a reset gate,
with the control values of the forget gate and input gate summing to 1.
Therefore, GRU simplifies the parameters on the basis of LSTM, allowing
the model to converge faster. <img data-src="https://s1.ax1x.com/2018/10/20/i0Td8P.jpg" alt="i0Td8P.jpg" /></p>
<h2 id="word-embedding">Word Embedding</h2></li>
<li><p>One of the major advantages of deep learning is its ability to
automatically learn features. In natural language processing, we
specifically use techniques like word2vec to learn the feature
representations of words, i.e., word embeddings.</p></li>
<li><p>Word vectors, also known as word embeddings, represent words in
the form of continuous value vectors (Distributed Representation),
rather than using a discrete method (One-hot Representation). In
traditional discrete representation, a word is represented by a vector
of length V, where V is the size of the dictionary. Only one element in
the vector is 1, with the rest being 0, and the position of the 1
indicates the word's index in the dictionary. Storing words in a
discrete manner is inefficient, and the vectors cannot reflect the
semantic and grammatical features of words, whereas word vectors can
address these issues. Word vectors reduce the dimension of the vector
from V to <span class="math inline">\(\sqrt[k] V\)</span> (usually k
takes 4), with the values of each element no longer being 1 and 0, but
continuous values. Word vectors are a byproduct of supervised learning
obtained from the Neural Network Language Model (NNLM) for corpus, and
the proposal of this model is based on a linguistic assumption: words
with similar semantics have similar contexts, that is, the NNLM model
can determine the corresponding central word under the given
context.</p></li>
<li><p>The figure below illustrates the skipgram model in word2vec: <img data-src="https://s1.ax1x.com/2018/10/20/i0Twgf.jpg"
alt="i0Twgf.jpg" /></p></li>
<li><p>The obtained word embedding matrix is as follows: <img data-src="https://s1.ax1x.com/2018/10/20/i0T0v8.jpg"
alt="i0T0v8.jpg" /></p></li>
<li><p>Mikolov et al. proposed the Word2Vec model based on NNLM, where
the input and output for supervised learning are respectively the center
word and its context (i.e., the Skip Gram model) or the context and the
center word (i.e., the CBOW model). Both methods can effectively train
high-quality word vectors, but the CBOW model calculates the center word
based on the context, has a fast training speed, and is suitable for
training on large corpora; the Skip Gram model can fully utilize the
training corpus, and its meaning is "jumping grammar model." It not only
uses adjacent words to form the context of the center word but also uses
words that are one word apart as part of the context. As shown in Figure
2-1, the context of a center word in the corpus includes four words. If
there are Wtotal words in the corpus, the Skip Gram model can calculate
4 · Wtotal times of loss and perform backpropagation learning, which is
four times the number of learning times for the corpus compared to the
CBOW model, so this model is suitable for fully utilizing small corpora
to train word vectors.</p></li>
<li><p>Word2Vec model training is completed, and the weight matrix
between the input layer and the hidden layer is the Word Embedding
Matrix. Multiplying the vector representing the discrete word with the
matrix yields the word vector, which is actually equivalent to looking
up the corresponding word vector (Embedding Look Up) in the word
embedding matrix. Word vectors can effectively represent the semantic
relationships between words, essentially providing a method for machine
learning models to extract text information features, facilitating the
numerical input of words into the model for processing. Traditional
language model training of word vectors incurs too much overhead in the
output Softmax layer.</p></li>
<li><p>The Word2Vec model employs both Hierarchical Softmax and Noise
Contrastive Estimation techniques, significantly accelerating the
training process, making it possible to train high-quality word vectors
with large-scale corpus in natural language processing.</p></li>
</ul>
<h2 id="attention">Attention</h2>
<ul>
<li>In NLP tasks, the attention mechanism was first applied to machine
translation, where a weight matrix is introduced to represent the
contribution degree of each element in the encoder sequence to each word
generated by the decoder. In practical implementation, the attention
mechanism generates an attention weight, which performs
attention-weighted generation of intermediate representations for the
hidden layer states of various encoder elements, rather than simply
using the hidden layer state of the last element. The simplest attention
is the decoder's attention to the encoder, which is divided into global
and local attention. Global attention generates attention weights for
the entire encoder sequence, while local attention first trains an
attention alignment position and then takes a window around this
position, weighting only the sequence within the window, making the
attention more precise and focused. One byproduct of the attention
mechanism is the alignment between words (Alignment). In machine
translation, the alignment relationship can be understood as the degree
of association between words and their translations. In automatic
summarization, the application of the attention mechanism can
effectively alleviate the problem of information loss when long
sequences are encoded into intermediate representations by the encoder.
<img data-src="https://s1.ax1x.com/2018/10/20/i0TDKS.jpg"
alt="i0TDKS.jpg" /></li>
</ul>
<h2 id="sequence-to-sequence">Sequence to sequence</h2>
<ul>
<li>seq2seq model, which uses an RNN as an encoder to encode the input
sequence data into intermediate semantic representation, and then
utilizes another RNN as a decoder to obtain the serialized output from
the intermediate semantic representation. Generalized
sequence-to-sequence and end-to-end learning may not necessarily use
RNNs; CNNs or pure attention mechanisms can also be used. <img data-src="https://s1.ax1x.com/2018/10/20/i0TrDg.jpg" alt="i0TrDg.jpg" /></li>
<li>Some personal understanding of sequence-to-sequence models:
<ul>
<li>(Information Theory) If the specific implementation forms of the
encoder and decoder are not considered, and it is only assumed that the
encoder can convert sequence data into an intermediate representation,
and the decoder can convert the intermediate representation back into
sequence data, then the entire sequence-to-sequence model is equivalent
to one round of encoding and decoding of abstract information. Since the
dimension of the intermediate representation is much smaller than the
total dimension of the encoder, this encoding is lossy. The
sequence-to-sequence model aims to make the result of the lossy encoding
extract the abstract information from the original text, so the goal of
training the network is to let the loss part be the redundant
information that is not needed in the abstract. The decoder is
equivalent to the inverse process of the encoder, restoring the abstract
sequence data from the intermediate representation that contains the
abstract information.</li>
<li>(Study and Application) If the entire model is likened to the human
brain, then the encoder's hidden layer states are equivalent to the
knowledge stored within the brain, while the decoder utilizes this
knowledge to solve problems, that is, the encoder is a learning process
and the decoder is an application process. This analogy can vividly
explain various subsequent improvements to the sequence-to-sequence
model: learning at time steps corresponds to learning on the real
timeline, with earlier learned knowledge being more easily forgotten in
the brain (hidden layer information from earlier time steps is severely
lost by the final step), and the brain's capacity being finite (the
information storage capacity of the intermediate representation is
fixed). Therefore, during learning, we selectively remember and forget
information (application of LSTM and GRU), and even highlight key points
in the process of memorization (attention mechanism), and can also rely
on querying information from the external environment to solve problems
without entirely depending on one's own memory (memory networks).</li>
<li>(Circular Recurrent Neural Network) From the perspective of data
flow and network structure, the entire sequence-to-sequence model can be
regarded as a long RNN with certain time steps limited for input and
output, as illustrated in Figure 3-1, the model is an RNN with 8 time
steps, where the first 5 time steps have no output, and the last 3 time
steps pass the output to the next time step to guide state changes.
Joint training of the encoder and decoder is equivalent to training this
large RNN. This RNN, which only performs input and output at some nodes,
is structurally suitable for handling sequence-to-sequence tasks.</li>
</ul></li>
</ul>
<h2 id="sequence-loss">Sequence Loss</h2>
<ul>
<li>The decoder outputs the probability distribution of the dictionary
at each step, selecting the word with the highest probability (or
performing 束搜索), with the loss being the cross-entropy between the
probability at each step and the 01 distribution of the standard word,
summed and averaged. In practice, a mask is also applied to address the
issue of varying sentence lengths.</li>
</ul>
<h1 id="basic-model">Basic Model</h1>
<ul>
<li>Preprocessing: For the application of sequence-to-sequence models,
certain preprocessing of the data is required, in addition to the
commonly used stop word removal and UNK replacement, as well as padding.
It also requires the design of start and end symbols for decoding, as
follows: <img data-src="https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg"
alt="i0TsbQ.jpg" /></li>
<li>After training word embeddings, perform an embedding lookup
operation on the input to obtain features <img data-src="https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg" alt="i0T6Ej.jpg" /></li>
<li>Feature input encoder receives intermediate representation <img data-src="https://s1.ax1x.com/2018/10/20/i0Tg5n.png" alt="i0Tg5n.png" /></li>
<li>Obtain the intermediate representation and output the abstract
(equivalent to label), input the decoder for decoding <img data-src="https://s1.ax1x.com/2018/10/20/i0TIrF.png" alt="i0TIrF.png" /></li>
<li>The complete sequence-to-sequence model structure after
incorporating the attention mechanism is as follows: <img data-src="https://s1.ax1x.com/2018/10/20/i0TRCq.jpg" alt="i0TRCq.jpg" /></li>
</ul>
<h1 id="emotional-fusion-mechanism">Emotional fusion mechanism</h1>
<ul>
<li>The emotional mechanism primarily supplements the emotional features
of text, manually constructing a six-dimensional feature through the
search of an emotional dictionary, and it is hoped in the future to
carry out this work by automatically constructing features.</li>
<li>Firstly, train an emotion classifier, filter the original corpus to
form an emotional corpus, and test the model on both the emotional
corpus and the general corpus <img data-src="https://s1.ax1x.com/2018/10/20/i0TW80.jpg" alt="i0TW80.jpg" /></li>
<li>Obtain sentiment vectors (i.e., sentiment features) from a
dictionary <img data-src="https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg"
alt="i0Tf2V.jpg" /></li>
<li>Directly concatenate the emotional features after the intermediate
representation, input decoder <img data-src="https://s1.ax1x.com/2018/10/20/i0ThvT.jpg" alt="i0ThvT.jpg" /></li>
</ul>
<h1 id="results">Results</h1>
<ul>
<li>Results are recorded in the form of ROUGE-F1 values, a comparison of
various methods under sentiment corpus <img data-src="https://s1.ax1x.com/2018/10/20/i0T5KU.png" alt="i0T5KU.png" /></li>
<li>Comparative Study of Sentiment Fusion Schemes under Common Corpus
<img data-src="https://s1.ax1x.com/2018/10/20/i0Tb5R.png"
alt="i0Tb5R.png" /></li>
<li>Emotional classification accuracy, as a reference, the previously
trained emotional classifier accuracy was 74% <img data-src="https://s1.ax1x.com/2018/10/20/i0Tob4.png" alt="i0Tob4.png" /></li>
<li>Because it is large corpus with small batch training, only ten
iterations were trained, and the effect of the test set in each
iteration is <img data-src="https://s1.ax1x.com/2018/10/20/i0T7VJ.png"
alt="i0T7VJ.png" /></li>
</ul>
<h1 id="problem">Problem</h1>
<ul>
<li>Problem of unknown replacement: Many literature mentions the use of
pointer switch technology to solve the rare words (unk) in generated
abstracts, that is, selecting words from the original text to replace
the unk in the abstract. However, since ROUGE evaluates the
co-occurrence degree of words, even if the words from the original text
are replaced, regardless of the position or word accuracy, it may result
in an increase in ROUGE value, causing the evaluation results to be
overestimated. This paper designs comparative experiments and finds that
even random replacements without any mechanism can improve the ROUGE
value</li>
<li>Corpus Repetition Issue: During the examination of the corpus, we
found a large number of short texts that are different but have the same
abstracts. The repeated corpus comes from different descriptions of the
same event or some functional text, such as "...... Wednesday...... Gold
prices rose" and "...... Thursday...... Gold prices rose" both
generating the same abstract "Gold prices rose." The repetition of short
abstracts can cause the model to learn some phrases that should not be
solidified at the decoding end. Moreover, if there are repeated
identical abstracts in the training set and test set, the solidified
abstracts can lead to an artificially high accuracy of generated
abstracts. For such texts, this paper conducted four groups of
experiments:
<ul>
<li>Without deduplication: Retain the original text, do not deduplicate,
and perform training and testing</li>
<li>De-duplication: Remove all short texts corresponding to duplicate
abstracts from the corpus.</li>
<li>Training De-duplication: Partial de-duplication, only removing
duplicate data from the training corpus, meaning that the well-trained
model is not affected by duplicate text.</li>
<li>Test deduplication: Partial deduplication, only removing the parts
of the test set that are duplicated in the training set. Good learning
models were affected by duplicate texts, but there was no corresponding
data in the test set for the duplicate texts. Under duplicate corpus
training, both ROUGE-1 and ROUGE-L exceeded 30, far beyond the normal
training models, and after deduplication, they returned to normal
levels. The results of the two partial deduplication methods indicate:
when training deduplication, models not affected by duplicate corpus did
not show significant reactions to the duplicate data in the test set,
approximating the normal models with complete deduplication; when
testing deduplication, although the model was affected by duplicate
corpus, there was no duplicate data in the test set for the model to
utilize the learned fixed abstracts, so the results would not be overly
high. Moreover, due to learning the pattern of the same abstract
corresponding to different short texts, the encoding end actually has a
more flexible structure, leading to ROUGE scores higher than those of
training deduplication. <img data-src="https://s1.ax1x.com/2018/10/20/i0TLP1.png" alt="i0TLP1.png" /></li>
</ul></li>
</ul>
<h1 id="environmental-implementation">Environmental Implementation</h1>
<ul>
<li>Here is the GitHub address: - Abstract_Summarization_Tensorflow</li>
<li>Ubuntu 16.04</li>
<li>Tensorflow 1.6</li>
<li>CUDA 9.0</li>
<li>Cudnn 7.1.2</li>
<li>Gigawords dataset, trained on part of the data, approximately
300,000</li>
<li>GTX1066, training time 3 to 4 hours</li>
</ul>
<h1 id="references">References</h1>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0TX26.jpg"
alt="i0TX26.jpg" /> <img data-src="https://s1.ax1x.com/2018/10/20/i0TO8x.jpg"
alt="i0TO8x.jpg" /></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="任务">任务</h1>
<ul>
<li>自动文摘是一类自然语言处理（Natural Language
Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。</li>
<li>自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TYEd.jpg" alt="i0TYEd.jpg" />
<figcaption aria-hidden="true">i0TYEd.jpg</figcaption>
</figure>
<h1 id="预备知识">预备知识</h1>
<h2 id="循环神经网络">循环神经网络</h2>
<ul>
<li>循环神经网络（Recurrent Neural
Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。</li>
<li>不展开时形式如下： <img data-src="https://s1.ax1x.com/2018/10/20/i0TtUA.jpg" alt="i0TtUA.jpg" /></li>
<li>按时间步展开之后： <img data-src="https://s1.ax1x.com/2018/10/20/i0TN4I.jpg" alt="i0TN4I.jpg" /></li>
</ul>
<h2 id="lstm和gru">LSTM和GRU</h2>
<ul>
<li><p>循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为
RNN 的扩展形式的出现，有效地解决了这个问题。</p></li>
<li><p>LSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。
<img data-src="https://s1.ax1x.com/2018/10/20/i0TaCt.jpg"
alt="i0TaCt.jpg" /></p></li>
<li><p>GRU即门控神经网络，与LSTM不同的是，GRU
将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此
GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。 <img data-src="https://s1.ax1x.com/2018/10/20/i0Td8P.jpg" alt="i0Td8P.jpg" /></p>
<h2 id="词嵌入">词嵌入</h2></li>
<li><p>深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。</p></li>
<li><p>词向量（Word Vector）表示，又称词嵌入（Word
Embedding），指以连续值向量形式表示（Distributed
Representation）词语，而不是用离散的方式表示（One-hot
Representation）。在传统的离散表示中，一个词用一个长度为 V
的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为
0，元素
1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维
度从 V 降低到 <span class="math inline">\(\sqrt[k] V\)</span>（一般 k 取
4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型
(Neural Network Language
Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即
NNLM 模型能够在给定上下文环境的条件下求出相应中心词。</p></li>
<li><p>下图展示了word2vec中的skipgram模型： <img data-src="https://s1.ax1x.com/2018/10/20/i0Twgf.jpg"
alt="i0Twgf.jpg" /></p></li>
<li><p>得到的词嵌入矩阵如下： <img data-src="https://s1.ax1x.com/2018/10/20/i0T0v8.jpg"
alt="i0T0v8.jpg" /></p></li>
<li><p>Mikolov等人在NNLM基础上提出了 Word2Vec
模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram
模型) 或者上下文与中心词（即 CBOW
模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW
模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram
模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图
2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal
个词语，则 Skip Gram 模型能计算 4 · Wtotal
次损失并进行反向传播学习，对于语料的学习次数是 CBOW
模型的四倍，因此该模型适合于充分利用小语料训练词向量。</p></li>
<li><p>Word2Vec
模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding
Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding
Look
Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出
Softmax 层开销太大，</p></li>
<li><p>而 Word2Vec 模型采用了分层 Softmax（Hierarchical
Softmax）和噪声对比估计（Noise Contrastive
Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。</p></li>
</ul>
<h2 id="注意力">注意力</h2>
<ul>
<li>在 NLP
任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。
<img data-src="https://s1.ax1x.com/2018/10/20/i0TDKS.jpg"
alt="i0TDKS.jpg" /></li>
</ul>
<h2 id="序列到序列">序列到序列</h2>
<ul>
<li>seq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。
<img data-src="https://s1.ax1x.com/2018/10/20/i0TrDg.jpg"
alt="i0TrDg.jpg" /></li>
<li>对于序列到序列模型的一些个人理解：
<ul>
<li>（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。</li>
<li>（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM
和 GRU
的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。</li>
<li>（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的
RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5
时间步没有输出，后 3
个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN
进行训练。这种只在部分节点进行输入输出的 RNN
从结构上就适合处理序列到序列任务。</li>
</ul></li>
</ul>
<h2 id="序列损失">序列损失</h2>
<ul>
<li>解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。</li>
</ul>
<h1 id="基本模型">基本模型</h1>
<ul>
<li>预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：
<img data-src="https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg"
alt="i0TsbQ.jpg" /></li>
<li>训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征 <img data-src="https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg" alt="i0T6Ej.jpg" /></li>
<li>特征输入编码器得到中间表示 <img data-src="https://s1.ax1x.com/2018/10/20/i0Tg5n.png" alt="i0Tg5n.png" /></li>
<li>拿到中间表示和输出文摘(相当于label)，输入解码器进行解码 <img data-src="https://s1.ax1x.com/2018/10/20/i0TIrF.png" alt="i0TIrF.png" /></li>
<li>加入注意力机制后完整的序列到序列模型结构如下： <img data-src="https://s1.ax1x.com/2018/10/20/i0TRCq.jpg" alt="i0TRCq.jpg" /></li>
</ul>
<h1 id="情感融合机制">情感融合机制</h1>
<ul>
<li>情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。</li>
<li>先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型
<img data-src="https://s1.ax1x.com/2018/10/20/i0TW80.jpg"
alt="i0TW80.jpg" /></li>
<li>查找词典得到情感向量（即情感特征） <img data-src="https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg" alt="i0Tf2V.jpg" /></li>
<li>将情感特征直接拼接在中间表示之后，输入解码器 <img data-src="https://s1.ax1x.com/2018/10/20/i0ThvT.jpg" alt="i0ThvT.jpg" /></li>
</ul>
<h1 id="结果">结果</h1>
<ul>
<li>结果由ROUGE-F1值形式记录，情感语料下各种方法对比 <img data-src="https://s1.ax1x.com/2018/10/20/i0T5KU.png" alt="i0T5KU.png" /></li>
<li>普通语料下情感融合方案对比 <img data-src="https://s1.ax1x.com/2018/10/20/i0Tb5R.png" alt="i0Tb5R.png" /></li>
<li>情感分类准确率，作为参考，之前训练的情感分类器准确率为74% <img data-src="https://s1.ax1x.com/2018/10/20/i0Tob4.png" alt="i0Tob4.png" /></li>
<li>因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果 <img data-src="https://s1.ax1x.com/2018/10/20/i0T7VJ.png" alt="i0T7VJ.png" /></li>
</ul>
<h1 id="问题">问题</h1>
<ul>
<li>unk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值
<img data-src="https://s1.ax1x.com/2018/10/20/i0THa9.png"
alt="i0THa9.png" /></li>
<li>语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“......
周三......
黄金价格上涨”和“......周四......黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：
<ul>
<li>不去重：保留原文本，不去重，进行训练和测试</li>
<li>去重：删除语料中所有重复文摘对应的的短文本-短文摘对。</li>
<li>训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。</li>
<li>测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。
重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了
30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致
ROUGE 指标高于训练去重的结果。 <img data-src="https://s1.ax1x.com/2018/10/20/i0TLP1.png" alt="i0TLP1.png" /></li>
</ul></li>
</ul>
<h1 id="实现环境">实现环境</h1>
<ul>
<li>这里是github地址：- <a
href="https://github.com/thinkwee/Abstract_Summarization_Tensorflow">Abstract_Summarization_Tensorflow</a></li>
<li>Ubuntu 16.04</li>
<li>Tensorflow 1.6</li>
<li>CUDA 9.0</li>
<li>Cudnn 7.1.2</li>
<li>Gigawords数据集，训练了部分数据，约30万</li>
<li>GTX1066，训练时间3到4个小时</li>
</ul>
<h1 id="参考文献">参考文献</h1>
<p><img data-src="https://s1.ax1x.com/2018/10/20/i0TX26.jpg"
alt="i0TX26.jpg" /> <img data-src="https://s1.ax1x.com/2018/10/20/i0TO8x.jpg"
alt="i0TO8x.jpg" /></p>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>abstractive summarization</tag>
        <tag>nlp</tag>
        <tag>seq2seq</tag>
        <tag>rnn</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title>Structured Neural Summarization, Paper Reading</title>
    <url>/2020/02/28/structured-summarization/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/325846e40f5450b0be19b6dd4c59bd38.png" width="500"/></p>
<p>reading note for STRUCTURED NEURAL SUMMARIZATION.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="intuition">Intuition</h1>
<ul>
<li>A natural idea of introducing GNN into seq2seq is to first obtain
token representations using a sequence encoder, then construct
relationships between tokens, build a graph, and feed the graph and
token representations into a GNN to obtain new token
representations.</li>
<li>The problem is that for summarization, the intuitive approach would
be to use sentences as nodes and construct a relationship graph between
sentences. This would result in sentence-level representations, lacking
word-level granularity, which would make it difficult to implement an
attention-based decoder.</li>
<li>The authors' approach is quite straightforward: directly create a
word-level GNN, where a document with 900 words becomes a graph with 900
nodes. The edge relationships are heterogeneous, consisting of three
types:
<ul>
<li>All words in a sentence connect to an additional sentence node, and
all words in an entity connect to an additional entity node. These edges
are called IN</li>
<li>Connecting the previous word to the next word, and the previous
sentence to the next sentence. These edges are called NEXT</li>
<li>Coreference resolution, referred to as REF The overall structure is
shown in the following image: <img data-src="https://s2.ax1x.com/2020/02/11/1Toe6P.png" alt="1Toe6P.png" /></li>
</ul></li>
</ul>
<h1 id="ggnn">GGNN</h1>
<ul>
<li><p>The GNN used by the authors is a Gated Graph Neural Network. The
original paper: GATED GRAPH SEQUENCE NEURAL NETWORKS</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{h}_{v}^{(1)}
&amp;=\left[\boldsymbol{x}_{v}^{\top}, \mathbf{0}\right]^{\top} \\
\mathbf{a}_{v}^{(t)}
&amp;=\mathbf{A}_{v:}^{\top}\left[\mathbf{h}_{1}^{(t-1) \top} \ldots
\mathbf{h}_{|\mathcal{V}|}^{(t-1) \top}\right]^{\top}+\mathbf{b} \\
\mathbf{z}_{v}^{t} &amp;=\sigma\left(\mathbf{W}^{z}
\mathbf{a}_{v}^{(t)}+\mathbf{U}^{z} \mathbf{h}_{v}^{(t-1)}\right)
\end{aligned} \\
\begin{aligned} \mathbf{r}_{v}^{t} &amp;=\sigma\left(\mathbf{W}^{r}
\mathbf{a}_{v}^{(t)}+\mathbf{U}^{r} \mathbf{h}_{v}^{(t-1)}\right) \\
\widetilde{\mathbf{h}_{v}^{(t)}} &amp;=\tanh \left(\mathbf{W}
\mathbf{a}_{v}^{(t)}+\mathbf{U}\left(\mathbf{r}_{v}^{t} \odot
\mathbf{h}_{v}^{(t-1)}\right)\right) \\ \mathbf{h}_{v}^{(t)}
&amp;=\left(1-\mathbf{z}_{v}^{t}\right) \odot
\mathbf{h}_{v}^{(t-1)}+\mathbf{z}_{v}^{t} \odot
\widetilde{\mathbf{h}_{v}^{(t)}} \end{aligned} \\
\]</span></p></li>
<li><p>GGNN was published in 2015, improving upon the GNN model from
2009.</p></li>
<li><p>The original GNN model essentially uses the graph's topological
relationships, masking some edges in a multi-layer linear network. The
representation of nodes at each layer is obtained through linear
transformations of neighboring nodes from the previous layer
(propagation), with the final layer using a linear layer to output node
labels. <img data-src="https://s2.ax1x.com/2020/02/27/3082wD.png"
alt="3082wD.png" /></p></li>
<li><p>GGNN considers directed and heterogeneous edges. Its adjacency
matrix A, as shown in the figure, is a linear layer of <span
class="math inline">\(\mathbf{A} \in \mathbb{R}^{D|\mathcal{V}| \times 2
D|\mathcal{V}|}\)</span>, with twice the width representing two output
channels in both directions. The input node representation matrix is
<span class="math inline">\(\mathbb{R}^{D|\mathcal{V}|}\)</span>, where
<span class="math inline">\(A\)</span> contains parameters that depend
on edge type and direction, essentially obtained through embedding
lookup. This is followed by a GRU-like update, with <span
class="math inline">\(z\)</span> and <span
class="math inline">\(r\)</span> being the update and reset gates,
respectively. The formula meanings are as follows:</p>
<ul>
<li>1: Initialize node embeddings by adding hand-crafted features
according to the specific task and padding to the same length</li>
<li>2: Obtain propagated information through a linear layer containing
adjacency information</li>
<li>3: Calculate the update gate based on the previous layer's state and
propagated information</li>
<li>4: Calculate the reset gate based on the previous layer's state and
propagated information</li>
<li>5,6: Similar to GRU</li>
</ul></li>
<li><p>For the output part, a simple linear layer can be applied to each
node for node-level tasks. To obtain the graph's representation, a
gating mechanism can be used (originally described as attention):</p>
<p><span class="math display">\[
\mathbf{h}_{\mathcal{G}}=\tanh \left(\sum_{v \in \mathcal{V}}
\sigma\left(i\left(\mathbf{h}_{v}^{(T)},
\boldsymbol{x}_{v}\right)\right) \odot \tanh
\left(j\left(\mathbf{h}_{v}^{(T)},
\boldsymbol{x}_{v}\right)\right)\right)
\]</span></p></li>
</ul>
<h1 id="ggsnn">GGSNN</h1>
<ul>
<li><p>The gated GNN can be extended to sequence output, namely GATED
GRAPH SEQUENCE NEURAL NETWORK. <img data-src="https://s2.ax1x.com/2020/02/28/3BkTun.png"
alt="3BkTun.png" /></p></li>
<li><p>As shown in the figure, a typical seq2seq model needs to encode
the input sequence and then decode step by step. However, in a graph,
one step already contains all sequence token information, with multiple
layers being a stacking of depth rather than temporal layers. Therefore,
we can start decoding at any depth, similar to CRF, as shown in the
figure: <span class="math inline">\(o\)</span> is the output, <span
class="math inline">\(X^k\)</span> is the input embedding matrix at the
k-th output step, <span class="math inline">\(H^{k,t}\)</span>
represents the k-th output, and the node embedding of the entire input
is propagated t steps in depth. Similar to transition and emission
matrices, the authors used two GGNNs <span
class="math inline">\(F_o,F_x\)</span> to complete the transfer and
emission of hidden states. They can share parameters in the propagation
part. Although only <span class="math inline">\(F_x\)</span>
transferring <span class="math inline">\(H\)</span> to <span
class="math inline">\(X\)</span> is written, in practice, similar to
LSTM, <span class="math inline">\(X^{k+1}\)</span> is also determined by
<span class="math inline">\(X^k\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{x}_{v}^{(k+1)}=\sigma\left(j\left(\mathbf{h}_{v}^{(k, T)},
\boldsymbol{x}_{v}^{(k)}\right)\right)
\]</span></p></li>
<li><p>Similarly, it's possible to directly input <span
class="math inline">\(X\)</span> for each decoding step, similar to
teacher forcing</p></li>
<li><p>The experiments in the paper were on relatively small state
spaces, different from text tasks. Refer to the usage in STRUCTURED
NEURAL SUMMARIZATION</p></li>
</ul>
<h1 id="sequence-gnns">Sequence GNNs</h1>
<ul>
<li><p>The authors introduced GGNN into the encoding side, equivalent to
supplementing the traditional seq2seq encoder with a GNN, but the
encoder output remains unchanged, and the decoder remains unchanged
(abandoning the GGSNN decoder design)</p></li>
<li><p>First, the authors described GGNN in clearer language, with each
step including propagation and update</p>
<ul>
<li>Propagation: <span
class="math inline">\(\boldsymbol{m}_{v}^{(i)}=g\left(\left\{f_{k}\left(\boldsymbol{h}_{u}^{(i)}\right)
| \text { there is an edge of type } k \text { from } u \text { to }
v\right\}\right.)\)</span>, i.e., collecting and summing neighboring
node information using edge-related linear transformations, where <span
class="math inline">\(f\)</span> is a linear layer and <span
class="math inline">\(g\)</span> is summation</li>
<li>Update: <span
class="math inline">\(\boldsymbol{h}_{v}^{(i+1)}=\operatorname{GRU}\left(\boldsymbol{m}_{v}^{(i)},
\boldsymbol{h}_{v}^{(i)}\right)\)</span></li>
</ul></li>
<li><p>In seq2seq, the encoder must provide two pieces of information:
token representation and context representation. Token-level
representation is obtained through GNN node embeddings, and for
context-level representation, in addition to the gated weighted sum of
nodes used in GGNN, they also concatenated the hidden state before and
after inputting the graph, seemingly concatenating the hidden states of
all nodes before and after graph input as the final node embedding. The
RNN output is directly concatenated with the graph embedding and then
passed through a linear layer. Note that the RNN output is essentially a
representation of the graph (entire sequence), so it can be directly
concatenated:</p>
<p><span class="math display">\[
\left[\mathbf{e}_{1}^{\prime} \ldots
\mathbf{e}_{N}^{\prime}\right]=\operatorname{GNN}\left(\left(S,\left[R_{1}
\ldots R_{K}\right],\left[\mathbf{e}_{1} \ldots
\mathbf{e}_{N}\right]\right)\right) \\
\]</span></p>
<p><span class="math display">\[
\sigma\left(w\left(\boldsymbol{h}_{v}^{(T)}\right)\right) \in[0,1] \\
\]</span></p>
<p><span class="math display">\[
\hat{\mathbf{e}}=\sum_{1&lt;i&lt;N}
\sigma\left(w\left(\mathbf{e}_{i}^{\prime}\right)\right) \cdot
\aleph\left(\mathbf{e}_{i}^{\prime}\right) \\
\]</span></p>
<p><span class="math display">\[
Embedding_{graph} = W \cdot(\mathbf{e} \hat{\mathbf{e}}) \\
\]</span></p></li>
<li><p>In practical engineering implementation, batching graphs of
different sizes is inconvenient. The authors used two tricks:</p>
<ul>
<li>Standard GNN approach: Combine small graphs into a large graph with
multiple connected subgraphs as a batch</li>
<li>Since copy and attention mechanisms require calculating weights
across the entire input sequence, after combining into a large graph,
the authors also preserved each node's index in the small graph. Then,
using TensorFlow's unsorted segment * operator (performing operations on
segments of different lengths), they can efficiently and numerically
stably perform softmax over the variable number of node representations
for each graph</li>
</ul></li>
<li><p>The authors used a simple LSTM encoder and decoder configuration,
mainly modifying the pointer generator code. GNN was stacked eight
layers</p></li>
<li><p>The final results did not surpass the pointer generator, but the
ablation with the pointer mechanism was quite significant, as shown in
the following figure: <img data-src="https://s2.ax1x.com/2020/02/11/1TTHPJ.png"
alt="1TTHPJ.png" /></p></li>
<li><p>The authors did not provide much analysis of the results, as the
paper used three datasets, with the other two being code summaries that
have naturally structured data, thus performing well. On purely natural
language datasets like CNNDM, the performance was not particularly
outstanding</p></li>
<li><p>However, in the ablation experiments, it's worth noting that even
without adding coreference information, simply using GNN to process
sentence structure performed better than LSTM</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="intuition">Intuition</h1>
<ul>
<li>将GNN引入seq2seq的一个很自然的想法就是先用sequence encoder得到token
representations，然后再构建token之间的关系，建图，将图和token
representations送入GNN，得到新的token表示。</li>
<li>问题在于，对于摘要，直觉的想法是以句子为节点，构建句子之间的关系图，这样最后得到的是句子的表示，不能到词级别的细粒度，这样的话attention
based decoder就不太好做。</li>
<li>本文作者的想法就很粗暴，干脆就做词级别的GNN，一篇文章900个词，就构建900个节点的图，而边的关系是异构的，分三种：
<ul>
<li>一句里的所有词连向一个额外添加的句子节点、一个实体里的所有词连向一个额外添加的实体节点，这类边都叫做IN</li>
<li>前一个词连后一个词，前一句连后一句，这类边叫NEXT</li>
<li>指代消解，这部分叫做REF 整体如下图所示： <img data-src="https://s2.ax1x.com/2020/02/11/1Toe6P.png" alt="1Toe6P.png" /></li>
</ul></li>
</ul>
<h1 id="ggnn">GGNN</h1>
<ul>
<li><p>作者使用的GNN是Gated Graph Neural Network。原论文见：GATED GRAPH
SEQUENCE NEURAL NETWORKS</p>
<p><span class="math display">\[
\begin{aligned} \mathbf{h}_{v}^{(1)}
&amp;=\left[\boldsymbol{x}_{v}^{\top}, \mathbf{0}\right]^{\top} \\
\mathbf{a}_{v}^{(t)}
&amp;=\mathbf{A}_{v:}^{\top}\left[\mathbf{h}_{1}^{(t-1) \top} \ldots
\mathbf{h}_{|\mathcal{V}|}^{(t-1) \top}\right]^{\top}+\mathbf{b} \\
\mathbf{z}_{v}^{t} &amp;=\sigma\left(\mathbf{W}^{z}
\mathbf{a}_{v}^{(t)}+\mathbf{U}^{z} \mathbf{h}_{v}^{(t-1)}\right)
\end{aligned} \\
\begin{aligned} \mathbf{r}_{v}^{t} &amp;=\sigma\left(\mathbf{W}^{r}
\mathbf{a}_{v}^{(t)}+\mathbf{U}^{r} \mathbf{h}_{v}^{(t-1)}\right) \\
\widetilde{\mathbf{h}_{v}^{(t)}} &amp;=\tanh \left(\mathbf{W}
\mathbf{a}_{v}^{(t)}+\mathbf{U}\left(\mathbf{r}_{v}^{t} \odot
\mathbf{h}_{v}^{(t-1)}\right)\right) \\ \mathbf{h}_{v}^{(t)}
&amp;=\left(1-\mathbf{z}_{v}^{t}\right) \odot
\mathbf{h}_{v}^{(t-1)}+\mathbf{z}_{v}^{t} \odot
\widetilde{\mathbf{h}_{v}^{(t)}} \end{aligned} \\
\]</span></p></li>
<li><p>GGNN发布于2015年，在2009年的GNN模型上改进。</p></li>
<li><p>原始的GNN模型相当于用图的拓扑关系，在多层线性网络中Mask掉部分边，节点在每一层的表示通过上一层中相邻的节点线性变换而来，即propagation，最后一层linear做output输出节点标签
<img data-src="https://s2.ax1x.com/2020/02/27/3082wD.png"
alt="3082wD.png" /></p></li>
<li><p>GGNN考虑了边的有向和异构。其邻接矩阵A如上图所示，是一个<span
class="math inline">\(\mathbf{A} \in \mathbb{R}^{D|\mathcal{V}| \times 2
D|\mathcal{V}|}\)</span>的线性层，两倍的宽代表双向两个输出频道，输入节点的表示矩阵<span
class="math inline">\(\mathbb{R}^{D|\mathcal{V}|}\)</span>，这里的<span
class="math inline">\(A\)</span>包含了参数，依赖于边的类型和方向，相当于这个包含了邻接信息的线性层也是通过embedding
lookup得到的。接下来就是类似于GRU的更新，<span
class="math inline">\(z\)</span>和<span
class="math inline">\(r\)</span>分别是更新门和重置门。所以公式含义如下</p>
<ul>
<li>1:初始化节点embedding，根据具体任务给每个节点补上手工特征，并Padding到相同长度</li>
<li>2:通过包含了邻接信息的线性层，得到propagate之后的信息</li>
<li>3:根据上一层状态和propagate信息计算更新门</li>
<li>4:根据上一层状态和propagate信息计算重置门</li>
<li>5,6:同GRU</li>
</ul></li>
<li><p>output部分，简单的线性层作用于每一个节点就可以做节点级别的任务，如果要获得整张图的表示，可以用一个门控机制来获取（原文表述为attention）：</p>
<p><span class="math display">\[
\mathbf{h}_{\mathcal{G}}=\tanh \left(\sum_{v \in \mathcal{V}}
\sigma\left(i\left(\mathbf{h}_{v}^{(T)},
\boldsymbol{x}_{v}\right)\right) \odot \tanh
\left(j\left(\mathbf{h}_{v}^{(T)},
\boldsymbol{x}_{v}\right)\right)\right)
\]</span></p></li>
</ul>
<h1 id="ggsnn">GGSNN</h1>
<ul>
<li><p>门控的GNN还可以扩展为sequence output，即GATED GRAPH SEQUENCE
NEURAL NETWORK。 <img data-src="https://s2.ax1x.com/2020/02/28/3BkTun.png"
alt="3BkTun.png" /></p></li>
<li><p>如上图所示，一般的seq2seq，需要把输入的seq编码，之后再逐步解码，但是再graph当中，一步的graph就已经包含了所有seq
token信息，多层只是深度层次上的叠加，而不是时序的层次，因此我们可以在任意的深度层次开始解码，类似于CRF，如图所示：<span
class="math inline">\(o\)</span>为输出，<span
class="math inline">\(X^k\)</span>为第k步输出时节点的input embedding
matrix，<span
class="math inline">\(H^{k,t}\)</span>代表第k步输出，同时整个输入的节点embedding在深度上传递了t步时，节点的hidden
state matrix。类似于转移与发射矩阵，作者分别用了两个GGNN<span
class="math inline">\(F_o,F_x\)</span>来完成hidden
state的转移和发射。两者可以共享propagation部分的参数。虽然只写了<span
class="math inline">\(F_x\)</span>将<span
class="math inline">\(H\)</span>转移到<span
class="math inline">\(X\)</span>，但实际上类似于LSTM，<span
class="math inline">\(X^{k+1}\)</span>同时还由<span
class="math inline">\(X^k\)</span>决定：</p>
<p><span class="math display">\[
\boldsymbol{x}_{v}^{(k+1)}=\sigma\left(j\left(\mathbf{h}_{v}^{(k, T)},
\boldsymbol{x}_{v}^{(k)}\right)\right)
\]</span></p></li>
<li><p>同样，也可以不需要从<span class="math inline">\(H\)</span>到<span
class="math inline">\(X\)</span>的转移，直接输入每一解码步的<span
class="math inline">\(X\)</span>，类似于teacher forcing</p></li>
<li><p>论文里的实验都是状态空间比较小，不同于文本任务。直接看STRUCTURED
NEURAL SUMMARIZATION里的用法</p></li>
</ul>
<h1 id="sequence-gnns">Sequence GNNs</h1>
<ul>
<li><p>作者将GGNN引入编码端，相当于传统的seq2seq
encoder最后用GNN补充了一次编码，但是encoder的输出不变，decoder不变（抛弃了GGSNN的decoder设计）</p></li>
<li><p>首先作者用更加清晰的语言描述了GGNN，每一步包含propagation 与
update</p>
<ul>
<li>propagation：<span
class="math inline">\(\boldsymbol{m}_{v}^{(i)}=g\left(\left\{f_{k}\left(\boldsymbol{h}_{u}^{(i)}\right)
| \text { there is an edge of type } k \text { from } u \text { to }
v\right\}\right.)\)</span>，即用边相关的线性变换收集邻域节点信息求和，其中<span
class="math inline">\(f\)</span>是线性层，<span
class="math inline">\(g\)</span>是求和</li>
<li>update：<span
class="math inline">\(\boldsymbol{h}_{v}^{(i+1)}=\operatorname{GRU}\left(\boldsymbol{m}_{v}^{(i)},
\boldsymbol{h}_{v}^{(i)}\right)\)</span></li>
</ul></li>
<li><p>seq2seq中的encoder至少要提供两点信息：token representation 和
context representation。token级别的已经拿到了，即GNN之后的节点
embedding，context级别即图的表示，这里作者除了沿用GGNN里门控算权重求和各节点之外，还拼接了输入图之前、RNN编码之后的hidden
state，看代码貌似是把所有节点输入图前后的hidden
state拼接起来，作为最终的节点embedding；把RNN的输出直接和图embedding表示拼接起来再过一个线性层。这里注意RNN的输出实际上是对图（整个序列）的一个表示，和graph
embedding是同一级别的，所以直接拼接：</p>
<p><span class="math display">\[
\left[\mathbf{e}_{1}^{\prime} \ldots
\mathbf{e}_{N}^{\prime}\right]=\operatorname{GNN}\left(\left(S,\left[R_{1}
\ldots R_{K}\right],\left[\mathbf{e}_{1} \ldots
\mathbf{e}_{N}\right]\right)\right) \\
\]</span></p>
<p><span class="math display">\[
\sigma\left(w\left(\boldsymbol{h}_{v}^{(T)}\right)\right) \in[0,1] \\
\]</span></p>
<p><span class="math display">\[
\hat{\mathbf{e}}=\sum_{1&lt;i&lt;N}
\sigma\left(w\left(\mathbf{e}_{i}^{\prime}\right)\right) \cdot
\aleph\left(\mathbf{e}_{i}^{\prime}\right) \\
\]</span></p>
<p><span class="math display">\[
Embedding_{graph} = W \cdot(\mathbf{e} \hat{\mathbf{e}}) \\
\]</span></p></li>
<li><p>在实际工程实现中，不同大小的图打包成batch不方便，作者也是采用了两个trick</p>
<ul>
<li>GNN的常规做法：把小图拼接成有多个连接子图的大图作为一个batch</li>
<li>由于copy和attention机制需要在整个输入序列上计算权重，拼接成大图之后作者也保留了每个节点在小图当中的index，然后通过tensorflow的unsorted
segment *操作符（即对不同长度的段分别做操作），可以完成一个efficient and
numerically stable softmax over the variable number of representations
of the nodes of each graph.</li>
</ul></li>
<li><p>作者只用了简单的LSTM的encoder和decoder配置，基本在pointer
generator的代码上做改动。GNN叠加八层</p></li>
<li><p>最后的结果并没有超过pointer
generator，但是引入pointer机制后的ablation比较明显，如下图， <img data-src="https://s2.ax1x.com/2020/02/11/1TTHPJ.png"
alt="1TTHPJ.png" /></p></li>
<li><p>作者也没有对结果做太多分析，因为论文做了三个数据集，其余两个是代码摘要，有比较自然的结构化数据，因此表现很好，在CNNDM这种纯自然语言数据集上表现并不是特别亮眼。</p></li>
<li><p>但是在消融实验中值得注意的是即便不添加指代信息，仅仅是让GNN处理句子结构，表现也比LSTM要好。</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>graph neural network</tag>
        <tag>deep learning</tag>
        <tag>summarization</tag>
        <tag>natural language processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Statistical Learning - A hand-write note</title>
    <url>/2018/08/09/statistical-handwriting/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/d67b97f7a9ddae88afabc277d249b281.png" width="500"/></p>
<p>The ten major algorithms of statistical learning methods have been
simplified and handwritten out (although I think the book itself is
already quite concise). Now there is only the process of the algorithms
themselves; in the future, if I have any new understandings, I will
supplement them. The writing is ugly, even I can't bear to look at it,
so I post it purely as a backup</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="abstract">Abstract</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HLjK.jpg" alt="i0HLjK.jpg" />
<figcaption aria-hidden="true">i0HLjK.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg" alt="i0Hb1x.jpg" />
<figcaption aria-hidden="true">i0Hb1x.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HoN9.jpg" alt="i0HoN9.jpg" />
<figcaption aria-hidden="true">i0HoN9.jpg</figcaption>
</figure>
<h1 id="perceptron">Perceptron</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HH91.jpg" alt="i0HH91.jpg" />
<figcaption aria-hidden="true">i0HH91.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HThR.jpg" alt="i0HThR.jpg" />
<figcaption aria-hidden="true">i0HThR.jpg</figcaption>
</figure>
<h1 id="k-nearest-neighbors">k-Nearest Neighbors</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg" alt="i0Hqc6.jpg" />
<figcaption aria-hidden="true">i0Hqc6.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HXnO.jpg" alt="i0HXnO.jpg" />
<figcaption aria-hidden="true">i0HXnO.jpg</figcaption>
</figure>
<h1 id="naive-bayes">Naive Bayes</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HjBD.jpg" alt="i0HjBD.jpg" />
<figcaption aria-hidden="true">i0HjBD.jpg</figcaption>
</figure>
<h1 id="decision-tree">Decision Tree</h1>
<ul>
<li>GBDT is written in the improvement method, and it is also possible
to expand and look at random forests, which is an bootstrap method that
utilizes decision trees. <img data-src="https://s1.ax1x.com/2018/10/20/i0HzAH.jpg" alt="i0HzAH.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HvHe.jpg" alt="i0HvHe.jpg" />
<figcaption aria-hidden="true">i0HvHe.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bP3t.jpg" alt="i0bP3t.jpg" />
<figcaption aria-hidden="true">i0bP3t.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bSNd.jpg" alt="i0bSNd.jpg" />
<figcaption aria-hidden="true">i0bSNd.jpg</figcaption>
</figure>
<h1 id="logistic-regression-maximum-entropy">Logistic regression,
maximum entropy</h1>
<ul>
<li>The mutual derivation between maximum entropy and logistic
regression is to be supplemented <img data-src="https://s1.ax1x.com/2018/10/20/i0bigP.jpg" alt="i0bigP.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bp4A.jpg" alt="i0bp4A.jpg" />
<figcaption aria-hidden="true">i0bp4A.jpg</figcaption>
</figure>
<h1 id="support-vector-machine">Support Vector Machine</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bFjf.jpg" alt="i0bFjf.jpg" />
<figcaption aria-hidden="true">i0bFjf.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bAu8.jpg" alt="i0bAu8.jpg" />
<figcaption aria-hidden="true">i0bAu8.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bEDS.jpg" alt="i0bEDS.jpg" />
<figcaption aria-hidden="true">i0bEDS.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0beEQ.jpg" alt="i0beEQ.jpg" />
<figcaption aria-hidden="true">i0beEQ.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bVHg.jpg" alt="i0bVHg.jpg" />
<figcaption aria-hidden="true">i0bVHg.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bmNj.jpg" alt="i0bmNj.jpg" />
<figcaption aria-hidden="true">i0bmNj.jpg</figcaption>
</figure>
<h1 id="enhancement-methods">Enhancement Methods</h1>
<ul>
<li>XGBoost <img data-src="https://s1.ax1x.com/2018/10/20/i0bn4s.jpg"
alt="i0bn4s.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bKCn.jpg" alt="i0bKCn.jpg" />
<figcaption aria-hidden="true">i0bKCn.jpg</figcaption>
</figure>
<h1 id="expectation-maximization-em-algorithm">Expectation-Maximization
(EM) algorithm</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bM3q.jpg" alt="i0bM3q.jpg" />
<figcaption aria-hidden="true">i0bM3q.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bQg0.jpg" alt="i0bQg0.jpg" />
<figcaption aria-hidden="true">i0bQg0.jpg</figcaption>
</figure>
<ul>
<li>When performing inference on Gaussian mixture models using the EM
algorithm, the parameters to be inferred include the mean, variance, and
proportion coefficient of k Gaussian models. The latent variable
represents the probability that the jth observation sample belongs to
the kth Gaussian model, referred to as responsibility, while <span
class="math inline">\(n_k\)</span> is the sum of the responsibility of
the kth Gaussian model over all samples, divided by <span
class="math inline">\(N\)</span> , which is used to update the GMM
proportion coefficient with its mean. The mean is updated using
responsibility-weighted samples, and the variance is updated in a
similar manner.</li>
<li>After updating the parameters, recalculate responsibility with these
parameters, recalculate the E-step, and then proceed to the M-step,
thereby completing the iteration.</li>
</ul>
<h1 id="hidden-markov">Hidden Markov</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0b3uT.jpg" alt="i0b3uT.jpg" />
<figcaption aria-hidden="true">i0b3uT.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0blvV.jpg" alt="i0blvV.jpg" />
<figcaption aria-hidden="true">i0blvV.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0b8DU.jpg" alt="i0b8DU.jpg" />
<figcaption aria-hidden="true">i0b8DU.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bGbF.jpg" alt="i0bGbF.jpg" />
<figcaption aria-hidden="true">i0bGbF.jpg</figcaption>
</figure>
<h1 id="conditional-random-field">Conditional Random Field</h1>
<ul>
<li>Address the solutions for three types of problems, as conditional
random fields are the conditional extension of the hidden Markov model,
and the algorithms are similar <img data-src="https://s1.ax1x.com/2018/10/20/i0bYE4.jpg" alt="i0bYE4.jpg" /></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="概论">概论</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HLjK.jpg" alt="i0HLjK.jpg" />
<figcaption aria-hidden="true">i0HLjK.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg" alt="i0Hb1x.jpg" />
<figcaption aria-hidden="true">i0Hb1x.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HoN9.jpg" alt="i0HoN9.jpg" />
<figcaption aria-hidden="true">i0HoN9.jpg</figcaption>
</figure>
<h1 id="感知机">感知机</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HH91.jpg" alt="i0HH91.jpg" />
<figcaption aria-hidden="true">i0HH91.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HThR.jpg" alt="i0HThR.jpg" />
<figcaption aria-hidden="true">i0HThR.jpg</figcaption>
</figure>
<h1 id="k近邻">k近邻</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg" alt="i0Hqc6.jpg" />
<figcaption aria-hidden="true">i0Hqc6.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HXnO.jpg" alt="i0HXnO.jpg" />
<figcaption aria-hidden="true">i0HXnO.jpg</figcaption>
</figure>
<h1 id="朴素贝叶斯">朴素贝叶斯</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HjBD.jpg" alt="i0HjBD.jpg" />
<figcaption aria-hidden="true">i0HjBD.jpg</figcaption>
</figure>
<h1 id="决策树">决策树</h1>
<ul>
<li>GBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。
<img data-src="https://s1.ax1x.com/2018/10/20/i0HzAH.jpg"
alt="i0HzAH.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0HvHe.jpg" alt="i0HvHe.jpg" />
<figcaption aria-hidden="true">i0HvHe.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bP3t.jpg" alt="i0bP3t.jpg" />
<figcaption aria-hidden="true">i0bP3t.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bSNd.jpg" alt="i0bSNd.jpg" />
<figcaption aria-hidden="true">i0bSNd.jpg</figcaption>
</figure>
<h1 id="逻辑斯蒂回归最大熵">逻辑斯蒂回归、最大熵</h1>
<ul>
<li>待补充最大熵和逻辑斯蒂回归之间的相互推导 <img data-src="https://s1.ax1x.com/2018/10/20/i0bigP.jpg" alt="i0bigP.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bp4A.jpg" alt="i0bp4A.jpg" />
<figcaption aria-hidden="true">i0bp4A.jpg</figcaption>
</figure>
<h1 id="支持向量机">支持向量机</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bFjf.jpg" alt="i0bFjf.jpg" />
<figcaption aria-hidden="true">i0bFjf.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bAu8.jpg" alt="i0bAu8.jpg" />
<figcaption aria-hidden="true">i0bAu8.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bEDS.jpg" alt="i0bEDS.jpg" />
<figcaption aria-hidden="true">i0bEDS.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0beEQ.jpg" alt="i0beEQ.jpg" />
<figcaption aria-hidden="true">i0beEQ.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bVHg.jpg" alt="i0bVHg.jpg" />
<figcaption aria-hidden="true">i0bVHg.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bmNj.jpg" alt="i0bmNj.jpg" />
<figcaption aria-hidden="true">i0bmNj.jpg</figcaption>
</figure>
<h1 id="提升方法">提升方法</h1>
<ul>
<li>待补充XGBoost <img data-src="https://s1.ax1x.com/2018/10/20/i0bn4s.jpg"
alt="i0bn4s.jpg" /></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bKCn.jpg" alt="i0bKCn.jpg" />
<figcaption aria-hidden="true">i0bKCn.jpg</figcaption>
</figure>
<h1 id="em算法">EM算法</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bM3q.jpg" alt="i0bM3q.jpg" />
<figcaption aria-hidden="true">i0bM3q.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bQg0.jpg" alt="i0bQg0.jpg" />
<figcaption aria-hidden="true">i0bQg0.jpg</figcaption>
</figure>
<ul>
<li>用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而<span
class="math inline">\(n_k\)</span>则是对第k个高斯模型在所有样本上的responsibility的总和，除以<span
class="math inline">\(N\)</span>即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。</li>
<li>在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。</li>
</ul>
<h1 id="隐马尔可夫">隐马尔可夫</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0b3uT.jpg" alt="i0b3uT.jpg" />
<figcaption aria-hidden="true">i0b3uT.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0blvV.jpg" alt="i0blvV.jpg" />
<figcaption aria-hidden="true">i0blvV.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0b8DU.jpg" alt="i0b8DU.jpg" />
<figcaption aria-hidden="true">i0b8DU.jpg</figcaption>
</figure>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0bGbF.jpg" alt="i0bGbF.jpg" />
<figcaption aria-hidden="true">i0bGbF.jpg</figcaption>
</figure>
<h1 id="条件随机场">条件随机场</h1>
<ul>
<li>待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似
<img data-src="https://s1.ax1x.com/2018/10/20/i0bYE4.jpg"
alt="i0bYE4.jpg" /></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>code</tag>
        <tag>statistical learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2020/02/13/svm/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/94237772906ab6b432dad7cb2a98d766.png" width="500"/></p>
<p>Long time no see, SVM.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="objective-function">Objective Function</h1>
<ul>
<li><p>The most primitive idea is naturally to consider the geometric
margin</p>
<p><span class="math display">\[
\begin{array}{cc}{\max _{\vec{w}, b} \gamma} \\ {s . t . \quad
\tilde{y}_{i}\left(\frac{\overrightarrow{\mathbf{w}}}{\|\overrightarrow{\mathbf{w}}\|_{2}}
\cdot
\overrightarrow{\mathbf{x}}_{i}+\frac{b}{\|\overrightarrow{\mathbf{w}}\|_{2}}\right)
\geq \gamma, i=1,2, \cdots, N}\end{array}
\]</span></p></li>
<li><p>However, the geometric margin can be represented by the
functional margin, and the functional margin can be scaled without
affecting the choice of the classification hyperplane. Therefore, we set
the functional margin to 1, then take the reciprocal to replace max with
min, simplifying the objective function</p>
<p><span class="math display">\[
\begin{array}{c}{\min _{\vec{w}, b}
\frac{1}{2}\|\overrightarrow{\mathbf{w}}\|_{2}^{2}} \\ {\text {s.t.}
\quad \tilde{y}_{i}\left(\overrightarrow{\mathbf{w}} \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right)-1 \geq 0, i=1,2, \cdots,
N}\end{array}
\]</span></p></li>
</ul>
<h1 id="min-max">Min-Max</h1>
<ul>
<li><p>After defining the problem as an optimization problem with
inequality constraints, Lagrangian duality is used to transform the
primal problem into a dual problem</p></li>
<li><p>The description in statistical learning methods is as
follows:</p>
<ul>
<li><p>For the optimization problem:</p>
<p><span class="math display">\[
\begin{array}{ll}{\min _{x \in \mathbf{R}^{n}} f(x)} &amp; {} \\ {\text
{ s.t. } \quad c_{i}(x) \leqslant 0, \quad i=1,2, \cdots, k} \\ {\qquad
\begin{array}{ll}{h_{j}(x)=0,} &amp; {j=1,2, \cdots,
l}\end{array}}\end{array}
\]</span></p></li>
<li><p>Introduce the generalized Lagrangian function</p>
<p><span class="math display">\[
L(x, \alpha, \beta)=f(x)+\sum_{i=1}^{k} \alpha_{i}
c_{i}(x)+\sum_{j=1}^{I} \beta_{j} h_{j}(x)
\]</span></p></li>
<li><p>Define</p>
<p><span class="math display">\[
\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha,
\beta)
\]</span></p></li>
<li><p>It can be determined that if the constraints are not satisfied,
the Lagrangian multipliers can be set to a non-zero value to make the
above expression infinite. If satisfied, the maximum value can only be
achieved when <span class="math inline">\(\alpha\)</span> is 0, and
<span class="math inline">\(\beta\)</span> does not play a role,
therefore:</p>
<p><span class="math display">\[
\theta_{P}(x)=\left\{\begin{array}{l}{f(x)} \ x \text{satisfies original
problem constraints} \\ {+\infty} \ \text{otherwise} \end{array}\right.
\]</span></p></li>
</ul></li>
<li><p>Thus, the original minimization of f is transformed into
minimizing a maximized <span class="math inline">\(\theta\)</span>,
namely</p>
<p><span class="math display">\[
\min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha,
\beta)
\]</span></p></li>
</ul>
<h1 id="duality">Duality</h1>
<ul>
<li><p>Swapping the positions of max and min yields the dual problem,
which is first optimizing for <span class="math inline">\(x\)</span>,
then optimizing for <span
class="math inline">\(\alpha,\beta\)</span></p>
<p><span class="math display">\[
\begin{array}{l}{\max _{\alpha, \beta} \theta_{D}(\alpha, \beta)=\max
_{\alpha, \beta} \min _{x} L(x, \alpha, \beta)} \\ {\text { s.t. } \quad
\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, k}\end{array}
\]</span></p></li>
<li><p>The relationship between the dual problem and the primal
problem:</p>
<p><span class="math display">\[
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x,
\alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i}
\geqslant 0} L(x, \alpha, \beta)=p^{*}
\]</span></p></li>
<li><p>Proof: First look at the inner part of both sides. The left side
is <span class="math inline">\(\min \ L\)</span>, the right side is
<span class="math inline">\(\max \ L\)</span>. Although the variables
are different, when the variables are fixed, there must be <span
class="math inline">\(\min _{x} L(x, \alpha, \beta) \leqslant L(x,
\alpha, \beta) \leqslant \max _{\alpha, \beta: \alpha_{i} \geqslant 0}
L(x, \alpha, \beta)\)</span>. Now looking at the whole thing as a
function and focusing on the outer part, comparing the max of <span
class="math inline">\(f_1\)</span> and the min of <span
class="math inline">\(f_2\)</span>, from the above we know that <span
class="math inline">\(f_1\)</span> is everywhere less than or equal to
<span class="math inline">\(f_2\)</span>. Therefore, even the maximum of
<span class="math inline">\(f_1\)</span> must be less than or equal to
the minimum of <span class="math inline">\(f_2\)</span>, which is the
so-called "chicken head is smaller than phoenix tail", with the premise
that each chicken in the chicken flock is smaller than or equal to each
phoenix in the phoenix flock.</p></li>
<li><p>SVM satisfies the strong duality relationship, meaning the above
equation reaches equality. Therefore, optimizing the original problem
can be transformed into optimizing the dual problem, which facilitates
the introduction of kernel functions.</p></li>
<li><p>The original min-max problem can be solved directly, but not by
directly taking partial derivatives of the min-max form, because taking
partial derivatives with respect to <span
class="math inline">\(\alpha\)</span> first is meaningless. We hope to
obtain <span class="math inline">\(w,b\)</span> such that for any <span
class="math inline">\(\alpha \neq 0\)</span>, <span
class="math inline">\(\max L\)</span> can be minimized.</p></li>
</ul>
<h1 id="solution">Solution</h1>
<ul>
<li><p>After converting to the dual problem, first take derivatives of
<span class="math inline">\(w,b\)</span> and set them to zero, then
substitute the obtained <span class="math inline">\(w,b\)</span> back
into the dual problem to get:</p>
<p><span class="math display">\[
\text{Substituting into the dual problem:}
L(\overrightarrow{\mathbf{w}}, b,
\vec{\alpha})=\frac{1}{2}\|\overrightarrow{\mathbf{w}}\|_{2}^{2}-\sum_{i=1}^{N}
\alpha_{i} \tilde{y}_{i}\left(\overrightarrow{\mathbf{w}} \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
\]</span></p>
<p><span class="math display">\[
\begin{aligned} L(\overrightarrow{\mathbf{w}}, b,
\vec{\alpha})=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} &amp; \alpha_{i}
\alpha_{j} \tilde{y}_{i}
\tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)-\sum_{i=1}^{N} \alpha_{i}
\tilde{y}_{i}\left[\left(\sum_{j=1}^{N} \alpha_{j} \tilde{y}_{j}
\overrightarrow{\mathbf{x}}_{j}\right) \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right]+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j}
\tilde{y}_{i} \tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned} \\
\]</span></p></li>
<li><p>Here, the first term's coefficient is 1/2, the <span
class="math inline">\(b\)</span> part in the second term is 0, and the
remaining part is actually the same as the first term, just with a
coefficient of 1. Therefore, subtracting gives -1/2. Finally, the outer
max optimization objective is obtained</p>
<p><span class="math display">\[
\begin{aligned} \max _{\vec{\alpha}}-\frac{1}{2} \sum_{i=1}^{N}
\sum_{j=1}^{N} \alpha_{i} \alpha_{j} \tilde{y}_{i}
\tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\ s .
t . \quad \sum_{i=1}^{N} \alpha_{i} \tilde{y}_{i}=0 \\ \alpha_{i} \geq
0, i=1,2, \cdots, N \end{aligned}
\]</span></p></li>
<li><p>The equality constraint condition is obtained by taking the
derivative of <span class="math inline">\(b\)</span> to zero. As
mentioned earlier, the objective function and constraints of SVM satisfy
the strong duality relationship. The necessary and sufficient condition
for strong duality is the KKT conditions, so the above equation should
also satisfy the KKT conditions, namely:</p>
<ul>
<li><p>Take partial derivatives of the Lagrangian function with respect
to <span class="math inline">\(w,b\)</span> and set to zero</p></li>
<li><p>The constraint part of the Lagrangian function (each term in the
summation) is zero, i.e., the complementary slackness condition</p></li>
<li><p>Original problem constraints</p></li>
<li><p>Lagrangian multipliers are non-negative</p>
<p><span class="math display">\[
\begin{aligned} \nabla_{\overrightarrow{\mathrm{w}}}
L\left(\overrightarrow{\mathrm{w}}^{*}, b^{*},
\vec{\alpha}^{*}\right)=&amp;
\overrightarrow{\mathrm{w}}^{*}-\sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i} \overrightarrow{\mathrm{x}}_{i}=0 \\ \nabla_{b}
L\left(\overrightarrow{\mathrm{w}}^{*}, b^{*},
\vec{\alpha}^{*}\right)=&amp; \sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i}=0 \\
\alpha_{i}^{*}\left[\tilde{y}_{i}\left(\overrightarrow{\mathrm{w}}^{*}
\cdot \overrightarrow{\mathrm{x}}_{i}+b^{*}\right)-1\right]=0, i=1,2,
\cdots, N \\ \tilde{y}_{i}\left(\overrightarrow{\mathrm{w}}^{*} \cdot
\overrightarrow{\mathrm{x}}_{i}+b^{*}\right)-1 \geq 0, i=1,2, \cdots, N
\\ \alpha_{i}^{*} \geq 0, i=1,2, \cdots, N \end{aligned}
\]</span></p></li>
</ul></li>
<li><p>From the KKT conditions, we can express <span
class="math inline">\(w\)</span> using <span
class="math inline">\(\alpha\)</span> (which was also obtained when
taking derivatives before). We know that <span
class="math inline">\(w\)</span> represents the direction of the
classification hyperplane, <span class="math inline">\(b\)</span>
represents the bias, and is determined by support vectors. Therefore,
those corresponding to non-zero <span class="math inline">\(\alpha
_j\)</span></p>
<p><span class="math display">\[
\tilde{y}_{i}\left(\overrightarrow{\mathbf{w}}^{*} \cdot
\overrightarrow{\mathbf{x}}_{i}+b^{*}\right)-1
\]</span></p>
<p>determines <span class="math inline">\(b\)</span> (because when <span
class="math inline">\(\alpha\)</span> is non-zero, by the complementary
slackness condition, the latter term is zero, so <span
class="math inline">\(b\)</span> can be solved). Finally, we get:</p>
<p><span class="math display">\[
b^{*}=\tilde{y}_{j}-\sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i}\left(\overrightarrow{\mathrm{x}}_{i} \cdot
\overrightarrow{\mathrm{x}}_{j}\right)
\]</span></p>
<p>It can be seen that <span class="math inline">\(w,b\)</span> are in
the form of summation, but most are zero. Only the non-zero <span
class="math inline">\(\alpha\)</span> terms (directly finding the
non-zero <span class="math inline">\(\alpha _j\)</span> in the <span
class="math inline">\(b\)</span> expression) play a role, meaning the
support vector machine is determined only by a few support
vectors.</p></li>
<li><p>Therefore, given the data, first solve the maximum of the dual
problem to obtain <span class="math inline">\(\alpha\)</span>, then
calculate <span class="math inline">\(w,b\)</span> from the non-zero
parts of <span class="math inline">\(\alpha\)</span>, to obtain the
hyperplane.</p></li>
<li><p>The definition and solving process of soft margin and non-linear
support vector machines are similar, just with different constraint
conditions and objective functions.</p></li>
<li><p>The solving of <span class="math inline">\(\alpha\)</span>
involves convex quadratic programming with many solution methods. One
advantage of support vector machines is that the learned parameters only
depend on support vectors, avoiding the curse of dimensionality during
inference. However, during the learning process, all samples need to be
calculated for optimization, so it is not friendly to large-scale data.
Here, the SMO algorithm can be used for optimization.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="目标函数">目标函数</h1>
<ul>
<li><p>最原始的想法自然是考虑几何间隔</p>
<p><span class="math display">\[
\begin{array}{cc}{\max _{\vec{w}, b} \gamma} \\ {s . t . \quad
\tilde{y}_{i}\left(\frac{\overrightarrow{\mathbf{w}}}{\|\overrightarrow{\mathbf{w}}\|_{2}}
\cdot
\overrightarrow{\mathbf{x}}_{i}+\frac{b}{\|\overrightarrow{\mathbf{w}}\|_{2}}\right)
\geq \gamma, i=1,2, \cdots, N}\end{array}
\]</span></p></li>
<li><p>但是几何间隔可以用函数间隔表示，且函数间隔可以缩放而不影响分类超平面的选择，因此才有令函数间隔等于1，再取倒数把max换成min，化简了目标函数</p>
<p><span class="math display">\[
\begin{array}{c}{\min _{\vec{w}, b}
\frac{1}{2}\|\overrightarrow{\mathbf{w}}\|_{2}^{2}} \\ {\text {s.t.}
\quad \tilde{y}_{i}\left(\overrightarrow{\mathbf{w}} \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right)-1 \geq 0, i=1,2, \cdots,
N}\end{array}
\]</span></p></li>
</ul>
<h1 id="min-max">min-max</h1>
<ul>
<li><p>在将问题定义为带有不等式约束的最优化问题之后，就要用到拉格朗日对偶性来将原始问题转为对偶问题</p></li>
<li><p>统计学习方法的描述如下：</p>
<ul>
<li><p>对于最优化问题：</p>
<p><span class="math display">\[
\begin{array}{ll}{\min _{x \in \mathbf{R}^{n}} f(x)} &amp; {} \\ {\text
{ s.t. } \quad c_{i}(x) \leqslant 0, \quad i=1,2, \cdots, k} \\ {\qquad
\begin{array}{ll}{h_{j}(x)=0,} &amp; {j=1,2, \cdots,
l}\end{array}}\end{array}
\]</span></p></li>
<li><p>引入广义拉格朗日函数</p>
<p><span class="math display">\[
L(x, \alpha, \beta)=f(x)+\sum_{i=1}^{k} \alpha_{i}
c_{i}(x)+\sum_{j=1}^{I} \beta_{j} h_{j}(x)
\]</span></p></li>
<li><p>定义</p>
<p><span class="math display">\[
\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha,
\beta)
\]</span></p></li>
<li><p>可以判断，假如不满足约束条件的话，可以令拉格朗日乘子不为0从而使上式无穷大，满足情况的话，上式要最大只能让<span
class="math inline">\(\alpha\)</span>为0，<span
class="math inline">\(\beta\)</span>则不起作用，因此：</p>
<p><span class="math display">\[
\theta_{P}(x)=\left\{\begin{array}{l}{f(x)} \ x满足原始问题约束 \\
{+\infty} \ 其他 \end{array}\right.
\]</span></p></li>
</ul></li>
<li><p>这样原始的最小化一个f就转换为最小化一个最大化的<span
class="math inline">\(\theta\)</span>，即</p>
<p><span class="math display">\[
\min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L(x, \alpha,
\beta)
\]</span></p></li>
</ul>
<h1 id="对偶">对偶</h1>
<ul>
<li><p>max和min对换位置就得到对偶问题，即先针对<span
class="math inline">\(x\)</span>优化，再针对<span
class="math inline">\(\alpha,\beta\)</span>优化</p>
<p><span class="math display">\[
\begin{array}{l}{\max _{\alpha, \beta} \theta_{D}(\alpha, \beta)=\max
_{\alpha, \beta} \min _{x} L(x, \alpha, \beta)} \\ {\text { s.t. } \quad
\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, k}\end{array}
\]</span></p></li>
<li><p>对偶问题和原始问题的关系：</p>
<p><span class="math display">\[
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L(x,
\alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i}
\geqslant 0} L(x, \alpha, \beta)=p^{*}
\]</span></p></li>
<li><p>证明：先看两边的里面一部分，左边是<span
class="math inline">\(\min \ L\)</span>，右边是<span
class="math inline">\(\max \
L\)</span>，尽管自变量不一样，但是当自变量固定时，必然有<span
class="math inline">\(\min _{x} L(x, \alpha, \beta) \leqslant L(x,
\alpha, \beta) \leqslant \max _{\alpha, \beta: \alpha_{i} \geqslant 0}
L(x, \alpha,
\beta)\)</span>，现在把里面整体看成一个函数，只看外面，即比较左边的<span
class="math inline">\(\max f_1\)</span>和<span
class="math inline">\(\min f_2\)</span>，由上可知<span
class="math inline">\(f_1\)</span>处处小于等于<span
class="math inline">\(f_2\)</span>，那即便是<span
class="math inline">\(f_1\)</span>的最大值，也一定小于等于<span
class="math inline">\(f_2\)</span>的最小值，也就是所谓的“鸡头小于凤尾”，前提是鸡群里的每一只鸡小于等于凤群里的每一只凤。</p></li>
<li><p>SVM满足强对偶关系，即上式取到等号，因此优化原问题可以转化为优化对偶问题，方便引入核函数。</p></li>
<li><p>原始的min-max问题可以直接解，但不是对min-max形式直接求偏导，因为先对<span
class="math inline">\(\alpha\)</span>求偏导没有意义，我们是希望得到<span
class="math inline">\(w,b\)</span>使得对任意的<span
class="math inline">\(\alpha \neq 0\)</span>，<span
class="math inline">\(\max L\)</span>都可以取到最小值。</p></li>
</ul>
<h1 id="求解">求解</h1>
<ul>
<li><p>转换为对偶问题之后，先对<span
class="math inline">\(w,b\)</span>求导令其为0，之后将得到的<span
class="math inline">\(w,b\)</span>回代入对偶问题得到：</p>
<p><span class="math display">\[
带入对偶问题：L(\overrightarrow{\mathbf{w}}, b,
\vec{\alpha})=\frac{1}{2}\|\overrightarrow{\mathbf{w}}\|_{2}^{2}-\sum_{i=1}^{N}
\alpha_{i} \tilde{y}_{i}\left(\overrightarrow{\mathbf{w}} \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
\]</span></p>
<p><span class="math display">\[
\begin{aligned} L(\overrightarrow{\mathbf{w}}, b,
\vec{\alpha})=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} &amp; \alpha_{i}
\alpha_{j} \tilde{y}_{i}
\tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)-\sum_{i=1}^{N} \alpha_{i}
\tilde{y}_{i}\left[\left(\sum_{j=1}^{N} \alpha_{j} \tilde{y}_{j}
\overrightarrow{\mathbf{x}}_{j}\right) \cdot
\overrightarrow{\mathbf{x}}_{i}+b\right]+\sum_{i=1}^{N} \alpha_{i} \\
&amp;=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j}
\tilde{y}_{i} \tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned} \\
\]</span></p></li>
<li><p>这里第一项系数为1/2，第二项中b的部分为0，剩余部分其实和第一项相同，只不过系数为1，因此相减得到-1/2。最后得到外层的max优化目标</p>
<p><span class="math display">\[
\begin{aligned} \max _{\vec{\alpha}}-\frac{1}{2} \sum_{i=1}^{N}
\sum_{j=1}^{N} \alpha_{i} \alpha_{j} \tilde{y}_{i}
\tilde{y}_{j}\left(\overrightarrow{\mathbf{x}}_{i} \cdot
\overrightarrow{\mathbf{x}}_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\ s .
t . \quad \sum_{i=1}^{N} \alpha_{i} \tilde{y}_{i}=0 \\ \alpha_{i} \geq
0, i=1,2, \cdots, N \end{aligned}
\]</span></p></li>
<li><p>其中等式约束条件是对<span
class="math inline">\(b\)</span>求导为0得到的。之前说到SVM的目标函数和约束条件满足强对偶关系，强对偶关系的充要条件是KKT条件，因此上式应该也满足KKT条件，即</p>
<ul>
<li><p>拉格朗日函数对<span
class="math inline">\(w,b\)</span>求偏导为0</p></li>
<li><p>拉格朗日函数中原问题的约束部分（求和的每一项）为0，即松弛互补条件</p></li>
<li><p>原始问题约束</p></li>
<li><p>拉格朗日乘子非负</p>
<p><span class="math display">\[
\begin{aligned} \nabla_{\overrightarrow{\mathrm{w}}}
L\left(\overrightarrow{\mathrm{w}}^{*}, b^{*},
\vec{\alpha}^{*}\right)=&amp;
\overrightarrow{\mathrm{w}}^{*}-\sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i} \overrightarrow{\mathrm{x}}_{i}=0 \\ \nabla_{b}
L\left(\overrightarrow{\mathrm{w}}^{*}, b^{*},
\vec{\alpha}^{*}\right)=&amp; \sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i}=0 \\
\alpha_{i}^{*}\left[\tilde{y}_{i}\left(\overrightarrow{\mathrm{w}}^{*}
\cdot \overrightarrow{\mathrm{x}}_{i}+b^{*}\right)-1\right]=0, i=1,2,
\cdots, N \\ \tilde{y}_{i}\left(\overrightarrow{\mathrm{w}}^{*} \cdot
\overrightarrow{\mathrm{x}}_{i}+b^{*}\right)-1 \geq 0, i=1,2, \cdots, N
\\ \alpha_{i}^{*} \geq 0, i=1,2, \cdots, N \end{aligned}
\]</span></p></li>
</ul></li>
<li><p>由KKT条件，我们可以用<span
class="math inline">\(\alpha\)</span>表示<span
class="math inline">\(w\)</span>（之前求导时也已经得到过），我们知道<span
class="math inline">\(w\)</span>代表分类超平面的方向，<span
class="math inline">\(b\)</span>代表偏置，由支持向量决定，因此那些<span
class="math inline">\(\alpha _j\)</span>不为0对应的</p>
<p><span class="math display">\[
\tilde{y}_{i}\left(\overrightarrow{\mathbf{w}}^{*} \cdot
\overrightarrow{\mathbf{x}}_{i}+b^{*}\right)-1
\]</span></p>
<p>决定了<span class="math inline">\(b\)</span>（因为<span
class="math inline">\(\alpha\)</span>不为0，由松弛互补条件，则后一项为0，就可以求得<span
class="math inline">\(b\)</span>）。最后得到：</p>
<p><span class="math display">\[
b^{*}=\tilde{y}_{j}-\sum_{i=1}^{N} \alpha_{i}^{*}
\tilde{y}_{i}\left(\overrightarrow{\mathrm{x}}_{i} \cdot
\overrightarrow{\mathrm{x}}_{j}\right)
\]</span></p>
<p>可以看到，<span
class="math inline">\(w,b\)</span>都是求和的形式，但大部分为0，只有不为0的<span
class="math inline">\(\alpha\)</span>（在<span
class="math inline">\(b\)</span>的表达式里直接找出不为0的记为<span
class="math inline">\(\alpha
_j\)</span>）项才起作用，即支持向量机只有少数的支持向量决定。</p></li>
<li><p>因此，给定数据，先求得对偶问题极大值，得到<span
class="math inline">\(\alpha\)</span>，再由<span
class="math inline">\(\alpha\)</span>中不为0的部分计算出<span
class="math inline">\(w,b\)</span>，得到超平面。</p></li>
<li><p>软间隔和非线性支持向量机的定义和求解过程类似，只不过约束条件和目标函数不同。</p></li>
<li><p><span
class="math inline">\(\alpha\)</span>的求解涉及凸二次规划，有很多解法。支持向量机的一个优点就是学习到的参数只依赖支持向量，推理时避免了维度灾难，但是在学习的过程中，需要对所有样本计算最优化，因此对于大规模数据不友好，这里可以用SMO算法来优化</p></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>note for building a blog</title>
    <url>/2017/01/16/setupmywebsite/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/d1c3632ae2916e58df8591e7ac188747.png" width="500"/></p>
<p>Always wanted to build my own blog, previously thought of using
WordPress, but suffered from laziness, didn't want to mess with the
server. Later, I stumbled upon GitHub Pages, which automatically
generates a website by uploading a js project, and everything is hosted
on GitHub. The official instructions also recommend using this for
writing blogs, so I started to try it. The general framework should be
that GitHub Pages generates the website from your github.io project on
GitHub, Hexo generates the static web page project from your blog
content and custom settings, and then uploads it to your repository. To
back up, we will establish two branches in the repository: one master
for Hexo to upload static web page files, and one hexo for saving the
local Hexo project. Below, I share some experiences and pitfalls
encountered.</p>
<ul>
<li><pre><code>  2017.2.8 update md writing software</code></pre></li>
<li><pre><code>  2017.2.10 update mathjax cdn, add long gallery, update domain name, access分流(blog2.0)</code></pre></li>
<li><pre><code>  2017.2.13 update optimization plugin, update top description, optimize long gallery, widen article width(blog3.0)</code></pre></li>
<li><pre><code>  2017.3.30 update top description original address</code></pre></li>
<li><pre><code>  2017.12.27 update异地恢复</code></pre></li>
<li><pre><code>  2018.7.6 update a more comprehensive reference website</code></pre></li>
</ul>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i07MIs.png" alt="i07MIs.png" />
<figcaption aria-hidden="true">i07MIs.png</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="premise">Premise</h1>
<ul>
<li>Install Node.js</li>
<li>Install git</li>
<li>Install your favorite text editing software, such as notepad++, for
writing blogs. Alternatively, use plugins and third-party markdown
writing software (recommended).</li>
<li>Optional: Install the Chrome plugin Minimalist Image Hosting or MPic
(image hosting software). Install GeoGebra (for drawing graphs)</li>
</ul>
<h1 id="github-hexo-initialization">GitHub &amp; Hexo
Initialization</h1>
<p>Choosing hexo as the blog tool, rather than the officially
recommended jekyll, is actually quite similar; one is based on Ruby, and
the other on Node.js. Hexo is said to be faster. The final process was
like this.</p>
<ul>
<li>Registering a GitHub account, this is omitted</li>
<li>Create a new repository, the name must be
"your_username.github.io"</li>
<li>Create a new hexo branch for this repository and set hexo as the
default branch</li>
<li>cd to a new local folder (used to store the blog project) in git
bash</li>
<li>Sequentially execute npm install hexo, hexo init, npm install, npm
install hexo-deployer-git</li>
<li>At this point, the Hexo blog project has been initialized in your
folder. Find _config.yml in the folder and modify the deploy parameter
to master, see the following text.</li>
<li>https://your_username.github.io/</li>
<li>The current state is that your blog project exists under the hexo
branch, and all changes are saved to the Hexo branch on GitHub, but
since the deploy parameter is set to master, using the hexo command to
generate blog web pages will update the master branch, so both branches
will be updated each time the blog is updated, one for the hexo project
and one for updating web pages</li>
</ul>
<p><strong>Afterward, all commands are executed in the Git Bash
command-line environment, with the working directory being the Hexo blog
directory, and note that you should switch to the hexo
branch</strong></p>
<h1 id="hexo-configuration">Hexo Configuration</h1>
<p>The directory format of the Hexo blog is as follows</p>
<p>- _config.yml - package.json - scaffolds - source | ├── _drafts | └──
_posts - themes</p>
<ul>
<li>config.yml sets the overall parameters of the blog here, such as the
blog name, URL, and git.</li>
<li>_posts stores your articles</li>
<li>theme, as the name implies, stores the blog interface theme</li>
<li>In the config.yml, configure the following key parameters:
<ul>
<li>Blog Website Title</li>
<li>Subtitle: Subtitle</li>
<li>description: One-sentence summary</li>
<li>author: Author Name</li>
<li>language: zh-Hans Language Check the supported languages in the
language section of the topic catalog based on the theme you choose</li>
<li>timezone: Asia/Shanghai time zone, this has a standard, China should
be written as Shanghai time zone</li>
<li>https://your_username.github.io/</li>
<li>type: git</li>
<li>repo:
https://github.com/your_username/your_username.github.io.git</li>
<li><strong>branch: master</strong></li>
</ul></li>
</ul>
<h1 id="update-hexo-project">Update Hexo project</h1>
<p>Sequentially execute</p>
<ul>
<li>git add . (Check if all files are updated)</li>
<li>git commit -m "Update Report" (submitting commit)</li>
<li>git push origin hexo (Upload updates to GitHub) The first update may
encounter some errors</li>
<li>First, pull then push; otherwise, GitHub will report an error if it
finds that some files on the local machine are not on the server</li>
<li>Permission denied: Go to the blog directory/.git/config, find the
Url, and change the SSH link to HTML format:
https://www.github.com/your_username/your_username.github.io.git</li>
<li>refusing to merge unrelated histories because there is no common
ancestor branch to merge, this can only be forced to merge, execute
command: git pull origin hexo --allow-unrelated-histories, then run ush
all operations are performed under the Hexo branch, because master only
stores static web page files, which do not require you to change, it is
generated by Hexo</li>
</ul>
<h1 id="hexo-write-blogs-and-update-webpages">Hexo write blogs and
update webpages</h1>
<p>Main commands are as follows</p>
<ul>
<li>hexo clean Clean cache and static files</li>
<li>hexo g or hexo generate to generate framework files; it is
recommended to clean first and then generate before updating the
website</li>
<li>hexo s or hexo server to open the local server for preview, enter
localhost:4000 as the URL, and press Ctrl+C to close</li>
<li>hexo new "filename" creates a new blog post, this filename is not
the title of the article, and after creating it, you can open it in the
_posts folder to write the blog</li>
<li>hexo d or hexo deploy, after writing the blog, clean &amp; generate,
then deploy the blog to GitHub Pages, the blog webpage is updated. There
are a few points to note</li>
<li>After the first new directory is created, execute `#npm install
hexo-deployer-git --save` to install git deployment; otherwise, the
upload will not work</li>
<li>Hexo's grammar specification is very strict; tags: or level headings
# must be followed by a space</li>
<li>I have some issues updating under the next topic, it's best to
generate the server twice for previewing, and use server -s during the
preview</li>
</ul>
<h1 id="several-choices-for-hexo-writing-windows">Several Choices for
Hexo Writing (Windows)</h1>
<ul>
<li><p>MarkFlyElephant: Here, the Chrome app version of MarkFlyElephant
is recommended, which can be opened offline. Advantages: The interface
is simple and clear, editing is free, the preview function is complete,
common operations (bold text, inserting images, links, code blocks,
quotes) are convenient, and it can be bound to Evernote for article
backup. Disadvantages: The operation is a bit 麻烦, and the text needs
to be copied back. It is not convenient to open multiple articles, free
trial for 10 days, 78 yuan per year, and it looks like this: <img data-src="https://s1.ax1x.com/2018/10/20/i078zV.jpg"
alt="i078zV.jpg" /></p></li>
<li><p>hexo admin plugin: Install the hexo-admin plugin for visual
management of blogs, installation method:</p>
<pre><code>npm install --save hexo-admin
hexo server -d
open http://localhost:4000/admin/</code></pre>
<p>http://localhost:4000/admin/* is a web management interface that can
manage articles, and it can also write articles, with real-time preview,
adding tags, etc., which is equivalent to extracting some functions and
making them into a GUI. **Advantages: multi-article management,
convenient operation, automatic saving; when editing, you can change the
font size and style according to the md syntax. Disadvantages: the
preview is not complete. It does not support mathjax mathematical
formula preview, the degree of customization is not high; there is a
minor bug in editing; there is a small issue with the filename when
creating a new article; it is not very good at supporting Chinese**, and
it looks like this: <img data-src="https://s1.ax1x.com/2018/10/20/i073R0.jpg"
alt="i073R0.jpg" /></p></li>
<li><p>Recommend installing both, use a third-party editor for writing
new articles or articles that require inserted formula code, and use
hexo admin if it's just for regular typing or updating previously
written articles.</p></li>
<li><p>Typora: There are many markdown writing software options, most of
which allow for editing and previewing simultaneously. Here, I highly
recommend a minimalist design software that advocates for the "what you
see is what you get" concept: Typora. **Advantages: What you see is what
you get, desktop software, complete features, and a convenient
experience similar to Word (especially convenient for inserting tables
and images). Disadvantages: Does not support synchronization and backup,
HTML needs to be designated in a separate area, and editing is slightly
less free.** It looks like this: <img data-src="https://s1.ax1x.com/2018/10/20/i071Gq.jpg"
alt="i071Gq.jpg" /></p></li>
<li><p>Mac systems are recommended to use Mou or Sublime online, I have
not used them and do not make any evaluations.</p></li>
<li><p>Always have Notepad++ on hand; there's no problem that Notepad++
can't solve.</p></li>
</ul>
<h1 id="topic-setting">Topic Setting</h1>
<ul>
<li>Generally, Hexo theme creators will open-source their themes on
GitHub, and you can directly clone them to your local machine using the
command, then change the theme in config.yml to this theme name.</li>
<li>Each topic has its own config.yml file, used for further
configuration of each topic, which can be modified according to the
author's instructions or the readme on GitHub.</li>
</ul>
<h1 id="writing-grammar">Writing Grammar</h1>
<ul>
<li>Grammar supports Markdown, can use HTML, LaTeX, and other grammars,
but must be declared at the beginning of the article</li>
</ul>
<h1 id="insert-image">Insert Image</h1>
<ul>
<li>Insert images are divided into local and online link insertion; it
is recommended to use online links</li>
<li>My method is to apply for a Qiniu Cloud account, create a new
storage space, obtain the domain name, ak, sk, then install an extension
in Chrome, Minimalist Image Hosting, configure the domain name, ak, sk
obtained just now in this extension, and then directly click to upload
the image, which is it. The extension will automatically generate the
markdown statement for inserting the image or an online link, which can
be directly written into the text. MPic is the same.</li>
</ul>
<h1 id="insert-formula">Insert Formula</h1>
<ul>
<li><p>Surround the formula with a pair of $$, grammatical support for
LaTeX</p></li>
<li><p>must declare in the beginning of the article like mathjax:
true</p></li>
<li><p>Because hexo first uses marked.js for preprocessing and then
processes it with MathJax, the backslash \ may be escaped as \, which
can lead to incorrect display of formulas. There are two solutions: type
two backslashes more, or the other solution is to modify the marked.js
file, ignore the escaping, and you can search for the method on
Baidu.</p></li>
<li><p>Suggest replacing CDN to accelerate:</p>
<pre><code>mathjax:
enable: true
per_page: true
cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</code></pre></li>
</ul>
<h1 id="tags-and-classification">Tags and Classification</h1>
<ul>
<li>The difference between tags and categories lies in that one article
may be assigned to a single category, but may have multiple tags</li>
<li>For example, an article on machine learning is categorized under
machine learning, but it can have multiple tags such as code,
mathematics, and originality</li>
<li>categories: Category Name</li>
<li>tags: tag_name or tags: [tag1, tag2,...], note not to use Chinese
commas</li>
</ul>
<h1 id="other-beautification">Other beautification</h1>
<ul>
<li>I use the Next theme, which allows settings like background, and a
quick search on Baidu will find the methods</li>
<li>Change the font according to the theme. In "next," the font is
changed directly in the "config," using Google Fonts. Due to issues with
accessing it in China, the host uses "//fonts.css.network."</li>
</ul>
<h1 id="home-article-quantity-setting">Home Article Quantity
Setting</h1>
<ul>
<li><p>Install plugin</p>
<pre><code>  npm install --save hexo-generator-index
  npm install --save hexo-generator-archive
  npm install --save hexo-generator-tag</code></pre></li>
<li><p>Add the following field to the site configuration file
config.yml</p>
<pre><code>index_generator:
per_page: 5

archive_generator:
per_page: 20
yearly: true
monthly: true

tag_generator:
per_page: 10</code></pre></li>
<li><p>index, archive, and tag prefixes represent the homepage, archive
page, and tag page respectively.</p></li>
</ul>
<h1 id="add-comment-feature">Add comment feature</h1>
<ul>
<li>Subject-dependent, I use the Next theme, which supports Duoshuo by
default; you only need to write your own duoshou_shortname in the theme
configuration file</li>
<li>Nowadays, it's best to shut down those whatnots and open a
background space on LeanCloud instead. You can use gitcomment, Disqus,
or Liker. Currently, I use Valine, mainly because it allows comments
without the need for login</li>
</ul>
<h1 id="optimization-plugin">Optimization Plugin</h1>
<ul>
<li>hexo-generated html files have many redundancies; here, it is
recommended to install a plugin to compress files, improving efficiency
hexo-all-minifier</li>
</ul>
<h1 id="top">Top</h1>
<ul>
<li>Thank you Netcan_Space for providing the solution, hoping the
official theme will add this feature: Add Hexo sticky feature</li>
</ul>
<h1 id="cnpm">cnpm</h1>
<ul>
<li>Use Taobao mirror to install plugins for faster speed, details on
Baidu cnpm installation</li>
</ul>
<h1 id="reactive-systems-science">Reactive Systems Science</h1>
<ul>
<li>Using hexo-generator-feed for rss: hexo-generator-feed</li>
</ul>
<h1 id="remote-recovery">Remote Recovery</h1>
<ul>
<li>Recently reinstalled the system, restored the local blog, but after
cloning from remote and operating locally with hexo, many problems were
found, summarized as follows
<ul>
<li>clone after directly installing dependencies, no need for hexo init,
otherwise it will affect the blog's config.yml</li>
<li>This time, the blog was actually completely rebuilt because there
were issues with importing the previous settings. It was later
discovered that there was no theme in the local blog backup at all!
Because my theme was cloned from someone else's repository, and the
entire blog was backed up through git, one repository cannot contain
another, so the theme and all settings were never backed up. In the
future, the .git folder under the theme directory should be
deleted.</li>
<li>Remember to install hexo-deployer-git</li>
<li>next has been updated, but some dependencies for the new features
still need to be checked, and you need to run npm install yourself</li>
<li>Found that the previous blog was too flashy, so this time I simply
removed all the additional features, focusing on writing</li>
<li>The above 2 figures are outdated; you can check the file directory
of the blog in my GitHub</li>
<li>There are many modules that have not been backed up due to their
long names, and I feel that Baidu Cloud is more reliable</li>
<li>Upgrade MathJax to 2.7.5</li>
<li>Adopt hexo-renderer-kramed and modify the inline.js escaping</li>
</ul></li>
</ul>
<h1 id="this-big-shot-is-reliable">This big shot is reliable</h1>
<ul>
<li>Website here: HEXO Website Memo</li>
<li>Mentioned is an excellent version control tool that resolves the
issue of repositories under themes, with a direct introduction available
at hexo-git-backup</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="前提">前提</h1>
<ul>
<li>安装好Node.js</li>
<li>安装好git</li>
<li>安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。</li>
<li>可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)</li>
</ul>
<h1 id="githubhexo初始化">GitHub&amp;Hexo初始化</h1>
<p>选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些
最后的过程是这样的</p>
<ul>
<li>注册github账号，这个就省略了</li>
<li>新建一个repository，命名必须为“你的账户名.github.io"</li>
<li>对这个repository新建一个hexo分支并把hexo设置为默认分支</li>
<li>git bash cd到你本地新建的一个文件夹（用于存放博客项目）</li>
<li>依次执行 npm install hexo、hexo init、npm install、npm install
hexo-deployer-git</li>
<li>此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到
_config.yml，修改deploy参数为master, 见后文</li>
<li>此时你的网址就是：https://你的账户名.github.io/</li>
<li>现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件</li>
</ul>
<p><strong>之后所有的命令都是在Git
Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支</strong></p>
<h1 id="hexo配置">Hexo配置</h1>
<p>Hexo博客的目录格式如下</p>
<p>├── _config.yml ├── package.json ├── scaffolds ├── source | ├──
_drafts | └── _posts └── themes</p>
<ul>
<li>config.yml在这里设置博客的总体参数，博客名，url，git什么的</li>
<li>_posts里面存放你的文章</li>
<li>theme顾名思义，存放博客界面主题</li>
<li>在config.yml中关键配置以下几个参数
<ul>
<li>title: 博客网站标题</li>
<li>subtitle: 副标题</li>
<li>description: 一句话简介</li>
<li>author: 作者名</li>
<li>language: zh-Hans
语言根据你选择的主题看，在主题目录的language里查看支持哪些语言</li>
<li>timezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区</li>
<li>url: https://你的用户名.github.io/</li>
<li>type: git</li>
<li>repo: https://github.com/你的用户名/你的用户名.github.io.git</li>
<li><strong>branch: master</strong></li>
</ul></li>
</ul>
<h1 id="更新hexo项目">更新Hexo项目</h1>
<p>依次执行</p>
<ul>
<li>git add . (检查所有文件是否更新)</li>
<li>git commit -m "更新报告" (提交commit)</li>
<li>git push origin hexo (上传更新到github)
<strong>第一次更新可能出现一些错误</strong></li>
<li>第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错</li>
<li>permission denied
去博客目录/.git/config中，找到Url,将ssh链接改成html格式：https://www.github.com/你的用户名/你的用户名.github.io.git</li>
<li>refusing to merge unrelated histories
因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull
origin hexo --allow-unrelated-histories，之后再ush
<strong>所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的</strong></li>
</ul>
<h1 id="hexo-写博客及更新网页">Hexo 写博客及更新网页</h1>
<p>主要命令如下</p>
<ul>
<li>hexo clean 清除缓存和静态文件</li>
<li>hexo g或者hexo generate 生成框架文件
建议每次更新网站之前先clean再generate</li>
<li>hexo s或者hexo server
打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭</li>
<li>hexo new"文件名"
新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了</li>
<li>hexo d或者hexo
deploy，写好博客，clean&amp;generate之后，部署博客到GitHub
Pages上，就更新了博客网页 <strong>其中有几点要注意</strong></li>
<li>第一次新建目录后执行#npm install hexo-deployer-git
--save安装git分发，否则上传不了</li>
<li>hexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格</li>
<li>我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server
-s</li>
</ul>
<h1 id="hexo写作的几种选择windows">Hexo写作的几种选择(Windows)</h1>
<ul>
<li><p><strong>马克飞象</strong>:这里推荐用马克飞象的chrome
app版本，可以离线打开。<strong>优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。
多个文章打开不方便，免费试用10天，78元一年</strong>,它长这个样子： <img data-src="https://s1.ax1x.com/2018/10/20/i078zV.jpg"
alt="i078zV.jpg" /></p></li>
<li><p><strong>hexo
admin插件</strong>:安装hexo-admin插件，可视化管理博客，安装方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-admin</span><br><span class="line">hexo server -d</span><br><span class="line">open http://localhost:4000/admin/</span><br></pre></td></tr></table></figure>
<p><em>http://localhost:4000/admin/</em>就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，<strong>优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好</strong>,它长这个样子：
<img data-src="https://s1.ax1x.com/2018/10/20/i073R0.jpg"
alt="i073R0.jpg" /></p></li>
<li><p>推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo
admin。</p></li>
<li><p><strong>Typora</strong>:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：<strong>Typora</strong>。<strong>优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。</strong>他长这个样子：
<img data-src="https://s1.ax1x.com/2018/10/20/i071Gq.jpg"
alt="i071Gq.jpg" /></p></li>
<li><p>Mac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。</p></li>
<li><p>常备一个notepad++,出现什么问题没有notepad++解决不了的。</p></li>
</ul>
<h1 id="主题设置">主题设置</h1>
<ul>
<li>一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可</li>
<li>每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改</li>
</ul>
<h1 id="写作语法">写作语法</h1>
<ul>
<li>语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明</li>
</ul>
<h1 id="插入图片">插入图片</h1>
<ul>
<li>插入图片分本地和在线链接插入，推荐使用在线链接</li>
<li>我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。</li>
</ul>
<h1 id="插入公式">插入公式</h1>
<ul>
<li><p>用一对$$将公式围起来，语法支持latex</p></li>
<li><p>必须在文章首部中声明如mathjax: true</p></li>
<li><p>因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\可能被转义为，从而导致公式显示不正确，解决方法有两种，多打两个\，另外一种是修改marked.js文件，无视转义，可以自行百度方法</p></li>
<li><p>建议更换cdn以加速:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">enable: true</span><br><span class="line">per_page: true</span><br><span class="line">cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="标签与分类">标签与分类</h1>
<ul>
<li>标签与分类的区别在于1篇文章归于一类，但可能有多个标签</li>
<li>比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签</li>
<li>分类需在首部加上categories: 分类名</li>
<li>标签需在首部加上tags: 标签名或者tags:
[标签1,标签2....],<strong>注意别打成中文逗号</strong></li>
</ul>
<h1 id="其他美化">其他美化</h1>
<ul>
<li>我用的是next主题，可以设置背景之类的，百度一下方法就能找到</li>
<li>更换字体，视主题而定，next里面是用config里直接改字体，用的是google
fonts,由于国内访问有问题，所以host用//fonts.css.network</li>
</ul>
<h1 id="首页文章数量设置">首页文章数量设置</h1>
<ul>
<li><p>安装插件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-generator-index</span><br><span class="line">npm install --save hexo-generator-archive</span><br><span class="line">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure></li>
<li><p>在站点配置文件config.yml中添加如下字段</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">index<span class="emphasis">_generator:</span></span><br><span class="line"><span class="emphasis">per_</span>page: 5</span><br><span class="line"></span><br><span class="line">archive<span class="emphasis">_generator:</span></span><br><span class="line"><span class="emphasis">per_</span>page: 20</span><br><span class="line">yearly: true</span><br><span class="line">monthly: true</span><br><span class="line"></span><br><span class="line">tag<span class="emphasis">_generator:</span></span><br><span class="line"><span class="emphasis">per_</span>page: 10</span><br></pre></td></tr></table></figure></li>
<li><p>index,
archive及tag开头分表代表主页，归档页面和标签页面。</p></li>
</ul>
<h1 id="添加评论功能">添加评论功能</h1>
<ul>
<li><del>视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可</del></li>
<li>现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论</li>
</ul>
<h1 id="优化插件">优化插件</h1>
<ul>
<li>hexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率
<a
href="https://github.com/chenzhutian/hexo-all-minifier">hexo-all-minifier</a></li>
</ul>
<h1 id="置顶">置顶</h1>
<ul>
<li>感谢Netcan_Space提供解决方案，希望官方theme加入此功能：<a
href="http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/">添加Hexo置顶功能</a></li>
</ul>
<h1 id="cnpm">cnpm</h1>
<ul>
<li>使用淘宝镜像安装插件提速，详情百度cnpm安装</li>
</ul>
<h1 id="rss">RSS</h1>
<ul>
<li>使用hexo-generator-feed使用rss:<a
href="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a></li>
</ul>
<h1 id="异地恢复">异地恢复</h1>
<ul>
<li>最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下
<ul>
<li>clone之后直接安装依赖项，不需要hexo
init，否则会情况博客的config.yml</li>
<li>这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除</li>
<li>记得安装hexo-deployer-git</li>
<li>next更新了，但是新功能的一些依赖还是需要看注释，自己npm install</li>
<li>发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作</li>
<li>上面那2张图已经过时了，可以去我的github里看博客的文件目录</li>
<li>有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱</li>
<li>升级mathjax到2.7.5</li>
<li>改用hexo-renderer-kramed，并修改inline.js转义</li>
</ul></li>
</ul>
<h1 id="这位大佬靠谱">这位大佬靠谱</h1>
<ul>
<li>网址在这：<a
href="https://www.vincentqin.tech/posts/build-a-website-using-hexo/">HEXO建站备忘录</a></li>
<li>里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍<a
href="https://github.com/coneycode/hexo-git-backup">hexo-git-backup</a></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>Edit-based Text Generation</title>
    <url>/2021/05/11/text-edit-generation/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/c762f093907fbcdae6c6077555cb0552.png" width="500"/></p>
<ul>
<li>Record the methods of editing seq2seq in recent years, which have
the advantages of high efficiency (partially autoregressive or
non-autoregressive decoding) and less data hungry (small output
vocabulary) for tasks with the same language input and output and minor
changes (error correction, simplification, summarization).</li>
<li>Mainly read five papers, sorted by their publication date on arXiv:
<ul>
<li>(LevT, Facebook) Levenshtein Transformer</li>
<li>(Huawei) EditNTS: An Neural Programmer-Interpreter Model for
Sentence Simplification through Explicit Editing</li>
<li>(LaserTagger, Google) Encode, Tag, Realize: High-Precision Text
Editing</li>
<li>(PIE) Parallel Iterative Edit Models for Local Sequence
Transduction</li>
<li>(Google) Felix: Flexible Text Editing Through Tagging and
Insertion</li>
</ul></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="levenshtein-transformer">Levenshtein Transformer</h1>
<ul>
<li>Contributions
<ul>
<li>Proposed an edit-based transformer using insertion and deletion
operations, improving efficiency by 5 times.</li>
<li>Designed a dual-policy reinforcement learning training method,
addressing the complementarity of insertion and deletion.</li>
<li>Unified the tasks of text generation and text refinement.</li>
</ul></li>
</ul>
<h2 id="problem-definition">Problem Definition</h2>
<ul>
<li><p>The authors unify text generation and text refinement into a
single Markov process, defined by the tuple (Y, A, <span
class="math inline">\(\xi\)</span>, R, <span
class="math inline">\(y_0\)</span>) (sequence, action set, environment,
reward function, initial sequence).</p>
<ul>
<li>Y represents the text sequence, with size <span
class="math inline">\(V^N\)</span>.</li>
<li><span class="math inline">\(\xi\)</span> is the environment. At each
decoding step, the agent receives a sequence y as input, takes an action
a, and then receives a reward r.</li>
<li>The reward function is defined as the distance between the current
sequence and the ground truth sequence, using Levenshtein distance (edit
distance) in this case.</li>
<li>The policy the agent learns is a mapping from sequence y to a
probability distribution over actions P(A).</li>
<li>It is worth noting that <span class="math inline">\(y_0\)</span> can
be an empty sequence, representing text generation, or a previously
generated sequence, representing text refinement.</li>
</ul></li>
<li><p>Actions</p>
<ul>
<li>Deletion operation: For each token in the sequence, perform binary
classification to decide whether it should be deleted. The first and
last tokens are retained without deletion checks to avoid breaking the
sequence boundaries.</li>
<li>Insertion operation: The insertion operation is performed in two
steps: first, predict the insert position by performing a multi-class
classification on each pair of adjacent token positions (the number of
insert positions). It is important to note that the input includes these
two tokens. Then, for each predicted position, a classification is done
over a vocabulary V to generate the specific word to insert.</li>
</ul></li>
<li><p>The entire process is divided into three steps: input sequence
<span class="math inline">\(y_0\)</span>, perform deletion operations in
parallel at all positions in the sequence to obtain <span
class="math inline">\(y_1\)</span>; for <span
class="math inline">\(y_1\)</span>, predict insert positions at all
positions in the sequence, add placeholders according to the predictions
to obtain <span class="math inline">\(y_2\)</span>; for all placeholders
in <span class="math inline">\(y_2\)</span>, predict specific words to
generate the final result, completing one iteration.</p></li>
<li><p>The three operations share a single transformer, and to reduce
complexity (turning one autoregressive text generation round into three
non-autoregressive operations), the classifiers for deletion and insert
position predictions are attached to the middle transformer blocks. The
classifier for text generation is only attached to the final transformer
block, as the first two tasks are relatively simple and do not require
complex feature extraction.</p></li>
</ul>
<h2 id="training">Training</h2>
<ul>
<li><p>The training process follows imitation learning. While there are
many reinforcement learning concepts, it essentially involves perturbing
the data and having the model learn to recover, still following Maximum
Likelihood Estimation (MLE). I will attempt to explain its specific
operations in the context of NLP tasks:</p>
<ul>
<li><p>The training still uses teacher forcing, meaning the model
directly learns to predict the oracle (ground truth) from the input,
without autoregression.</p></li>
<li><p>For Levenshtein Transformer, three components are needed during
training:</p>
<ul>
<li>Training input: The input is the text requiring edit operations
(state distribution fed during training). These sentences should be
original texts that have been disturbed by perturbation operations
(roll-in policy). This is easy to understand—e.g., deleting some tokens
and then having the model learn to insert them. The construction of this
input involves two steps: determining the original text and applying
perturbation operations. The original text can either be empty or the
ground truth. In the design of the perturbation operations, the authors
introduce the so-called dual policy, which essentially means allowing
the model’s own operations to also serve as perturbations.</li>
</ul>
<table>
<colgroup>
<col style="width: 39%" />
<col style="width: 24%" />
<col style="width: 21%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Perturbation Type</th>
<th>Learn from Model</th>
<th>Dual Policy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Randomly drop words from oracle text as input</td>
<td>Deletion</td>
<td>Insertion</td>
<td></td>
</tr>
<tr class="even">
<td>Directly use oracle input text as input</td>
<td>Insertion</td>
<td>Deletion</td>
<td></td>
</tr>
<tr class="odd">
<td>Use the text after model deletion as input</td>
<td>Deletion</td>
<td>Insertion</td>
<td>√</td>
</tr>
<tr class="even">
<td>Use the text after model insertion as input</td>
<td>Insertion</td>
<td>Deletion</td>
<td>√</td>
</tr>
</tbody>
</table>
<p>The authors use probabilities to choose the operations for
constructing data, as shown in the diagram below:</p>
<p><a href="https://imgtu.com/i/gaoEPx"><img data-src="https://z3.ax1x.com/2021/05/11/gaoEPx.png"
alt="gaoEPx.png" /></a></p>
<ul>
<li>Training output: After learning the opposite operations of the
perturbation, the model needs to restore the original text. Once the
input and output are determined, the ground truth actions (expert
policy) are set. Generally, the model should restore the oracle, but
since direct restoration can be difficult, the model is trained to
restore a low-noise version with a certain probability. This low-noise
version is derived from sequence distillation. Specifically, a normal
transformer is trained on a unified dataset, and this transformer is
used to infer each piece of data, using beam search to find non-top-1
sentences as low-noise versions for the model to learn to restore.</li>
<li>Ground truth actions during training: Once the input and output are
determined, the minimum edit distance operation is computed using
dynamic programming as the ground truth action (expert policy) for the
model to learn.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="inference">Inference</h2>
<ul>
<li>During inference, the model performs multiple rounds of editing on
the input text (each round includes one deletion, prediction of
position, and insertion), greedily selecting the operation with the
highest probability.</li>
<li>Multiple rounds of editing continue until:
<ul>
<li>A loop occurs (i.e., the same text reappears after editing),</li>
<li>A preset maximum number of rounds is reached.</li>
</ul></li>
<li>Too many placeholders can lead to overly short generated text, and a
penalty term can be added to control the number of placeholders.</li>
<li>The use of two editing operations adds a degree of interpretability
to the model’s inference.</li>
</ul>
<h2 id="experiments">Experiments</h2>
<ul>
<li>The evaluation metrics include six items: BLEU scores for three
translation pairs and ROUGE-1, ROUGE-2, and L values for the Gigaword
summarization task. For English-Japanese translation and Gigaword, the
original Transformer achieved the best results, outperforming LevT by
about one point. For the other two translation datasets, the
sequence-distilled LevT outperformed the original Transformer by about
one point.</li>
<li>For LevT, training with sequence distillation is generally better
than using the original oracle sequence.</li>
<li>In terms of inference speed, the LevT with sequence distillation is
the fastest, with single sentence inference around 90ms, while the
original Transformer takes 200-300ms for translation and 116ms for
summarization tasks. The authors also report the number of inference
iterations, showing that LevT requires an average of only 2 iterations,
i.e., two deletions and two insertions, applied to the entire sentence.
In contrast, the original Transformer’s average number of iterations is
proportional to the length of the sentence. For short summarization
datasets, the average number of iterations is 10.1, while for
translation datasets, it reaches over 20. This demonstrates another
advantage of LevT: its inference speed is not overly sensitive to text
length and is primarily related to the number of iterations required for
processing the entire sentence. The growth rate of iterations with
length is smaller than the logarithmic complexity and far smaller than
the quadratic complexity of traditional Transformers. The inference
latency of the Transformer scales roughly linearly with text length,
while LevT remains around 90ms with just two iterations.</li>
<li>Ablation studies show that the design of the two operations,
parameter sharing, and the dual policy all contribute to better final
metrics.</li>
<li>For deletion and insert position prediction, early exiting after the
first transformer block yields good results, with BLEU dropping 0.4 but
speeding up inference by a factor of 3.5 to 5 times compared to using
the sixth layer.</li>
<li>Additionally, the authors found that a LevT trained on translation
tasks can be used zero-shot for text refinement, with performance within
0.3 BLEU points of a separately trained text refinement LevT, and even
better performance on Ro-En. Both outperform traditional Transformers.
This adaptability to text refinement aligns with intuition, as text
refinement involves smaller edits compared to generating new text, where
the hypothesis space is larger and more difficult to reach an ideal
result. After fine-tuning on a text refinement dataset, LevT
outperformed all models comprehensively.</li>
</ul>
<h1 id="editnts">EditNTS</h1>
<ul>
<li><p>A sentence simplification model is proposed that explicitly
performs three types of operations: insertion, deletion, and
retention.</p></li>
<li><p>Unlike LevT, EditNTS remains autoregressive, meaning it does not
operate on the entire sentence simultaneously, and it introduces more
information when predicting operations.</p></li>
<li><p>The model consists of two parts: an LSTM-based seq2seq model as
the <em>programmer</em> that predicts the operation for each token
position, generating a sequence of editing operations; and an additional
<em>interpreter</em> that executes these operations to generate the
edited sentence. The interpreter also includes a simple LSTM that
aggregates the contextual information of the edited sentence. <a
href="https://imgtu.com/i/gdTVa9"><img data-src="https://z3.ax1x.com/2021/05/12/gdTVa9.png"
alt="gdTVa9.png" /></a></p></li>
<li><p>Three key points to note:</p>
<ul>
<li>After predicting deletion and retention, the programmer moves to the
next word. However, when predicting an insertion operation, the
programmer stays on the current word and continues predicting the next
operation, which enables the insertion of multiple words.</li>
<li>The interpreter not only performs the editing based on the
programmer’s predicted results, but also uses an LSTM to aggregate the
contextual information of the sentence at the current time step after
editing.</li>
<li>When predicting editing operations, the programmer uses four pieces
of information:
<ul>
<li>The encoder’s context, which comes from the last-layer
attention-weighted output of the encoder LSTM.</li>
<li>The word being processed, which comes from the decoder LSTM’s hidden
state.</li>
<li>The already edited sentence’s context, which comes from the
interpreter LSTM’s attention-weighted output.</li>
<li>The already generated editing operation sequence, which comes from
the attention-weighted edit label output. Here, a simple attention
mechanism is used, not an LSTM.</li>
</ul></li>
</ul></li>
<li><p>The encoder input also introduces position embedding.</p></li>
<li><p>Label construction, similar to LevT, involves creating a ground
truth sequence of editing operations based on an edit distance dynamic
programming approach. When multiple operation sequences exist, the
sequence with more insertions is prioritized. The authors experimented
with other prioritization strategies (e.g., prioritizing deletions,
random, introducing replacement operations), but found that prioritizing
insertions yielded the best results.</p></li>
<li><p>The model uses 100-dimensional GloVe vectors to initialize word
vectors and edit label embeddings, 30-dimensional vectors to initialize
position embeddings, and uses inverse frequency of the edit labels as
loss weights to balance the loss contributions from each label.</p></li>
<li><p>The results are not very impressive. In terms of metrics, the
model does not outperform non-editing methods, and while human
evaluation shows some improvement, it is subjective. Ablation studies do
not show significant improvements.</p></li>
</ul>
<h1 id="lasertagger">LaserTagger</h1>
<ul>
<li>LaserTagger also uses the three operations: retention, deletion, and
insertion, but it incorporates BERT, with Google's engineering
capabilities enabling an acceleration factor of 100-200 times.</li>
<li>Contributions:
<ul>
<li>Introduced the model and proposed a method for generating label
dictionaries from data.</li>
<li>Proposed two versions based on BERT: one directly uses BERT for
tagging, which is faster, and the other uses BERT to initialize a
seq2seq encoder, which yields better results.</li>
</ul></li>
</ul>
<h2 id="text-editing-as-tagging">Text Editing as Tagging</h2>
<ul>
<li>LaserTagger transforms the text editing problem into a sequence
labeling problem, which consists of three main components:
<ul>
<li><strong>Labeling Operations</strong>: Instead of using the three
basic operations (retain, delete, insert), the labels are divided into
two parts: a <em>base tag</em> (B), which includes <em>retain</em> and
<em>delete</em> operations, and an <em>added phrase</em> (P), which can
take <em>V</em> different values representing inserted fragments (empty
space, words, or phrases). The vocabulary for P is built from the
training data. The combination of B and P generates the labels, so there
are a total of 2V possible labels. Task-specific labels can also be
added based on downstream tasks.</li>
<li><strong>Building the Added Phrase Vocabulary</strong>: Constructing
the vocabulary is a combinatorial optimization problem. On one hand, we
want the vocabulary to be as small as possible; on the other, we want it
to cover the most editing scenarios. Finding the perfect solution is
NP-hard, so the authors used another approach. For each training pair,
they compute the longest common subsequence (LCS) between the input and
the target sequence using dynamic programming. The added phrases are
derived by subtracting the LCS from the target sequence. These phrases
are then ranked by frequency, and the top-k phrases are selected. For
public datasets, the top 500 phrases cover 85% of the data.</li>
<li><strong>Constructing the Label Sequence</strong>: This is done using
a greedy approach, matching the shortest phrase to insert for each word,
without the need for dynamic programming. <a
href="https://imgtu.com/i/gdxc0x"><img data-src="https://z3.ax1x.com/2021/05/12/gdxc0x.png" alt="gdxc0x.png" /></a>
This approach may sometimes result in cases where no edits are made,
which are then filtered out from the training set. The authors argue
that this can also be seen as a form of noise reduction.</li>
<li><strong>Editing the Sentence Based on Predictions</strong>: This is
the direct operation where, based on downstream tasks, special labels
may be predicted to perform task-specific operations. The authors argue
that modularizing the editing operations makes the model more flexible
than an end-to-end approach.</li>
</ul></li>
</ul>
<h2 id="experiment">Experiment</h2>
<ul>
<li>There are two versions of the model: one directly uses BERT for
tagging, and the other uses seq2seq with a BERT-initialized encoder. In
the first case, a transformer block is added to BERT’s last layer to
perform the tagging.</li>
<li>Four tasks were tested: sentence fusion, sentence splitting and
rephrasing, summarization, and grammar correction. All tasks
outperformed the baseline, with sentence fusion achieving new SOTA
results. However, LaserTagger's standout performance lies in its
efficiency, achieving up to 200 times acceleration with just the BERT
encoder (since the text generation is reduced to a sequence labeling
task with a small dictionary). It is also very friendly for few-shot
learning.</li>
<li>The authors also analyzed common issues with seq2seq models and
editing-based models, their respective advantages, and the causes of
these problems:
<ul>
<li><strong>Imaginary Words</strong>: seq2seq models may generate
non-existent words at the subword level, whereas LaserTagger does not
have this problem.</li>
<li><strong>Premature Termination</strong>: seq2seq models may
prematurely generate EOS tokens, resulting in incomplete or overly short
sentences. While this is theoretically possible for LaserTagger, the
authors have not observed it in practice, as it would require predicting
many deletion operations, which are rare in the training data.</li>
<li><strong>Repetitive Phrases</strong>: In sentence splitting tasks,
seq2seq models often repeat phrases, while LaserTagger tends to avoid
splitting or uses a "lazy split."</li>
<li><strong>Hallucination</strong>: Both models may generate text that
is unrelated or counterfactual, but seq2seq models tend to hide this
issue more effectively (appearing fluent but actually illogical), while
LaserTagger is more transparent.</li>
<li><strong>Coreference Resolution</strong>: Both models struggle with
coreference resolution. Seq2seq models may incorrectly replace pronouns
with the wrong noun, while LaserTagger may fail to resolve pronouns
properly.</li>
<li><strong>Error Deletion</strong>: While LaserTagger appears more
controllable in terms of text generation, it still sometimes deletes
words, leaving the remaining sentence fluent but incorrect in
meaning.</li>
<li><strong>Lazy Split</strong>: In sentence splitting tasks,
LaserTagger might only split the sentence without performing any
post-processing on the resulting sentences.</li>
</ul></li>
</ul>
<h1 id="pie">PIE</h1>
<ul>
<li>PIE also combines BERT with sequence labeling, while constructing a
phrase dictionary from the data.</li>
<li>The editing operations include copying (retaining), insertion,
deletion, and morphological transformation (for grammar
correction).</li>
<li>Constructing the phrase dictionary, ground truth editing operations,
and the method are quite similar to LaserTagger.</li>
<li>BERT is extended on both sides with two additional components to
obtain the information required for replacement or insertion:
<ul>
<li>The input layer includes M for mask identifiers (embedding), p for
positional embeddings, and X for word embeddings.</li>
<li>h represents the original BERT information, containing both word and
positional information.</li>
<li>r represents replacement information, where the word at the current
position is masked and replaced by M, and attention is not applied to
the current word's h during the calculation.</li>
<li>a represents insertion information, where the current word is masked
and replaced by M, and the positional embedding p is replaced by the
average positional embedding of neighboring tokens. <a
href="https://imgtu.com/i/gwZarT"><img data-src="https://z3.ax1x.com/2021/05/12/gwZarT.png"
alt="gwZarT.png" /></a></li>
<li>Afterward, the three types of information are used to compute the
probabilities for different operations, which are then normalized. CARDT
represents the operations: copy (retain), insert, replace, delete, and
morphological transformation. <a href="https://imgtu.com/i/gwZ5IH"><img data-src="https://z3.ax1x.com/2021/05/12/gwZ5IH.png"
alt="gwZ5IH.png" /></a></li>
<li>The first term in the formula is the score for each editing
operation. The second term is the score for retaining the current word,
with deletion and replacement not contributing to the score. Other
operations are parameterized using the word embedding <span
class="math inline">\(\phi(x_i)\)</span>, though the exact meaning of
<span class="math inline">\(\phi\)</span> is unclear. The third term
represents the effect of new words, which is only relevant for
replacement and insertion.</li>
</ul></li>
<li>Inference also involves multiple rounds of editing (output becoming
input), stopping when a repeated sentence is produced.</li>
<li>The motivation behind the design of the editing operations is not
fully clear, and the final results are quite average. Only under
ensemble settings does PIE perform on par with a Transformer +
Pre-training + LM + Spellcheck + Ensemble Decoding model. Ablation
studies show that pre-trained models are the main contributor to
performance improvement.</li>
</ul>
<h1 id="felix">Felix</h1>
<ul>
<li>Felix divides the editing operations into two non-autoregressive
parts: first, a Transformer-based pointer network performs tagging and
reordering, inserting placeholders; second, a masked language model
(MLM) predicts the words for the placeholders.</li>
<li>The design of Felix considers three needs:
<ul>
<li>Flexible editing, suitable for various text generation tasks.</li>
<li>Efficient utilization of pre-trained models like BERT.</li>
<li>Efficient inference. <a href="https://imgtu.com/i/gwKovR"><img data-src="https://z3.ax1x.com/2021/05/12/gwKovR.png"
alt="gwKovR.png" /></a></li>
</ul></li>
<li>The diagram below clearly shows the tagging design of Felix, where
<em>y^t</em> represents the editing operation sequence predicted by the
tagging model, and <em>y^m</em> represents the intermediate sequence
after inserting special tokens (REPL, MASK) based on the editing
operation sequence. This intermediate sequence is then fed to the MLM to
predict the words that need to be inserted, resulting in the final
prediction. <a href="https://imgtu.com/i/gwMHzj"><img data-src="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png"
alt="gwMHzj.md.png" /></a></li>
<li>Two points to note:
<ul>
<li>Although the MLM only predicts the insertion of words, it still
requires the full editing operation information. Instead of simply
deleting the words predicted for deletion, the authors use REPL to
bracket the span that should be deleted, providing the MLM with
information about the deletions.</li>
<li>The diagram shows two forms of Mask and Infill, which involve
different masking strategies for the intermediate sequence and deciding
who is responsible for tasks like multi-token insertion. In the Mask
approach, the task is handled by the tagging model, which directly
predicts multi-word insertions (e.g., <span
class="math inline">\(DEL^{INS\_2}\)</span> for inserting two words,
corresponding to two MASK tokens in the intermediate sequence). In the
Infill approach, the task is handled by the MLM, where four MASK tokens
are prepared for each insertion position, with extra tokens predicted as
PAD.</li>
</ul></li>
<li>To model more flexibly, the tagging model also performs reordering,
as swapping two words requires first deleting and then adding them,
which increases complexity. The reordering is handled using pointer
network attention, which directs each word to its subsequent word, with
the first word indicated by a special CLS token. During inference,
controlled beam search is used to avoid cyclical pointer sequences.</li>
<li>Compared to LaserTagger, Felix performs better in terms of
performance and small sample learning, and it does not suffer from the
limitations of phrase vocabulary. The authors also performed experiments
on phrase vocabulary and reordering, with results available in the
original paper.</li>
</ul>
<h1 id="summary">Summary</h1>
<p>Here's a summary of the key models with their editing operations,
performance, and tasks:</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 12%" />
<col style="width: 14%" />
<col style="width: 18%" />
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Editing Operations</strong></th>
<th><strong>Acceleration</strong></th>
<th><strong>Multiple Word Insertion</strong></th>
<th><strong>Ground Truth Construction</strong></th>
<th><strong>Editing Failure</strong></th>
<th><strong>Test Tasks</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LevT</strong></td>
<td>Insert, Delete</td>
<td>Multi-round non-autoregressive, 5x</td>
<td>Predict placeholder count first, then replace</td>
<td>Dynamic programming based on edit distance</td>
<td>Degrades to text generation</td>
<td>Translation, Summarization</td>
</tr>
<tr class="even">
<td><strong>EditNTS</strong></td>
<td>Insert, Delete, Retain</td>
<td>Autoregressive, acceleration not mentioned</td>
<td>Stay at position for insertion until done</td>
<td>Dynamic programming based on edit distance</td>
<td>Degrades to text generation</td>
<td>Text Simplification</td>
</tr>
<tr class="odd">
<td><strong>LaserTagger</strong></td>
<td>Insert, Delete, Retain</td>
<td>One-round sequence labeling, 100+ times</td>
<td>Direct insertion of phrases, phrase vocabulary built from training
data</td>
<td>Greedy matching</td>
<td>No editing (lazy)</td>
<td>Sentence Fusion, Sentence Splitting and Paraphrasing, Summarization,
Grammar Correction</td>
</tr>
<tr class="even">
<td><strong>PIE</strong></td>
<td>Insert, Delete, Replace, Copy, Morphological Transformation</td>
<td>Sequence labeling, 2x</td>
<td>Direct insertion of phrases, phrase vocabulary built from training
data</td>
<td>Greedy matching</td>
<td>Not specified</td>
<td>Grammar Correction</td>
</tr>
<tr class="odd">
<td><strong>Felix</strong></td>
<td>Delete, Insert, Retain, Reordering</td>
<td>Tagging + MLM, faster than LaserTagger</td>
<td>Use multiple masks for MLM to predict</td>
<td>Simple comparison</td>
<td>Poor MLM prediction</td>
<td>Sentence Fusion, Post-processing in Machine Translation,
Summarization, Text Simplification</td>
</tr>
</tbody>
</table>
<p>This table gives an overview of each model's characteristics,
strengths, and weaknesses across different editing operations and
tasks.</p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="levenshtein-transformer">Levenshtein Transformer</h1>
<ul>
<li>贡献点
<ul>
<li>提出了基于插入和删除操作的编辑式transformer，效率提升5倍</li>
<li>针对插入和删除互补性，设计了一种dual policy式的强化学习训练方法</li>
<li>统一了文本生成和文本完善（refinement）两个任务</li>
</ul></li>
</ul>
<h2 id="定义问题">定义问题</h2>
<ul>
<li>作者将文本生成和文本完善统一为一个马尔科夫过程，由元组(Y,A,<span
class="math inline">\(\xi\)</span>,R,<span
class="math inline">\(y_0\)</span>)定义（序列，动作集合，环境，奖励函数，初始序列）
<ul>
<li>Y即文本序列，大小为<span class="math inline">\(V^N\)</span></li>
<li><span
class="math inline">\(\xi\)</span>是环境，在解码的每一步，agent接收一个序列y作为输入，采取动作a，然后得到奖励r</li>
<li>奖励函数定义为当前序列和ground
truth序列之间的距离，显然本文就使用了Levenshtein距离（编辑距离）</li>
<li>agent要学习的策略即一个从序列y到动作概率分布P(A)的映射</li>
<li>值得注意的是，<span
class="math inline">\(y_0\)</span>可以是空序列，此时即文本生成，也可以是一个生成好的序列，此时的任务即文本完善</li>
</ul></li>
<li>动作
<ul>
<li>删除操作：对序列中的每一个token做二分类，决定是否应该删除。对于序列中第一个和最后一个token直接保留，不做删除操作的判断，避免序列边界被破坏</li>
<li>插入操作：插入操作分为两步：首先预测insert
position,对每两个相邻token位置做一个多分类（insert
position的数量），值得注意的是输入包含了这两个token；之后再针对每个预测位置做一个词典V大小的分类，生成具体的词插入。</li>
</ul></li>
<li>整个流程分为了三步：输入序列<span
class="math inline">\(y_0\)</span>，并行的对序列所有位置做删除操作，得到<span
class="math inline">\(y_1\)</span>；对<span
class="math inline">\(y_1\)</span>，对序列所有位置预测insert
position，根据预测结果添加placeholder得到序列<span
class="math inline">\(y_2\)</span>；对<span
class="math inline">\(y_2\)</span>中的所有placeholder预测具体的词得到最终结果，完成一轮迭代。</li>
<li>三步操作共享一个transformer，同时为了降低复杂度（一轮自回归文本生成变成了三轮非自回归操作），将删除和insert
position预测操作的分类器接在中间的transformer
block上，只对文本生成的分类器接在最后一层transformer
block上，因为前两个任务比较简单，不需要复杂的特征提取。</li>
</ul>
<h2 id="训练">训练</h2>
<ul>
<li><p>训练的过程为imitation
learning，这里有较多强化学习概念，但实际上就是对数据做扰动然后让模型学习还原，且依然是MLE。我尝试在NLP任务中用其具体操作来解释</p>
<ul>
<li><p>训练依然是teacher forcing，即直接对输入去学习预测oracle（ground
truth），不存在自回归。</p></li>
<li><p>对于Levenshtein Transformer，训练时我们需要三部分：</p>
<ul>
<li>训练时的输入：输入即需要做编辑操作的文本（state distribution fed
during training)，这些句子应该是一些原始文本做了一些扰动操作（roll-in
policy）得到的。很容易理解，比如我故意删除了一些token，然后让模型去学习插入。因此该部分文本构建分为两步：确定原始文本，执行扰动操作。其中原始文本可以是空文本，也可以是ground
truth文本。在设计扰动操作时，引入了作者所谓的dual
policy，实际上就是让模型自己的操作也作为扰动操作</li>
</ul>
<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th>操作</th>
<th>扰动类型</th>
<th>让模型学习</th>
<th>dual policy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>对oracle文本随机丢词作为输入</td>
<td>删除</td>
<td>插入</td>
<td></td>
</tr>
<tr class="even">
<td>直接将oracle输入文本作为输入</td>
<td>插入</td>
<td>删除</td>
<td></td>
</tr>
<tr class="odd">
<td>使用模型执行删除动作后的文本作为输入</td>
<td>删除</td>
<td>插入</td>
<td>√</td>
</tr>
<tr class="even">
<td>使用模型执行插入动作后的文本作为输入</td>
<td>插入</td>
<td>删除</td>
<td>√</td>
</tr>
</tbody>
</table>
<p>这里作者使用概率来选择各个操作构造数据，示意图如下： <a
href="https://imgtu.com/i/gaoEPx"><img data-src="https://z3.ax1x.com/2021/05/11/gaoEPx.png"
alt="gaoEPx.png" /></a></p>
<ul>
<li>训练时的输出：即模型学习到与扰动相反的操作之后，需要还原的原始文本，当确定了输入输出之后，ground
truth的动作（expert
policy）就确定了。一般来讲自然是要还原到oracle，但是作者考虑到直接学习太难，因此以一定概率模型只需要还原出一个低噪声版本即可。这个低噪声版本由序列蒸馏得到，具体做法是用统一数据集训练一个正常的transformer，然后用这个transformer对每条数据做推断，使用beam
search找出非top 1的句子作为该条数据的低噪声版本给模型学习还原。</li>
<li>训练时的ground truth
action：在确定了输入和输出之后，利用动态规划得到编辑距离消耗最小的操作作为ground
truth action(expert policy)让模型学习</li>
</ul></li>
</ul></li>
</ul>
<h2 id="推断">推断</h2>
<ul>
<li>推断时，直接对输入文本进行多轮编辑（每一轮包括一次删除、预测位置、插入），直接贪心的选择概率最大的操作。</li>
<li>多轮编辑直到：
<ul>
<li>出现循环（即重复出现的编辑后文本）</li>
<li>预设最大轮次</li>
</ul></li>
<li>添加placeholder过多会导致生成的文本过短，可以根据实际情况对placeholder添加惩罚项</li>
<li>用两个编辑操作能够对模型的推断带来一定的可解释性</li>
</ul>
<h2 id="实验">实验</h2>
<ul>
<li>指标结果主要包含六项，三类语对的翻译BLEU值以及摘要Gigaword的ROUGE-1,2，L值。在英日翻译和Gigaword全指标上，原始Transformer取得最好结果，比LevT好一个点左右，在剩下两个翻译数据集上LevT使用序列蒸馏的结果最好，比原始Transformer好一个点左右。</li>
<li>对于LevT，序列蒸馏来训练比用原始oracle序列普遍要好</li>
<li>在推断速度上，使用序列蒸馏的LevT效率最高，推断单句基本在90ms左右，而原始Transformer在翻译数据上要200-300ms，在摘要数据上也要116ms。作者还给出了推断迭代次数，这里可以看到LevT平均只要2次迭代，即两次删除+两次插入，注意这是对整句同时进行操作，而Transformer的平均迭代次数就是平均推断文本长度。短摘要数据集平均迭代10.1次，即平均长度10.1，而翻译数据集上则达到了20+。这样也能看出LevT的另一个优势，就是推断速度对于推断文本长度不是那么敏感，其主要和对整句的处理迭代次数相关，而迭代次数随长度的增长速度小于对数复杂度，远小于传统Transformer的平方复杂度。Transformer的推断延迟和长度基本成正比，而LevT都在90ms左右，两次迭代。</li>
<li>消融表明了两种操作的设计、参数共享、dual
policy都能提升最终指标。</li>
<li>对删除和预测insert position直接用第一层结果early
exit就能取得较好效果，相比取用第六层，BLEU降低0.4，换来加速比从3.5倍提升到5倍</li>
<li>同时作者发现，在翻译任务上训练的LevT可以zero-shot直接用于文本完善，效果比重新训练一个文本完善LevT最小只差了0.3个点，在Ro-En上甚至表现更好，且都优于传统Transformer。这里对于文本完善的适应性也符合直觉，完善文本自然是进行较小的编辑修改，完全重新生成的假设空间更大，难以达到理想目标。在文本完善数据集上微调之后，效果更是全面优于所有模型。</li>
</ul>
<h1 id="editnts">EditNTS</h1>
<ul>
<li>提出了一个句子简化模型，显式地执行三类操作：插入、删除、保留</li>
<li>与LevT不同的是，EditNTS依然是自回归的，并没有同时对整句操作，并且在预测操作时引入了更多的信息</li>
<li>模型包含两部分：基于LSTM的seq2seq作为programmer，预测每个token位置的操作，产生编辑操作序列；以及一个额外的interpreter来执行操作生成编辑后的句子，这个interpreter还包含一个简单的LSTM来汇总编辑后句子的上下文信息
<a href="https://imgtu.com/i/gdTVa9"><img data-src="https://z3.ax1x.com/2021/05/12/gdTVa9.png"
alt="gdTVa9.png" /></a></li>
<li>值得注意的有三点：
<ul>
<li>预测出删除和保留后，programmer就移动到下一个单词，而预测出插入操作时，programmer不移动单词，接着在这个单词上预测下一个操作，来满足插入多个词的场景</li>
<li>interpreter除了根据programmer预测结果完成编辑外，还用了一个LSTM来汇总当前时间步的编辑后句子信息</li>
<li>programmer在预测编辑操作时，用到了四部分信息：
<ul>
<li>encoder的context，来自encoder lstm last layer attention-weighted
output</li>
<li>当前操作的词，来自decoder lstm hidden state</li>
<li>已经编辑好的句子的context，来自interpreter lstm attention-weighted
output</li>
<li>已经产生的编辑操作序列，来自attention-weighted edit label
output，这里只用了简单的attention，没有用lstm</li>
</ul></li>
</ul></li>
<li>encoder输入还额外引入了pos embedding</li>
<li>标签构造，类似LevT，需要在句子简化数据集上构建出ground
truth编辑操作，依然是基于编辑距离的动态规划得到操作序列，当多个操作序列存在时，优先选择插入序列多的，作者尝试过其他优先方式（优先删除、随机、引入替换操作），效果均不如直接优先插入。</li>
<li>使用100维的glove向量来初始化词向量和编辑label
embedding，用30维向量初始化pos
embedding，使用编辑标签逆频次作为损失权重来平衡各个标签的损失占比。</li>
<li>结果并不是很优秀，在指标上没有比非编辑方法好，在人类评估上好一些但存在主观性。消融也不是很显著。</li>
</ul>
<h1 id="lasertagger">LaserTagger</h1>
<ul>
<li>依然是保留、删除、插入三种操作，不过用上了BERT，且google的工程能力使得加速比达到了100-200倍</li>
<li>贡献点
<ul>
<li>提出了模型，并提出了从数据中生成标签词典的方法</li>
<li>提出了基于BERT的和用BERT初始化seq2seq
encoder的两个版本，前者加速快，后者效果好</li>
</ul></li>
</ul>
<h2 id="text-editing-as-tagging">Text Editing as Tagging</h2>
<ul>
<li>LaserTagger将文本编辑问题转换为序列标注问题，主要包含三部分
<ul>
<li>标注操作：将编辑转化标注，其标签并不是保留删除插入三种，而是分为两部分：base
tag B，有保留和删除两种；added phrase
P，有V种，代表插入的片段（空白、词或者短语），从训练数据中构建一个P
vocabulary得到，P和B组合得到标签，因此标签总共有2V种。可以根据下游人物添加任务相关的标签。</li>
<li>added phrase
vocabulary构建：构建词典是一个组合优化问题，一方面我们希望这个词典尽可能小，另一方面我们希望这个词袋能够覆盖最多的编辑情况，完美的解决该问题是NP-hard的，作者采用了另外一种方式。对每个训练文本对，通过动态规划求出最长公共子序列，用目标序列减去最长公共子序列就得到了需要添加的phrase，之后对phrase按照出现频次排序，取top
k，在公开数据集上top 500就能覆盖85%的数据。</li>
<li>构建标注序列：直接见伪算法，这里不需要动态规划，而是采取了一种贪心的方式，逐字匹配，找最短phrase插入
<a href="https://imgtu.com/i/gdxc0x"><img data-src="https://z3.ax1x.com/2021/05/12/gdxc0x.png" alt="gdxc0x.png" /></a>
这样可能会出现不能编辑的情况，这种情况就从训练集中筛去，作者认为这样也可以看成是一种降噪。</li>
<li>根据预测结果编辑句子：就是直接操作，可能根据下游任务预测出特别的标签进行任务相关的操作。作者认为将编辑操作模块化出来比直接端到端要更灵活。</li>
</ul></li>
</ul>
<h2 id="实验-1">实验</h2>
<ul>
<li>模型有两种，直接用BERT做标注或者用一个seq2seq，encoder用BERT初始化，前者的话就是在BERT最后一层是再加一层transformer
block做标注。</li>
<li>四项任务，句子融合，分句与转述，摘要，语法纠错。均超过baseline，其中句子融合取得新的SOTA。但是LaserTagger最亮眼的表现在于其效率，只用BERT
encoder能达到最高200倍的加速比（毕竟把文本生成变成了一个小词典的序列标注），同时对于小样本学习非常友好。</li>
<li>作者还分析了seq2seq和基于编辑的模型常出现的问题及其原因、两者的优势等等
<ul>
<li>想象词：seq2seq会在subword粒度上组合生成不存在的词，LaserTagger不存在这种情况</li>
<li>过早结束：seq2seq可能很早生成EOS导致句子不完整或者过短，而LaserTagger理论上存在这种可能，实际上作者没有见到过，因为这意味着模型要预测出一堆删除操作，而这在训练数据中是几乎没有的。</li>
<li>重复短语：在分句任务中，seq2seq经常会重复生成短语，而LaserTagger会选择不分句，或者lazy
split</li>
<li>Hallucination：即生成的文本与源文本无关或者反常识反事实，两类模型都会出现这类问题，seq2seq可能隐藏的更深（看起来流畅，实际上不合理）</li>
<li>共指消解：两类模型都存在这种问题，seq2seq容易将代词替换为错误名词，LaserTagger容易保留代词不管</li>
<li>错误删除：LaserTagger看似文本生成较为可控，但任然存在一些情况，即模型删除了部分词，剩下句子语义流畅，但含义错误</li>
<li>lazy
split：在分句任务中，LasterTagger可能只分句，而不对分开的句子做任何后处理。</li>
</ul></li>
</ul>
<h1 id="pie">PIE</h1>
<ul>
<li>也是BERT+序列标注，同时从数据中构造phrase词典</li>
<li>编辑操作包括复制（保留）、插入、删除、词形变换（用于语法纠错）</li>
<li>构造phrase词典、构造ground truth编辑操作和LaserTagger基本一致</li>
<li>将BERT左右扩展了两部分来获得替换或者增加所需的信息：
<ul>
<li>输入层，M为mask标识符的embedding，p为positional embedding，X为word
embedding</li>
<li>h即原始BERT信息，包含词和位置信息</li>
<li>r即替换信息，只不过把当前位置的词mask掉，用M替代，且计算注意力时不查询当前位置的h</li>
<li>a即插入信息，只不过把当前位置的词mask掉，用M替代，且p替换为相邻位置p的平均
<a href="https://imgtu.com/i/gwZarT"><img data-src="https://z3.ax1x.com/2021/05/12/gwZarT.png"
alt="gwZarT.png" /></a></li>
<li>之后利用三类信息来分别计算不同操作的概率，并归一化，CARDT分别代表复制（保留）、插入、替换、删除、词形变换
<a href="https://imgtu.com/i/gwZ5IH"><img data-src="https://z3.ax1x.com/2021/05/12/gwZ5IH.png"
alt="gwZ5IH.png" /></a></li>
<li>上式第一项是每一项编辑操作的得分；第二项是保留当前词的得分，删除和替换不得分，其余的则用当前词embedding
<span class="math inline">\(\phi(x_i)\)</span>来参数化得分，这里<span
class="math inline">\(\phi\)</span>的意义不明，原文是Embedding of w,
represented by <span class="math inline">\(\phi(w)\)</span> is obtained
by summing up in-dividual output embeddings of tokens in
w；第三项是新词带来的影响，只有替换和插入有。</li>
</ul></li>
<li>推断时也是多轮推断（输出变输入），直到出现重复句子</li>
<li>这里编辑操作设计的动机和解释不是很清楚，最终结果也很一般，ensemble情况下才与Transformer
+ Pre-training + LM+ Spellcheck + Ensemble Decoding
的工作持平，而且从消融来看，预训练模型是带来性能提升的主要原因。</li>
</ul>
<h1 id="felix">Felix</h1>
<ul>
<li>将编辑操作分为两个非自回归的部分，首先用一个基于Transformer的指针网络做tagging和reordering，并插入placeholder；然后用一个MLM对placeholder做预测</li>
<li>Felix的设计考虑三点需求
<ul>
<li>灵活的编辑，适合多种文本生成任务</li>
<li>充分利用BERT等预训练模型</li>
<li>高效推断 <a href="https://imgtu.com/i/gwKovR"><img data-src="https://z3.ax1x.com/2021/05/12/gwKovR.png"
alt="gwKovR.png" /></a></li>
</ul></li>
<li>下图可以清晰的表示出Felix的标签设计，其中<em>y<sup>t<em>是tagging模型需要预测出的编辑操作序列，</em>y</sup>m</em>是根据编辑操作序列，添加相关special
token(REPL,MASK)之后的中间状态序列，中间状态序列直接喂给MLM来预测需要插入的词，最终得到Pred里的结果</li>
<li><a href="https://imgtu.com/i/gwMHzj"><img data-src="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png"
alt="gwMHzj.md.png" /></a></li>
<li>这里需要注意两点：
<ul>
<li>MLM虽然只完成插入词的预测，但也需要完整的编辑操作信息，而不是输入MLM时直接把预测删除操作位置的词删掉，这里作者的做法是用REPL括起来要删除的span，给MLM也提供了删除编辑的信息。</li>
<li>图中给出了Mask和Infill两种形式，其实就是对中间状态序列的MASK设计不同，涉及到多token插入的任务交给谁：Mask方式里是把任务交给了tagging模型，直接预测出插入多个词的编辑操作，例如<span
class="math inline">\(DEL^{INS\_2}\)</span>，即插入两个，对应的就在中间序列中生成两个MASK让模型预测；Infill方式里把这个任务交给了MLM，统一对每个插入位置准备四个MASK让模型预测，多了的部分就预测PAD</li>
</ul></li>
<li>为了更灵活的建模，tagging部分的模型还需要做reordering，不然交互两个词的位置就得先删再添加，增加了复杂度。这里是通过指针网络注意力来为每个词确定其指向的后一个词，第一个词由特殊标识CLS指示。在推断时，使用了受控的beam
search避免指向顺序产生循环。</li>
<li>主要与LaserTagger对比，在性能，小样本上的表现要优于LaserTagger,且没有了phrase
vocabulary的限制。作者也就phrase
vocabulary和reordering做了实验，结果很多可见原论文。</li>
</ul>
<h1 id="总结">总结</h1>
<ul>
<li><p>总结各个模型</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 21%" />
<col style="width: 15%" />
<col style="width: 9%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">编辑操作</th>
<th style="text-align: center;">加速</th>
<th style="text-align: center;">插入多个词</th>
<th style="text-align: center;">构建ground truth编辑序列</th>
<th style="text-align: center;">编辑失败</th>
<th style="text-align: center;">测试任务</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LevT</td>
<td style="text-align: center;">插入删除</td>
<td style="text-align: center;">多轮非自回归，5倍</td>
<td style="text-align: center;">先预测placeholder数量，再替换</td>
<td style="text-align: center;">基于编辑距离动态规划</td>
<td style="text-align: center;">退化为文本生成</td>
<td style="text-align: center;">翻译、摘要</td>
</tr>
<tr class="even">
<td style="text-align: center;">EditNTS</td>
<td style="text-align: center;">插入删除保留</td>
<td style="text-align: center;">自回归，未提及加速</td>
<td style="text-align: center;">插入时原地停留，直到插入完毕</td>
<td style="text-align: center;">基于编辑距离动态规划</td>
<td style="text-align: center;">退化为文本生成</td>
<td style="text-align: center;">文本简化</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LaserTagger</td>
<td style="text-align: center;">插入删除保留</td>
<td style="text-align: center;">一轮序列标注，100+倍</td>
<td
style="text-align: center;">直接插入phrase，根据训练数据构建phrase词典</td>
<td style="text-align: center;">贪心匹配</td>
<td style="text-align: center;">不编辑</td>
<td
style="text-align: center;">句子融合，分句与转述，摘要，语法纠错</td>
</tr>
<tr class="even">
<td style="text-align: center;">PIE</td>
<td style="text-align: center;">插入删除替换复制词形变换</td>
<td style="text-align: center;">序列标注，2倍</td>
<td
style="text-align: center;">直接插入phrase，根据训练数据构建phrase词典</td>
<td style="text-align: center;">贪心匹配</td>
<td style="text-align: center;">未说明</td>
<td style="text-align: center;">语法纠错</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Felix</td>
<td style="text-align: center;">删除插入保留重排序</td>
<td style="text-align: center;">标注+MLM，快于LaserTagger</td>
<td style="text-align: center;">设置多个Mask让MLM预测</td>
<td style="text-align: center;">简单比较</td>
<td style="text-align: center;">较差的MLM预测结果</td>
<td
style="text-align: center;">句子融合、机器翻译后处理、摘要、文本简化</td>
</tr>
</tbody>
</table></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>theory</tag>
        <tag>nlp</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for VC Dimension</title>
    <url>/2020/05/15/vc-dimension/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/abd4e74033f7fc6f361451c8ccf02dfa.png" width="500"/></p>
<p>A brief review of the VC dimension. All discussions are based on the
simple case of binary classification.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="hoeffdings-inequality">Hoeffding's Inequality</h1>
<ul>
<li><p>A major assumption in machine learning is that the model trained
on the training set can generalize to the test set. More specifically,
using algorithm A and training set D, we find a hypothesis g in the
hypothesis space H that approximates the target hypothesis f. Let <span
class="math inline">\(E_{in}(g)\)</span> represent the error (empirical
error) on the training set, and <span
class="math inline">\(E_{out}(g)\)</span> represent the error
(generalization error, expected error) on all possible samples outside
the training set.</p></li>
<li><p>Given <span class="math inline">\(E_{out}(f) = 0\)</span>, we
also hope that the obtained g satisfies <span
class="math inline">\(E_{out}(g) = 0\)</span>, which contains two pieces
of information:</p>
<ul>
<li>We need <span class="math inline">\(E_{in}(g) \approx
E_{out}(g)\)</span></li>
<li>We need <span class="math inline">\(E_{in}(g) \approx
0\)</span></li>
</ul></li>
<li><p>The second point <span class="math inline">\(E_{in}(g) \approx
0\)</span> is what we do when training the model, reducing the error on
the training set, while the first point <span
class="math inline">\(E_{in}(g) \approx E_{out}(g)\)</span>, i.e., how
to ensure the model's generalization ability, is what VC dimension
measures.</p></li>
<li><p>The training set can be seen as a sampling of the sample space,
and we use Hoeffding's inequality to estimate quantities of the entire
sample space from the sampling:</p>
<p><span class="math display">\[
\mathbb{P}[|\nu-\mu|&gt;\epsilon] \leq 2 \exp \left(-2 \epsilon^{2}
N\right)
\]</span></p></li>
<li><p>Where <span class="math inline">\(\nu\)</span> is the calculated
quantity on the training set, <span class="math inline">\(\mu\)</span>
is the corresponding quantity on the entire sample space. Naturally, we
can view error (loss) as a kind of calculation, giving:</p>
<p><span class="math display">\[
\mathbb{P}\left[\left|E_{i n}(h)-E_{o u t}(h)\right|&gt;\epsilon\right]
\leq 2 \exp \left(-2 \epsilon^{2} N\right)
\]</span></p></li>
<li><p>Here h is a certain hypothesis (any one, not the best hypothesis
g we selected through training). The left side is the probability of the
difference between training and true loss exceeding a certain threshold,
which can be used to measure generalization ability. The right side
depends only on the difference threshold and training set size N. This
aligns with our intuition that the larger the training set, the stronger
the generalization ability.</p></li>
<li><p>However, the above situation is for a fixed hypothesis h. We call
the case where the empirical error and generalization error differ
greatly a "bad case". The above inequality shows that for a fixed h, the
probability of a bad case occurring is very small. But our algorithm A
selects hypotheses from the entire hypothesis space H. We are more
concerned with the probability that no h among all h experiences a bad
case, i.e.:</p>
<p><span class="math display">\[
\begin{array}{c}
\mathbb{P}\left[\mathbf{E}\left(h_{1}\right)&gt;\epsilon \cup
\mathbf{E}\left(h_{2}\right)&gt;\epsilon \ldots \cup
\mathbf{E}\left(h_{M}\right)&gt;\epsilon\right] \\
\leq
\mathbb{P}\left[\mathbf{E}\left(h_{1}\right)&gt;\epsilon\right]+\mathbb{P}\left[\mathbf{E}\left(h_{2}\right)&gt;\epsilon\right]
\ldots+\mathbb{P}\left[\mathbf{E}\left(h_{M}\right)&gt;\epsilon\right]
\\
\leq 2 M \exp \left(-2 \epsilon^{2} N\right)
\end{array}
\]</span></p></li>
<li><p>Where <span class="math inline">\(\mathbf{E}\)</span> represents
the difference between empirical error and generalization error. Here we
further bound this upper limit, considering the maximum case, assuming
the events of each h's empirical and generalization errors differing
beyond a certain threshold are independent. The probability of the union
of events is the sum of individual event probabilities. We ultimately
obtain an upper limit of bad cases for all h as <span
class="math inline">\(2 M \exp (-2 \epsilon^{2} N)\)</span></p></li>
<li><p>Now we've encountered a problem. Previously, since only N existed
in the negative exponential term, as long as the training set size was
large enough, this upper limit was finite, giving us confidence in
machine learning's generalization ability. Now, with an M (the capacity
of the hypothesis space) multiplied in front, the upper limit may no
longer be finite. This also aligns with intuition: imagine that the
larger the hypothesis space, the more data is needed to train and select
a good hypothesis. If the data volume is fixed, the larger the
hypothesis space, the harder it is to select a g close to the true
hypothesis f.</p></li>
</ul>
<h1 id="effective-hypotheses">Effective Hypotheses</h1>
<ul>
<li><p>Next, we formally enter the discussion of VC dimension. First, we
discuss the number of effective hypotheses. Considering the above
inequality, we made a large approximation by assuming that the events of
each h's empirical and generalization errors differing beyond a
threshold are independent, and the probability of the union of events is
the sum of individual event probabilities. But this is not actually the
case. For example, for two-dimensionally linearly separable data with
separation intervals, several parallel separation planes (lines) that
correctly classify the training set and are close to each other have
very similar characteristics. For most data points, the results under
these two separation planes are the same, and the probability
distribution of generalization ability differences also has significant
overlap. Treating them as independent is inappropriate.</p></li>
<li><p>Let's look again at M in the inequality, which literally means
the capacity of the hypothesis space, actually a measure of the
hypothesis space's expression capability. Under a certain amount of
training data, the richer the hypothesis space's expression ability, the
harder it is for the learning algorithm to find a good hypothesis. The
capacity of the hypothesis space is the sum of all hypotheses, where one
hypothesis can be seen as a set of parameters under a specific model. Is
there a more effective measure of the hypothesis space's expression
ability?</p></li>
<li><p>What if we look at the model's classification results instead of
the model parameters themselves? For example, previously we considered
hypotheses with the same parameters (same line) as the same hypothesis.
Now, we consider hypotheses with the same classification results
(different lines that classify points the same way) as the same
hypothesis. This seems more reasonable because expression capability
ultimately falls on handling all possible classification results of the
data, and both empirical and generalization errors are measured through
misclassified points.</p></li>
<li><p>Here we introduce several terms:</p>
<ul>
<li>Dichotomy: Denoted as <span class="math inline">\(h\left(X_{1},
X_{2}, \ldots, X_{N}\right)\)</span>. The dichotomy of N points is a
classification result of N points. For binary classification, there can
be at most <span class="math inline">\(2^N\)</span> dichotomies.</li>
<li>Dichotomies of hypothesis space H on N points in training set D,
denoted as <span class="math inline">\(\mathcal{H}\left(X_{1}, X_{2},
\ldots, X_{N}\right)\)</span>. The number of dichotomies depends not
only on the dataset but also on the hypothesis space H, because the
hypothesis space may not achieve all dichotomies. For example, for four
points forming a square with diagonal labels the same, a linear
hypothesis space cannot achieve this dichotomy.</li>
<li>Growth function: The number of dichotomies in the hypothesis space
on the training set depends not only on the training set size but also
on the specific training set. For example, if the extracted training set
does not include (four points forming a square with diagonal labels the
same), the number of dichotomies for lines on this training set might be
the same as the maximum possible dichotomies for this training set.
Therefore, to exclude the influence of specific training sets, we define
the maximum number of dichotomies the hypothesis space H can achieve on
all possible training sets of size N as the growth function: <span
class="math inline">\(m_{\mathcal{H}}(N)=\max _{X_{1}, X 2, \ldots,
X_{N} \in \mathcal{X}}\left|\mathcal{H}\left(X_{1}, X_{2}, \ldots,
X_{N}\right)\right|\)</span></li>
<li>Shatter: When the growth function reaches the theoretically maximum
dichotomy of <span class="math inline">\(2^N\)</span>. The meaning here
is that this hypothesis space is good enough for a dataset of size N
(for any dataset of size N in this task setting), capable of considering
all situations and providing corresponding hypotheses for any
classification result. That is, for a dataset with total capacity N, the
model can achieve a perfect solution.</li>
<li>Break point: Obviously, the smaller N is, the easier it is to
shatter; the larger N is, the harder it is to shatter. Starting from N=1
and gradually increasing until a critical point N=k where the hypothesis
space cannot shatter these k points, we call k the break point of this
hypothesis space. It can be proven that for N&gt;k, shattering is
impossible.</li>
</ul></li>
<li><p>We can see that the growth function is actually a measure of the
hypothesis space's expression capability. VC dimension considers using
this measure to replace the hypothesis space capacity in the Hoeffding
inequality. Of course, it cannot be directly substituted.
Specifically:</p>
<p><span class="math display">\[
\forall g \in \mathcal{H}, \mathbb{P}\left[\left|E_{i n}(g)-E_{o u
t}(g)\right|&gt;\epsilon\right] \leq 4 m_{\mathcal{H}}(2 N) \exp
\left(-\frac{1}{8} \epsilon^{2} N\right)
\]</span></p></li>
<li><p>The right side is the VC bound, which is complex to prove and
omitted. Now let's consider whether the upper bound is finite. We can
see that the important M has been replaced by <span
class="math inline">\(m_H\)</span>, the growth function. If the growth
function is bounded or its growth rate with N is lower than the
reduction rate of the exponential part, we can say that the upper bound
of the difference between empirical and generalization errors is finite.
In fact, if a break point k exists, then:</p>
<p><span class="math display">\[
m_{\mathcal{H}}(N) \leq B(N, k) \leq
\sum_{i=0}^{k-1}\left(\begin{array}{c}
N \\
i
\end{array}\right) \leq N^{k-1}
\]</span></p></li>
<li><p>Where B(N, k) is the upper bound of the growth function when the
break point is k. We can see that the upper bound of the growth function
is polynomial-level (k-1 power), so the growth function is at most
polynomial-level, while the VC bound's <span class="math inline">\(\exp
\left(-2 \epsilon^{2} N\right)\)</span> is exponential-level. Clearly,
the VC bound exists and is finite.</p></li>
<li><p>Finally, for cases where a break point exists, we can say that
generalization ability is guaranteed, and machine learning is
viable.</p></li>
</ul>
<h1 id="vc-dimension">VC Dimension</h1>
<ul>
<li><p>The VC bound ensures the feasibility of learning, while VC
dimension considers the expression capability of the hypothesis space.
We previously discussed the growth function as a measure of the
hypothesis space's expression capability, and the two are closely
related.</p></li>
<li><p>The VC dimension is defined as follows: Given a hypothesis space
H with an existing break point, the VC dimension is the size of the
largest dataset that can be shattered, i.e.:</p>
<p><span class="math display">\[
V C(\mathcal{H})=\max \left\{N: m_{\mathcal{H}}(N)=2^{N}\right\}
\]</span></p></li>
<li><p>Recall the two concepts in the VC dimension definition: growth
function and shattering. The growth function defines the hypothesis
space's ability to solve dichotomies, and shattering represents the
hypothesis space's ability to handle a certain amount of dataset, with a
corresponding hypothesis for each possible dichotomy. The VC dimension
measures the hypothesis space's capability from the perspective of
dataset size. The more complex the hypothesis space and the stronger its
expression capability, the larger the dataset it can shatter, with
sufficient hypotheses to solve every possible dichotomy on a larger
dataset - note that this is not every theoretically possible dichotomy,
because the VC dimension takes the maximum dataset size that can
actually be shattered, meaning the largest existing dataset size that
can be shattered by this hypothesis space.</p></li>
<li><p>So what exactly is the VC dimension? We actually defined it
earlier: it's k-1, meaning datasets with capacity less than the break
point can be shattered, so the maximum dataset size that can be
shattered is the break point - 1.</p></li>
<li><p>Notice that this k-1 is the polynomial degree in the growth
function's upper bound of the VC bound, so the VC bound can also be
written as:</p>
<p><span class="math display">\[
\forall g \in \mathcal{H}, \mathbb{P}\left[\left|E_{i n}(g)-E_{o u
t}(g)\right|&gt;\epsilon\right] \leq 4(2 N)^{V C(\mathcal{H})} \exp
\left(-\frac{1}{8} \epsilon^{2} N\right)
\]</span></p></li>
</ul>
<h1 id="measuring-generalization-ability">Measuring Generalization
Ability</h1>
<ul>
<li><p>From the VC bound inequality, we can see that the difference
between empirical error and generalization error (measuring
generalization ability) is associated with the probability of bad cases
on the right side of the inequality. If we specify the probability of
bad cases occurring as:</p>
<p><span class="math display">\[
4(2 N)^{V C(\mathcal{H})} \exp \left(-\frac{1}{8} \epsilon^{2}
N\right)=\delta
\]</span></p>
<p>We can conversely calculate the difference measuring generalization
ability:</p>
<p><span class="math display">\[
\epsilon=\sqrt{\frac{8}{N} \ln \left(\frac{4(2 N)^{V
C(\mathcal{H})}}{\delta}\right)}
\]</span></p></li>
<li><p>Therefore, in machine learning foundations and techniques,
teachers often write a VC bound to estimate the generalization error
formula:</p>
<p><span class="math display">\[
E_{\text {out }}(\mathbf{w}) \leq E_{\text {in
}}(\mathbf{w})+\Omega(\mathcal{H})
\]</span></p></li>
<li><p>Where <span class="math inline">\(\Omega(\mathcal{H})\)</span> is
<span class="math inline">\(\sqrt{\frac{8}{N} \ln \left(\frac{4(2 N)^{V
C(\mathcal{H})}}{\delta}\right)}\)</span></p></li>
</ul>
<h1 id="references">References</h1>
<ul>
<li>https://zhuanlan.zhihu.com/p/59113933</li>
<li>https://www.coursera.org/learn/ntumlone-mathematicalfoundations</li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="hoeffding不等式">Hoeffding不等式</h1>
<ul>
<li><p>机器学习的一个大假设就是，训练集上训练出的模型能够泛化到测试集，详细一点说即我们用算法A和训练集D，在假设空间H里中找到一个假设g，使得该假设g和需要学习的目标假设f近似。令g在训练集上的误差（经验误差）为<span
class="math inline">\(E_{in}(g)\)</span>，在除了训练集外所有可能样本上的误差（泛化误差，期望误差）为<span
class="math inline">\(E_{out}(g)\)</span>。</p></li>
<li><p>已知<span class="math inline">\(E_{out}(f) =
0\)</span>，我们也希望得到的g满足<span class="math inline">\(E_{out}(g)
= 0\)</span>，这里包含了两点信息</p>
<ul>
<li>需要<span class="math inline">\(E_{in}(g) \approx
E_{out}(g)\)</span></li>
<li>需要<span class="math inline">\(E_{in}(g) \approx 0\)</span></li>
</ul></li>
<li><p>第二点<span class="math inline">\(E_{in}(g) \approx
0\)</span>就是我们训练模型所做的事，在训练集上减小误差，而第一点<span
class="math inline">\(E_{in}(g) \approx
E_{out}(g)\)</span>，即如何保证模型的泛化能力，这是VC所衡量的事情。</p></li>
<li><p>训练集可以看成是样本空间的一个采样，从采样上计算一些量来估计整个样本空间的量，我们有Hoeffding不等式</p>
<p><span class="math display">\[
\mathbb{P}[|\nu-\mu|&gt;\epsilon] \leq 2 \exp \left(-2 \epsilon^{2}
N\right)
\]</span></p></li>
<li><p>其中<span
class="math inline">\(\nu\)</span>是训练集上的计算量，<span
class="math inline">\(\mu\)</span>是整个样本空间上对应的量。我们自然可以把误差（损失）看成是一种计算量，得到：</p>
<p><span class="math display">\[
\mathbb{P}\left[\left|E_{i n}(h)-E_{o u t}(h)\right|&gt;\epsilon\right]
\leq 2 \exp \left(-2 \epsilon^{2} N\right)
\]</span></p></li>
<li><p>其中h是某一假设（任意一个，不是我们通过训练挑出的最好假设g），左边即训练和真实损失差距大于某个相差阈值的概率，可以用来衡量泛化能力，而右侧发现只和相差阈值和训练集容量N相关。这符合我们的直觉，即训练集越大，泛化能力越强。</p></li>
<li><p>但问题是以上情况是针对固定的一个假设h，我们称经验误差和泛化误差相差很多的情况为bad
case，那上述不等式说明了对于固定的一个h，bad
case出现的概率很小。但是我们的算法A是在整个假设空间H里挑选假设，我们更关注对于所有h，其中任意一个h都不出现bad
case出现的概率，即</p>
<p><span class="math display">\[
\begin{array}{c}
\mathbb{P}\left[\mathbf{E}\left(h_{1}\right)&gt;\epsilon \cup
\mathbf{E}\left(h_{2}\right)&gt;\epsilon \ldots \cup
\mathbf{E}\left(h_{M}\right)&gt;\epsilon\right] \\
\leq
\mathbb{P}\left[\mathbf{E}\left(h_{1}\right)&gt;\epsilon\right]+\mathbb{P}\left[\mathbf{E}\left(h_{2}\right)&gt;\epsilon\right]
\ldots+\mathbb{P}\left[\mathbf{E}\left(h_{M}\right)&gt;\epsilon\right]
\\
\leq 2 M \exp \left(-2 \epsilon^{2} N\right)
\end{array}
\]</span></p></li>
<li><p>其中<span
class="math inline">\(\mathbf{E}\)</span>即经验误差和泛化误差的差距，这里我们进一步放缩了这个上界，考虑最大的情况，也就是各个h经验误差和泛化误差的差距大于某个阈值的事件相互独立，事件的并的概率是各个事件概率之和，最终得到了针对所有h，bad
case的上限是<span class="math inline">\(2 M \exp (-2 \epsilon^{2}
N)\)</span></p></li>
<li><p>这下就出了问题，之前由于只有N存在于负指数项中，所以只要训练集容量足够大，这个上限是有限的，我们就对机器学习的泛化能力有信心；现在前面乘了一个M，即假设空间的容量，这下上限就不一定是有限（存在）的了。这也符合直觉，想象一下，假设空间越大，就需要越多的数据来训练，使得算法挑选出一个好的假设；假如数据量一定，假设空间越大，就越难挑出一个和真实假设f相近的g</p></li>
</ul>
<h1 id="有效的假设">有效的假设</h1>
<ul>
<li><p>接下来正式进入VC维的探讨。首先我们讨论有效假设数。考虑到上面的不等式，我们其实做了一个很大的放缩，即假定各个h经验误差和泛化误差的差距大于某个阈值的事件相互独立，事件的并的概率是各个事件概率之和。但实际上并不是这样的。例如对于二维线性可分的数据，存在分离间隔，那么分离间隔中的几个能正确分类训练集的几个相互平行且相差不远的分离面（直线），他们其实非常相近，对于绝大多数数据点而言在这两个分离面下的结果相同，同样泛化能力出现差距的概率分布也有很大重叠，将其视为相互独立是不合适的。</p></li>
<li><p>我们再回头看看不等式中的M，其字面意思就是假设空间的容量，实际上是假设空间表达能力的一种度量，在一定量的训练集下，假设空间表达能力越丰富，学习算法就越难找到一个好的假设。假设空间的容量，是所有假设的和，其中一个假设可以看成是确定模型下的一组参数。有没有其他的更为有效的衡量假设空间表达能力的量？</p></li>
<li><p>假如我们不看模型参数本身，而是看模型对数据的分类结果呢？例如，之前是相同的参数（相同的直线）视为相同的假设，现在用相同的分类结果（不同的直线，把点做了相同的分类）的参数视为相同的假设。这样看来似乎更加合理，因为表达能力最终落实在对数据的所有可能分类结果的处理，而无论经验误差还是泛化误差也都是通过误分类点来衡量。</p></li>
<li><p>这里引入了几个术语：</p>
<ul>
<li>对分(dichotomy)：记为<span class="math inline">\(h\left(X_{1},
X_{2}, \ldots,
X_{N}\right)\)</span>。N个点的对分即N个点的一种分类结果，显然对于二分类问题，N个点最多有<span
class="math inline">\(2^N\)</span>个dichotomy。</li>
<li>假设空间H在训练集D里N个点上的对分，记为<span
class="math inline">\(\mathcal{H}\left(X_{1}, X_{2}, \ldots,
X_{N}\right)\)</span>，这里对分的个数除了和数据集相关，还和假设空间H相关，因为假设空间并不一定能取到所有的dichotomy，例如四个点组成正方形，对角线的标签一样，那么一条直线的假设空间就取不到这种对分。</li>
<li>增长函数(growth
function)：假设空间在训练集上的对分个数不仅与训练集大小相关，还与具体的训练集相关，例如抽出来的训练集不包含（四个点组成正方形，对角线的标签一样）这种对分，那么可能直线在该训练集上取到的对分数量就和该训练集最多可能的对分数量一样。因此为了排除具体的训练集影响，我们设在所有大小为N的可能训练集上假设空间H能取到的最大对分数量为增长函数，定为：<span
class="math inline">\(m_{\mathcal{H}}(N)=\max _{X_{1}, X 2, \ldots,
X_{N} \in \mathcal{X}}\left|\mathcal{H}\left(X_{1}, X_{2}, \ldots,
X_{N}\right)\right|\)</span></li>
<li>打散(shatter)：即增长函数取到了理论上最大的对分数<span
class="math inline">\(2^N\)</span>，这里的含义就是这个假设空间对于大小为N的数据集（该任务设定下任意一个大小为N数据集）来说足够好了，能够考虑到所有情况，针对任何一种分类结果给出对应的假设。也即对于该任务，总容量为N的数据集，该模型有能力取到完美解。</li>
<li>break
point：显然N越小，越容易打散；N越大，越不容易打散。那么从N=1开始逐渐增加，直到某个临界点N=k时，假设空间无法打散这k个点了，我们就称k为该假设空间的break
point。可以证明N&gt;k的情况都无法打散</li>
</ul></li>
<li><p>可以看到其实到增长函数这里，这就是一个假设空间表达能力的一种衡量指标了。VC维考虑的就是用这种衡量指标替换Hoeffding不等式中的假设空间容量。当然不能直接替换，具体而言，是</p>
<p><span class="math display">\[
\forall g \in \mathcal{H}, \mathbb{P}\left[\left|E_{i n}(g)-E_{o u
t}(g)\right|&gt;\epsilon\right] \leq 4 m_{\mathcal{H}}(2 N) \exp
\left(-\frac{1}{8} \epsilon^{2} N\right)
\]</span></p></li>
<li><p>上式右边即VC界，证明比较复杂略。那么这样我们再考虑上界是不是有限的。可以看到最重要的M被换成了<span
class="math inline">\(m_H\)</span>，即增长函数，假如增长函数有界或者说随N的增加速率低于指数部分的缩减速率，那么我们就可以说经验误差和泛化误差的差值上界是有限的。实际上，假如break
point k存在，那么有</p>
<p><span class="math display">\[
m_{\mathcal{H}}(N) \leq B(N, k) \leq
\sum_{i=0}^{k-1}\left(\begin{array}{c}
N \\
i
\end{array}\right) \leq N^{k-1}
\]</span></p></li>
<li><p>其中B(N, k)就是break
point为k时增长函数的上界，可以发现增长函数上界的上界是多项式级别的(k-1次方)，那么增长函数最多是多项式级别，而VC界中的<span
class="math inline">\(\exp \left(-2 \epsilon^{2}
N\right)\)</span>是指数级别，显然VC界是存在的，有限的。</p></li>
<li><p>那么终于，对于break
point存在的情况，我们可以说泛化能力是有保证的，机器学习是有保证的。</p></li>
</ul>
<h1 id="vc维">VC维</h1>
<ul>
<li><p>VC界保证了学习的可行性，而VC维考虑的是假设空间的表达能力。前面说到增长函数是对于假设空间表达能力的一种衡量，两者也存在密切关联。</p></li>
<li><p>VC维定义为，给定假设空间H，其break
point存在，那么其VC维是能够打散的最大数据集的大小，即</p>
<p><span class="math display">\[
V C(\mathcal{H})=\max \left\{N: m_{\mathcal{H}}(N)=2^{N}\right\}
\]</span></p></li>
<li><p>回忆VC维定义中的两个概念：增长函数和打散。增长函数定义了假设空间解决对分的能力，打散代表着假设空间有能力解决一定量的数据集，对于每一种可能的对分都有对应假设，那么VC维就是从数据集的量来衡量假设空间的能力，假设空间越复杂，表达能力越强，就能打散更大的数据集，有足够多的假设解决更大数据集上的每一种上可能的对分——注意，不是每一种理论上可能的对分，因为VC维取的是能够达到的最大数据集容量，是存在而不是任意，即最大存在大小为VC维的数据集能够被该假设空间打散。</p></li>
<li><p>那么VC维具体是多少呢？其实之前我们定义过了，就是k-1，即容量小于break
point的数据集都能被打散，那么能被打散的数据集容量最大就是break point -
1</p></li>
<li><p>注意到这个k-1不就是VC界中增长函数上界中的多项式次数吗，因此VC界又可以写成：</p>
<p><span class="math display">\[
\forall g \in \mathcal{H}, \mathbb{P}\left[\left|E_{i n}(g)-E_{o u
t}(g)\right|&gt;\epsilon\right] \leq 4(2 N)^{V C(\mathcal{H})} \exp
\left(-\frac{1}{8} \epsilon^{2} N\right)
\]</span></p></li>
</ul>
<h1 id="衡量量化能力">衡量量化能力</h1>
<ul>
<li><p>由VC界的不等式可以看到，经验误差与泛化误差之间的差距（衡量泛化能力）和不等式右边即bad
case发生的概率相关联，假如我们给定了bad case发生的概率，设为：</p>
<p><span class="math display">\[
4(2 N)^{V C(\mathcal{H})} \exp \left(-\frac{1}{8} \epsilon^{2}
N\right)=\delta
\]</span></p>
<p>那么可以反过来算出衡量泛化能力的差距</p>
<p><span class="math display">\[
\epsilon=\sqrt{\frac{8}{N} \ln \left(\frac{4(2 N)^{V
C(\mathcal{H})}}{\delta}\right)}
\]</span></p></li>
<li><p>所以在机器学习基石和技法当中，老师经常写出一个VC
bound用来估计泛化误差的公式：</p>
<p><span class="math display">\[
E_{\text {out }}(\mathbf{w}) \leq E_{\text {in
}}(\mathbf{w})+\Omega(\mathcal{H})
\]</span></p></li>
<li><p>其中的<span
class="math inline">\(\Omega(\mathcal{H})\)</span>即<span
class="math inline">\(\sqrt{\frac{8}{N} \ln \left(\frac{4(2 N)^{V
C(\mathcal{H})}}{\delta}\right)}\)</span></p></li>
</ul>
<h1 id="参考">参考</h1>
<ul>
<li>https://zhuanlan.zhihu.com/p/59113933</li>
<li>https://www.coursera.org/learn/ntumlone-mathematicalfoundations</li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>statistical learning</tag>
        <tag>vc dimension</tag>
      </tags>
  </entry>
  <entry>
    <title>(Welcome) to the Era of Wild</title>
    <url>/2025/10/05/wild-era/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/10/07/0b1adc08119a47115c49c4eaac3887e7.png" width="300"></p>
<p>Connecting the dots, (welcome) to the era of wild.</p>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><p>Three quarters of 2025 have already passed, and many exciting new
developments have occurred in AI this year. Looking back, I feel many
dots have once again been strung together:</p>
<ul>
<li>DeepSeek R1</li>
<li>ClaudeCode/Gemini CLI/Codex</li>
<li>Era of Experience</li>
<li>The Second Half</li>
<li>RLVR</li>
<li>Agentic RL</li>
<li>HealthBench</li>
<li>GDPval</li>
<li>Sora 2 App</li>
<li>OpenAI Dev Day</li>
</ul>
<p>A line that connects them is that AI will no longer be optimized
merely toward datasets, toward metrics such as accuracy, but will be
directly optimized toward the deep and complex goals in real-world human
social activities. We put AI in the Wild, hoping it can directly
participate in human social production activities, learn from feedback
from the real world, and that such feedback can reach directly to the
model, until every next token prediction, rather than being overly
diluted by layers of external mechanisms.</p>
<h1 id="evolution-of-tasks">Evolution of Tasks</h1>
<p>From the perspective of NLP, we have been making task move from
simulation toward the real:</p>
<ul>
<li>In the NLP era, the tasks the model completed were to classify a
sentiment label for each text fragment, or to annotate dependency
relations in a sentence, whereas humans (except linguists/NLPer)
generally do not and do not need to do this</li>
<li>In the LLM era, models organize all NLP tasks into the form of
prompts for processing, and humans do the same when communicating with
language</li>
<li>In the Agentic era, models operate tools and produce artifact, and
many human professions are precisely to do these things</li>
<li>In the Wild era, models and humans will jointly form a society, and
task will no longer distinguish between real and simulated</li>
</ul>
<p>On the surface, Wild AI looks like making application, but because of
the change in the nature of task, it differs from traditional vertical
domain AI: it is not about collecting NLP/LLM-formatted data from the
domain to continue training models, but about deploying AI into the real
world and directly optimizing for the domain’s ultimate goals.</p>
<p>From a technical perspective, this can be seen as application with
RL, but the exploration of RL in LLM is still long; how to accurately
bring back feedback produced by the final objectives to the model may
require changes in human-machine interaction patterns, changes in model
frameworks, and even paradigms beyond RL.</p>
<p>In the most naive sense, this is still RLHF and not a completely new
path. It is precisely this path that helped OpenAI launch ChatGPT,
allowing large models to truly reach human users, and the effects
brought by real users’ preference feedback later became widely known:
they made large models truly understood by the world and opened a new
LLM era. And now everyone will race along this path at an unprecedented
speed.</p>
<h1 id="deep-goals">Deep Goals</h1>
<p>What are the deep and complex goals and feedback in human social
activities? Possible (but not exhaustive) examples include:</p>
<ul>
<li>GDP</li>
<li>employment rate</li>
<li>average life expectancy</li>
<li>crime rate</li>
<li>annual profit</li>
<li>cutoff score</li>
<li>box office</li>
<li>scientific discovery</li>
<li>h-index</li>
<li>global temperature</li>
<li>……</li>
</ul>
<p>Why Does Wild AI Optimize for These Goals?</p>
<ul>
<li>One of reinforcement learning’s most utilitarian advantages is
optimizing for non-differentiable objectives, and there exist too many
non-differentiable gaps between the model and real-world utility</li>
<li>People have discovered the potential of reinforcement learning plus
strong priors; it may be possible to optimize for these goals</li>
<li>Scaling needs a new story; from training to inference, the next
stage of scaling requires new data and new dimensions. The infinitely
many environments in the real world, extremely long chains, and goals
that currently seem fantastical all provide new soil for that story</li>
<li>The First Half of chasing dataset SOTA has become ineffective</li>
<li>Large companies have limited patience for long-term investment in
AI</li>
</ul>
<p>Recommendation systems are a field that has long entered the Wild
Era: they optimize for GMV, profoundly transform our lives, trigger
countless discussions such as information cocoons, and together with
mobile internet shape the current landscape of tech company giants. The
research directions of academia and industry have gradually diverged.
And LLMs/AI entering the Wild Era will go further than recommendation
systems in every aspect (or put it another way, be more severe).</p>
<h1 id="welcome-to-the-era-of-wild">Welcome to the Era of Wild?</h1>
<p>I will use the word Welcome cautiously. Goodhart’s law tells us that
once a metric becomes a target it is no longer a good metric. The
indicators currently defined for measuring human society already have
many problems; even without AI’s participation, many pathological
phenomena have already occurred in people’s pursuit of these indicators,
and AI’s involvement may further accelerate this kind of hacking. In
addition, from another perspective, certain individuals or organizations
might also realize their objectives through Feedback Hijacking.</p>
<p>On the other hand, previously people were divided over whether AI
should be developed first or governed first, but at present the
development-first technical path has become somewhat clearer, and
companies that develop first will not stop: AI’s transformation of the
world will not pause because of debates. Under this premise, the
technology for governing AI may actually need to evolve faster.</p>
<h1 id="the-future">The Future</h1>
<p>In the new Wild Era, what technologies are still needed to develop
AI?</p>
<ul>
<li>Before moving into real society, first achieve truly reliable AI
decision mechanisms, rather than still relying on probabilities</li>
<li>A new cycle of four major components: model/algo/data/infras.
Currently when people do Agentic AI, they are still working within
well-cared-for datasets and environments. But upon reaching real
environments, current model architectures may not be suitable for
extremely long chains, algorithms cannot fully utilize feedback, and the
base infrastructure cannot support large-scale efficient training and
inference. And when model/data/base infrastructure are all optimized to
saturation, people will further go into the real environment to mine the
next data treasure, bringing more complex data situations, which in turn
will prompt the evolution of model/algorithm/base infrastructure,
forming a cycle</li>
<li>Simulators: letting AI directly transform the real world is still
somewhat radical; how to realistically simulate feedback and simulate
the impact caused by AI is indispensable. Simulators can on the one hand
conduct rehearsals before real transformation, and on the other hand
scale up training, just like reward model</li>
<li>New feedback curation standards: like pretraining data, people need
various complex strategies to ensure data quality, avoiding bad feedback
signals brought by next token. In the real world, feedback signals are
more complex and also require more systematic vetting</li>
<li>Hard to Verify: currently our progress is almost entirely on the
Easy to Verify side within the Generator Verifier Asymmetry; after
moving to the real world, we will flip to the other side, and how to
mine and utilize feedback in the Hard to Verify domain is also a major
topic</li>
<li>Collective intelligence: after AI enters human society, countless
entities paired with AI will emerge, the entire human society mirrored
as an AI society. Will such bottom-up intelligent collectives give rise
to new intelligence and new social mechanisms? Will this correspondingly
require new evaluation systems?</li>
<li>When optimizing for deep goals in real production activities, should
evaluation and feedback be safely separated to avoid falling into the
Goodhart trap?</li>
<li>Bidirectional optimization: maybe we will not only optimize AI for
society, but also optimize society for AI</li>
<li>Mechanism-driven alignment: current alignment signals such as human
preferences are still controlled by humans determining the direction of
alignment; if preferences are inverted, the same process can make the
model learn badly. In the future is it possible to set environmental
mechanisms such that when models interact and experience in society and
learn from feedback, no party can dominate the alignment signals, but
rather such mechanisms achieve common goals</li>
<li>World Sensor: a feedback collector designed for human social
activities, bridging the environment and the model</li>
</ul>
<p>This is an era of confusion, bubbles, temptation, risk, and
opportunity coexisting.</p>
<p>(Welcome) to the Era of Wild.</p>
<h1 id="citation">Citation</h1>
<p>If you found the topics in this blog post interesting and would like
to cite it, you may use the following BibTeX entry: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{wild_era_202510,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {(Welcome) to the Era of Wild},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {10},</span><br><span class="line">  url = {https://thinkwee.top/2025/10/05/wild-era/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><p>2025年已经过去了四分之三，今年的AI发生了很多激动人心的新进展，回顾这¾年，我感觉很多dots又一次被串联起来:</p>
<ul>
<li>DeepSeek R1</li>
<li>ClaudeCode/Gemini CLI/Codex</li>
<li>Era of Experience</li>
<li>The Second Half</li>
<li>RLVR</li>
<li>Agentic RL</li>
<li>HealthBench</li>
<li>GDPval</li>
<li>Sora 2 App</li>
<li>OpenAI Dev Day</li>
</ul>
<p>串联起它们的一条线是，AI将不再仅仅面向数据集优化，面向accuracy这样的指标优化，而是将直接面向真实世界人类社会活动里深层次的复杂目标优化。我们&nbsp;<em>put
AI in the
Wild</em>，希望其能直接参与人类社会生产活动，从真实世界的反馈中学习，而且这种反馈能直达模型，直到
every next token prediction，而不是被层层外部机制过分稀释。</p>
<h1 id="任务的演变">任务的演变</h1>
<p>从NLP的视角来看，我们一直在让task从模拟向真实迈进:</p>
<ul>
<li>NLP
时代，模型完成的任务是为每个文本片段分类一个情感标签，或者在句子中标注依存关系，而人类（除了语言学家/NLPer）一般不会也无需这么做</li>
<li>LLM
时代，模型将所有NLP任务组织为prompt的形式进行处理，人类用语言沟通时会这么做</li>
<li>Agentic
时代，模型操作工具，产出artifact，很多人类的职业就是做这些事情</li>
<li>Wild 时代，模型和人类将共同组成社会，task将不再区分真实还是模拟</li>
</ul>
<p>表面上看，<em>Wild AI</em>&nbsp;像是在做application，但由于 task
性质的变化，它与传统垂域 AI
有所区别：不是从垂域里收集NLP/LLM形式的数据继续训练模型，而是将AI投放到真实世界，直接面向垂域最终的目标优化。</p>
<p>如果从技术上看，这可以看成是application with RL，但是RL in
LLM的探索还很长，如何把最终目标带来的反馈准确的带回给模型，这其中可能需要人机交互模式的变革，模型框架的变革，甚至超过RL的新范式。</p>
<p>最朴素的来讲，这依然是RLHF，并不是全新的路线。正是这条路线帮助 OpenAI
推出了
ChatGPT，使得大模型真正触达人类用户，而真实用户的偏好反馈带来的效果后来也众所周知了：让大模型真正被世人所理解，开启了新的LLM时代。而现在大家会在这条路线上以前所未有的速度狂飙。</p>
<h1 id="深层目标">深层目标</h1>
<p>什么是人类社会活动里深层次的复杂目标和反馈？可能（但不全面）的例子有：</p>
<ul>
<li>GDP</li>
<li>就业率</li>
<li>平均寿命</li>
<li>犯罪率</li>
<li>年利润</li>
<li>分数线</li>
<li>票房</li>
<li>科学发现</li>
<li>h-index</li>
<li>全球气温</li>
<li>......</li>
</ul>
<p>为什么Wild AI面向这些目标优化？</p>
<ul>
<li>强化学习最功利的一个优势就是，面向不可导的目标优化，而模型到真实世界效用之间存在太多不可导的gap</li>
<li>大家发现了强化学习+强先验的潜力，有可能做到面向这些目标优化了</li>
<li>Scaling需要新的故事，从training到inference，下一阶段的 scaling
需要新的数据与维度。而真实世界里无限多的环境、极长的链路、当前看来天方夜谭的目标都给故事提供了新的土壤</li>
<li>刷数据集SOTA的The First Half失效了</li>
<li>大型企业对AI长期投入的耐心有限</li>
</ul>
<p>推荐系统是一个早已进入 <em>Wild Era</em> 的领域：它面向 GMV
优化，深刻改造了我们的生活，引起了无数类似信息茧房之类的讨论，和移动互联网一起塑造了今天的科技公司巨头格局，学术界和工业界的研究方向渐行渐远。而进入
<em>Wild Era</em> 的
LLM/AI，其在各个方面上都会比推荐系统更进一步（或者说更加严重）。</p>
<h1 id="欢迎来到-wild-era">欢迎来到 <em>Wild Era</em>？</h1>
<p>我会谨慎使用Welcome这个词。Goodhart's
law告诉我们，一项指标一旦变成了目标，它将不再是个好指标。当前人们定义的用于衡量人类社会的各项指标本来就存在诸多问题，即便没有AI参与，人们在追求这些指标的过程中已经发生了很多畸形现象，而AI参与进来，可能会进一步加速这种hacking。此外，从另一个角度来看，某些个人或组织也可能通过Feedback
Hijacking实现自己的目的。</p>
<p>另一方面，之前大家对于AI是先发展还是先治理各执一词，但目前来看先发展的技术路线又清晰了一些，先发展的公司也不会停下脚步：AI对于世界的改造不会因为各执一词的讨论而暂停。在这个前提下，如何治理AI的技术反而可能需要更快的进化。</p>
<h1 id="未来">未来</h1>
<p>在新的Wild Era，发展AI还需要哪些技术？</p>
<ul>
<li>迈向真实社会之前，首先要实现真正可依赖的AI决策机制，而不是依然靠概率</li>
<li>新的四大件循环：model/algo/data/infras。当前大家做Agentic
AI，依然还是在呵护的很好的数据集和环境中做。而到达真实环境之后，当前的模型可能架构不适用于超长的链路，算法没法充分的利用反馈，基架支撑不起大规模高效训练和推理。而当模型/数据/基架都优化到趋于饱和时，人们就会进一步去真实环境里挖掘下一个数据宝藏，带来更复杂的数据情况，进而接着促使模型/算法/基架进化，形成循环。</li>
<li>模拟器：让AI直接改造真实世界还是有些激进，如何真实的模拟反馈，模拟AI带来的影响不可或缺。模拟器一方面能在真实改造之前进行演习，另一方面也能scale
up训练，就像reward model一样。</li>
<li>新的feedback整理规范：就像预训练数据一样，人们需要各种复杂的策略来保证数据质量，避免next
token带来不好的反馈信号。在真实世界，反馈信号更加复杂，也需要更成体系的甄别。</li>
<li>Hard to Verify：当前我们的进展几乎都在Generator Verifier Asymmetry里
Easy to Verify的那一侧，到真实世界之后，我们会翻到另一侧，Hard to
Verify的领域如何挖掘并利用反馈也是一个大课题。</li>
<li>群体智能：AI进入人类社会后，将会出现无数pair with
AI的实体，整个人类社会镜像为AI社会，这种自下而上的智能集群是否会涌现新的智能，新的社会机制？是否对应需要新的评估体系？</li>
<li>当面向真实生产活动深层次目标优化时，评估和反馈是否应该安全分离，避免步入Goodhart陷阱？</li>
<li>双向优化：也许我们不仅仅是optimize AI for society， 也会optimize
society for AI。</li>
<li>机制驱动的对齐：当前的对齐信号比如人类偏好，依然是人类掌握着对齐的方向，偏好取反，相同的流程，就能让模型学坏。未来是否有可能设置环境机制，当模型在社会中交互、体验，并从反馈中学习时，没有任何一方能主宰对齐信号，而是靠这类机制达成共同目标。</li>
<li>World
Sensor：面向人类社会活动设计的反馈收集器，架起环境和模型的桥梁。</li>
</ul>
<p>这是一个迷茫，泡沫，诱惑，风险，机遇并存的时代。</p>
<p>(Welcome) to the Era of Wild.</p>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:
</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article{wild_era_202510,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {(Welcome) to the Era of Wild},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {10},</span><br><span class="line">  url = {https://thinkwee.top/2025/10/05/wild-era/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    ]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>rl</tag>
        <tag>scaling</tag>
        <tag>llm</tag>
        <tag>agent</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Variational Auto-Encoder</title>
    <url>/2019/03/20/vae/</url>
    <content><![CDATA[<p><img data-src="https://i.mji.rip/2025/07/16/86c202d0b5712f432526beca2939166e.png" width="500"/></p>
<ul>
<li><p>Variational Autoencoder Learning Notes</p></li>
<li><p>Reference Article:</p>
<ul>
<li><a href="https://arxiv.org/pdf/1312.6114.pdf">Auto-Encoding
Variational Bayes</a></li>
<li><a href="https://dfdazac.github.io/01-vae.html">Daniel Daza, The
Variational Autoencoder</a></li>
<li><a href="https://spaces.ac.cn/tag/vae/">Sūshén's VAE series</a></li>
</ul></li>
<li><p>On VAE, the original paper and the two blogs above have already
explained it very clearly. I am just repeating and paraphrasing, just to
go through it myself. If anyone reads this blog, I recommend reading
these three reference sources first</p></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="directly-view-the-network-structure">Directly view the network
structure</h1>
<ul>
<li>Variational autoencoders use variational inference, but because the
parameter estimation part employs the gradient descent method of neural
networks, their network structure can be directly depicted—we actually
refer to them as autoencoders, and this is also because their structure
has many similarities with autoencoders. Even if one does not start from
a Bayesian perspective, VAE can be directly regarded as a type of
special autoencoder.</li>
<li>Taking the mnist experiment from the original paper as an example,
we directly examine the network structure of the VAE, and then
generalize the model and explain the details:
<ul>
<li>An encoder and a decoder, the goal is to reconstruct the error and
obtain useful encoding, just like an autoencoder</li>
<li>However, variational autoencoders do not directly encode the input,
but rather assume that the encoding follows a multivariate normal
distribution; the encoder encodes the mean and variance of this
multivariate normal distribution</li>
<li>That is, VAE assumes that the encoding is simple, following a normal
distribution, while what I train and utilize is the decoding, this
decoder can decode and reconstruct the input from samples of the normal
distribution, or in other words, generate the output</li>
</ul></li>
<li>mnist input is 28*28, batch_size is 128, assuming the hidden layer
dim is 200, and the parameter dim is 10, then the entire network
is:</li>
</ul>
<ol type="1">
<li>Input <span class="math inline">\(x\)</span> [128,28*28], pass
through a linear+ReLu, obtain encoder hidden layer <span
class="math inline">\(h\)</span> [128,200]</li>
<li>Through two linear transformations, obtain normal distribution
parameters <span class="math inline">\(\mu _e\)</span> [128,10], <span
class="math inline">\(\log \sigma _e\)</span> [128,10]</li>
<li>From a standard multivariate normal distribution, sample <span
class="math inline">\(\epsilon \sim N(0,I)\)</span> , to obtain <span
class="math inline">\(\epsilon\)</span> [128,10]</li>
<li>Combine the parameters obtained through the network <span
class="math inline">\(\mu _e\)</span> , <span class="math inline">\(\log
\sigma _e\)</span> to the sampling values of the standard
multidimensional normal distribution, <span class="math inline">\(z =
\mu _e + \sigma _e \bigodot \epsilon\)</span> [128,10], <span
class="math inline">\(z\)</span> is the encoding</li>
<li>encoder part is completed, next is the decoder part: encoding <span
class="math inline">\(z\)</span> through overlinear + ReLu to obtain the
decoder hidden layer state h[128,200]</li>
<li>Decoding the hidden state h through linear + sigmoid yields <span
class="math inline">\(\mu _d\)</span> [128, 28*28], i.e., the output
after decoding</li>
<li>Decoding output <span class="math inline">\(\mu _d\)</span> and
input <span class="math inline">\(x\)</span> to calculate the Bernoulli
cross-entropy loss</li>
<li>In addition, a regularity-like loss term should be added <span
class="math inline">\(\frac 12 \sum _{i=1}^{10} (\sigma _{ei}^2 + \mu
_{ei}^2 -log(\sigma _{ei}^2) - 1)\)</span></li>
</ol>
<h1 id="direct-network-analysis">Direct Network Analysis</h1>
<ul>
<li>From the network, the biggest difference between VAE and AE is that
it does not directly encode the input but introduces the concept of
probability into the network structure, constructing an encoding that
satisfies a multi-dimensional normal distribution for each input</li>
<li>One advantage of this approach is that the encoding can perform
interpolation, achieving a continuous change in generated images: the
original AE has a fixed encoding for each specific input, while the
network in VAE is only responsible for generating fixed mean and
variance, i.e., a fixed normal distribution. The actual encoding is just
a sampling from this fixed normal distribution, still uncertain. During
the training process, VAE trains an area rather than a point, so the
obtained encoding has continuity, and similar images can be generated
near the center point of the area. Moreover, interpolation can be
performed between the two encoding areas for the two types of image
inputs, realizing a smooth transition between the two generated
images.</li>
<li>Step 1, 2, 3, and 4 involve the encoder sampling an encoding from a
distribution of <span class="math inline">\(N(\mu _e,\sigma _e
^2)\)</span> , however, there is a reparameterization trick, namely:
<ul>
<li>The encoder neural network should originally be fitted to a
distribution satisfying <span class="math inline">\(N(\mu _e,\sigma _e
^2)\)</span> , and then samples taken from the distribution</li>
<li>However, the values obtained from the sampling cannot be
backpropagated</li>
<li>Therefore, the neural network is modified to only fit the parameters
of the distribution, then samples from a simple standard multivariate
normal distribution, and the sampled values are processed by the fitting
parameters, i.e., <span class="math inline">\(z = \mu _e + \sigma _e
\bigodot \epsilon\)</span> , to achieve the effect of <span
class="math inline">\(z\)</span> as if directly sampled from <span
class="math inline">\(N(\mu _e,\sigma _e ^2)\)</span> , while the neural
network merely fits the parameters, allowing for backpropagation, and
the sampling is equivalent to performing some weighted operations with
specified weights, participating in the network training</li>
</ul></li>
<li>Step 5, 6, and 7 are ordinary decoding, as the output is a 28*28
black and white image, so it is directly decoded into a 28*28 binary
vector, and compared with the input to calculate cross-entropy</li>
<li>The key is 8, how is this regular term obtained?</li>
</ul>
<h1 id="regular-term">Regular term</h1>
<ul>
<li><p>This regular term actually represents the KL divergence between
the encoded normal distribution and the standard normal distribution,
i.e., (where K is the dimension of the multivariate normal distribution,
which is 10 in the above example):</p>
<p><span class="math display">\[
KL(N(\mu _e,\sigma _e ^2)||N(0,I)) =  \frac 12 \sum _{i=1}^{K} (\sigma
_{ei}^2 + \mu _{ei}^2 -log(\sigma _{ei}^2) - 1)
\]</span></p></li>
<li><p>That is to say, we hope the normal distribution we encode is
close to the standard normal distribution, why?</p></li>
<li><p>There are many different interpretations here:</p>
<ul>
<li>The first type: We hope that for different classes of inputs, the
encoding can be encoded into the same large area, that is, while the
regions within are compact, the distance between regions should not be
too far, and it is best to reflect the distance in terms of image
features, for example, taking mnist as an example, the images of 4 and 9
are relatively similar, while the difference with the image of 0 is
large, then the distance between their encoding regions can reflect the
similarity; or, during the process from 0 to 8, the intermediate states
will resemble 9, then the encoding region of 9 should be between the
encoding regions of 0 and 8. However, in reality, the encoder network
may learn such an encoding method: for different classes of inputs, the
difference <span class="math inline">\(\mu\)</span> is large, it
separates the encoding regions of different class inputs (more
accurately, non-similar inputs, here in unsupervised learning, there are
no classes) quite far apart. The neural network does this for a reason:
to make it easier for the decoder to distinguish between different
inputs during decoding. This goes against our original intention of
encoding them into continuous regions for easy interpolation. Therefore,
we strongly hope that the learned encoding distribution is approximately
a standard normal distribution, so that they are all in a large area, of
course, not too similar, otherwise everyone is the same, the decoder's
burden is too heavy, and it cannot decode the differences, which is the
role of the reconstruction loss mentioned earlier.</li>
<li>The second type: The effect of VAE is equivalent to adding Gaussian
noise to the standard autoencoder, making the decoder robust to noise.
The size of the KL divergence represents the strength of the noise: a
smaller KL divergence indicates that the noise is closer to the standard
Gaussian noise, i.e., stronger; a larger KL divergence indicates a
weaker noise strength, here understood as the noise being assimilated,
not that the variance has decreased, because the noise should be
unrelated to the input signal and always maintain Gaussian noise or
other specified distributions. If the noise becomes increasingly distant
from the specified distribution and more related to the input, its role
as noise diminishes accordingly.</li>
<li>The third: This is the most rigorous understanding, where the KL
divergence is obtained from the perspective of variational inference,
and the entire model is derived from Bayesian framework reasoning. The
network structure exists because the author uses neural networks to fit
the parameters, and the specification of the hyperparameters and
distributions is a special case of this framework in the mnist
generation task, after all, the original text refers to it as
autoencoder variational Bayesian (a method), not variational autoencoder
(a structure). Next, let's look at how the entire model is derived from
the perspective of the original paper, and naturally obtain this KL
divergence regularization term.</li>
</ul></li>
</ul>
<h1 id="variational-autoencoder-bayesian">Variational Autoencoder
Bayesian</h1>
<ul>
<li><p>The entire decoder section can be regarded as a generative model,
with its probability graph being: <img data-src="https://s2.ax1x.com/2019/03/20/AKu5FA.png"
alt="AKu5FA.png" /></p></li>
<li><p><span class="math inline">\(z\)</span> is the encoding, <span
class="math inline">\(\theta\)</span> is the decoder parameters we hope
to obtain, controlling the decoder to decode (generate) from the
encoding ( <span class="math inline">\(x\)</span> )</p></li>
<li><p>The problem now returns to the inference of probabilistic
graphical models: Given the observed variable x, how to obtain the
parameter <span class="math inline">\(\theta\)</span> ?</p></li>
<li><p>The author's approach is not a complete copy of variational
inference; in VAE, the <span class="math inline">\(q\)</span>
distribution is also used to approximate the posterior distribution
<span class="math inline">\(p(z|x)\)</span> . The log-likelihood of the
observables is decomposed into ELBO and KL(q||p(z|x)). The difference is
that in variational inference, q is obtained using the EM method, while
in VAE, q is fitted using a neural network (the input of the neural
network is <span class="math inline">\(z\)</span> , and therefore <span
class="math inline">\(q\)</span> itself is also a posterior distribution
<span class="math inline">\(q(z|x)\)</span> .</p>
<p><span class="math display">\[
\log p(x|\theta) = \log p(x,z|\theta) - \log p(z|x,\theta) \\
\]</span></p>
<p><span class="math display">\[
= \log \frac{p(x,z|\theta)}{q(z|x,\phi)} - \log
\frac{p(z|x,\theta)}{q(z|x,\phi)} \\
\]</span></p>
<p><span class="math display">\[
= \log p(x,z|\theta) - \log q(z|x,\phi) - \log
\frac{p(z|x,\theta)}{q(z|x,\phi)} \\
\]</span></p>
<p><span class="math display">\[
= [ \int _z q(z|x,\phi) \log p(x,z|\theta)dz - \int _z q(z|x,\phi) \log
q(z|x,\phi)dz ] + [- \int _z \log \frac{p(z|x,\theta)}{q(z|x,\phi)}
q(z|x,\phi) dz ]\\
\]</span></p></li>
<li><p>Note that we actually aim to obtain the parameters <span
class="math inline">\(\theta\)</span> and <span
class="math inline">\(\phi\)</span> that maximize the logarithmic
likelihood of the observations, while the latent variable <span
class="math inline">\(z\)</span> can be obtained with the model given
the input.</p></li>
<li><p>It can be seen that, under the condition of the measurement,
i.e., the log-likelihood, the larger the value in the previous brackets,
i.e., the ELBO, the closer the subsequent KL divergence, i.e., the
posterior distribution <span class="math inline">\(q(z|x,\phi)\)</span>
and the posterior true distribution <span
class="math inline">\(p(z|x,\theta)\)</span> . This posterior
distribution, i.e., given <span class="math inline">\(x\)</span> , to
obtain <span class="math inline">\(z\)</span> , is actually the encoder,
so the smaller the KL divergence, the better the encoder's performance.
Therefore, we should maximize the ELBO. The ELBO can be rewritten
as:</p>
<p><span class="math display">\[
ELBO = \int _z q(z|x,\phi) \log p(x,z|\theta)dz - \int _z q(z|x,\phi)
\log q(z|x,\phi)dz \\
\]</span></p>
<p><span class="math display">\[
= E_{q(z|x,\phi)}[\log p(x,z|\theta)-\log q(z|x,\phi)] \\
\]</span></p>
<p><span class="math display">\[
= E_{q(z|x,\phi)}[\log p(x|z,\theta)]-KL(q(z|x,\phi)||(p(z|\theta))) \\
\]</span></p></li>
<li><p>Another KL divergence has appeared! This KL divergence is the KL
divergence between the posterior distribution of the latent variables
encoded by the encoder and the prior distribution of the latent
variables. The first part, <span
class="math inline">\(p(x|z,\theta)\)</span> , which calculates the
distribution of the observable variables from the known latent
variables, is actually the decoder. Therefore, <span
class="math inline">\(\phi\)</span> and <span
class="math inline">\(\theta\)</span> correspond to the parameters of
the encoder and decoder, respectively, which are actually the parameters
of the neural network. The former is called the variational parameter,
and the latter is called the generative parameter.</p></li>
<li><p>We aim to maximize this ELBO, and the VAE directly uses it as the
objective function of the network structure, performing gradient descent
and taking derivatives with respect to <span
class="math inline">\(\theta\)</span> and <span
class="math inline">\(\phi\)</span> . In this case, the first part <span
class="math inline">\(E_{q(z|x,\phi)}[\log p(x|z,\theta)]\)</span>
calculates the expectation using the Monte Carlo method, i.e., sampling
multiple <span class="math inline">\(z\)</span> from <span
class="math inline">\(z \sim q(z|x,\phi)\)</span> , and then calculating
the mean to find the expectation, where the reparameterization technique
mentioned above is applied.</p></li>
<li><p>At this point, the entire probabilistic graphical model,
including the inference part, becomes <img data-src="https://s2.ax1x.com/2019/03/20/AKuhod.png"
alt="AKuhod.png" /></p></li>
<li><p>Process: Obtain the observation x -&gt; Obtain samples of z
through reparameterization -&gt; Input the samples of z into the target
function (ELBO) for differentiation -&gt; Gradient descent, update
parameters <span class="math inline">\(\theta\)</span> and <span
class="math inline">\(\phi\)</span></p></li>
</ul>
<h1 id="return-to-mnist">Return to mnist</h1>
<ul>
<li><p>In the MNIST experiment, the authors set the prior of the latent
variables, the q distribution, the base distribution in
reparameterization <span class="math inline">\(\epsilon\)</span> , and
the posterior distribution of the observations to be:</p>
<p><span class="math display">\[
p(z) = N(z|0,I) \\
\]</span></p>
<p><span class="math display">\[
q(z|x,\phi) = N(z|\mu _e , diag(\sigma _e)) \\
\]</span></p>
<p><span class="math display">\[
\epsilon \sim N(0,I) \\
\]</span></p>
<p><span class="math display">\[
p(x|z,\theta) = \prod _{i=1}^D \mu _{d_i}^{x_i} (1-\mu _{d_i})^{1-x_i}
\\
\]</span></p></li>
<li><p>The model parameters <span class="math inline">\(\phi = [\mu_e ,
\sigma _e]\)</span> , <span class="math inline">\(\theta=\mu _d\)</span>
are obtained through neural network learning</p></li>
<li><p>The first part of the ELBO objective function, the expectation
part, has already been completed through reparameterization, the
internal</p>
<p><span class="math display">\[
\log p(x|z,\theta) = \sum _{i=1}^D x_i \log \mu _{d_i} + (1-x_i) \log
(1- \mu _{d_i}) \\
\]</span></p></li>
<li><p>Bernoulli cross-entropy, where in network design the sigmoid
function is added to the last layer, is to ensure that the output <span
class="math inline">\(mu_d\)</span> satisfies the probability.</p></li>
<li><p>The latter part of the ELBO objective function, i.e., the KL
divergence between the posterior q distribution of the latent variables
and the prior p distribution, becomes the regularization term mentioned
above, making the approximate distribution closer to the prior
distribution</p></li>
<li><p>The entire model considers both the reconstruction loss and the
prior information</p></li>
<li><p>Therefore, the ELBO can be written as:</p>
<p><span class="math display">\[
ELBO = - reconstruction loss - regularization term
\]</span></p></li>
</ul>
<h1 id="effect">Effect</h1>
<ul>
<li>Reconstruction Effect on the MNIST Dataset <img data-src="https://s2.ax1x.com/2019/03/20/AKufdH.png" alt="AKufdH.png" /></li>
<li>The effect obtained from variance disturbance <img data-src="https://s2.ax1x.com/2019/03/20/AKuWee.png" alt="AKuWee.png" /></li>
<li>The effect of mean perturbation <img data-src="https://s2.ax1x.com/2019/03/20/AKu2LD.png" alt="AKu2LD.png" /></li>
<li>Interpolation results for 4 and 9 <img data-src="https://s2.ax1x.com/2019/03/20/AKuIJI.png" alt="AKuIJI.png" /></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="直接看网络结构">直接看网络结构</h1>
<ul>
<li>变分自编码器用了变分推断，但是因为其中的参数估计部分用的是神经网络的梯度下降方法，因此可以直接画出其网络结构——实际上我们称其为自编码器，也是因为其结构上和自编码器有许多相通之处，如果不从贝叶斯的角度出发，甚至可以将VAE直接看成一类特殊的自编码器。</li>
<li>以原论文的mnist实验为例，我们直接看VAE的网络结构，之后再一般化模型并解释细节：
<ul>
<li>整体和自编码器一样，一个encoder和一个decoder,目标是重构误差，获取有用的编码</li>
<li>然而变分自编码器不对输入直接编码，而是假定编码服从多维正态分布，encoder编码的是这个多维正态分布的均值和方差</li>
<li>也就是VAE假定编码很简单，就是服从正态分布，而我要训练出来并利用的是解码，这个解码器能从正态分布的采样中解码还原出输入，或者说，生成输出</li>
</ul></li>
<li>mnist的输入为28*28，batch_size为128，假定隐层dim为200，参数dim为10，则整个网络为：</li>
</ul>
<ol type="1">
<li>输入<span
class="math inline">\(x\)</span>[128,28*28]，过一个linear+ReLu，得到编码器隐层<span
class="math inline">\(h\)</span>[128,200]</li>
<li><span
class="math inline">\(h\)</span>分别过两个linear，得到正态分布的参数<span
class="math inline">\(\mu _e\)</span>[128,10]，<span
class="math inline">\(\log \sigma _e\)</span>[128,10]</li>
<li>从一个标准多维正态分布中采样<span class="math inline">\(\epsilon
\sim N(0,I)\)</span>，得到<span
class="math inline">\(\epsilon\)</span>[128,10]</li>
<li>组合通过网络得到的参数<span class="math inline">\(\mu
_e\)</span>，<span class="math inline">\(\log \sigma
_e\)</span>到标准多维正态分布的采样值中，<span class="math inline">\(z =
\mu _e + \sigma _e \bigodot \epsilon\)</span>[128,10]，<span
class="math inline">\(z\)</span>即编码</li>
<li>encoder部分就此完成，接下来是decoder部分：编码<span
class="math inline">\(z\)</span>过linear+ReLu得到解码隐层状态h[128,200]</li>
<li>解码隐层状态h经过linear+sigmoid得到<span class="math inline">\(\mu
_d\)</span>[128,28*28]，即解码后的输出</li>
<li>解码后输出<span class="math inline">\(\mu _d\)</span>与输入<span
class="math inline">\(x\)</span>计算伯努利交叉熵损失</li>
<li>此外还要加上一个类似正则项的损失<span class="math inline">\(\frac 12
\sum _{i=1}^{10} (\sigma _{ei}^2 + \mu _{ei}^2 -log(\sigma _{ei}^2) -
1)\)</span></li>
</ol>
<h1 id="直接对网络分析">直接对网络分析</h1>
<ul>
<li>从网络可以看到，VAE相比AE最大的区别就是不直接对输入编码，而是将概率的思想引入了网络结构，对每一个输入单独构建一个满足多维正态分布的编码</li>
<li>这么做的一个好处是，编码可以做插值，实现生成图像连续的变化：原始的AE对于每一个确定输入有确定的编码，而VAE中的网络只负责生成确定的均值和方差，也就是确定的正态分布，实际的编码只是在这个确定的正态分布中采样得到，依然是不确定的。在训练过程中VAE是对一片区域进行训练，而不是点训练，因此得到的编码具有连续性，在区域中心点附近也能生成相似的图像，甚至可以在两类图像输入确定的两个编码区域之间插值，实现两种生成图像的平滑过渡。</li>
<li>步骤1，2，3，4实现的是编码器从一个<span class="math inline">\(N(\mu
_e,\sigma _e
^2)\)</span>的分布中采样得到编码，然而这里存在一个reparameterization的技巧，即：
<ul>
<li>本来应该是让编码器神经网络拟合一个满足<span
class="math inline">\(N(\mu _e,\sigma _e
^2)\)</span>的分布，再从分布中采样</li>
<li>但是采样得到的值无法进行反向传播</li>
<li>因此改成神经网络只拟合分布的参数，然后从一个简单的标准多维正态分布中采样，采样值经过拟合参数处理，即<span
class="math inline">\(z = \mu _e + \sigma _e \bigodot
\epsilon\)</span>，来达到<span
class="math inline">\(z\)</span>仿佛直接从<span
class="math inline">\(N(\mu _e,\sigma _e
^2)\)</span>采样得到的效果，而神经网络仅仅拟合参数，可以进行反向传播，采样在其中相当于做了一些指定权值的加权，参与进网络的训练</li>
</ul></li>
<li>步骤5，6，7则是普通的解码，因为输出是28*28的黑白图像，因此直接解码成28*28的二值向量，与输入比对计算交叉熵</li>
<li>关键是8，这个正则项如何得到？</li>
</ul>
<h1 id="正则项">正则项</h1>
<ul>
<li><p>这个正则项实际上是编码得到的正态分布和标准正态分布之间的KL散度，即（其中K是多维正态分布的维度，在上例中是10）：</p>
<p><span class="math display">\[
KL(N(\mu _e,\sigma _e ^2)||N(0,I)) =  \frac 12 \sum _{i=1}^{K} (\sigma
_{ei}^2 + \mu _{ei}^2 -log(\sigma _{ei}^2) - 1)
\]</span></p></li>
<li><p>也就是说，我们希望编码的正态分布接近标准正态分布，为什么？</p></li>
<li><p>这里就有很多种说法了：</p>
<ul>
<li>第一种：我们希望的是对于不同类的输入，编码能编码到同一个大区域，即不同区域内部紧凑的同时，区域之间的距离也不应该太远，最好能体现图像特征上的距离，比如以mnist为例，4和9的图像比较近似，和0的图像差异比较大，则他们的编码区域之间的距离能反映相似的关系；或者说从0变到8的过程中中间状态会像9，那么9的编码区域能在0和8的编码区域之间最好。然而实际上是，编码器网络可能会学到这样的编码方法：对于不同类的输入，<span
class="math inline">\(\mu\)</span>相差很大，它将不同类输入（准确的说是不近似的输入，这里是无监督学习，没有类别）的编码区域隔得很开。神经网络这么做是有道理的：让解码器方便区别不同的输入进行解码。这就与我们希望它编码成连续区域方便插值的初衷相悖，因此我们强制希望所学到的编码分布都近似标准正态分布，这样都在一个大区域中，当然也不能太近似，不然大家都一样，解码器负担太大，根本解码不出来区别，这就是前面的重构损失的作用。</li>
<li>第二种：VAE的效果相当于在标准的自编码器中加入了高斯噪声，使得decoder对噪声具有鲁棒性。KL散度的大小代表了噪声的强弱：KL散度小，噪声越贴近标准高斯噪声，即强度大；KL散度大，噪声强度就小，这里理解为噪声被同化了，而不是说方差变小了，因为噪声应该与输入信号无关，一直保持高斯噪声或者其他指定的分布，如果噪声变得和指定分布越来越远，和输入越来越相关，那其作为噪声的作用也就越来越小了。</li>
<li>第三种：也是最严谨的一种理解，这个KL散度是从变分推断的角度出发得到的，整个模型也是从贝叶斯框架推理得到的。其之所以有网络结构是因为作者用了神经网络来拟合参数，神经网络的超参、分布的指定也是该框架在mnist生成任务中的一种特例，毕竟原文叫自编码变分贝叶斯（一种方法），而不是变分自编码网络（一种结构）。接下来我们从原论文的角度来看看整个模型如何推导出来，并自然而然得到这个KL散度正则项。</li>
</ul></li>
</ul>
<h1 id="变分自编码贝叶斯">变分自编码贝叶斯</h1>
<ul>
<li><p>整个解码器部分我们可以看成一个生成模型，其概率图为： <img data-src="https://s2.ax1x.com/2019/03/20/AKu5FA.png"
alt="AKu5FA.png" /></p></li>
<li><p><span class="math inline">\(z\)</span>即编码，<span
class="math inline">\(\theta\)</span>是我们希望得到的解码器参数，控制解码器从编码中解码出（生成出）<span
class="math inline">\(x\)</span></p></li>
<li><p>现在问题回归到概率图模型的推断：已知观测变量x，怎么得到参数<span
class="math inline">\(\theta\)</span>？</p></li>
<li><p>作者采取的思路并不是完全照搬<a
href="https://thinkwee.top/2018/08/28/inference-algorithm/#more">变分推断</a>，在VAE中也采用了<span
class="math inline">\(q\)</span>分布来近似后验分布<span
class="math inline">\(p(z|x)\)</span>，并将观测量的对数似然拆分成ELBO和KL(q||p(z|x))，不同的是变分推断中用EM的方式得到q，而在VAE中用神经网络的方式拟合q（神经网络输入为<span
class="math inline">\(z\)</span>，因此<span
class="math inline">\(q\)</span>本身也是后验分布<span
class="math inline">\(q(z|x)\)</span>。完整写下来：</p>
<p><span class="math display">\[
\log p(x|\theta) = \log p(x,z|\theta) - \log p(z|x,\theta) \\
\]</span></p>
<p><span class="math display">\[
= \log \frac{p(x,z|\theta)}{q(z|x,\phi)} - \log
\frac{p(z|x,\theta)}{q(z|x,\phi)} \\
\]</span></p>
<p><span class="math display">\[
= \log p(x,z|\theta) - \log q(z|x,\phi) - \log
\frac{p(z|x,\theta)}{q(z|x,\phi)} \\
\]</span></p>
<p><span class="math display">\[
= [ \int _z q(z|x,\phi) \log p(x,z|\theta)dz - \int _z q(z|x,\phi) \log
q(z|x,\phi)dz ] + [- \int _z \log \frac{p(z|x,\theta)}{q(z|x,\phi)}
q(z|x,\phi) dz ]\\
\]</span></p></li>
<li><p>注意，我们实际希望得到的是使得观测量对数似然最大的参数<span
class="math inline">\(\theta\)</span>和<span
class="math inline">\(\phi\)</span>，而隐变量<span
class="math inline">\(z\)</span>可以在输入确定的情况下随模型得到。</p></li>
<li><p>可以看到，在观测量即对数似然确定的情况下，前一个中括号内即ELBO值越大，则后面的KL散度，即后验<span
class="math inline">\(q(z|x,\phi)\)</span>分布和后验真实分布<span
class="math inline">\(p(z|x,\theta)\)</span>越相近。这个后验分布，即已知<span
class="math inline">\(x\)</span>得到<span
class="math inline">\(z\)</span>实际上就是编码器，因此这个KL散度越小则编码器效果越好，既然如此我们就应该最大化ELBO，ELBO可以改写成：</p>
<p><span class="math display">\[
ELBO = \int _z q(z|x,\phi) \log p(x,z|\theta)dz - \int _z q(z|x,\phi)
\log q(z|x,\phi)dz \\
\]</span></p>
<p><span class="math display">\[
= E_{q(z|x,\phi)}[\log p(x,z|\theta)-\log q(z|x,\phi)] \\
\]</span></p>
<p><span class="math display">\[
= E_{q(z|x,\phi)}[\log p(x|z,\theta)]-KL(q(z|x,\phi)||(p(z|\theta))) \\
\]</span></p></li>
<li><p>又出现了一个KL散度！这个KL散度是编码器编码出的隐变量后验分布和隐变量先验分布之间的KL散度。而前半部分，<span
class="math inline">\(p(x|z,\theta)\)</span>，已知隐变量求出观测量的分布，实际上就是解码器。因此<span
class="math inline">\(\phi\)</span>和<span
class="math inline">\(\theta\)</span>分别对应编码器和解码器的参数，实际上即神经网络的参数。前者称为variational
parameter，后者称为generative parameter</p></li>
<li><p>我们要使得这个ELBO最大，VAE就直接将其作为网络结构的目标函数，做梯度下降，分别对<span
class="math inline">\(\theta\)</span>和<span
class="math inline">\(\phi\)</span>求导。在这里前半部分<span
class="math inline">\(E_{q(z|x,\phi)}[\log
p(x|z,\theta)]\)</span>求期望，用的是蒙特卡罗方法，即从<span
class="math inline">\(z \sim q(z|x,\phi)\)</span>中采样多个<span
class="math inline">\(z\)</span>，再求均值求期望，这里用到了上面说到的reparameterization技巧。</p></li>
<li><p>此时整个概率图模型，加上推断部分，变成了 <a
href="https://imgchr.com/i/AKuhod"><img data-src="https://s2.ax1x.com/2019/03/20/AKuhod.png"
alt="AKuhod.png" /></a></p></li>
<li><p>流程：得到观测量x-&gt;通过reparameterization得到z的样本-&gt;将z的样本带入目标函数（ELBO）求导-&gt;梯度下降，更新参数<span
class="math inline">\(\theta\)</span>和<span
class="math inline">\(\phi\)</span></p></li>
</ul>
<h1 id="回到mnist">回到mnist</h1>
<ul>
<li><p>在mnist实验中，作者设置隐变量的先验、q分布、reparameterization中的基础分布<span
class="math inline">\(\epsilon\)</span>、观测量的后验分布分别为：</p>
<p><span class="math display">\[
p(z) = N(z|0,I) \\
\]</span></p>
<p><span class="math display">\[
q(z|x,\phi) = N(z|\mu _e , diag(\sigma _e)) \\
\]</span></p>
<p><span class="math display">\[
\epsilon \sim N(0,I) \\
\]</span></p>
<p><span class="math display">\[
p(x|z,\theta) = \prod _{i=1}^D \mu _{d_i}^{x_i} (1-\mu _{d_i})^{1-x_i}
\\
\]</span></p></li>
<li><p>其中模型参数<span class="math inline">\(\phi = [\mu_e , \sigma
_e]\)</span>,<span class="math inline">\(\theta=\mu
_d\)</span>通过神经网络学习得到</p></li>
<li><p>而目标函数ELBO的前半部分，求期望部分已经通过reparameterization完成，内部的</p>
<p><span class="math display">\[
\log p(x|z,\theta) = \sum _{i=1}^D x_i \log \mu _{d_i} + (1-x_i) \log
(1- \mu _{d_i}) \\
\]</span></p></li>
<li><p>即伯努利交叉熵，在网络设计是最后一层增加sigmoid函数也就是为了输出的<span
class="math inline">\(mu_d\)</span>满足为概率。</p></li>
<li><p>目标函数ELBO的后半部分，即隐变量的后验q分布和先验p分布之间的KL散度，就成为了上面所说的正则项,使得近似分布靠近先验分布</p></li>
<li><p>整个模型既考虑了重构损失，也考虑了先验信息</p></li>
<li><p><a
href="https://dfdazac.github.io/01-vae.html">因此ELBO可以写成</a>：</p>
<p><span class="math display">\[
ELBO = -重构误差损失-正则惩罚
\]</span></p></li>
</ul>
<h1 id="效果">效果</h1>
<ul>
<li>在mnist数据集上的重构效果 <a href="https://imgchr.com/i/AKufdH"><img data-src="https://s2.ax1x.com/2019/03/20/AKufdH.png"
alt="AKufdH.png" /></a></li>
<li>对方差扰动得到的效果 <a href="https://imgchr.com/i/AKuWee"><img data-src="https://s2.ax1x.com/2019/03/20/AKuWee.png"
alt="AKuWee.png" /></a></li>
<li>对均值扰动得到的效果 <a href="https://imgchr.com/i/AKu2LD"><img data-src="https://s2.ax1x.com/2019/03/20/AKu2LD.png"
alt="AKu2LD.png" /></a></li>
<li>对4和9进行插值的结果 <a href="https://imgchr.com/i/AKuIJI"><img data-src="https://s2.ax1x.com/2019/03/20/AKuIJI.png"
alt="AKuIJI.png" /></a></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>mcmc</tag>
        <tag>vae</tag>
      </tags>
  </entry>
</search>
