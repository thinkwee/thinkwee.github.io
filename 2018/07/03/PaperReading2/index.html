<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Distractor Mechanism External Information Attention Pointer Copy Network PGNet Extractive Summary Based on RNN Transformer Selection gate mechanism">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Reading 2">
<meta property="og:url" content="https://thinkwee.top/2018/07/03/PaperReading2/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Distractor Mechanism External Information Attention Pointer Copy Network PGNet Extractive Summary Based on RNN Transformer Selection gate mechanism">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0o47d.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oh0H.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oLjS.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oIAA.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0ootI.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oTht.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0ob1f.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oqc8.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oXng.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0TSNn.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/15/ivQCE8.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oh0H.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oLjS.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oIAA.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0ootI.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oTht.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0ob1f.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oqc8.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0oXng.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/10/20/i0TSNn.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/15/ivQCE8.png">
<meta property="article:published_time" content="2018-07-03T07:18:52.000Z">
<meta property="article:modified_time" content="2025-07-15T20:35:28.777Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="math">
<meta property="article:tag" content="abstractive summarization">
<meta property="article:tag" content="theory">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png">


<link rel="canonical" href="https://thinkwee.top/2018/07/03/PaperReading2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2018/07/03/PaperReading2/","path":"2018/07/03/PaperReading2/","title":"Paper Reading 2"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper Reading 2 | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">51</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#distraction-based-neural-networks-for-document-summarization"><span class="nav-number">1.</span> <span class="nav-text">Distraction-Based
Neural Networks for Document Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#document-modeling-with-external-attention-for-sentence-extraction"><span class="nav-number">2.</span> <span class="nav-text">Document
Modeling with External Attention for Sentence Extraction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#get-to-the-point-summarization-with-pointer-generator-networks"><span class="nav-number">3.</span> <span class="nav-text">Get
To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents"><span class="nav-number">4.</span> <span class="nav-text">SummaRuNNer:
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-is-all-you-need"><span class="nav-number">5.</span> <span class="nav-text">Attention Is All You Need</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-joint-selective-mechanism-for-abstractive-sentence-summarization"><span class="nav-number">6.</span> <span class="nav-text">A
Joint Selective Mechanism for Abstractive Sentence Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#distraction-based-neural-networks-for-document-summarization"><span class="nav-number">7.</span> <span class="nav-text">Distraction-Based
Neural Networks for Document Summarization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#document-modeling-with-external-attention-for-sentence-extraction"><span class="nav-number">8.</span> <span class="nav-text">Document
Modeling with External Attention for Sentence Extraction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#get-to-the-point-summarization-with-pointer-generator-networks"><span class="nav-number">9.</span> <span class="nav-text">Get
To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents"><span class="nav-number">10.</span> <span class="nav-text">SummaRuNNer
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention-is-all-you-need"><span class="nav-number">11.</span> <span class="nav-text">Attention Is All You Need</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#a-joint-selective-mechanism-for-abstractive-sentence-summarization"><span class="nav-number">12.</span> <span class="nav-text">A
Joint Selective Mechanism for Abstractive Sentence Summarization</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2018/07/03/PaperReading2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper Reading 2 | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Reading 2
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-07-03 15:18:52" itemprop="dateCreated datePublished" datetime="2018-07-03T15:18:52+08:00">2018-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-16 04:35:28" itemprop="dateModified" datetime="2025-07-16T04:35:28+08:00">2025-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span id="/2018/07/03/PaperReading2/" class="post-meta-item leancloud_visitors" data-flag-title="Paper Reading 2" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>18 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/07/16/814348a7a047f33afd7b70e437cf6ddb.png" width="500"/></p>
<ul>
<li>Distractor Mechanism</li>
<li>External Information Attention</li>
<li>Pointer Copy Network PGNet</li>
<li>Extractive Summary Based on RNN</li>
<li>Transformer</li>
<li>Selection gate mechanism</li>
</ul>
<span id="more"></span>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0o47d.jpg" alt="i0o47d.jpg" />
<figcaption aria-hidden="true">i0o47d.jpg</figcaption>
</figure>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1
id="distraction-based-neural-networks-for-document-summarization">Distraction-Based
Neural Networks for Document Summarization</h1>
<ul>
<li><p>Not only using attention mechanisms but also attention dispersion
mechanisms to better capture the overall meaning of the document.
Experiments have shown that this mechanism is particularly effective
when the input is long text. <img data-src="https://s1.ax1x.com/2018/10/20/i0oh0H.png"
alt="i0oh0H.png" /></p></li>
<li><p>Introducing a control layer between the encoder and decoder to
achieve attention concentration and attention dispersion, using two
layers of GRU:</p>
<p><span class="math display">\[
s_t = GRU _1 (s_t^{temp},c_t) \\
s_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\
\]</span></p></li>
<li><p>This control layer captures the connection between <span
class="math inline">\(s_t^{&#39;}\)</span> and <span
class="math inline">\(c_t\)</span> , where the former encodes the
current and previous output information, and the latter encodes the
current input that has been processed through attention focusing and
attention dispersion, while <span
class="math inline">\(e(y_{t-1})\)</span> is the embedding of the
previous input.</p></li>
<li><p>Three Attention Diversion Models</p>
<ul>
<li><p>M1: Calculate c_t for the control layer, distribute it over the
inputs, where c_t is the context c_t^{temp} encoded by a standard
attention mechanism, obtained by subtracting the historical context,
similar to a coverage mechanism</p>
<p><span class="math display">\[
c_t = tanh (W_c c_t^{temp} - U_c \sum _{j=1}^{t-1} c_j) \\
c_t^{temp} = \sum _{i=1}^{T_x} \alpha _{t,i} h_i \\
\]</span></p></li>
<li><p>M2: Distribute the attention weights, similarly, subtract the
historical attention and then normalize</p>
<p><span class="math display">\[
\alpha _{t,i}^{temp} = v_{\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a
\sum _{j=1}^{t-1}\alpha _{j,i}) \\
\alpha _{t,i} = \frac {exp(\alpha _{t,i}^{temp})}{\sum _{j=1}^{T_x}
exp(\alpha _{t,j}^{temp})} \\
\]</span></p></li>
<li><p>M3: Perform dispersion at the decoding end, calculate the
distances between the current <span class="math inline">\(c_t\)</span> ,
<span class="math inline">\(s_t\)</span> , <span
class="math inline">\(\alpha _t\)</span> , and the historical <span
class="math inline">\(c_t\)</span> , <span
class="math inline">\(s_t\)</span> , <span class="math inline">\(\alpha
_t\)</span> , and output the probabilities together as the scores relied
on during the 束 search during decoding.</p>
<p><span class="math display">\[
d_{\alpha , t} = \min KL(\alpha _t , \alpha _i) \\
d_{c , t} = \max cosine(c _t , c _i) \\
d_{s , t} = \max cosine(s _t , s _i) \\
\]</span></p></li>
</ul></li>
</ul>
<h1
id="document-modeling-with-external-attention-for-sentence-extraction">Document
Modeling with External Attention for Sentence Extraction</h1>
<ul>
<li>A retrieval-based summarization model was constructed, consisting of
a hierarchical document encoder and an extractor based on external
information attention. In the summarization task, the external
information is image captions and document titles.</li>
<li>By implicitly estimating the local and global relevance of each
sentence to the document and explicitly considering external
information, it determines whether each sentence should be included in
the abstract.</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oLjS.png" alt="i0oLjS.png" />
<figcaption aria-hidden="true">i0oLjS.png</figcaption>
</figure>
<ul>
<li>Sentence-level Encoder: As shown in the figure, using CNN encoding,
each sentence is encoded with three convolutional kernels of sizes 2 and
4 respectively, and the resulting vectors are subjected to maxpooling to
generate a single value, thus the final vector is 6-dimensional.</li>
<li>Document-level encoder: Input the 6-dimensional vector of a
document's sentence sequentially into LSTM for encoding.</li>
<li>Sentence Extractor: Composed of an LSTM with attention mechanism,
unlike the general generative seq2seq, the encoding of the sentence is
not only used as the encoding input in the seq2seq but also as the
decoding input, with one being in reverse order and the other in normal
order. The extractor relies on the encoding side input <span
class="math inline">\(s_t\)</span> , the previous time step state on the
decoding side <span class="math inline">\(h_t\)</span> , and the
attention-weighted external information <span
class="math inline">\(h_t^{&#39;}\)</span> .</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oIAA.png" alt="i0oIAA.png" />
<figcaption aria-hidden="true">i0oIAA.png</figcaption>
</figure>
<h1
id="get-to-the-point-summarization-with-pointer-generator-networks">Get
To The Point: Summarization with Pointer-Generator Networks</h1>
<ul>
<li><p>Presented two mechanisms, Pointer-Generator addresses the OOV
problem, and coverage resolves the issue of repeated words</p></li>
<li><p>Pointer-Generator: Learning pointer probabilities through
context, the decoder state of the current timestep, and input</p>
<p><span class="math display">\[
p_{gen} = \sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\
P(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \sum _{i:w_i = w} a_i^t \\
\]</span></p></li>
<li><p>Pointer probability indicates whether a word should be normally
generated or sampled from the input according to the current attention
distribution, in the above formula. If the current label is OOV, the
left part is 0, maximizing the right part to allow the attention
distribution to indicate the position of the copied word; if the label
is a newly generated word (not mentioned in the original text), the
right part is 0, and maximizing the left part means generating words
normally using the decoder. Overall, it learns the correct pointer
probability.</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ootI.png" alt="i0ootI.png" />
<figcaption aria-hidden="true">i0ootI.png</figcaption>
</figure>
<ul>
<li><p>Coverage: Utilizing the coverage mechanism to adjust attention,
so that words that received more attention in previous timesteps receive
less attention</p></li>
<li><p>Common Attention Calculation</p>
<p><span class="math display">\[
e_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\
a^t = softmax(e^t) \\
\]</span></p></li>
<li><p>Maintain a coverage vector indicating how much attention each
word has received prior to this:</p>
<p><span class="math display">\[
c^t = \sum _{t^{temp} = 0}^t-1 a^{t^{temp}}
\]</span></p></li>
<li><p>Then use its corrected attention generation to make the attention
generation consider the previous accumulated attention</p>
<p><span class="math display">\[
e_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})
\]</span></p></li>
<li><p>And add a coverage loss to the loss function</p>
<p><span class="math display">\[
covloss_t = \sum _i \min (a_i^t , c_i^t)
\]</span></p></li>
<li><p>The use of min means that we only penalize the overlapping parts
of the attention and coverage distributions, i.e., if coverage is large
and attention is also large, then covloss is large; if coverage is
small, regardless of the attention, covloss is small</p></li>
</ul>
<h1
id="summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents">SummaRuNNer:
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oTht.png" alt="i0oTht.png" />
<figcaption aria-hidden="true">i0oTht.png</figcaption>
</figure>
<ul>
<li><p>Using RNN for extractive summarization, the model decision
process can be visualized, and an end-to-end training method is
employed</p></li>
<li><p>Treat extraction as a sentence classification task, visit each
sentence in the order of the original text, and decide whether to
include it in the abstract, with this decision considering the results
of previous decisions.</p></li>
<li><p>Encoding at the word level using a bidirectional GRU, followed by
encoding at the sentence level using another bidirectional GRU; the
encodings from both layers are concatenated in reverse order and then
averaged through pooling</p>
<p><span class="math display">\[
d = tanh(W_d \frac {1}{N_d} \sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)
\]</span></p></li>
<li><p>d is the encoding of the entire document, <span
class="math inline">\(h_j^f\)</span> and <span
class="math inline">\(h_j^b\)</span> represent the forward and reverse
encodings of the sentence through GRU</p></li>
<li><p>Afterward, a neural network is trained for binary classification
based on the coding of the entire document, the coding of the sentences,
and the dynamic representation of the abstract at the current sentence
position, to determine whether each sentence should be included in the
abstract:</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ob1f.png" alt="i0ob1f.png" />
<figcaption aria-hidden="true">i0ob1f.png</figcaption>
</figure>
<ul>
<li><p>sj represents the abstraction generated up to position j,
obtained by weighted summation of the encoding of previous sentences
using the binary classification probability of each sentence:</p>
<p><span class="math display">\[
s_j = \sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)
\]</span></p></li>
<li><p>First line: The parameter is the encoding of the current
sentence, representing the content of the current sentence</p></li>
<li><p>Second line: Parameters are document encoding and sentence
encoding, indicating the significance of the current sentence to the
document</p></li>
<li><p>Third line: The parameters are the sentence encoding and the
dynamic encoding of the summary, indicating the redundancy of the
current sentence to the generated summary.</p></li>
<li><p>Fourth and fifth lines: Considered the relative and absolute
positions of sentences within the document. (The absolute position
denotes the actual sentence number, whereas the relative position refers
to a quantized representation that divides each document into a fixed
number of segments and computes the segment ID of a given
sentence.)</p></li>
<li><p>Finally, perform the maximum likelihood estimation on the entire
model:</p>
<p><span class="math display">\[
l(W,b) = -\sum _{d=1}^N \sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 |
h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))
\]</span></p></li>
<li><p>The author applies this extraction method to generative
summarization corpora, that is, how to label each sentence in the
original text with a binary classification. The author believes that the
subset of sentences labeled as 1 should correspond to the maximum ROUGE
value of the generative summary, but finding all subsets is too
time-consuming, so a greedy method is used: sentences are added one by
one to the subset, and if no remaining sentence can increase the ROUGE
value of the current subset, it is not added. In this way, the
generative summarization corpora are converted into extraction
summarization corpora.</p></li>
<li><p>Another approach is to train directly on the generative abstract
corpus, taking the dynamic abstract representation mentioned above,
specifically the last sentence which contains the entire document's
abstract representation s, and inputting it into a decoder to generate
the generative abstract. Since the abstract representation is the only
input to the decoder, training the decoder also allows learning good
abstract representations, thereby completing the task of extractive
summarization.</p></li>
<li><p>Because several components are included in generating the binary
classification probabilities, normalizing them allows for the
visualization of the contributions made by each component, thereby
illustrating the decision-making process:</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oqc8.png" alt="i0oqc8.png" />
<figcaption aria-hidden="true">i0oqc8.png</figcaption>
</figure>
<h1 id="attention-is-all-you-need">Attention Is All You Need</h1>
<ul>
<li>Abandoned RNN and CNN for seq2seq tasks, directly using multi-head
attention to compose network blocks and stack them, adding BN layers and
residual connections to construct a deep network</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oXng.png" alt="i0oXng.png" />
<figcaption aria-hidden="true">i0oXng.png</figcaption>
</figure>
<ul>
<li>The benefit of using attention exclusively is speed.</li>
<li>In order to utilize residuals, all submodules (multi-head attention
and fully connected) are unified to output dimensions of 512</li>
<li>Encoding end: 6 blocks, each containing an attention and a fully
connected sub-module, both using residuals and batch normalization.</li>
<li>Decoder side: Also consists of 6 blocks, the difference being the
addition of an attention mechanism to process the output from the
encoding side, and the attention mechanism connected to the decoder
input uses a mask to ensure directionality, that is, the output at the
i-th position is only related to the output at previous positions.</li>
<li>The six blocks of encoding and decoding are all stacked
(stacked)</li>
<li>The general attention model refers to a mechanism that maps a query
and a series of key-value pairs to an output, where the output is a
weighted sum of the values, and the weight of each value is calculated
by a compatibility function corresponding to the key and the query
input. The traditional attention keys and values are the same, both
being the hidden layer states at each input position, with the query
being the current output, and the compatibility function being various
attention calculation methods. The three arrows pointing to attention in
the diagram represent key, value, and query respectively.</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TSNn.png" alt="i0TSNn.png" />
<figcaption aria-hidden="true">i0TSNn.png</figcaption>
</figure>
<ul>
<li><p>Multi-head attention is composed of multiple parallel scaled
dot-product attention mechanisms.</p></li>
<li><p>Scaled dot-product attention, as shown, first performs a dot
product between the query and key, then scales, and if it is the
attention from the decoder input, a mask is added. After that, it passes
through the softmax function to perform a dot product with the value to
obtain the attention weights. In actual computation, to accelerate, a
series of queries, keys, and values are calculated together, so Q, K,
and V are all matrices. The scaling is to prevent the dot product
attention from being at the ends of softmax when the dimension of k is
too large, resulting in small gradients.</p>
<p><span class="math display">\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}}) V
\]</span></p></li>
<li><p>Multi-head attention is a scaled dot-product attention with h
projections on V, K, and Q, learning different features, and finally
concatenating and performing a linear transformation. The authors
believe that this multi-head design allows the model to learn the
information of representation subspaces at different positions.</p>
<p><span class="math display">\[
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\
where \ \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\
\]</span></p></li>
<li><p>In the paper, 8 heads are taken, and to ensure dimension
consistency, the dimensions of individual q, k, and v are set to
512/8=64</p></li>
<li><p>This multi-head attention is used in three places in the
model:</p>
<ul>
<li>-based attention mechanism.</li>
<li>Self-attention between encoding blocks and blocks</li>
<li>Decoding blocks and inter-block self-attention</li>
</ul></li>
<li><p>In each block, there is also a fully connected layer, which
contains two linear transformations, with ReLU activation inserted in
between, and the same parameters are used at each input position, but
the parameters of the fully connected layers in different blocks are
different</p>
<p><span class="math display">\[
FFN(x) =\max (0,xW_1+b_1)W_2 +b_2 \\
\]</span></p></li>
<li><p>The complete use of attention would discard the sequential order
information of the sequence; to utilize this information, trigonometric
positional encoding is added to make use of relative positional
information:</p>
<p><span class="math display">\[
PE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\
\]</span></p></li>
</ul>
<h1
id="a-joint-selective-mechanism-for-abstractive-sentence-summarization">A
Joint Selective Mechanism for Abstractive Sentence Summarization</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/11/15/ivQCE8.png" alt="ivQCE8.png" />
<figcaption aria-hidden="true">ivQCE8.png</figcaption>
</figure>
<ul>
<li><p>Abstracts differ from translations; end-to-end frameworks should
model the loss (information compression) rather than simply align as
translations do</p></li>
<li><p>The author made two improvements to loss modeling:</p>
<ul>
<li>After encoding is completed, a threshold is added for trimming the
encoded information</li>
<li>Added a selection loss, focusing on both input and output, to assist
the threshold operation</li>
</ul></li>
<li><p>The selection of the threshold considers both the hidden layer
states after encoding and the original word embeddings, and acts upon
the hidden layer states, truncating the hidden vectors before passing
them through attention-weighted generation of context. The authors
believe that this process is equivalent to allowing the network to
observe the word embeddings before and after the rnn processing, thereby
knowing which word in the input is important for generating the
abstract:</p>
<p><span class="math display">\[
g_i = \sigma (W_g h_i + U_g u_i) \\
h_i^{&#39;} = h_i \cdot g_i \\
\]</span></p></li>
<li><p>The selection of the loss function constructs a review threshold
at the decoding end, considering the hidden layers of the encoding end
and the original input, the hidden layers of the decoding end and the
original input, and the review threshold at each position of the
decoding end is the average of the review thresholds at all positions of
the encoding end:</p>
<p><span class="math display">\[
r_{i,t} = \sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\
r_i = \frac 1m \sum _{t=2}^{m+1} r_{i,t} \\
\]</span></p></li>
<li><p>The author believes that the role of the review threshold is
equivalent to allowing the network reading to generate abstracts and to
review the input text, so that it knows how to select
abstracts.</p></li>
<li><p>Afterward, use the Euclidean distance with the selection
threshold and review threshold as the selection loss, and add it to the
total loss:</p>
<p><span class="math display">\[
d(g,r) = \frac 1n \sum _{i=1}^n |r_i - g_i | \\
L = -p(y|x,\theta) + \lambda d(g,r) \\
\]</span></p></li>
<li><p>The author does not explain why the Euclidean distance between
the review threshold and the selection threshold is taken as the loss
function, nor does it clarify the distinction between the selection
threshold and attention. It seems like a type of attention mechanism
that considers the original input embedding, and it first trims each
hidden layer at every time step before traditional attention weighting.
The selected visual examples are also very clever, precisely
demonstrating that this selection mechanism can identify shifts in
sentences, thus changing the selected words, which contrasts with the
original paper proposing the selection threshold, Selective Encoding for
Abstractive Sentence Summarization. The original paper also does not
explain the motivation for this design.</p></li>
</ul>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1
id="distraction-based-neural-networks-for-document-summarization">Distraction-Based
Neural Networks for Document Summarization</h1>
<ul>
<li><p>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。
<img data-src="https://s1.ax1x.com/2018/10/20/i0oh0H.png"
alt="i0oh0H.png" /></p></li>
<li><p>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：</p>
<p><span class="math display">\[
s_t = GRU _1 (s_t^{temp},c_t) \\
s_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\
\]</span></p></li>
<li><p>这个控制层捕捉<span
class="math inline">\(s_t^{&#39;}\)</span>和<span
class="math inline">\(c_t\)</span>之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而<span
class="math inline">\(e(y_{t-1})\)</span>是上一次输入的embedding。</p></li>
<li><p>三种注意力分散模型</p>
<ul>
<li><p>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制</p>
<p><span class="math display">\[
c_t = tanh (W_c c_t^{temp} - U_c \sum _{j=1}^{t-1} c_j) \\
c_t^{temp} = \sum _{i=1}^{T_x} \alpha _{t,i} h_i \\
\]</span></p></li>
<li><p>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化</p>
<p><span class="math display">\[
\alpha _{t,i}^{temp} = v_{\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a
\sum _{j=1}^{t-1}\alpha _{j,i}) \\
\alpha _{t,i} = \frac {exp(\alpha _{t,i}^{temp})}{\sum _{j=1}^{T_x}
exp(\alpha _{t,j}^{temp})} \\
\]</span></p></li>
<li><p>M3：在解码端做分散，计算当前的<span
class="math inline">\(c_t\)</span>，<span
class="math inline">\(s_t\)</span>，<span class="math inline">\(\alpha
_t\)</span>和历史的<span class="math inline">\(c_t\)</span>，<span
class="math inline">\(s_t\)</span>，<span class="math inline">\(\alpha
_t\)</span>之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。</p>
<p><span class="math display">\[
d_{\alpha , t} = \min KL(\alpha _t , \alpha _i) \\
d_{c , t} = \max cosine(c _t , c _i) \\
d_{s , t} = \max cosine(s _t , s _i) \\
\]</span></p></li>
</ul></li>
</ul>
<h1
id="document-modeling-with-external-attention-for-sentence-extraction">Document
Modeling with External Attention for Sentence Extraction</h1>
<ul>
<li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。
在文摘任务中，外部信息是图片配字和文档标题。</li>
<li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oLjS.png" alt="i0oLjS.png" />
<figcaption aria-hidden="true">i0oLjS.png</figcaption>
</figure>
<ul>
<li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li>
<li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li>
<li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入<span
class="math inline">\(s_t\)</span>，解码端的上一时间步状态<span
class="math inline">\(h_t\)</span>，以及进行了注意力加权的外部信息<span
class="math inline">\(h_t^{&#39;}\)</span>：</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oIAA.png" alt="i0oIAA.png" />
<figcaption aria-hidden="true">i0oIAA.png</figcaption>
</figure>
<h1
id="get-to-the-point-summarization-with-pointer-generator-networks">Get
To The Point: Summarization with Pointer-Generator Networks</h1>
<ul>
<li><p>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</p></li>
<li><p>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率</p>
<p><span class="math display">\[
p_{gen} = \sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\
P(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \sum _{i:w_i = w} a_i^t \\
\]</span></p></li>
<li><p>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ootI.png" alt="i0ootI.png" />
<figcaption aria-hidden="true">i0ootI.png</figcaption>
</figure>
<ul>
<li><p>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</p></li>
<li><p>普通注意力计算</p>
<p><span class="math display">\[
e_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\
a^t = softmax(e^t) \\
\]</span></p></li>
<li><p>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</p>
<p><span class="math display">\[
c^t = \sum _{t^{temp} = 0}^t-1 a^{t^{temp}}
\]</span></p></li>
<li><p>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</p>
<p><span class="math display">\[
e_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})
\]</span></p></li>
<li><p>并在损失函数里加上coverage损失</p>
<p><span class="math display">\[
covloss_t = \sum _i \min (a_i^t , c_i^t)
\]</span></p></li>
<li><p>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</p></li>
</ul>
<h1
id="summarunner-a-recurrent-neural-network-based-sequence-model-for-extractive-summarization-of-documents">SummaRuNNer
A Recurrent Neural Network based Sequence Model for Extractive
Summarization of Documents</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oTht.png" alt="i0oTht.png" />
<figcaption aria-hidden="true">i0oTht.png</figcaption>
</figure>
<ul>
<li><p>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</p></li>
<li><p>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</p></li>
<li><p>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</p>
<p><span class="math display">\[
d = tanh(W_d \frac {1}{N_d} \sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)
\]</span></p></li>
<li><p>其中d是整篇文档的编码，<span
class="math inline">\(h_j^f\)</span>和<span
class="math inline">\(h_j^b\)</span>代表句子经过GRU的正反向编码</p></li>
<li><p>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0ob1f.png" alt="i0ob1f.png" />
<figcaption aria-hidden="true">i0ob1f.png</figcaption>
</figure>
<ul>
<li><p>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</p>
<p><span class="math display">\[
s_j = \sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)
\]</span></p></li>
<li><p>第一行：参数为当前句子编码，表示当前句子的内容</p></li>
<li><p>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</p></li>
<li><p>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We
squash the summary representation using the tanh operation so that the
magnitude of summary remains the same for all time-steps.）</p></li>
<li><p>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The
absolute position denotes the actual sentence number, whereas the
relative position refers to a quantized representation that divides each
document into a fixed number of segments and computes the segment ID of
a given sentence.）</p></li>
<li><p>最后对整个模型做最大似然估计:</p>
<p><span class="math display">\[
l(W,b) = -\sum _{d=1}^N \sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 |
h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))
\]</span></p></li>
<li><p>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</p></li>
<li><p>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</p></li>
<li><p>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</p></li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oqc8.png" alt="i0oqc8.png" />
<figcaption aria-hidden="true">i0oqc8.png</figcaption>
</figure>
<h1 id="attention-is-all-you-need">Attention Is All You Need</h1>
<ul>
<li>抛弃了RNN和CNN做seq2seq任务，直接用multi head
attention组成网络块叠加，加入BN层和残差连接构造深层网络</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0oXng.png" alt="i0oXng.png" />
<figcaption aria-hidden="true">i0oXng.png</figcaption>
</figure>
<ul>
<li>完全使用attention的一个好处就是快。</li>
<li>为了使用残差，所有的子模块（multi-head
attention和全连接）都统一输出维度为512</li>
<li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li>
<li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li>
<li>编码与解码的6个块都是堆叠的(stack)，</li>
<li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li>
</ul>
<figure>
<img data-src="https://s1.ax1x.com/2018/10/20/i0TSNn.png" alt="i0TSNn.png" />
<figcaption aria-hidden="true">i0TSNn.png</figcaption>
</figure>
<ul>
<li><p>Multi-head attention由多个scaled dot-product
attention并行组成。</p></li>
<li><p>Scaled dot-product
attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</p>
<p><span class="math display">\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}}) V
\]</span></p></li>
<li><p>Multi-head attention就是有h个scaled dot-product
attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</p>
<p><span class="math display">\[
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\
where \ \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\
\]</span></p></li>
<li><p>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</p></li>
<li><p>这种multi-head attention用在了模型的三个地方：</p>
<ul>
<li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked
multi-head attention的输出。也就是传统的attention位置</li>
<li>编码端块与块之间的自注意力</li>
<li>解码端块与块之间的自注意力</li>
</ul></li>
<li><p>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</p>
<p><span class="math display">\[
FFN(x) =\max (0,xW_1+b_1)W_2 +b_2 \\
\]</span></p></li>
<li><p>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</p>
<p><span class="math display">\[
PE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\
\]</span></p></li>
</ul>
<h1
id="a-joint-selective-mechanism-for-abstractive-sentence-summarization">A
Joint Selective Mechanism for Abstractive Sentence Summarization</h1>
<figure>
<img data-src="https://s1.ax1x.com/2018/11/15/ivQCE8.png" alt="ivQCE8.png" />
<figcaption aria-hidden="true">ivQCE8.png</figcaption>
</figure>
<ul>
<li><p>文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模</p></li>
<li><p>作者针对损失建模，做了两点改进：</p>
<ul>
<li>在编码完成之后添加了一个门限用于裁剪编码信息</li>
<li>添加了一个选择损失，同时关注输入和输出，辅助门限工作</li>
</ul></li>
<li><p>选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：</p>
<p><span class="math display">\[
g_i = \sigma (W_g h_i + U_g u_i) \\
h_i^{&#39;} = h_i \cdot g_i \\
\]</span></p></li>
<li><p>而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：</p>
<p><span class="math display">\[
r_{i,t} = \sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\
r_i = \frac 1m \sum _{t=2}^{m+1} r_{i,t} \\
\]</span></p></li>
<li><p>作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。</p></li>
<li><p>之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:</p>
<p><span class="math display">\[
d(g,r) = \frac 1n \sum _{i=1}^n |r_i - g_i | \\
L = -p(y|x,\theta) + \lambda d(g,r) \\
\]</span></p></li>
<li><p>作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective
Encoding for Abstractive Sentence Summarization
也没有说出这种设计的动机。</p></li>
</ul>
</div>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/abstractive-summarization/" rel="tag"># abstractive summarization</a>
              <a href="/tags/theory/" rel="tag"># theory</a>
              <a href="/tags/nlp/" rel="tag"># nlp</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/03/07/PaperReading/" rel="prev" title="Paper Reading 1">
                  <i class="fa fa-angle-left"></i> Paper Reading 1
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2018/07/04/seq2seq-summarization/" rel="next" title="Seq2seq based Summarization">
                  Seq2seq based Summarization <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:34</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2018/07/03/PaperReading2/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
