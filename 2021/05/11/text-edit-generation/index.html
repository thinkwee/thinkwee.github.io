<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Record the methods of editing seq2seq in recent years, which have the advantages of high efficiency (partially autoregressive or non-autoregressive decoding) and less data hungry (small output vocab">
<meta property="og:type" content="article">
<meta property="og:title" content="Edit-based Text Generation">
<meta property="og:url" content="https://thinkwee.top/2021/05/11/text-edit-generation/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="Record the methods of editing seq2seq in recent years, which have the advantages of high efficiency (partially autoregressive or non-autoregressive decoding) and less data hungry (small output vocab">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/11/gaoEPx.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gdTVa9.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gdxc0x.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwZarT.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwZ5IH.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwKovR.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/11/gaoEPx.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gdTVa9.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gdxc0x.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwZarT.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwZ5IH.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwKovR.png">
<meta property="og:image" content="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png">
<meta property="article:published_time" content="2021-05-11T07:45:26.000Z">
<meta property="article:modified_time" content="2025-01-30T02:10:45.363Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="math">
<meta property="article:tag" content="theory">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="seq2seq">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z3.ax1x.com/2021/05/11/gaoEPx.png">


<link rel="canonical" href="https://thinkwee.top/2021/05/11/text-edit-generation/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2021/05/11/text-edit-generation/","path":"2021/05/11/text-edit-generation/","title":"Edit-based Text Generation"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Edit-based Text Generation | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">50</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#levenshtein-transformer"><span class="nav-number">1.</span> <span class="nav-text">Levenshtein Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-definition"><span class="nav-number">1.1.</span> <span class="nav-text">Problem Definition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training"><span class="nav-number">1.2.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inference"><span class="nav-number">1.3.</span> <span class="nav-text">Inference</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#experiments"><span class="nav-number">1.4.</span> <span class="nav-text">Experiments</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#editnts"><span class="nav-number">2.</span> <span class="nav-text">EditNTS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lasertagger"><span class="nav-number">3.</span> <span class="nav-text">LaserTagger</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#text-editing-as-tagging"><span class="nav-number">3.1.</span> <span class="nav-text">Text Editing as Tagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#experiment"><span class="nav-number">3.2.</span> <span class="nav-text">Experiment</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pie"><span class="nav-number">4.</span> <span class="nav-text">PIE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#felix"><span class="nav-number">5.</span> <span class="nav-text">Felix</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#summary"><span class="nav-number">6.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#levenshtein-transformer"><span class="nav-number">7.</span> <span class="nav-text">Levenshtein Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E9%97%AE%E9%A2%98"><span class="nav-number">7.1.</span> <span class="nav-text">定义问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">7.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E6%96%AD"><span class="nav-number">7.3.</span> <span class="nav-text">推断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">7.4.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#editnts"><span class="nav-number">8.</span> <span class="nav-text">EditNTS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lasertagger"><span class="nav-number">9.</span> <span class="nav-text">LaserTagger</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#text-editing-as-tagging"><span class="nav-number">9.1.</span> <span class="nav-text">Text Editing as Tagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="nav-number">9.2.</span> <span class="nav-text">实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pie"><span class="nav-number">10.</span> <span class="nav-text">PIE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#felix"><span class="nav-number">11.</span> <span class="nav-text">Felix</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">12.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2021/05/11/text-edit-generation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Edit-based Text Generation | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Edit-based Text Generation
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-11 15:45:26" itemprop="dateCreated datePublished" datetime="2021-05-11T15:45:26+08:00">2021-05-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-30 10:10:45" itemprop="dateModified" datetime="2025-01-30T10:10:45+08:00">2025-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span id="/2021/05/11/text-edit-generation/" class="post-meta-item leancloud_visitors" data-flag-title="Edit-based Text Generation" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>27k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>24 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><hr />
<ul>
<li>Record the methods of editing seq2seq in recent years, which have
the advantages of high efficiency (partially autoregressive or
non-autoregressive decoding) and less data hungry (small output
vocabulary) for tasks with the same language input and output and minor
changes (error correction, simplification, summarization).</li>
<li>Mainly read five papers, sorted by their publication date on arXiv:
<ul>
<li>(LevT, Facebook) Levenshtein Transformer</li>
<li>(Huawei) EditNTS: An Neural Programmer-Interpreter Model for
Sentence Simplification through Explicit Editing</li>
<li>(LaserTagger, Google) Encode, Tag, Realize: High-Precision Text
Editing</li>
<li>(PIE) Parallel Iterative Edit Models for Local Sequence
Transduction</li>
<li>(Google) Felix: Flexible Text Editing Through Tagging and
Insertion</li>
</ul></li>
</ul>
<span id="more"></span>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><h1 id="levenshtein-transformer">Levenshtein Transformer</h1>
<ul>
<li>Contributions
<ul>
<li>Proposed an edit-based transformer using insertion and deletion
operations, improving efficiency by 5 times.</li>
<li>Designed a dual-policy reinforcement learning training method,
addressing the complementarity of insertion and deletion.</li>
<li>Unified the tasks of text generation and text refinement.</li>
</ul></li>
</ul>
<h2 id="problem-definition">Problem Definition</h2>
<ul>
<li><p>The authors unify text generation and text refinement into a
single Markov process, defined by the tuple (Y, A, <span
class="math inline">\(\xi\)</span>, R, <span
class="math inline">\(y_0\)</span>) (sequence, action set, environment,
reward function, initial sequence).</p>
<ul>
<li>Y represents the text sequence, with size <span
class="math inline">\(V^N\)</span>.</li>
<li><span class="math inline">\(\xi\)</span> is the environment. At each
decoding step, the agent receives a sequence y as input, takes an action
a, and then receives a reward r.</li>
<li>The reward function is defined as the distance between the current
sequence and the ground truth sequence, using Levenshtein distance (edit
distance) in this case.</li>
<li>The policy the agent learns is a mapping from sequence y to a
probability distribution over actions P(A).</li>
<li>It is worth noting that <span class="math inline">\(y_0\)</span> can
be an empty sequence, representing text generation, or a previously
generated sequence, representing text refinement.</li>
</ul></li>
<li><p>Actions</p>
<ul>
<li>Deletion operation: For each token in the sequence, perform binary
classification to decide whether it should be deleted. The first and
last tokens are retained without deletion checks to avoid breaking the
sequence boundaries.</li>
<li>Insertion operation: The insertion operation is performed in two
steps: first, predict the insert position by performing a multi-class
classification on each pair of adjacent token positions (the number of
insert positions). It is important to note that the input includes these
two tokens. Then, for each predicted position, a classification is done
over a vocabulary V to generate the specific word to insert.</li>
</ul></li>
<li><p>The entire process is divided into three steps: input sequence
<span class="math inline">\(y_0\)</span>, perform deletion operations in
parallel at all positions in the sequence to obtain <span
class="math inline">\(y_1\)</span>; for <span
class="math inline">\(y_1\)</span>, predict insert positions at all
positions in the sequence, add placeholders according to the predictions
to obtain <span class="math inline">\(y_2\)</span>; for all placeholders
in <span class="math inline">\(y_2\)</span>, predict specific words to
generate the final result, completing one iteration.</p></li>
<li><p>The three operations share a single transformer, and to reduce
complexity (turning one autoregressive text generation round into three
non-autoregressive operations), the classifiers for deletion and insert
position predictions are attached to the middle transformer blocks. The
classifier for text generation is only attached to the final transformer
block, as the first two tasks are relatively simple and do not require
complex feature extraction.</p></li>
</ul>
<h2 id="training">Training</h2>
<ul>
<li><p>The training process follows imitation learning. While there are
many reinforcement learning concepts, it essentially involves perturbing
the data and having the model learn to recover, still following Maximum
Likelihood Estimation (MLE). I will attempt to explain its specific
operations in the context of NLP tasks:</p>
<ul>
<li><p>The training still uses teacher forcing, meaning the model
directly learns to predict the oracle (ground truth) from the input,
without autoregression.</p></li>
<li><p>For Levenshtein Transformer, three components are needed during
training:</p>
<ul>
<li>Training input: The input is the text requiring edit operations
(state distribution fed during training). These sentences should be
original texts that have been disturbed by perturbation operations
(roll-in policy). This is easy to understand—e.g., deleting some tokens
and then having the model learn to insert them. The construction of this
input involves two steps: determining the original text and applying
perturbation operations. The original text can either be empty or the
ground truth. In the design of the perturbation operations, the authors
introduce the so-called dual policy, which essentially means allowing
the model’s own operations to also serve as perturbations.</li>
</ul>
<table>
<colgroup>
<col style="width: 39%" />
<col style="width: 24%" />
<col style="width: 21%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Operation</th>
<th>Perturbation Type</th>
<th>Learn from Model</th>
<th>Dual Policy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Randomly drop words from oracle text as input</td>
<td>Deletion</td>
<td>Insertion</td>
<td></td>
</tr>
<tr class="even">
<td>Directly use oracle input text as input</td>
<td>Insertion</td>
<td>Deletion</td>
<td></td>
</tr>
<tr class="odd">
<td>Use the text after model deletion as input</td>
<td>Deletion</td>
<td>Insertion</td>
<td>√</td>
</tr>
<tr class="even">
<td>Use the text after model insertion as input</td>
<td>Insertion</td>
<td>Deletion</td>
<td>√</td>
</tr>
</tbody>
</table>
<p>The authors use probabilities to choose the operations for
constructing data, as shown in the diagram below:</p>
<p><a target="_blank" rel="noopener" href="https://imgtu.com/i/gaoEPx"><img data-src="https://z3.ax1x.com/2021/05/11/gaoEPx.png"
alt="gaoEPx.png" /></a></p>
<ul>
<li>Training output: After learning the opposite operations of the
perturbation, the model needs to restore the original text. Once the
input and output are determined, the ground truth actions (expert
policy) are set. Generally, the model should restore the oracle, but
since direct restoration can be difficult, the model is trained to
restore a low-noise version with a certain probability. This low-noise
version is derived from sequence distillation. Specifically, a normal
transformer is trained on a unified dataset, and this transformer is
used to infer each piece of data, using beam search to find non-top-1
sentences as low-noise versions for the model to learn to restore.</li>
<li>Ground truth actions during training: Once the input and output are
determined, the minimum edit distance operation is computed using
dynamic programming as the ground truth action (expert policy) for the
model to learn.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="inference">Inference</h2>
<ul>
<li>During inference, the model performs multiple rounds of editing on
the input text (each round includes one deletion, prediction of
position, and insertion), greedily selecting the operation with the
highest probability.</li>
<li>Multiple rounds of editing continue until:
<ul>
<li>A loop occurs (i.e., the same text reappears after editing),</li>
<li>A preset maximum number of rounds is reached.</li>
</ul></li>
<li>Too many placeholders can lead to overly short generated text, and a
penalty term can be added to control the number of placeholders.</li>
<li>The use of two editing operations adds a degree of interpretability
to the model’s inference.</li>
</ul>
<h2 id="experiments">Experiments</h2>
<ul>
<li>The evaluation metrics include six items: BLEU scores for three
translation pairs and ROUGE-1, ROUGE-2, and L values for the Gigaword
summarization task. For English-Japanese translation and Gigaword, the
original Transformer achieved the best results, outperforming LevT by
about one point. For the other two translation datasets, the
sequence-distilled LevT outperformed the original Transformer by about
one point.</li>
<li>For LevT, training with sequence distillation is generally better
than using the original oracle sequence.</li>
<li>In terms of inference speed, the LevT with sequence distillation is
the fastest, with single sentence inference around 90ms, while the
original Transformer takes 200-300ms for translation and 116ms for
summarization tasks. The authors also report the number of inference
iterations, showing that LevT requires an average of only 2 iterations,
i.e., two deletions and two insertions, applied to the entire sentence.
In contrast, the original Transformer’s average number of iterations is
proportional to the length of the sentence. For short summarization
datasets, the average number of iterations is 10.1, while for
translation datasets, it reaches over 20. This demonstrates another
advantage of LevT: its inference speed is not overly sensitive to text
length and is primarily related to the number of iterations required for
processing the entire sentence. The growth rate of iterations with
length is smaller than the logarithmic complexity and far smaller than
the quadratic complexity of traditional Transformers. The inference
latency of the Transformer scales roughly linearly with text length,
while LevT remains around 90ms with just two iterations.</li>
<li>Ablation studies show that the design of the two operations,
parameter sharing, and the dual policy all contribute to better final
metrics.</li>
<li>For deletion and insert position prediction, early exiting after the
first transformer block yields good results, with BLEU dropping 0.4 but
speeding up inference by a factor of 3.5 to 5 times compared to using
the sixth layer.</li>
<li>Additionally, the authors found that a LevT trained on translation
tasks can be used zero-shot for text refinement, with performance within
0.3 BLEU points of a separately trained text refinement LevT, and even
better performance on Ro-En. Both outperform traditional Transformers.
This adaptability to text refinement aligns with intuition, as text
refinement involves smaller edits compared to generating new text, where
the hypothesis space is larger and more difficult to reach an ideal
result. After fine-tuning on a text refinement dataset, LevT
outperformed all models comprehensively.</li>
</ul>
<h1 id="editnts">EditNTS</h1>
<ul>
<li><p>A sentence simplification model is proposed that explicitly
performs three types of operations: insertion, deletion, and
retention.</p></li>
<li><p>Unlike LevT, EditNTS remains autoregressive, meaning it does not
operate on the entire sentence simultaneously, and it introduces more
information when predicting operations.</p></li>
<li><p>The model consists of two parts: an LSTM-based seq2seq model as
the <em>programmer</em> that predicts the operation for each token
position, generating a sequence of editing operations; and an additional
<em>interpreter</em> that executes these operations to generate the
edited sentence. The interpreter also includes a simple LSTM that
aggregates the contextual information of the edited sentence. <a
target="_blank" rel="noopener" href="https://imgtu.com/i/gdTVa9"><img data-src="https://z3.ax1x.com/2021/05/12/gdTVa9.png"
alt="gdTVa9.png" /></a></p></li>
<li><p>Three key points to note:</p>
<ul>
<li>After predicting deletion and retention, the programmer moves to the
next word. However, when predicting an insertion operation, the
programmer stays on the current word and continues predicting the next
operation, which enables the insertion of multiple words.</li>
<li>The interpreter not only performs the editing based on the
programmer’s predicted results, but also uses an LSTM to aggregate the
contextual information of the sentence at the current time step after
editing.</li>
<li>When predicting editing operations, the programmer uses four pieces
of information:
<ul>
<li>The encoder’s context, which comes from the last-layer
attention-weighted output of the encoder LSTM.</li>
<li>The word being processed, which comes from the decoder LSTM’s hidden
state.</li>
<li>The already edited sentence’s context, which comes from the
interpreter LSTM’s attention-weighted output.</li>
<li>The already generated editing operation sequence, which comes from
the attention-weighted edit label output. Here, a simple attention
mechanism is used, not an LSTM.</li>
</ul></li>
</ul></li>
<li><p>The encoder input also introduces position embedding.</p></li>
<li><p>Label construction, similar to LevT, involves creating a ground
truth sequence of editing operations based on an edit distance dynamic
programming approach. When multiple operation sequences exist, the
sequence with more insertions is prioritized. The authors experimented
with other prioritization strategies (e.g., prioritizing deletions,
random, introducing replacement operations), but found that prioritizing
insertions yielded the best results.</p></li>
<li><p>The model uses 100-dimensional GloVe vectors to initialize word
vectors and edit label embeddings, 30-dimensional vectors to initialize
position embeddings, and uses inverse frequency of the edit labels as
loss weights to balance the loss contributions from each label.</p></li>
<li><p>The results are not very impressive. In terms of metrics, the
model does not outperform non-editing methods, and while human
evaluation shows some improvement, it is subjective. Ablation studies do
not show significant improvements.</p></li>
</ul>
<h1 id="lasertagger">LaserTagger</h1>
<ul>
<li>LaserTagger also uses the three operations: retention, deletion, and
insertion, but it incorporates BERT, with Google's engineering
capabilities enabling an acceleration factor of 100-200 times.</li>
<li>Contributions:
<ul>
<li>Introduced the model and proposed a method for generating label
dictionaries from data.</li>
<li>Proposed two versions based on BERT: one directly uses BERT for
tagging, which is faster, and the other uses BERT to initialize a
seq2seq encoder, which yields better results.</li>
</ul></li>
</ul>
<h2 id="text-editing-as-tagging">Text Editing as Tagging</h2>
<ul>
<li>LaserTagger transforms the text editing problem into a sequence
labeling problem, which consists of three main components:
<ul>
<li><strong>Labeling Operations</strong>: Instead of using the three
basic operations (retain, delete, insert), the labels are divided into
two parts: a <em>base tag</em> (B), which includes <em>retain</em> and
<em>delete</em> operations, and an <em>added phrase</em> (P), which can
take <em>V</em> different values representing inserted fragments (empty
space, words, or phrases). The vocabulary for P is built from the
training data. The combination of B and P generates the labels, so there
are a total of 2V possible labels. Task-specific labels can also be
added based on downstream tasks.</li>
<li><strong>Building the Added Phrase Vocabulary</strong>: Constructing
the vocabulary is a combinatorial optimization problem. On one hand, we
want the vocabulary to be as small as possible; on the other, we want it
to cover the most editing scenarios. Finding the perfect solution is
NP-hard, so the authors used another approach. For each training pair,
they compute the longest common subsequence (LCS) between the input and
the target sequence using dynamic programming. The added phrases are
derived by subtracting the LCS from the target sequence. These phrases
are then ranked by frequency, and the top-k phrases are selected. For
public datasets, the top 500 phrases cover 85% of the data.</li>
<li><strong>Constructing the Label Sequence</strong>: This is done using
a greedy approach, matching the shortest phrase to insert for each word,
without the need for dynamic programming. <a
target="_blank" rel="noopener" href="https://imgtu.com/i/gdxc0x"><img data-src="https://z3.ax1x.com/2021/05/12/gdxc0x.png" alt="gdxc0x.png" /></a>
This approach may sometimes result in cases where no edits are made,
which are then filtered out from the training set. The authors argue
that this can also be seen as a form of noise reduction.</li>
<li><strong>Editing the Sentence Based on Predictions</strong>: This is
the direct operation where, based on downstream tasks, special labels
may be predicted to perform task-specific operations. The authors argue
that modularizing the editing operations makes the model more flexible
than an end-to-end approach.</li>
</ul></li>
</ul>
<h2 id="experiment">Experiment</h2>
<ul>
<li>There are two versions of the model: one directly uses BERT for
tagging, and the other uses seq2seq with a BERT-initialized encoder. In
the first case, a transformer block is added to BERT’s last layer to
perform the tagging.</li>
<li>Four tasks were tested: sentence fusion, sentence splitting and
rephrasing, summarization, and grammar correction. All tasks
outperformed the baseline, with sentence fusion achieving new SOTA
results. However, LaserTagger's standout performance lies in its
efficiency, achieving up to 200 times acceleration with just the BERT
encoder (since the text generation is reduced to a sequence labeling
task with a small dictionary). It is also very friendly for few-shot
learning.</li>
<li>The authors also analyzed common issues with seq2seq models and
editing-based models, their respective advantages, and the causes of
these problems:
<ul>
<li><strong>Imaginary Words</strong>: seq2seq models may generate
non-existent words at the subword level, whereas LaserTagger does not
have this problem.</li>
<li><strong>Premature Termination</strong>: seq2seq models may
prematurely generate EOS tokens, resulting in incomplete or overly short
sentences. While this is theoretically possible for LaserTagger, the
authors have not observed it in practice, as it would require predicting
many deletion operations, which are rare in the training data.</li>
<li><strong>Repetitive Phrases</strong>: In sentence splitting tasks,
seq2seq models often repeat phrases, while LaserTagger tends to avoid
splitting or uses a "lazy split."</li>
<li><strong>Hallucination</strong>: Both models may generate text that
is unrelated or counterfactual, but seq2seq models tend to hide this
issue more effectively (appearing fluent but actually illogical), while
LaserTagger is more transparent.</li>
<li><strong>Coreference Resolution</strong>: Both models struggle with
coreference resolution. Seq2seq models may incorrectly replace pronouns
with the wrong noun, while LaserTagger may fail to resolve pronouns
properly.</li>
<li><strong>Error Deletion</strong>: While LaserTagger appears more
controllable in terms of text generation, it still sometimes deletes
words, leaving the remaining sentence fluent but incorrect in
meaning.</li>
<li><strong>Lazy Split</strong>: In sentence splitting tasks,
LaserTagger might only split the sentence without performing any
post-processing on the resulting sentences.</li>
</ul></li>
</ul>
<h1 id="pie">PIE</h1>
<ul>
<li>PIE also combines BERT with sequence labeling, while constructing a
phrase dictionary from the data.</li>
<li>The editing operations include copying (retaining), insertion,
deletion, and morphological transformation (for grammar
correction).</li>
<li>Constructing the phrase dictionary, ground truth editing operations,
and the method are quite similar to LaserTagger.</li>
<li>BERT is extended on both sides with two additional components to
obtain the information required for replacement or insertion:
<ul>
<li>The input layer includes M for mask identifiers (embedding), p for
positional embeddings, and X for word embeddings.</li>
<li>h represents the original BERT information, containing both word and
positional information.</li>
<li>r represents replacement information, where the word at the current
position is masked and replaced by M, and attention is not applied to
the current word's h during the calculation.</li>
<li>a represents insertion information, where the current word is masked
and replaced by M, and the positional embedding p is replaced by the
average positional embedding of neighboring tokens. <a
target="_blank" rel="noopener" href="https://imgtu.com/i/gwZarT"><img data-src="https://z3.ax1x.com/2021/05/12/gwZarT.png"
alt="gwZarT.png" /></a></li>
<li>Afterward, the three types of information are used to compute the
probabilities for different operations, which are then normalized. CARDT
represents the operations: copy (retain), insert, replace, delete, and
morphological transformation. <a target="_blank" rel="noopener" href="https://imgtu.com/i/gwZ5IH"><img data-src="https://z3.ax1x.com/2021/05/12/gwZ5IH.png"
alt="gwZ5IH.png" /></a></li>
<li>The first term in the formula is the score for each editing
operation. The second term is the score for retaining the current word,
with deletion and replacement not contributing to the score. Other
operations are parameterized using the word embedding <span
class="math inline">\(\phi(x_i)\)</span>, though the exact meaning of
<span class="math inline">\(\phi\)</span> is unclear. The third term
represents the effect of new words, which is only relevant for
replacement and insertion.</li>
</ul></li>
<li>Inference also involves multiple rounds of editing (output becoming
input), stopping when a repeated sentence is produced.</li>
<li>The motivation behind the design of the editing operations is not
fully clear, and the final results are quite average. Only under
ensemble settings does PIE perform on par with a Transformer +
Pre-training + LM + Spellcheck + Ensemble Decoding model. Ablation
studies show that pre-trained models are the main contributor to
performance improvement.</li>
</ul>
<h1 id="felix">Felix</h1>
<ul>
<li>Felix divides the editing operations into two non-autoregressive
parts: first, a Transformer-based pointer network performs tagging and
reordering, inserting placeholders; second, a masked language model
(MLM) predicts the words for the placeholders.</li>
<li>The design of Felix considers three needs:
<ul>
<li>Flexible editing, suitable for various text generation tasks.</li>
<li>Efficient utilization of pre-trained models like BERT.</li>
<li>Efficient inference. <a target="_blank" rel="noopener" href="https://imgtu.com/i/gwKovR"><img data-src="https://z3.ax1x.com/2021/05/12/gwKovR.png"
alt="gwKovR.png" /></a></li>
</ul></li>
<li>The diagram below clearly shows the tagging design of Felix, where
<em>y^t</em> represents the editing operation sequence predicted by the
tagging model, and <em>y^m</em> represents the intermediate sequence
after inserting special tokens (REPL, MASK) based on the editing
operation sequence. This intermediate sequence is then fed to the MLM to
predict the words that need to be inserted, resulting in the final
prediction. <a target="_blank" rel="noopener" href="https://imgtu.com/i/gwMHzj"><img data-src="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png"
alt="gwMHzj.md.png" /></a></li>
<li>Two points to note:
<ul>
<li>Although the MLM only predicts the insertion of words, it still
requires the full editing operation information. Instead of simply
deleting the words predicted for deletion, the authors use REPL to
bracket the span that should be deleted, providing the MLM with
information about the deletions.</li>
<li>The diagram shows two forms of Mask and Infill, which involve
different masking strategies for the intermediate sequence and deciding
who is responsible for tasks like multi-token insertion. In the Mask
approach, the task is handled by the tagging model, which directly
predicts multi-word insertions (e.g., <span
class="math inline">\(DEL^{INS\_2}\)</span> for inserting two words,
corresponding to two MASK tokens in the intermediate sequence). In the
Infill approach, the task is handled by the MLM, where four MASK tokens
are prepared for each insertion position, with extra tokens predicted as
PAD.</li>
</ul></li>
<li>To model more flexibly, the tagging model also performs reordering,
as swapping two words requires first deleting and then adding them,
which increases complexity. The reordering is handled using pointer
network attention, which directs each word to its subsequent word, with
the first word indicated by a special CLS token. During inference,
controlled beam search is used to avoid cyclical pointer sequences.</li>
<li>Compared to LaserTagger, Felix performs better in terms of
performance and small sample learning, and it does not suffer from the
limitations of phrase vocabulary. The authors also performed experiments
on phrase vocabulary and reordering, with results available in the
original paper.</li>
</ul>
<h1 id="summary">Summary</h1>
<p>Here's a summary of the key models with their editing operations,
performance, and tasks:</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 12%" />
<col style="width: 14%" />
<col style="width: 18%" />
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Editing Operations</strong></th>
<th><strong>Acceleration</strong></th>
<th><strong>Multiple Word Insertion</strong></th>
<th><strong>Ground Truth Construction</strong></th>
<th><strong>Editing Failure</strong></th>
<th><strong>Test Tasks</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LevT</strong></td>
<td>Insert, Delete</td>
<td>Multi-round non-autoregressive, 5x</td>
<td>Predict placeholder count first, then replace</td>
<td>Dynamic programming based on edit distance</td>
<td>Degrades to text generation</td>
<td>Translation, Summarization</td>
</tr>
<tr class="even">
<td><strong>EditNTS</strong></td>
<td>Insert, Delete, Retain</td>
<td>Autoregressive, acceleration not mentioned</td>
<td>Stay at position for insertion until done</td>
<td>Dynamic programming based on edit distance</td>
<td>Degrades to text generation</td>
<td>Text Simplification</td>
</tr>
<tr class="odd">
<td><strong>LaserTagger</strong></td>
<td>Insert, Delete, Retain</td>
<td>One-round sequence labeling, 100+ times</td>
<td>Direct insertion of phrases, phrase vocabulary built from training
data</td>
<td>Greedy matching</td>
<td>No editing (lazy)</td>
<td>Sentence Fusion, Sentence Splitting and Paraphrasing, Summarization,
Grammar Correction</td>
</tr>
<tr class="even">
<td><strong>PIE</strong></td>
<td>Insert, Delete, Replace, Copy, Morphological Transformation</td>
<td>Sequence labeling, 2x</td>
<td>Direct insertion of phrases, phrase vocabulary built from training
data</td>
<td>Greedy matching</td>
<td>Not specified</td>
<td>Grammar Correction</td>
</tr>
<tr class="odd">
<td><strong>Felix</strong></td>
<td>Delete, Insert, Retain, Reordering</td>
<td>Tagging + MLM, faster than LaserTagger</td>
<td>Use multiple masks for MLM to predict</td>
<td>Simple comparison</td>
<td>Poor MLM prediction</td>
<td>Sentence Fusion, Post-processing in Machine Translation,
Summarization, Text Simplification</td>
</tr>
</tbody>
</table>
<p>This table gives an overview of each model's characteristics,
strengths, and weaknesses across different editing operations and
tasks.</p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><h1 id="levenshtein-transformer">Levenshtein Transformer</h1>
<ul>
<li>贡献点
<ul>
<li>提出了基于插入和删除操作的编辑式transformer，效率提升5倍</li>
<li>针对插入和删除互补性，设计了一种dual policy式的强化学习训练方法</li>
<li>统一了文本生成和文本完善（refinement）两个任务</li>
</ul></li>
</ul>
<h2 id="定义问题">定义问题</h2>
<ul>
<li>作者将文本生成和文本完善统一为一个马尔科夫过程，由元组(Y,A,<span
class="math inline">\(\xi\)</span>,R,<span
class="math inline">\(y_0\)</span>)定义（序列，动作集合，环境，奖励函数，初始序列）
<ul>
<li>Y即文本序列，大小为<span class="math inline">\(V^N\)</span></li>
<li><span
class="math inline">\(\xi\)</span>是环境，在解码的每一步，agent接收一个序列y作为输入，采取动作a，然后得到奖励r</li>
<li>奖励函数定义为当前序列和ground
truth序列之间的距离，显然本文就使用了Levenshtein距离（编辑距离）</li>
<li>agent要学习的策略即一个从序列y到动作概率分布P(A)的映射</li>
<li>值得注意的是，<span
class="math inline">\(y_0\)</span>可以是空序列，此时即文本生成，也可以是一个生成好的序列，此时的任务即文本完善</li>
</ul></li>
<li>动作
<ul>
<li>删除操作：对序列中的每一个token做二分类，决定是否应该删除。对于序列中第一个和最后一个token直接保留，不做删除操作的判断，避免序列边界被破坏</li>
<li>插入操作：插入操作分为两步：首先预测insert
position,对每两个相邻token位置做一个多分类（insert
position的数量），值得注意的是输入包含了这两个token；之后再针对每个预测位置做一个词典V大小的分类，生成具体的词插入。</li>
</ul></li>
<li>整个流程分为了三步：输入序列<span
class="math inline">\(y_0\)</span>，并行的对序列所有位置做删除操作，得到<span
class="math inline">\(y_1\)</span>；对<span
class="math inline">\(y_1\)</span>，对序列所有位置预测insert
position，根据预测结果添加placeholder得到序列<span
class="math inline">\(y_2\)</span>；对<span
class="math inline">\(y_2\)</span>中的所有placeholder预测具体的词得到最终结果，完成一轮迭代。</li>
<li>三步操作共享一个transformer，同时为了降低复杂度（一轮自回归文本生成变成了三轮非自回归操作），将删除和insert
position预测操作的分类器接在中间的transformer
block上，只对文本生成的分类器接在最后一层transformer
block上，因为前两个任务比较简单，不需要复杂的特征提取。</li>
</ul>
<h2 id="训练">训练</h2>
<ul>
<li><p>训练的过程为imitation
learning，这里有较多强化学习概念，但实际上就是对数据做扰动然后让模型学习还原，且依然是MLE。我尝试在NLP任务中用其具体操作来解释</p>
<ul>
<li><p>训练依然是teacher forcing，即直接对输入去学习预测oracle（ground
truth），不存在自回归。</p></li>
<li><p>对于Levenshtein Transformer，训练时我们需要三部分：</p>
<ul>
<li>训练时的输入：输入即需要做编辑操作的文本（state distribution fed
during training)，这些句子应该是一些原始文本做了一些扰动操作（roll-in
policy）得到的。很容易理解，比如我故意删除了一些token，然后让模型去学习插入。因此该部分文本构建分为两步：确定原始文本，执行扰动操作。其中原始文本可以是空文本，也可以是ground
truth文本。在设计扰动操作时，引入了作者所谓的dual
policy，实际上就是让模型自己的操作也作为扰动操作</li>
</ul>
<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 10%" />
<col style="width: 13%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th>操作</th>
<th>扰动类型</th>
<th>让模型学习</th>
<th>dual policy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>对oracle文本随机丢词作为输入</td>
<td>删除</td>
<td>插入</td>
<td></td>
</tr>
<tr class="even">
<td>直接将oracle输入文本作为输入</td>
<td>插入</td>
<td>删除</td>
<td></td>
</tr>
<tr class="odd">
<td>使用模型执行删除动作后的文本作为输入</td>
<td>删除</td>
<td>插入</td>
<td>√</td>
</tr>
<tr class="even">
<td>使用模型执行插入动作后的文本作为输入</td>
<td>插入</td>
<td>删除</td>
<td>√</td>
</tr>
</tbody>
</table>
<p>这里作者使用概率来选择各个操作构造数据，示意图如下： <a
target="_blank" rel="noopener" href="https://imgtu.com/i/gaoEPx"><img data-src="https://z3.ax1x.com/2021/05/11/gaoEPx.png"
alt="gaoEPx.png" /></a></p>
<ul>
<li>训练时的输出：即模型学习到与扰动相反的操作之后，需要还原的原始文本，当确定了输入输出之后，ground
truth的动作（expert
policy）就确定了。一般来讲自然是要还原到oracle，但是作者考虑到直接学习太难，因此以一定概率模型只需要还原出一个低噪声版本即可。这个低噪声版本由序列蒸馏得到，具体做法是用统一数据集训练一个正常的transformer，然后用这个transformer对每条数据做推断，使用beam
search找出非top 1的句子作为该条数据的低噪声版本给模型学习还原。</li>
<li>训练时的ground truth
action：在确定了输入和输出之后，利用动态规划得到编辑距离消耗最小的操作作为ground
truth action(expert policy)让模型学习</li>
</ul></li>
</ul></li>
</ul>
<h2 id="推断">推断</h2>
<ul>
<li>推断时，直接对输入文本进行多轮编辑（每一轮包括一次删除、预测位置、插入），直接贪心的选择概率最大的操作。</li>
<li>多轮编辑直到：
<ul>
<li>出现循环（即重复出现的编辑后文本）</li>
<li>预设最大轮次</li>
</ul></li>
<li>添加placeholder过多会导致生成的文本过短，可以根据实际情况对placeholder添加惩罚项</li>
<li>用两个编辑操作能够对模型的推断带来一定的可解释性</li>
</ul>
<h2 id="实验">实验</h2>
<ul>
<li>指标结果主要包含六项，三类语对的翻译BLEU值以及摘要Gigaword的ROUGE-1,2，L值。在英日翻译和Gigaword全指标上，原始Transformer取得最好结果，比LevT好一个点左右，在剩下两个翻译数据集上LevT使用序列蒸馏的结果最好，比原始Transformer好一个点左右。</li>
<li>对于LevT，序列蒸馏来训练比用原始oracle序列普遍要好</li>
<li>在推断速度上，使用序列蒸馏的LevT效率最高，推断单句基本在90ms左右，而原始Transformer在翻译数据上要200-300ms，在摘要数据上也要116ms。作者还给出了推断迭代次数，这里可以看到LevT平均只要2次迭代，即两次删除+两次插入，注意这是对整句同时进行操作，而Transformer的平均迭代次数就是平均推断文本长度。短摘要数据集平均迭代10.1次，即平均长度10.1，而翻译数据集上则达到了20+。这样也能看出LevT的另一个优势，就是推断速度对于推断文本长度不是那么敏感，其主要和对整句的处理迭代次数相关，而迭代次数随长度的增长速度小于对数复杂度，远小于传统Transformer的平方复杂度。Transformer的推断延迟和长度基本成正比，而LevT都在90ms左右，两次迭代。</li>
<li>消融表明了两种操作的设计、参数共享、dual
policy都能提升最终指标。</li>
<li>对删除和预测insert position直接用第一层结果early
exit就能取得较好效果，相比取用第六层，BLEU降低0.4，换来加速比从3.5倍提升到5倍</li>
<li>同时作者发现，在翻译任务上训练的LevT可以zero-shot直接用于文本完善，效果比重新训练一个文本完善LevT最小只差了0.3个点，在Ro-En上甚至表现更好，且都优于传统Transformer。这里对于文本完善的适应性也符合直觉，完善文本自然是进行较小的编辑修改，完全重新生成的假设空间更大，难以达到理想目标。在文本完善数据集上微调之后，效果更是全面优于所有模型。</li>
</ul>
<h1 id="editnts">EditNTS</h1>
<ul>
<li>提出了一个句子简化模型，显式地执行三类操作：插入、删除、保留</li>
<li>与LevT不同的是，EditNTS依然是自回归的，并没有同时对整句操作，并且在预测操作时引入了更多的信息</li>
<li>模型包含两部分：基于LSTM的seq2seq作为programmer，预测每个token位置的操作，产生编辑操作序列；以及一个额外的interpreter来执行操作生成编辑后的句子，这个interpreter还包含一个简单的LSTM来汇总编辑后句子的上下文信息
<a target="_blank" rel="noopener" href="https://imgtu.com/i/gdTVa9"><img data-src="https://z3.ax1x.com/2021/05/12/gdTVa9.png"
alt="gdTVa9.png" /></a></li>
<li>值得注意的有三点：
<ul>
<li>预测出删除和保留后，programmer就移动到下一个单词，而预测出插入操作时，programmer不移动单词，接着在这个单词上预测下一个操作，来满足插入多个词的场景</li>
<li>interpreter除了根据programmer预测结果完成编辑外，还用了一个LSTM来汇总当前时间步的编辑后句子信息</li>
<li>programmer在预测编辑操作时，用到了四部分信息：
<ul>
<li>encoder的context，来自encoder lstm last layer attention-weighted
output</li>
<li>当前操作的词，来自decoder lstm hidden state</li>
<li>已经编辑好的句子的context，来自interpreter lstm attention-weighted
output</li>
<li>已经产生的编辑操作序列，来自attention-weighted edit label
output，这里只用了简单的attention，没有用lstm</li>
</ul></li>
</ul></li>
<li>encoder输入还额外引入了pos embedding</li>
<li>标签构造，类似LevT，需要在句子简化数据集上构建出ground
truth编辑操作，依然是基于编辑距离的动态规划得到操作序列，当多个操作序列存在时，优先选择插入序列多的，作者尝试过其他优先方式（优先删除、随机、引入替换操作），效果均不如直接优先插入。</li>
<li>使用100维的glove向量来初始化词向量和编辑label
embedding，用30维向量初始化pos
embedding，使用编辑标签逆频次作为损失权重来平衡各个标签的损失占比。</li>
<li>结果并不是很优秀，在指标上没有比非编辑方法好，在人类评估上好一些但存在主观性。消融也不是很显著。</li>
</ul>
<h1 id="lasertagger">LaserTagger</h1>
<ul>
<li>依然是保留、删除、插入三种操作，不过用上了BERT，且google的工程能力使得加速比达到了100-200倍</li>
<li>贡献点
<ul>
<li>提出了模型，并提出了从数据中生成标签词典的方法</li>
<li>提出了基于BERT的和用BERT初始化seq2seq
encoder的两个版本，前者加速快，后者效果好</li>
</ul></li>
</ul>
<h2 id="text-editing-as-tagging">Text Editing as Tagging</h2>
<ul>
<li>LaserTagger将文本编辑问题转换为序列标注问题，主要包含三部分
<ul>
<li>标注操作：将编辑转化标注，其标签并不是保留删除插入三种，而是分为两部分：base
tag B，有保留和删除两种；added phrase
P，有V种，代表插入的片段（空白、词或者短语），从训练数据中构建一个P
vocabulary得到，P和B组合得到标签，因此标签总共有2V种。可以根据下游人物添加任务相关的标签。</li>
<li>added phrase
vocabulary构建：构建词典是一个组合优化问题，一方面我们希望这个词典尽可能小，另一方面我们希望这个词袋能够覆盖最多的编辑情况，完美的解决该问题是NP-hard的，作者采用了另外一种方式。对每个训练文本对，通过动态规划求出最长公共子序列，用目标序列减去最长公共子序列就得到了需要添加的phrase，之后对phrase按照出现频次排序，取top
k，在公开数据集上top 500就能覆盖85%的数据。</li>
<li>构建标注序列：直接见伪算法，这里不需要动态规划，而是采取了一种贪心的方式，逐字匹配，找最短phrase插入
<a target="_blank" rel="noopener" href="https://imgtu.com/i/gdxc0x"><img data-src="https://z3.ax1x.com/2021/05/12/gdxc0x.png" alt="gdxc0x.png" /></a>
这样可能会出现不能编辑的情况，这种情况就从训练集中筛去，作者认为这样也可以看成是一种降噪。</li>
<li>根据预测结果编辑句子：就是直接操作，可能根据下游任务预测出特别的标签进行任务相关的操作。作者认为将编辑操作模块化出来比直接端到端要更灵活。</li>
</ul></li>
</ul>
<h2 id="实验-1">实验</h2>
<ul>
<li>模型有两种，直接用BERT做标注或者用一个seq2seq，encoder用BERT初始化，前者的话就是在BERT最后一层是再加一层transformer
block做标注。</li>
<li>四项任务，句子融合，分句与转述，摘要，语法纠错。均超过baseline，其中句子融合取得新的SOTA。但是LaserTagger最亮眼的表现在于其效率，只用BERT
encoder能达到最高200倍的加速比（毕竟把文本生成变成了一个小词典的序列标注），同时对于小样本学习非常友好。</li>
<li>作者还分析了seq2seq和基于编辑的模型常出现的问题及其原因、两者的优势等等
<ul>
<li>想象词：seq2seq会在subword粒度上组合生成不存在的词，LaserTagger不存在这种情况</li>
<li>过早结束：seq2seq可能很早生成EOS导致句子不完整或者过短，而LaserTagger理论上存在这种可能，实际上作者没有见到过，因为这意味着模型要预测出一堆删除操作，而这在训练数据中是几乎没有的。</li>
<li>重复短语：在分句任务中，seq2seq经常会重复生成短语，而LaserTagger会选择不分句，或者lazy
split</li>
<li>Hallucination：即生成的文本与源文本无关或者反常识反事实，两类模型都会出现这类问题，seq2seq可能隐藏的更深（看起来流畅，实际上不合理）</li>
<li>共指消解：两类模型都存在这种问题，seq2seq容易将代词替换为错误名词，LaserTagger容易保留代词不管</li>
<li>错误删除：LaserTagger看似文本生成较为可控，但任然存在一些情况，即模型删除了部分词，剩下句子语义流畅，但含义错误</li>
<li>lazy
split：在分句任务中，LasterTagger可能只分句，而不对分开的句子做任何后处理。</li>
</ul></li>
</ul>
<h1 id="pie">PIE</h1>
<ul>
<li>也是BERT+序列标注，同时从数据中构造phrase词典</li>
<li>编辑操作包括复制（保留）、插入、删除、词形变换（用于语法纠错）</li>
<li>构造phrase词典、构造ground truth编辑操作和LaserTagger基本一致</li>
<li>将BERT左右扩展了两部分来获得替换或者增加所需的信息：
<ul>
<li>输入层，M为mask标识符的embedding，p为positional embedding，X为word
embedding</li>
<li>h即原始BERT信息，包含词和位置信息</li>
<li>r即替换信息，只不过把当前位置的词mask掉，用M替代，且计算注意力时不查询当前位置的h</li>
<li>a即插入信息，只不过把当前位置的词mask掉，用M替代，且p替换为相邻位置p的平均
<a target="_blank" rel="noopener" href="https://imgtu.com/i/gwZarT"><img data-src="https://z3.ax1x.com/2021/05/12/gwZarT.png"
alt="gwZarT.png" /></a></li>
<li>之后利用三类信息来分别计算不同操作的概率，并归一化，CARDT分别代表复制（保留）、插入、替换、删除、词形变换
<a target="_blank" rel="noopener" href="https://imgtu.com/i/gwZ5IH"><img data-src="https://z3.ax1x.com/2021/05/12/gwZ5IH.png"
alt="gwZ5IH.png" /></a></li>
<li>上式第一项是每一项编辑操作的得分；第二项是保留当前词的得分，删除和替换不得分，其余的则用当前词embedding
<span class="math inline">\(\phi(x_i)\)</span>来参数化得分，这里<span
class="math inline">\(\phi\)</span>的意义不明，原文是Embedding of w,
represented by <span class="math inline">\(\phi(w)\)</span> is obtained
by summing up in-dividual output embeddings of tokens in
w；第三项是新词带来的影响，只有替换和插入有。</li>
</ul></li>
<li>推断时也是多轮推断（输出变输入），直到出现重复句子</li>
<li>这里编辑操作设计的动机和解释不是很清楚，最终结果也很一般，ensemble情况下才与Transformer
+ Pre-training + LM+ Spellcheck + Ensemble Decoding
的工作持平，而且从消融来看，预训练模型是带来性能提升的主要原因。</li>
</ul>
<h1 id="felix">Felix</h1>
<ul>
<li>将编辑操作分为两个非自回归的部分，首先用一个基于Transformer的指针网络做tagging和reordering，并插入placeholder；然后用一个MLM对placeholder做预测</li>
<li>Felix的设计考虑三点需求
<ul>
<li>灵活的编辑，适合多种文本生成任务</li>
<li>充分利用BERT等预训练模型</li>
<li>高效推断 <a target="_blank" rel="noopener" href="https://imgtu.com/i/gwKovR"><img data-src="https://z3.ax1x.com/2021/05/12/gwKovR.png"
alt="gwKovR.png" /></a></li>
</ul></li>
<li>下图可以清晰的表示出Felix的标签设计，其中<em>y<sup>t<em>是tagging模型需要预测出的编辑操作序列，</em>y</sup>m</em>是根据编辑操作序列，添加相关special
token(REPL,MASK)之后的中间状态序列，中间状态序列直接喂给MLM来预测需要插入的词，最终得到Pred里的结果</li>
<li><a target="_blank" rel="noopener" href="https://imgtu.com/i/gwMHzj"><img data-src="https://z3.ax1x.com/2021/05/12/gwMHzj.md.png"
alt="gwMHzj.md.png" /></a></li>
<li>这里需要注意两点：
<ul>
<li>MLM虽然只完成插入词的预测，但也需要完整的编辑操作信息，而不是输入MLM时直接把预测删除操作位置的词删掉，这里作者的做法是用REPL括起来要删除的span，给MLM也提供了删除编辑的信息。</li>
<li>图中给出了Mask和Infill两种形式，其实就是对中间状态序列的MASK设计不同，涉及到多token插入的任务交给谁：Mask方式里是把任务交给了tagging模型，直接预测出插入多个词的编辑操作，例如<span
class="math inline">\(DEL^{INS\_2}\)</span>，即插入两个，对应的就在中间序列中生成两个MASK让模型预测；Infill方式里把这个任务交给了MLM，统一对每个插入位置准备四个MASK让模型预测，多了的部分就预测PAD</li>
</ul></li>
<li>为了更灵活的建模，tagging部分的模型还需要做reordering，不然交互两个词的位置就得先删再添加，增加了复杂度。这里是通过指针网络注意力来为每个词确定其指向的后一个词，第一个词由特殊标识CLS指示。在推断时，使用了受控的beam
search避免指向顺序产生循环。</li>
<li>主要与LaserTagger对比，在性能，小样本上的表现要优于LaserTagger,且没有了phrase
vocabulary的限制。作者也就phrase
vocabulary和reordering做了实验，结果很多可见原论文。</li>
</ul>
<h1 id="总结">总结</h1>
<ul>
<li><p>总结各个模型</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 21%" />
<col style="width: 15%" />
<col style="width: 9%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">模型</th>
<th style="text-align: center;">编辑操作</th>
<th style="text-align: center;">加速</th>
<th style="text-align: center;">插入多个词</th>
<th style="text-align: center;">构建ground truth编辑序列</th>
<th style="text-align: center;">编辑失败</th>
<th style="text-align: center;">测试任务</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LevT</td>
<td style="text-align: center;">插入删除</td>
<td style="text-align: center;">多轮非自回归，5倍</td>
<td style="text-align: center;">先预测placeholder数量，再替换</td>
<td style="text-align: center;">基于编辑距离动态规划</td>
<td style="text-align: center;">退化为文本生成</td>
<td style="text-align: center;">翻译、摘要</td>
</tr>
<tr class="even">
<td style="text-align: center;">EditNTS</td>
<td style="text-align: center;">插入删除保留</td>
<td style="text-align: center;">自回归，未提及加速</td>
<td style="text-align: center;">插入时原地停留，直到插入完毕</td>
<td style="text-align: center;">基于编辑距离动态规划</td>
<td style="text-align: center;">退化为文本生成</td>
<td style="text-align: center;">文本简化</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LaserTagger</td>
<td style="text-align: center;">插入删除保留</td>
<td style="text-align: center;">一轮序列标注，100+倍</td>
<td
style="text-align: center;">直接插入phrase，根据训练数据构建phrase词典</td>
<td style="text-align: center;">贪心匹配</td>
<td style="text-align: center;">不编辑</td>
<td
style="text-align: center;">句子融合，分句与转述，摘要，语法纠错</td>
</tr>
<tr class="even">
<td style="text-align: center;">PIE</td>
<td style="text-align: center;">插入删除替换复制词形变换</td>
<td style="text-align: center;">序列标注，2倍</td>
<td
style="text-align: center;">直接插入phrase，根据训练数据构建phrase词典</td>
<td style="text-align: center;">贪心匹配</td>
<td style="text-align: center;">未说明</td>
<td style="text-align: center;">语法纠错</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Felix</td>
<td style="text-align: center;">删除插入保留重排序</td>
<td style="text-align: center;">标注+MLM，快于LaserTagger</td>
<td style="text-align: center;">设置多个Mask让MLM预测</td>
<td style="text-align: center;">简单比较</td>
<td style="text-align: center;">较差的MLM预测结果</td>
<td
style="text-align: center;">句子融合、机器翻译后处理、摘要、文本简化</td>
</tr>
</tbody>
</table></li>
</ul>
</div>
<script src="https://giscus.app/client.js"
        data-repo="thinkwee/thinkwee.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA=="
        data-category="Announcements"
        data-category-id="DIC_kwDOBL7ZNM4CkozI"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/theory/" rel="tag"># theory</a>
              <a href="/tags/nlp/" rel="tag"># nlp</a>
              <a href="/tags/seq2seq/" rel="tag"># seq2seq</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/05/15/vc-dimension/" rel="prev" title="Note for VC Dimension">
                  <i class="fa fa-angle-left"></i> Note for VC Dimension
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/05/13/pet/" rel="next" title="Prompt - Task Reformulation in NLP">
                  Prompt - Task Reformulation in NLP <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:35</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":null,"security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2021/05/11/text-edit-generation/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
