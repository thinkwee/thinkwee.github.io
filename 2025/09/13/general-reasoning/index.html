<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"thinkwee.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"duration":50,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="How far have we gone towards general reasoning? How far are we from the general reasoning?">
<meta property="og:type" content="article">
<meta property="og:title" content="Towards General Reasoning">
<meta property="og:url" content="https://thinkwee.top/2025/09/13/general-reasoning/index.html">
<meta property="og:site_name" content="Thinkwee&#39;s Blog">
<meta property="og:description" content="How far have we gone towards general reasoning? How far are we from the general reasoning?">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/6afdb242505675e387a7e2498a292346.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png">
<meta property="og:image" content="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png">
<meta property="article:published_time" content="2025-09-13T02:56:16.000Z">
<meta property="article:modified_time" content="2025-10-06T20:17:32.400Z">
<meta property="article:author" content="Thinkwee">
<meta property="article:tag" content="math">
<meta property="article:tag" content="rl">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="agent">
<meta property="article:tag" content="inference">
<meta property="article:tag" content="reasoning">
<meta property="article:tag" content="questions">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.mji.rip/2025/09/13/6afdb242505675e387a7e2498a292346.png">


<link rel="canonical" href="https://thinkwee.top/2025/09/13/general-reasoning/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://thinkwee.top/2025/09/13/general-reasoning/","path":"2025/09/13/general-reasoning/","title":"Towards General Reasoning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Towards General Reasoning | Thinkwee's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-96114782-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-96114782-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Thinkwee's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Too Stupid to Give Up Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">53</span></a></li><li class="menu-item menu-item-multi-agent-ebook"><a href="/multiagent_ebook/" rel="section"><i class="fa fa-book fa-fw"></i>Multi-Agent EBook</a></li><li class="menu-item menu-item-iagents"><a href="/iagents/" rel="section"><i class="fa fa-robot fa-fw"></i>iAgents</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#what"><span class="nav-number">1.</span> <span class="nav-text">What</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#why"><span class="nav-number">2.</span> <span class="nav-text">Why</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#what-it-actually-is"><span class="nav-number">3.</span> <span class="nav-text">What it actually is</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how"><span class="nav-number">4.</span> <span class="nav-text">How</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#observations"><span class="nav-number">5.</span> <span class="nav-text">Observations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#is-changing-only-the-reward-enough"><span class="nav-number">6.</span> <span class="nav-text">Is changing only the reward
enough?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#one-more-thing-climb-the-solververifier-asymmetry"><span class="nav-number">7.</span> <span class="nav-text">One more
thing: Climb the Solver–Verifier Asymmetry</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#citation"><span class="nav-number">8.</span> <span class="nav-text">Citation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">9.</span> <span class="nav-text">是什么</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="nav-number">10.</span> <span class="nav-text">为什么</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">11.</span> <span class="nav-text">本质是什么</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%81%9A"><span class="nav-number">12.</span> <span class="nav-text">怎么做</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%82%E5%AF%9F"><span class="nav-number">13.</span> <span class="nav-text">观察</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E5%A5%96%E5%8A%B1%E5%B0%B1%E8%B6%B3%E5%A4%9F%E4%BA%86%E5%90%97"><span class="nav-number">14.</span> <span class="nav-text">修改奖励就足够了吗</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#one-more-thing-%E6%94%80%E7%88%ACsolververifier%E4%B8%8D%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="nav-number">15.</span> <span class="nav-text">One more thing:
攀爬Solver–Verifier不对称性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">16.</span> <span class="nav-text">引用</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Thinkwee</p>
  <div class="site-description" itemprop="description">Failed Better</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/thinkwee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thinkwee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:thinkwee2767@gmail.com" title="E-Mail → mailto:thinkwee2767@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/thinkwee2767" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;thinkwee2767" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?hl=en&user=QvW2leIAAAAJ" title="GScholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl&#x3D;en&amp;user&#x3D;QvW2leIAAAAJ" rel="noopener me" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>GScholar</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thinkwee.top/2025/09/13/general-reasoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Thinkwee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkwee's Blog">
      <meta itemprop="description" content="Failed Better">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Towards General Reasoning | Thinkwee's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Towards General Reasoning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-09-13 10:56:16" itemprop="dateCreated datePublished" datetime="2025-09-13T10:56:16+08:00">2025-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-10-07 04:17:32" itemprop="dateModified" datetime="2025-10-07T04:17:32+08:00">2025-10-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span id="/2025/09/13/general-reasoning/" class="post-meta-item leancloud_visitors" data-flag-title="Towards General Reasoning" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>26k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>23 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://i.mji.rip/2025/09/13/6afdb242505675e387a7e2498a292346.png" width="400"></p>
<p>How far have we gone towards general reasoning? How far are we from
the general reasoning? <span id="more"></span></p>
<style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>
<div id="en-content" class="lang-content" style="display: block;"><p><img data-src="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png" width="400"></p>
<p>Recently our paper <em>NOVER: Incentive Training for Language Models
via Verifier-Free Reinforcement Learning</em> was accepted to EMNLP.
NOVER uses the LLM's perplexity of the ground truth conditioned on the
reasoning trajectory as the reward, which extends the RLVR paradigm
beyond math and code, enabling learning of general reasoning on
arbitrary text-to-text tasks without extra models or verifiers.</p>
<p><img data-src="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png" width="600"></p>
<p>When we began NOVER in February, most RLVR work focused on
mathematical and code reasoning; RLVR research that targets general or
hard-to-verify domains was scarce. Nearly six months later many
interesting related papers have emerged. Due to limited resources, many
ideas in our experiments were not fully validated and were left out of
the paper. This post organizes those ideas and surveys recent relevant
work to assess how far we have come on general reasoning and how far we
still must go.</p>
<h1 id="what">What</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png" width="1000">
<figcaption>
NOVER extends RLVR to general reasoning like Science Explanation and
Proof, Social Reasoning, Creative Writing and Translation
</figcaption>
</figure>
<p>NOVER’s target problem is, for tasks whose answers are unstructured
natural text and therefore unsuitable for rule-based verifiers, how can
we apply RLVR to acquire reasoning ability?</p>
<p>Such cases are common. A math problem often has a unique canonical
answer that can be placed in a <code>\boxed{}</code>, while explanations
of physical or chemical phenomena can legitimately take many different
forms (think of long, opinionated answers on Quora-like sites). For
entity-level QA we can use exact match, but for long-form generation
such as document-level summarization/translation/creative writing, there
is no reliable rule (ROUGE, BLEU and similar metrics have long been
shown to be unreliable). The same applies across vertical domains
(medicine, law, social sciences, psychology, literature, education)
where many ground-truth instances are free-form.</p>
<p>A few concepts are easy to conflate; NOVER focuses only on the
following:</p>
<ul>
<li>the ground truth exists (this is not unsupervised learning); we do
not desire a reward model to directly give a judgment, but rather a
verifier that compares ground truth and prediction;</li>
<li>the ground truth may be subjective or objective, but the dataset
contains at least one reference.</li>
<li>Even when the ground truth is objective, rule-based verifiers are
often failed: objective GTs can be expressed in many textual forms,
otherwise we would simply use exact match as the reward. Moreover, even
for math, where answers are seemingly easy to verify (multiple choice
labels, numbers, formulas, short text, booleans), model responses vary
widely and rule-based verifiers are not robust
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png" width="600">
<figcaption>
Error patterns on easy-to-verify tasks from compass verifier
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>
</figcaption>
</figure>
<h1 id="why">Why</h1>
<p>Why pursue general reasoning? Many non-math/code tasks (creative,
humanities, or long-form scenarios) appear to be not a suitable targets
for a “reasoning” model. But consider:</p>
<ul>
<li>It is still unknown whether RLVR is the ultimate correct paradigm
for training reasoning models.</li>
<li>It is still unknown whether CoT genuinely corresponds to human-style
reasoning.</li>
<li>It is still unknown whether CoTs learned via RLVR truly represent a
model’s internal reasoning.</li>
</ul>
<p>Currently RLVR is better seen as a method to train native CoT
outputs, and CoT is simply “say something before the final answer.” That
something is not necessarily reasoning (some works find that even
repeating the question can increase accuracy); it is model-generated
tokens that, when produced before the answer, help the LLM better
exploit learned patterns and increase the prediction probability of the
next correct answer tokens.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png" width="400">
<figcaption>
DataAlchemy's show that CoT's gains can arise from reuse and
interpolation of patterns near the training distribution
<sup class="refplus-num"><a href="#ref-is_cot_mirage">[2]</a></sup>
</figcaption>
</figure>
<p>From that practical viewpoint, producing a bit more text that
improves answer quality is no harm (users often try a “deeper thinking”
mode in chat systems expecting better answers).</p>
<p>Another reason to study general reasoning is that, <strong>for an
LLM, task difficulty is tied to verification difficulty</strong>. We aim
to keep pushing the frontier of problems that models can solve: some
tasks that are hard for humans (e.g., olympiad math) might still be
learnable by models if supplied with a correct, sufficiently informative
reward. Conversely, tasks whose rewards are hard to formalize are harder
for models to learn.</p>
<h1 id="what-it-actually-is">What it actually is</h1>
<p>What we need for RLVR on free-form text is a good verified signal,
which is actually a reward function that measures semantic agreement
between ground truth and model prediction. That is exactly what we
pursue in the Natural Language Generation. The most basic target is
cross-entropy (ppl). <strong>From this perspective NOVER essentially
moves the SFT loss into the RLVR setting, and recent work shows SFT and
RL differences are often not large.</strong></p>
<p>Although NOVER used ppl, perplexity may not be optimal. We can
arrange verified signals along an axis from fine to coarse granularity:
the coarser the signal, the more information is lost and the sparser the
reward becomes. On this axis three main approaches appear:</p>
<ul>
<li>Perplexity-based signals.</li>
<li>Rubrics / checklists.</li>
<li>Trained verifier models that yield binary (0/1) rewards.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png" width="800">
<figcaption>
The Axis of Verified Signals
</figcaption>
</figure>
<p>Compared with binary rewards, ppl provides a denser signal, extends
naturally to free-form text, and avoids reward saturation; but it loses
the absolute correctness signal, i.e., the model never observes a strict
correct/incorrect label and we cannot use pass@k-style metrics to assess
sample difficulty. Rubrics/checklists sit between these extremes: they
are more fine-grained than binary rewards but still sparser than ppl.
High-quality rubrics typically require sample-wise, human expert
annotation. Several recent works explore rubric-style
solutions<sup class="refplus-num"><a href="#ref-ace_rl">[3]</a></sup><sup class="refplus-num"><a href="#ref-checklists_are_better">[4]</a></sup><sup class="refplus-num"><a href="#ref-ticking_all_the_boxes">[5]</a></sup><sup class="refplus-num"><a href="#ref-rubric_anchors">[6]</a></sup><sup class="refplus-num"><a href="#ref-rubrics_as_rewards">[7]</a></sup>.
Baichuan-M2 in particular develops a fairly detailed Verifier System
that functions as a model-driven environment, with a Patient Simulator
(data generator) and a Rubrics Generator (rollout evaluator)
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png" width="600">
<figcaption>
Baichuan-M2's Verifier
System<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>
</figcaption>
</figure>
<p>Rubrics also enable controlled synthetic data generation for
debiasing reward
models<sup class="refplus-num"><a href="#ref-robust_reward_modeling">[9]</a></sup>,
so the reward model focuses on true causal factors and resists hacks
stemming from format, length, or tone. OpenAI’s Deliberative Alignment
can be seen as an outcome-RL approach that uses safety-oriented rubrics
<sup class="refplus-num"><a href="#ref-deliberative_alignment">[10]</a></sup>.</p>
<h1 id="how">How</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png" width="400">
<figcaption>
NOVER's reward is derived from the policy model's conditional ppl of the
ground truth given the reasoning trajectory
</figcaption>
</figure>
<p>NOVER applies a crude but direct approach: for a rollout, compute the
policy model’s conditional ppl of the ground-truth answer given the
rollout's reasoning trajectory as the reward.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png" width="600">
<figcaption>
The idea of reasoning advantage (RA).
</figcaption>
</figure>
<p>The idea of reasoning-ppl based improvements has appeared before. A
short NeurIPS 2024 LanGame workshop paper called this notion reasoning
advantage (RA), essentially the relative change in reasoning ppl
compared to a no-reasoning baseline. That paper used RA for data
selection, which is essentially keeping CoT examples with high RA for
SFT, so it can be viewed as an offline-RL style method
<sup class="refplus-num"><a href="#ref-on_reward_functions">[11]</a></sup>.</p>
<p>Fortuitously, I experimented with relative reasoning ppl in NOVER and
later found the LANGame writeup: it is an intuitive and reasonable
design.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png" width="600">
<figcaption>
The idea of longPPL.
</figcaption>
</figure>
<p>Another related refinement on ppl is longPPL which measures ppl on a
context-dependent subset of tokens: longPPL subtracts the ppl without
long context from the ppl with long context, thereby focusing evaluation
on tokens that truly depend on long-range context
<sup class="refplus-num"><a href="#ref-what_is_wrong_with_perplexity">[12]</a></sup>.
RA shares the same spirit: we want the reward to come from those tokens
in the ground truth that genuinely require CoT reasoning.</p>
<p>More Interestingly, in GRPO a simple group normalization makes
relative ppl improvements and absolute ppl effectively equivalent on
advantage calculation, so absolute reasoning ppl itself is a solid
reward signal.</p>
<p>But applying ppl directly has issues.</p>
<ul>
<li>First, ppl is numerically unstable: advantage estimates vary across
batches and exhibits length bias. NOVER converted ppl into in-group
quantiles to produce more stable rewards. QRPO applies quantile
transforms more rigorously: it maps rewards to quantiles of the
base-policy reward distribution across the dataset, making the partition
function tractable and enabling numerically stable pointwise rewards
even in offline RL
<sup class="refplus-num"><a href="#ref-quantile_reward_policy_optimization">[13]</a></sup>.</li>
<li>Which model should be used to compute ppl? In principle a stronger
external model could be a more accurate verifier, but the gap between
large and small model cause problems, which is similar to bad
distillation results when using DPO to train small models from
GPT-distilled labels. NOVER uses the policy model itself to compute ppl,
which saves extra models and eases scaling. We found that using a
separate large verifier (closed-source SOTA or a specialized verifier)
often leads to LM-hack-LM issues, whereas using the policy model’s own
ppl yields smoother learning curves.</li>
<li>With small batches and limited compute, training is unstable. NOVER
introduced a policy-proxy sync: periodically copy policy parameters to a
proxy model and compute ppl from the proxy during training. This
effectively increases the batch size (similar in spirit to gradient
accumulation) and stabilizes reward estimates.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png" width="400">
<figcaption>
RLPR shows that ppl can accurately measure the reasoning advantage.
</figcaption>
</figure>
<p>Several contemporaneous works adopt related ideas but differ in how
they stabilize ppl numerics.</p>
<ul>
<li>VeriFree
<sup class="refplus-num"><a href="#ref-reinforcing_general_reasoning">[14]</a></sup>
uses reasoning ppl directly, but restricts to short answers (≤7 tokens)
where ppl is less unstable, and shows ppl can approach or exceed
verifier-based baselines on short QA.</li>
<li>RLPR <sup class="refplus-num"><a href="#ref-rlpr">[15]</a></sup>
uses relative token probabilities (the per-token mean probability,
clipped, then advantage computed) rather than ppl and provides detailed
ablations showing direct ppl can lose 20 points if used naively.</li>
<li>DRO
<sup class="refplus-num"><a href="#ref-direct_reasoning_optimization">[16]</a></sup>
targets long answers and uses relative reasoning ppl with per-token
weighting for high-variance ground-truth tokens and local weight
decay.</li>
<li>DeepWriter
<sup class="refplus-num"><a href="#ref-reverse_engineered_reasoning">[17]</a></sup>
focuses on long-form writing but uses reasoning ppl purely as a scoring
metric to filter and iteratively rewrite drafts (not an RL loop),
avoiding numeric instability by staying in a supervised selection
regime.</li>
</ul>
<h1 id="observations">Observations</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png" width="600">
<figcaption>
Collapse modes in training.
</figcaption>
</figure>
<p>We experienced many collapse modes early in training: completion
lengths exploding, ill rollouts where the model produces garbled text,
and simultaneous blowups of format rewards. We applied the tricks above
to stabilize training (see the paper’s ablation for details on the
“curse of proxy”).</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png" width="600">
<figcaption>
The curse of proxy.
</figcaption>
</figure>
<p>A small but useful trick is reward dependency: when multiple reward
terms are simply summed the model can be uncertain which objective
produced a given penalty or bonus. Practically, we found it effective to
gate task rewards on a strict format reward: unless the format reward is
satisfied, set all other rewards to zero. When the format reward gained,
the model is usually “sane”, no hallucination or gibberish. This
dependency can also pull the model back from training collapse.</p>
<p>We also found that excessive strictness on format rewards may hinder
exploration
<sup class="refplus-num"><a href="#ref-simplerl_zoo">[18]</a></sup>.
For example, one interesting reward hacking on format we observed was
nested <code>&lt;think&gt;</code> tags in CoT: models can nest a
sub-reasoning reflection inside an outer <code>&lt;think&gt;</code>
block to game the signal, e.g.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;think&gt;</span><br><span class="line">  inner thoughts</span><br><span class="line">  &lt;think&gt;</span><br><span class="line">    reflection on the earlier thoughts</span><br><span class="line">  &lt;/think&gt;</span><br><span class="line">  continue reasoning</span><br><span class="line">&lt;/think&gt;</span><br><span class="line">&lt;answer&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/answer&gt;</span><br></pre></td></tr></table></figure>
<p>Stronger base models exploit dense semantic signals better. For
example, we converted multiple-choice questions into free-form answers
where the model must output both the option letter and the full option
text; comparing 7B vs 3B, the 7B model better leverages ppl to rank
rollouts:</p>
<ul>
<li>rank 1: option letter and option text both correct</li>
<li>rank 2: letter wrong, option text correct</li>
<li>rank 3: letter correct, option text similar to another option</li>
<li>rank 4: letter correct, option text completely wrong</li>
<li>…</li>
<li>lowest: everything wrong</li>
</ul>
<p>Looking at rollouts beyond the answer, ppl can indirectly reflect
differences in the reasoning details. In an astronomy example that
required an explanation plus numeric computation, we asked GPT to
analyze each rollout (reasoning plus result) sorted by reasoning ppl;
the model’s qualitative analyses correlated with ppl rankings.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png" width="600">
<figcaption>
The correlation between ppl rankings and GPT's qualitative analyses.
</figcaption>
</figure>
<p>NOVER also partially works on non-Qwen models, though weaker bases
(e.g., some Mistral checkpoints) show erratic behavior. Zero-shot CoT
can be seen as an untrained base exploration strategy; if that baseline
is close to or exceeds the base model, RL typically provides gains.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png" width="600">
<figcaption>
NOVER partially works on non-Qwen models.
</figcaption>
</figure>
<p>We also observed (without exhaustive experiments) that many
general-reasoning datasets are annotated by closed-source large models
and thus are not perfectly objective or correct (loose definitions,
symbol misuse). Perplexity can still provide a useful guiding signal: in
some cases models learned complex reasoning patterns from the ppl signal
that can produce arguably more correct answers than the original ground
truth.</p>
<h1 id="is-changing-only-the-reward-enough">Is changing only the reward
enough?</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png" width="1000">
<figcaption>
Some works on reproducing GRPO.
</figcaption>
</figure>
<p>No, but reward design is the most obvious gap when extending
rule-based verification to general reasoning. What's more, from the
bitter-lesson viewpoint many algorithmic tricks are spurious: data and
compute dominate. By March many people were reproducing GRPO and noting
its fragility; our NOVER training surfaced similar issues. Many
algorithmic “tricks” proposed in these papers have marginal effects
compared with data and scale.</p>
<p>So advancing general reasoning faces larger challenges in data and
base models; algorithmic work will be required later to make training
more efficient and stable.</p>
<ul>
<li>Data: existing general-reasoning datasets vary widely in quality;
cleaning consumes substantial effort, and much data is LLM-annotated
(distilled from GPT or similar) rather than human-curated. The data are
static and finite. RL itself is sample-efficient in some senses; the
cost-effective path to scaling is not simply more examples but
higher-quality environments and feedback.</li>
<li>Base model: the base model governs exploration in RL. Practically,
it should already possess zero-shot instruction following and CoT
capability; richer knowledge helps. Debates over whether RL can raise
the ultimate capability ceiling are not the key point: post-training
often elicits latent abilities rather than creates them. Some works
already explore combining memory and elicitation, and I believe
mid-training vs post-training may form new positive feedback loops.</li>
</ul>
<h1 id="one-more-thing-climb-the-solververifier-asymmetry">One more
thing: Climb the Solver–Verifier Asymmetry</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png" width="600">
<figcaption>
The Solver-Verifier Asymmetry.
</figcaption>
</figure>
<p>A central concept in RLVR is the solver-verifier asymmetry: for some
tasks verification is easier than solving, while for others verification
is harder. Much of RLVR excels when verification is simpler than
solving. The opposite side, where verification is harder, includes:</p>
<ul>
<li>General Reasoning with hard-to-verify free-form answers</li>
<li>Situations requiring long time horizons to obtain a return (e.g., a
business plan whose real feedback arrives after weeks, months, or
years). Those cases resemble deep conversion problems in recommender
systems: we need accurate attribution and systems that handle extremely
sparse feedback.</li>
<li>Scenarios that may require large human labeling efforts or
hard-to-acquire real users to verify the solution, which motivates the
development of effective user simulators.</li>
</ul>
<p>The verifier-free design of NOVER introduces a new possibility
(though not yet tested):</p>
<p><strong>whether it is feasible to synchronize the intelligence of the
policy model to the verifier model, thereby enabling co-evolution of
solver and verifier along the Solver-Verifier Asymmetry
diagonal</strong>.</p>
<p>A stronger policy model would lead to a stronger verifier model,
which in turn could train an even stronger policy model. The key lies in
the transmission of intelligence. NOVER’s design of using perplexity as
the reward naturally <strong>unifies the form of intelligence in both
solver and verifier: both aim to increase the probability of generating
the ground truth on good reasoning trajectories.</strong> In this way,
co-evolution can be achieved through standard RL without the need to
design additional adversarial or self-play tasks. Here, the direction of
intelligence transfer is from solving to verifying. A symmetric related
work is LLaVA-Critic-R1, which found that a strong preference model can
yield a strong policy model, though it required constructing an
additional task.
<sup class="refplus-num"><a href="#ref-llava_critic_r1">[19]</a></sup>.</p>
<p>If we want to achieve such fully automatic co-climbing, we have RL
training which performs horizontal climbing (fix verifier y, improve
solver x), we have Intelligence Sync which would perform vertical
climbing (fix solver x, improve verifier y). However, we also need a
third variable: tasks and data. Each point in the solver–verifier grid
corresponds to specific tasks and datasets. As argued in my earlier post
on <a href="https://thinkwee.top/2025/07/17/env-matrix/#more">Scaling
the Environment</a>, beyond solver and verifier there is also the
question generator. Most current reasoning-evolution work focuses on
self-improvement via model consistency or entropy patterns; some
approaches implement co-evolution of two modules, while a tri-evolution
of three modules has not been explored:</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png" width="600">
<figcaption>
The Trinity of Solver-Verifier-Generator.
</figcaption>
</figure>
<ul>
<li>R-Zero and Self-Questioning Language Models consider adversarial
generation between a generator and a solver
<sup class="refplus-num"><a href="#ref-r_zero">[20]</a></sup><sup class="refplus-num"><a href="#ref-self_questioning">[21]</a></sup>.</li>
<li>URPO reframes verification as a solving task and unifies data
training. COOPER trains a verifier from positive/negative samples
constructed from current policy rollouts. Both lines implement
solver–verifier co-evolution
<sup class="refplus-num"><a href="#ref-urpo">[22]</a></sup><sup class="refplus-num"><a href="#ref-cooper">[23]</a></sup>.</li>
</ul>
<p>Another route to continual solver improvement is self-play: with a
suitable environment, two solvers can game and thereby improve each
other without worrying about asymmetry. For general reasoning such
environments are hard to design because the “rules” are nebulous. Recent
works have proved that models can learn rules
<sup class="refplus-num"><a href="#ref-llms_can_learn_rules">[24]</a></sup>
and <a target="_blank" rel="noopener" href="https://husky-morocco-f72.notion.site/From-f-x-and-g-x-to-f-g-x-LLMs-Learn-New-Skills-in-RL-by-Composing-Old-Ones-2499aba4486f802c8108e76a12af3020">combine
atom skills to learn new skills</a> through synthetic data and task, but
existing real general-reasoning datasets are limited enumerations rather
than comprehensive rule sets. This is still essentially static
datset/benchmark-driven RL. In the AI “second half,” we should seek
real-world environments and problems rather than static datasets.</p>
<p>Between static data and the real world lies a middleware: simulators.
Simulators trade fidelity for feedback speed—like reward models or
verifier models—and for general reasoning a useful simulator might look
like a patient simulator in medical domains (see Baichuan-M2’s case),
since real patients raise ethical and regulatory issues and validation
can be slow
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<p>A different idea is to forgo task-specific environments and instead
play games: self-play on games could improve math and general reasoning
if reasoning patterns transfer across games and tasks
<sup class="refplus-num"><a href="#ref-play_to_generalize">[25]</a></sup><sup class="refplus-num"><a href="#ref-spiral">[26]</a></sup>.
If feasible, we could use game environments and self-play to continually
evolve general-reasoning models.</p>
<h1 id="citation">Citation</h1>
<p>If you found the topics in this blog post interesting and would like
to cite it, you may use the following BibTeX entry: </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@article{general_reasoning_202509,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Towards General Reasoning},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {9},</span><br><span class="line">  url = {https://thinkwee.top/2025/09/13/general-reasoning/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<div id="zh-content" class="lang-content" style="display: none;"><p><img data-src="https://i.mji.rip/2025/09/13/fc1d7e4647d71b982f9ad9a0cbbbeaa0.png" width="600"></p>
<p>以下为机器翻译，一个中文原生的版本请参考<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ocpI3j3rwlt_9Zo1wYAF6Q">公众号文章</a></p>
<p>最近，我们的论文 <em>NOVER: Incentive Training for Language Models
via Verifier-Free Reinforcement Learning</em> 被 EMNLP 录用。NOVER 将
LLM 在推理轨迹下对真实数据的困惑度作为奖励，这一创新将 RLVR
范式从数学和代码领域拓展至更广泛的文本处理，使得模型能够在任意文本到文本任务中学习通用推理，且无需依赖额外的模型或验证器。</p>
<p><img data-src="https://i.mji.rip/2025/09/13/d82968f183f79eeca1606f08b6fcf9f6.png" width="600"></p>
<p>当我们在二月启动 NOVER 项目时，大多数 RLVR
研究主要集中在数学和代码推理领域；而针对通用或难以验证领域的 RLVR
研究则相对匮乏。近六个月来，涌现了许多有趣的相关论文。由于资源有限，我们实验中的许多想法未能得到充分验证，因此未能纳入论文。本文旨在整理这些未充分验证的想法，并综述近期相关研究，以评估我们在通用推理方面取得的进展以及未来仍需努力的方向。</p>
<h1 id="是什么">是什么</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b019a44cb56556aa079fdfd7c182c4d2.png" width="1000">
<figcaption>
NOVER 将 RLVR
拓展至通用推理领域，包括科学解释与证明、社会推理、创意写作及翻译
</figcaption>
</figure>
<p>NOVER 的目标问题在于，对于那些答案为非结构化自然文本的任务，如何应用
RLVR 来获取推理能力？</p>
<p>此类情况并不少见。数学问题通常有一个唯一的规范答案，可以放在
<code>\boxed{}</code>
中；而物理或化学现象的解释可以合法地采用多种形式（想想 Quora
等类似网站上的长篇、主观性答案）。对于实体级
QA，我们可以使用精确匹配，但对于长文本生成（如文档级摘要、翻译、创意写作），没有可靠的规则（ROUGE、BLEU
等指标已被证明不可靠）。这种现象在垂直领域（医学、法律、社会科学、心理学、文学、教育）同样存在，许多真实示例的答案为自由文本。</p>
<p>有几个概念容易混淆；NOVER 仅关注以下几点：</p>
<ul>
<li>真实答案存在（这不是无监督学习）；我们不希望奖励模型直接给出判断，而是需要一个验证器来比较真实答案和预测；</li>
<li>真实答案可能是主观的或客观的，但数据集至少包含一个参考。</li>
<li>即使真实答案是客观的，基于规则的验证器也经常失败：客观的真实答案可以以多种文本形式表达，否则我们只需使用精确匹配作为奖励。此外，即使是数学问题，其答案看似容易验证（多选标签、数字、公式、短文本、布尔值），但模型响应差异很大，基于规则的验证器也不可靠
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>.</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/7624446cf9788e075ec0a68461713615.png" width="600">
<figcaption>
Compass Verifier在易验证任务中发现的错误模式
<sup class="refplus-num"><a href="#ref-compass_verifier">[1]</a></sup>
</figcaption>
</figure>
<h1 id="为什么">为什么</h1>
<p>为什么追求通用推理？许多非数学/代码任务（创意、人文、长文本场景）似乎不适合“推理”模型。但考虑：</p>
<ul>
<li>目前仍不清楚 RLVR 是否是训练推理模型的终极正确范式。</li>
<li>目前仍不清楚 CoT 是否真正对应于人类推理风格。</li>
<li>目前仍不清楚通过 RLVR 学习的 CoT 是否真正代表模型的内部推理。</li>
</ul>
<p>所以当下我们可以将 RLVR视为训练原生 CoT 输出的方法，而CoT
只是“在最终答案之前说一些话”。这个“一些话”不一定是推理（一些工作发现重复问题也能提高准确性）；它是模型生成的在最终答案之前的tokens，帮助
LLM 更好地利用学习到的模式并增加下一个正确答案令牌的预测概率。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e066a152bb031bf47c99411fb95f17f6.png" width="400">
<figcaption>
DataAlchemy 证明了 CoT 的收益源于训练分布附近模式的重复和插值
<sup class="refplus-num"><a href="#ref-is_cot_mirage">[2]</a></sup>
</figcaption>
</figure>
<p>那么从实际角度来看，产生更多文本以提高答案质量并无害处（用户经常在聊天系统中尝试“深度思考”模式以获得更好的答案）。</p>
<p>另一个研究通用推理的原因是，<strong>对于
LLM，任务难度与验证难度相关</strong>。我们致力于推动模型可以解决的问题的边界：一些对人类来说很困难的任务（例如，奥林匹克数学）只要有足够的奖励，模型很容易学习。相反，任务的奖励难以形式化，对模型来说更难学习。</p>
<h1 id="本质是什么">本质是什么</h1>
<p>我们需要在自由文本上进行 RLVR
的验证信号，这实际上是一个奖励函数，用于衡量真实答案和模型预测之间的语义一致性。这正是我们在自然语言生成中追求的。最基本的目标是交叉熵（ppl）。<strong>从这一角度来看，NOVER
本质上将 SFT 损失转移到 RLVR 设置中，而最近的工作表明 SFT 和 RL
之间的差异通常并不大。</strong></p>
<p>尽管 NOVER 使用了
ppl，但困惑度可能不是最优的。我们可以沿着从细到粗的轴线排列验证信号：信号越粗糙，信息损失越多，奖励越稀疏。在这个轴线上，出现了三种主要方法：</p>
<ul>
<li>困惑度为基础的信号。</li>
<li>Rubrics/Checklists。</li>
<li>训练验证器模型，产生二进制（0/1）奖励。</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/59636a40c655c608a6733bb59d27039c.png" width="800">
<figcaption>
验证信号坐标轴
</figcaption>
</figure>
<p>与二元奖励相比, ppl
提供了更密集的信号，自然扩展到自由文本，并避免了奖励饱和；但它失去了绝对正确性的信号，即模型从未观察到严格的正确/不正确标签，我们无法使用
pass@k
风格的指标来评估样本难度。评分/检查清单介于这些极端之间：它们比二元奖励更细粒度，但仍比
ppl
更稀疏。高质量的评分通常需要样本级、人工专家标注。最近的一些工作探索了Rubrics/Checklists解决方案<sup class="refplus-num"><a href="#ref-ace_rl">[3]</a></sup><sup class="refplus-num"><a href="#ref-checklists_are_better">[4]</a></sup><sup class="refplus-num"><a href="#ref-ticking_all_the_boxes">[5]</a></sup><sup class="refplus-num"><a href="#ref-rubric_anchors">[6]</a></sup><sup class="refplus-num"><a href="#ref-rubrics_as_rewards">[7]</a></sup>.
特别是 Baichuan-M2
开发了一个相当详细的验证器系统，作为模型驱动的环境，具有患者模拟器（数据生成器）和评分生成器（rollout
评估器）
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>.</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/6a376729962f2a3b2ae838d70e7aa97f.png" width="600">
<figcaption>
Baichuan-M2
的验证器系统<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>
</figcaption>
</figure>
<p>Rubrics/Checklists
也促进了有控制的合成数据生成，以减少奖励模型的偏差<sup class="refplus-num"><a href="#ref-robust_reward_modeling">[9]</a></sup>,
这样奖励模型专注于真正的因果因素，并抵抗来自格式、长度或语气等hack。OpenAI
的 Deliberative Alignment 可以被视为一种 outcome-RL
方法，它使用安全导向的评分/检查清单
<sup class="refplus-num"><a href="#ref-deliberative_alignment">[10]</a></sup>.</p>
<h1 id="怎么做">怎么做</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/1ae265ffdbc0c5855689ef5c6b64c909.png" width="400">
<figcaption>
NOVER 的奖励是从策略模型在推理轨迹下对真实答案的条件困惑度
</figcaption>
</figure>
<p>NOVER 应用了一个粗糙但直接的方法：对于一个
rollout，计算策略模型在推理轨迹下对真实答案的条件困惑度作为奖励。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/2746aac29b4bcba4045197579be651f6.png" width="600">
<figcaption>
推理优势（RA）的想法。
</figcaption>
</figure>
<p>推理-ppl 基于的改进想法之前已经出现过。一篇短篇 NeurIPS 2024 LanGame
研讨会论文称这个概念为推理优势（RA），本质上是指推理困惑度与无推理基线的相对变化。该论文使用
RA 进行数据选择，即保持 CoT 示例，使其具有较高的 RA，以便用于
SFT，因此可以被视为一种离线 RL 风格的方法
<sup class="refplus-num"><a href="#ref-on_reward_functions">[11]</a></sup>.</p>
<p>巧合的是我先在NOVER中尝试了相对reasoning
perplexity的想法，然后才发现这篇有关RA的workshop论文：这说明相对提升的想法非常符合直觉。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/165b17d45b447fe0d39c0960d5557f99.png" width="600">
<figcaption>
longPPL 的想法。
</figcaption>
</figure>
<p>另一个与 ppl 相关的改进是
longPPL，它测量上下文依赖的子集令牌的困惑度：longPPL
从带有长上下文的困惑度中减去没有长上下文的困惑度，从而专注于那些真正依赖于长距离上下文的令牌
<sup class="refplus-num"><a href="#ref-what_is_wrong_with_perplexity">[12]</a></sup>.
RA 共享相同的理念：我们希望奖励来自那些在真实答案中真正需要 CoT
推理的令牌。</p>
<p>更有趣的是，在 GRPO 中，一个简单的组归一化使相对 ppl 改进和绝对 ppl
在优势计算上有效等价，因此绝对推理 ppl 本身是一个 solid 奖励信号。</p>
<p>但直接应用 ppl 有以下问题：</p>
<ul>
<li>首先，ppl
是数值不稳定的：优势估计在批次之间变化并表现出长度偏差。NOVER 将 ppl
转换为组量化，以产生更稳定的奖励。QRPO
应用量化变换更严格：它将奖励映射到数据集上基策略奖励分布的量化，使分区函数可处理，即使在离线
RL 中也能实现数值稳定的逐点奖励
<sup class="refplus-num"><a href="#ref-quantile_reward_policy_optimization">[13]</a></sup>.</li>
<li>应该使用哪个模型来计算
ppl？原则上，一个更强大的外部模型可以是一个更准确的验证器，但大模型和小模型之间的差距会导致问题，这与使用
DPO 从 GPT 蒸馏标签训练小模型时的糟糕蒸馏结果类似。NOVER
使用策略模型本身来计算
ppl，这节省了额外模型并降低了缩放难度。我们发现使用单独的大型验证器（闭源
SOTA 或专用验证器）通常会导致 LM-hack-LM 问题，而使用策略模型的 ppl
产生更平滑的学习曲线。</li>
<li>在小批次和有限计算的情况下，训练不稳定。NOVER
引入了一个策略代理同步：定期将策略参数复制到代理模型，并在训练期间从代理计算
ppl。这有效地增加了批次大小（类似于梯度累积）并稳定了奖励估计。</li>
</ul>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/55663a25c9fb5ef4fd4d482dd0508251.png" width="400">
<figcaption>
RLPR 证明了 ppl 可以准确测量推理优势。
</figcaption>
</figure>
<p>许多同时期的作品采用了相关想法，但差异在于如何稳定 ppl 数值。</p>
<ul>
<li>VeriFree
<sup class="refplus-num"><a href="#ref-reinforcing_general_reasoning">[14]</a></sup>
直接使用推理 ppl，但限制为短答案（≤7 个令牌），其中 ppl 更稳定，并展示了
ppl 可以在短 QA 上接近或超过基于验证器的基线。</li>
<li>RLPR <sup class="refplus-num"><a href="#ref-rlpr">[15]</a></sup>
使用相对令牌概率（每个令牌的平均概率，裁剪，然后计算优势）而不是
ppl，并提供了详细的消融实验，表明直接使用 ppl 如果使用不当会损失 20
分。</li>
<li>DRO
<sup class="refplus-num"><a href="#ref-direct_reasoning_optimization">[16]</a></sup>
针对长答案，使用相对推理 ppl
进行每个令牌加权，用于高方差真实答案令牌和局部权重衰减。</li>
<li>DeepWriter
<sup class="refplus-num"><a href="#ref-reverse_engineered_reasoning">[17]</a></sup>
专注于长文本写作，但纯粹使用推理 ppl
作为评分指标来过滤和迭代重写草稿（不是 RL
训练），通过保持在监督选择制度中避免数值不稳定性。</li>
</ul>
<h1 id="观察">观察</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/b6a2baa9c386467e6e30989a3390adf8.png" width="600">
<figcaption>
训练中的崩溃模式。
</figcaption>
</figure>
<p>我们早期训练中遇到了许多崩溃模式：completion
length爆炸，模型产生混乱文本的糟糕
rollout，以及格式奖励的同时爆炸。我们应用了上述技巧来稳定训练（见论文的消融实验，详细介绍“代理的诅咒”）。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/bbbd4be0ecaebc5827a9b09efa82bbcc.png" width="600">
<figcaption>
The curse of proxy.
</figcaption>
</figure>
<p>一个小的但有用的小技巧是奖励依赖：当多个奖励项简单相加时，模型可能不确定哪个目标产生了给定的惩罚或奖励。实际上，我们发现将任务奖励限制在严格的格式奖励上有效：除非格式奖励满足，否则将所有其他奖励设置为零。当格式奖励获得时，模型通常是“合理的”，没有幻觉或乱码。这种依赖也可以将模型从训练崩溃中拉回来。</p>
<p>我们发现，对格式奖励的过度严格可能会阻碍探索
<sup class="refplus-num"><a href="#ref-simplerl_zoo">[18]</a></sup>.
例如，我们观察到的一种有趣的格式奖励 hack 是 CoT 中的嵌套
<code>&lt;think&gt;</code> 标签：模型可以在外层
<code>&lt;think&gt;</code> 块内嵌套一个子推理反射，以游戏信号，例如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;think&gt;</span><br><span class="line">  inner thoughts</span><br><span class="line">  &lt;think&gt;</span><br><span class="line">    reflection on the earlier thoughts</span><br><span class="line">  &lt;/think&gt;</span><br><span class="line">  continue reasoning</span><br><span class="line">&lt;/think&gt;</span><br><span class="line">&lt;answer&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/answer&gt;</span><br></pre></td></tr></table></figure>
<p>更强大的基础模型能更有效地利用密集的语义信号。例如，我们将选择题转换为开放式答案，要求模型同时输出选项字母和完整的选项文本；在比较
7B 和 3B 模型时，7B 模型在利用 ppl 对输出结果进行排序方面表现更优：</p>
<ul>
<li>rank 1: 选项字母和选项内容均正确</li>
<li>rank 2: 字母填错，但选项文本正确</li>
<li>rank 3: 字母正确，选项文本与另一个选项相似</li>
<li>rank 4: 字母正确但选项文本完全错误</li>
<li>…</li>
<li>lowest: 完全错误</li>
</ul>
<p>通过分析答案之外的推理输出，人们可以间接了解推理过程的差异。在一个需要解释和数值计算的天文问题中，我们让
GPT
分析每个推理过程（包括推理和结果），并按照推理质量进行排序；模型的定性分析结果与质量排名相符。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/3c6a8ca4f84f7aa6d787a4121aed2482.png" width="600">
<figcaption>
PPL 排名和 GPT 定性分析之间的关系。
</figcaption>
</figure>
<p>NOVER 也部分适用于非 Qwen 模型，但一些较弱的基座（例如某些 Mistral
检查点）会表现出异常行为。零样本 CoT
可以看作是一种未训练的基座探索策略；如果该基线接近或超过基础模型，强化学习（RL）通常能带来收益。</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e346a72fe2821b7eb95045dde4fbd235.png" width="600">
<figcaption>
NOVER 在一定程度上可以支持非 Qwen 模型。
</figcaption>
</figure>
<p>我们还注意到（并未进行详尽实验），许多通用推理数据集是由封闭式大型模型标注的，因此其客观性和准确性并不完美（定义松散，符号误用）。困惑度依然能提供有价值的指导信号：在某些情况下，模型通过困惑度信号学习到了复杂的推理模式，这些模式产生的答案可能比原始的真实标签更为合理。</p>
<h1 id="修改奖励就足够了吗">修改奖励就足够了吗</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/8cfc464fbb6cc0c7bc08f82c929c5475.png" width="1000">
<figcaption>
Some works on reproducing GRPO.
</figcaption>
</figure>
<p>不，但在将基于规则的验证扩展到通用推理时，奖励设计是最明显的不足。此外，从经验教训来看，许多算法技巧都是徒劳的：数据和计算才是关键。到三月，许多人都在复制
GRPO 并指出其脆弱性；我们的 NOVER
训练也暴露了类似的问题。与数据和规模相比，这些论文中提出的许多算法“技巧”的效果并不显著。</p>
<p>推进通用推理在数据基础模型方面面临更大挑战，后期需要通过算法工作来提升训练的效率和稳定性。</p>
<ul>
<li>现有的通用推理数据集质量参差不齐；清理数据需要耗费大量精力，而且许多数据是由
LLM 标注的（源自 GPT
或类似模型），而非人工精心编辑。这些数据是静态且数量有限的。强化学习在样本效率方面具有优势；实现规模化扩展的具成本效益的路径并非简单地增加更多示例，而是要提升环境和反馈的质量。</li>
<li>基础模型负责强化学习中的探索。实际上，它应已具备零样本指令跟随和思维链（CoT）能力，更丰富的知识会更有利。关于强化学习能否达到最终能力上限的讨论并非重点：训练后往往能激发潜在能力而非创造新能力。部分研究已探索结合记忆与启发式方法，我认为中期训练与训练后可能形成新的正反馈循环。</li>
</ul>
<h1 id="one-more-thing-攀爬solververifier不对称性">One more thing:
攀爬Solver–Verifier不对称性</h1>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/d53696cd2ce193f6011f24093c6e0f26.png" width="600">
<figcaption>
The Solver-Verifier Asymmetry.
</figcaption>
</figure>
<p>RLVR
的一个核心概念是求解器-验证器的不对称性：对于某些任务，验证比求解要容易，而对于另一些任务，验证则更困难。当验证相对求解较为简单时，RLVR
的表现尤为出色。而验证更困难的情况则包括：</p>
<ul>
<li>面对难以验证的自由回答的一般性推理</li>
<li>需要长时间才能获得回报的情境（例如，一个商业计划，其真实反馈可能在数周、数月甚至数年后才出现）。这些情况类似于推荐系统中的深度转化问题：我们需要精确的归因方法以及能够处理极度稀疏反馈的系统。</li>
<li>可能需要大量人工标注或难以获取的真实用户来验证解答的场景，这推动了高效用户模拟器的开发。</li>
</ul>
<p>NOVER 的 无验证器（verifier-free）
设计带来了一个新的可能性（尽管尚未实验）：</p>
<p><strong>是否可以将策略模型（policy
model）的智能同步到验证器模型（verifier
model），从而使求解器与验证器能够沿着 Solver-Verifier Asymmetry
的对角线共同进化？</strong></p>
<p>更强的策略模型会带来更强的验证器模型，而更强的验证器模型又能训练出更强的策略模型。关键在于智能的传递。NOVER
基于困惑度（perplexity）作为奖励的设计，自然地统一了求解器和验证器的智能形式：二者都旨在提高在良好推理轨迹上生成真值（ground
truth）的概率。 因此，可以通过标准的 RL
来实现共同进化，而无需额外设计对抗或自博弈任务。在这里，智能的传递方向是从求解到验证。一个对称的相关工作是
LLaVA-Critic-R1，它发现强大的偏好模型可以带来强大的策略模型，但它需要构造一个额外的任务。<sup class="refplus-num"><a href="#ref-llava_critic_r1">[19]</a></sup></p>
<p>如果我们希望实现这种完全自动化的共同攀爬，那么现有的 RL
训练相当于执行 横向攀爬（固定 verifier y，提升 solver x），而
智能同步（Intelligence Sync） 则会执行 纵向攀爬（固定 solver x，提升
verifier
y）。然而，我们还需要第三个变量：任务与数据。在求解器–验证器网格中的每一个点，都对应着特定的任务和数据集。正如我在早先关于
Scaling the Environment
的文章中所论述的，除了求解器与验证器之外，还有出题器（question
generator）。目前大多数关于推理进化的研究都集中在通过模型一致性或熵模式实现自我改进；部分方法实现了两个模块的共同进化，但三个模块的三重进化（tri-evolution）尚未被探索：</p>
<figure>
<img data-src="https://i.mji.rip/2025/09/13/e73159cd71bc40614799577a41c770f5.png" width="600">
<figcaption>
The Trinity of Solver-Verifier-Generator.
</figcaption>
</figure>
<ul>
<li>R-Zero 和 Self-Questioning Language Models
考虑了生成器与求解器之间的对抗式生成
<sup class="refplus-num"><a href="#ref-r_zero">[20]</a></sup><sup class="refplus-num"><a href="#ref-self_questioning">[21]</a></sup>。</li>
<li>URPO 将验证重新表述为一个求解任务并统一了数据训练；COOPER
则从当前策略的 rollout
构造正/负样本来训练验证器。这两条路线都实现了求解器–验证器的共同进化
<sup class="refplus-num"><a href="#ref-urpo">[22]</a></sup><sup class="refplus-num"><a href="#ref-cooper">[23]</a></sup>。</li>
</ul>
<p>另一条持续改进求解器的路径是
自博弈（self-play）：在合适的环境下，两个求解器可以通过对弈来相互提升，而不必担心非对称性。对于一般性推理，这类环境很难设计，因为“规则”本身是模糊的。近期有研究证明模型能够学习规则
<sup class="refplus-num"><a href="#ref-llms_can_learn_rules">[24]</a></sup>，并且可以通过合成数据和任务
组合原子技能以学习新技能，但现有的真实通用推理数据集仍然只是有限的枚举，而非全面的规则集。这依旧本质上是静态数据集/基准驱动的
RL。在 AI
的“下半场”，我们应当寻求真实世界的环境与问题，而非停留在静态数据集。</p>
<p>在静态数据与真实世界之间存在一种中间件：模拟器（simulators）。模拟器以牺牲真实性换取反馈速度——类似奖励模型或验证器模型——而在通用推理场景中，一个有用的模拟器可能会类似医学领域的病人模拟器（参考
Baichuan-M2 的案例），因为真实病人涉及伦理与监管问题，验证也往往较慢
<sup class="refplus-num"><a href="#ref-baichuan_m2">[8]</a></sup>。</p>
<p>另一种思路是放弃特定任务环境，而转向
博弈环境：如果推理模式能够跨游戏与任务迁移，那么在游戏中的自博弈可能提升数学与通用推理能力
<sup class="refplus-num"><a href="#ref-play_to_generalize">[25]</a></sup><sup class="refplus-num"><a href="#ref-spiral">[26]</a></sup>。若可行，我们就能够利用游戏环境和自博弈来持续进化通用推理模型。</p>
<h1 id="引用">引用</h1>
<p>如果你觉得这篇博文的话题很有趣，需要引用时，可以使用如下bibtex:
</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@article{general_reasoning_202509,</span><br><span class="line">  author = {Wei Liu},</span><br><span class="line">  title = {Towards General Reasoning},</span><br><span class="line">  year = {2025},</span><br><span class="line">  month = {9},</span><br><span class="line">  url = {https://thinkwee.top/2025/09/13/general-reasoning/#more},</span><br><span class="line">  note = {Blog post}</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p></p>
</div>
<script src="https://giscus.app/client.js" data-repo="thinkwee/thinkwee.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk3OTYxNjMwOA==" data-category="Announcements" data-category-id="DIC_kwDOBL7ZNM4CkozI" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
</script>
<ul id="refplus"><li id="ref-compass_verifier" data-num="1">[1]  CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward.</li><li id="ref-is_cot_mirage" data-num="2">[2]  Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens.</li><li id="ref-ace_rl" data-num="3">[3]  ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning.</li><li id="ref-checklists_are_better" data-num="4">[4]  Checklists Are Better Than Reward Models For Aligning Language Models.</li><li id="ref-ticking_all_the_boxes" data-num="5">[5]  TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation.</li><li id="ref-rubric_anchors" data-num="6">[6]  Reinforcement Learning with Rubric Anchors.</li><li id="ref-rubrics_as_rewards" data-num="7">[7]  Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains.</li><li id="ref-baichuan_m2" data-num="8">[8]  Baichuan-M2: Scaling Medical Capability with Large Verifier System.</li><li id="ref-robust_reward_modeling" data-num="9">[9]  Robust Reward Modeling via Causal Rubrics.</li><li id="ref-deliberative_alignment" data-num="10">[10]  Deliberative Alignment: Reasoning Enables Safer Language Models.</li><li id="ref-on_reward_functions" data-num="11">[11]  On Reward Functions For Self-Improving Chain-of-Thought Reasoning Without Supervised Datasets.</li><li id="ref-what_is_wrong_with_perplexity" data-num="12">[12]  What is Wrong with Perplexity for Long-context Language Modeling?</li><li id="ref-quantile_reward_policy_optimization" data-num="13">[13]  Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions.</li><li id="ref-reinforcing_general_reasoning" data-num="14">[14]  Reinforcing General Reasoning without Verifiers.</li><li id="ref-rlpr" data-num="15">[15]  RLPR: Extrapolating RLVR to General Domains without Verifier.</li><li id="ref-direct_reasoning_optimization" data-num="16">[16]  Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks.</li><li id="ref-reverse_engineered_reasoning" data-num="17">[17]  Reverse-Engineered Reasoning for Open-Ended Generation.</li><li id="ref-simplerl_zoo" data-num="18">[18]  SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild.</li><li id="ref-llava_critic_r1" data-num="19">[19]  LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model.</li><li id="ref-r_zero" data-num="20">[20]  R-Zero: Self-Evolving Reasoning LLM from Zero Data.</li><li id="ref-self_questioning" data-num="21">[21]  Self-Questioning Language Models.</li><li id="ref-urpo" data-num="22">[22]  URPO: A Unified Reward &amp; Policy Optimization Framework for Large Language Models.</li><li id="ref-cooper" data-num="23">[23]  COOPER: CO-OPTIMIZING POLICY AND REWARD MODELS IN REINFORCEMENT LEARNING FOR LARGE LANGUAGE MODELS.</li><li id="ref-llms_can_learn_rules" data-num="24">[24]  Large Language Models can Learn Rules.</li><li id="ref-play_to_generalize" data-num="25">[25]  Play to Generalize: Learning to Reason Through Game Play.</li><li id="ref-spiral" data-num="26">[26]  SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning.</li></ul>

    <style>
    #refplus, #refplus li{ 
        padding:0;
        margin:0;
        list-style:none;
    }；
    </style>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script>
    document.querySelectorAll(".refplus-num").forEach((ref) => {
        let refid = ref.firstChild.href.replace(location.origin+location.pathname,'');
        let refel = document.querySelector(refid);
        let refnum = refel.dataset.num;
        let ref_content = refel.innerText.replace(`[${refnum}]`,'');
        tippy(ref, {
            content: ref_content,
        });
    });
    </script>
    
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/math/" rel="tag"># math</a>
              <a href="/tags/rl/" rel="tag"># rl</a>
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/agent/" rel="tag"># agent</a>
              <a href="/tags/inference/" rel="tag"># inference</a>
              <a href="/tags/reasoning/" rel="tag"># reasoning</a>
              <a href="/tags/questions/" rel="tag"># questions</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/17/env-matrix/" rel="prev" title="Scaling the Environment">
                  <i class="fa fa-angle-left"></i> Scaling the Environment
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/05/wild-era/" rel="next" title="(Welcome) to the Era of Wild">
                  (Welcome) to the Era of Wild <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP/IP地址/域名信息备案管理系统 </a>
      <img src="http://www.beian.gov.cn/img/new/gongan.png" alt=""><a href="https://beian.mps.gov.cn/#/query/webSearch?code=%E4%BA%ACICP%E5%A4%872023015408%E5%8F%B7" rel="noopener" target="_blank">京ICP备2023015408号 </a>
  </div>
  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Thinkwee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">1.1m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">17:08</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/thinkwee" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>





  <script src="/js/third-party/addtoany.js"></script>

  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"g58NgfMJBlwTyftr6hizdozq-gzGzoHsz","app_key":"1nA1tNVxeeSlAumHogP0PvSd","server_url":"https://leancloud.cn","security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":false,"archive":false,"delay":true,"timeout":3000,"priority":true,"url":"https://thinkwee.top/2025/09/13/general-reasoning/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
