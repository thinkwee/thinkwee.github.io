---
title: Align Beyond PPO
date: 2023-06-05 20:40:55
tags:
---
https://arxiv.org/pdf/2305.18290.pdf
https://arxiv.org/pdf/2306.01150.pdf
https://arxiv.org/pdf/2305.17333.pdf
https://arxiv.org/pdf/2304.15004.pdf
https://arxiv.org/pdf/2305.11206.pdf
https://arxiv.org/pdf/2304.12244.pdf
https://arxiv.org/pdf/2305.16960.pdf
https://arxiv.org/pdf/2305.16334.pdf
https://arxiv.org/pdf/2305.13246.pdf
https://arxiv.org/pdf/2306.03341.pdf
https://arxiv.org/pdf/2306.01693.pdf 

Fine-Grained Human Feedback Gives Better Rewards for Language Model Training

ppo实现了instruct-following吗
rlfh实现了instruct-following吗
为什么实现了
除了ppo能否实现instruct-following
除了rlfh能否实现instruct-following
能否实现更通用的alignment
reward能否更细粒度，语言本身能否成为reward
为什么一定要有reward的概念，监督信号哪里不够
现在的研究都只能改造小模型、公开模型，就像测试环境一样，openai是正式环境，环境的gap可能影响学术研究的方向吗
