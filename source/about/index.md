title: About Me
comments: false
html: true
---
***

<style>
    .bc {
        display: inline-block;
        padding: 0px 5px;
        font-size: 14px;
        text-align: center;
        width: 40px; /* 固定按钮宽度为150像素 */
        text-decoration: none;
        background-color: #FFFFFF; /* Apple-style blue color */
        color: black;
        margin-bottom: 5px; /* 调整按钮之间的下外边距 */
        border-radius: 8px; /* Slight border radius for a softer look */
        border: 1px solid #CCCCCC; /* Border color same as background color */
        transition: background-color 0.3s ease; /* Smooth transition on hover */
    }

    .bc:hover {
        background-color: #999999; /* Darker blue color on hover */
        color: white;
        border: 1px solid transparent; /* 将边框颜色设置为透明 */
    }
    .bp {
        display: inline-block;
        padding: 0px 5px;
        font-size: 14px;
        width: 40px; /* 固定按钮宽度为150像素 */
        text-align: center;
        text-decoration: none;
        margin-bottom: 5px; /* 调整按钮之间的下外边距 */
        background-color: #FFFFFF; /* Apple-style blue color */
        color: black;
        border-radius: 8px; /* Slight border radius for a softer look */
        border: 1px solid #CCCCCC; /* Border color same as background color */
        transition: background-color 0.3s ease; /* Smooth transition on hover */
    }

    .bp:hover {
        background-color: #6699FF; /* Darker blue color on hover */
        color: white;
        border: 1px solid transparent; /* 将边框颜色设置为透明 */
    }
</style>



<p align="center">
  <img src="/img/avatar.jpg" alt="Your Image Description" width="200" height="200">
</p>
<center>2023.5 at XiaMen</center>

# About Me
- Hello, I'm Wei Liu (刘维). Here are my 
  <a href="mailto:thinkwee2767@gmail.com" style="display: inline-flex; align-items: center; text-decoration: none; line-height: 1;">
    <img src="https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/gmail.svg" style="height: 1em; width: 1em; margin: 0 0.2em; display: inline-block; vertical-align: -0.1em;" alt="Email">Email
  </a>, 
  <a href="https://github.com/thinkwee" target="_blank" rel="noopener" style="display: inline-flex; align-items: center; text-decoration: none; line-height: 1;">
    <img src="https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/github.svg" style="height: 1em; width: 1em; margin: 0 0.2em; display: inline-block; vertical-align: -0.1em;" alt="GitHub">Github
  </a>, 
  and 
  <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=QvW2leIAAAAJ" target="_blank" rel="noopener" style="display: inline-flex; align-items: center; text-decoration: none; line-height: 1;">
    <img src="https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/googlescholar.svg" style="height: 1em; width: 1em; margin: 0 0.2em; display: inline-block; vertical-align: -0.1em;" alt="Google Scholar">Google Scholar
  </a>.
  **I am now actively looking for PhD positions and job positions (NLP/LLM/Agent), feel free to contact me~**
    -   2014-2018: Bachelor of Communication Engineering in BUPT
    -   2018-2021: Master of Computer Engineering in CIST Lab@BUPT
    -   2021-2023: Application Research, [Tencent](https://www.tencent.com/en-us/about.html)
    -   2023.8-present: Working at [THUNLP](https://nlp.csai.tsinghua.edu.cn/) with Prof. [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/) with a focus on LLM Multi-Agent System.
        -   See my works [here](https://thinkwee.top/multiagent_ebook/#more-works).
        -   Our LLM Multi-Agent framework, [ChatDev](https://github.com/OpenBMB/ChatDev), has reached [Github \#1 for 3 times and has earned 26k 🌟](https://trendshift.io/repositories/1245)!
        -   **2024.9 Our paper about human-agent coexist society, [iAgents](https://arxiv.org/abs/2406.14928), is accepted by NeurIPS 2024!**. Try demo [here](https://thinkwee.top/iagents/).

# Research Interests
-   Natural Language Generation, especially on **Compressing and Summarizing Languages**.
-   Memorization and reasoning in LLMs.
-   Develop robust, safe, efficient, and human-centric **LLM Multi-Agent System**.
-   Served as reviewer for ACL(2021,2022,2024)/EMNLP(2020,2024)/NeurIPS(2024)/ICLR(2024).

# Industrial Experience
-   At Tencent, I aim to improve the performance of News Feed Recommendations and Advertising.
    -   Improving the NLU ability for News Feed Recommendation.
    -   Resolving the mismatch between commercial inclinations and content interests for Wechat Ads.
    -   Stability, Warm-Up, Efficiency-Quality Tradeoff, Interpretability & Explainability on Large Recommendation System.
    -   Diverse user interest modeling.

# Publications
-   Multi-Agents powered by LLMs:
    -   <a href="https://arxiv.org/abs/2406.14928" class="bp">paper</a>  <a href="https://github.com/thinkwee/iAgents" class="bc">code</a> Autonomous Agents for Collaborative Task under Information Asymmetry. NeurIPS 2024
    -   <a href="https://arxiv.org/abs/2307.07924" class="bp">paper</a>  <a href="https://github.com/OpenBMB/ChatDev" class="bc">code</a> Communicative Agents for Software Development. ACL 2024
    -   <a href="https://arxiv.org/abs/2312.17025" class="bp">paper</a>  <a href="https://github.com/OpenBMB/ChatDev" class="bc">code</a> Experiential Co-Learning of Software-Developing Agents. ACL 2024
    -   <a href="https://arxiv.org/pdf/2405.04219" class="bp">paper</a>  <a href="https://github.com/OpenBMB/ChatDev" class="bc">code</a> Iterative Experience Refinement of Software-Developing Agents. Arxiv
    -   <a href="https://arxiv.org/pdf/2406.07155" class="bp">paper</a>  <a href="https://github.com/OpenBMB/ChatDev" class="bc">code</a> Scaling Large-Language-Model-based Multi-Agent Collaboration. Arxiv
    -   <a href="https://arxiv.org/pdf/2406.08979" class="bp">paper</a>  <a href="https://github.com/OpenBMB/ChatDev" class="bc">code</a> Multi-Agent Software Development through Cross-Team Collaboration. Arxiv
-   More Accurate and Controllable Keyphrase Prediction: 
    -   <a href="https://arxiv.org/pdf/2106.04847.pdf" class="bp">paper</a> <a href="https://github.com/thinkwee/UniKeyphrase" class="bc">code</a> UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. ACL 2021 findings
    -   <a href="https://ojs.aaai.org/index.php/AAAI/article/download/21402/version/19689/21151" class="bp">paper</a> <a href="https://github.com/m1594730237/FastAndConstrainedKeyphrase" class="bc">code</a> Fast and Constrained Absent Keyphrase Generation by Prompt-Based Learning. AAAI 2022
-   More Comprehensive and Factual Summarization: 
    -   <a href="https://www.aclweb.org/anthology/K19-1077/" class="bp">paper</a> <a href="https://github.com/thinkwee/DPP_CNN_Summarization" class="bc">code</a> In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes. CoNLL 2021
    -   <a href="https://arxiv.org/pdf/2106.10084.pdf" class="bp">paper</a> <a href="https://github.com/thinkwee/SubjectiveBiasABS" class="bc">code</a> Subjective Bias in Abstractive Summarization. Arxiv
    -   <a href="https://arxiv.org/pdf/2112.01147.pdf" class="bp">paper</a> <a href="https://github.com/thinkwee/co2sum" class="bc">code</a> CO2Sum: Contrastive Learning for Factual-Consistent Abstractive Summarization. Arxiv
    -   <a href="https://www.aclweb.org/anthology/W19-8904.pdf" class="bp">paper</a> <a href="https://github.com/thinkwee/multiling2019_wiki" class="bc">code</a> Multi-lingual Wikipedia Summarization and Title Generation On Low Resource Corpus. RANLP 2019 workshop
    -   <a href="https://www.researchgate.net/publication/332432404_A_Multi-View_Abstractive_Summarization_Model_Jointly_Considering_Semantics_and_Sentiment" class="bp">paper</a> A Multi-View Abstractive Summarization Model Jointly Considering Semantics and Sentiment. CCIS 2018 
    -   <a href="http://ceur-ws.org/Vol-2414/paper20.pdf" class="bp">paper</a> CIST@CLSciSumm-19: Automatic Scientific Paper Summarization with Citances and Facets. SIGIR 2019 workhop
    -   <a href="https://www.aclweb.org/anthology/2020.sdp-1.25.pdf" class="bp">paper</a> CIST@CL-SciSumm 2020, LongSumm 2020: Automatic Scientific Document Summarization. EMNLP 2020 workshop
