---
title: Seq2seq预训练调研
date: 2021-06-16 16:40:58
tags:
  - pretrained language model
  - nlg
  - seq2seq
  -	nlp
categories:
  - 自然语言处理
html: true
mathjax: true
---
***
-	调研近年来关于文本生成预训练模型的工作（挖坑暂未填）
	-	微软:mass, unilm v1,v2,v3, prophetnet, MPNet, DEBERTa, COCO-LM, XNLG, FST, CMLM, SSR, POINTER, pretrain-LB
	-	北大:GLM
	-	google:seq2seq pretrain, t5, pegasus
	-	百度:PLATO, ernie-gen, PLATO-2
	-	阿里:PALM
	-	facebook:bart
	-	stanford: TED
	-	JD:reinstating-abs

<!--more-->

