---
title: 2019-2020的阅读
date: 2019-12-16 16:07:25
categories: 自然语言处理
tags:
  - graph neural network
  - deep learning
  - summarization
  -	natural language processing
mathjax: true
html: true
---

跨年读论文。
-	边池化
-	Discourse-Aware，抽取式摘要
-	Discourse-Aware，生成式摘要


<!--more-->

# Edge Contraction Pooling for Graph Neural Networks
-	一种新的GNN池化方式，考虑了边
-	池化在GNN中的意义：
	-	识别基于特征或者基于结构的聚类
	-	减少计算量
-	作者提出的edgepool能够提高图分类和节点分类的性能。
-	pooling有两种，fixed和learned，作者简单介绍了三种learned pooling method
	-	DiffPool：DiffPool学习到一种概率分配，用一个GNN学习embedding，用一个GNN学习聚类分配，将聚类分配视为一个soft assign matrix$S$，基于节点特征将每个节点分配给一个聚类，聚类数量事先固定，每一层同时对embedding和邻接矩阵进行pooling，如下：
	$$
	\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1} \times d}} \\ 
	{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}}\end{array} \\
	$$
	问题在于：聚类数量不可变；基于节点特征分配而不考虑节点之间距离；聚类分配矩阵与节点数目成线性关系，难以scale；难以训练
	-	TopKPool：简单粗暴，学习到一个投影向量，将每个节点的特征投影加权为一个单值，取topk个节点保留作为Pooling，问题在于不能改变图（加节点），以及这种hard assignment容易丢失信息
	-	SAGPool：对TopK的改进，对邻域节点使用了注意力加权，再投影，不过依然是topk的hard assignment。
-	edge pooling的思想是通过边的收缩(edge contraction)来降采样，给定一条边e，两边节点$v_i$和$v_j$，边收缩指的是将i和j的所有邻接节点全部接到一个新节点$v_e$，这个操作显然是可以叠加多次，类似于CNN的不断扩大感受野。
-	如何选边？
	-	先对边计算分数，这里简单的将边连接的两个节点的embedding拼接再线性变换
	$$
	r(e_{ij}) = W (n_i || n_j) + b
	$$
	-	之后对所有的分数做softmax归一化，注意这里作者加了0.5使得均值为1，作者给出的解释是数值计算更稳定且梯度传导更好
	$$
	s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
	$$
	-	按照分数开始收缩边，假如边连接了已经收缩的边节点那就不再收缩了。这样每次都能减少一半的节点。
-	新的节点分数直接用边分数加权两端节点特征和得到：
	$$
	\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
	$$

# Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization

# A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents