---
title: 2019-2020的阅读
date: 2019-12-16 16:07:25
categories: 自然语言处理
tags:
  - graph neural network
  - deep learning
  - summarization
  -	natural language processing
mathjax: true
html: true
---

跨年读论文。
-	边池化
-	Discourse-Aware，抽取式摘要
-	Discourse-Aware，生成式摘要


<!--more-->

# Edge Contraction Pooling for Graph Neural Networks
-	一种新的GNN池化方式，考虑了边
-	池化在GNN中的意义：
	-	识别基于特征或者基于结构的聚类
	-	减少计算量
-	作者提出的edgepool能够提高图分类和节点分类的性能。
-	pooling有两种，fixed和learned，作者简单介绍了三种learned pooling method
	-	DiffPool：DiffPool学习到一种概率分配，用一个GNN学习embedding，用一个GNN学习聚类分配，将聚类分配视为一个soft assign matrix$S$，基于节点特征将每个节点分配给一个聚类，聚类数量事先固定，每一层同时对embedding和邻接矩阵进行pooling，如下：
	$$
	\begin{array}{l}{X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1} \times d}} \\ 
	{A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}}\end{array} \\
	$$
	问题在于：聚类数量不可变；基于节点特征分配而不考虑节点之间距离；聚类分配矩阵与节点数目成线性关系，难以scale；难以训练
	-	TopKPool：简单粗暴，学习到一个投影向量，将每个节点的特征投影加权为一个单值，取topk个节点保留作为Pooling，问题在于不能改变图（加节点），以及这种hard assignment容易丢失信息
	-	SAGPool：对TopK的改进，对邻域节点使用了注意力加权，再投影，不过依然是topk的hard assignment。
-	edge pooling的思想是通过边的收缩(edge contraction)来降采样，给定一条边e，两边节点$v_i$和$v_j$，边收缩指的是将i和j的所有邻接节点全部接到一个新节点$v_e$，这个操作显然是可以叠加多次，类似于CNN的不断扩大感受野。
-	如何选边？
	-	先对边计算分数，这里简单的将边连接的两个节点的embedding拼接再线性变换
	$$
	r(e_{ij}) = W (n_i || n_j) + b
	$$
	-	之后对所有的分数做softmax归一化，注意这里作者加了0.5使得均值为1，作者给出的解释是数值计算更稳定且梯度传导更好
	$$
	s_{ij} = 0.5 + softmax_{r_{*j}}(R_{ij})
	$$
	-	按照分数开始收缩边，假如边连接了已经收缩的边节点那就不再收缩了。这样每次都能减少一半的节点。
-	新的节点分数直接用边分数加权两端节点特征和得到：
	$$
	\hat{n}_{i j}=s_{i j}\left(n_{i}+n_{j}\right)
	$$

# Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization
-	以hierarchical lstm encoder+lstm decoder的抽取式摘要作为baseline，添加了一个三层attention用来加入篇章信息，这里的篇章信息具体指的是句子级别的elaborate关系，即某一句详细阐述或者补充说明了另一句，作者认为document summarization这种篇章级别的任务当然需要篇章信息。
-	作者使用了attention来学习句子之间的elaborate有向边，具体如下图：
![1MfDOI.png](https://s2.ax1x.com/2020/01/29/1MfDOI.png)
-	三个组件
	-	Parent Attention：使用hierarchical encoder得到每个句子的表示，之后用attention表示句子k是句子i父节点的概率，即elaborate的边由k指向i（作者没有用self attention）
	$$
	\begin{aligned} p(k | i, \mathbf{H}) &=\operatorname{softmax}(g(k, i)) \\ g(k, i) &=v_{a}^{\mathrm{T}} \tanh \left(U_{a} \cdot H_{k}+W_{a} H_{i}\right) \end{aligned}
	$$
	-	Recursive Attention：即计算多跳父节点，得到k是i的d跳父节点概率，这里简单的用注意力矩阵幂应该就可以得到，注意要对root句子（虚节点）做特殊处理，root没有父节点：
	$$
	\alpha_{d, k, i}=\left\{\begin{array}{ll}{p(k | i, \mathbf{H})} & {(d=1)} \\ {\sum_{l=0}^{N} \alpha_{d-1, k, l} \times \alpha_{1, l, i}} & {(d>1)}\end{array}\right.
	$$
	-	Selective Attention：综合得到的attention信息，首先将句子i某一跳所有父节点的信息加权求和：
	$$
	\gamma_{d, i}=\sum_{k=0}^{N} \alpha_{d, k, i} H_{k}
	$$
	之后再用selective attention计算该跳的权重，依赖于句子i的encoder和decoder state$H,s$，以及所有父节点的encoder state：
	$$
	\beta_{d, i}=\operatorname{softmax}\left(\mathbf{W}_{\beta}\left[H_{i} ; s_{i} ; K\right]\right)
	$$
	得到权重加权所有跳的信息，并补充进decoder input当中（拼接）
	$$
	\Omega_{i}=\sum_{d} \beta_{d, i} \gamma_{d, i} \\
	p\left(y_{i} | \mathbf{x}, \theta\right)=\operatorname{softmax}\left(\mathbf{W}_{o} \tanh \left(\mathbf{W}_{c^{\prime}}\left[H_{i} ; s_{t} ; K ; \Omega_{i}\right]\right)\right) \\
	$$
-	这里，作者说提到了修辞结构分析（RST）目前没有很好的off-the-shelf tools，误差大，这是硬伤，因此提出了一个联合学习的框架，后来发现联合学习是指训练集上依然用已有的RST Parser得到elaborate edges，用以指导Parent Attention，之后测试集就不需要了，这样的话Parser当中的误差对模型的影响依然很大。目标函数为：
	$$
	-\log p(\mathbf{y} | \mathbf{x})-\lambda \cdot \sum_{k=1}^{N} \sum_{i=1}^{N} E_{k, i} \log \alpha_{1, k, i}
	$$
	其中第二项就是用parser得到的边指导attention
-	作者先用HILDA parser得到RST格式的篇章标注信息，然后用Single-document summarization as a tree knapsack
problem一文中的方法转换为dependency的格式
-	虽然依然依赖于parser进行训练，但是作者做了两个Baseline，一个是不用parser，直接将前一句作为下一句的elaborate parent，另一个也不用parser，让attention自己学习，结果发现baseline都不如注入了parser信息的attention模型。让attention自己学习最差，其次是学一个固定的前句父节点。作者提出的模型相比baseline在daily mail数据集上抽短文本（75)比抽长文本(275)优势更大，这里有ROUGE指标偏爱长文本的原因，也说明在字数限制下，抽最重要的信息方面，discourse的信息确实可以起到帮助。
-	这篇文章可以看成一个attention模型(self attention + multi-blocks)，注入了一些先验信息来帮助在单文档抽取式摘要获得更好的结果。

# A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents
-	NAACL 2018的一篇论文，依然是考虑了篇章信息，不过是在科研论文数据集上做生成式摘要。
-	这里的discourse有些狭义了，指的是科研论文里的每一个section，其实还是一个hierarchical attention，作者也直接在pointer-generator上改了，结构如下：
![1MHPr4.png](https://s2.ax1x.com/2020/01/29/1MHPr4.png)
-	值得称赞的是作者提供了两个大规模长文档的科研论文摘要数据集，pubmed以及arxiv，均达到十万规模即便，平均原文长度达到3000+和4900+，平均摘要长度也过百，是很有价值的超长单文档摘要数据集。
