---
title: Lagrange,KTT,PCA,SVM
date: 2017-03-18 11:20:35
categories: 机器学习
tags:
  - code
  - machinelearning
mathjax: true
photos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170318/112133081.png
html: true
---
***
介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用
图片来自wikipedia关于拉格朗日乘子法的形象介绍
<!--more-->

# 拉格朗日乘子法
-	拉格朗日乘子法是一种求约束条件下极值的方法，描述为
	$$
	在约束条件g(x,y)=c下 \\
	求函数f(x,y)的极值 \\
	$$
	其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。
-	由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。
-	显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为
	$$
	\nabla f (x, y) = \nabla (\lambda \left(g \left(x, y \right) - c \right))
	$$
	$\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。
	拉格朗日方程即$ F(x,y)=\nabla \Big[f \left(x, y \right) + \lambda \left(g \left(x, y \right) - c \right) \Big] $
-	求解上面的式子，就得到一组$(x,y,\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。
-	拉格朗日系数的含义是最大增长值，$-\frac{\partial \Lambda}{\partial {c_k}} = \lambda_k$

# 卡罗需-库恩-塔克条件
-	如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件
-	包含不等约束的极值问题描述为
	$$
	在约束条件: \\
	h_j(X)=0 j=1,2,...,p \\
	g_k(X)\leq 0 k=1,2,...q \\
	求函数f(X)的极值 \\
	$$
-	拉格朗日函数为
	$$
	L(X,\lambda ,\mu)=f(X)+\sum _{j=1}^p \lambda _j h_j(X) + \sum _{k=1}^q \mu g_k(X)
	$$
-	KTT条件为:
	$$
	\frac{dL}{dX}=0 \\
	\lambda _j \neq 0 \\
	\mu _k \geq 0 \\
	\mu _k g_k(X)=0 \\
	h_j(X)=0 \\
	g_k(X) \leq 0\\
	$$

# PCA
-	PCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本
-	记$x_1,...,x_p$为原始p个维度，新维度是$\xi _1,....,\xi _p$
-	新维度是原始维度的线性组合，表示为
	$$
	\xi _i = \sum _{j=1}^{p}  \alpha _{ij} x_j = \alpha _i^T x
	$$
-	为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即
	$$
	\alpha _i^T \alpha _i=1
	$$
-	令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类
-	此时问题就转化为具有约束条件的极值问题，约束条件为$\alpha _i^T \alpha _i=1$，求$var(\xi _i)$的极值，可以用拉格朗日乘子法求解
-	当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\xi _2 \xi _1\-E[\xi _1][\xi _2]]=0$即两个新维度不相关，求出第二个新维度
-	依次求出p个新维度
-	PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k<q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间
-	如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除
-	PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取	

# SVM
-	在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面
-	划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移
-	求解一个SVM，即找到满足约束
	$$
	\begin{cases}
	w^Tx_i+b \geq +1, y_i=+1 \\
	w^Tx_i+b \leq -1, y_i=-1 \\
	\end{cases}
	$$
	的条件下，使得两个异类支持向量到超平面的距离$\frac{2}{||w||}$最大
	这可以重写为最优化问题
	$$
	min_{w,b} \frac 12 {||w||}^2 \\
	s.t. y_i(w^Tx_i+b) \geq 1,i=1,2,...,m \\
	$$
	推导见:[线性可分支持向量机与硬间隔最大化](http://thinkwee.top/2017/02/12/StatisticalLearningNote/#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96)
-	对于这个最优化问题，它的拉格朗日方程是
	$$
	L(w,b,\alpha )=\frac 12 {||w||}^2+\sum _{i=1}^{m} \alpha _i (1-y_i(w^Tx_i+b))
	$$
	其中$\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题
	$$
	max _{\alpha } \sum _{i=1}^m \alpha _i -\frac 12 \sum _{i=1}^m \sum _{j=1}^m \alpha _i \alpha _j y_i y_j x_i^T x_j \\
	s.t. \sum _{i=1}^m \alpha _i y_i=0, \\
	\alpha _i \geq 0,i=1,2,...,m \\
	$$
	上式满足KTT条件，通过SMO算法求解
-	未完待续