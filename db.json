{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next-reloaded/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/me.jpg","path":"images/me.jpg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"source/baidu_verify_l3vacuMD8f.html","hash":"a960f42945b99b15d34d7ae9a694d91cdd0553da","modified":1533184748099},{"_id":"themes/next-reloaded/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1537587774444},{"_id":"themes/next-reloaded/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1537587774444},{"_id":"themes/next-reloaded/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1537587774444},{"_id":"themes/next-reloaded/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1537587774444},{"_id":"themes/next-reloaded/.gitignore","hash":"0378adb9c2dc4855b3198184df4863cb30e4059c","modified":1537587774446},{"_id":"themes/next-reloaded/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1537587774446},{"_id":"themes/next-reloaded/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1537587774446},{"_id":"themes/next-reloaded/LICENSE.md","hash":"f0190c7d83a98464549a6b3a51bb206148d88e1b","modified":1537587774446},{"_id":"themes/next-reloaded/README.md","hash":"460ef40953c2eccea4f87bc67043aac2eb87b1a9","modified":1537587774447},{"_id":"themes/next-reloaded/_config.yml","hash":"98335c39dc948d15008af4b9984912210a75c39c","modified":1537595075780},{"_id":"themes/next-reloaded/bower.json","hash":"960159e57be380d14aef75360412575cd8cb938f","modified":1537587774447},{"_id":"themes/next-reloaded/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1537587774447},{"_id":"themes/next-reloaded/gulpfile.coffee","hash":"67eaf2515100971f6195b60eeebbfe5e8de895ab","modified":1537587774453},{"_id":"themes/next-reloaded/package.json","hash":"2e6d67099c08a38d46bf3c43b580f7315284aeb8","modified":1537587774471},{"_id":"source/_posts/AM-Model-for-ASS.md","hash":"a0f57db2e3011f63574447c654c968d801a32674","modified":1532309318487},{"_id":"source/_posts/Lagrange.md","hash":"9dc62493ae5bf10807933e47168b82a9f0b3d8f8","modified":1535449407016},{"_id":"source/_posts/LinearAlgebra1.md","hash":"67b9da5f9ca78631570702f2bb083bf3354dc98a","modified":1537104405179},{"_id":"source/_posts/LinearAlgebra3.md","hash":"6492c8bb8865fb766376b079b12a8b4ea11dd30a","modified":1537278267280},{"_id":"source/_posts/LinearAlgebra2.md","hash":"5ad2abcbcf513e0cb96a0d5c5dde1ce54782bc11","modified":1535937714832},{"_id":"source/_posts/MachineLearningNote.md","hash":"5e7c8e658eaf6f2da5f83bfbaf89ffea6bdc4385","modified":1536977034578},{"_id":"source/_posts/NeuralNetworks1.md","hash":"97370f102f3422e63d36de50eca50c1fcaf0e648","modified":1532309318487},{"_id":"source/_posts/NLPBasic.md","hash":"833812d1d5b34fec3bdd1ba259d31bbf8c99ebfa","modified":1532309318487},{"_id":"source/_posts/PaperReading.md","hash":"8f833a121b69d4e4627a62fa3461d44de675af24","modified":1537451354711},{"_id":"source/_posts/PythonNotes.md","hash":"7fb9bb8f52148028daec19de9b763365ebc27840","modified":1532309318502},{"_id":"source/_posts/PaperReading2.md","hash":"aaa8e857ebc8d8ccbcf2b4b78ac490bdcc742a96","modified":1536411640145},{"_id":"source/_posts/TitanicLinearRegression.md","hash":"86522f39e1832997143dbddf5f5303102eabef43","modified":1532309318502},{"_id":"source/_posts/buptroomreview.md","hash":"94878f32bdb337bd769b429f80fddf662ad176a1","modified":1532309318502},{"_id":"source/_posts/convex-optimization.md","hash":"6e25b559cd9025e7948eedd91e38bf15a783397b","modified":1533348888460},{"_id":"source/_posts/cvxopt.md","hash":"f597df32c14b83e5efbb9e7385dd7b973dc29082","modified":1530862631346},{"_id":"source/_posts/dachuang.md","hash":"28b353549bef237fbdf485aac5a84291cfad5aa9","modified":1532309318502},{"_id":"source/_posts/dachuangserver.md","hash":"13140c83c3dc10e4121e29df8cf0bdb1c86a936e","modified":1532309318502},{"_id":"source/_posts/deepbayes2018.md","hash":"14ead7116772858334db9120e7cb1b2fe64c0f48","modified":1537594911054},{"_id":"source/_posts/dpp.md","hash":"aa4315b4fcfe608cddd1606cccdc48373cfefe03","modified":1537018955426},{"_id":"source/_posts/hlda.md","hash":"964cef6db5d7115a9f11176896a367e97e5a070c","modified":1535967579248},{"_id":"source/_posts/inference-algorithm.md","hash":"62fa849aff228061552484532dcabaf637ad0516","modified":1537515923962},{"_id":"source/_posts/kmeans.md","hash":"66e5f4fa9e4c8491ed9645eb274cc124f8133678","modified":1535879760160},{"_id":"source/_posts/lda.md","hash":"b312aaaac96e4114c6e6a632d4b86182ff841f87","modified":1535941882157},{"_id":"source/_posts/mnist.md","hash":"6c386b59f7487cd6a647756986a159376d182cdf","modified":1532309318502},{"_id":"source/_posts/numpycookbook.md","hash":"825205ee8785deee60e69b66a767eb684632e961","modified":1532309318502},{"_id":"source/_posts/oj.md","hash":"8d875ddfdf996a4612755002ac8dae643d0f625c","modified":1532309318502},{"_id":"source/_posts/pandas-skill.md","hash":"18bb8f8b0e291e079374e949b60805234f9c5896","modified":1532309318518},{"_id":"source/_posts/pandas-func.md","hash":"2eb497dcc35bfcc52ff043cd47d0bf4c2c54e639","modified":1532309318502},{"_id":"source/_posts/seq2seq-summarization.md","hash":"5372b810567701ed081aa6b4052c6dc9c9801f3b","modified":1532309318518},{"_id":"source/_posts/setupmywebsite.md","hash":"a5d25b49492f5f63a59a4793fe032772fee29444","modified":1532309318518},{"_id":"source/_posts/statistical-handwriting.md","hash":"b6eba22f3d3c3d890688ad49c33c90bd9c858a5f","modified":1535449410992},{"_id":"source/_posts/tensorflowtutorial.md","hash":"a96730eecda5b2b3b677044bc66ddae82167bda6","modified":1532309318518},{"_id":"source/_posts/trie.md","hash":"67475a3c018d3d7b56e1b7132355ab961c2efbbd","modified":1532309318518},{"_id":"source/about/index.md","hash":"2f356ce3252b176d8051e1d3ad77840a18c27bb6","modified":1537019239097},{"_id":"source/categories/index.md","hash":"ba5ab51915c24815b4a73993e9f71c408ce03bcc","modified":1532309318518},{"_id":"source/tags/index.md","hash":"94074ed5c8e4bb909be5edb0c9124ca5ae2ff5ef","modified":1532309318518},{"_id":"themes/next-reloaded/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1537587774435},{"_id":"themes/next-reloaded/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1537587765315},{"_id":"themes/next-reloaded/.git/config","hash":"7f8b507977a695e25b27d4c2e8aaed71410536bf","modified":1537587774437},{"_id":"themes/next-reloaded/.git/index","hash":"ed7f001fc78b574a2b346530f8bb559c2e13c9f5","modified":1537587774509},{"_id":"themes/next-reloaded/.git/packed-refs","hash":"cfff1327f48418de87782f5c317e0f5e898e217e","modified":1537587774433},{"_id":"themes/next-reloaded/.github/CODE_OF_CONDUCT.md","hash":"22f2ccc5522563b67c8663849fc1d6cbae93a8ff","modified":1537587774445},{"_id":"themes/next-reloaded/.github/CONTRIBUTING.md","hash":"40b0fd9fdb8586fab98e23f713940e7a2f80e2f9","modified":1537587774445},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE.md","hash":"1e212fe229bd659726b4a3bcf4b5b14e0310ba3a","modified":1537587774445},{"_id":"themes/next-reloaded/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1537587774446},{"_id":"themes/next-reloaded/.github/PULL_REQUEST_TEMPLATE.md","hash":"36201119490a04107c8179b10202548a9d0e5e60","modified":1537587774445},{"_id":"themes/next-reloaded/.github/stale.yml","hash":"dbd5e6bf89b76ad1f2b081578b239c7ae32755af","modified":1537587774446},{"_id":"themes/next-reloaded/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1537587774448},{"_id":"themes/next-reloaded/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1537587774448},{"_id":"themes/next-reloaded/docs/ALGOLIA-SEARCH.md","hash":"1dada3c3404445a00367882b8f97cdf092b7943d","modified":1537587774448},{"_id":"themes/next-reloaded/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1537587774448},{"_id":"themes/next-reloaded/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"f2fd611a84dd6f9ed5395b63e187e29e9effbcd3","modified":1537587774449},{"_id":"themes/next-reloaded/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1537587774449},{"_id":"themes/next-reloaded/docs/LICENSE","hash":"5b702310012d480b40529fd10cf1872f687277a0","modified":1537587774449},{"_id":"themes/next-reloaded/docs/MATH.md","hash":"34a46ca9a05b4570903beaadd4807e6759afb52e","modified":1537587774449},{"_id":"themes/next-reloaded/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1537587774449},{"_id":"themes/next-reloaded/layout/_layout.swig","hash":"2db9b03efc68be842b8f79fe2f0fd7fb09e8885a","modified":1537587774456},{"_id":"themes/next-reloaded/layout/archive.swig","hash":"4b53070008775ecfd03953bd1b4adfcb0fabcaac","modified":1537587774470},{"_id":"themes/next-reloaded/layout/category.swig","hash":"f0e3338bfa5efb205d2c28e635e9611f1fff3b55","modified":1537587774470},{"_id":"themes/next-reloaded/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1537587774470},{"_id":"themes/next-reloaded/layout/page.swig","hash":"9ddf40303f82e3db76d59dc82b6d4eadfed203c6","modified":1537587774470},{"_id":"themes/next-reloaded/layout/post.swig","hash":"0554f42f90f4a524666c2b520be30b689c1d6a87","modified":1537587774471},{"_id":"themes/next-reloaded/layout/schedule.swig","hash":"d99b9eff0cff38caf095445f27c08aaf11a5b862","modified":1537587774471},{"_id":"themes/next-reloaded/layout/tag.swig","hash":"7cda2822e50b9fee9848a6b81e6c2d1aca830aeb","modified":1537587774471},{"_id":"themes/next-reloaded/languages/de.yml","hash":"641e49587d41bb87e4d5932dc3d975754ded7953","modified":1537587774453},{"_id":"themes/next-reloaded/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1537587774453},{"_id":"themes/next-reloaded/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1537587774453},{"_id":"themes/next-reloaded/languages/fr.yml","hash":"ebcd1f188af8c3f5ef1f0923e794c839fbfae2d4","modified":1537587774453},{"_id":"themes/next-reloaded/languages/id.yml","hash":"9709a4dbacc56a1571a96b139b872128d6959e90","modified":1537587774453},{"_id":"themes/next-reloaded/languages/it.yml","hash":"4e3adeb10c0fa627935d69ae1783ce0894f5dee5","modified":1537587774454},{"_id":"themes/next-reloaded/languages/ja.yml","hash":"82afb0a5637ad67065fa5b2624fa56c7c240c3c6","modified":1537587774454},{"_id":"themes/next-reloaded/languages/ko.yml","hash":"33e065ceb21590b8eb32430a69e76c2f057eb758","modified":1537587774454},{"_id":"themes/next-reloaded/languages/nl.yml","hash":"060efc260c1c529469d739d97dcee79683e8f411","modified":1537587774454},{"_id":"themes/next-reloaded/languages/pt-BR.yml","hash":"dc09e290e908744ca28e093dbdd859ca2a20290e","modified":1537587774454},{"_id":"themes/next-reloaded/languages/pt.yml","hash":"53e2a52b9d5dc20c04080acd4f5b954e8699780f","modified":1537587774454},{"_id":"themes/next-reloaded/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1537587774455},{"_id":"themes/next-reloaded/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1537587774455},{"_id":"themes/next-reloaded/languages/vi.yml","hash":"e2b3b18359ab41d58c64b2002acfedd60a7505a4","modified":1537587774455},{"_id":"themes/next-reloaded/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1537587774455},{"_id":"themes/next-reloaded/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1537587774455},{"_id":"themes/next-reloaded/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1537587774455},{"_id":"themes/next-reloaded/scripts/helpers.js","hash":"7849f9b9a86fc82d6e186e32a5e26e1f27c49b47","modified":1537587774471},{"_id":"themes/next-reloaded/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1537587774471},{"_id":"themes/next-reloaded/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1537587774472},{"_id":"themes/next-reloaded/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1537587774508},{"_id":"themes/next-reloaded/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1537587774508},{"_id":"themes/next-reloaded/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1537587774508},{"_id":"themes/next-reloaded/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774493},{"_id":"source/_posts/task_em_files/task_em_35_1.png","hash":"9e8e18921f19ca3a15820d5b793b2568ba9be60a","modified":1537584002367},{"_id":"source/_posts/task_em_files/task_em_9_1.png","hash":"08546d7019a45bc175aafc6aed6556ef46e1ca55","modified":1537584002367},{"_id":"themes/next-reloaded/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1537587765315},{"_id":"themes/next-reloaded/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1537587765316},{"_id":"themes/next-reloaded/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1537587765316},{"_id":"themes/next-reloaded/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1537587765317},{"_id":"themes/next-reloaded/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1537587765317},{"_id":"themes/next-reloaded/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1537587765317},{"_id":"themes/next-reloaded/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1537587765318},{"_id":"themes/next-reloaded/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1537587765318},{"_id":"themes/next-reloaded/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1537587765319},{"_id":"themes/next-reloaded/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1537587765319},{"_id":"themes/next-reloaded/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1537587765320},{"_id":"themes/next-reloaded/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1537587765320},{"_id":"themes/next-reloaded/.git/logs/HEAD","hash":"ed4a8308bc7529fece74ead0ad0d6c2042e26dfa","modified":1537587774436},{"_id":"themes/next-reloaded/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1537587774450},{"_id":"themes/next-reloaded/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1537587774450},{"_id":"themes/next-reloaded/docs/ru/README.md","hash":"caaad965f9d54f82382c934f44a507d37a863fa3","modified":1537587774450},{"_id":"themes/next-reloaded/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1537587774450},{"_id":"themes/next-reloaded/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"e771c5b745608c6fb5ae2fa1c06c61b3699627ec","modified":1537587774451},{"_id":"themes/next-reloaded/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"9b512cb820627fcc45c9f77c7a122aa99e021bd5","modified":1537587774451},{"_id":"themes/next-reloaded/docs/zh-CN/CONTRIBUTING.md","hash":"6ea741f380dc3e90661d12db7e115a94b77643a4","modified":1537587774451},{"_id":"themes/next-reloaded/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1537587774452},{"_id":"themes/next-reloaded/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1537587774451},{"_id":"themes/next-reloaded/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"7214fcd1b5830e62b3ac0836ad2d1b0fa391ae12","modified":1537587774452},{"_id":"themes/next-reloaded/docs/zh-CN/MATH.md","hash":"4d68054b062b3c8404b146a155d9624d2d25dd9b","modified":1537587774452},{"_id":"themes/next-reloaded/docs/zh-CN/README.md","hash":"0e6652be1b6bca87479f2601ff89105def200b4c","modified":1537587774452},{"_id":"themes/next-reloaded/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1537587774452},{"_id":"themes/next-reloaded/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1537587774456},{"_id":"themes/next-reloaded/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1537587774456},{"_id":"themes/next-reloaded/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1537587774456},{"_id":"themes/next-reloaded/layout/_macro/post-copyright.swig","hash":"8ff97c3f4eb888207bdc50066053c97d890e0202","modified":1537587774457},{"_id":"themes/next-reloaded/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1537587774457},{"_id":"themes/next-reloaded/layout/_macro/post-related.swig","hash":"e8dfb86eb62b9c2bc1435d6d1afa95d3b4c7b931","modified":1537587774457},{"_id":"themes/next-reloaded/layout/_macro/post.swig","hash":"00cd242c032341ab0fb5d7f66d4a9aa652eab183","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_macro/reward.swig","hash":"56733f92352b891b0bb1baca293f68f6c8928b0b","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_macro/sidebar.swig","hash":"a9519b4f82aa516e516317d0447bf5c5c7ffb544","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_macro/wechat-subscriber.swig","hash":"7a9b687087793bf5e218cbc345214f927fa5601a","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_partials/breadcrumb.swig","hash":"4b48fdbfe3bf41ddeda4ff74c1ff17ab9f15c14e","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_partials/comments.swig","hash":"412d2a0e18a237e490e1b8bce5547558768b9da3","modified":1537587774458},{"_id":"themes/next-reloaded/layout/_partials/footer.swig","hash":"ec4f8a69dbb2d8ebab7ef28e342eae543eaee241","modified":1537587774459},{"_id":"themes/next-reloaded/layout/_partials/page-header.swig","hash":"206cbd6ac9ca6a219a8516f59beae25b3c770199","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_third-party/bookmark.swig","hash":"ed62ea83d3f2c9db2ea57bf23a7d765ed82504c2","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/copy-code.swig","hash":"c62c37474c8de78cf34e54c6219b503ec28d9815","modified":1537587774467},{"_id":"themes/next-reloaded/layout/_third-party/exturl.swig","hash":"53861f78a1fb52e96a43cf6909e1530dcf6cbff8","modified":1537587774467},{"_id":"themes/next-reloaded/layout/_third-party/github-banner.swig","hash":"736cb278fa09d3b4ed6f305b56353941ea918793","modified":1537587774467},{"_id":"themes/next-reloaded/layout/_third-party/needsharebutton.swig","hash":"debba9b7110f635204a15df148194d4c2fd2668b","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/pangu.swig","hash":"ccf0035086e14dcefa24c2907301edf4c37d5448","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/scroll-cookie.swig","hash":"a174d4a0c9188f9c4a2652f49bfa7a60ad0a83e3","modified":1537587774469},{"_id":"themes/next-reloaded/layout/_scripts/boostrap.swig","hash":"27fb1d81151b9b79683e488579df19eee7e654d6","modified":1537587774462},{"_id":"themes/next-reloaded/layout/_scripts/commons.swig","hash":"f544e21883d249d5a341c684c97bd25831008f07","modified":1537587774462},{"_id":"themes/next-reloaded/layout/_scripts/noscript.swig","hash":"f8c7e729ad6e72b4c705a2c5d5041589c2b4cc52","modified":1537587774462},{"_id":"themes/next-reloaded/layout/_scripts/vendors.swig","hash":"f134aeb8d5bee351e5277edb92ac694af314b75f","modified":1537587774463},{"_id":"themes/next-reloaded/scripts/tags/button.js","hash":"510f3711a9c9d3e2a844250647d90e8359e7d130","modified":1537587774472},{"_id":"themes/next-reloaded/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1537587774472},{"_id":"themes/next-reloaded/scripts/tags/exturl.js","hash":"83e48148d2f4f8543f6833effa8a26eb0b60f2f0","modified":1537587774472},{"_id":"themes/next-reloaded/scripts/tags/full-image.js","hash":"ef2c2985a3edb9f69740740c5bc42d4b692c86fd","modified":1537587774472},{"_id":"themes/next-reloaded/scripts/tags/group-pictures.js","hash":"aface11629970a5c4ee38aaa15550d2caee59560","modified":1537587774473},{"_id":"themes/next-reloaded/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1537587774473},{"_id":"themes/next-reloaded/scripts/tags/note.js","hash":"bd3310a5890bded1bda9ba3ad6f98ee44ecb101a","modified":1537587774473},{"_id":"themes/next-reloaded/scripts/tags/label.js","hash":"bb502616bfabe85de5de903074ec6afe627f8413","modified":1537587774473},{"_id":"themes/next-reloaded/scripts/tags/tabs.js","hash":"2d257e26718d4011509fd6f530d2ea37e50e3e66","modified":1537587774473},{"_id":"themes/next-reloaded/source/css/main.styl","hash":"2a62e2a11e9cdcc69e538d856d6f9ce228a07c93","modified":1537587774493},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1537587774494},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1537587774494},{"_id":"themes/next-reloaded/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1537587774494},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1537587774494},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1537587774495},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/me.jpg","hash":"18e72cb16043219176b3cefe8b330f27f0cc502d","modified":1532309318791},{"_id":"themes/next-reloaded/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1537587774496},{"_id":"themes/next-reloaded/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1537587774497},{"_id":"themes/next-reloaded/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1537587774496},{"_id":"themes/next-reloaded/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774463},{"_id":"themes/next-reloaded/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774463},{"_id":"themes/next-reloaded/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774488},{"_id":"themes/next-reloaded/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774492},{"_id":"themes/next-reloaded/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1537587774493},{"_id":"themes/next-reloaded/.git/refs/heads/master","hash":"7bdd38a064aad0ca1621c8222a31faf91187ed57","modified":1537587774436},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1537587774456},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-item.swig","hash":"aab518204d3125e948796a9ba6b56b09cade2d92","modified":1537587774457},{"_id":"themes/next-reloaded/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1537587774459},{"_id":"themes/next-reloaded/layout/_partials/head/head-unique.swig","hash":"0b8349710caf9691741a457aa746add35245048e","modified":1537587774459},{"_id":"themes/next-reloaded/layout/_partials/head/head.swig","hash":"09109a5c5a301e7cc5e3c7aec32b0164739fc7d4","modified":1537587774459},{"_id":"themes/next-reloaded/layout/_partials/header/brand.swig","hash":"19050627bc23b0b2c2e65e7248c0f88468eb8ea5","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_partials/header/menu.swig","hash":"f3ae3168801304af3d80ec3b84264e1d4201cb89","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_partials/header/sub-menu.swig","hash":"3f11ae8e9084f39628cd2006931d39a2069b9dd6","modified":1537587774460},{"_id":"themes/next-reloaded/layout/_partials/search/index.swig","hash":"f6454c452b2e90a8c760321bce7e3dc6119b71fa","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1537587774461},{"_id":"themes/next-reloaded/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1537587774462},{"_id":"themes/next-reloaded/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1537587774464},{"_id":"themes/next-reloaded/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1537587774464},{"_id":"themes/next-reloaded/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1537587774464},{"_id":"themes/next-reloaded/layout/_third-party/analytics/busuanzi-counter.swig","hash":"1ce5fa218a44cf6e620583e9013e0b674dd6a989","modified":1537587774464},{"_id":"themes/next-reloaded/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1537587774464},{"_id":"themes/next-reloaded/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/google-analytics.swig","hash":"cfc932c5db04fef64cc56d3ba0b8ddf3a15a63bd","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/lean-analytics.swig","hash":"bd8382d4f12df207ed52f75f6e5fb06527970167","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1537587774465},{"_id":"themes/next-reloaded/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/comments/disqus.swig","hash":"2440f1e66cb8e39cc2bacfd783fb6fe904a628e9","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/comments/gitment.swig","hash":"292cdd1059b76d3d10486b71c99d9afb3e59ea44","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/comments/index.swig","hash":"34cc66d4dbada2d561ba6f70fd9a75207c5adbd4","modified":1537587774466},{"_id":"themes/next-reloaded/layout/_third-party/comments/livere.swig","hash":"c49a3b10b2f2f64a7ac41fa3d436fd2c8d31cca8","modified":1537587774467},{"_id":"themes/next-reloaded/layout/_third-party/comments/valine.swig","hash":"d05e70a416963023ee1978e4168d58bcfd956ed3","modified":1537587774467},{"_id":"themes/next-reloaded/layout/_third-party/math/index.swig","hash":"30e9e55d9af2ced6e6b156a042026a8b480f0ab0","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/math/katex.swig","hash":"860de4ce6fccc516d2f779a4b600a4214d8c18e2","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/math/mathjax.swig","hash":"c0c2fd87e71f0beb542aa50c0bc875da5aa5c44b","modified":1537587774468},{"_id":"themes/next-reloaded/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1537587774469},{"_id":"themes/next-reloaded/layout/_third-party/search/localsearch.swig","hash":"71c897f9b107dd0de1b7f649633cf583c206a9db","modified":1537587774469},{"_id":"themes/next-reloaded/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1537587774470},{"_id":"themes/next-reloaded/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1537587774470},{"_id":"themes/next-reloaded/layout/_scripts/pages/post-details.swig","hash":"580e9d9c4d8783ee6200d845ae16c98979bf1ea3","modified":1537587774463},{"_id":"themes/next-reloaded/layout/_scripts/schemes/gemini.swig","hash":"c381f638315a007b1baf5fea879161001fe50cd0","modified":1537587774463},{"_id":"themes/next-reloaded/layout/_scripts/schemes/pisces.swig","hash":"c381f638315a007b1baf5fea879161001fe50cd0","modified":1537587774463},{"_id":"themes/next-reloaded/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_mixins/Pisces.styl","hash":"8aa98ae349908736ba43196c42498fd5bdeb780a","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_mixins/base.styl","hash":"0882d76333ab409e8d3362c284b91a0f0ae761ae","modified":1537587774487},{"_id":"themes/next-reloaded/source/css/_variables/Gemini.styl","hash":"8d6cf29f63c90364f4d3c336f7c9bb794b4c46cc","modified":1537587774492},{"_id":"themes/next-reloaded/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1537587774492},{"_id":"themes/next-reloaded/source/css/_variables/Pisces.styl","hash":"72927abd51d3a607a6ba32cf882390792b34e834","modified":1537587774493},{"_id":"themes/next-reloaded/source/css/_variables/base.styl","hash":"1625e3e1da6c453108526e027dae25e311b18adf","modified":1537587774493},{"_id":"themes/next-reloaded/source/js/src/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1537587774497},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1537587774497},{"_id":"themes/next-reloaded/source/js/src/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1537587774497},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","hash":"c7e2a588b679d46379124141bb2f30bc2f3210e2","modified":1537587774497},{"_id":"themes/next-reloaded/source/js/src/motion.js","hash":"4c7f94e499743f4cc958f6cd1260a93d765d3051","modified":1537587774498},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1537587774498},{"_id":"themes/next-reloaded/source/js/src/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1537587774498},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1537587774498},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1537587774499},{"_id":"themes/next-reloaded/source/js/src/utils.js","hash":"fa3aab0ba7bc5138c6f8fac436efe5b8dcb97567","modified":1537587774499},{"_id":"themes/next-reloaded/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1537587774499},{"_id":"themes/next-reloaded/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1537587774499},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1537587774500},{"_id":"themes/next-reloaded/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1537587774500},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1537587774500},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1537587774507},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1537587774508},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1537587774508},{"_id":"themes/next-reloaded/.git/objects/pack/pack-3e52764a8afa019df9f67388c6a7bda8276f576c.idx","hash":"2336e2ab2f148df64e14a0751475a63ee4688833","modified":1537587774410},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1537587774505},{"_id":"themes/next-reloaded/.git/logs/refs/heads/master","hash":"ed4a8308bc7529fece74ead0ad0d6c2042e26dfa","modified":1537587774436},{"_id":"themes/next-reloaded/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1537587774434},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/assets.swig","hash":"6e076b5e183eedf425a445e99851f938789c3194","modified":1537587774469},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1537587774469},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1537587774474},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1537587774474},{"_id":"themes/next-reloaded/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1537587774474},{"_id":"themes/next-reloaded/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1537587774474},{"_id":"themes/next-reloaded/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1537587774474},{"_id":"themes/next-reloaded/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1537587774478},{"_id":"themes/next-reloaded/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/outline/outline.styl","hash":"fbb6be577529c750ef7c872fe7abdc7ab0faf0f8","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/base.styl","hash":"97bb39756e85f5b27bba7f43270105ad01d736c9","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1537587774486},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/mobile.styl","hash":"ab775f23ad475efb25376b5aad752ae8b56cfd8c","modified":1537587774486},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1537587774486},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1537587774486},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/tables.styl","hash":"52bc8ba71b91d954530b35dfc63b402a02b1321d","modified":1537587774486},{"_id":"themes/next-reloaded/source/css/_schemes/Gemini/index.styl","hash":"7e4e499964c2112d47e6f9d2b6e87c31ec8269e2","modified":1537587774488},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1537587774488},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1537587774488},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1537587774488},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_menu.styl","hash":"f3aa863adf972569b72f2df6bc6a914e7daace99","modified":1537587774489},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fa6c00fdaf8f0ca3b690a5a556671745fb67e2c9","modified":1537587774489},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1537587774489},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/index.styl","hash":"2ccb9bdc309b7c1ef183a3dbb0a4621bec54a328","modified":1537587774489},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_menu.styl","hash":"3cc7646583218d16925ced7b70865e63a901d4a7","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/index.styl","hash":"b26f8a3394d8357a5bfd24d9f8bf62d7b4063ebb","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1537587774491},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_layout.styl","hash":"ccf5a4761cb0ce451b5e994cfabf8769248a45c1","modified":1537587774491},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_menu.styl","hash":"cc961108b12ab97d9216606ceb1cd1cd31ab20f0","modified":1537587774491},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sub-menu.styl","hash":"be72740313a9a0477b8a22f62e4c8ffa6d23a2e5","modified":1537587774492},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1537587774491},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/index.styl","hash":"a558803ca81cceae2bdc22c18ef638fcc023681b","modified":1537587774492},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sidebar.styl","hash":"df16dc995eb9ad498df2edcfc3e20528fc9aa133","modified":1537587774492},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","hash":"6b37727883ab6f2a7211300d93289a337944838b","modified":1537587774498},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1537587774500},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1537587774501},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1537587774501},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1537587774505},{"_id":"themes/next-reloaded/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1537587774505},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1537587774504},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1537587774504},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1537587774507},{"_id":"themes/next-reloaded/.git/logs/refs/remotes/origin/HEAD","hash":"ed4a8308bc7529fece74ead0ad0d6c2042e26dfa","modified":1537587774434},{"_id":"themes/next-reloaded/source/css/_common/components/footer/footer.styl","hash":"7dd247c8869fdefb5a007045d00f3ef8ceecf300","modified":1537587774475},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1537587774476},{"_id":"themes/next-reloaded/source/css/_common/components/header/github-banner.styl","hash":"3f3d2a43d1a326bad25b633c8ec9ddd87867224c","modified":1537587774475},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/highlight.styl","hash":"835c1340571bd6c4ec263c482cf13283fb047e49","modified":1537587774476},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/header/header.styl","hash":"34f5ac3c1ed2dd31e9297cc4c0733e71bc2e252f","modified":1537587774475},{"_id":"themes/next-reloaded/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1537587774475},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1537587774476},{"_id":"themes/next-reloaded/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1537587774476},{"_id":"themes/next-reloaded/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1537587774476},{"_id":"themes/next-reloaded/source/css/_common/components/pages/breadcrumb.styl","hash":"630be616447a982413030e561bbd3a80ac14b120","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/pages/pages.styl","hash":"ad4cae23c8e383f4fabc9a2a95bca6055020d22e","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1537587774478},{"_id":"themes/next-reloaded/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1537587774477},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1537587774478},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1537587774478},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1537587774478},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-expand.styl","hash":"317c9ceda655e9dc373ce8e7b71d20b794fce9a4","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reading_progress.styl","hash":"82bc7fa5d38d98e98cc25f9a73189024fda25e63","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-meta.styl","hash":"330c8884efb5612e7eb03986d87d29e8b0651974","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reward.styl","hash":"ff50a32ae6fea5fcdf2939dc9b01e1eb76cc73f0","modified":1537587774479},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-title.styl","hash":"adfd6d2d3b34adc4b476a0ea91e19020456a3b1a","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-widgets.styl","hash":"5c3141d58970a0614896b6a62fd7a8a8caf4e401","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/post/post.styl","hash":"bb8162bb7c7b0b255a0e8e234eb382a0879a4962","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"fa57ec9a6f1943c0558856dfba2d6b8faca0cd4d","modified":1537587774480},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author.styl","hash":"00a504620c454287111dc0ace64c989e1ff97f54","modified":1537587774481},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1537587774481},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1537587774481},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1537587774481},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1537587774481},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"9e05a2232faabb41bcebb51d545d897a76f077da","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar.styl","hash":"94d76e6da600a36d80e2470326ebb6b3be447ccb","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/algolia-search.styl","hash":"f4d8144c22544bdb89787c14ab9d39578dae4b7c","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/third-party.styl","hash":"c31fbaec7e6373ecfb8588500b972d451695a6ad","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/related-posts.styl","hash":"acfdd76b1c90d2e384affb3d0006a39b524609d2","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1537587774485},{"_id":"themes/next-reloaded/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/tags/exturl.styl","hash":"02ee0eb49c256ecb4e71bbc65072f9147418d7d7","modified":1537587774482},{"_id":"themes/next-reloaded/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1537587774483},{"_id":"themes/next-reloaded/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1537587774483},{"_id":"themes/next-reloaded/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1537587774483},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1537587774483},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1537587774483},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1537587774484},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1537587774489},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1537587774490},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1537587774491},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1537587774503},{"_id":"themes/next-reloaded/.git/objects/pack/pack-3e52764a8afa019df9f67388c6a7bda8276f576c.pack","hash":"b0f6ef7281302885e46ecdf8782c3fd8b935ba52","modified":1537587774412},{"_id":"public/images/algolia_logo.svg","hash":"278ffcea4876b37657f2e192bda48c6bc7dd8784","modified":1537594727672},{"_id":"public/images/apple-touch-icon-next.png","hash":"b972160c147e9bec3a0f7432e6e80dfa92581b0b","modified":1537594727672},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1537594727692},{"_id":"public/images/cc-by-nc-nd.svg","hash":"a001671db56914e9e85147ba7df8c35d9806e664","modified":1537594727692},{"_id":"public/images/cc-by-nc.svg","hash":"0d183dfe21e7b8f037c1572821386e309f147c50","modified":1537594727692},{"_id":"public/images/cc-by-nc-sa.svg","hash":"716c8de2e05cf9bf7c6650856104a8fe5d2d5443","modified":1537594727692},{"_id":"public/images/cc-by-nd.svg","hash":"3c57ccdb3762c8d6eb2669a65bbbc18c7bbd46a7","modified":1537594727692},{"_id":"public/images/cc-by.svg","hash":"c72feca504a517f6f5aaade0205a0c1b71f16c5a","modified":1537594727692},{"_id":"public/images/cc-by-sa.svg","hash":"acb7a5ffef273b46e9353f869608ed13eb2825fe","modified":1537594727693},{"_id":"public/images/cc-zero.svg","hash":"ae0a67df89e2c066f6ac836174224373af114f09","modified":1537594727693},{"_id":"public/images/favicon-16x16-next.png","hash":"5ee510e58b7b9e062a22da28ce1eb35a2f381021","modified":1537594727693},{"_id":"public/images/favicon-32x32-next.png","hash":"f88e49404e4c2a326e51ae65ea5b2375b5d5fde8","modified":1537594727693},{"_id":"public/images/logo.svg","hash":"d02fedf124aa26d4def2631bbccafa053653abd6","modified":1537594727693},{"_id":"public/images/me.jpg","hash":"f7201f215b2b655c4f4c03a4b4bf8abd2d96e319","modified":1537594727693},{"_id":"public/images/quote-l.svg","hash":"48136591a60f3dc722d8f00b66e16de2aec3802f","modified":1537594727693},{"_id":"public/images/searchicon.png","hash":"00eed611c73d736ddea861a92d8c91d44aefef5c","modified":1537594727693},{"_id":"public/images/quote-r.svg","hash":"dc3f40e3409dd502478a8c6bb41c68d50bd71ca6","modified":1537594727693},{"_id":"public/baidu_verify_l3vacuMD8f.html","hash":"4a21538b131c0df7a108e0c54bc3a26ada26e6ea","modified":1537594727693},{"_id":"public/baidusitemap.xml","hash":"1bcf9c3a29013ebe9b396bb2a39079e302878c0a","modified":1537594727694},{"_id":"public/search.xml","hash":"a8dedbb687a4f45f9489a096a983d7c9884a6ba3","modified":1537594727694},{"_id":"public/sitemap.xml","hash":"4be5199745a397346e611525150d6396b9ac2424","modified":1537594727694},{"_id":"public/about/index.html","hash":"7f52a8ee3aa5c377c22c7961e190e699d18513fd","modified":1537594727743},{"_id":"public/categories/index.html","hash":"30af7069d990a03bb4df3018257cb2bc939adea0","modified":1537594727743},{"_id":"public/tags/index.html","hash":"a06a2b3a8c0ef6ed240b597871433bcf7803c049","modified":1537594727743},{"_id":"public/2018/09/22/deepbayes2018/index.html","hash":"fb292a1710ae34978fc37b8463316288a4596fcf","modified":1537594727743},{"_id":"public/2018/09/14/dpp/index.html","hash":"b675c6d58a2ee8b0ec8bb36f4c4fd46e08a3fb8b","modified":1537594727743},{"_id":"public/2018/09/03/hlda/index.html","hash":"2250af8d05309e6ad2a2021f062f06e1499182a2","modified":1537594727743},{"_id":"public/2018/08/28/inference-algorithm/index.html","hash":"9cca3d4c6abd41ba3e644249889eb8ac4e151842","modified":1537594727743},{"_id":"public/2018/08/09/statistical-handwriting/index.html","hash":"d9068722486f3f0bf499249bb03ed5e424f9c40c","modified":1537594727743},{"_id":"public/2018/08/04/convex-optimization/index.html","hash":"cf53069571ac1fe2c2c961a3f8ae34110ca9ff50","modified":1537594727743},{"_id":"public/2018/07/23/lda/index.html","hash":"097baa20d444a1da9e17717ad7c51c91f35a9100","modified":1537594727743},{"_id":"public/2018/07/04/seq2seq-summarization/index.html","hash":"b492ac794846c769eee2606bfe9d21260af5ab70","modified":1537594727743},{"_id":"public/2018/07/03/PaperReading2/index.html","hash":"9e238786b1d59022b145d3fa60ba54f23382a228","modified":1537594727743},{"_id":"public/2018/07/03/cvxopt/index.html","hash":"4f70fb2e7fa38c8c71179cec060fc17633c6e073","modified":1537594727743},{"_id":"public/2018/03/07/PaperReading/index.html","hash":"e1087b17c302b3d13bff338cda3f6b3dcf8d6acd","modified":1537594727743},{"_id":"public/2018/03/07/NLPBasic/index.html","hash":"5eb7931defd062b3fce2d2493947c54469d1f311","modified":1537594727743},{"_id":"public/2018/01/10/tensorflowtutorial/index.html","hash":"8653521d85cd5e859c24193a627659d5641bf859","modified":1537594727743},{"_id":"public/2017/12/27/AM-Model-for-ASS/index.html","hash":"5587638937a2bf0c9de26d30532d6773f9d27d2e","modified":1537594727743},{"_id":"public/2017/05/26/dachuangserver/index.html","hash":"c8c085367b08133210745a42ba117489a830fb4c","modified":1537594727744},{"_id":"public/2017/05/02/trie/index.html","hash":"fd0e9e3603b0621d5ffcd75b33b98f021797e2e6","modified":1537594727744},{"_id":"public/2017/03/28/PythonNotes/index.html","hash":"b5782dd250a649bbe8aec7c89fdcab4115de7e74","modified":1537594727744},{"_id":"public/2017/03/27/oj/index.html","hash":"b3817e4b604b5d388ef6e534fa02fe37a9464e90","modified":1537594727744},{"_id":"public/2017/03/18/Lagrange/index.html","hash":"d55b6537d147c6ae524ed50c764274bbfb954c99","modified":1537594727744},{"_id":"public/2017/03/16/kmeans/index.html","hash":"34d290239aff833bc8da0e6191b6051ca098d902","modified":1537594727744},{"_id":"public/2017/03/16/mnist/index.html","hash":"f3c0f2a70b80473117c957fb508a92bb7548455b","modified":1537594727744},{"_id":"public/2017/03/09/dachuang/index.html","hash":"075d909dbe10f49789b4b2c73ab65e6c015503f7","modified":1537594727744},{"_id":"public/2017/02/12/MachineLearningNote/index.html","hash":"07ffee42fdf8aac2ae94db6a9eb45f8d6b814e02","modified":1537594727744},{"_id":"public/2017/02/10/NeuralNetworks1/index.html","hash":"39ebd94833ea687d070c3f994f247bf734932089","modified":1537594727744},{"_id":"public/2017/02/07/TitanicLinearRegression/index.html","hash":"a9b1501f178236d20a84ea05342593e2e406cfe2","modified":1537594727744},{"_id":"public/2017/02/04/pandas-func/index.html","hash":"c99c5b674b38459cee39e6a51ed59bea4bfa7d69","modified":1537594727744},{"_id":"public/2017/02/04/pandas-skill/index.html","hash":"6081cc3091fc53324bd3c9484d5a437ada73ba97","modified":1537594727745},{"_id":"public/2017/01/23/numpycookbook/index.html","hash":"12e5d3354cbc0d8a9de2ff8d65479eb83017c7b2","modified":1537594727745},{"_id":"public/2017/01/22/LinearAlgebra3/index.html","hash":"90a04159a70363faeef0512296b02fd72afb2151","modified":1537594727745},{"_id":"public/2017/01/21/LinearAlgebra2/index.html","hash":"7fa61d0d6572e9be8e2897b5d978da80cd837a8c","modified":1537594727745},{"_id":"public/2017/01/21/LinearAlgebra1/index.html","hash":"49af085db30527f2829b0fba594c12bce20a56a1","modified":1537594727745},{"_id":"public/2017/01/16/setupmywebsite/index.html","hash":"8597515eeb83eda182ce9e6351a555c340d6cf6b","modified":1537594727745},{"_id":"public/2017/01/16/buptroomreview/index.html","hash":"719ffd50d13614211d0ef231184f763a8bdbea8e","modified":1537594727745},{"_id":"public/archives/index.html","hash":"8db07795a9fc9c951dde90ef5071ae14db3d96e6","modified":1537594727745},{"_id":"public/archives/page/2/index.html","hash":"17c881da870afc2e31d7557545772edacea4caa9","modified":1537594727745},{"_id":"public/archives/page/3/index.html","hash":"3ed59033a0d61d4b61487bd640509ccf566dffc1","modified":1537594727745},{"_id":"public/archives/page/4/index.html","hash":"5a0fa61797e4d9f2422b7fdf37083d975ce0e8d5","modified":1537594727745},{"_id":"public/archives/2017/index.html","hash":"95e028cce227edd3f33d24758e57660921c63f7f","modified":1537594727745},{"_id":"public/archives/2017/page/2/index.html","hash":"dfda30d38c4fbad0176b89dd8afe56833570674b","modified":1537594727745},{"_id":"public/archives/2017/01/index.html","hash":"8bf47d215e47b93194fbf45afad0ebfbdace67ef","modified":1537594727745},{"_id":"public/archives/2017/02/index.html","hash":"4f7fcdd372e5f39fb49605dbbd3217d54da47db5","modified":1537594727745},{"_id":"public/archives/2017/03/index.html","hash":"ffebe2806131f091549f03a806538d0e722560bc","modified":1537594727745},{"_id":"public/archives/2017/05/index.html","hash":"0a16c28cb6adb5a67822e157e9cac9b747ae1498","modified":1537594727745},{"_id":"public/archives/2017/12/index.html","hash":"9fe32965731a112e1158768dd25d59996f988d94","modified":1537594727746},{"_id":"public/archives/2018/index.html","hash":"ab96b81d4f93d79c5e15d326dcf31fd4406469a8","modified":1537594727746},{"_id":"public/archives/2018/page/2/index.html","hash":"2fc8efc58fb52fa8d562f503b522e0d7f9da318b","modified":1537594727746},{"_id":"public/archives/2018/01/index.html","hash":"1d5d4891befe7a6a08f5a0a9311ce671b82044f9","modified":1537594727746},{"_id":"public/archives/2018/03/index.html","hash":"74115403f9f2bce907d1c185cd5d13df14071f31","modified":1537594727746},{"_id":"public/archives/2018/07/index.html","hash":"22d10e0218e1656e678a29f9d5e5214d38306eab","modified":1537594727746},{"_id":"public/archives/2018/08/index.html","hash":"a9d6580fe3620dc19ccec784298285588cd6b71e","modified":1537594727746},{"_id":"public/archives/2018/09/index.html","hash":"20343a6899cace1c621fdcc3a06ebdbb20b0f4c1","modified":1537594727746},{"_id":"public/index.html","hash":"6f23ce9163355ba6d594702da24142432f7e6fea","modified":1537594727746},{"_id":"public/page/2/index.html","hash":"954a46bfea68fa739748b96ecb56391912e6578c","modified":1537594727746},{"_id":"public/page/3/index.html","hash":"8b7ef05056e976032cac65ee6fa7e3a6c6ffc89a","modified":1537594727746},{"_id":"public/page/4/index.html","hash":"661744718aee1bbfc8a09861fdbf9fb7de57b6fb","modified":1537594727746},{"_id":"public/categories/自然语言处理/index.html","hash":"549c4994abc5a42a004e2401ba4b8d7b141b87f5","modified":1537594727746},{"_id":"public/categories/数学/index.html","hash":"35cc95bedd77d03d590141eb52be327ba229c8a2","modified":1537594727746},{"_id":"public/categories/机器学习/index.html","hash":"4a597069309acd420d9aaac54f2973ce3aa890a7","modified":1537594727746},{"_id":"public/categories/机器学习/page/2/index.html","hash":"4abcc30b37ac69b782cca4ac79826aa91cbc8f25","modified":1537594727746},{"_id":"public/categories/Python/index.html","hash":"6f02c3440600aee4cacb0cb9eeb9d307ceddfb79","modified":1537594727746},{"_id":"public/categories/Android/index.html","hash":"cfde804035f377d89f82bd0f0add0d351c4f2ee3","modified":1537594727746},{"_id":"public/categories/算法/index.html","hash":"764fb22da77ed5565a33ddb71f2c8a26db77e6c2","modified":1537594727747},{"_id":"public/categories/瞎折腾/index.html","hash":"64fba2c62cee43789ef29aa23f6288c978f6e577","modified":1537594727747},{"_id":"public/tags/abstractive-summarization/index.html","hash":"034c8359351077c11f757f3d6d41c5c5e9b271b2","modified":1537594727747},{"_id":"public/tags/math/index.html","hash":"1b9bc1584f1c372716ccdb2c75e2089f776375aa","modified":1537594727747},{"_id":"public/tags/math/page/2/index.html","hash":"7801047ec655c41d2290d8fef2bd3c4bc578e2f5","modified":1537594727747},{"_id":"public/tags/machinelearning/index.html","hash":"efcfd000635a53878ebb3ecdb8782c7f8748f31c","modified":1537594727747},{"_id":"public/tags/machinelearning/page/2/index.html","hash":"4ac3d04fcfca07dfc9bcb0e639c080d034b8943a","modified":1537594727747},{"_id":"public/tags/theory/index.html","hash":"b3192af136541d92b6bbdbea9beb0ad7b9fc3577","modified":1537594727747},{"_id":"public/tags/nlp/index.html","hash":"289fdccbff8e7f0f84db3006ab18419a9c396f86","modified":1537594727747},{"_id":"public/tags/linearalgebra/index.html","hash":"ed2cb453a5bdaa1c1fc0c5d0bd17c211a3e17a07","modified":1537594727747},{"_id":"public/tags/code/index.html","hash":"549b2800c5899636c3e55c506d671ae4bb0baab7","modified":1537594727747},{"_id":"public/tags/code/page/2/index.html","hash":"dbb5873bf1943ee0dfe8ee3b6f3b5344cf34073b","modified":1537594727747},{"_id":"public/tags/machine-learning/index.html","hash":"3a03918bd45a1384a13e5d270d10b673ad3ee84c","modified":1537594727747},{"_id":"public/tags/python/index.html","hash":"1912986c62eb76452c99471f69b33a1457801d60","modified":1537594727747},{"_id":"public/tags/convex-optimization/index.html","hash":"9230cd05c03877cc51e32a4a65a7d4ae684dd9f0","modified":1537594727747},{"_id":"public/tags/android/index.html","hash":"91200739514f4ae904b3bf4e96fc58549e5becc2","modified":1537594727747},{"_id":"public/tags/server/index.html","hash":"2faa8d5e79362d80ccafd16ea39b8bd87497926a","modified":1537594727747},{"_id":"public/tags/linux/index.html","hash":"6dd04ea735cb644fdd254614e03bfc9d0de88261","modified":1537594727747},{"_id":"public/tags/dpp/index.html","hash":"e59772ff8ac040635180119522a7883a6d85fe78","modified":1537594727748},{"_id":"public/tags/lda/index.html","hash":"cca4afbb584a54ab7dae760cc0be78e5cd5558cc","modified":1537594727748},{"_id":"public/tags/mcmc/index.html","hash":"6bdb9c1e5630e345c8bc046fad36138e095495b7","modified":1537594727748},{"_id":"public/tags/inference/index.html","hash":"ba67118898089f2aade756baa5057c0e1c7f718d","modified":1537594727748},{"_id":"public/tags/variational-inference/index.html","hash":"e985659eae25530e8428a2685d45173ab2cb5943","modified":1537594727748},{"_id":"public/tags/em/index.html","hash":"0288cfcbcddf03a63d6a70077e01a7b0965832f0","modified":1537594727748},{"_id":"public/tags/c/index.html","hash":"53c724f81d638fc296b177d261738614ea890745","modified":1537594727748},{"_id":"public/tags/algorithm/index.html","hash":"a02b73dd941db12f6d17c4d2087811ada1f8dfdf","modified":1537594727748},{"_id":"public/tags/web/index.html","hash":"7415598bb43e63ebd4f4b3401234fb68c14d5931","modified":1537594727748},{"_id":"public/tags/hexo/index.html","hash":"fb12d947179bc981be86b79dc0b6dff5dc33eacd","modified":1537594727748},{"_id":"public/tags/github/index.html","hash":"f90fdca281d1acbb98bd64a27e839165a5e73032","modified":1537594727748},{"_id":"public/tags/statistical-learning/index.html","hash":"afef07951b5f51ce24969bc99c76358bf0b39db4","modified":1537594727748},{"_id":"public/tags/seq2seq/index.html","hash":"d38c2076c2f048d0af4a6e362ebe3f610502ba10","modified":1537594727748},{"_id":"public/tags/rnn/index.html","hash":"7d91d2785126bba0763157c3cbefc0ae70d401ad","modified":1537594727748},{"_id":"public/tags/lstm/index.html","hash":"cc6b36726fc7956ff61dfd9d510931e58fdd0f0d","modified":1537594727748},{"_id":"public/tags/gru/index.html","hash":"c160f548124f38b7ea14a0e27711a43d84a9eec9","modified":1537594727748},{"_id":"public/tags/tensorflow/index.html","hash":"8eb81654e9aecf503fbf2f2a32f2dd7e42314738","modified":1537594727748},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1537594727754},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1537594727754},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1537594727754},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1537594727754},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1537594728116},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1537594729857},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1537594729865},{"_id":"public/js/src/affix.js","hash":"07e8c8994973dcdde6dec52dbfb7102a1d442372","modified":1537594729865},{"_id":"public/js/src/exturl.js","hash":"5422e0ba39e779b323bde675821041068b0c034f","modified":1537594729865},{"_id":"public/js/src/algolia-search.js","hash":"3bbff78516bd1c1fdda02ee407df8215cbb8bc6d","modified":1537594729865},{"_id":"public/js/src/scroll-cookie.js","hash":"c77ab4b14b0b7b3e84f10f9c5a31b22391927c21","modified":1537594729865},{"_id":"public/js/src/post-details.js","hash":"7432395631d22345d7e3f59a5fd8b5e14a792106","modified":1537594729865},{"_id":"public/js/src/motion.js","hash":"dca8159525184bc6e7a27a073d6901bb4f71695c","modified":1537594729865},{"_id":"public/js/src/js.cookie.js","hash":"1bb986eb7ea7e85e908043d70a5cade6fb88233c","modified":1537594729865},{"_id":"public/js/src/scrollspy.js","hash":"ca78e894b3aca23bab5603161ae9a23e8687d07a","modified":1537594729865},{"_id":"public/js/src/bootstrap.js","hash":"b2c3f777b2829acb799268839061b6a7f11f5cab","modified":1537594729865},{"_id":"public/js/src/utils.js","hash":"bb5c99e2cc13d30a8ab20ee4410998af53b1ca8d","modified":1537594729865},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1537594729865},{"_id":"public/lib/velocity/velocity.ui.js","hash":"e91a994322cec2fbbb1238d1e064fb690800fc21","modified":1537594729865},{"_id":"public/js/src/schemes/pisces.js","hash":"4b251892042405d87a162f208823b187519e723d","modified":1537594729865},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1537594729865},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"ce719d36e1c3fd44fb0d57f1f4fc8b4e13ac1f46","modified":1537594729865},{"_id":"public/css/main.css","hash":"cd13712a9512e3ea44bfe6c4aae19d5045c3e188","modified":1537594729865},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1537594729865},{"_id":"public/lib/jquery/index.js","hash":"14f3524199c230b0d3651e721b8aee2d3ced94a7","modified":1537594729865},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1537594729865},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"f531d8edfb5d3178a7281d5d30d398fb3712d8f9","modified":1537594729865},{"_id":"public/lib/velocity/velocity.js","hash":"e9f16901028fef3ca8aa1c0f8d433319ce33b46c","modified":1537594729865},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1537594729865}],"Category":[{"name":"自然语言处理","_id":"cjmd072cc0005qcw6zn9stpak"},{"name":"数学","_id":"cjmd072ci000bqcw61krk319l"},{"name":"机器学习","_id":"cjmd072ck000gqcw67f4pnp2m"},{"name":"Python","_id":"cjmd072cz001bqcw6g8q5kys5"},{"name":"Android","_id":"cjmd072d2001jqcw62y9n0ryl"},{"name":"算法","_id":"cjmd072df002hqcw64nmrxvpy"},{"name":"瞎折腾","_id":"cjmd072dj002qqcw6o531jf93"}],"Data":[],"Page":[{"layout":"false","_content":"l3vacuMD8f","source":"baidu_verify_l3vacuMD8f.html","raw":"---\nlayout: false\n---\nl3vacuMD8f","date":"2018-09-13T02:00:19.833Z","updated":"2018-08-02T04:39:08.099Z","path":"baidu_verify_l3vacuMD8f.html","title":"","comments":1,"_id":"cjmd072650000qcw6ypdt3zmn","content":"l3vacuMD8f","site":{"data":{}},"excerpt":"","more":"l3vacuMD8f"},{"title":"About Thinkwee","html":true,"_content":"***\n\n<center>\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170501/115029968.jpg)\n</center>\n\n<center> <font size=6 >Thinkwee's Blog</font ></center>\n<center> <font size=5 >Liu Wei/Richard Liu</font ></center>\n<center> <font size=4 >\tBeijing University of Posts and Telecommunications</font ></center>\n<center> <font size=3 >\tIntelligent Science and Technology Center</font ></center>\n<center> <font size=3 >\tCommunication Engineering|Undergraduate</font ></center>\n<center> <font size=3 >\tComputer Science | Postgraduate</font ></center>\n<center> <font size=3 >\tNLP(Automatic Summarization)</font ></center>\n<center> <font size=2 >\tC++(just write algorithm)</font ></center>\n<center> <font size=2 >\tPython(just for machine learning)</font ></center>\n<center> <font size=2 >\tJava(learn it by coding android app)</font ></center>\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/215217834.gif)\n\n\n","source":"about/index.md","raw":"title: About Thinkwee\nhtml: true\n---\n***\n\n<center>\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170501/115029968.jpg)\n</center>\n\n<center> <font size=6 >Thinkwee's Blog</font ></center>\n<center> <font size=5 >Liu Wei/Richard Liu</font ></center>\n<center> <font size=4 >\tBeijing University of Posts and Telecommunications</font ></center>\n<center> <font size=3 >\tIntelligent Science and Technology Center</font ></center>\n<center> <font size=3 >\tCommunication Engineering|Undergraduate</font ></center>\n<center> <font size=3 >\tComputer Science | Postgraduate</font ></center>\n<center> <font size=3 >\tNLP(Automatic Summarization)</font ></center>\n<center> <font size=2 >\tC++(just write algorithm)</font ></center>\n<center> <font size=2 >\tPython(just for machine learning)</font ></center>\n<center> <font size=2 >\tJava(learn it by coding android app)</font ></center>\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/215217834.gif)\n\n\n","date":"2018-09-15T13:47:19.097Z","updated":"2018-09-15T13:47:19.097Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjmd072c80002qcw60uu0asjn","content":"<hr><center><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170501/115029968.jpg\" alt=\"mark\"><br></center><center><font size=\"6\">Thinkwee’s Blog</font></center><br><center><font size=\"5\">Liu Wei/Richard Liu</font></center><br><center><font size=\"4\">Beijing University of Posts and Telecommunications</font></center><br><center><font size=\"3\">Intelligent Science and Technology Center</font></center><br><center><font size=\"3\">Communication Engineering|Undergraduate</font></center><br><center><font size=\"3\">Computer Science | Postgraduate</font></center><br><center><font size=\"3\">NLP(Automatic Summarization)</font></center><br><center><font size=\"2\">C++(just write algorithm)</font></center><br><center><font size=\"2\">Python(just for machine learning)</font></center><br><center><font size=\"2\">Java(learn it by coding android app)</font></center><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/215217834.gif\" alt=\"mark\"></p>","site":{"data":{}},"excerpt":"","more":"<hr><center><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170501/115029968.jpg\" alt=\"mark\"><br></center><center><font size=\"6\">Thinkwee’s Blog</font></center><br><center><font size=\"5\">Liu Wei/Richard Liu</font></center><br><center><font size=\"4\">Beijing University of Posts and Telecommunications</font></center><br><center><font size=\"3\">Intelligent Science and Technology Center</font></center><br><center><font size=\"3\">Communication Engineering|Undergraduate</font></center><br><center><font size=\"3\">Computer Science | Postgraduate</font></center><br><center><font size=\"3\">NLP(Automatic Summarization)</font></center><br><center><font size=\"2\">C++(just write algorithm)</font></center><br><center><font size=\"2\">Python(just for machine learning)</font></center><br><center><font size=\"2\">Java(learn it by coding android app)</font></center><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/215217834.gif\" alt=\"mark\"></p>"},{"title":"categories","type":"categories","_content":"","source":"categories/index.md","raw":"title: categories\ntype: \"categories\"\n---\n","date":"2018-09-13T02:00:10.597Z","updated":"2018-07-23T01:28:38.518Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjmd072cb0004qcw6seft2gpk","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","type":"tags","_content":"","source":"tags/index.md","raw":"title: 标签\ntype: \"tags\"\n---","date":"2018-09-13T02:00:10.607Z","updated":"2018-07-23T01:28:38.518Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjmd072cf0008qcw6cmfeu0ip","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"论文翻译：用于生成式自动文摘的一种神经注意力模型","date":"2017-12-27T11:20:43.000Z","author":"Thinkwee","mathjax":true,"_content":"\n论文翻译：\n-\tA Neural Attention Model for Abstractive Sentence Summarization\n\n作者：\n-\tAlexander M. Rush(Facebook AI Research / Harvard SEAS)\n-\tSumit Chopra(Facebook AI Research)\n-\tJason Weston(Facebook AI Research)\n<!--more-->\n\n# 原文地址\n-\t[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)\n\n# 翻译\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ce57b735eC.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AJ01DbFcGa.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AdiCCclB88.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/9lDk2jjmFB.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/fHk47h6CdD.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CgEDiFd913.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/KGD08gJ22E.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/FkaFDDH8Db.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekAkH14d34.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/I4KLcIkGmL.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ge10L0fLig.JPG)","source":"_posts/AM-Model-for-ASS.md","raw":"---\ntitle: 论文翻译：用于生成式自动文摘的一种神经注意力模型\ndate: 2017-12-27 19:20:43\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  - nlp\ncategories:\n  - 自然语言处理\nauthor: Thinkwee\nmathjax: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180307/G6AJc4Fe8L.png?imageslim\n---\n\n论文翻译：\n-\tA Neural Attention Model for Abstractive Sentence Summarization\n\n作者：\n-\tAlexander M. Rush(Facebook AI Research / Harvard SEAS)\n-\tSumit Chopra(Facebook AI Research)\n-\tJason Weston(Facebook AI Research)\n<!--more-->\n\n# 原文地址\n-\t[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)\n\n# 翻译\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ce57b735eC.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AJ01DbFcGa.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AdiCCclB88.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/9lDk2jjmFB.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/fHk47h6CdD.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CgEDiFd913.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/KGD08gJ22E.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/FkaFDDH8Db.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekAkH14d34.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/I4KLcIkGmL.JPG)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ge10L0fLig.JPG)","slug":"AM-Model-for-ASS","published":1,"updated":"2018-07-23T01:28:38.487Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180307/G6AJc4Fe8L.png?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072c50001qcw6gskbd9a6","content":"<p>论文翻译：</p><ul><li>A Neural Attention Model for Abstractive Sentence Summarization</li></ul><p>作者：</p><ul><li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li><li>Sumit Chopra(Facebook AI Research)</li><li>Jason Weston(Facebook AI Research)<a id=\"more\"></a></li></ul><h1 id=\"原文地址\"><a href=\"#原文地址\" class=\"headerlink\" title=\"原文地址\"></a>原文地址</h1><ul><li><a href=\"https://arxiv.org/pdf/1509.00685.pdf\" target=\"_blank\" rel=\"noopener\">A Neural Attention Model for Abstractive Sentence Summarization</a></li></ul><h1 id=\"翻译\"><a href=\"#翻译\" class=\"headerlink\" title=\"翻译\"></a>翻译</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ce57b735eC.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AJ01DbFcGa.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AdiCCclB88.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/9lDk2jjmFB.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/fHk47h6CdD.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CgEDiFd913.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/KGD08gJ22E.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/FkaFDDH8Db.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekAkH14d34.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/I4KLcIkGmL.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ge10L0fLig.JPG\" alt=\"mark\"></p>","site":{"data":{}},"excerpt":"<p>论文翻译：</p><ul><li>A Neural Attention Model for Abstractive Sentence Summarization</li></ul><p>作者：</p><ul><li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li><li>Sumit Chopra(Facebook AI Research)</li><li>Jason Weston(Facebook AI Research)","more":"</li></ul><h1 id=\"原文地址\"><a href=\"#原文地址\" class=\"headerlink\" title=\"原文地址\"></a>原文地址</h1><ul><li><a href=\"https://arxiv.org/pdf/1509.00685.pdf\" target=\"_blank\" rel=\"noopener\">A Neural Attention Model for Abstractive Sentence Summarization</a></li></ul><h1 id=\"翻译\"><a href=\"#翻译\" class=\"headerlink\" title=\"翻译\"></a>翻译</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ce57b735eC.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AJ01DbFcGa.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/AdiCCclB88.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/9lDk2jjmFB.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/fHk47h6CdD.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CgEDiFd913.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/KGD08gJ22E.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/FkaFDDH8Db.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekAkH14d34.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/I4KLcIkGmL.JPG\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Ge10L0fLig.JPG\" alt=\"mark\"></p>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"论文翻译：用于生成式自动文摘的一种神经注意力模型","path":"2017/12/27/AM-Model-for-ASS/","eyeCatchImage":null,"excerpt":"<p>论文翻译：</p><ul><li>A Neural Attention Model for Abstractive Sentence Summarization</li></ul><p>作者：</p><ul><li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li><li>Sumit Chopra(Facebook AI Research)</li><li>Jason Weston(Facebook AI Research)","date":"2017-12-27T11:20:43.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记1","date":"2017-01-21T03:45:28.000Z","mathjax":true,"html":true,"_content":"\n***\n# <font size=5 >第一讲：方程组的几何解释\n\n-\t从3个角度看待方程组：行图形，列图像，矩阵\n\n-\t例如对方程组：\n\n$$\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}\n$$\n\n<!--more-->\n\n## 行图像\n\n-\t行图像为：\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t也可以写成\n\n$$\nAx=b\n$$\n\n-\t即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png)\n\n## 列图像\n-\t列图像为：\n$$\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png)\n\n## 矩阵\n\n-\t现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？\n-\t如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆\n\n# <font size=5 >第二讲：消元、回代和置换 \n\n## 消元\n\n-\t考虑方程组\n$$\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}\n$$\n-\t他的A矩阵为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t经过行变换后为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再变换为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}\n$$\n-\t这样一系列变换即消元\n-\t变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）\n-\t矩阵\n$$\n\\left[\\begin{array}{c|c}\n A & X \\\\\n\\end{array}\\right]\n$$\n-\t称为增广矩阵(Augmented matrix)。b做同样变换可以得到c\n\n## 回代\n-\t解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例\n-\t因为U为上三角矩阵，z很容易求得\n-\t将z代入第二行求得y\n-\t将z,y代入第一行求得x\n-\t这个过程即回代\n\n## 置换\n\n$$\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A\n$$\n-\t这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3\n\n同理\n$$\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}\n$$\n-\t这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3\n-\t可以推出，交换A两行的矩阵为\n\n$$\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A\n$$\n-\t交换A两列的矩阵为\n\n$$\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵\n-\t在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$\n-\t消元可写成\n$$\nE_{32}E_{31}E_{21}A=U\n$$\n\n# <font size=5 >第三讲：乘法和逆矩阵\n\n## 矩阵乘法\n\n-\t考虑矩阵乘法\n$$\nA*B=C\n$$\n-\t第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$\n-\t第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列\n- \t第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行\n-\t第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C\n-\t第五种算法：矩阵分块算\n\n## 逆矩阵\n-\t对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵\n-\t对方阵，左逆矩阵与右逆矩阵相同\n-\t若存在非零矩阵X,使得$AX=0$,则A不可逆\n-\t求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵\n -\t证明：\n$$\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\\n$$\n\n# <font size=5 >第四讲：A的LU分解\n\n## LU分解\n-\t$(AB)^{-1}=B^{-1}A^{-1}$\n-\t对A的转置矩阵$A^T$,易得\n$$\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\\n$$\n-\t对单个矩阵而言，转置和求逆可以互换\n-\t矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例\n$$\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\\n$$\n-\t为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数\n\n## 消元消耗\n-\t记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为\n$$\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3\n$$\n\n## 群\n-\t以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)\n-\t对这些矩阵，$P^{-1}=P^T$\n-\t这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群\n-\tn*n矩阵共有n!个行置换矩阵\n\n# 第五讲：转置、置换、向量空间R\n\n## 置换\n-\t置换矩阵是用来完成行交换的矩阵\n-\tA=LU,L对角线上都是1，下方为消元乘数，U下三角为0\n-\tPA=LU用于描述包含行交换的lu分解\n-\tP(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价\n\n## 转置\n-\t行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$\n-\t$(AB)^T=B^TA^T$\n-\t对称矩阵(symmetric),$A^T=A$\n-\t对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$\n\n\n## 向量空间\n-\t向量可以相加减，点乘\n-\t**空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件**\n-\t例如$R^2$，代表所有实数的二维向量空间\n-\t向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)\n-\t不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的\n-\t在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)\n-\t$R^2$的子空间都有哪些？\n -\t$R^2$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t(0,0),简写为Z\n-\t$R^3$的子空间都有哪些？\n -\t$R^3$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t过零点的无限大平面\n -\t(0,0,0)\n \n\n\n## 通过矩阵构造向量子空间\n$$\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png)\n\n# <font size=5 >第六讲：列空间和零空间 \n\n## 列空间\n-\t上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间\n-\t对任意子空间S、T,$S \\bigcap T$是一个子空间\n\n-\t举个栗子\n$$\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n$$\n-\tC(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间\n-\t下面将子空间与线性方程组联系起来\n-\t现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？\n\t-\t前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，**即列空间无法填充整个四维空间**\n\t-\t后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，**等同于只有b在A的列空间内，x有解**\n-\t如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间\n\n## 零空间\n-\t零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x\n-\t列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n$$\n-\t显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间\n-\t为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：......矩阵乘法可以展开......分配率......\n\n\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}\n$$\n-\t我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？\n-\t显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0\n-\t列空间和零空间是两种构造子空间的方法\n\t-\t从几个向量通过线性组合来得到子空间\n\t-\t从一个方程组，通过让x满足特定条件来得到子空间\n\n# <font size=5 >第七讲：主变量、特解\n\n## 主变量\n-\t如何用算法解Ax=0\n-\t举个栗子:\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}\n$$\n-\t第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来\n-\t消元不改变方程的组，因为消元改动列空间,不改动解空间\n-\t第一次消元之后,第一列只有第一行的主元不为零\n\n\n\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}\n$$\n-\t此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U\n$$\n-\t如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)\n-\t现在我们可以解Ux=0,并进行回代\n-\t自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的**主变量**x1,x3可以通过回代解出\n\n## 特解\n-\t在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)\n-\t所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。\n-\t两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为**特解**，根据特解我们可以得到解空间：两组特解的线性组合,a\\*(-2,1,0,0)+b\\*(2,0,-2,1)\n-\t 秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量\n\n## 简化行阶梯形式\n-\tU还能进一步简化 \n$$\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t而且需将主元化为1,因为b=0,所以第二行可以直接除以2\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R\n$$\n-\t简化行阶梯形式以最简形式包含了矩阵的所有信息\n-\t单位矩阵位于主行与主列交汇处\n-\t最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列\n$$\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n$$\n其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列\n\n## 零空间矩阵\n\n-\t零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解\n$$\nR*N=0\n$$\n$$\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}\n$$\n-\t整个方程可以写成\n$$\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0\n$$\n$$\nx_{pivot}=-F\n$$\n\n## 最后举个栗子过一遍算法\n-\t原矩阵\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}\n$$\n-\t第一遍消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}\n$$\n-\t第二遍消元(进行一次行交换使得第二个主元在第二行)\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U\n$$\n-\t显然r=2,1个自由变量,令自由变量为1，得到特解x\n$$\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n-\t零空间就是cx,一条直线，这个x为零空间的基\n-\t接下来继续化简U\n$$\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}\n$$\n\n$$\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U\n$$\n\n$$\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N\n$$\n\n# <font size=5 >第八讲：可解性与解的结构\n\n## 可解性\n$$\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}\n$$\n-\t写成增广矩阵形式：\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]\n$$\n-\t消元得到:\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]\n$$\n-\t第一列和第三列为主列，第二列和第四列是自由列\n-\t可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里\n-\t**如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零**\n-\t如何求Ax=b的所有解？\n -\t第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5\n -\t第二步：完整的解为一个特解加上零空间中任意向量\n -\t$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$\n -\t在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)\n -\t完整解为：\n$$\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n -\t其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点\n \n## 解的结构\n-\t现在考虑秩为r的m*n矩阵，r<=m，r<=n ，r取满秩时的情况,r=min(m,n)\n-\t列满秩：r=n<m，此时没有自由变量 ，**N(A)={0}**,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n$$\n-\t行满秩：r=m<n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI & F \\\\\n\t\\end{bmatrix}\n$$\n-\tr=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一\n\n## 一个网友从向量空间角度的解释\n\n>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。\n>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）\n>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）\n>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。\n\n\n","source":"_posts/LinearAlgebra1.md","raw":"---\ntitle: MIT线性代数笔记1\ndate: 2017-01-21 11:45:28\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# <font size=5 >第一讲：方程组的几何解释\n\n-\t从3个角度看待方程组：行图形，列图像，矩阵\n\n-\t例如对方程组：\n\n$$\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}\n$$\n\n<!--more-->\n\n## 行图像\n\n-\t行图像为：\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t也可以写成\n\n$$\nAx=b\n$$\n\n-\t即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png)\n\n## 列图像\n-\t列图像为：\n$$\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png)\n\n## 矩阵\n\n-\t现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？\n-\t如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆\n\n# <font size=5 >第二讲：消元、回代和置换 \n\n## 消元\n\n-\t考虑方程组\n$$\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}\n$$\n-\t他的A矩阵为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t经过行变换后为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再变换为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}\n$$\n-\t这样一系列变换即消元\n-\t变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）\n-\t矩阵\n$$\n\\left[\\begin{array}{c|c}\n A & X \\\\\n\\end{array}\\right]\n$$\n-\t称为增广矩阵(Augmented matrix)。b做同样变换可以得到c\n\n## 回代\n-\t解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例\n-\t因为U为上三角矩阵，z很容易求得\n-\t将z代入第二行求得y\n-\t将z,y代入第一行求得x\n-\t这个过程即回代\n\n## 置换\n\n$$\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A\n$$\n-\t这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3\n\n同理\n$$\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}\n$$\n-\t这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3\n-\t可以推出，交换A两行的矩阵为\n\n$$\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A\n$$\n-\t交换A两列的矩阵为\n\n$$\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵\n-\t在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$\n-\t消元可写成\n$$\nE_{32}E_{31}E_{21}A=U\n$$\n\n# <font size=5 >第三讲：乘法和逆矩阵\n\n## 矩阵乘法\n\n-\t考虑矩阵乘法\n$$\nA*B=C\n$$\n-\t第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$\n-\t第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列\n- \t第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行\n-\t第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C\n-\t第五种算法：矩阵分块算\n\n## 逆矩阵\n-\t对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵\n-\t对方阵，左逆矩阵与右逆矩阵相同\n-\t若存在非零矩阵X,使得$AX=0$,则A不可逆\n-\t求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵\n -\t证明：\n$$\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\\n$$\n\n# <font size=5 >第四讲：A的LU分解\n\n## LU分解\n-\t$(AB)^{-1}=B^{-1}A^{-1}$\n-\t对A的转置矩阵$A^T$,易得\n$$\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\\n$$\n-\t对单个矩阵而言，转置和求逆可以互换\n-\t矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例\n$$\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\\n$$\n-\t为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数\n\n## 消元消耗\n-\t记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为\n$$\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3\n$$\n\n## 群\n-\t以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)\n-\t对这些矩阵，$P^{-1}=P^T$\n-\t这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群\n-\tn*n矩阵共有n!个行置换矩阵\n\n# 第五讲：转置、置换、向量空间R\n\n## 置换\n-\t置换矩阵是用来完成行交换的矩阵\n-\tA=LU,L对角线上都是1，下方为消元乘数，U下三角为0\n-\tPA=LU用于描述包含行交换的lu分解\n-\tP(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价\n\n## 转置\n-\t行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$\n-\t$(AB)^T=B^TA^T$\n-\t对称矩阵(symmetric),$A^T=A$\n-\t对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$\n\n\n## 向量空间\n-\t向量可以相加减，点乘\n-\t**空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件**\n-\t例如$R^2$，代表所有实数的二维向量空间\n-\t向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)\n-\t不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的\n-\t在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)\n-\t$R^2$的子空间都有哪些？\n -\t$R^2$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t(0,0),简写为Z\n-\t$R^3$的子空间都有哪些？\n -\t$R^3$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t过零点的无限大平面\n -\t(0,0,0)\n \n\n\n## 通过矩阵构造向量子空间\n$$\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png)\n\n# <font size=5 >第六讲：列空间和零空间 \n\n## 列空间\n-\t上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间\n-\t对任意子空间S、T,$S \\bigcap T$是一个子空间\n\n-\t举个栗子\n$$\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n$$\n-\tC(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间\n-\t下面将子空间与线性方程组联系起来\n-\t现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？\n\t-\t前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，**即列空间无法填充整个四维空间**\n\t-\t后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，**等同于只有b在A的列空间内，x有解**\n-\t如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间\n\n## 零空间\n-\t零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x\n-\t列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n$$\n-\t显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间\n-\t为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：......矩阵乘法可以展开......分配率......\n\n\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}\n$$\n-\t我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？\n-\t显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0\n-\t列空间和零空间是两种构造子空间的方法\n\t-\t从几个向量通过线性组合来得到子空间\n\t-\t从一个方程组，通过让x满足特定条件来得到子空间\n\n# <font size=5 >第七讲：主变量、特解\n\n## 主变量\n-\t如何用算法解Ax=0\n-\t举个栗子:\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}\n$$\n-\t第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来\n-\t消元不改变方程的组，因为消元改动列空间,不改动解空间\n-\t第一次消元之后,第一列只有第一行的主元不为零\n\n\n\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}\n$$\n-\t此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U\n$$\n-\t如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)\n-\t现在我们可以解Ux=0,并进行回代\n-\t自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的**主变量**x1,x3可以通过回代解出\n\n## 特解\n-\t在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)\n-\t所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。\n-\t两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为**特解**，根据特解我们可以得到解空间：两组特解的线性组合,a\\*(-2,1,0,0)+b\\*(2,0,-2,1)\n-\t 秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量\n\n## 简化行阶梯形式\n-\tU还能进一步简化 \n$$\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t而且需将主元化为1,因为b=0,所以第二行可以直接除以2\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R\n$$\n-\t简化行阶梯形式以最简形式包含了矩阵的所有信息\n-\t单位矩阵位于主行与主列交汇处\n-\t最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列\n$$\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n$$\n其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列\n\n## 零空间矩阵\n\n-\t零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解\n$$\nR*N=0\n$$\n$$\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}\n$$\n-\t整个方程可以写成\n$$\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0\n$$\n$$\nx_{pivot}=-F\n$$\n\n## 最后举个栗子过一遍算法\n-\t原矩阵\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}\n$$\n-\t第一遍消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}\n$$\n-\t第二遍消元(进行一次行交换使得第二个主元在第二行)\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U\n$$\n-\t显然r=2,1个自由变量,令自由变量为1，得到特解x\n$$\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n-\t零空间就是cx,一条直线，这个x为零空间的基\n-\t接下来继续化简U\n$$\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}\n$$\n\n$$\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U\n$$\n\n$$\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N\n$$\n\n# <font size=5 >第八讲：可解性与解的结构\n\n## 可解性\n$$\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}\n$$\n-\t写成增广矩阵形式：\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]\n$$\n-\t消元得到:\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]\n$$\n-\t第一列和第三列为主列，第二列和第四列是自由列\n-\t可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里\n-\t**如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零**\n-\t如何求Ax=b的所有解？\n -\t第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5\n -\t第二步：完整的解为一个特解加上零空间中任意向量\n -\t$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$\n -\t在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)\n -\t完整解为：\n$$\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n -\t其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点\n \n## 解的结构\n-\t现在考虑秩为r的m*n矩阵，r<=m，r<=n ，r取满秩时的情况,r=min(m,n)\n-\t列满秩：r=n<m，此时没有自由变量 ，**N(A)={0}**,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n$$\n-\t行满秩：r=m<n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI & F \\\\\n\t\\end{bmatrix}\n$$\n-\tr=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一\n\n## 一个网友从向量空间角度的解释\n\n>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。\n>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）\n>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）\n>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。\n\n\n","slug":"LinearAlgebra1","published":1,"updated":"2018-09-16T13:26:45.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072c90003qcw60txh9z51","content":"<hr><h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul><li><p>从3个角度看待方程组：行图形，列图像，矩阵</p></li><li><p>例如对方程组：</p></li></ul><p>$$\\begin{cases}<br>2x-y=0\\\\<br>-x+2y=3\\\\<br>\\end{cases}<br>$$</p><a id=\"more\"></a><h2 id=\"行图像\"><a href=\"#行图像\" class=\"headerlink\" title=\"行图像\"></a>行图像</h2><ul><li>行图像为：</li></ul><p>$$<br>\\begin{bmatrix}<br>2 &amp; -1 \\\\<br>-1 &amp; 2 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x \\\\<br>y \\\\<br>\\end{bmatrix}<br>=<br>\\begin{bmatrix}<br>0 \\\\<br>3 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>也可以写成</li></ul><p>$$<br>Ax=b<br>$$</p><ul><li>即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png\" alt=\"mark\"></p><h2 id=\"列图像\"><a href=\"#列图像\" class=\"headerlink\" title=\"列图像\"></a>列图像</h2><ul><li><p>列图像为：<br>$$<br>x<br>\\begin{bmatrix}<br>2 \\\\<br>-1 \\\\<br>\\end{bmatrix}<br>+y<br>\\begin{bmatrix}<br>-1 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>=<br>\\begin{bmatrix}<br>0 \\\\<br>3 \\\\<br>\\end{bmatrix}<br>$$</p></li><li><p>方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png\" alt=\"mark\"></p></li></ul><h2 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h2><ul><li>现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？</li><li>如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆</li></ul><h1 id=\"第二讲：消元、回代和置换\"><a href=\"#第二讲：消元、回代和置换\" class=\"headerlink\" title=\"第二讲：消元、回代和置换\"></a><font size=\"5\">第二讲：消元、回代和置换</font></h1><h2 id=\"消元\"><a href=\"#消元\" class=\"headerlink\" title=\"消元\"></a>消元</h2><ul><li>考虑方程组<br>$$\\begin{cases}<br>x+2y+z=2\\\\<br>3x+8y+z=12\\\\<br>4y+z=2\\\\<br>\\end{cases}<br>$$</li><li>他的A矩阵为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>3 &amp; 8 &amp; 1 \\\\<br>0 &amp; 4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>经过行变换后为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>0 &amp; 2 &amp; -2 \\\\<br>0 &amp; 4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>再变换为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>0 &amp; 2 &amp; -2 \\\\<br>0 &amp; 0 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</li><li>这样一系列变换即消元</li><li>变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）</li><li>矩阵<br>$$<br>\\left[\\begin{array}{c|c}<br>A &amp; X \\\\<br>\\end{array}\\right]<br>$$</li><li>称为增广矩阵(Augmented matrix)。b做同样变换可以得到c</li></ul><h2 id=\"回代\"><a href=\"#回代\" class=\"headerlink\" title=\"回代\"></a>回代</h2><ul><li>解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例</li><li>因为U为上三角矩阵，z很容易求得</li><li>将z代入第二行求得y</li><li>将z,y代入第一行求得x</li><li>这个过程即回代</li></ul><h2 id=\"置换\"><a href=\"#置换\" class=\"headerlink\" title=\"置换\"></a>置换</h2><p>$$<br>\\begin{bmatrix}<br>a &amp; b &amp; c \\\\<br>\\end{bmatrix}*A<br>$$</p><ul><li>这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3</li></ul><p>同理<br>$$<br>A*\\begin{bmatrix}<br>a \\\\<br>b \\\\<br>c \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3</li><li>可以推出，交换A两行的矩阵为</li></ul><p>$$<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}*A<br>$$</p><ul><li>交换A两列的矩阵为</li></ul><p>$$<br>A*\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵</li><li>在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$</li><li>消元可写成<br>$$<br>E_{32}E_{31}E_{21}A=U<br>$$</li></ul><h1 id=\"第三讲：乘法和逆矩阵\"><a href=\"#第三讲：乘法和逆矩阵\" class=\"headerlink\" title=\"第三讲：乘法和逆矩阵\"></a><font size=\"5\">第三讲：乘法和逆矩阵</font></h1><h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><ul><li>考虑矩阵乘法<br>$$<br>A*B=C<br>$$</li><li>第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$</li><li>第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列</li><li>第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行</li><li>第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C</li><li>第五种算法：矩阵分块算</li></ul><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul><li>对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵</li><li>对方阵，左逆矩阵与右逆矩阵相同</li><li>若存在非零矩阵X,使得$AX=0$,则A不可逆</li><li>求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵<ul><li>证明：<br>$$<br>EA=I \\\\<br>E=A^{-1} \\\\<br>EI=A^{-1} \\\\<br>$$</li></ul></li></ul><h1 id=\"第四讲：A的LU分解\"><a href=\"#第四讲：A的LU分解\" class=\"headerlink\" title=\"第四讲：A的LU分解\"></a><font size=\"5\">第四讲：A的LU分解</font></h1><h2 id=\"LU分解\"><a href=\"#LU分解\" class=\"headerlink\" title=\"LU分解\"></a>LU分解</h2><ul><li>$(AB)^{-1}=B^{-1}A^{-1}$</li><li>对A的转置矩阵$A^T$,易得<br>$$<br>AA^{-1}=I \\\\<br>(A^{-1})^TA^T=I \\\\<br>所以(A^T)^{-1}=(A^{-1})^T \\\\<br>$$</li><li>对单个矩阵而言，转置和求逆可以互换</li><li>矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例<br>$$<br>E_{32}E_{31}E_{21}A=U \\\\<br>所以可得L: \\\\<br>L=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\<br>$$</li><li>为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数</li></ul><h2 id=\"消元消耗\"><a href=\"#消元消耗\" class=\"headerlink\" title=\"消元消耗\"></a>消元消耗</h2><ul><li>记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为<br>$$<br>\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3<br>$$</li></ul><h2 id=\"群\"><a href=\"#群\" class=\"headerlink\" title=\"群\"></a>群</h2><ul><li>以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)</li><li>对这些矩阵，$P^{-1}=P^T$</li><li>这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群</li><li>n*n矩阵共有n!个行置换矩阵</li></ul><h1 id=\"第五讲：转置、置换、向量空间R\"><a href=\"#第五讲：转置、置换、向量空间R\" class=\"headerlink\" title=\"第五讲：转置、置换、向量空间R\"></a>第五讲：转置、置换、向量空间R</h1><h2 id=\"置换-1\"><a href=\"#置换-1\" class=\"headerlink\" title=\"置换\"></a>置换</h2><ul><li>置换矩阵是用来完成行交换的矩阵</li><li>A=LU,L对角线上都是1，下方为消元乘数，U下三角为0</li><li>PA=LU用于描述包含行交换的lu分解</li><li>P(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价</li></ul><h2 id=\"转置\"><a href=\"#转置\" class=\"headerlink\" title=\"转置\"></a>转置</h2><ul><li>行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$</li><li>$(AB)^T=B^TA^T$</li><li>对称矩阵(symmetric),$A^T=A$</li><li>对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$</li></ul><h2 id=\"向量空间\"><a href=\"#向量空间\" class=\"headerlink\" title=\"向量空间\"></a>向量空间</h2><ul><li>向量可以相加减，点乘</li><li><strong>空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件</strong></li><li>例如$R^2$，代表所有实数的二维向量空间</li><li>向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)</li><li>不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的</li><li>在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)</li><li>$R^2$的子空间都有哪些？<ul><li>$R^2$本身</li><li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li><li>(0,0),简写为Z</li></ul></li><li>$R^3$的子空间都有哪些？<ul><li>$R^3$本身</li><li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li><li>过零点的无限大平面</li><li>(0,0,0)</li></ul></li></ul><h2 id=\"通过矩阵构造向量子空间\"><a href=\"#通过矩阵构造向量子空间\" class=\"headerlink\" title=\"通过矩阵构造向量子空间\"></a>通过矩阵构造向量子空间</h2><p>$$<br>A=\\begin{bmatrix}<br>1 &amp; 3 \\\\<br>2 &amp; 3 \\\\<br>4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png\" alt=\"mark\"></li></ul><h1 id=\"第六讲：列空间和零空间\"><a href=\"#第六讲：列空间和零空间\" class=\"headerlink\" title=\"第六讲：列空间和零空间\"></a><font size=\"5\">第六讲：列空间和零空间</font></h1><h2 id=\"列空间\"><a href=\"#列空间\" class=\"headerlink\" title=\"列空间\"></a>列空间</h2><ul><li>上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间</li><li><p>对任意子空间S、T,$S \\bigcap T$是一个子空间</p></li><li><p>举个栗子<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</p></li><li>C(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间</li><li>下面将子空间与线性方程组联系起来</li><li>现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？<ul><li>前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，<strong>即列空间无法填充整个四维空间</strong></li><li>后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，<strong>等同于只有b在A的列空间内，x有解</strong></li></ul></li><li>如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间</li></ul><h2 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h2><ul><li>零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x</li><li>列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间<br>$$<br>\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>X_1 \\\\<br>X_2 \\\\<br>X_3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>0 \\\\<br>0 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li><li>显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间</li><li>为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：……矩阵乘法可以展开……分配率……</li></ul><p>$$<br>\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>X_1 \\\\<br>X_2 \\\\<br>X_3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>3 \\\\<br>4 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？</li><li>显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0</li><li>列空间和零空间是两种构造子空间的方法<ul><li>从几个向量通过线性组合来得到子空间</li><li>从一个方程组，通过让x满足特定条件来得到子空间</li></ul></li></ul><h1 id=\"第七讲：主变量、特解\"><a href=\"#第七讲：主变量、特解\" class=\"headerlink\" title=\"第七讲：主变量、特解\"></a><font size=\"5\">第七讲：主变量、特解</font></h1><h2 id=\"主变量\"><a href=\"#主变量\" class=\"headerlink\" title=\"主变量\"></a>主变量</h2><ul><li>如何用算法解Ax=0</li><li>举个栗子:<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>2 &amp; 4 &amp; 6 &amp; 8 \\\\<br>3 &amp; 6 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}<br>$$</li><li>第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来</li><li>消元不改变方程的组，因为消元改动列空间,不改动解空间</li><li>第一次消元之后,第一列只有第一行的主元不为零</li></ul><p>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=U<br>$$</li><li>如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)</li><li>现在我们可以解Ux=0,并进行回代</li><li>自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的<strong>主变量</strong>x1,x3可以通过回代解出</li></ul><h2 id=\"特解\"><a href=\"#特解\" class=\"headerlink\" title=\"特解\"></a>特解</h2><ul><li>在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)</li><li>所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。</li><li>两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为<strong>特解</strong>，根据特解我们可以得到解空间：两组特解的线性组合,a*(-2,1,0,0)+b*(2,0,-2,1)</li><li>秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量</li></ul><h2 id=\"简化行阶梯形式\"><a href=\"#简化行阶梯形式\" class=\"headerlink\" title=\"简化行阶梯形式\"></a>简化行阶梯形式</h2><ul><li>U还能进一步简化<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 0 &amp; -2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>而且需将主元化为1,因为b=0,所以第二行可以直接除以2<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 0 &amp; -2 \\\\<br>0 &amp; 0 &amp; 1 &amp; 2 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=R<br>$$</li><li>简化行阶梯形式以最简形式包含了矩阵的所有信息</li><li>单位矩阵位于主行与主列交汇处</li><li>最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列<br>$$<br>R=\\begin{bmatrix}<br>I &amp; F \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$<br>其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列</li></ul><h2 id=\"零空间矩阵\"><a href=\"#零空间矩阵\" class=\"headerlink\" title=\"零空间矩阵\"></a>零空间矩阵</h2><ul><li>零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解<br>$$<br>R*N=0<br>$$<br>$$<br>N=\\begin{bmatrix}<br>-F \\\\<br>I \\\\<br>\\end{bmatrix}<br>$$</li><li>整个方程可以写成<br>$$<br>\\begin{bmatrix}<br>I &amp; F \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x_{pivot} \\\\<br>x_{free} \\\\<br>\\end{bmatrix}=0<br>$$<br>$$<br>x_{pivot}=-F<br>$$</li></ul><h2 id=\"最后举个栗子过一遍算法\"><a href=\"#最后举个栗子过一遍算法\" class=\"headerlink\" title=\"最后举个栗子过一遍算法\"></a>最后举个栗子过一遍算法</h2><ul><li>原矩阵<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>2 &amp; 4 &amp; 6 \\\\<br>2 &amp; 6 &amp; 8 \\\\<br>2 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}<br>$$</li><li>第一遍消元<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 2 &amp; 2 \\\\<br>0 &amp; 4 &amp; 4 \\\\<br>\\end{bmatrix}<br>$$</li><li>第二遍消元(进行一次行交换使得第二个主元在第二行)<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>0 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=U<br>$$</li><li>显然r=2,1个自由变量,令自由变量为1，得到特解x<br>$$<br>x=\\begin{bmatrix}<br>-1 \\\\<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>零空间就是cx,一条直线，这个x为零空间的基</li><li>接下来继续化简U<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 0 &amp; 1 \\\\<br>0 &amp; 1 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=R=<br>\\begin{bmatrix}<br>I &amp; F \\\\<br>0 &amp; 0 \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li></ul><p>$$<br>F=\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix}=U<br>$$</p><p>$$<br>x=\\begin{bmatrix}<br>-F \\\\<br>I \\\\<br>\\end{bmatrix}=N<br>$$</p><h1 id=\"第八讲：可解性与解的结构\"><a href=\"#第八讲：可解性与解的结构\" class=\"headerlink\" title=\"第八讲：可解性与解的结构\"></a><font size=\"5\">第八讲：可解性与解的结构</font></h1><h2 id=\"可解性\"><a href=\"#可解性\" class=\"headerlink\" title=\"可解性\"></a>可解性</h2><p>$$\\begin{cases}<br>x_1+2x_2+2x_3+2x_4=b_1\\\\<br>2x_1+4x_2+6x_3+8x_4=b_2\\\\<br>3x_1+6x_2+8x_3+10x_4=b_3\\\\<br>\\end{cases}<br>$$</p><ul><li>写成增广矩阵形式：<br>$$<br>\\left[\\begin{array}{c c c c|c}<br>1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\\\<br>2 &amp; 4 &amp; 6 &amp; 8 &amp; b_2 \\\\<br>3 &amp; 6 &amp; 8 &amp; 10 &amp; b_3 \\\\<br>\\end{array}\\right]<br>$$</li><li>消元得到:<br>$$<br>\\left[\\begin{array}{c c c c|c}<br>1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 &amp; b_2-2b_1 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3-b_2-b_1 \\\\<br>\\end{array}\\right]<br>$$</li><li>第一列和第三列为主列，第二列和第四列是自由列</li><li>可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里</li><li><strong>如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零</strong></li><li>如何求Ax=b的所有解？<ul><li>第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5</li><li>第二步：完整的解为一个特解加上零空间中任意向量</li><li>$Ax_{particular}=b \\\\ Ax_{nullspace}=0 \\\\ A(x_{particular}+x_{nullspace})=b$</li><li>在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)</li><li>完整解为：<br>$$<br>x_{complete}=<br>\\begin{bmatrix}<br>-2 \\\\<br>0 \\\\<br>1.5 \\\\<br>0 \\\\<br>\\end{bmatrix}+<br>c_1\\begin{bmatrix}<br>-2 \\\\<br>1\\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}+<br>c_2\\begin{bmatrix}<br>2 \\\\<br>0 \\\\<br>-2 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点</li></ul></li></ul><h2 id=\"解的结构\"><a href=\"#解的结构\" class=\"headerlink\" title=\"解的结构\"></a>解的结构</h2><ul><li>现在考虑秩为r的m*n矩阵，r&lt;=m，r&lt;=n ，r取满秩时的情况,r=min(m,n)</li><li>列满秩：r=n&lt;m，此时没有自由变量 ，<strong>N(A)={0}</strong>,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为<br>$$<br>R=\\begin{bmatrix}<br>I \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li><li>行满秩：r=m&lt;n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为<br>$$<br>R=\\begin{bmatrix}<br>I &amp; F \\\\<br>\\end{bmatrix}<br>$$</li><li>r=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一</li></ul><h2 id=\"一个网友从向量空间角度的解释\"><a href=\"#一个网友从向量空间角度的解释\" class=\"headerlink\" title=\"一个网友从向量空间角度的解释\"></a>一个网友从向量空间角度的解释</h2><blockquote><p>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。<br>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）<br>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）<br>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。</p></blockquote>","site":{"data":{}},"excerpt":"<hr><h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul><li><p>从3个角度看待方程组：行图形，列图像，矩阵</p></li><li><p>例如对方程组：</p></li></ul><p>$$\\begin{cases}<br>2x-y=0\\\\<br>-x+2y=3\\\\<br>\\end{cases}<br>$$</p>","more":"<h2 id=\"行图像\"><a href=\"#行图像\" class=\"headerlink\" title=\"行图像\"></a>行图像</h2><ul><li>行图像为：</li></ul><p>$$<br>\\begin{bmatrix}<br>2 &amp; -1 \\\\<br>-1 &amp; 2 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x \\\\<br>y \\\\<br>\\end{bmatrix}<br>=<br>\\begin{bmatrix}<br>0 \\\\<br>3 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>也可以写成</li></ul><p>$$<br>Ax=b<br>$$</p><ul><li>即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png\" alt=\"mark\"></p><h2 id=\"列图像\"><a href=\"#列图像\" class=\"headerlink\" title=\"列图像\"></a>列图像</h2><ul><li><p>列图像为：<br>$$<br>x<br>\\begin{bmatrix}<br>2 \\\\<br>-1 \\\\<br>\\end{bmatrix}<br>+y<br>\\begin{bmatrix}<br>-1 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>=<br>\\begin{bmatrix}<br>0 \\\\<br>3 \\\\<br>\\end{bmatrix}<br>$$</p></li><li><p>方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png\" alt=\"mark\"></p></li></ul><h2 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h2><ul><li>现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？</li><li>如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆</li></ul><h1 id=\"第二讲：消元、回代和置换\"><a href=\"#第二讲：消元、回代和置换\" class=\"headerlink\" title=\"第二讲：消元、回代和置换\"></a><font size=\"5\">第二讲：消元、回代和置换</font></h1><h2 id=\"消元\"><a href=\"#消元\" class=\"headerlink\" title=\"消元\"></a>消元</h2><ul><li>考虑方程组<br>$$\\begin{cases}<br>x+2y+z=2\\\\<br>3x+8y+z=12\\\\<br>4y+z=2\\\\<br>\\end{cases}<br>$$</li><li>他的A矩阵为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>3 &amp; 8 &amp; 1 \\\\<br>0 &amp; 4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>经过行变换后为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>0 &amp; 2 &amp; -2 \\\\<br>0 &amp; 4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>再变换为<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 1 \\\\<br>0 &amp; 2 &amp; -2 \\\\<br>0 &amp; 0 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</li><li>这样一系列变换即消元</li><li>变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）</li><li>矩阵<br>$$<br>\\left[\\begin{array}{c|c}<br>A &amp; X \\\\<br>\\end{array}\\right]<br>$$</li><li>称为增广矩阵(Augmented matrix)。b做同样变换可以得到c</li></ul><h2 id=\"回代\"><a href=\"#回代\" class=\"headerlink\" title=\"回代\"></a>回代</h2><ul><li>解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例</li><li>因为U为上三角矩阵，z很容易求得</li><li>将z代入第二行求得y</li><li>将z,y代入第一行求得x</li><li>这个过程即回代</li></ul><h2 id=\"置换\"><a href=\"#置换\" class=\"headerlink\" title=\"置换\"></a>置换</h2><p>$$<br>\\begin{bmatrix}<br>a &amp; b &amp; c \\\\<br>\\end{bmatrix}*A<br>$$</p><ul><li>这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3</li></ul><p>同理<br>$$<br>A*\\begin{bmatrix}<br>a \\\\<br>b \\\\<br>c \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3</li><li>可以推出，交换A两行的矩阵为</li></ul><p>$$<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}*A<br>$$</p><ul><li>交换A两列的矩阵为</li></ul><p>$$<br>A*\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵</li><li>在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$</li><li>消元可写成<br>$$<br>E_{32}E_{31}E_{21}A=U<br>$$</li></ul><h1 id=\"第三讲：乘法和逆矩阵\"><a href=\"#第三讲：乘法和逆矩阵\" class=\"headerlink\" title=\"第三讲：乘法和逆矩阵\"></a><font size=\"5\">第三讲：乘法和逆矩阵</font></h1><h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><ul><li>考虑矩阵乘法<br>$$<br>A*B=C<br>$$</li><li>第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$</li><li>第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列</li><li>第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行</li><li>第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C</li><li>第五种算法：矩阵分块算</li></ul><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul><li>对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵</li><li>对方阵，左逆矩阵与右逆矩阵相同</li><li>若存在非零矩阵X,使得$AX=0$,则A不可逆</li><li>求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵<ul><li>证明：<br>$$<br>EA=I \\\\<br>E=A^{-1} \\\\<br>EI=A^{-1} \\\\<br>$$</li></ul></li></ul><h1 id=\"第四讲：A的LU分解\"><a href=\"#第四讲：A的LU分解\" class=\"headerlink\" title=\"第四讲：A的LU分解\"></a><font size=\"5\">第四讲：A的LU分解</font></h1><h2 id=\"LU分解\"><a href=\"#LU分解\" class=\"headerlink\" title=\"LU分解\"></a>LU分解</h2><ul><li>$(AB)^{-1}=B^{-1}A^{-1}$</li><li>对A的转置矩阵$A^T$,易得<br>$$<br>AA^{-1}=I \\\\<br>(A^{-1})^TA^T=I \\\\<br>所以(A^T)^{-1}=(A^{-1})^T \\\\<br>$$</li><li>对单个矩阵而言，转置和求逆可以互换</li><li>矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例<br>$$<br>E_{32}E_{31}E_{21}A=U \\\\<br>所以可得L: \\\\<br>L=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\<br>$$</li><li>为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数</li></ul><h2 id=\"消元消耗\"><a href=\"#消元消耗\" class=\"headerlink\" title=\"消元消耗\"></a>消元消耗</h2><ul><li>记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为<br>$$<br>\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3<br>$$</li></ul><h2 id=\"群\"><a href=\"#群\" class=\"headerlink\" title=\"群\"></a>群</h2><ul><li>以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)</li><li>对这些矩阵，$P^{-1}=P^T$</li><li>这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群</li><li>n*n矩阵共有n!个行置换矩阵</li></ul><h1 id=\"第五讲：转置、置换、向量空间R\"><a href=\"#第五讲：转置、置换、向量空间R\" class=\"headerlink\" title=\"第五讲：转置、置换、向量空间R\"></a>第五讲：转置、置换、向量空间R</h1><h2 id=\"置换-1\"><a href=\"#置换-1\" class=\"headerlink\" title=\"置换\"></a>置换</h2><ul><li>置换矩阵是用来完成行交换的矩阵</li><li>A=LU,L对角线上都是1，下方为消元乘数，U下三角为0</li><li>PA=LU用于描述包含行交换的lu分解</li><li>P(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价</li></ul><h2 id=\"转置\"><a href=\"#转置\" class=\"headerlink\" title=\"转置\"></a>转置</h2><ul><li>行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$</li><li>$(AB)^T=B^TA^T$</li><li>对称矩阵(symmetric),$A^T=A$</li><li>对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$</li></ul><h2 id=\"向量空间\"><a href=\"#向量空间\" class=\"headerlink\" title=\"向量空间\"></a>向量空间</h2><ul><li>向量可以相加减，点乘</li><li><strong>空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件</strong></li><li>例如$R^2$，代表所有实数的二维向量空间</li><li>向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)</li><li>不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的</li><li>在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)</li><li>$R^2$的子空间都有哪些？<ul><li>$R^2$本身</li><li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li><li>(0,0),简写为Z</li></ul></li><li>$R^3$的子空间都有哪些？<ul><li>$R^3$本身</li><li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li><li>过零点的无限大平面</li><li>(0,0,0)</li></ul></li></ul><h2 id=\"通过矩阵构造向量子空间\"><a href=\"#通过矩阵构造向量子空间\" class=\"headerlink\" title=\"通过矩阵构造向量子空间\"></a>通过矩阵构造向量子空间</h2><p>$$<br>A=\\begin{bmatrix}<br>1 &amp; 3 \\\\<br>2 &amp; 3 \\\\<br>4 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png\" alt=\"mark\"></li></ul><h1 id=\"第六讲：列空间和零空间\"><a href=\"#第六讲：列空间和零空间\" class=\"headerlink\" title=\"第六讲：列空间和零空间\"></a><font size=\"5\">第六讲：列空间和零空间</font></h1><h2 id=\"列空间\"><a href=\"#列空间\" class=\"headerlink\" title=\"列空间\"></a>列空间</h2><ul><li>上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间</li><li><p>对任意子空间S、T,$S \\bigcap T$是一个子空间</p></li><li><p>举个栗子<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</p></li><li>C(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间</li><li>下面将子空间与线性方程组联系起来</li><li>现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？<ul><li>前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，<strong>即列空间无法填充整个四维空间</strong></li><li>后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，<strong>等同于只有b在A的列空间内，x有解</strong></li></ul></li><li>如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间</li></ul><h2 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h2><ul><li>零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x</li><li>列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间<br>$$<br>\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>X_1 \\\\<br>X_2 \\\\<br>X_3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>0 \\\\<br>0 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li><li>显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间</li><li>为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：……矩阵乘法可以展开……分配率……</li></ul><p>$$<br>\\begin{bmatrix}<br>1 &amp; 1 &amp; 2 \\\\<br>2 &amp; 1 &amp; 3 \\\\<br>3 &amp; 1 &amp; 5 \\\\<br>4 &amp; 1 &amp; 5 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>X_1 \\\\<br>X_2 \\\\<br>X_3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>3 \\\\<br>4 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？</li><li>显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0</li><li>列空间和零空间是两种构造子空间的方法<ul><li>从几个向量通过线性组合来得到子空间</li><li>从一个方程组，通过让x满足特定条件来得到子空间</li></ul></li></ul><h1 id=\"第七讲：主变量、特解\"><a href=\"#第七讲：主变量、特解\" class=\"headerlink\" title=\"第七讲：主变量、特解\"></a><font size=\"5\">第七讲：主变量、特解</font></h1><h2 id=\"主变量\"><a href=\"#主变量\" class=\"headerlink\" title=\"主变量\"></a>主变量</h2><ul><li>如何用算法解Ax=0</li><li>举个栗子:<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>2 &amp; 4 &amp; 6 &amp; 8 \\\\<br>3 &amp; 6 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}<br>$$</li><li>第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来</li><li>消元不改变方程的组，因为消元改动列空间,不改动解空间</li><li>第一次消元之后,第一列只有第一行的主元不为零</li></ul><p>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=U<br>$$</li><li>如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)</li><li>现在我们可以解Ux=0,并进行回代</li><li>自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的<strong>主变量</strong>x1,x3可以通过回代解出</li></ul><h2 id=\"特解\"><a href=\"#特解\" class=\"headerlink\" title=\"特解\"></a>特解</h2><ul><li>在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)</li><li>所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。</li><li>两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为<strong>特解</strong>，根据特解我们可以得到解空间：两组特解的线性组合,a*(-2,1,0,0)+b*(2,0,-2,1)</li><li>秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量</li></ul><h2 id=\"简化行阶梯形式\"><a href=\"#简化行阶梯形式\" class=\"headerlink\" title=\"简化行阶梯形式\"></a>简化行阶梯形式</h2><ul><li>U还能进一步简化<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 0 &amp; -2 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>而且需将主元化为1,因为b=0,所以第二行可以直接除以2<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 2 &amp; 0 &amp; -2 \\\\<br>0 &amp; 0 &amp; 1 &amp; 2 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=R<br>$$</li><li>简化行阶梯形式以最简形式包含了矩阵的所有信息</li><li>单位矩阵位于主行与主列交汇处</li><li>最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列<br>$$<br>R=\\begin{bmatrix}<br>I &amp; F \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$<br>其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列</li></ul><h2 id=\"零空间矩阵\"><a href=\"#零空间矩阵\" class=\"headerlink\" title=\"零空间矩阵\"></a>零空间矩阵</h2><ul><li>零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解<br>$$<br>R*N=0<br>$$<br>$$<br>N=\\begin{bmatrix}<br>-F \\\\<br>I \\\\<br>\\end{bmatrix}<br>$$</li><li>整个方程可以写成<br>$$<br>\\begin{bmatrix}<br>I &amp; F \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x_{pivot} \\\\<br>x_{free} \\\\<br>\\end{bmatrix}=0<br>$$<br>$$<br>x_{pivot}=-F<br>$$</li></ul><h2 id=\"最后举个栗子过一遍算法\"><a href=\"#最后举个栗子过一遍算法\" class=\"headerlink\" title=\"最后举个栗子过一遍算法\"></a>最后举个栗子过一遍算法</h2><ul><li>原矩阵<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>2 &amp; 4 &amp; 6 \\\\<br>2 &amp; 6 &amp; 8 \\\\<br>2 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}<br>$$</li><li>第一遍消元<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 2 &amp; 2 \\\\<br>0 &amp; 4 &amp; 4 \\\\<br>\\end{bmatrix}<br>$$</li><li>第二遍消元(进行一次行交换使得第二个主元在第二行)<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\\\<br>0 &amp; 2 &amp; 2 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=U<br>$$</li><li>显然r=2,1个自由变量,令自由变量为1，得到特解x<br>$$<br>x=\\begin{bmatrix}<br>-1 \\\\<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>零空间就是cx,一条直线，这个x为零空间的基</li><li>接下来继续化简U<br>$$<br>U=\\begin{bmatrix}<br>1 &amp; 0 &amp; 1 \\\\<br>0 &amp; 1 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}=R=<br>\\begin{bmatrix}<br>I &amp; F \\\\<br>0 &amp; 0 \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li></ul><p>$$<br>F=\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix}=U<br>$$</p><p>$$<br>x=\\begin{bmatrix}<br>-F \\\\<br>I \\\\<br>\\end{bmatrix}=N<br>$$</p><h1 id=\"第八讲：可解性与解的结构\"><a href=\"#第八讲：可解性与解的结构\" class=\"headerlink\" title=\"第八讲：可解性与解的结构\"></a><font size=\"5\">第八讲：可解性与解的结构</font></h1><h2 id=\"可解性\"><a href=\"#可解性\" class=\"headerlink\" title=\"可解性\"></a>可解性</h2><p>$$\\begin{cases}<br>x_1+2x_2+2x_3+2x_4=b_1\\\\<br>2x_1+4x_2+6x_3+8x_4=b_2\\\\<br>3x_1+6x_2+8x_3+10x_4=b_3\\\\<br>\\end{cases}<br>$$</p><ul><li>写成增广矩阵形式：<br>$$<br>\\left[\\begin{array}{c c c c|c}<br>1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\\\<br>2 &amp; 4 &amp; 6 &amp; 8 &amp; b_2 \\\\<br>3 &amp; 6 &amp; 8 &amp; 10 &amp; b_3 \\\\<br>\\end{array}\\right]<br>$$</li><li>消元得到:<br>$$<br>\\left[\\begin{array}{c c c c|c}<br>1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\\\<br>0 &amp; 0 &amp; 2 &amp; 4 &amp; b_2-2b_1 \\\\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3-b_2-b_1 \\\\<br>\\end{array}\\right]<br>$$</li><li>第一列和第三列为主列，第二列和第四列是自由列</li><li>可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里</li><li><strong>如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零</strong></li><li>如何求Ax=b的所有解？<ul><li>第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5</li><li>第二步：完整的解为一个特解加上零空间中任意向量</li><li>$Ax_{particular}=b \\\\ Ax_{nullspace}=0 \\\\ A(x_{particular}+x_{nullspace})=b$</li><li>在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)</li><li>完整解为：<br>$$<br>x_{complete}=<br>\\begin{bmatrix}<br>-2 \\\\<br>0 \\\\<br>1.5 \\\\<br>0 \\\\<br>\\end{bmatrix}+<br>c_1\\begin{bmatrix}<br>-2 \\\\<br>1\\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}+<br>c_2\\begin{bmatrix}<br>2 \\\\<br>0 \\\\<br>-2 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点</li></ul></li></ul><h2 id=\"解的结构\"><a href=\"#解的结构\" class=\"headerlink\" title=\"解的结构\"></a>解的结构</h2><ul><li>现在考虑秩为r的m*n矩阵，r&lt;=m，r&lt;=n ，r取满秩时的情况,r=min(m,n)</li><li>列满秩：r=n&lt;m，此时没有自由变量 ，<strong>N(A)={0}</strong>,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为<br>$$<br>R=\\begin{bmatrix}<br>I \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li><li>行满秩：r=m&lt;n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为<br>$$<br>R=\\begin{bmatrix}<br>I &amp; F \\\\<br>\\end{bmatrix}<br>$$</li><li>r=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一</li></ul><h2 id=\"一个网友从向量空间角度的解释\"><a href=\"#一个网友从向量空间角度的解释\" class=\"headerlink\" title=\"一个网友从向量空间角度的解释\"></a>一个网友从向量空间角度的解释</h2><blockquote><p>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。<br>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）<br>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）<br>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。</p></blockquote>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sun Sep 16 2018 21:26:45 GMT+0800 (中国标准时间)","title":"MIT线性代数笔记1","path":"2017/01/21/LinearAlgebra1/","eyeCatchImage":null,"excerpt":"<hr><h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul><li><p>从3个角度看待方程组：行图形，列图像，矩阵</p></li><li><p>例如对方程组：</p></li></ul><p>$$\\begin{cases}<br>2x-y=0\\\\<br>-x+2y=3\\\\<br>\\end{cases}<br>$$</p>","date":"2017-01-21T03:45:28.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Lagrange,KTT,PCA,SVM","date":"2017-03-18T03:20:35.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170318/112133081.png"],"html":true,"_content":"***\n介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用\n图片来自wikipedia关于拉格朗日乘子法的形象介绍\n<!--more-->\n\n# 拉格朗日乘子法\n-\t拉格朗日乘子法是一种求约束条件下极值的方法，描述为\n\t$$\n\t在约束条件g(x,y)=c下 \\\\\n\t求函数f(x,y)的极值 \\\\\n\t$$\n\t其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。\n-\t由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。\n-\t显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为\n\t$$\n\t\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))\n\t$$\n\t$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。\n\t拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $\n-\t求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。\n-\t拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$\n\n# 卡罗需-库恩-塔克条件\n-\t如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件\n-\t包含不等约束的极值问题描述为\n\t$$\n\t在约束条件: \\\\\n\th_j(X)=0 j=1,2,...,p \\\\\n\tg_k(X)\\leq 0 k=1,2,...q \\\\\n\t求函数f(X)的极值 \\\\\n\t$$\n-\t拉格朗日函数为\n\t$$\n\tL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)\n\t$$\n-\tKTT条件为:\n\t$$\n\t\\frac{dL}{dX}=0 \\\\\n\t\\lambda _j \\neq 0 \\\\\n\t\\mu _k \\geq 0 \\\\\n\t\\mu _k g_k(X)=0 \\\\\n\th_j(X)=0 \\\\\n\tg_k(X) \\leq 0\\\\\n\t$$\n\n# PCA\n-\tPCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本\n-\t记$x_1,...,x_p$为原始p个维度，新维度是$\\xi _1,....,\\xi _p$\n-\t新维度是原始维度的线性组合，表示为\n\t$$\n\t\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x\n\t$$\n-\t为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即\n\t$$\n\t\\alpha _i^T \\alpha _i=1\n\t$$\n-\t令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类\n-\t此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解\n-\t当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1\\-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度\n-\t依次求出p个新维度\n-\tPCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k<q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间\n-\t如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除\n-\tPCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取\t\n\n# SVM\n-\t在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面\n-\t划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移\n-\t求解一个SVM，即找到满足约束\n\t$$\n\t\\begin{cases}\n\tw^Tx_i+b \\geq +1, y_i=+1 \\\\\n\tw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\t\\end{cases}\n\t$$\n\t的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大\n\t这可以重写为最优化问题\n\t$$\n\tmin_{w,b} \\frac 12 {||w||}^2 \\\\\n\ts.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\\n\t$$\n\t推导见:[线性可分支持向量机与硬间隔最大化](http://thinkwee.top/2017/02/12/StatisticalLearningNote/#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96)\n-\t对于这个最优化问题，它的拉格朗日方程是\n\t$$\n\tL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))\n\t$$\n\t其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题\n\t$$\n\tmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\n\ts.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\t\\alpha _i \\geq 0,i=1,2,...,m \\\\\n\t$$\n\t上式满足KTT条件，通过SMO算法求解\n-\t未完待续","source":"_posts/Lagrange.md","raw":"---\ntitle: Lagrange,KTT,PCA,SVM\ndate: 2017-03-18 11:20:35\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170318/112133081.png\nhtml: true\n---\n***\n介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用\n图片来自wikipedia关于拉格朗日乘子法的形象介绍\n<!--more-->\n\n# 拉格朗日乘子法\n-\t拉格朗日乘子法是一种求约束条件下极值的方法，描述为\n\t$$\n\t在约束条件g(x,y)=c下 \\\\\n\t求函数f(x,y)的极值 \\\\\n\t$$\n\t其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。\n-\t由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。\n-\t显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为\n\t$$\n\t\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))\n\t$$\n\t$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。\n\t拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $\n-\t求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。\n-\t拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$\n\n# 卡罗需-库恩-塔克条件\n-\t如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件\n-\t包含不等约束的极值问题描述为\n\t$$\n\t在约束条件: \\\\\n\th_j(X)=0 j=1,2,...,p \\\\\n\tg_k(X)\\leq 0 k=1,2,...q \\\\\n\t求函数f(X)的极值 \\\\\n\t$$\n-\t拉格朗日函数为\n\t$$\n\tL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)\n\t$$\n-\tKTT条件为:\n\t$$\n\t\\frac{dL}{dX}=0 \\\\\n\t\\lambda _j \\neq 0 \\\\\n\t\\mu _k \\geq 0 \\\\\n\t\\mu _k g_k(X)=0 \\\\\n\th_j(X)=0 \\\\\n\tg_k(X) \\leq 0\\\\\n\t$$\n\n# PCA\n-\tPCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本\n-\t记$x_1,...,x_p$为原始p个维度，新维度是$\\xi _1,....,\\xi _p$\n-\t新维度是原始维度的线性组合，表示为\n\t$$\n\t\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x\n\t$$\n-\t为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即\n\t$$\n\t\\alpha _i^T \\alpha _i=1\n\t$$\n-\t令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类\n-\t此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解\n-\t当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1\\-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度\n-\t依次求出p个新维度\n-\tPCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k<q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间\n-\t如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除\n-\tPCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取\t\n\n# SVM\n-\t在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面\n-\t划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移\n-\t求解一个SVM，即找到满足约束\n\t$$\n\t\\begin{cases}\n\tw^Tx_i+b \\geq +1, y_i=+1 \\\\\n\tw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\t\\end{cases}\n\t$$\n\t的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大\n\t这可以重写为最优化问题\n\t$$\n\tmin_{w,b} \\frac 12 {||w||}^2 \\\\\n\ts.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\\n\t$$\n\t推导见:[线性可分支持向量机与硬间隔最大化](http://thinkwee.top/2017/02/12/StatisticalLearningNote/#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96)\n-\t对于这个最优化问题，它的拉格朗日方程是\n\t$$\n\tL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))\n\t$$\n\t其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题\n\t$$\n\tmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\n\ts.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\t\\alpha _i \\geq 0,i=1,2,...,m \\\\\n\t$$\n\t上式满足KTT条件，通过SMO算法求解\n-\t未完待续","slug":"Lagrange","published":1,"updated":"2018-08-28T09:43:27.016Z","comments":1,"layout":"post","link":"","_id":"cjmd072ce0007qcw6rbhpipf9","content":"<hr><p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br>图片来自wikipedia关于拉格朗日乘子法的形象介绍<br><a id=\"more\"></a></p><h1 id=\"拉格朗日乘子法\"><a href=\"#拉格朗日乘子法\" class=\"headerlink\" title=\"拉格朗日乘子法\"></a>拉格朗日乘子法</h1><ul><li>拉格朗日乘子法是一种求约束条件下极值的方法，描述为<br>$$<br>在约束条件g(x,y)=c下 \\\\<br>求函数f(x,y)的极值 \\\\<br>$$<br>其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。</li><li>由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。</li><li>显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为<br>$$<br>\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))<br>$$<br>$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。<br>拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $</li><li>求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。</li><li>拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$</li></ul><h1 id=\"卡罗需-库恩-塔克条件\"><a href=\"#卡罗需-库恩-塔克条件\" class=\"headerlink\" title=\"卡罗需-库恩-塔克条件\"></a>卡罗需-库恩-塔克条件</h1><ul><li>如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件</li><li>包含不等约束的极值问题描述为<br>$$<br>在约束条件: \\\\<br>h_j(X)=0 j=1,2,…,p \\\\<br>g_k(X)\\leq 0 k=1,2,…q \\\\<br>求函数f(X)的极值 \\\\<br>$$</li><li>拉格朗日函数为<br>$$<br>L(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)<br>$$</li><li>KTT条件为:<br>$$<br>\\frac{dL}{dX}=0 \\\\<br>\\lambda _j \\neq 0 \\\\<br>\\mu _k \\geq 0 \\\\<br>\\mu _k g_k(X)=0 \\\\<br>h_j(X)=0 \\\\<br>g_k(X) \\leq 0\\\\<br>$$</li></ul><h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><ul><li>PCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本</li><li>记$x_1,…,x_p$为原始p个维度，新维度是$\\xi _1,….,\\xi _p$</li><li>新维度是原始维度的线性组合，表示为<br>$$<br>\\xi _i = \\sum _{j=1}^{p} \\alpha _{ij} x_j = \\alpha _i^T x<br>$$</li><li>为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即<br>$$<br>\\alpha _i^T \\alpha _i=1<br>$$</li><li>令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类</li><li>此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解</li><li>当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度</li><li>依次求出p个新维度</li><li>PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k&lt;q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间</li><li>如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除</li><li>PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取</li></ul><h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><ul><li>在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面</li><li>划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移</li><li>求解一个SVM，即找到满足约束<br>$$<br>\\begin{cases}<br>w^Tx_i+b \\geq +1, y_i=+1 \\\\<br>w^Tx_i+b \\leq -1, y_i=-1 \\\\<br>\\end{cases}<br>$$<br>的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大<br>这可以重写为最优化问题<br>$$<br>min_{w,b} \\frac 12 {||w||}^2 \\\\<br>s.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,…,m \\\\<br>$$<br>推导见:<a href=\"http://thinkwee.top/2017/02/12/StatisticalLearningNote/#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96\">线性可分支持向量机与硬间隔最大化</a></li><li>对于这个最优化问题，它的拉格朗日方程是<br>$$<br>L(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))<br>$$<br>其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题<br>$$<br>max _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\<br>s.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\<br>\\alpha _i \\geq 0,i=1,2,…,m \\\\<br>$$<br>上式满足KTT条件，通过SMO算法求解</li><li>未完待续</li></ul>","site":{"data":{}},"excerpt":"<hr><p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br>图片来自wikipedia关于拉格朗日乘子法的形象介绍<br>","more":"</p><h1 id=\"拉格朗日乘子法\"><a href=\"#拉格朗日乘子法\" class=\"headerlink\" title=\"拉格朗日乘子法\"></a>拉格朗日乘子法</h1><ul><li>拉格朗日乘子法是一种求约束条件下极值的方法，描述为<br>$$<br>在约束条件g(x,y)=c下 \\\\<br>求函数f(x,y)的极值 \\\\<br>$$<br>其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。</li><li>由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。</li><li>显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为<br>$$<br>\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))<br>$$<br>$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。<br>拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $</li><li>求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。</li><li>拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$</li></ul><h1 id=\"卡罗需-库恩-塔克条件\"><a href=\"#卡罗需-库恩-塔克条件\" class=\"headerlink\" title=\"卡罗需-库恩-塔克条件\"></a>卡罗需-库恩-塔克条件</h1><ul><li>如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件</li><li>包含不等约束的极值问题描述为<br>$$<br>在约束条件: \\\\<br>h_j(X)=0 j=1,2,…,p \\\\<br>g_k(X)\\leq 0 k=1,2,…q \\\\<br>求函数f(X)的极值 \\\\<br>$$</li><li>拉格朗日函数为<br>$$<br>L(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)<br>$$</li><li>KTT条件为:<br>$$<br>\\frac{dL}{dX}=0 \\\\<br>\\lambda _j \\neq 0 \\\\<br>\\mu _k \\geq 0 \\\\<br>\\mu _k g_k(X)=0 \\\\<br>h_j(X)=0 \\\\<br>g_k(X) \\leq 0\\\\<br>$$</li></ul><h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><ul><li>PCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本</li><li>记$x_1,…,x_p$为原始p个维度，新维度是$\\xi _1,….,\\xi _p$</li><li>新维度是原始维度的线性组合，表示为<br>$$<br>\\xi _i = \\sum _{j=1}^{p} \\alpha _{ij} x_j = \\alpha _i^T x<br>$$</li><li>为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即<br>$$<br>\\alpha _i^T \\alpha _i=1<br>$$</li><li>令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类</li><li>此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解</li><li>当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度</li><li>依次求出p个新维度</li><li>PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k&lt;q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间</li><li>如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除</li><li>PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取</li></ul><h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><ul><li>在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面</li><li>划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移</li><li>求解一个SVM，即找到满足约束<br>$$<br>\\begin{cases}<br>w^Tx_i+b \\geq +1, y_i=+1 \\\\<br>w^Tx_i+b \\leq -1, y_i=-1 \\\\<br>\\end{cases}<br>$$<br>的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大<br>这可以重写为最优化问题<br>$$<br>min_{w,b} \\frac 12 {||w||}^2 \\\\<br>s.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,…,m \\\\<br>$$<br>推导见:<a href=\"http://thinkwee.top/2017/02/12/StatisticalLearningNote/#%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%A1%AC%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96\">线性可分支持向量机与硬间隔最大化</a></li><li>对于这个最优化问题，它的拉格朗日方程是<br>$$<br>L(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))<br>$$<br>其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题<br>$$<br>max _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\<br>s.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\<br>\\alpha _i \\geq 0,i=1,2,…,m \\\\<br>$$<br>上式满足KTT条件，通过SMO算法求解</li><li>未完待续</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Tue Aug 28 2018 17:43:27 GMT+0800 (中国标准时间)","title":"Lagrange,KTT,PCA,SVM","path":"2017/03/18/Lagrange/","eyeCatchImage":null,"excerpt":"<hr><p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br>图片来自wikipedia关于拉格朗日乘子法的形象介绍<br>","date":"2017-03-18T03:20:35.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记3","date":"2017-01-22T11:21:02.000Z","mathjax":true,"html":true,"_content":"\n***\n# 第十七讲：行列式及其性质\n\n## 行列式\n-\t矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$\n-\t行列式的性质\n -\t$detI=1$\n -\t交换行，行列式的值的符号会相反\n -\t一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶\n -\t两行相等使得行列式为0(由性质二可以直接推出)\n -\t矩阵消元不改变其行列式(证明见下)\n -\t某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)\n -\t$detA=0$当且仅当A是奇异矩阵\n -\t$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$\n -\t$detA^{-1}detA=1$\n -\t$detA^2=(detA)^2$\n -\t$det2A=2^n detA$\n -\t$detA^T=detA$(证明见下)\n<!--more-->\n-\t行列式按行是线性的，但行列式本身不是线性的\n\t$$\n\t\\begin{vmatrix}\n\t1 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{vmatrix}=1 \\\\\n\t\\begin{vmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{vmatrix}=-1 \\\\\n\t\\begin{vmatrix}\n\tta & tb \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\tt\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix} \\\\\n\t\\begin{vmatrix}\n\tt+a & t+b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\tt & t \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明消元不改变行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc-la & d-lb \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}-l\n\t\\begin{vmatrix}\n\ta & b \\\\\n\ta & b \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明转置不改变行列式\n\t$$\n\tA=LU \\\\\n\t$$\n-\t即证 $|U^TL^T|=|LU|$\n$$\n|U^T||L^T|=|L||U|\n$$ \n-\t以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 \n\t\n\t\n## 三角阵行列式\n-\t对三角阵U的行列式,值为对角线上元素乘积(主元乘积)\n-\t为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式\n-\t为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3...d_nI$，其中单位矩阵的行列式为1\n-\t奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积\n\n## A little more\n-\t进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的\n\t\n# 第十八讲：行列式公式和代数余子式\n\n## 行列式公式\n-\t推导2*2行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\t0 & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\t0 & d \\\\\n\t\\end{vmatrix} \\\\\n\t=0+ad-bc+0\n\t$$\n\t我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案\n-\t如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。\n-\t例如\n\t$$\n\t\\begin{vmatrix}\n\ta & 0 & 0\\\\\n\t0 & 0 & b\\\\\n\t0 & c & 0\\\\\n\t\\end{vmatrix}\n\t$$\n\t先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$\n-\tn*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分\n-\t行列式公式就是这$n!$个部分加起来\n\n\n## 代数余子式\n-\t$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n-\t提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式\n-\t从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式\n-\t$a_{ij}$的代数余子式记作$c_{ij}$\n-\t注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号\n-\t$detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}$\t\n\n# 第十九讲：克拉默法则，逆矩阵，体积\n\n## 逆矩阵\n-\t只有行列式不为0时，矩阵才是可逆的\n-\t逆矩阵公式\n\t$$\n\tA^{-1}=\\frac{1}{detA}C^T\n\t$$\n\t其中$C_{ij}$是$A_{ij}$的代数余子式\n-\t证明：即证$AC^T=(detA)I$\n\t$$\n\t\\begin{bmatrix}\n\ta_{11} & ... & a_{1n} \\\\\n\ta_{n1} & ... & a_{nn} \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tc_{11} & ... & c_{n1} \\\\\n\tc_{1n} & ... & c_{nn} \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\tdetA & 0 & 0 \\\\\n\t0 & detA & 0 \\\\\n\t0 & 0 & detA \\\\\n\t\\end{bmatrix}\n\t$$\n\t对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n\t其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0\n\t\n## 克拉默法则\n-\t解Ax=b\n\t$$\n\tAx=b \\\\\n\tx=A^{-1}b \\\\\n\tx=\\frac{1}{detA}C^Tb \\\\\n\t \\\\\n\tx_1=\\frac{detB_1}{detA} \\\\\n\tx_3=\\frac{detB_2}{detA} \\\\\n\t... \\\\\n\t$$\n-\t克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变\n\n## 体积\n-\tA的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积\n-\t矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。\n-\t(1)单位矩阵对应单位立方体，体积为1\n-\t对正交矩阵Q,\n\t$$\n\tQQ^T=I \\\\\n\t|QQ^T|=|I| \\\\\n\t|Q||Q^T|=1 \\\\\n\t{|Q|}^2=1 \\\\\n\t|Q|=1 \\\\\n\t$$\n\tQ对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度\n-\t(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍\n-\t(2)交换矩阵两行，盒子的体积不变\n-\t(3b)矩阵某一行拆分，盒子也相应切分为两部分\n-\t以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证\n\t\n# 第二十讲：特征值和特征向量\n\n## 特征向量\n-\t给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax\n-\t当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值\n-\t如果A是奇异矩阵，$\\lambda = 0$是一个特征值\n\n## 几个例子\n-\t如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.\n-\t再举一例\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =1, x=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =-1, x=\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix} \\\\\t\n\t$$\n-\tn*n矩阵有n个特征值\n-\t特征值的和等于对角线元素和，这个和称为迹(trace)，\n-\t如何求解$Ax=\\lambda x$\n\t$$\n\t(A-\\lambda I)x=0 \\\\\n\t$$\n-\t可见方程有非零解，$(A-\\lambda I)$必须是奇异的 \n\t即: \n\t$$\n\tdet(A-\\lambda I)=0 \\\\\n\t$$\n-\t$$\n\tIf \\qquad Ax=\\lambda x \\\\\n\tThen \\qquad (A+3I)x=(\\lambda +3)x \\\\\n\t$$\n-\t因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$\n-\tA+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积\n-\t再举一例，对旋转矩阵Q\n\t$$\n\tQ=\n\t\\begin{bmatrix}\n\t0 & -1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\ttrace=0=\\lambda _1 +\\lambda _2 \\\\\n\tdet=1=\\lambda _1 \\lambda _2 \\\\\n\t$$\n-\t但是可以看出 $\\lambda _1，\\lambda _2$无实数解 \n-\t再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t3 & 1 \\\\\n\t0 & 3 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t3-\\lambda & 1 \\\\\n\t0 & 3-\\lambda \\\\\n\t\\end{vmatrix}\n\t==(3-\\lambda )^2=0 \\\\\n\t\\lambda _1=\\lambda _2=3 \\\\\n\tx_1=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t$$\n\t\n# 第二十一讲：对角化和A的幂\n\n## 对角化\n-\t假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵\n-\t以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下\n-\t$$\n\tAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n\t=[x_1,x_2,...x_n]\n\t\\begin{bmatrix}\n\t\\lambda _1 & 0 & ... & 0 \\\\\n\t0 & \\lambda _2 & ... & 0 \\\\\n\t... & ... & ... & ... \\\\\n\t0 & 0  & 0 & \\lambda _n \\\\\n\t\\end{bmatrix} \\\\\n\t=S \\Lambda \\\\\n\t$$\n\n\n-\t假设S可逆，即n个特征向量无关，此时可以得到\n\t$$\n\tS^{-1}AS=\\Lambda \\\\\n\tA=S\\Lambda S^{-1} \\\\\n\t$$\n-\t$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解\n-\t$$\n\tif \\qquad Ax=\\lambda x \\\\\n\tA^2 x=\\lambda AX=\\lambda ^2 x \\\\\n\tA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\\n\t$$\n-\t上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理\n-\t特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式\n-\t什么样的矩阵的幂趋向于0(稳定)\n\t$$\n\tA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\n\tif \\quad all |\\lambda _i|<1 \\\\ \n\t$$\n-\t哪些矩阵可以对角化？\n\t如果所有特征值不同，则A可以对角化\n-\t如果矩阵A已经是对角阵，则$\\Lambda$与A相同\n-\t特征值重复的次数称为代数重度，对三角阵，如\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t2 & 1 \\\\\n\t0 & 2 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t2-\\lambda & 1 \\\\\n\t0 & 2-\\lambda \\\\\n\t\\end{vmatrix}=0 \\\\\n\t\\lambda =2 \\\\\n\tA-\\lambda I=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t0 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2\n-\t特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。\n\n## A的幂\n-\t多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂\n-\t$$\n\tgive \\quad u_0 \\\\\n\tu_{k+1}=Au_k \\\\\n\tu_k=A^ku_0 \\\\\n\thow \\quad to \\quad solve \\quad u_k \\\\\n\tu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\n\tAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\n\tA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n\t=S\\Lambda ^{100} C \\\\\n\t=u_{100} \\\\\n\t$$\n-\t因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例\n\t$$\n\tF_0=0 \\\\\n\tF_1=1 \\\\\n\tF_2=1 \\\\\n\tF_3=2 \\\\\n\tF_4=3 \\\\\n\tF_5=5 \\\\\n\t..... \\\\\n\tF_{100}=? \\\\\n\t$$\n-\t斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系\n\t$$\n\tF_{k+2}=F_{k+1}+F_k \\\\\n\tF_{k+1}=F_{k+1} \\\\\n\t$$\n-\t定义向量\n\t$$\n\tu_k=\n\t\\begin{bmatrix}\n\tF_{k+1} \\\\\n\tF_k \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t利用这个向量可以将前两个等式写成矩阵形式 \n\t$$\n\tu_{k+1}=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix}\n\tu_k \\\\\n\tA=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\\n\t$$\n-\t得到两个特征值，我们很容易得到特征向量\n-\t回到斐波那契数列，斐波那契数列的增长速率由我们构造的\"数列更新矩阵\"的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +...+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式\n\t$$\n\tF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\\n\t$$\n-\t再有初始值有\n\t$$\n\tu_0=\n\t\\begin{bmatrix}\n\tF_1 \\\\\n\tF_0 \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t=c_1x_1+c_2x_2\n\t$$\n-\t其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值\n\n## 总结\n-\t我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式\n-\t这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂\n-\t我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式\n-\t求出矩阵的特征值，特征向量\n-\t由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式\n-\t由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值\n-\t以上是差分方程的一个例子，下一节将讨论微分方程\n\n# 第二十二讲：微分方程和exp(At)\n\n## 微分方程\n-\t常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解\n-\t举个例子\n\t$$\n\t\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\t\\frac{du_2}{dt}=u_1-2u_2 \\\\\n\tu(0)=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t首先我们列出系数矩阵，并找出矩阵的特征值和特征向量\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t-1 & 2 \\\\\n\t1 & -2 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量\n\t$$\n\tx_1=\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\tx_2=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t微分方程解的通解形式将是\n\t$$\n\tu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2\n\t$$\n-\t为什么？\n\t$$\n\t\\frac{du}{dt} \\\\\n\t=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n\t=A c_1 e^{\\lambda _1 t}x_1 \\\\\n\tbecause \\quad A x_1=\\lambda _1 x_1 \\\\\n\t$$\n-\t在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$\n-\t在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$\n-\t$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值\n-\t可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$\n-\t什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0\n-\t如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0\n-\t什么时候存在稳态？特征值中只存在0和负数，就如上面的例子\n-\t什么时候解无法收敛？任何特征值的实数部分大于0\n-\t改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散\n-\t如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？\n-\t矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如\n\t$$\n\t\\begin{bmatrix}\n\t-2 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0\n\t\n## exp(At)\t\n-\t是否可以把解表示成$S,\\Lambda$的形式\n-\t矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦\n-\t$$\n\t\\frac{du}{dt} = Au \\\\\n\tset \\quad u=Sv \\\\\n\tS \\frac{dv}{dt} = ASv \\\\\n\t\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\n\tv(t)=e^{\\Lambda t}v(0) \\\\\n\tu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\\n\t$$\n\t\n# 第二十一讲：马尔科夫矩阵;傅立叶级数\n\n## 马尔科夫矩阵\n-\t一个典型的马尔科夫矩阵\n\t$$\n\t\\begin{bmatrix}\n\t0.1 & 0.01 & 0.3 \\\\\n\t0.2 & 0.99 & 0.3 \\\\\n\t0.7 & 0 & 0.4 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵\n-\t$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1\n\n\n-\t在上一讲中我们谈到矩阵的幂可以分解为\n\t$$\n\tu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....\n\t$$\n-\t当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0\n-\t当每一列和为1时，必然存在一个特征值$\\lambda =1$\n-\t证明：\n\t$$\n\tA-I=\n\t\\begin{bmatrix}\n\t-0.9 & 0.01 & 0.3 \\\\\n\t0.2 & -0.01 & 0.3 \\\\\n\t0.7 & 0 & -0.6 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。\n-\t对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$\n-\t一个例子，u是麻省和加州的人数，A是人口流动矩阵\n\t$$\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k+1}\n\t=\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k}\n\t$$\n-\t可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省\n-\t对马尔科夫矩阵A\n\t$$\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda _1 =1 \\\\\n\t\\lambda _2 =0.7 \\\\\n\t$$\n-\t对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)\n-\t得到我们要研究的公式\n\t$$\n\tu_k=c_1\\*1^k\\*\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t+c_2\\*(0.7)^k\\*\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。\n-\t行向量为和为1是另外一种定义马尔科夫矩阵的方式\n\n## 傅里叶级数\n-\t先讨论带有标准正交基的投影问题\n-\t假设$q_1....q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合\n-\t现在我们要求出线性组合系数$x_1....x_n$\n\t$v=x_1q_1+x_2q_2+...x_nq_n$\n\t一种方法是将$v$与$q_i$做内积，逐一求出系数\n\t$$\n\tq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\\n\t$$\n-\t写成矩阵形式\n\t$$\n\t\\begin{bmatrix}\n\tq_1 & q_2 & ... & q_n \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\t... \\\\\n\tx_n \\\\\n\t\\end{bmatrix}=\n\tv \\\\\n\tQx=v \\\\\n\tx=Q^{-1}v=Q^Tv \\\\\n\t$$\n-\t现在讨论傅里叶级数\n-\t我们希望将函数分解\n\t$$\n\tf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......\n\t$$\n-\t关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。\n-\t如何求出傅里叶系数？\n-\t利用之前的向量例子来求\n-\t将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如\n\t$$\n\t\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\\n\t$$\n\n# 第二十二讲：对称矩阵及其正定性\n\n## 对称矩阵\n-\t对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交\n-\t对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵\n-\t对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵\n-\t为什么特征值都是实数？\n-\t$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{\\*}=\\lambda ^{\\*} x^{\\*}$\n-\t即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{\\* T}A^T=x^{\\* T} \\lambda ^{\\* T} $\n-\t上式中$A=A^T$，且两边同乘以$x$，与$x^{\\* T}A\\lambda x^{\\* T}x$对比可得$\\lambda ^{\\*}=\\lambda$，即特征值是实数\n-\t可见，对于复数矩阵，需要$A=A^{\\* T}$才满足对称\n-\t对于对称矩阵\n\t$$\n\tA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n\t=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\\n\t$$\n-\t所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合\n-\t对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式\n\n## 正定性\n-\t正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数\n-\t特征值的符号与稳定性有关\n-\t主元、行列式、特征值三位一体，线性代数将其统一\n\n\n\n\n\n","source":"_posts/LinearAlgebra3.md","raw":"---\ntitle: MIT线性代数笔记3\ndate: 2017-01-22 19:21:02\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# 第十七讲：行列式及其性质\n\n## 行列式\n-\t矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$\n-\t行列式的性质\n -\t$detI=1$\n -\t交换行，行列式的值的符号会相反\n -\t一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶\n -\t两行相等使得行列式为0(由性质二可以直接推出)\n -\t矩阵消元不改变其行列式(证明见下)\n -\t某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)\n -\t$detA=0$当且仅当A是奇异矩阵\n -\t$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$\n -\t$detA^{-1}detA=1$\n -\t$detA^2=(detA)^2$\n -\t$det2A=2^n detA$\n -\t$detA^T=detA$(证明见下)\n<!--more-->\n-\t行列式按行是线性的，但行列式本身不是线性的\n\t$$\n\t\\begin{vmatrix}\n\t1 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{vmatrix}=1 \\\\\n\t\\begin{vmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{vmatrix}=-1 \\\\\n\t\\begin{vmatrix}\n\tta & tb \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\tt\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix} \\\\\n\t\\begin{vmatrix}\n\tt+a & t+b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\tt & t \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明消元不改变行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc-la & d-lb \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}-l\n\t\\begin{vmatrix}\n\ta & b \\\\\n\ta & b \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明转置不改变行列式\n\t$$\n\tA=LU \\\\\n\t$$\n-\t即证 $|U^TL^T|=|LU|$\n$$\n|U^T||L^T|=|L||U|\n$$ \n-\t以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 \n\t\n\t\n## 三角阵行列式\n-\t对三角阵U的行列式,值为对角线上元素乘积(主元乘积)\n-\t为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式\n-\t为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3...d_nI$，其中单位矩阵的行列式为1\n-\t奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积\n\n## A little more\n-\t进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的\n\t\n# 第十八讲：行列式公式和代数余子式\n\n## 行列式公式\n-\t推导2*2行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\t0 & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\t0 & d \\\\\n\t\\end{vmatrix} \\\\\n\t=0+ad-bc+0\n\t$$\n\t我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案\n-\t如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。\n-\t例如\n\t$$\n\t\\begin{vmatrix}\n\ta & 0 & 0\\\\\n\t0 & 0 & b\\\\\n\t0 & c & 0\\\\\n\t\\end{vmatrix}\n\t$$\n\t先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$\n-\tn*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分\n-\t行列式公式就是这$n!$个部分加起来\n\n\n## 代数余子式\n-\t$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n-\t提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式\n-\t从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式\n-\t$a_{ij}$的代数余子式记作$c_{ij}$\n-\t注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号\n-\t$detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}$\t\n\n# 第十九讲：克拉默法则，逆矩阵，体积\n\n## 逆矩阵\n-\t只有行列式不为0时，矩阵才是可逆的\n-\t逆矩阵公式\n\t$$\n\tA^{-1}=\\frac{1}{detA}C^T\n\t$$\n\t其中$C_{ij}$是$A_{ij}$的代数余子式\n-\t证明：即证$AC^T=(detA)I$\n\t$$\n\t\\begin{bmatrix}\n\ta_{11} & ... & a_{1n} \\\\\n\ta_{n1} & ... & a_{nn} \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tc_{11} & ... & c_{n1} \\\\\n\tc_{1n} & ... & c_{nn} \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\tdetA & 0 & 0 \\\\\n\t0 & detA & 0 \\\\\n\t0 & 0 & detA \\\\\n\t\\end{bmatrix}\n\t$$\n\t对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n\t其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0\n\t\n## 克拉默法则\n-\t解Ax=b\n\t$$\n\tAx=b \\\\\n\tx=A^{-1}b \\\\\n\tx=\\frac{1}{detA}C^Tb \\\\\n\t \\\\\n\tx_1=\\frac{detB_1}{detA} \\\\\n\tx_3=\\frac{detB_2}{detA} \\\\\n\t... \\\\\n\t$$\n-\t克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变\n\n## 体积\n-\tA的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积\n-\t矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。\n-\t(1)单位矩阵对应单位立方体，体积为1\n-\t对正交矩阵Q,\n\t$$\n\tQQ^T=I \\\\\n\t|QQ^T|=|I| \\\\\n\t|Q||Q^T|=1 \\\\\n\t{|Q|}^2=1 \\\\\n\t|Q|=1 \\\\\n\t$$\n\tQ对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度\n-\t(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍\n-\t(2)交换矩阵两行，盒子的体积不变\n-\t(3b)矩阵某一行拆分，盒子也相应切分为两部分\n-\t以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证\n\t\n# 第二十讲：特征值和特征向量\n\n## 特征向量\n-\t给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax\n-\t当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值\n-\t如果A是奇异矩阵，$\\lambda = 0$是一个特征值\n\n## 几个例子\n-\t如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.\n-\t再举一例\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =1, x=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =-1, x=\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix} \\\\\t\n\t$$\n-\tn*n矩阵有n个特征值\n-\t特征值的和等于对角线元素和，这个和称为迹(trace)，\n-\t如何求解$Ax=\\lambda x$\n\t$$\n\t(A-\\lambda I)x=0 \\\\\n\t$$\n-\t可见方程有非零解，$(A-\\lambda I)$必须是奇异的 \n\t即: \n\t$$\n\tdet(A-\\lambda I)=0 \\\\\n\t$$\n-\t$$\n\tIf \\qquad Ax=\\lambda x \\\\\n\tThen \\qquad (A+3I)x=(\\lambda +3)x \\\\\n\t$$\n-\t因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$\n-\tA+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积\n-\t再举一例，对旋转矩阵Q\n\t$$\n\tQ=\n\t\\begin{bmatrix}\n\t0 & -1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\ttrace=0=\\lambda _1 +\\lambda _2 \\\\\n\tdet=1=\\lambda _1 \\lambda _2 \\\\\n\t$$\n-\t但是可以看出 $\\lambda _1，\\lambda _2$无实数解 \n-\t再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t3 & 1 \\\\\n\t0 & 3 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t3-\\lambda & 1 \\\\\n\t0 & 3-\\lambda \\\\\n\t\\end{vmatrix}\n\t==(3-\\lambda )^2=0 \\\\\n\t\\lambda _1=\\lambda _2=3 \\\\\n\tx_1=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t$$\n\t\n# 第二十一讲：对角化和A的幂\n\n## 对角化\n-\t假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵\n-\t以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下\n-\t$$\n\tAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n\t=[x_1,x_2,...x_n]\n\t\\begin{bmatrix}\n\t\\lambda _1 & 0 & ... & 0 \\\\\n\t0 & \\lambda _2 & ... & 0 \\\\\n\t... & ... & ... & ... \\\\\n\t0 & 0  & 0 & \\lambda _n \\\\\n\t\\end{bmatrix} \\\\\n\t=S \\Lambda \\\\\n\t$$\n\n\n-\t假设S可逆，即n个特征向量无关，此时可以得到\n\t$$\n\tS^{-1}AS=\\Lambda \\\\\n\tA=S\\Lambda S^{-1} \\\\\n\t$$\n-\t$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解\n-\t$$\n\tif \\qquad Ax=\\lambda x \\\\\n\tA^2 x=\\lambda AX=\\lambda ^2 x \\\\\n\tA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\\n\t$$\n-\t上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理\n-\t特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式\n-\t什么样的矩阵的幂趋向于0(稳定)\n\t$$\n\tA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\n\tif \\quad all |\\lambda _i|<1 \\\\ \n\t$$\n-\t哪些矩阵可以对角化？\n\t如果所有特征值不同，则A可以对角化\n-\t如果矩阵A已经是对角阵，则$\\Lambda$与A相同\n-\t特征值重复的次数称为代数重度，对三角阵，如\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t2 & 1 \\\\\n\t0 & 2 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t2-\\lambda & 1 \\\\\n\t0 & 2-\\lambda \\\\\n\t\\end{vmatrix}=0 \\\\\n\t\\lambda =2 \\\\\n\tA-\\lambda I=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t0 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2\n-\t特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。\n\n## A的幂\n-\t多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂\n-\t$$\n\tgive \\quad u_0 \\\\\n\tu_{k+1}=Au_k \\\\\n\tu_k=A^ku_0 \\\\\n\thow \\quad to \\quad solve \\quad u_k \\\\\n\tu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\n\tAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\n\tA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n\t=S\\Lambda ^{100} C \\\\\n\t=u_{100} \\\\\n\t$$\n-\t因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例\n\t$$\n\tF_0=0 \\\\\n\tF_1=1 \\\\\n\tF_2=1 \\\\\n\tF_3=2 \\\\\n\tF_4=3 \\\\\n\tF_5=5 \\\\\n\t..... \\\\\n\tF_{100}=? \\\\\n\t$$\n-\t斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系\n\t$$\n\tF_{k+2}=F_{k+1}+F_k \\\\\n\tF_{k+1}=F_{k+1} \\\\\n\t$$\n-\t定义向量\n\t$$\n\tu_k=\n\t\\begin{bmatrix}\n\tF_{k+1} \\\\\n\tF_k \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t利用这个向量可以将前两个等式写成矩阵形式 \n\t$$\n\tu_{k+1}=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix}\n\tu_k \\\\\n\tA=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\\n\t$$\n-\t得到两个特征值，我们很容易得到特征向量\n-\t回到斐波那契数列，斐波那契数列的增长速率由我们构造的\"数列更新矩阵\"的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +...+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式\n\t$$\n\tF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\\n\t$$\n-\t再有初始值有\n\t$$\n\tu_0=\n\t\\begin{bmatrix}\n\tF_1 \\\\\n\tF_0 \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t=c_1x_1+c_2x_2\n\t$$\n-\t其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值\n\n## 总结\n-\t我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式\n-\t这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂\n-\t我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式\n-\t求出矩阵的特征值，特征向量\n-\t由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式\n-\t由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值\n-\t以上是差分方程的一个例子，下一节将讨论微分方程\n\n# 第二十二讲：微分方程和exp(At)\n\n## 微分方程\n-\t常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解\n-\t举个例子\n\t$$\n\t\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\t\\frac{du_2}{dt}=u_1-2u_2 \\\\\n\tu(0)=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t首先我们列出系数矩阵，并找出矩阵的特征值和特征向量\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t-1 & 2 \\\\\n\t1 & -2 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量\n\t$$\n\tx_1=\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\tx_2=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t微分方程解的通解形式将是\n\t$$\n\tu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2\n\t$$\n-\t为什么？\n\t$$\n\t\\frac{du}{dt} \\\\\n\t=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n\t=A c_1 e^{\\lambda _1 t}x_1 \\\\\n\tbecause \\quad A x_1=\\lambda _1 x_1 \\\\\n\t$$\n-\t在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$\n-\t在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$\n-\t$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值\n-\t可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$\n-\t什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0\n-\t如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0\n-\t什么时候存在稳态？特征值中只存在0和负数，就如上面的例子\n-\t什么时候解无法收敛？任何特征值的实数部分大于0\n-\t改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散\n-\t如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？\n-\t矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如\n\t$$\n\t\\begin{bmatrix}\n\t-2 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0\n\t\n## exp(At)\t\n-\t是否可以把解表示成$S,\\Lambda$的形式\n-\t矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦\n-\t$$\n\t\\frac{du}{dt} = Au \\\\\n\tset \\quad u=Sv \\\\\n\tS \\frac{dv}{dt} = ASv \\\\\n\t\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\n\tv(t)=e^{\\Lambda t}v(0) \\\\\n\tu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\\n\t$$\n\t\n# 第二十一讲：马尔科夫矩阵;傅立叶级数\n\n## 马尔科夫矩阵\n-\t一个典型的马尔科夫矩阵\n\t$$\n\t\\begin{bmatrix}\n\t0.1 & 0.01 & 0.3 \\\\\n\t0.2 & 0.99 & 0.3 \\\\\n\t0.7 & 0 & 0.4 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵\n-\t$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1\n\n\n-\t在上一讲中我们谈到矩阵的幂可以分解为\n\t$$\n\tu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....\n\t$$\n-\t当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0\n-\t当每一列和为1时，必然存在一个特征值$\\lambda =1$\n-\t证明：\n\t$$\n\tA-I=\n\t\\begin{bmatrix}\n\t-0.9 & 0.01 & 0.3 \\\\\n\t0.2 & -0.01 & 0.3 \\\\\n\t0.7 & 0 & -0.6 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。\n-\t对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$\n-\t一个例子，u是麻省和加州的人数，A是人口流动矩阵\n\t$$\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k+1}\n\t=\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k}\n\t$$\n-\t可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省\n-\t对马尔科夫矩阵A\n\t$$\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda _1 =1 \\\\\n\t\\lambda _2 =0.7 \\\\\n\t$$\n-\t对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)\n-\t得到我们要研究的公式\n\t$$\n\tu_k=c_1\\*1^k\\*\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t+c_2\\*(0.7)^k\\*\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。\n-\t行向量为和为1是另外一种定义马尔科夫矩阵的方式\n\n## 傅里叶级数\n-\t先讨论带有标准正交基的投影问题\n-\t假设$q_1....q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合\n-\t现在我们要求出线性组合系数$x_1....x_n$\n\t$v=x_1q_1+x_2q_2+...x_nq_n$\n\t一种方法是将$v$与$q_i$做内积，逐一求出系数\n\t$$\n\tq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\\n\t$$\n-\t写成矩阵形式\n\t$$\n\t\\begin{bmatrix}\n\tq_1 & q_2 & ... & q_n \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\t... \\\\\n\tx_n \\\\\n\t\\end{bmatrix}=\n\tv \\\\\n\tQx=v \\\\\n\tx=Q^{-1}v=Q^Tv \\\\\n\t$$\n-\t现在讨论傅里叶级数\n-\t我们希望将函数分解\n\t$$\n\tf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......\n\t$$\n-\t关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。\n-\t如何求出傅里叶系数？\n-\t利用之前的向量例子来求\n-\t将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如\n\t$$\n\t\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\\n\t$$\n\n# 第二十二讲：对称矩阵及其正定性\n\n## 对称矩阵\n-\t对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交\n-\t对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵\n-\t对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵\n-\t为什么特征值都是实数？\n-\t$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{\\*}=\\lambda ^{\\*} x^{\\*}$\n-\t即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{\\* T}A^T=x^{\\* T} \\lambda ^{\\* T} $\n-\t上式中$A=A^T$，且两边同乘以$x$，与$x^{\\* T}A\\lambda x^{\\* T}x$对比可得$\\lambda ^{\\*}=\\lambda$，即特征值是实数\n-\t可见，对于复数矩阵，需要$A=A^{\\* T}$才满足对称\n-\t对于对称矩阵\n\t$$\n\tA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n\t=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\\n\t$$\n-\t所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合\n-\t对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式\n\n## 正定性\n-\t正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数\n-\t特征值的符号与稳定性有关\n-\t主元、行列式、特征值三位一体，线性代数将其统一\n\n\n\n\n\n","slug":"LinearAlgebra3","published":1,"updated":"2018-09-18T13:44:27.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072cf0009qcw6pre1bacp","content":"<hr><h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul><li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li><li>行列式的性质<ul><li>$detI=1$</li><li>交换行，行列式的值的符号会相反</li><li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li><li>两行相等使得行列式为0(由性质二可以直接推出)</li><li>矩阵消元不改变其行列式(证明见下)</li><li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li><li>$detA=0$当且仅当A是奇异矩阵</li><li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li><li>$detA^{-1}detA=1$</li><li>$detA^2=(detA)^2$</li><li>$det2A=2^n detA$</li><li>$detA^T=detA$(证明见下)<a id=\"more\"></a></li></ul></li><li>行列式按行是线性的，但行列式本身不是线性的<br>$$<br>\\begin{vmatrix}<br>1 &amp; 0 \\\\<br>0 &amp; 1 \\\\<br>\\end{vmatrix}=1 \\\\<br>\\begin{vmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{vmatrix}=-1 \\\\<br>\\begin{vmatrix}<br>ta &amp; tb \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>t\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix} \\\\<br>\\begin{vmatrix}<br>t+a &amp; t+b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>t &amp; t \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}<br>$$</li><li>证明消元不改变行列式<br>$$<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c-la &amp; d-lb \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}-l<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>a &amp; b \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}<br>$$</li><li>证明转置不改变行列式<br>$$<br>A=LU \\\\<br>$$</li><li>即证 $|U^TL^T|=|LU|$<br>$$<br>|U^T||L^T|=|L||U|<br>$$</li><li>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等</li></ul><h2 id=\"三角阵行列式\"><a href=\"#三角阵行列式\" class=\"headerlink\" title=\"三角阵行列式\"></a>三角阵行列式</h2><ul><li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li><li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li><li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3…d_nI$，其中单位矩阵的行列式为1</li><li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li></ul><h2 id=\"A-little-more\"><a href=\"#A-little-more\" class=\"headerlink\" title=\"A little more\"></a>A little more</h2><ul><li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li></ul><h1 id=\"第十八讲：行列式公式和代数余子式\"><a href=\"#第十八讲：行列式公式和代数余子式\" class=\"headerlink\" title=\"第十八讲：行列式公式和代数余子式\"></a>第十八讲：行列式公式和代数余子式</h1><h2 id=\"行列式公式\"><a href=\"#行列式公式\" class=\"headerlink\" title=\"行列式公式\"></a>行列式公式</h2><ul><li>推导2*2行列式<br>$$<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>c &amp; 0 \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>0 &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>c &amp; 0 \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>0 &amp; d \\\\<br>\\end{vmatrix} \\\\<br>=0+ad-bc+0<br>$$<br>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</li><li>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</li><li>例如<br>$$<br>\\begin{vmatrix}<br>a &amp; 0 &amp; 0\\\\<br>0 &amp; 0 &amp; b\\\\<br>0 &amp; c &amp; 0\\\\<br>\\end{vmatrix}<br>$$<br>先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$</li><li>n*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分</li><li>行列式公式就是这$n!$个部分加起来</li></ul><h2 id=\"代数余子式\"><a href=\"#代数余子式\" class=\"headerlink\" title=\"代数余子式\"></a>代数余子式</h2><ul><li>$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$</li><li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li><li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li><li>$a_{ij}$的代数余子式记作$c_{ij}$</li><li>注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li><li>$detA=a_{11}C_{11}+a_{12}C_{12}+….+a_{1n}C_{1n}$</li></ul><h1 id=\"第十九讲：克拉默法则，逆矩阵，体积\"><a href=\"#第十九讲：克拉默法则，逆矩阵，体积\" class=\"headerlink\" title=\"第十九讲：克拉默法则，逆矩阵，体积\"></a>第十九讲：克拉默法则，逆矩阵，体积</h1><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul><li>只有行列式不为0时，矩阵才是可逆的</li><li>逆矩阵公式<br>$$<br>A^{-1}=\\frac{1}{detA}C^T<br>$$<br>其中$C_{ij}$是$A_{ij}$的代数余子式</li><li>证明：即证$AC^T=(detA)I$<br>$$<br>\\begin{bmatrix}<br>a_{11} &amp; … &amp; a_{1n} \\\\<br>a_{n1} &amp; … &amp; a_{nn} \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>c_{11} &amp; … &amp; c_{n1} \\\\<br>c_{1n} &amp; … &amp; c_{nn} \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>detA &amp; 0 &amp; 0 \\\\<br>0 &amp; detA &amp; 0 \\\\<br>0 &amp; 0 &amp; detA \\\\<br>\\end{bmatrix}<br>$$<br>对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$<br>其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</li></ul><h2 id=\"克拉默法则\"><a href=\"#克拉默法则\" class=\"headerlink\" title=\"克拉默法则\"></a>克拉默法则</h2><ul><li>解Ax=b<br>$$<br>Ax=b \\\\<br>x=A^{-1}b \\\\<br>x=\\frac{1}{detA}C^Tb \\\\<br>\\\\<br>x_1=\\frac{detB_1}{detA} \\\\<br>x_3=\\frac{detB_2}{detA} \\\\<br>… \\\\<br>$$</li><li>克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变</li></ul><h2 id=\"体积\"><a href=\"#体积\" class=\"headerlink\" title=\"体积\"></a>体积</h2><ul><li>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</li><li>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</li><li>(1)单位矩阵对应单位立方体，体积为1</li><li>对正交矩阵Q,<br>$$<br>QQ^T=I \\\\<br>|QQ^T|=|I| \\\\<br>|Q||Q^T|=1 \\\\<br>{|Q|}^2=1 \\\\<br>|Q|=1 \\\\<br>$$<br>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</li><li>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</li><li>(2)交换矩阵两行，盒子的体积不变</li><li>(3b)矩阵某一行拆分，盒子也相应切分为两部分</li><li>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</li></ul><h1 id=\"第二十讲：特征值和特征向量\"><a href=\"#第二十讲：特征值和特征向量\" class=\"headerlink\" title=\"第二十讲：特征值和特征向量\"></a>第二十讲：特征值和特征向量</h1><h2 id=\"特征向量\"><a href=\"#特征向量\" class=\"headerlink\" title=\"特征向量\"></a>特征向量</h2><ul><li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li><li>当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值</li><li>如果A是奇异矩阵，$\\lambda = 0$是一个特征值</li></ul><h2 id=\"几个例子\"><a href=\"#几个例子\" class=\"headerlink\" title=\"几个例子\"></a>几个例子</h2><ul><li>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</li><li>再举一例<br>$$<br>A=<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =1, x=<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>Ax=<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =-1, x=<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>Ax=<br>\\begin{bmatrix}<br>1 \\\\<br>-1 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>n*n矩阵有n个特征值</li><li>特征值的和等于对角线元素和，这个和称为迹(trace)，</li><li>如何求解$Ax=\\lambda x$<br>$$<br>(A-\\lambda I)x=0 \\\\<br>$$</li><li>可见方程有非零解，$(A-\\lambda I)$必须是奇异的<br>即:<br>$$<br>det(A-\\lambda I)=0 \\\\<br>$$</li><li>$$<br>If \\qquad Ax=\\lambda x \\\\<br>Then \\qquad (A+3I)x=(\\lambda +3)x \\\\<br>$$</li><li>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$</li><li>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</li><li>再举一例，对旋转矩阵Q<br>$$<br>Q=<br>\\begin{bmatrix}<br>0 &amp; -1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>trace=0=\\lambda _1 +\\lambda _2 \\\\<br>det=1=\\lambda _1 \\lambda _2 \\\\<br>$$</li><li>但是可以看出 $\\lambda _1，\\lambda _2$无实数解</li><li>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)<br>$$<br>A=<br>\\begin{bmatrix}<br>3 &amp; 1 \\\\<br>0 &amp; 3 \\\\<br>\\end{bmatrix} \\\\<br>det(A-\\lambda I)=<br>\\begin{vmatrix}<br>3-\\lambda &amp; 1 \\\\<br>0 &amp; 3-\\lambda \\\\<br>\\end{vmatrix}<br>==(3-\\lambda )^2=0 \\\\<br>\\lambda _1=\\lambda _2=3 \\\\<br>x_1=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li></ul><h1 id=\"第二十一讲：对角化和A的幂\"><a href=\"#第二十一讲：对角化和A的幂\" class=\"headerlink\" title=\"第二十一讲：对角化和A的幂\"></a>第二十一讲：对角化和A的幂</h1><h2 id=\"对角化\"><a href=\"#对角化\" class=\"headerlink\" title=\"对角化\"></a>对角化</h2><ul><li>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</li><li>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</li><li>$$<br>AS=A[x_1,x_2…x_n]=[\\lambda _1 x_1,….\\lambda _n x_n] \\\\<br>=[x_1,x_2,…x_n]<br>\\begin{bmatrix}<br>\\lambda _1 &amp; 0 &amp; … &amp; 0 \\\\<br>0 &amp; \\lambda _2 &amp; … &amp; 0 \\\\<br>… &amp; … &amp; … &amp; … \\\\<br>0 &amp; 0 &amp; 0 &amp; \\lambda _n \\\\<br>\\end{bmatrix} \\\\<br>=S \\Lambda \\\\<br>$$</li></ul><ul><li>假设S可逆，即n个特征向量无关，此时可以得到<br>$$<br>S^{-1}AS=\\Lambda \\\\<br>A=S\\Lambda S^{-1} \\\\<br>$$</li><li>$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解</li><li>$$<br>if \\qquad Ax=\\lambda x \\\\<br>A^2 x=\\lambda AX=\\lambda ^2 x \\\\<br>A^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\<br>$$</li><li>上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理</li><li>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</li><li>什么样的矩阵的幂趋向于0(稳定)<br>$$<br>A^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\<br>if \\quad all |\\lambda _i|&lt;1 \\\\<br>$$</li><li>哪些矩阵可以对角化？<br>如果所有特征值不同，则A可以对角化</li><li>如果矩阵A已经是对角阵，则$\\Lambda$与A相同</li><li>特征值重复的次数称为代数重度，对三角阵，如<br>$$<br>A=<br>\\begin{bmatrix}<br>2 &amp; 1 \\\\<br>0 &amp; 2 \\\\<br>\\end{bmatrix} \\\\<br>det(A-\\lambda I)=<br>\\begin{vmatrix}<br>2-\\lambda &amp; 1 \\\\<br>0 &amp; 2-\\lambda \\\\<br>\\end{vmatrix}=0 \\\\<br>\\lambda =2 \\\\<br>A-\\lambda I=<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2</li><li>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</li></ul><h2 id=\"A的幂\"><a href=\"#A的幂\" class=\"headerlink\" title=\"A的幂\"></a>A的幂</h2><ul><li>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</li><li>$$<br>give \\quad u_0 \\\\<br>u_{k+1}=Au_k \\\\<br>u_k=A^ku_0 \\\\<br>how \\quad to \\quad solve \\quad u_k \\\\<br>u_0=c_1x_1+c_2x_2+…+c_nx_n=SC \\\\<br>Au_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +…+c_n \\lambda _n x_n \\\\<br>A^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +…+c_n \\lambda _n^{100} x_n \\\\<br>=S\\Lambda ^{100} C \\\\<br>=u_{100} \\\\<br>$$</li><li>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例<br>$$<br>F_0=0 \\\\<br>F_1=1 \\\\<br>F_2=1 \\\\<br>F_3=2 \\\\<br>F_4=3 \\\\<br>F_5=5 \\\\<br>….. \\\\<br>F_{100}=? \\\\<br>$$</li><li>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系<br>$$<br>F_{k+2}=F_{k+1}+F_k \\\\<br>F_{k+1}=F_{k+1} \\\\<br>$$</li><li>定义向量<br>$$<br>u_k=<br>\\begin{bmatrix}<br>F_{k+1} \\\\<br>F_k \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>利用这个向量可以将前两个等式写成矩阵形式<br>$$<br>u_{k+1}=<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}<br>u_k \\\\<br>A=<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\<br>$$</li><li>得到两个特征值，我们很容易得到特征向量</li><li>回到斐波那契数列，斐波那契数列的增长速率由我们构造的”数列更新矩阵”的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +…+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式<br>$$<br>F_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\<br>$$</li><li>再有初始值有<br>$$<br>u_0=<br>\\begin{bmatrix}<br>F_1 \\\\<br>F_0 \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>=c_1x_1+c_2x_2<br>$$</li><li>其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式</li><li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li><li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li><li>求出矩阵的特征值，特征向量</li><li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式</li><li>由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值</li><li>以上是差分方程的一个例子，下一节将讨论微分方程</li></ul><h1 id=\"第二十二讲：微分方程和exp-At\"><a href=\"#第二十二讲：微分方程和exp-At\" class=\"headerlink\" title=\"第二十二讲：微分方程和exp(At)\"></a>第二十二讲：微分方程和exp(At)</h1><h2 id=\"微分方程\"><a href=\"#微分方程\" class=\"headerlink\" title=\"微分方程\"></a>微分方程</h2><ul><li>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</li><li>举个例子<br>$$<br>\\frac{du_1}{dt}=-u_1+2u_2 \\\\<br>\\frac{du_2}{dt}=u_1-2u_2 \\\\<br>u(0)=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量<br>$$<br>A=<br>\\begin{bmatrix}<br>-1 &amp; 2 \\\\<br>1 &amp; -2 \\\\<br>\\end{bmatrix}<br>$$</li><li>易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量<br>$$<br>x_1=<br>\\begin{bmatrix}<br>2 \\\\<br>1 \\\\<br>\\end{bmatrix} \\\\<br>x_2=<br>\\begin{bmatrix}<br>1 \\\\<br>-1 \\\\<br>\\end{bmatrix}<br>$$</li><li>微分方程解的通解形式将是<br>$$<br>u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2<br>$$</li><li>为什么？<br>$$<br>\\frac{du}{dt} \\\\<br>=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\<br>=A c_1 e^{\\lambda _1 t}x_1 \\\\<br>because \\quad A x_1=\\lambda _1 x_1 \\\\<br>$$</li><li>在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$</li><li>在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$</li><li>$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</li><li>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$</li><li>什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0</li><li>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</li><li>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</li><li>什么时候解无法收敛？任何特征值的实数部分大于0</li><li>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</li><li>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</li><li>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如<br>$$<br>\\begin{bmatrix}<br>-2 &amp; 0 \\\\<br>0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</li></ul><h2 id=\"exp-At\"><a href=\"#exp-At\" class=\"headerlink\" title=\"exp(At)\"></a>exp(At)</h2><ul><li>是否可以把解表示成$S,\\Lambda$的形式</li><li>矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦</li><li>$$<br>\\frac{du}{dt} = Au \\\\<br>set \\quad u=Sv \\\\<br>S \\frac{dv}{dt} = ASv \\\\<br>\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\<br>v(t)=e^{\\Lambda t}v(0) \\\\<br>u(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\<br>$$</li></ul><h1 id=\"第二十一讲：马尔科夫矩阵-傅立叶级数\"><a href=\"#第二十一讲：马尔科夫矩阵-傅立叶级数\" class=\"headerlink\" title=\"第二十一讲：马尔科夫矩阵;傅立叶级数\"></a>第二十一讲：马尔科夫矩阵;傅立叶级数</h1><h2 id=\"马尔科夫矩阵\"><a href=\"#马尔科夫矩阵\" class=\"headerlink\" title=\"马尔科夫矩阵\"></a>马尔科夫矩阵</h2><ul><li>一个典型的马尔科夫矩阵<br>$$<br>\\begin{bmatrix}<br>0.1 &amp; 0.01 &amp; 0.3 \\\\<br>0.2 &amp; 0.99 &amp; 0.3 \\\\<br>0.7 &amp; 0 &amp; 0.4 \\\\<br>\\end{bmatrix}<br>$$</li><li>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</li><li>$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1</li></ul><ul><li>在上一讲中我们谈到矩阵的幂可以分解为<br>$$<br>u_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+…..<br>$$</li><li>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</li><li>当每一列和为1时，必然存在一个特征值$\\lambda =1$</li><li>证明：<br>$$<br>A-I=<br>\\begin{bmatrix}<br>-0.9 &amp; 0.01 &amp; 0.3 \\\\<br>0.2 &amp; -0.01 &amp; 0.3 \\\\<br>0.7 &amp; 0 &amp; -0.6 \\\\<br>\\end{bmatrix}<br>$$</li><li>若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</li><li>对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$</li><li>一个例子，u是麻省和加州的人数，A是人口流动矩阵<br>$$<br>\\begin{bmatrix}<br>u_{cal} \\\\<br>u_{mass} \\\\<br>\\end{bmatrix}_{t=k+1}<br>=<br>\\begin{bmatrix}<br>0.9 &amp; 0.2 \\\\<br>0.1 &amp; 0.8 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>u_{cal} \\\\<br>u_{mass} \\\\<br>\\end{bmatrix}_{t=k}<br>$$</li><li>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</li><li>对马尔科夫矩阵A<br>$$<br>\\begin{bmatrix}<br>0.9 &amp; 0.2 \\\\<br>0.1 &amp; 0.8 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda _1 =1 \\\\<br>\\lambda _2 =0.7 \\\\<br>$$</li><li>对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)</li><li>得到我们要研究的公式<br>$$<br>u_k=c_1*1^k*<br>\\begin{bmatrix}<br>2 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>+c_2*(0.7)^k*<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</li><li>行向量为和为1是另外一种定义马尔科夫矩阵的方式</li></ul><h2 id=\"傅里叶级数\"><a href=\"#傅里叶级数\" class=\"headerlink\" title=\"傅里叶级数\"></a>傅里叶级数</h2><ul><li>先讨论带有标准正交基的投影问题</li><li>假设$q_1….q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合</li><li>现在我们要求出线性组合系数$x_1….x_n$<br>$v=x_1q_1+x_2q_2+…x_nq_n$<br>一种方法是将$v$与$q_i$做内积，逐一求出系数<br>$$<br>q_1^Tv=x_1q_1^Tq_1+0+0+0….+0=x_1 \\\\<br>$$</li><li>写成矩阵形式<br>$$<br>\\begin{bmatrix}<br>q_1 &amp; q_2 &amp; … &amp; q_n \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x_1 \\\\<br>x_2 \\\\<br>… \\\\<br>x_n \\\\<br>\\end{bmatrix}=<br>v \\\\<br>Qx=v \\\\<br>x=Q^{-1}v=Q^Tv \\\\<br>$$</li><li>现在讨论傅里叶级数</li><li>我们希望将函数分解<br>$$<br>f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+…….<br>$$</li><li>关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</li><li>如何求出傅里叶系数？</li><li>利用之前的向量例子来求</li><li>将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如<br>$$<br>\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0…+0=\\pi a_1 \\\\<br>$$</li></ul><h1 id=\"第二十二讲：对称矩阵及其正定性\"><a href=\"#第二十二讲：对称矩阵及其正定性\" class=\"headerlink\" title=\"第二十二讲：对称矩阵及其正定性\"></a>第二十二讲：对称矩阵及其正定性</h1><h2 id=\"对称矩阵\"><a href=\"#对称矩阵\" class=\"headerlink\" title=\"对称矩阵\"></a>对称矩阵</h2><ul><li>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</li><li>对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵</li><li>对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵</li><li>为什么特征值都是实数？</li><li>$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{*}=\\lambda ^{*} x^{*}$</li><li>即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{* T}A^T=x^{* T} \\lambda ^{* T} $</li><li>上式中$A=A^T$，且两边同乘以$x$，与$x^{* T}A\\lambda x^{* T}x$对比可得$\\lambda ^{*}=\\lambda$，即特征值是实数</li><li>可见，对于复数矩阵，需要$A=A^{* T}$才满足对称</li><li>对于对称矩阵<br>$$<br>A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\<br>=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+…. \\\\<br>$$</li><li>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</li><li>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</li></ul><h2 id=\"正定性\"><a href=\"#正定性\" class=\"headerlink\" title=\"正定性\"></a>正定性</h2><ul><li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li><li>特征值的符号与稳定性有关</li><li>主元、行列式、特征值三位一体，线性代数将其统一</li></ul>","site":{"data":{}},"excerpt":"<hr><h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul><li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li><li>行列式的性质<ul><li>$detI=1$</li><li>交换行，行列式的值的符号会相反</li><li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li><li>两行相等使得行列式为0(由性质二可以直接推出)</li><li>矩阵消元不改变其行列式(证明见下)</li><li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li><li>$detA=0$当且仅当A是奇异矩阵</li><li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li><li>$detA^{-1}detA=1$</li><li>$detA^2=(detA)^2$</li><li>$det2A=2^n detA$</li><li>$detA^T=detA$(证明见下)","more":"</li></ul></li><li>行列式按行是线性的，但行列式本身不是线性的<br>$$<br>\\begin{vmatrix}<br>1 &amp; 0 \\\\<br>0 &amp; 1 \\\\<br>\\end{vmatrix}=1 \\\\<br>\\begin{vmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{vmatrix}=-1 \\\\<br>\\begin{vmatrix}<br>ta &amp; tb \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>t\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix} \\\\<br>\\begin{vmatrix}<br>t+a &amp; t+b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>t &amp; t \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}<br>$$</li><li>证明消元不改变行列式<br>$$<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c-la &amp; d-lb \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}-l<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>a &amp; b \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}<br>$$</li><li>证明转置不改变行列式<br>$$<br>A=LU \\\\<br>$$</li><li>即证 $|U^TL^T|=|LU|$<br>$$<br>|U^T||L^T|=|L||U|<br>$$</li><li>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等</li></ul><h2 id=\"三角阵行列式\"><a href=\"#三角阵行列式\" class=\"headerlink\" title=\"三角阵行列式\"></a>三角阵行列式</h2><ul><li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li><li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li><li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3…d_nI$，其中单位矩阵的行列式为1</li><li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li></ul><h2 id=\"A-little-more\"><a href=\"#A-little-more\" class=\"headerlink\" title=\"A little more\"></a>A little more</h2><ul><li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li></ul><h1 id=\"第十八讲：行列式公式和代数余子式\"><a href=\"#第十八讲：行列式公式和代数余子式\" class=\"headerlink\" title=\"第十八讲：行列式公式和代数余子式\"></a>第十八讲：行列式公式和代数余子式</h1><h2 id=\"行列式公式\"><a href=\"#行列式公式\" class=\"headerlink\" title=\"行列式公式\"></a>行列式公式</h2><ul><li>推导2*2行列式<br>$$<br>\\begin{vmatrix}<br>a &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>c &amp; d \\\\<br>\\end{vmatrix}=<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>c &amp; 0 \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>a &amp; 0 \\\\<br>0 &amp; d \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>c &amp; 0 \\\\<br>\\end{vmatrix}+<br>\\begin{vmatrix}<br>0 &amp; b \\\\<br>0 &amp; d \\\\<br>\\end{vmatrix} \\\\<br>=0+ad-bc+0<br>$$<br>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</li><li>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</li><li>例如<br>$$<br>\\begin{vmatrix}<br>a &amp; 0 &amp; 0\\\\<br>0 &amp; 0 &amp; b\\\\<br>0 &amp; c &amp; 0\\\\<br>\\end{vmatrix}<br>$$<br>先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$</li><li>n*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分</li><li>行列式公式就是这$n!$个部分加起来</li></ul><h2 id=\"代数余子式\"><a href=\"#代数余子式\" class=\"headerlink\" title=\"代数余子式\"></a>代数余子式</h2><ul><li>$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$</li><li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li><li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li><li>$a_{ij}$的代数余子式记作$c_{ij}$</li><li>注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li><li>$detA=a_{11}C_{11}+a_{12}C_{12}+….+a_{1n}C_{1n}$</li></ul><h1 id=\"第十九讲：克拉默法则，逆矩阵，体积\"><a href=\"#第十九讲：克拉默法则，逆矩阵，体积\" class=\"headerlink\" title=\"第十九讲：克拉默法则，逆矩阵，体积\"></a>第十九讲：克拉默法则，逆矩阵，体积</h1><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul><li>只有行列式不为0时，矩阵才是可逆的</li><li>逆矩阵公式<br>$$<br>A^{-1}=\\frac{1}{detA}C^T<br>$$<br>其中$C_{ij}$是$A_{ij}$的代数余子式</li><li>证明：即证$AC^T=(detA)I$<br>$$<br>\\begin{bmatrix}<br>a_{11} &amp; … &amp; a_{1n} \\\\<br>a_{n1} &amp; … &amp; a_{nn} \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>c_{11} &amp; … &amp; c_{n1} \\\\<br>c_{1n} &amp; … &amp; c_{nn} \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>detA &amp; 0 &amp; 0 \\\\<br>0 &amp; detA &amp; 0 \\\\<br>0 &amp; 0 &amp; detA \\\\<br>\\end{bmatrix}<br>$$<br>对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$<br>其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</li></ul><h2 id=\"克拉默法则\"><a href=\"#克拉默法则\" class=\"headerlink\" title=\"克拉默法则\"></a>克拉默法则</h2><ul><li>解Ax=b<br>$$<br>Ax=b \\\\<br>x=A^{-1}b \\\\<br>x=\\frac{1}{detA}C^Tb \\\\<br>\\\\<br>x_1=\\frac{detB_1}{detA} \\\\<br>x_3=\\frac{detB_2}{detA} \\\\<br>… \\\\<br>$$</li><li>克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变</li></ul><h2 id=\"体积\"><a href=\"#体积\" class=\"headerlink\" title=\"体积\"></a>体积</h2><ul><li>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</li><li>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</li><li>(1)单位矩阵对应单位立方体，体积为1</li><li>对正交矩阵Q,<br>$$<br>QQ^T=I \\\\<br>|QQ^T|=|I| \\\\<br>|Q||Q^T|=1 \\\\<br>{|Q|}^2=1 \\\\<br>|Q|=1 \\\\<br>$$<br>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</li><li>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</li><li>(2)交换矩阵两行，盒子的体积不变</li><li>(3b)矩阵某一行拆分，盒子也相应切分为两部分</li><li>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</li></ul><h1 id=\"第二十讲：特征值和特征向量\"><a href=\"#第二十讲：特征值和特征向量\" class=\"headerlink\" title=\"第二十讲：特征值和特征向量\"></a>第二十讲：特征值和特征向量</h1><h2 id=\"特征向量\"><a href=\"#特征向量\" class=\"headerlink\" title=\"特征向量\"></a>特征向量</h2><ul><li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li><li>当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值</li><li>如果A是奇异矩阵，$\\lambda = 0$是一个特征值</li></ul><h2 id=\"几个例子\"><a href=\"#几个例子\" class=\"headerlink\" title=\"几个例子\"></a>几个例子</h2><ul><li>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</li><li>再举一例<br>$$<br>A=<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =1, x=<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>Ax=<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =-1, x=<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>Ax=<br>\\begin{bmatrix}<br>1 \\\\<br>-1 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>n*n矩阵有n个特征值</li><li>特征值的和等于对角线元素和，这个和称为迹(trace)，</li><li>如何求解$Ax=\\lambda x$<br>$$<br>(A-\\lambda I)x=0 \\\\<br>$$</li><li>可见方程有非零解，$(A-\\lambda I)$必须是奇异的<br>即:<br>$$<br>det(A-\\lambda I)=0 \\\\<br>$$</li><li>$$<br>If \\qquad Ax=\\lambda x \\\\<br>Then \\qquad (A+3I)x=(\\lambda +3)x \\\\<br>$$</li><li>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$</li><li>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</li><li>再举一例，对旋转矩阵Q<br>$$<br>Q=<br>\\begin{bmatrix}<br>0 &amp; -1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>trace=0=\\lambda _1 +\\lambda _2 \\\\<br>det=1=\\lambda _1 \\lambda _2 \\\\<br>$$</li><li>但是可以看出 $\\lambda _1，\\lambda _2$无实数解</li><li>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)<br>$$<br>A=<br>\\begin{bmatrix}<br>3 &amp; 1 \\\\<br>0 &amp; 3 \\\\<br>\\end{bmatrix} \\\\<br>det(A-\\lambda I)=<br>\\begin{vmatrix}<br>3-\\lambda &amp; 1 \\\\<br>0 &amp; 3-\\lambda \\\\<br>\\end{vmatrix}<br>==(3-\\lambda )^2=0 \\\\<br>\\lambda _1=\\lambda _2=3 \\\\<br>x_1=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>$$</li></ul><h1 id=\"第二十一讲：对角化和A的幂\"><a href=\"#第二十一讲：对角化和A的幂\" class=\"headerlink\" title=\"第二十一讲：对角化和A的幂\"></a>第二十一讲：对角化和A的幂</h1><h2 id=\"对角化\"><a href=\"#对角化\" class=\"headerlink\" title=\"对角化\"></a>对角化</h2><ul><li>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</li><li>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</li><li>$$<br>AS=A[x_1,x_2…x_n]=[\\lambda _1 x_1,….\\lambda _n x_n] \\\\<br>=[x_1,x_2,…x_n]<br>\\begin{bmatrix}<br>\\lambda _1 &amp; 0 &amp; … &amp; 0 \\\\<br>0 &amp; \\lambda _2 &amp; … &amp; 0 \\\\<br>… &amp; … &amp; … &amp; … \\\\<br>0 &amp; 0 &amp; 0 &amp; \\lambda _n \\\\<br>\\end{bmatrix} \\\\<br>=S \\Lambda \\\\<br>$$</li></ul><ul><li>假设S可逆，即n个特征向量无关，此时可以得到<br>$$<br>S^{-1}AS=\\Lambda \\\\<br>A=S\\Lambda S^{-1} \\\\<br>$$</li><li>$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解</li><li>$$<br>if \\qquad Ax=\\lambda x \\\\<br>A^2 x=\\lambda AX=\\lambda ^2 x \\\\<br>A^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\<br>$$</li><li>上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理</li><li>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</li><li>什么样的矩阵的幂趋向于0(稳定)<br>$$<br>A^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\<br>if \\quad all |\\lambda _i|&lt;1 \\\\<br>$$</li><li>哪些矩阵可以对角化？<br>如果所有特征值不同，则A可以对角化</li><li>如果矩阵A已经是对角阵，则$\\Lambda$与A相同</li><li>特征值重复的次数称为代数重度，对三角阵，如<br>$$<br>A=<br>\\begin{bmatrix}<br>2 &amp; 1 \\\\<br>0 &amp; 2 \\\\<br>\\end{bmatrix} \\\\<br>det(A-\\lambda I)=<br>\\begin{vmatrix}<br>2-\\lambda &amp; 1 \\\\<br>0 &amp; 2-\\lambda \\\\<br>\\end{vmatrix}=0 \\\\<br>\\lambda =2 \\\\<br>A-\\lambda I=<br>\\begin{bmatrix}<br>0 &amp; 1 \\\\<br>0 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2</li><li>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</li></ul><h2 id=\"A的幂\"><a href=\"#A的幂\" class=\"headerlink\" title=\"A的幂\"></a>A的幂</h2><ul><li>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</li><li>$$<br>give \\quad u_0 \\\\<br>u_{k+1}=Au_k \\\\<br>u_k=A^ku_0 \\\\<br>how \\quad to \\quad solve \\quad u_k \\\\<br>u_0=c_1x_1+c_2x_2+…+c_nx_n=SC \\\\<br>Au_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +…+c_n \\lambda _n x_n \\\\<br>A^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +…+c_n \\lambda _n^{100} x_n \\\\<br>=S\\Lambda ^{100} C \\\\<br>=u_{100} \\\\<br>$$</li><li>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例<br>$$<br>F_0=0 \\\\<br>F_1=1 \\\\<br>F_2=1 \\\\<br>F_3=2 \\\\<br>F_4=3 \\\\<br>F_5=5 \\\\<br>….. \\\\<br>F_{100}=? \\\\<br>$$</li><li>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系<br>$$<br>F_{k+2}=F_{k+1}+F_k \\\\<br>F_{k+1}=F_{k+1} \\\\<br>$$</li><li>定义向量<br>$$<br>u_k=<br>\\begin{bmatrix}<br>F_{k+1} \\\\<br>F_k \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>利用这个向量可以将前两个等式写成矩阵形式<br>$$<br>u_{k+1}=<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix}<br>u_k \\\\<br>A=<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 0 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\<br>$$</li><li>得到两个特征值，我们很容易得到特征向量</li><li>回到斐波那契数列，斐波那契数列的增长速率由我们构造的”数列更新矩阵”的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +…+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式<br>$$<br>F_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\<br>$$</li><li>再有初始值有<br>$$<br>u_0=<br>\\begin{bmatrix}<br>F_1 \\\\<br>F_0 \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>=c_1x_1+c_2x_2<br>$$</li><li>其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式</li><li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li><li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li><li>求出矩阵的特征值，特征向量</li><li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式</li><li>由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值</li><li>以上是差分方程的一个例子，下一节将讨论微分方程</li></ul><h1 id=\"第二十二讲：微分方程和exp-At\"><a href=\"#第二十二讲：微分方程和exp-At\" class=\"headerlink\" title=\"第二十二讲：微分方程和exp(At)\"></a>第二十二讲：微分方程和exp(At)</h1><h2 id=\"微分方程\"><a href=\"#微分方程\" class=\"headerlink\" title=\"微分方程\"></a>微分方程</h2><ul><li>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</li><li>举个例子<br>$$<br>\\frac{du_1}{dt}=-u_1+2u_2 \\\\<br>\\frac{du_2}{dt}=u_1-2u_2 \\\\<br>u(0)=<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量<br>$$<br>A=<br>\\begin{bmatrix}<br>-1 &amp; 2 \\\\<br>1 &amp; -2 \\\\<br>\\end{bmatrix}<br>$$</li><li>易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量<br>$$<br>x_1=<br>\\begin{bmatrix}<br>2 \\\\<br>1 \\\\<br>\\end{bmatrix} \\\\<br>x_2=<br>\\begin{bmatrix}<br>1 \\\\<br>-1 \\\\<br>\\end{bmatrix}<br>$$</li><li>微分方程解的通解形式将是<br>$$<br>u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2<br>$$</li><li>为什么？<br>$$<br>\\frac{du}{dt} \\\\<br>=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\<br>=A c_1 e^{\\lambda _1 t}x_1 \\\\<br>because \\quad A x_1=\\lambda _1 x_1 \\\\<br>$$</li><li>在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$</li><li>在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$</li><li>$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</li><li>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$</li><li>什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0</li><li>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</li><li>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</li><li>什么时候解无法收敛？任何特征值的实数部分大于0</li><li>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</li><li>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</li><li>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如<br>$$<br>\\begin{bmatrix}<br>-2 &amp; 0 \\\\<br>0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</li></ul><h2 id=\"exp-At\"><a href=\"#exp-At\" class=\"headerlink\" title=\"exp(At)\"></a>exp(At)</h2><ul><li>是否可以把解表示成$S,\\Lambda$的形式</li><li>矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦</li><li>$$<br>\\frac{du}{dt} = Au \\\\<br>set \\quad u=Sv \\\\<br>S \\frac{dv}{dt} = ASv \\\\<br>\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\<br>v(t)=e^{\\Lambda t}v(0) \\\\<br>u(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\<br>$$</li></ul><h1 id=\"第二十一讲：马尔科夫矩阵-傅立叶级数\"><a href=\"#第二十一讲：马尔科夫矩阵-傅立叶级数\" class=\"headerlink\" title=\"第二十一讲：马尔科夫矩阵;傅立叶级数\"></a>第二十一讲：马尔科夫矩阵;傅立叶级数</h1><h2 id=\"马尔科夫矩阵\"><a href=\"#马尔科夫矩阵\" class=\"headerlink\" title=\"马尔科夫矩阵\"></a>马尔科夫矩阵</h2><ul><li>一个典型的马尔科夫矩阵<br>$$<br>\\begin{bmatrix}<br>0.1 &amp; 0.01 &amp; 0.3 \\\\<br>0.2 &amp; 0.99 &amp; 0.3 \\\\<br>0.7 &amp; 0 &amp; 0.4 \\\\<br>\\end{bmatrix}<br>$$</li><li>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</li><li>$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1</li></ul><ul><li>在上一讲中我们谈到矩阵的幂可以分解为<br>$$<br>u_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+…..<br>$$</li><li>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</li><li>当每一列和为1时，必然存在一个特征值$\\lambda =1$</li><li>证明：<br>$$<br>A-I=<br>\\begin{bmatrix}<br>-0.9 &amp; 0.01 &amp; 0.3 \\\\<br>0.2 &amp; -0.01 &amp; 0.3 \\\\<br>0.7 &amp; 0 &amp; -0.6 \\\\<br>\\end{bmatrix}<br>$$</li><li>若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</li><li>对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$</li><li>一个例子，u是麻省和加州的人数，A是人口流动矩阵<br>$$<br>\\begin{bmatrix}<br>u_{cal} \\\\<br>u_{mass} \\\\<br>\\end{bmatrix}_{t=k+1}<br>=<br>\\begin{bmatrix}<br>0.9 &amp; 0.2 \\\\<br>0.1 &amp; 0.8 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>u_{cal} \\\\<br>u_{mass} \\\\<br>\\end{bmatrix}_{t=k}<br>$$</li><li>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</li><li>对马尔科夫矩阵A<br>$$<br>\\begin{bmatrix}<br>0.9 &amp; 0.2 \\\\<br>0.1 &amp; 0.8 \\\\<br>\\end{bmatrix} \\\\<br>\\lambda _1 =1 \\\\<br>\\lambda _2 =0.7 \\\\<br>$$</li><li>对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)</li><li>得到我们要研究的公式<br>$$<br>u_k=c_1*1^k*<br>\\begin{bmatrix}<br>2 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>+c_2*(0.7)^k*<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</li><li>行向量为和为1是另外一种定义马尔科夫矩阵的方式</li></ul><h2 id=\"傅里叶级数\"><a href=\"#傅里叶级数\" class=\"headerlink\" title=\"傅里叶级数\"></a>傅里叶级数</h2><ul><li>先讨论带有标准正交基的投影问题</li><li>假设$q_1….q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合</li><li>现在我们要求出线性组合系数$x_1….x_n$<br>$v=x_1q_1+x_2q_2+…x_nq_n$<br>一种方法是将$v$与$q_i$做内积，逐一求出系数<br>$$<br>q_1^Tv=x_1q_1^Tq_1+0+0+0….+0=x_1 \\\\<br>$$</li><li>写成矩阵形式<br>$$<br>\\begin{bmatrix}<br>q_1 &amp; q_2 &amp; … &amp; q_n \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>x_1 \\\\<br>x_2 \\\\<br>… \\\\<br>x_n \\\\<br>\\end{bmatrix}=<br>v \\\\<br>Qx=v \\\\<br>x=Q^{-1}v=Q^Tv \\\\<br>$$</li><li>现在讨论傅里叶级数</li><li>我们希望将函数分解<br>$$<br>f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+…….<br>$$</li><li>关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</li><li>如何求出傅里叶系数？</li><li>利用之前的向量例子来求</li><li>将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如<br>$$<br>\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0…+0=\\pi a_1 \\\\<br>$$</li></ul><h1 id=\"第二十二讲：对称矩阵及其正定性\"><a href=\"#第二十二讲：对称矩阵及其正定性\" class=\"headerlink\" title=\"第二十二讲：对称矩阵及其正定性\"></a>第二十二讲：对称矩阵及其正定性</h1><h2 id=\"对称矩阵\"><a href=\"#对称矩阵\" class=\"headerlink\" title=\"对称矩阵\"></a>对称矩阵</h2><ul><li>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</li><li>对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵</li><li>对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵</li><li>为什么特征值都是实数？</li><li>$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{*}=\\lambda ^{*} x^{*}$</li><li>即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{* T}A^T=x^{* T} \\lambda ^{* T} $</li><li>上式中$A=A^T$，且两边同乘以$x$，与$x^{* T}A\\lambda x^{* T}x$对比可得$\\lambda ^{*}=\\lambda$，即特征值是实数</li><li>可见，对于复数矩阵，需要$A=A^{* T}$才满足对称</li><li>对于对称矩阵<br>$$<br>A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\<br>=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+…. \\\\<br>$$</li><li>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</li><li>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</li></ul><h2 id=\"正定性\"><a href=\"#正定性\" class=\"headerlink\" title=\"正定性\"></a>正定性</h2><ul><li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li><li>特征值的符号与稳定性有关</li><li>主元、行列式、特征值三位一体，线性代数将其统一</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Tue Sep 18 2018 21:44:27 GMT+0800 (中国标准时间)","title":"MIT线性代数笔记3","path":"2017/01/22/LinearAlgebra3/","eyeCatchImage":null,"excerpt":"<hr><h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul><li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li><li>行列式的性质<ul><li>$detI=1$</li><li>交换行，行列式的值的符号会相反</li><li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li><li>两行相等使得行列式为0(由性质二可以直接推出)</li><li>矩阵消元不改变其行列式(证明见下)</li><li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li><li>$detA=0$当且仅当A是奇异矩阵</li><li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li><li>$detA^{-1}detA=1$</li><li>$detA^2=(detA)^2$</li><li>$det2A=2^n detA$</li><li>$detA^T=detA$(证明见下)","date":"2017-01-22T11:21:02.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记2","date":"2017-01-21T11:28:03.000Z","mathjax":true,"html":true,"_content":"\n***\n# 第九讲：线性相关性、基、维数\n\n## 线性相关性\n-\t背景知识:假设一个矩阵A，m<n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。\n-\t什么条件下，$x_1,x_2,x_3...x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。\n-\t如果向量组里存在一个零向量，则这个向量组不可能线性无关。\n-\t假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。\n-\t对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。\n-\t换一种方式解释：当$v_1,v_2...v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。\n-\t列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。\n\n<!--more-->\n\n\n## 生成空间、基\n-\t$v_1...,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。\n-\t向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。\n-\t举个栗子：求$R^3$的一组基，最容易想到的是\n$$\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}\n$$\n-\t这是一组标准基，另一个栗子:\n$$\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}\n$$\n\n-\t显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。\n-\t如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。\n-\t若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。\n-\t基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。\n\n## 维数\n-\t上面提到的所有基向量的个数相同，这个个数就是空间的维数。**不是基向量的维数，而是基向量的个数**\n\n## 最后举个栗子\n对矩阵A\n$$\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t四列并不线性无关，可取第一列第二列为主列\n-\t2=A的秩=主列数=列空间维数\n-\t第一列和第二列构成列空间的一组基。\n-\t如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。\n-\t零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：\n$$\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1\t\\\\\n\\end{bmatrix}\n$$\n-\t这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。\n\n# 第十讲：四个基本子空间\n-\t列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。\n-\t分别处于$R^m、R^n、R^n、R^m$空间中\n-\t列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r\n-\t列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行\n-\t行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化\n-\t为什么叫做左零空间？\n$$\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\\n$$\n-\t易得rref=E，即EA=R \n-\t通过E可以计算左零空\n-\t求左零空间即找一个产生零行向量的行组合 \n-\t左零空间的基就是R非0行对应的E行,共m-r行 \n\n\n# 第十一讲：矩阵空间、秩1矩阵和小世界图\n\n## 矩阵空间\n-\t可以看成向量空间，可以数乘，可以相加\n-\t以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n$$\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基\n-\t接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3\n-\t如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9\n-\t整理一下可得\n$$\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\\n$$\n-\t再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间\n$$\n\\frac{d^2y}{dx^2}+y=0 \n$$\n-\t他的几个解为 \n$$\ny=cos(x),y=sin(x) \n$$\n-\t完整解为  \n$$\ny=c_1cos(x)+c_2sin(x) \n$$\n-\t即得到一个向量空间，基为2\n\n\n## 秩1矩阵\n-\t先写一个简单的秩1矩阵\n$$\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}\n$$\n-\t所有的秩1矩阵都可以表示为一列乘一行\n-\t秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成\n-\t再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为\n$$\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为\n$$\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n$$\n-\t矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$\n-\t整理一下\n$$\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\\n$$\n\n## 小世界图\n-\t仅仅引入了图的概念，为下一讲准备\n\n# 第十二讲：图和网络\n## 图\n-  图的一些基础概念，略过\n\n## 网络\n\n-  图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0\n\n-  构成回路的几行线性相关，回路意味着相关\n\n-  关联矩阵A描述了图的拓扑结构\n\n-  $dimN(A^T)=m-r​$\n\n-  假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路\n\n-  树就是没有回路的图\n\n-  再来看看$dimN(A^T)=m-r$\n\n-\t$dimN(A^T)$=无关回路数  \n-\t$m$=边数 \n-\t$r=n-1$=节点数-1 (因为零空间是一维的) \n-\t即:节点数-边数+回路数=1(欧拉公式) \n\n\n\n## <font size=4>总结\n\n- 将电势记为e,$e=Ax$\n\n- 电势差导致电流产生，$y=Ce$\n\n- 电流满足基尔霍夫电流方程,$A^Ty=0$\n\n- 将三个方程联立：\n  $$\n  A^TCAx=f\n  $$\n  这就是应用数学中最基本的平衡方程\n\n# 第十三讲：正交向量与子空间\n\n## 正交向量\n-\t正交即垂直，意味着在n维空间内，这些向量的夹角是90度\n-\t当$x^Ty=0$,x与y正交，证明：\n-\t若x与y正交，易得:\n$$\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\\n$$\n-\t即：\n$$\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\\n$$\n-\t即：\n$$\nx^Ty=0 \\\\\n$$\n\n\n-\t子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交\n-\t若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己\n-\t行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分\n-\t图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立\n-\t图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。\n\n## 正交子空间\n-\t例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交\n-\t因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量\n-\t以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$\n-\t$A^TA$是一个$n*n$的方阵，而且对称\n-\t坏方程转换为好方程，两边同乘$A^T$\n-\t$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同\n-\t$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质\n\n# 第十四讲：子空间投影\n## 投影\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG)\n-\t在二维情况下讨论投影\n-\t一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p\n-\tp在a的一维子空间里，是a的x倍，即p=xa\n-\ta垂直于e，即\n$$\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\\n$$\n-\t从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了\n\n## 投影矩阵\n-\t现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$\n-\t用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1\n-\t投影矩阵的另外两条性质：\n -\t对称,即$P^T=P$\n -\t两次投影在相同的位置，即$P^2=P$\n \n## 投影的意义\n-\t下面在高维情况下讨论\n-\t当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解\n-\t如何找？将b微调，使得b在列空间中\n-\t怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{'}=p$,p时b在列空间上的投影\n-\t现在我们要求$x^{'}$,$p=Ax^{'}$，误差向量$e=b-Ax^{'}$，由投影定义可知e需要垂直于A的列空间\n-\t综上可得\n$$\nA^T(b-Ax^{'})=0 \\\\\n$$\n-\t由上式可以看出e在A的左零空间，与列空间正交。解上式可得\n$$\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\\n$$\n-\t即投影矩阵P的n维模式:\n$$\nA(A^TA)^{-1}A^T \\\\\n$$\n-\t投影矩阵P的n维模式依然保留了1维模式的两个性质\n-\t现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线\n-\t已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$\n-\t假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n写成线代形式为:\n$$\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}\n$$\n-\t即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{'}$，这样就可以求出拟合直线。下一讲继续此例\n\n# 第十五讲：投影矩阵和最小二乘法\n\n## 投影矩阵\n-\t回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：\n\tb在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$\n\tb正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$\n\n\n-\tp是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：\n$$\nb=p+e \\\\\np=Pb \\\\\n$$\n-\t所以\n$$\ne=(I-P)b \\\\\n$$\n-\t所以左零空间的投影矩阵为$(I-P)$ \n\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG)\n\t\n## 最小二乘法\n-\t回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG)\n-\t设直线为$y=C+Dt$，代入三个点坐标得到一个方程组\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n-\t此方程组无解但是存在最优价，从代数角度看：\n$$\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\\n$$\n-\t分别对C和D求偏导为0，得到方程组: \n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。\n$$\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n$$\n-\tA满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。\n-\t现在要解出$x^{'},p$，已知$p=Ax^{'}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有\n$$\nA^TAx^{'}=A^Tb\n$$\n-\t代入数值可得\n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线\n-\t回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p\n-\t如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：\n\t如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量\n\t$$\n\tA^TAx=0 \\\\\n\tx^TA^TAx=0 \\\\\n\t(Ax)^T(Ax)=0 \\\\\n\tAx=0 \\\\\n\t$$\n-\t又因为A各列线性无关，所以\n\t$$\n\tx=0\n\t$$\n-\t即证\n-\t对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容\n\n# 第十六讲：正交矩阵和Gram-Schmidt正交化\n\n## 正交矩阵\n-\t已知一组正交向量集\n$$\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}\n$$\n$$\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\\n$$\n-\t所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如\n$$\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n$$\n-\tQ不一定是方阵。Q的各列将是列空间的标准正交基\n-\t对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$\n\n## Gram-Schmidt正交化\n-\t给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：\n$$\nB=e=b-\\frac{A^Tb}{A^TA}A\n$$\n-\t正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$\n-\t扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量\n$$\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B\n$$\n-\t由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,...$与$a,b,....$在同一列空间内，正交化可以写成\n$$\nA=QR \\\\\n$$\n-\t即\n$$\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\\n$$\n-\t其中，因为$QQ^T=I$\n-\t所以$R=Q^TA$\n-\t$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵","source":"_posts/LinearAlgebra2.md","raw":"---\ntitle: MIT线性代数笔记2\ndate: 2017-01-21 19:28:03\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# 第九讲：线性相关性、基、维数\n\n## 线性相关性\n-\t背景知识:假设一个矩阵A，m<n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。\n-\t什么条件下，$x_1,x_2,x_3...x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。\n-\t如果向量组里存在一个零向量，则这个向量组不可能线性无关。\n-\t假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。\n-\t对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。\n-\t换一种方式解释：当$v_1,v_2...v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。\n-\t列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。\n\n<!--more-->\n\n\n## 生成空间、基\n-\t$v_1...,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。\n-\t向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。\n-\t举个栗子：求$R^3$的一组基，最容易想到的是\n$$\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}\n$$\n-\t这是一组标准基，另一个栗子:\n$$\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}\n$$\n\n-\t显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。\n-\t如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。\n-\t若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。\n-\t基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。\n\n## 维数\n-\t上面提到的所有基向量的个数相同，这个个数就是空间的维数。**不是基向量的维数，而是基向量的个数**\n\n## 最后举个栗子\n对矩阵A\n$$\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t四列并不线性无关，可取第一列第二列为主列\n-\t2=A的秩=主列数=列空间维数\n-\t第一列和第二列构成列空间的一组基。\n-\t如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。\n-\t零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：\n$$\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1\t\\\\\n\\end{bmatrix}\n$$\n-\t这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。\n\n# 第十讲：四个基本子空间\n-\t列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。\n-\t分别处于$R^m、R^n、R^n、R^m$空间中\n-\t列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r\n-\t列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行\n-\t行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化\n-\t为什么叫做左零空间？\n$$\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\\n$$\n-\t易得rref=E，即EA=R \n-\t通过E可以计算左零空\n-\t求左零空间即找一个产生零行向量的行组合 \n-\t左零空间的基就是R非0行对应的E行,共m-r行 \n\n\n# 第十一讲：矩阵空间、秩1矩阵和小世界图\n\n## 矩阵空间\n-\t可以看成向量空间，可以数乘，可以相加\n-\t以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n$$\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基\n-\t接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3\n-\t如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9\n-\t整理一下可得\n$$\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\\n$$\n-\t再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间\n$$\n\\frac{d^2y}{dx^2}+y=0 \n$$\n-\t他的几个解为 \n$$\ny=cos(x),y=sin(x) \n$$\n-\t完整解为  \n$$\ny=c_1cos(x)+c_2sin(x) \n$$\n-\t即得到一个向量空间，基为2\n\n\n## 秩1矩阵\n-\t先写一个简单的秩1矩阵\n$$\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}\n$$\n-\t所有的秩1矩阵都可以表示为一列乘一行\n-\t秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成\n-\t再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为\n$$\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为\n$$\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n$$\n-\t矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$\n-\t整理一下\n$$\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\\n$$\n\n## 小世界图\n-\t仅仅引入了图的概念，为下一讲准备\n\n# 第十二讲：图和网络\n## 图\n-  图的一些基础概念，略过\n\n## 网络\n\n-  图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0\n\n-  构成回路的几行线性相关，回路意味着相关\n\n-  关联矩阵A描述了图的拓扑结构\n\n-  $dimN(A^T)=m-r​$\n\n-  假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路\n\n-  树就是没有回路的图\n\n-  再来看看$dimN(A^T)=m-r$\n\n-\t$dimN(A^T)$=无关回路数  \n-\t$m$=边数 \n-\t$r=n-1$=节点数-1 (因为零空间是一维的) \n-\t即:节点数-边数+回路数=1(欧拉公式) \n\n\n\n## <font size=4>总结\n\n- 将电势记为e,$e=Ax$\n\n- 电势差导致电流产生，$y=Ce$\n\n- 电流满足基尔霍夫电流方程,$A^Ty=0$\n\n- 将三个方程联立：\n  $$\n  A^TCAx=f\n  $$\n  这就是应用数学中最基本的平衡方程\n\n# 第十三讲：正交向量与子空间\n\n## 正交向量\n-\t正交即垂直，意味着在n维空间内，这些向量的夹角是90度\n-\t当$x^Ty=0$,x与y正交，证明：\n-\t若x与y正交，易得:\n$$\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\\n$$\n-\t即：\n$$\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\\n$$\n-\t即：\n$$\nx^Ty=0 \\\\\n$$\n\n\n-\t子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交\n-\t若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己\n-\t行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分\n-\t图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立\n-\t图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。\n\n## 正交子空间\n-\t例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交\n-\t因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量\n-\t以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$\n-\t$A^TA$是一个$n*n$的方阵，而且对称\n-\t坏方程转换为好方程，两边同乘$A^T$\n-\t$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同\n-\t$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质\n\n# 第十四讲：子空间投影\n## 投影\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG)\n-\t在二维情况下讨论投影\n-\t一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p\n-\tp在a的一维子空间里，是a的x倍，即p=xa\n-\ta垂直于e，即\n$$\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\\n$$\n-\t从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了\n\n## 投影矩阵\n-\t现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$\n-\t用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1\n-\t投影矩阵的另外两条性质：\n -\t对称,即$P^T=P$\n -\t两次投影在相同的位置，即$P^2=P$\n \n## 投影的意义\n-\t下面在高维情况下讨论\n-\t当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解\n-\t如何找？将b微调，使得b在列空间中\n-\t怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{'}=p$,p时b在列空间上的投影\n-\t现在我们要求$x^{'}$,$p=Ax^{'}$，误差向量$e=b-Ax^{'}$，由投影定义可知e需要垂直于A的列空间\n-\t综上可得\n$$\nA^T(b-Ax^{'})=0 \\\\\n$$\n-\t由上式可以看出e在A的左零空间，与列空间正交。解上式可得\n$$\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\\n$$\n-\t即投影矩阵P的n维模式:\n$$\nA(A^TA)^{-1}A^T \\\\\n$$\n-\t投影矩阵P的n维模式依然保留了1维模式的两个性质\n-\t现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线\n-\t已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$\n-\t假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n写成线代形式为:\n$$\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}\n$$\n-\t即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{'}$，这样就可以求出拟合直线。下一讲继续此例\n\n# 第十五讲：投影矩阵和最小二乘法\n\n## 投影矩阵\n-\t回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：\n\tb在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$\n\tb正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$\n\n\n-\tp是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：\n$$\nb=p+e \\\\\np=Pb \\\\\n$$\n-\t所以\n$$\ne=(I-P)b \\\\\n$$\n-\t所以左零空间的投影矩阵为$(I-P)$ \n\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG)\n\t\n## 最小二乘法\n-\t回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG)\n-\t设直线为$y=C+Dt$，代入三个点坐标得到一个方程组\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n-\t此方程组无解但是存在最优价，从代数角度看：\n$$\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\\n$$\n-\t分别对C和D求偏导为0，得到方程组: \n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。\n$$\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n$$\n-\tA满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。\n-\t现在要解出$x^{'},p$，已知$p=Ax^{'}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有\n$$\nA^TAx^{'}=A^Tb\n$$\n-\t代入数值可得\n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线\n-\t回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p\n-\t如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：\n\t如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量\n\t$$\n\tA^TAx=0 \\\\\n\tx^TA^TAx=0 \\\\\n\t(Ax)^T(Ax)=0 \\\\\n\tAx=0 \\\\\n\t$$\n-\t又因为A各列线性无关，所以\n\t$$\n\tx=0\n\t$$\n-\t即证\n-\t对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容\n\n# 第十六讲：正交矩阵和Gram-Schmidt正交化\n\n## 正交矩阵\n-\t已知一组正交向量集\n$$\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}\n$$\n$$\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\\n$$\n-\t所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如\n$$\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n$$\n-\tQ不一定是方阵。Q的各列将是列空间的标准正交基\n-\t对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$\n\n## Gram-Schmidt正交化\n-\t给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：\n$$\nB=e=b-\\frac{A^Tb}{A^TA}A\n$$\n-\t正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$\n-\t扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量\n$$\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B\n$$\n-\t由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,...$与$a,b,....$在同一列空间内，正交化可以写成\n$$\nA=QR \\\\\n$$\n-\t即\n$$\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\\n$$\n-\t其中，因为$QQ^T=I$\n-\t所以$R=Q^TA$\n-\t$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵","slug":"LinearAlgebra2","published":1,"updated":"2018-09-03T01:21:54.832Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072cg000aqcw6yd4i21pl","content":"<hr><h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul><li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li><li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li><li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li><li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li><li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li><li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li><li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li></ul><a id=\"more\"></a><h2 id=\"生成空间、基\"><a href=\"#生成空间、基\" class=\"headerlink\" title=\"生成空间、基\"></a>生成空间、基</h2><ul><li>$v_1…,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。</li><li>向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。</li><li>举个栗子：求$R^3$的一组基，最容易想到的是<br>$$<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>0 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li><p>这是一组标准基，另一个栗子:<br>$$<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>2 \\\\<br>2 \\\\<br>5 \\\\<br>\\end{bmatrix}<br>$$</p></li><li><p>显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。</p></li><li>如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。</li><li>若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。</li><li>基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。</li></ul><h2 id=\"维数\"><a href=\"#维数\" class=\"headerlink\" title=\"维数\"></a>维数</h2><ul><li>上面提到的所有基向量的个数相同，这个个数就是空间的维数。<strong>不是基向量的维数，而是基向量的个数</strong></li></ul><h2 id=\"最后举个栗子\"><a href=\"#最后举个栗子\" class=\"headerlink\" title=\"最后举个栗子\"></a>最后举个栗子</h2><p>对矩阵A<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 &amp;1 \\\\<br>1 &amp; 1 &amp; 2 &amp; 1 \\\\<br>1 &amp; 2 &amp; 3 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>四列并不线性无关，可取第一列第二列为主列</li><li>2=A的秩=主列数=列空间维数</li><li>第一列和第二列构成列空间的一组基。</li><li>如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。</li><li>零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：<br>$$<br>\\begin{bmatrix}<br>-1 \\\\<br>-1 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。</li></ul><h1 id=\"第十讲：四个基本子空间\"><a href=\"#第十讲：四个基本子空间\" class=\"headerlink\" title=\"第十讲：四个基本子空间\"></a>第十讲：四个基本子空间</h1><ul><li>列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。</li><li>分别处于$R^m、R^n、R^n、R^m$空间中</li><li>列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r</li><li>列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行</li><li>行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化</li><li>为什么叫做左零空间？<br>$$<br>rref\\begin{bmatrix}<br>A_{m*n} &amp; I_{m*n}<br>\\end{bmatrix}\\rightarrow<br>\\begin{bmatrix}<br>R_{m*n} &amp; E_{m*n}<br>\\end{bmatrix} \\\\<br>$$</li><li>易得rref=E，即EA=R</li><li>通过E可以计算左零空</li><li>求左零空间即找一个产生零行向量的行组合</li><li>左零空间的基就是R非0行对应的E行,共m-r行</li></ul><h1 id=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"><a href=\"#第十一讲：矩阵空间、秩1矩阵和小世界图\" class=\"headerlink\" title=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"></a>第十一讲：矩阵空间、秩1矩阵和小世界图</h1><h2 id=\"矩阵空间\"><a href=\"#矩阵空间\" class=\"headerlink\" title=\"矩阵空间\"></a>矩阵空间</h2><ul><li>可以看成向量空间，可以数乘，可以相加</li><li>以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9<br>$$<br>\\begin{bmatrix}<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 1 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}…..<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6<br>$$<br>\\begin{bmatrix}<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 1 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$<br>$$<br>\\begin{bmatrix}<br>0 &amp; 1 &amp; 0 \\\\<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>1 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 1 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基</li><li>接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3</li><li>如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9</li><li>整理一下可得<br>$$<br>dim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\<br>dim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\<br>$$</li><li>再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间<br>$$<br>\\frac{d^2y}{dx^2}+y=0<br>$$</li><li>他的几个解为<br>$$<br>y=cos(x),y=sin(x)<br>$$</li><li>完整解为<br>$$<br>y=c_1cos(x)+c_2sin(x)<br>$$</li><li>即得到一个向量空间，基为2</li></ul><h2 id=\"秩1矩阵\"><a href=\"#秩1矩阵\" class=\"headerlink\" title=\"秩1矩阵\"></a>秩1矩阵</h2><ul><li>先写一个简单的秩1矩阵<br>$$<br>\\begin{bmatrix}<br>1 &amp; 4 &amp; 5 \\\\<br>2 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>\\end{bmatrix}*<br>\\begin{bmatrix}<br>1 &amp; 4 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</li><li>所有的秩1矩阵都可以表示为一列乘一行</li><li>秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成</li><li>再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为<br>$$<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix},<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix},<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$</li><li>整理一下<br>$$<br>dim(N(A))+dim(C(A^T))=3+1=4=n \\\\<br>dim(C(A))+dim(N(A^T))=1+0=1=m \\\\<br>$$</li></ul><h2 id=\"小世界图\"><a href=\"#小世界图\" class=\"headerlink\" title=\"小世界图\"></a>小世界图</h2><ul><li>仅仅引入了图的概念，为下一讲准备</li></ul><h1 id=\"第十二讲：图和网络\"><a href=\"#第十二讲：图和网络\" class=\"headerlink\" title=\"第十二讲：图和网络\"></a>第十二讲：图和网络</h1><h2 id=\"图\"><a href=\"#图\" class=\"headerlink\" title=\"图\"></a>图</h2><ul><li>图的一些基础概念，略过</li></ul><h2 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h2><ul><li><p>图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0</p></li><li><p>构成回路的几行线性相关，回路意味着相关</p></li><li><p>关联矩阵A描述了图的拓扑结构</p></li><li><p>$dimN(A^T)=m-r​$</p></li><li><p>假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路</p></li><li><p>树就是没有回路的图</p></li><li><p>再来看看$dimN(A^T)=m-r$</p></li><li><p>$dimN(A^T)$=无关回路数</p></li><li>$m$=边数</li><li>$r=n-1$=节点数-1 (因为零空间是一维的)</li><li>即:节点数-边数+回路数=1(欧拉公式)</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><font size=\"4\">总结</font></h2><ul><li><p>将电势记为e,$e=Ax$</p></li><li><p>电势差导致电流产生，$y=Ce$</p></li><li><p>电流满足基尔霍夫电流方程,$A^Ty=0$</p></li><li><p>将三个方程联立：<br>$$<br>A^TCAx=f<br>$$<br>这就是应用数学中最基本的平衡方程</p></li></ul><h1 id=\"第十三讲：正交向量与子空间\"><a href=\"#第十三讲：正交向量与子空间\" class=\"headerlink\" title=\"第十三讲：正交向量与子空间\"></a>第十三讲：正交向量与子空间</h1><h2 id=\"正交向量\"><a href=\"#正交向量\" class=\"headerlink\" title=\"正交向量\"></a>正交向量</h2><ul><li>正交即垂直，意味着在n维空间内，这些向量的夹角是90度</li><li>当$x^Ty=0$,x与y正交，证明：</li><li>若x与y正交，易得:<br>$$<br>{||x||}^2+{||y||}^2={||x+y||}^2 \\\\<br>$$</li><li>即：<br>$$<br>x^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\<br>$$</li><li>即：<br>$$<br>x^Ty=0 \\\\<br>$$</li></ul><ul><li>子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交</li><li>若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己</li><li>行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分</li><li>图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立</li><li>图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。</li></ul><h2 id=\"正交子空间\"><a href=\"#正交子空间\" class=\"headerlink\" title=\"正交子空间\"></a>正交子空间</h2><ul><li>例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交</li><li>因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量</li><li>以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$</li><li>$A^TA$是一个$n*n$的方阵，而且对称</li><li>坏方程转换为好方程，两边同乘$A^T$</li><li>$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同</li><li>$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质</li></ul><h1 id=\"第十四讲：子空间投影\"><a href=\"#第十四讲：子空间投影\" class=\"headerlink\" title=\"第十四讲：子空间投影\"></a>第十四讲：子空间投影</h1><h2 id=\"投影\"><a href=\"#投影\" class=\"headerlink\" title=\"投影\"></a>投影</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG\" alt=\"mark\"></p><ul><li>在二维情况下讨论投影</li><li>一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p</li><li>p在a的一维子空间里，是a的x倍，即p=xa</li><li>a垂直于e，即<br>$$<br>a^T(b-xa)=0 \\\\<br>xa^Ta=a^Tb \\\\<br>x= \\frac {a^Tb}{a^Ta} \\\\<br>p=a\\frac {a^Tb}{a^Ta} \\\\<br>$$</li><li>从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了</li></ul><h2 id=\"投影矩阵\"><a href=\"#投影矩阵\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul><li>现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$</li><li>用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1</li><li>投影矩阵的另外两条性质：<ul><li>对称,即$P^T=P$</li><li>两次投影在相同的位置，即$P^2=P$</li></ul></li></ul><h2 id=\"投影的意义\"><a href=\"#投影的意义\" class=\"headerlink\" title=\"投影的意义\"></a>投影的意义</h2><ul><li>下面在高维情况下讨论</li><li>当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解</li><li>如何找？将b微调，使得b在列空间中</li><li>怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{‘}=p$,p时b在列空间上的投影</li><li>现在我们要求$x^{‘}$,$p=Ax^{‘}$，误差向量$e=b-Ax^{‘}$，由投影定义可知e需要垂直于A的列空间</li><li>综上可得<br>$$<br>A^T(b-Ax^{‘})=0 \\\\<br>$$</li><li>由上式可以看出e在A的左零空间，与列空间正交。解上式可得<br>$$<br>x^{‘}=(A^TA)^{-1}A^Tb \\\\<br>p=Ax^{‘}=A(A^TA)^{-1}A^Tb \\\\<br>$$</li><li>即投影矩阵P的n维模式:<br>$$<br>A(A^TA)^{-1}A^T \\\\<br>$$</li><li>投影矩阵P的n维模式依然保留了1维模式的两个性质</li><li>现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线</li><li>已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$</li><li>假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则<br>$$<br>C+D=1 \\\\<br>C+2D=2 \\\\<br>C+3D=2 \\\\<br>$$<br>写成线代形式为:<br>$$<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 2 \\\\<br>1 &amp; 3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>C \\\\<br>D \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>$$</li><li>即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{‘}$，这样就可以求出拟合直线。下一讲继续此例</li></ul><h1 id=\"第十五讲：投影矩阵和最小二乘法\"><a href=\"#第十五讲：投影矩阵和最小二乘法\" class=\"headerlink\" title=\"第十五讲：投影矩阵和最小二乘法\"></a>第十五讲：投影矩阵和最小二乘法</h1><h2 id=\"投影矩阵-1\"><a href=\"#投影矩阵-1\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul><li>回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：<br>b在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$<br>b正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$</li></ul><ul><li>p是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：<br>$$<br>b=p+e \\\\<br>p=Pb \\\\<br>$$</li><li>所以<br>$$<br>e=(I-P)b \\\\<br>$$</li><li><p>所以左零空间的投影矩阵为$(I-P)$</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG\" alt=\"mark\"></p></li></ul><h2 id=\"最小二乘法\"><a href=\"#最小二乘法\" class=\"headerlink\" title=\"最小二乘法\"></a>最小二乘法</h2><ul><li>回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG\" alt=\"mark\"></li><li>设直线为$y=C+Dt$，代入三个点坐标得到一个方程组<br>$$<br>C+D=1 \\\\<br>C+2D=2 \\\\<br>C+3D=2 \\\\<br>$$</li><li>此方程组无解但是存在最优价，从代数角度看：<br>$$<br>||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\<br>$$</li><li><p>分别对C和D求偏导为0，得到方程组:<br>$$<br>\\begin{cases}<br>3C+6D=5\\\\<br>6C+14D=11\\\\<br>\\end{cases}<br>$$</p></li><li><p>写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。<br>$$<br>Ax=b \\\\<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 2 \\\\<br>1 &amp; 3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>C \\\\<br>D \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>$$</p></li><li>A满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。</li><li>现在要解出$x^{‘},p$，已知$p=Ax^{‘}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有<br>$$<br>A^TAx^{‘}=A^Tb<br>$$</li><li><p>代入数值可得<br>$$<br>\\begin{cases}<br>3C+6D=5\\\\<br>6C+14D=11\\\\<br>\\end{cases}<br>$$</p></li><li><p>与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线</p></li><li>回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p</li><li>如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：<br>如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量<br>$$<br>A^TAx=0 \\\\<br>x^TA^TAx=0 \\\\<br>(Ax)^T(Ax)=0 \\\\<br>Ax=0 \\\\<br>$$</li><li>又因为A各列线性无关，所以<br>$$<br>x=0<br>$$</li><li>即证</li><li>对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容</li></ul><h1 id=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"><a href=\"#第十六讲：正交矩阵和Gram-Schmidt正交化\" class=\"headerlink\" title=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"></a>第十六讲：正交矩阵和Gram-Schmidt正交化</h1><h2 id=\"正交矩阵\"><a href=\"#正交矩阵\" class=\"headerlink\" title=\"正交矩阵\"></a>正交矩阵</h2><ul><li>已知一组正交向量集<br>$$<br>q_i^Tq_j=<br>\\begin{cases}<br>0 \\quad if \\quad i \\neq j \\\\<br>1 \\quad if \\quad i=j \\\\<br>\\end{cases}<br>$$<br>$$<br>Q=<br>\\begin{bmatrix}<br>q_1 &amp; q_2 &amp; … &amp; q_n \\\\<br>\\end{bmatrix} \\\\<br>Q^TQ=I \\\\<br>$$</li><li>所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如<br>$$<br>Q=\\begin{bmatrix}<br>cos \\theta &amp; -sin \\theta \\\\<br>sin \\theta &amp; cos \\theta \\\\<br>\\end{bmatrix}or<br>\\frac {1}{\\sqrt 2}<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; -1 \\\\<br>\\end{bmatrix}<br>$$</li><li>Q不一定是方阵。Q的各列将是列空间的标准正交基</li><li>对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$</li></ul><h2 id=\"Gram-Schmidt正交化\"><a href=\"#Gram-Schmidt正交化\" class=\"headerlink\" title=\"Gram-Schmidt正交化\"></a>Gram-Schmidt正交化</h2><ul><li>给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：<br>$$<br>B=e=b-\\frac{A^Tb}{A^TA}A<br>$$</li><li>正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$</li><li>扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量<br>$$<br>C=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B<br>$$</li><li>由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,…$与$a,b,….$在同一列空间内，正交化可以写成<br>$$<br>A=QR \\\\<br>$$</li><li>即<br>$$<br>\\begin{bmatrix}<br>a &amp; b \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>q_1 &amp; q_2 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>q_1^Ta &amp; q_1^Tb \\\\<br>q_2^Ta &amp; q_2^Tb \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>其中，因为$QQ^T=I$</li><li>所以$R=Q^TA$</li><li>$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵</li></ul>","site":{"data":{}},"excerpt":"<hr><h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul><li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li><li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li><li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li><li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li><li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li><li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li><li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li></ul>","more":"<h2 id=\"生成空间、基\"><a href=\"#生成空间、基\" class=\"headerlink\" title=\"生成空间、基\"></a>生成空间、基</h2><ul><li>$v_1…,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。</li><li>向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。</li><li>举个栗子：求$R^3$的一组基，最容易想到的是<br>$$<br>\\begin{bmatrix}<br>1 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>0 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li><p>这是一组标准基，另一个栗子:<br>$$<br>\\begin{bmatrix}<br>1 \\\\<br>1 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>2 \\\\<br>2 \\\\<br>5 \\\\<br>\\end{bmatrix}<br>$$</p></li><li><p>显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。</p></li><li>如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。</li><li>若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。</li><li>基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。</li></ul><h2 id=\"维数\"><a href=\"#维数\" class=\"headerlink\" title=\"维数\"></a>维数</h2><ul><li>上面提到的所有基向量的个数相同，这个个数就是空间的维数。<strong>不是基向量的维数，而是基向量的个数</strong></li></ul><h2 id=\"最后举个栗子\"><a href=\"#最后举个栗子\" class=\"headerlink\" title=\"最后举个栗子\"></a>最后举个栗子</h2><p>对矩阵A<br>$$<br>\\begin{bmatrix}<br>1 &amp; 2 &amp; 3 &amp;1 \\\\<br>1 &amp; 1 &amp; 2 &amp; 1 \\\\<br>1 &amp; 2 &amp; 3 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</p><ul><li>四列并不线性无关，可取第一列第二列为主列</li><li>2=A的秩=主列数=列空间维数</li><li>第一列和第二列构成列空间的一组基。</li><li>如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。</li><li>零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：<br>$$<br>\\begin{bmatrix}<br>-1 \\\\<br>-1 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix}<br>,<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。</li></ul><h1 id=\"第十讲：四个基本子空间\"><a href=\"#第十讲：四个基本子空间\" class=\"headerlink\" title=\"第十讲：四个基本子空间\"></a>第十讲：四个基本子空间</h1><ul><li>列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。</li><li>分别处于$R^m、R^n、R^n、R^m$空间中</li><li>列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r</li><li>列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行</li><li>行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化</li><li>为什么叫做左零空间？<br>$$<br>rref\\begin{bmatrix}<br>A_{m*n} &amp; I_{m*n}<br>\\end{bmatrix}\\rightarrow<br>\\begin{bmatrix}<br>R_{m*n} &amp; E_{m*n}<br>\\end{bmatrix} \\\\<br>$$</li><li>易得rref=E，即EA=R</li><li>通过E可以计算左零空</li><li>求左零空间即找一个产生零行向量的行组合</li><li>左零空间的基就是R非0行对应的E行,共m-r行</li></ul><h1 id=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"><a href=\"#第十一讲：矩阵空间、秩1矩阵和小世界图\" class=\"headerlink\" title=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"></a>第十一讲：矩阵空间、秩1矩阵和小世界图</h1><h2 id=\"矩阵空间\"><a href=\"#矩阵空间\" class=\"headerlink\" title=\"矩阵空间\"></a>矩阵空间</h2><ul><li>可以看成向量空间，可以数乘，可以相加</li><li>以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9<br>$$<br>\\begin{bmatrix}<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 1 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}…..<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6<br>$$<br>\\begin{bmatrix}<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 1 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$<br>$$<br>\\begin{bmatrix}<br>0 &amp; 1 &amp; 0 \\\\<br>1 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 0 &amp; 0 \\\\<br>1 &amp; 0 &amp; 0 \\\\<br>\\end{bmatrix}，<br>\\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\\\<br>0 &amp; 0 &amp; 1 \\\\<br>0 &amp; 1 &amp; 0 \\\\<br>\\end{bmatrix}<br>$$</li><li>对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基</li><li>接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3</li><li>如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9</li><li>整理一下可得<br>$$<br>dim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\<br>dim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\<br>$$</li><li>再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间<br>$$<br>\\frac{d^2y}{dx^2}+y=0<br>$$</li><li>他的几个解为<br>$$<br>y=cos(x),y=sin(x)<br>$$</li><li>完整解为<br>$$<br>y=c_1cos(x)+c_2sin(x)<br>$$</li><li>即得到一个向量空间，基为2</li></ul><h2 id=\"秩1矩阵\"><a href=\"#秩1矩阵\" class=\"headerlink\" title=\"秩1矩阵\"></a>秩1矩阵</h2><ul><li>先写一个简单的秩1矩阵<br>$$<br>\\begin{bmatrix}<br>1 &amp; 4 &amp; 5 \\\\<br>2 &amp; 8 &amp; 10 \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>\\end{bmatrix}*<br>\\begin{bmatrix}<br>1 &amp; 4 &amp; 5 \\\\<br>\\end{bmatrix}<br>$$</li><li>所有的秩1矩阵都可以表示为一列乘一行</li><li>秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成</li><li>再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为<br>$$<br>A=\\begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; 1 \\\\<br>\\end{bmatrix}<br>$$</li><li>易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为<br>$$<br>\\begin{bmatrix}<br>-1 \\\\<br>1 \\\\<br>0 \\\\<br>0 \\\\<br>\\end{bmatrix},<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>1 \\\\<br>0 \\\\<br>\\end{bmatrix},<br>\\begin{bmatrix}<br>-1 \\\\<br>0 \\\\<br>0 \\\\<br>1 \\\\<br>\\end{bmatrix}<br>$$</li><li>矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$</li><li>整理一下<br>$$<br>dim(N(A))+dim(C(A^T))=3+1=4=n \\\\<br>dim(C(A))+dim(N(A^T))=1+0=1=m \\\\<br>$$</li></ul><h2 id=\"小世界图\"><a href=\"#小世界图\" class=\"headerlink\" title=\"小世界图\"></a>小世界图</h2><ul><li>仅仅引入了图的概念，为下一讲准备</li></ul><h1 id=\"第十二讲：图和网络\"><a href=\"#第十二讲：图和网络\" class=\"headerlink\" title=\"第十二讲：图和网络\"></a>第十二讲：图和网络</h1><h2 id=\"图\"><a href=\"#图\" class=\"headerlink\" title=\"图\"></a>图</h2><ul><li>图的一些基础概念，略过</li></ul><h2 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h2><ul><li><p>图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0</p></li><li><p>构成回路的几行线性相关，回路意味着相关</p></li><li><p>关联矩阵A描述了图的拓扑结构</p></li><li><p>$dimN(A^T)=m-r​$</p></li><li><p>假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路</p></li><li><p>树就是没有回路的图</p></li><li><p>再来看看$dimN(A^T)=m-r$</p></li><li><p>$dimN(A^T)$=无关回路数</p></li><li>$m$=边数</li><li>$r=n-1$=节点数-1 (因为零空间是一维的)</li><li>即:节点数-边数+回路数=1(欧拉公式)</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><font size=\"4\">总结</font></h2><ul><li><p>将电势记为e,$e=Ax$</p></li><li><p>电势差导致电流产生，$y=Ce$</p></li><li><p>电流满足基尔霍夫电流方程,$A^Ty=0$</p></li><li><p>将三个方程联立：<br>$$<br>A^TCAx=f<br>$$<br>这就是应用数学中最基本的平衡方程</p></li></ul><h1 id=\"第十三讲：正交向量与子空间\"><a href=\"#第十三讲：正交向量与子空间\" class=\"headerlink\" title=\"第十三讲：正交向量与子空间\"></a>第十三讲：正交向量与子空间</h1><h2 id=\"正交向量\"><a href=\"#正交向量\" class=\"headerlink\" title=\"正交向量\"></a>正交向量</h2><ul><li>正交即垂直，意味着在n维空间内，这些向量的夹角是90度</li><li>当$x^Ty=0$,x与y正交，证明：</li><li>若x与y正交，易得:<br>$$<br>{||x||}^2+{||y||}^2={||x+y||}^2 \\\\<br>$$</li><li>即：<br>$$<br>x^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\<br>$$</li><li>即：<br>$$<br>x^Ty=0 \\\\<br>$$</li></ul><ul><li>子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交</li><li>若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己</li><li>行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分</li><li>图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立</li><li>图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。</li></ul><h2 id=\"正交子空间\"><a href=\"#正交子空间\" class=\"headerlink\" title=\"正交子空间\"></a>正交子空间</h2><ul><li>例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交</li><li>因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量</li><li>以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$</li><li>$A^TA$是一个$n*n$的方阵，而且对称</li><li>坏方程转换为好方程，两边同乘$A^T$</li><li>$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同</li><li>$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质</li></ul><h1 id=\"第十四讲：子空间投影\"><a href=\"#第十四讲：子空间投影\" class=\"headerlink\" title=\"第十四讲：子空间投影\"></a>第十四讲：子空间投影</h1><h2 id=\"投影\"><a href=\"#投影\" class=\"headerlink\" title=\"投影\"></a>投影</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG\" alt=\"mark\"></p><ul><li>在二维情况下讨论投影</li><li>一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p</li><li>p在a的一维子空间里，是a的x倍，即p=xa</li><li>a垂直于e，即<br>$$<br>a^T(b-xa)=0 \\\\<br>xa^Ta=a^Tb \\\\<br>x= \\frac {a^Tb}{a^Ta} \\\\<br>p=a\\frac {a^Tb}{a^Ta} \\\\<br>$$</li><li>从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了</li></ul><h2 id=\"投影矩阵\"><a href=\"#投影矩阵\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul><li>现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$</li><li>用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1</li><li>投影矩阵的另外两条性质：<ul><li>对称,即$P^T=P$</li><li>两次投影在相同的位置，即$P^2=P$</li></ul></li></ul><h2 id=\"投影的意义\"><a href=\"#投影的意义\" class=\"headerlink\" title=\"投影的意义\"></a>投影的意义</h2><ul><li>下面在高维情况下讨论</li><li>当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解</li><li>如何找？将b微调，使得b在列空间中</li><li>怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{‘}=p$,p时b在列空间上的投影</li><li>现在我们要求$x^{‘}$,$p=Ax^{‘}$，误差向量$e=b-Ax^{‘}$，由投影定义可知e需要垂直于A的列空间</li><li>综上可得<br>$$<br>A^T(b-Ax^{‘})=0 \\\\<br>$$</li><li>由上式可以看出e在A的左零空间，与列空间正交。解上式可得<br>$$<br>x^{‘}=(A^TA)^{-1}A^Tb \\\\<br>p=Ax^{‘}=A(A^TA)^{-1}A^Tb \\\\<br>$$</li><li>即投影矩阵P的n维模式:<br>$$<br>A(A^TA)^{-1}A^T \\\\<br>$$</li><li>投影矩阵P的n维模式依然保留了1维模式的两个性质</li><li>现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线</li><li>已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$</li><li>假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则<br>$$<br>C+D=1 \\\\<br>C+2D=2 \\\\<br>C+3D=2 \\\\<br>$$<br>写成线代形式为:<br>$$<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 2 \\\\<br>1 &amp; 3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>C \\\\<br>D \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>$$</li><li>即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{‘}$，这样就可以求出拟合直线。下一讲继续此例</li></ul><h1 id=\"第十五讲：投影矩阵和最小二乘法\"><a href=\"#第十五讲：投影矩阵和最小二乘法\" class=\"headerlink\" title=\"第十五讲：投影矩阵和最小二乘法\"></a>第十五讲：投影矩阵和最小二乘法</h1><h2 id=\"投影矩阵-1\"><a href=\"#投影矩阵-1\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul><li>回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：<br>b在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$<br>b正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$</li></ul><ul><li>p是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：<br>$$<br>b=p+e \\\\<br>p=Pb \\\\<br>$$</li><li>所以<br>$$<br>e=(I-P)b \\\\<br>$$</li><li><p>所以左零空间的投影矩阵为$(I-P)$</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG\" alt=\"mark\"></p></li></ul><h2 id=\"最小二乘法\"><a href=\"#最小二乘法\" class=\"headerlink\" title=\"最小二乘法\"></a>最小二乘法</h2><ul><li>回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG\" alt=\"mark\"></li><li>设直线为$y=C+Dt$，代入三个点坐标得到一个方程组<br>$$<br>C+D=1 \\\\<br>C+2D=2 \\\\<br>C+3D=2 \\\\<br>$$</li><li>此方程组无解但是存在最优价，从代数角度看：<br>$$<br>||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\<br>$$</li><li><p>分别对C和D求偏导为0，得到方程组:<br>$$<br>\\begin{cases}<br>3C+6D=5\\\\<br>6C+14D=11\\\\<br>\\end{cases}<br>$$</p></li><li><p>写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。<br>$$<br>Ax=b \\\\<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; 2 \\\\<br>1 &amp; 3 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>C \\\\<br>D \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>1 \\\\<br>2 \\\\<br>2 \\\\<br>\\end{bmatrix}<br>$$</p></li><li>A满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。</li><li>现在要解出$x^{‘},p$，已知$p=Ax^{‘}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有<br>$$<br>A^TAx^{‘}=A^Tb<br>$$</li><li><p>代入数值可得<br>$$<br>\\begin{cases}<br>3C+6D=5\\\\<br>6C+14D=11\\\\<br>\\end{cases}<br>$$</p></li><li><p>与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线</p></li><li>回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p</li><li>如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：<br>如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量<br>$$<br>A^TAx=0 \\\\<br>x^TA^TAx=0 \\\\<br>(Ax)^T(Ax)=0 \\\\<br>Ax=0 \\\\<br>$$</li><li>又因为A各列线性无关，所以<br>$$<br>x=0<br>$$</li><li>即证</li><li>对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容</li></ul><h1 id=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"><a href=\"#第十六讲：正交矩阵和Gram-Schmidt正交化\" class=\"headerlink\" title=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"></a>第十六讲：正交矩阵和Gram-Schmidt正交化</h1><h2 id=\"正交矩阵\"><a href=\"#正交矩阵\" class=\"headerlink\" title=\"正交矩阵\"></a>正交矩阵</h2><ul><li>已知一组正交向量集<br>$$<br>q_i^Tq_j=<br>\\begin{cases}<br>0 \\quad if \\quad i \\neq j \\\\<br>1 \\quad if \\quad i=j \\\\<br>\\end{cases}<br>$$<br>$$<br>Q=<br>\\begin{bmatrix}<br>q_1 &amp; q_2 &amp; … &amp; q_n \\\\<br>\\end{bmatrix} \\\\<br>Q^TQ=I \\\\<br>$$</li><li>所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如<br>$$<br>Q=\\begin{bmatrix}<br>cos \\theta &amp; -sin \\theta \\\\<br>sin \\theta &amp; cos \\theta \\\\<br>\\end{bmatrix}or<br>\\frac {1}{\\sqrt 2}<br>\\begin{bmatrix}<br>1 &amp; 1 \\\\<br>1 &amp; -1 \\\\<br>\\end{bmatrix}<br>$$</li><li>Q不一定是方阵。Q的各列将是列空间的标准正交基</li><li>对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$</li></ul><h2 id=\"Gram-Schmidt正交化\"><a href=\"#Gram-Schmidt正交化\" class=\"headerlink\" title=\"Gram-Schmidt正交化\"></a>Gram-Schmidt正交化</h2><ul><li>给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：<br>$$<br>B=e=b-\\frac{A^Tb}{A^TA}A<br>$$</li><li>正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$</li><li>扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量<br>$$<br>C=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B<br>$$</li><li>由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,…$与$a,b,….$在同一列空间内，正交化可以写成<br>$$<br>A=QR \\\\<br>$$</li><li>即<br>$$<br>\\begin{bmatrix}<br>a &amp; b \\\\<br>\\end{bmatrix}=<br>\\begin{bmatrix}<br>q_1 &amp; q_2 \\\\<br>\\end{bmatrix}<br>\\begin{bmatrix}<br>q_1^Ta &amp; q_1^Tb \\\\<br>q_2^Ta &amp; q_2^Tb \\\\<br>\\end{bmatrix} \\\\<br>$$</li><li>其中，因为$QQ^T=I$</li><li>所以$R=Q^TA$</li><li>$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Sep 03 2018 09:21:54 GMT+0800 (中国标准时间)","title":"MIT线性代数笔记2","path":"2017/01/21/LinearAlgebra2/","eyeCatchImage":null,"excerpt":"<hr><h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul><li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li><li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li><li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li><li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li><li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li><li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li><li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li></ul>","date":"2017-01-21T11:28:03.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习笔记","mathjax":true,"html":true,"date":"2017-02-12T14:40:38.000Z","_content":"\n***\n记录机器学习中关于一些概念和算法的笔记，来源于:\n-\t选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)\n-\t西瓜书\n-\t《统计学习方法》\n-\t《深度学习》（感谢中文翻译：[exacity/deeplearningbook-chinese](https://github.com/exacity/deeplearningbook-chinese)）\n\n更新：\n-\t2017-02-12 更新概论\n-\t2017-03-01 更新k近邻\n-\t2017-03-08 更新SVM\n-\t2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识\n-\t2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容\n<!--more-->\n\n# <font size=5 >统计学习方法概论</font>\n## <font size=4 >统计学习，监督学习，三要素</font>\n-\t如果一个系统能够通过执行某个过程改进它的性能，这就是学习\n-\t统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析\n-\t得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析\n-\t监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测\n-\t每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征\n-\t监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)\n-\t监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数\n-\t统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法\n-\t损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$\n-\t理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化\n\n## <font size=4 >模型评估，模型选择</font>\n-\t模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的\n-\t过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量\n-\t模型选择的两种方法：正则化和交叉验证\n\n## <font size=4 >正则化，交叉验证</font>\n-\t即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高\n-\t一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择\n-\t交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证\n\n## <font size=4 >泛化能力</font>\n-\t泛化误差:对未知数据预测的误差\n-\t泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大\n-\t对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$\n\n$$\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\\n$$\n\n## <font size=4 >生成模型，判别模型</font>\n-\t生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型\n-\t判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等\n-\t生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况\n-\t判别方法准确率高，可以抽象数据，简化学习问题\n\n## <font size=4 >分类，标注，回归</font>\n-\t分类，即输出取离散有限值，分类决策函数也叫分类器\n-\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n$$\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n$$\n-\t标注:输入一个观测序列，输出一个标记序列\n-\t回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合\n\n# <font size=5 >k近邻法</font>\n## <font size=4 >k近邻法</font>\n\n- k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。\n- k值选择，距离度量以及分类决策规则是k近邻法三要素。\n-\tk近邻法是一种懒惰学习，他不对样本进行训练。\n\n## <font size=4 >k近邻算法</font>\n\n- 对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:\n  $$\n  y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K\n  $$\n  其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。\n\n- k=1时称为最近邻算法\n\n- k近邻算法没有显式的学习过程\n\n## <font size=4 >k近邻模型</font>\n\n- k近邻模型即对特征空间的划分。\n\n- 特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。\n\n-\t距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。\n\n-\t欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：\n  $$\n  L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}\n  $$\n\n-\tk值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单\n\n-\t一般采用交叉验证法确定k值\n\n-\t多数表决规则等价于经验风险最小化\n\n## <font size=4 >kd树</font>\n-\tkd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树\n-\tkd树的每一个节点对应于一个k维超矩形区域\n-\t构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。\n-\t构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：\n\t-\t从根节点出发，递归向下搜索目标点所在区域，直到叶节点\n\t-\t以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离\n\t-\t递归向上回退，对每个节点做如下操作\n\t\t-\t如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离\n\t\t-\t该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。\n\t\t-\t直到搜索到根节点，此时的当前最近点即目标点的最近邻点。\n-\tkd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索\n\n\n# <font size=5 >支持向量机</font>\n## <font size=4 >线性可分支持向量机与硬间隔最大化</font>\n-\t学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示\n-\t这里的x是特征向量$(x_1,x_2,...)$，而y是特征向量的标签\n-\t给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机\n-\t在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为\n$$\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}\n$$\n-\t而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔\n$$\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\\n$$\n-\t同时定义相对距离为函数间隔\n$$\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\\n$$\n-\t硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言\n-\t求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题\n$$\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\\n$$\n-\t我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为\n$$\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\\n$$\n-\t上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法\n-\t最大间隔分离超平面存在且唯一，证明略\n-\t在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量\n-\t对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$\n-\t由此可见支持向量机由很少的重要的训练样本(支持向量)决定\n-\t为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题\n-\t待补充\n\n# <font size=5 >线代基础</font>\n## <font size=4 >Moore-penrose</font>\n-\t对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆\n$$\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T\n$$ \n-\t计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：\n$$\nA^+=VD^+U^T\n$$\n\t其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。\n-\t当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。\n-\t当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。\n-\t待补充\n\n## <font size=4 >迹</font>\n-\t迹运算返回的是矩阵对角元素的和.\n-\t使用迹运算可以描述矩阵Frobenius范数的方式：\n$$\n||A_F||=\\sqrt{Tr(AA^T)}\n$$\n-\t迹具有转置不变性和轮换不变性\n-\t标量的迹是其本身\n\n## <font size=4 >PCA解释</font>\n-\t待补充\n\n# <font size=5 >概率论信息论</font>\n## <font size=4 >Logistic Sigmoid</font>\n-\tLogistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/cla6i1maiE.png?imageslim)\n-\tSoftmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/KhILG09Ij4.png?imageslim)\n-\t两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象(?)。\n-\tSoftmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。\n-\t利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aA4kidJj2d.png?imageslim)\n-\t整流线性单元使用函数:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CamKlC3dk8.png?imageslim)\n-\t其软化版本是softplus:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/daC267kkFk.png?imageslim)\n-\tSigmoid和softplus函数的一些性质：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/dJJb644g25.png?imageslim)\n\t\n## <font size=4 >KL散度和交叉熵</font>\n-\tKL散度\t用以衡量PQ两个分布之间的差异，非负且不对称：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BDDchFGAAI.png?imageslim)\n-\t交叉熵：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BG2cdG0efL.png?imageslim)\n-\t交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数\n-\t在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)\n-\t对q按照p求自信息期望即二元交叉熵(Logistic代价函数):\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Ebg5CkFI9A.png?imageslim)\n-\t同理可得多元交叉熵(Softmaxs代价函数):\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fhih6BdcbH.png?imageslim)\n\n## <font size=4 >交叉熵与最大对数似然关系</font>\t\n-\t已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。\n在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jlKK7jmHlG.png?imageslim)\n-\t因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIkh1LjjBH.png?imageslim)\n-\t可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:\n-\t最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数\n-\t最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/L3E7HC8D8J.png?imageslim)\n-\t最大似然估计具有一致性。\n\n# <font size=5 >计算方法</font>\n## <font size=4 >梯度下降</font>\n-\t问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？\n-\t原理：将输入向导数的反方向移动一小步可以减小函数输出。\n-\t将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。\n-\t一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/F3A336L1gH.png?imageslim)\n\n## <font size=4 >牛顿法</font>\n-\t二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/K9Bhe01f2l.png?imageslim)\n\n## <font size=4 >约束优化</font>\n-\t只包含等式约束条件：Lagrange \n-\t包含不等式约束条件：KTT\n\n# <font size=5 >修改算法</font>\n## <font size=4 >修改假设空间</font>\n-\t机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。\n-\t调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEGe5ge292.png?imageslim)\n-\t如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\n\n## <font size=4 >正则化</font>\n-\t没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。\n-\t一个例子是权重衰减，加入权重衰减正则化项的代价函数是：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Bif507aI3F.png?imageslim)\n\tλ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。\n\n\n\n\n\n","source":"_posts/MachineLearningNote.md","raw":"---\ntitle: 机器学习笔记\ncategories: 机器学习\ntags:\n- code\n- machine learning\n\nmathjax: true\nhtml: true\ndate: 2017-02-12 22:40:38\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/20170328/174921231.png\n---\n\n***\n记录机器学习中关于一些概念和算法的笔记，来源于:\n-\t选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)\n-\t西瓜书\n-\t《统计学习方法》\n-\t《深度学习》（感谢中文翻译：[exacity/deeplearningbook-chinese](https://github.com/exacity/deeplearningbook-chinese)）\n\n更新：\n-\t2017-02-12 更新概论\n-\t2017-03-01 更新k近邻\n-\t2017-03-08 更新SVM\n-\t2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识\n-\t2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容\n<!--more-->\n\n# <font size=5 >统计学习方法概论</font>\n## <font size=4 >统计学习，监督学习，三要素</font>\n-\t如果一个系统能够通过执行某个过程改进它的性能，这就是学习\n-\t统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析\n-\t得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析\n-\t监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测\n-\t每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征\n-\t监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)\n-\t监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数\n-\t统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法\n-\t损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$\n-\t理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化\n\n## <font size=4 >模型评估，模型选择</font>\n-\t模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的\n-\t过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量\n-\t模型选择的两种方法：正则化和交叉验证\n\n## <font size=4 >正则化，交叉验证</font>\n-\t即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高\n-\t一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择\n-\t交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证\n\n## <font size=4 >泛化能力</font>\n-\t泛化误差:对未知数据预测的误差\n-\t泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大\n-\t对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$\n\n$$\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\\n$$\n\n## <font size=4 >生成模型，判别模型</font>\n-\t生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型\n-\t判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等\n-\t生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况\n-\t判别方法准确率高，可以抽象数据，简化学习问题\n\n## <font size=4 >分类，标注，回归</font>\n-\t分类，即输出取离散有限值，分类决策函数也叫分类器\n-\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n$$\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n$$\n-\t标注:输入一个观测序列，输出一个标记序列\n-\t回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合\n\n# <font size=5 >k近邻法</font>\n## <font size=4 >k近邻法</font>\n\n- k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。\n- k值选择，距离度量以及分类决策规则是k近邻法三要素。\n-\tk近邻法是一种懒惰学习，他不对样本进行训练。\n\n## <font size=4 >k近邻算法</font>\n\n- 对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:\n  $$\n  y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K\n  $$\n  其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。\n\n- k=1时称为最近邻算法\n\n- k近邻算法没有显式的学习过程\n\n## <font size=4 >k近邻模型</font>\n\n- k近邻模型即对特征空间的划分。\n\n- 特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。\n\n-\t距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。\n\n-\t欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：\n  $$\n  L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}\n  $$\n\n-\tk值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单\n\n-\t一般采用交叉验证法确定k值\n\n-\t多数表决规则等价于经验风险最小化\n\n## <font size=4 >kd树</font>\n-\tkd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树\n-\tkd树的每一个节点对应于一个k维超矩形区域\n-\t构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。\n-\t构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：\n\t-\t从根节点出发，递归向下搜索目标点所在区域，直到叶节点\n\t-\t以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离\n\t-\t递归向上回退，对每个节点做如下操作\n\t\t-\t如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离\n\t\t-\t该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。\n\t\t-\t直到搜索到根节点，此时的当前最近点即目标点的最近邻点。\n-\tkd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索\n\n\n# <font size=5 >支持向量机</font>\n## <font size=4 >线性可分支持向量机与硬间隔最大化</font>\n-\t学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示\n-\t这里的x是特征向量$(x_1,x_2,...)$，而y是特征向量的标签\n-\t给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机\n-\t在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为\n$$\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}\n$$\n-\t而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔\n$$\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\\n$$\n-\t同时定义相对距离为函数间隔\n$$\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\\n$$\n-\t硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言\n-\t求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题\n$$\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\\n$$\n-\t我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为\n$$\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\\n$$\n-\t上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法\n-\t最大间隔分离超平面存在且唯一，证明略\n-\t在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量\n-\t对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$\n-\t由此可见支持向量机由很少的重要的训练样本(支持向量)决定\n-\t为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题\n-\t待补充\n\n# <font size=5 >线代基础</font>\n## <font size=4 >Moore-penrose</font>\n-\t对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆\n$$\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T\n$$ \n-\t计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：\n$$\nA^+=VD^+U^T\n$$\n\t其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。\n-\t当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。\n-\t当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。\n-\t待补充\n\n## <font size=4 >迹</font>\n-\t迹运算返回的是矩阵对角元素的和.\n-\t使用迹运算可以描述矩阵Frobenius范数的方式：\n$$\n||A_F||=\\sqrt{Tr(AA^T)}\n$$\n-\t迹具有转置不变性和轮换不变性\n-\t标量的迹是其本身\n\n## <font size=4 >PCA解释</font>\n-\t待补充\n\n# <font size=5 >概率论信息论</font>\n## <font size=4 >Logistic Sigmoid</font>\n-\tLogistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/cla6i1maiE.png?imageslim)\n-\tSoftmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/KhILG09Ij4.png?imageslim)\n-\t两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象(?)。\n-\tSoftmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。\n-\t利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aA4kidJj2d.png?imageslim)\n-\t整流线性单元使用函数:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CamKlC3dk8.png?imageslim)\n-\t其软化版本是softplus:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/daC267kkFk.png?imageslim)\n-\tSigmoid和softplus函数的一些性质：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/dJJb644g25.png?imageslim)\n\t\n## <font size=4 >KL散度和交叉熵</font>\n-\tKL散度\t用以衡量PQ两个分布之间的差异，非负且不对称：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BDDchFGAAI.png?imageslim)\n-\t交叉熵：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BG2cdG0efL.png?imageslim)\n-\t交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数\n-\t在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)\n-\t对q按照p求自信息期望即二元交叉熵(Logistic代价函数):\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Ebg5CkFI9A.png?imageslim)\n-\t同理可得多元交叉熵(Softmaxs代价函数):\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fhih6BdcbH.png?imageslim)\n\n## <font size=4 >交叉熵与最大对数似然关系</font>\t\n-\t已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。\n在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jlKK7jmHlG.png?imageslim)\n-\t因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIkh1LjjBH.png?imageslim)\n-\t可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:\n-\t最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数\n-\t最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/L3E7HC8D8J.png?imageslim)\n-\t最大似然估计具有一致性。\n\n# <font size=5 >计算方法</font>\n## <font size=4 >梯度下降</font>\n-\t问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？\n-\t原理：将输入向导数的反方向移动一小步可以减小函数输出。\n-\t将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。\n-\t一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/F3A336L1gH.png?imageslim)\n\n## <font size=4 >牛顿法</font>\n-\t二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/K9Bhe01f2l.png?imageslim)\n\n## <font size=4 >约束优化</font>\n-\t只包含等式约束条件：Lagrange \n-\t包含不等式约束条件：KTT\n\n# <font size=5 >修改算法</font>\n## <font size=4 >修改假设空间</font>\n-\t机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。\n-\t调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEGe5ge292.png?imageslim)\n-\t如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\n\n## <font size=4 >正则化</font>\n-\t没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。\n-\t一个例子是权重衰减，加入权重衰减正则化项的代价函数是：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Bif507aI3F.png?imageslim)\n\tλ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。\n\n\n\n\n\n","slug":"MachineLearningNote","published":1,"updated":"2018-09-15T02:03:54.578Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170328/174921231.png"],"comments":1,"layout":"post","link":"","_id":"cjmd072ci000dqcw68lvwmb07","content":"<hr><p>记录机器学习中关于一些概念和算法的笔记，来源于:</p><ul><li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li><li>西瓜书</li><li>《统计学习方法》</li><li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li></ul><p>更新：</p><ul><li>2017-02-12 更新概论</li><li>2017-03-01 更新k近邻</li><li>2017-03-08 更新SVM</li><li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li><li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容<a id=\"more\"></a></li></ul><h1 id=\"统计学习方法概论\"><a href=\"#统计学习方法概论\" class=\"headerlink\" title=\"统计学习方法概论\"></a><font size=\"5\">统计学习方法概论</font></h1><h2 id=\"统计学习，监督学习，三要素\"><a href=\"#统计学习，监督学习，三要素\" class=\"headerlink\" title=\"统计学习，监督学习，三要素\"></a><font size=\"4\">统计学习，监督学习，三要素</font></h2><ul><li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li><li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li><li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li><li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li><li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li><li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li><li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li><li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li><li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$</li><li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li></ul><h2 id=\"模型评估，模型选择\"><a href=\"#模型评估，模型选择\" class=\"headerlink\" title=\"模型评估，模型选择\"></a><font size=\"4\">模型评估，模型选择</font></h2><ul><li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li><li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li><li>模型选择的两种方法：正则化和交叉验证</li></ul><h2 id=\"正则化，交叉验证\"><a href=\"#正则化，交叉验证\" class=\"headerlink\" title=\"正则化，交叉验证\"></a><font size=\"4\">正则化，交叉验证</font></h2><ul><li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li><li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li><li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li></ul><h2 id=\"泛化能力\"><a href=\"#泛化能力\" class=\"headerlink\" title=\"泛化能力\"></a><font size=\"4\">泛化能力</font></h2><ul><li>泛化误差:对未知数据预测的误差</li><li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li><li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$</li></ul><p>$$<br>R_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\<br>\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\<br>$$</p><h2 id=\"生成模型，判别模型\"><a href=\"#生成模型，判别模型\" class=\"headerlink\" title=\"生成模型，判别模型\"></a><font size=\"4\">生成模型，判别模型</font></h2><ul><li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li><li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li><li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li><li>判别方法准确率高，可以抽象数据，简化学习问题</li></ul><h2 id=\"分类，标注，回归\"><a href=\"#分类，标注，回归\" class=\"headerlink\" title=\"分类，标注，回归\"></a><font size=\"4\">分类，标注，回归</font></h2><ul><li>分类，即输出取离散有限值，分类决策函数也叫分类器</li><li>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<br>$$<br>精确率:P=\\frac{TP}{TP+FP} \\\\<br>召回率:R=\\frac{TP}{TP+FN} \\\\<br>1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\<br>$$</li><li>标注:输入一个观测序列，输出一个标记序列</li><li>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</li></ul><h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"5\">k近邻法</font></h1><h2 id=\"k近邻法-1\"><a href=\"#k近邻法-1\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"4\">k近邻法</font></h2><ul><li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li><li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li><li>k近邻法是一种懒惰学习，他不对样本进行训练。</li></ul><h2 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a><font size=\"4\">k近邻算法</font></h2><ul><li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:<br>$$<br>y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\ i=1,2,…,N; \\ j=1,2,…,K<br>$$<br>其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。</p></li><li><p>k=1时称为最近邻算法</p></li><li><p>k近邻算法没有显式的学习过程</p></li></ul><h2 id=\"k近邻模型\"><a href=\"#k近邻模型\" class=\"headerlink\" title=\"k近邻模型\"></a><font size=\"4\">k近邻模型</font></h2><ul><li><p>k近邻模型即对特征空间的划分。</p></li><li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p></li><li><p>距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。</p></li><li><p>欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：<br>$$<br>L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}<br>$$</p></li><li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p></li><li><p>一般采用交叉验证法确定k值</p></li><li><p>多数表决规则等价于经验风险最小化</p></li></ul><h2 id=\"kd树\"><a href=\"#kd树\" class=\"headerlink\" title=\"kd树\"></a><font size=\"4\">kd树</font></h2><ul><li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li><li>kd树的每一个节点对应于一个k维超矩形区域</li><li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li><li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：<ul><li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li><li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li><li>递归向上回退，对每个节点做如下操作<ul><li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li><li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li><li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li></ul></li></ul></li><li>kd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索</li></ul><h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a><font size=\"5\">支持向量机</font></h1><h2 id=\"线性可分支持向量机与硬间隔最大化\"><a href=\"#线性可分支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可分支持向量机与硬间隔最大化\"></a><font size=\"4\">线性可分支持向量机与硬间隔最大化</font></h2><ul><li>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示</li><li>这里的x是特征向量$(x_1,x_2,…)$，而y是特征向量的标签</li><li>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机</li><li>在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为<br>$$<br>\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}<br>$$</li><li>而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔<br>$$<br>\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\<br>定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\<br>\\gamma=min\\gamma _i \\\\<br>$$</li><li>同时定义相对距离为函数间隔<br>$$<br>\\gamma _i=y_i(wx_i+b) \\\\<br>\\gamma =min\\gamma _i \\\\<br>$$</li><li>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</li><li>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题<br>$$<br>max_{(w,b)} \\gamma \\\\<br>s.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,…,N \\\\<br>$$</li><li>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为<br>$$<br>min_{(w,b)} \\frac12 {||w||}^2 \\\\<br>s.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\<br>$$</li><li>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</li><li>最大间隔分离超平面存在且唯一，证明略</li><li>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</li><li>对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$</li><li>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</li><li>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</li><li>待补充</li></ul><h1 id=\"线代基础\"><a href=\"#线代基础\" class=\"headerlink\" title=\"线代基础\"></a><font size=\"5\">线代基础</font></h1><h2 id=\"Moore-penrose\"><a href=\"#Moore-penrose\" class=\"headerlink\" title=\"Moore-penrose\"></a><font size=\"4\">Moore-penrose</font></h2><ul><li>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆<br>$$<br>A^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T<br>$$</li><li>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：<br>$$<br>A^+=VD^+U^T<br>$$<br>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。</li><li>当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。</li><li>当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。</li><li>待补充</li></ul><h2 id=\"迹\"><a href=\"#迹\" class=\"headerlink\" title=\"迹\"></a><font size=\"4\">迹</font></h2><ul><li>迹运算返回的是矩阵对角元素的和.</li><li>使用迹运算可以描述矩阵Frobenius范数的方式：<br>$$<br>||A_F||=\\sqrt{Tr(AA^T)}<br>$$</li><li>迹具有转置不变性和轮换不变性</li><li>标量的迹是其本身</li></ul><h2 id=\"PCA解释\"><a href=\"#PCA解释\" class=\"headerlink\" title=\"PCA解释\"></a><font size=\"4\">PCA解释</font></h2><ul><li>待补充</li></ul><h1 id=\"概率论信息论\"><a href=\"#概率论信息论\" class=\"headerlink\" title=\"概率论信息论\"></a><font size=\"5\">概率论信息论</font></h1><h2 id=\"Logistic-Sigmoid\"><a href=\"#Logistic-Sigmoid\" class=\"headerlink\" title=\"Logistic Sigmoid\"></a><font size=\"4\">Logistic Sigmoid</font></h2><ul><li>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/cla6i1maiE.png?imageslim\" alt=\"mark\"></li><li>Softmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/KhILG09Ij4.png?imageslim\" alt=\"mark\"></li><li>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象(?)。</li><li>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</li><li>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aA4kidJj2d.png?imageslim\" alt=\"mark\"></li><li>整流线性单元使用函数:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CamKlC3dk8.png?imageslim\" alt=\"mark\"></li><li>其软化版本是softplus:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/daC267kkFk.png?imageslim\" alt=\"mark\"></li><li>Sigmoid和softplus函数的一些性质：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/dJJb644g25.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"KL散度和交叉熵\"><a href=\"#KL散度和交叉熵\" class=\"headerlink\" title=\"KL散度和交叉熵\"></a><font size=\"4\">KL散度和交叉熵</font></h2><ul><li>KL散度 用以衡量PQ两个分布之间的差异，非负且不对称：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BDDchFGAAI.png?imageslim\" alt=\"mark\"></li><li>交叉熵：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BG2cdG0efL.png?imageslim\" alt=\"mark\"></li><li>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</li><li>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</li><li>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Ebg5CkFI9A.png?imageslim\" alt=\"mark\"></li><li>同理可得多元交叉熵(Softmaxs代价函数):<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fhih6BdcbH.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"交叉熵与最大对数似然关系\"><a href=\"#交叉熵与最大对数似然关系\" class=\"headerlink\" title=\"交叉熵与最大对数似然关系\"></a><font size=\"4\">交叉熵与最大对数似然关系</font></h2><ul><li>已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。<br>在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jlKK7jmHlG.png?imageslim\" alt=\"mark\"></li><li>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIkh1LjjBH.png?imageslim\" alt=\"mark\"></li><li>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</li><li>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</li><li>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/L3E7HC8D8J.png?imageslim\" alt=\"mark\"></li><li>最大似然估计具有一致性。</li></ul><h1 id=\"计算方法\"><a href=\"#计算方法\" class=\"headerlink\" title=\"计算方法\"></a><font size=\"5\">计算方法</font></h1><h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a><font size=\"4\">梯度下降</font></h2><ul><li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li><li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li><li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li><li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/F3A336L1gH.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a><font size=\"4\">牛顿法</font></h2><ul><li>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/K9Bhe01f2l.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"约束优化\"><a href=\"#约束优化\" class=\"headerlink\" title=\"约束优化\"></a><font size=\"4\">约束优化</font></h2><ul><li>只包含等式约束条件：Lagrange</li><li>包含不等式约束条件：KTT</li></ul><h1 id=\"修改算法\"><a href=\"#修改算法\" class=\"headerlink\" title=\"修改算法\"></a><font size=\"5\">修改算法</font></h1><h2 id=\"修改假设空间\"><a href=\"#修改假设空间\" class=\"headerlink\" title=\"修改假设空间\"></a><font size=\"4\">修改假设空间</font></h2><ul><li>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</li><li>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEGe5ge292.png?imageslim\" alt=\"mark\"></li><li>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：<br>![mark](<a href=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\" target=\"_blank\" rel=\"noopener\">http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</a></li></ul><h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><ul><li>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</li><li>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Bif507aI3F.png?imageslim\" alt=\"mark\"><br>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</li></ul>","site":{"data":{}},"excerpt":"<hr><p>记录机器学习中关于一些概念和算法的笔记，来源于:</p><ul><li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li><li>西瓜书</li><li>《统计学习方法》</li><li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li></ul><p>更新：</p><ul><li>2017-02-12 更新概论</li><li>2017-03-01 更新k近邻</li><li>2017-03-08 更新SVM</li><li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li><li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容","more":"</li></ul><h1 id=\"统计学习方法概论\"><a href=\"#统计学习方法概论\" class=\"headerlink\" title=\"统计学习方法概论\"></a><font size=\"5\">统计学习方法概论</font></h1><h2 id=\"统计学习，监督学习，三要素\"><a href=\"#统计学习，监督学习，三要素\" class=\"headerlink\" title=\"统计学习，监督学习，三要素\"></a><font size=\"4\">统计学习，监督学习，三要素</font></h2><ul><li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li><li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li><li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li><li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li><li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li><li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li><li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li><li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li><li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$</li><li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li></ul><h2 id=\"模型评估，模型选择\"><a href=\"#模型评估，模型选择\" class=\"headerlink\" title=\"模型评估，模型选择\"></a><font size=\"4\">模型评估，模型选择</font></h2><ul><li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li><li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li><li>模型选择的两种方法：正则化和交叉验证</li></ul><h2 id=\"正则化，交叉验证\"><a href=\"#正则化，交叉验证\" class=\"headerlink\" title=\"正则化，交叉验证\"></a><font size=\"4\">正则化，交叉验证</font></h2><ul><li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li><li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li><li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li></ul><h2 id=\"泛化能力\"><a href=\"#泛化能力\" class=\"headerlink\" title=\"泛化能力\"></a><font size=\"4\">泛化能力</font></h2><ul><li>泛化误差:对未知数据预测的误差</li><li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li><li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$</li></ul><p>$$<br>R_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\<br>\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\<br>$$</p><h2 id=\"生成模型，判别模型\"><a href=\"#生成模型，判别模型\" class=\"headerlink\" title=\"生成模型，判别模型\"></a><font size=\"4\">生成模型，判别模型</font></h2><ul><li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li><li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li><li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li><li>判别方法准确率高，可以抽象数据，简化学习问题</li></ul><h2 id=\"分类，标注，回归\"><a href=\"#分类，标注，回归\" class=\"headerlink\" title=\"分类，标注，回归\"></a><font size=\"4\">分类，标注，回归</font></h2><ul><li>分类，即输出取离散有限值，分类决策函数也叫分类器</li><li>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<br>$$<br>精确率:P=\\frac{TP}{TP+FP} \\\\<br>召回率:R=\\frac{TP}{TP+FN} \\\\<br>1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\<br>$$</li><li>标注:输入一个观测序列，输出一个标记序列</li><li>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</li></ul><h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"5\">k近邻法</font></h1><h2 id=\"k近邻法-1\"><a href=\"#k近邻法-1\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"4\">k近邻法</font></h2><ul><li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li><li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li><li>k近邻法是一种懒惰学习，他不对样本进行训练。</li></ul><h2 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a><font size=\"4\">k近邻算法</font></h2><ul><li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:<br>$$<br>y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\ i=1,2,…,N; \\ j=1,2,…,K<br>$$<br>其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。</p></li><li><p>k=1时称为最近邻算法</p></li><li><p>k近邻算法没有显式的学习过程</p></li></ul><h2 id=\"k近邻模型\"><a href=\"#k近邻模型\" class=\"headerlink\" title=\"k近邻模型\"></a><font size=\"4\">k近邻模型</font></h2><ul><li><p>k近邻模型即对特征空间的划分。</p></li><li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p></li><li><p>距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。</p></li><li><p>欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：<br>$$<br>L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}<br>$$</p></li><li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p></li><li><p>一般采用交叉验证法确定k值</p></li><li><p>多数表决规则等价于经验风险最小化</p></li></ul><h2 id=\"kd树\"><a href=\"#kd树\" class=\"headerlink\" title=\"kd树\"></a><font size=\"4\">kd树</font></h2><ul><li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li><li>kd树的每一个节点对应于一个k维超矩形区域</li><li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li><li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：<ul><li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li><li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li><li>递归向上回退，对每个节点做如下操作<ul><li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li><li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li><li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li></ul></li></ul></li><li>kd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索</li></ul><h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a><font size=\"5\">支持向量机</font></h1><h2 id=\"线性可分支持向量机与硬间隔最大化\"><a href=\"#线性可分支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可分支持向量机与硬间隔最大化\"></a><font size=\"4\">线性可分支持向量机与硬间隔最大化</font></h2><ul><li>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示</li><li>这里的x是特征向量$(x_1,x_2,…)$，而y是特征向量的标签</li><li>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机</li><li>在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为<br>$$<br>\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}<br>$$</li><li>而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔<br>$$<br>\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\<br>定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\<br>\\gamma=min\\gamma _i \\\\<br>$$</li><li>同时定义相对距离为函数间隔<br>$$<br>\\gamma _i=y_i(wx_i+b) \\\\<br>\\gamma =min\\gamma _i \\\\<br>$$</li><li>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</li><li>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题<br>$$<br>max_{(w,b)} \\gamma \\\\<br>s.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,…,N \\\\<br>$$</li><li>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为<br>$$<br>min_{(w,b)} \\frac12 {||w||}^2 \\\\<br>s.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\<br>$$</li><li>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</li><li>最大间隔分离超平面存在且唯一，证明略</li><li>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</li><li>对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$</li><li>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</li><li>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</li><li>待补充</li></ul><h1 id=\"线代基础\"><a href=\"#线代基础\" class=\"headerlink\" title=\"线代基础\"></a><font size=\"5\">线代基础</font></h1><h2 id=\"Moore-penrose\"><a href=\"#Moore-penrose\" class=\"headerlink\" title=\"Moore-penrose\"></a><font size=\"4\">Moore-penrose</font></h2><ul><li>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆<br>$$<br>A^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T<br>$$</li><li>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：<br>$$<br>A^+=VD^+U^T<br>$$<br>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。</li><li>当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。</li><li>当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。</li><li>待补充</li></ul><h2 id=\"迹\"><a href=\"#迹\" class=\"headerlink\" title=\"迹\"></a><font size=\"4\">迹</font></h2><ul><li>迹运算返回的是矩阵对角元素的和.</li><li>使用迹运算可以描述矩阵Frobenius范数的方式：<br>$$<br>||A_F||=\\sqrt{Tr(AA^T)}<br>$$</li><li>迹具有转置不变性和轮换不变性</li><li>标量的迹是其本身</li></ul><h2 id=\"PCA解释\"><a href=\"#PCA解释\" class=\"headerlink\" title=\"PCA解释\"></a><font size=\"4\">PCA解释</font></h2><ul><li>待补充</li></ul><h1 id=\"概率论信息论\"><a href=\"#概率论信息论\" class=\"headerlink\" title=\"概率论信息论\"></a><font size=\"5\">概率论信息论</font></h1><h2 id=\"Logistic-Sigmoid\"><a href=\"#Logistic-Sigmoid\" class=\"headerlink\" title=\"Logistic Sigmoid\"></a><font size=\"4\">Logistic Sigmoid</font></h2><ul><li>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/cla6i1maiE.png?imageslim\" alt=\"mark\"></li><li>Softmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/KhILG09Ij4.png?imageslim\" alt=\"mark\"></li><li>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象(?)。</li><li>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</li><li>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aA4kidJj2d.png?imageslim\" alt=\"mark\"></li><li>整流线性单元使用函数:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CamKlC3dk8.png?imageslim\" alt=\"mark\"></li><li>其软化版本是softplus:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/daC267kkFk.png?imageslim\" alt=\"mark\"></li><li>Sigmoid和softplus函数的一些性质：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/dJJb644g25.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"KL散度和交叉熵\"><a href=\"#KL散度和交叉熵\" class=\"headerlink\" title=\"KL散度和交叉熵\"></a><font size=\"4\">KL散度和交叉熵</font></h2><ul><li>KL散度 用以衡量PQ两个分布之间的差异，非负且不对称：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BDDchFGAAI.png?imageslim\" alt=\"mark\"></li><li>交叉熵：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/BG2cdG0efL.png?imageslim\" alt=\"mark\"></li><li>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</li><li>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</li><li>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Ebg5CkFI9A.png?imageslim\" alt=\"mark\"></li><li>同理可得多元交叉熵(Softmaxs代价函数):<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fhih6BdcbH.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"交叉熵与最大对数似然关系\"><a href=\"#交叉熵与最大对数似然关系\" class=\"headerlink\" title=\"交叉熵与最大对数似然关系\"></a><font size=\"4\">交叉熵与最大对数似然关系</font></h2><ul><li>已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。<br>在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jlKK7jmHlG.png?imageslim\" alt=\"mark\"></li><li>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIkh1LjjBH.png?imageslim\" alt=\"mark\"></li><li>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</li><li>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</li><li>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/L3E7HC8D8J.png?imageslim\" alt=\"mark\"></li><li>最大似然估计具有一致性。</li></ul><h1 id=\"计算方法\"><a href=\"#计算方法\" class=\"headerlink\" title=\"计算方法\"></a><font size=\"5\">计算方法</font></h1><h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a><font size=\"4\">梯度下降</font></h2><ul><li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li><li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li><li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li><li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/F3A336L1gH.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a><font size=\"4\">牛顿法</font></h2><ul><li>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/K9Bhe01f2l.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"约束优化\"><a href=\"#约束优化\" class=\"headerlink\" title=\"约束优化\"></a><font size=\"4\">约束优化</font></h2><ul><li>只包含等式约束条件：Lagrange</li><li>包含不等式约束条件：KTT</li></ul><h1 id=\"修改算法\"><a href=\"#修改算法\" class=\"headerlink\" title=\"修改算法\"></a><font size=\"5\">修改算法</font></h1><h2 id=\"修改假设空间\"><a href=\"#修改假设空间\" class=\"headerlink\" title=\"修改假设空间\"></a><font size=\"4\">修改假设空间</font></h2><ul><li>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</li><li>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEGe5ge292.png?imageslim\" alt=\"mark\"></li><li>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：<br>![mark](<a href=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\" target=\"_blank\" rel=\"noopener\">http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GK8AHej2Fe.png?imagesli此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</a></li></ul><h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><ul><li>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</li><li>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Bif507aI3F.png?imageslim\" alt=\"mark\"><br>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sat Sep 15 2018 10:03:54 GMT+0800 (中国标准时间)","title":"机器学习笔记","path":"2017/02/12/MachineLearningNote/","eyeCatchImage":null,"excerpt":"<hr><p>记录机器学习中关于一些概念和算法的笔记，来源于:</p><ul><li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li><li>西瓜书</li><li>《统计学习方法》</li><li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li></ul><p>更新：</p><ul><li>2017-02-12 更新概论</li><li>2017-03-01 更新k近邻</li><li>2017-03-08 更新SVM</li><li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li><li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容","date":"2017-02-12T14:40:38.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["code","machine learning"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:神经网络基础","date":"2017-02-10T04:20:17.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/225753048.png"],"html":true,"_content":"\n***\n以简单的点集分类和泰坦尼克号作为例子，学习BP神经网络\n\n\n<!--more-->\n\n# <font size=5 >模型</font>\n\n## <font size=4 >模型结构</font>\n\n- 就以文章头部那张图片为例，机器学习(监督学习部分)是找到一个模型，通过已知的条件和结果，调整模型中各个参数的值，最后得到一个模型，对于某一类问题，将条件输入模型，可以输出(预测)较为正确的结果，神经网络的模型结构就如图所示，左边的input就是条件，右边的output就是结果\n- 模型至少有2个层，输入层与输出层中间可能有多个层，每一层包含若干个节点，每一个节点是一个函数，函数的输入是上一层所有节点的输出通过路径加权再减去这个节点本身的阈值，输出再作为下一层每一个个节点输入的一部分。如果某个节点有输出，则叫做被激活，节点所代表的函数就是激活函数，路径上的加权系数以及阈值就是网络参数。当一个神经网络构建好且学习好后，输入一组条件，每一层的节点依次被上一层节点激活输出，直到最后一层的节点输出结果。机器学习学习的成果就是正确的网络参数，一个神经网络的数据模型也可以看成一组网络参数。在本例中我们研究分类问题，最后的输出节点输出这个条件下属于某一类的概率。\n\n## <font size=4 >网络参数</font>\n\n- 节点的输出可以写成\n  $$\n  y=f(\\sum_{i=1}^nw_ix_i-\\theta )\n  $$\n  其中n是连接到这个节点的路径数，$w_i$是路径上的权值，$\\theta$是阈值,$f$是激活函数。阈值的理解:可以看成上一层有一个哑节点，它的输出恒为-1，这个哑节点连接过来的权值$w_{n+1}$就是阈值。阈值和权值统称为网络参数。\n\n- 网络参数的学习其实就是根据这次训练的结果和理想结果对比，将参数增加一个$\\Delta$值，进行调整。\n  $$\n  \\Delta w_i=\\eta (y-y_0)x_i\n  $$\n  上式即最简单的感知机模型(无隐层)的权重调整函数，其中$\\eta$称为学习率。\n  \n## <font size=4 >激活函数</font>\n\n- 激活函数有许多种，比如最简单的阶跃函数(加权输入大于阈值则输出1否则无输出)，或者Sigmoid函数、tanh函数。输出节点一般用Softmax函数。\n\n- Sigmoid函数有一种很好的性质:$f'(x)=f(x)(1-f(x))$\n\n  tanh函数也有一种很好的性质:$tanh'(x)=1-tanh^2(x)$\n\n  可以看到这两种函数都可以通过本身计算出导函数，而在多层前馈神经网络中，我们需要利用负梯度来调整网络参数，计算负梯度需要利用导数，这样的性质能方便推导公式。\n\n\n# <font size=5 >损失函数</font>\n\n## <font size=4 >损失评估</font>\n\n- 每一次训练后，我们需要知道这一次训练是否降低了与目标之间的误差，这个误差的量化就利用损失函数\n- 损失评估可以防止BP神经网络过拟合，例如早停策略:将数据分成训练集和验证集，训练集用来计算梯度，更新网络参数，验证机用来评估误差，若训练集误差降低而验证集误差升高，则停止训练以防止过拟合，同时返回具有最小验证集误差的网络参数。\n\n## <font size=4 >交叉熵损失</font>\n\n- 交叉熵损失的计算公式\n  $$\n  L_i=-log(\\frac {e^f_{yi}} {\\sum_je^{f_j}})\n  $$\n  f是最后一层的节点的输出，可以看到括号里实际上是将最后一层输出的每一类的概率指数函数归一化，再求log。假如某一行训练元素，正确分类到a类，结果输出a类的概率低，则括号里的值接近0,$L_i$就会趋近无穷，即损失太多，反之，括号里的值接近1，$L_i$趋近0，即几乎无损失。\n\n  一般还会加入正则化项\n  $$\n  L=\\frac1N \\sum_iL_I+ \\frac12 \\lambda \\sum_k\\sum_lW_{k,l}^2\n  $$\n\n\n\n\n\n\n\n\n## <font size=4 >均方误差</font>\n\n- 均方误差的衡量就很简单暴力\n  $$\n  E_k=\\frac12\\sum_{j=1}^l(y_0j^k-y_j^k)^2\n  $$\n  $\\frac 12$是为了方便做负梯度计算\n\n\n# <font size=5 >BP算法</font>\n\n## <font size=4 >负梯度</font>\n\n- 梯度下降法或最速下降法是求解无约束最优化问题的一种常见方法，是迭代算法，每一步需要求解目标函数的梯度向量。\n\n- BP算法以目标的负梯度方向对网络参数进行调整，目标函数即损失函数，即\n\n $$\n \\Delta w=-\\eta \\frac {\\partial L}{\\partial w}\n $$\n\n  这样求出来的意义是，调整值是损失函数的负梯度乘学习率，即让损失减小的方向调整\n\n- 对于均方误差和交叉熵损失两种损失函数，求出来的负梯度不同，但都可以通过他们的性质优化，以交叉熵损失为例，求出来的负梯度为：\n\n  ![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/194245109.jpg)\n\n  其中$W_1,b_1$是第一层网络参数，$W_2,b_2$是第二层网络参数\n\n## <font size=4 >更新公式</font>\n\n- 对每一次学习先得到预测结果，再得出损失函数，从而计算出负梯度，乘上学习率并正则化，就是最终这一次学习到的网络参数调整值\n\n\n# <font size=5 >代码实现</font>\n\n## <font size=4 >初始化与可视化</font>\n\n```python\nclass Config:\n    nn_input_dim = 2  # input layer dimensionality\n    nn_output_dim = 2  # output layer dimensionality\n    # Gradient descent parameters (I picked these by hand)\n    epsilon = 0.01  # learning rate for gradient descent\n    reg_lambda = 0.01  # regularization strength\n\n\ndef generate_data():\n    np.random.seed(0)\n    X, y = datasets.make_moons(200, noise=0.20)\n    return X, y\n\n\ndef visualize(X, y, model):\n    # plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n    # plt.show()\n    plot_decision_boundary(lambda x:predict(model,x), X, y)\n    plt.title(\"Logistic Regression\")\n\n\ndef plot_decision_boundary(pred_func, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    plt.show()\n```\n\n\n\n## <font size=4 >计算损失</font>\n\n- 其中probs即指数归一化后的各类别概率\n- probs[range(num_examples), y]即每一个预测结果中实际正确类别的预测概率，用log还原指数并累加即交叉熵损失data_loss,之后再正则化\n\n```python\n# Helper function to evaluate the total loss on the dataset\ndef calculate_loss(model, X, y):\n    num_examples = len(X)  # training set size\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation to calculate our predictions\n    z1 = X.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    data_loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    data_loss += Config.reg_lambda / 2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n    return 1. / num_examples * data_loss\n```\n\n\n\n## <font size=4 >逆向误差传输</font>\n\n```python\n# This function learns parameters for the neural network and returns the model.\n# - nn_hdim: Number of nodes in the hidden layer\n# - num_passes: Number of passes through the training data for gradient descent\n# - print_loss: If True, print the loss every 1000 iterations\ndef build_model(X, y, nn_hdim, num_passes=20000, print_loss=False):\n    # Initialize the parameters to random values. We need to learn these.\n    num_examples = len(X)\n    np.random.seed(0)\n    W1 = np.random.randn(Config.nn_input_dim, nn_hdim) / np.sqrt(Config.nn_input_dim)\n    b1 = np.zeros((1, nn_hdim))\n    W2 = np.random.randn(nn_hdim, Config.nn_output_dim) / np.sqrt(nn_hdim)\n    b2 = np.zeros((1, Config.nn_output_dim))\n\n    # This is what we return at the end\n    model = {}\n\n    # Gradient descent. For each batch...\n    for i in range(0, num_passes):\n\n        # Forward propagation\n        z1 = X.dot(W1) + b1\n        a1 = np.tanh(z1)\n        z2 = a1.dot(W2) + b2\n        exp_scores = np.exp(z2)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Backpropagation\n        delta3 = probs\n        delta3[range(num_examples), y] -= 1\n        dW2 = (a1.T).dot(delta3)\n        db2 = np.sum(delta3, axis=0, keepdims=True)\n        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n        dW1 = np.dot(X.T, delta2)\n        db1 = np.sum(delta2, axis=0)\n\n        # Add regularization terms (b1 and b2 don't have regularization terms)\n        dW2 += Config.reg_lambda * W2\n        dW1 += Config.reg_lambda * W1\n\n        # Gradient descent parameter update\n        W1 += -Config.epsilon * dW1\n        b1 += -Config.epsilon * db1\n        W2 += -Config.epsilon * dW2\n        b2 += -Config.epsilon * db2\n\n        # Assign new parameters to the model\n        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\n        # Optionally print the loss.\n        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n        if print_loss and i % 1000 == 0:\n            print(\"Loss after iteration %i: %f\" % (i, calculate_loss(model, X, y)))\n\n    return model\n```\n\n\n\n## <font size=4 >预测</font>\n\n```python\ndef predict(model, x):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return np.argmax(probs, axis=1)\n```\n\n\n\n\n# <font size=5 >学习率，正则化</font>\n\n## <font size=4 >梯度下降的学习速率</font>\n\n## <font size=4 >正则化</font>\n\n# <font size=5 >点集分类结果</font>\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/223945007.JPG)\n\n# <font size=5 >泰坦尼克号预测调参</font>\n\n# <font size=5 >泰坦尼克号预测结果</font>\n\n# <font size=5 >参考</font>\n\n>[IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)\n>[CS231n Convolutional Neural Networks or Visual Recognition---optimization](http://cs231n.github.io/optimization-1/#gd)\n>[CS231n Convolutional Neural Networks or Visual Recognition---neural-networks](http://cs231n.github.io/neural-networks-3/#gradcheck)\n>[龙心尘](http://blog.csdn.net/longxinchen_ml/article/details/50521933)\n\n\n\n\n","source":"_posts/NeuralNetworks1.md","raw":"---\ntitle: '机器学习入门:神经网络基础'\ndate: 2017-02-10 12:20:17\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/225753048.png\nhtml: true\n---\n\n***\n以简单的点集分类和泰坦尼克号作为例子，学习BP神经网络\n\n\n<!--more-->\n\n# <font size=5 >模型</font>\n\n## <font size=4 >模型结构</font>\n\n- 就以文章头部那张图片为例，机器学习(监督学习部分)是找到一个模型，通过已知的条件和结果，调整模型中各个参数的值，最后得到一个模型，对于某一类问题，将条件输入模型，可以输出(预测)较为正确的结果，神经网络的模型结构就如图所示，左边的input就是条件，右边的output就是结果\n- 模型至少有2个层，输入层与输出层中间可能有多个层，每一层包含若干个节点，每一个节点是一个函数，函数的输入是上一层所有节点的输出通过路径加权再减去这个节点本身的阈值，输出再作为下一层每一个个节点输入的一部分。如果某个节点有输出，则叫做被激活，节点所代表的函数就是激活函数，路径上的加权系数以及阈值就是网络参数。当一个神经网络构建好且学习好后，输入一组条件，每一层的节点依次被上一层节点激活输出，直到最后一层的节点输出结果。机器学习学习的成果就是正确的网络参数，一个神经网络的数据模型也可以看成一组网络参数。在本例中我们研究分类问题，最后的输出节点输出这个条件下属于某一类的概率。\n\n## <font size=4 >网络参数</font>\n\n- 节点的输出可以写成\n  $$\n  y=f(\\sum_{i=1}^nw_ix_i-\\theta )\n  $$\n  其中n是连接到这个节点的路径数，$w_i$是路径上的权值，$\\theta$是阈值,$f$是激活函数。阈值的理解:可以看成上一层有一个哑节点，它的输出恒为-1，这个哑节点连接过来的权值$w_{n+1}$就是阈值。阈值和权值统称为网络参数。\n\n- 网络参数的学习其实就是根据这次训练的结果和理想结果对比，将参数增加一个$\\Delta$值，进行调整。\n  $$\n  \\Delta w_i=\\eta (y-y_0)x_i\n  $$\n  上式即最简单的感知机模型(无隐层)的权重调整函数，其中$\\eta$称为学习率。\n  \n## <font size=4 >激活函数</font>\n\n- 激活函数有许多种，比如最简单的阶跃函数(加权输入大于阈值则输出1否则无输出)，或者Sigmoid函数、tanh函数。输出节点一般用Softmax函数。\n\n- Sigmoid函数有一种很好的性质:$f'(x)=f(x)(1-f(x))$\n\n  tanh函数也有一种很好的性质:$tanh'(x)=1-tanh^2(x)$\n\n  可以看到这两种函数都可以通过本身计算出导函数，而在多层前馈神经网络中，我们需要利用负梯度来调整网络参数，计算负梯度需要利用导数，这样的性质能方便推导公式。\n\n\n# <font size=5 >损失函数</font>\n\n## <font size=4 >损失评估</font>\n\n- 每一次训练后，我们需要知道这一次训练是否降低了与目标之间的误差，这个误差的量化就利用损失函数\n- 损失评估可以防止BP神经网络过拟合，例如早停策略:将数据分成训练集和验证集，训练集用来计算梯度，更新网络参数，验证机用来评估误差，若训练集误差降低而验证集误差升高，则停止训练以防止过拟合，同时返回具有最小验证集误差的网络参数。\n\n## <font size=4 >交叉熵损失</font>\n\n- 交叉熵损失的计算公式\n  $$\n  L_i=-log(\\frac {e^f_{yi}} {\\sum_je^{f_j}})\n  $$\n  f是最后一层的节点的输出，可以看到括号里实际上是将最后一层输出的每一类的概率指数函数归一化，再求log。假如某一行训练元素，正确分类到a类，结果输出a类的概率低，则括号里的值接近0,$L_i$就会趋近无穷，即损失太多，反之，括号里的值接近1，$L_i$趋近0，即几乎无损失。\n\n  一般还会加入正则化项\n  $$\n  L=\\frac1N \\sum_iL_I+ \\frac12 \\lambda \\sum_k\\sum_lW_{k,l}^2\n  $$\n\n\n\n\n\n\n\n\n## <font size=4 >均方误差</font>\n\n- 均方误差的衡量就很简单暴力\n  $$\n  E_k=\\frac12\\sum_{j=1}^l(y_0j^k-y_j^k)^2\n  $$\n  $\\frac 12$是为了方便做负梯度计算\n\n\n# <font size=5 >BP算法</font>\n\n## <font size=4 >负梯度</font>\n\n- 梯度下降法或最速下降法是求解无约束最优化问题的一种常见方法，是迭代算法，每一步需要求解目标函数的梯度向量。\n\n- BP算法以目标的负梯度方向对网络参数进行调整，目标函数即损失函数，即\n\n $$\n \\Delta w=-\\eta \\frac {\\partial L}{\\partial w}\n $$\n\n  这样求出来的意义是，调整值是损失函数的负梯度乘学习率，即让损失减小的方向调整\n\n- 对于均方误差和交叉熵损失两种损失函数，求出来的负梯度不同，但都可以通过他们的性质优化，以交叉熵损失为例，求出来的负梯度为：\n\n  ![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/194245109.jpg)\n\n  其中$W_1,b_1$是第一层网络参数，$W_2,b_2$是第二层网络参数\n\n## <font size=4 >更新公式</font>\n\n- 对每一次学习先得到预测结果，再得出损失函数，从而计算出负梯度，乘上学习率并正则化，就是最终这一次学习到的网络参数调整值\n\n\n# <font size=5 >代码实现</font>\n\n## <font size=4 >初始化与可视化</font>\n\n```python\nclass Config:\n    nn_input_dim = 2  # input layer dimensionality\n    nn_output_dim = 2  # output layer dimensionality\n    # Gradient descent parameters (I picked these by hand)\n    epsilon = 0.01  # learning rate for gradient descent\n    reg_lambda = 0.01  # regularization strength\n\n\ndef generate_data():\n    np.random.seed(0)\n    X, y = datasets.make_moons(200, noise=0.20)\n    return X, y\n\n\ndef visualize(X, y, model):\n    # plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n    # plt.show()\n    plot_decision_boundary(lambda x:predict(model,x), X, y)\n    plt.title(\"Logistic Regression\")\n\n\ndef plot_decision_boundary(pred_func, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    plt.show()\n```\n\n\n\n## <font size=4 >计算损失</font>\n\n- 其中probs即指数归一化后的各类别概率\n- probs[range(num_examples), y]即每一个预测结果中实际正确类别的预测概率，用log还原指数并累加即交叉熵损失data_loss,之后再正则化\n\n```python\n# Helper function to evaluate the total loss on the dataset\ndef calculate_loss(model, X, y):\n    num_examples = len(X)  # training set size\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation to calculate our predictions\n    z1 = X.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    data_loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    data_loss += Config.reg_lambda / 2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n    return 1. / num_examples * data_loss\n```\n\n\n\n## <font size=4 >逆向误差传输</font>\n\n```python\n# This function learns parameters for the neural network and returns the model.\n# - nn_hdim: Number of nodes in the hidden layer\n# - num_passes: Number of passes through the training data for gradient descent\n# - print_loss: If True, print the loss every 1000 iterations\ndef build_model(X, y, nn_hdim, num_passes=20000, print_loss=False):\n    # Initialize the parameters to random values. We need to learn these.\n    num_examples = len(X)\n    np.random.seed(0)\n    W1 = np.random.randn(Config.nn_input_dim, nn_hdim) / np.sqrt(Config.nn_input_dim)\n    b1 = np.zeros((1, nn_hdim))\n    W2 = np.random.randn(nn_hdim, Config.nn_output_dim) / np.sqrt(nn_hdim)\n    b2 = np.zeros((1, Config.nn_output_dim))\n\n    # This is what we return at the end\n    model = {}\n\n    # Gradient descent. For each batch...\n    for i in range(0, num_passes):\n\n        # Forward propagation\n        z1 = X.dot(W1) + b1\n        a1 = np.tanh(z1)\n        z2 = a1.dot(W2) + b2\n        exp_scores = np.exp(z2)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Backpropagation\n        delta3 = probs\n        delta3[range(num_examples), y] -= 1\n        dW2 = (a1.T).dot(delta3)\n        db2 = np.sum(delta3, axis=0, keepdims=True)\n        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n        dW1 = np.dot(X.T, delta2)\n        db1 = np.sum(delta2, axis=0)\n\n        # Add regularization terms (b1 and b2 don't have regularization terms)\n        dW2 += Config.reg_lambda * W2\n        dW1 += Config.reg_lambda * W1\n\n        # Gradient descent parameter update\n        W1 += -Config.epsilon * dW1\n        b1 += -Config.epsilon * db1\n        W2 += -Config.epsilon * dW2\n        b2 += -Config.epsilon * db2\n\n        # Assign new parameters to the model\n        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n\n        # Optionally print the loss.\n        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n        if print_loss and i % 1000 == 0:\n            print(\"Loss after iteration %i: %f\" % (i, calculate_loss(model, X, y)))\n\n    return model\n```\n\n\n\n## <font size=4 >预测</font>\n\n```python\ndef predict(model, x):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return np.argmax(probs, axis=1)\n```\n\n\n\n\n# <font size=5 >学习率，正则化</font>\n\n## <font size=4 >梯度下降的学习速率</font>\n\n## <font size=4 >正则化</font>\n\n# <font size=5 >点集分类结果</font>\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/223945007.JPG)\n\n# <font size=5 >泰坦尼克号预测调参</font>\n\n# <font size=5 >泰坦尼克号预测结果</font>\n\n# <font size=5 >参考</font>\n\n>[IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)\n>[CS231n Convolutional Neural Networks or Visual Recognition---optimization](http://cs231n.github.io/optimization-1/#gd)\n>[CS231n Convolutional Neural Networks or Visual Recognition---neural-networks](http://cs231n.github.io/neural-networks-3/#gradcheck)\n>[龙心尘](http://blog.csdn.net/longxinchen_ml/article/details/50521933)\n\n\n\n\n","slug":"NeuralNetworks1","published":1,"updated":"2018-07-23T01:28:38.487Z","comments":1,"layout":"post","link":"","_id":"cjmd072cj000eqcw6fqw2quk3","content":"<hr><p>以简单的点集分类和泰坦尼克号作为例子，学习BP神经网络</p><a id=\"more\"></a><h1 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a><font size=\"5\">模型</font></h1><h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a><font size=\"4\">模型结构</font></h2><ul><li>就以文章头部那张图片为例，机器学习(监督学习部分)是找到一个模型，通过已知的条件和结果，调整模型中各个参数的值，最后得到一个模型，对于某一类问题，将条件输入模型，可以输出(预测)较为正确的结果，神经网络的模型结构就如图所示，左边的input就是条件，右边的output就是结果</li><li>模型至少有2个层，输入层与输出层中间可能有多个层，每一层包含若干个节点，每一个节点是一个函数，函数的输入是上一层所有节点的输出通过路径加权再减去这个节点本身的阈值，输出再作为下一层每一个个节点输入的一部分。如果某个节点有输出，则叫做被激活，节点所代表的函数就是激活函数，路径上的加权系数以及阈值就是网络参数。当一个神经网络构建好且学习好后，输入一组条件，每一层的节点依次被上一层节点激活输出，直到最后一层的节点输出结果。机器学习学习的成果就是正确的网络参数，一个神经网络的数据模型也可以看成一组网络参数。在本例中我们研究分类问题，最后的输出节点输出这个条件下属于某一类的概率。</li></ul><h2 id=\"网络参数\"><a href=\"#网络参数\" class=\"headerlink\" title=\"网络参数\"></a><font size=\"4\">网络参数</font></h2><ul><li><p>节点的输出可以写成<br>$$<br>y=f(\\sum_{i=1}^nw_ix_i-\\theta )<br>$$<br>其中n是连接到这个节点的路径数，$w_i$是路径上的权值，$\\theta$是阈值,$f$是激活函数。阈值的理解:可以看成上一层有一个哑节点，它的输出恒为-1，这个哑节点连接过来的权值$w_{n+1}$就是阈值。阈值和权值统称为网络参数。</p></li><li><p>网络参数的学习其实就是根据这次训练的结果和理想结果对比，将参数增加一个$\\Delta$值，进行调整。<br>$$<br>\\Delta w_i=\\eta (y-y_0)x_i<br>$$<br>上式即最简单的感知机模型(无隐层)的权重调整函数，其中$\\eta$称为学习率。</p></li></ul><h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a><font size=\"4\">激活函数</font></h2><ul><li><p>激活函数有许多种，比如最简单的阶跃函数(加权输入大于阈值则输出1否则无输出)，或者Sigmoid函数、tanh函数。输出节点一般用Softmax函数。</p></li><li><p>Sigmoid函数有一种很好的性质:$f’(x)=f(x)(1-f(x))$</p><p>tanh函数也有一种很好的性质:$tanh’(x)=1-tanh^2(x)$</p><p>可以看到这两种函数都可以通过本身计算出导函数，而在多层前馈神经网络中，我们需要利用负梯度来调整网络参数，计算负梯度需要利用导数，这样的性质能方便推导公式。</p></li></ul><h1 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a><font size=\"5\">损失函数</font></h1><h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a><font size=\"4\">损失评估</font></h2><ul><li>每一次训练后，我们需要知道这一次训练是否降低了与目标之间的误差，这个误差的量化就利用损失函数</li><li>损失评估可以防止BP神经网络过拟合，例如早停策略:将数据分成训练集和验证集，训练集用来计算梯度，更新网络参数，验证机用来评估误差，若训练集误差降低而验证集误差升高，则停止训练以防止过拟合，同时返回具有最小验证集误差的网络参数。</li></ul><h2 id=\"交叉熵损失\"><a href=\"#交叉熵损失\" class=\"headerlink\" title=\"交叉熵损失\"></a><font size=\"4\">交叉熵损失</font></h2><ul><li><p>交叉熵损失的计算公式<br>$$<br>L_i=-log(\\frac {e^f_{yi}} {\\sum_je^{f_j}})<br>$$<br>f是最后一层的节点的输出，可以看到括号里实际上是将最后一层输出的每一类的概率指数函数归一化，再求log。假如某一行训练元素，正确分类到a类，结果输出a类的概率低，则括号里的值接近0,$L_i$就会趋近无穷，即损失太多，反之，括号里的值接近1，$L_i$趋近0，即几乎无损失。</p><p>一般还会加入正则化项<br>$$<br>L=\\frac1N \\sum_iL_I+ \\frac12 \\lambda \\sum_k\\sum_lW_{k,l}^2<br>$$</p></li></ul><h2 id=\"均方误差\"><a href=\"#均方误差\" class=\"headerlink\" title=\"均方误差\"></a><font size=\"4\">均方误差</font></h2><ul><li>均方误差的衡量就很简单暴力<br>$$<br>E_k=\\frac12\\sum_{j=1}^l(y_0j^k-y_j^k)^2<br>$$<br>$\\frac 12$是为了方便做负梯度计算</li></ul><h1 id=\"BP算法\"><a href=\"#BP算法\" class=\"headerlink\" title=\"BP算法\"></a><font size=\"5\">BP算法</font></h1><h2 id=\"负梯度\"><a href=\"#负梯度\" class=\"headerlink\" title=\"负梯度\"></a><font size=\"4\">负梯度</font></h2><ul><li><p>梯度下降法或最速下降法是求解无约束最优化问题的一种常见方法，是迭代算法，每一步需要求解目标函数的梯度向量。</p></li><li><p>BP算法以目标的负梯度方向对网络参数进行调整，目标函数即损失函数，即</p><p>$$<br>\\Delta w=-\\eta \\frac {\\partial L}{\\partial w}<br>$$</p><p>这样求出来的意义是，调整值是损失函数的负梯度乘学习率，即让损失减小的方向调整</p></li><li><p>对于均方误差和交叉熵损失两种损失函数，求出来的负梯度不同，但都可以通过他们的性质优化，以交叉熵损失为例，求出来的负梯度为：</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/194245109.jpg\" alt=\"mark\"></p><p>其中$W_1,b_1$是第一层网络参数，$W_2,b_2$是第二层网络参数</p></li></ul><h2 id=\"更新公式\"><a href=\"#更新公式\" class=\"headerlink\" title=\"更新公式\"></a><font size=\"4\">更新公式</font></h2><ul><li>对每一次学习先得到预测结果，再得出损失函数，从而计算出负梯度，乘上学习率并正则化，就是最终这一次学习到的网络参数调整值</li></ul><h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a><font size=\"5\">代码实现</font></h1><h2 id=\"初始化与可视化\"><a href=\"#初始化与可视化\" class=\"headerlink\" title=\"初始化与可视化\"></a><font size=\"4\">初始化与可视化</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Config</span>:</span></span><br><span class=\"line\">    nn_input_dim = <span class=\"number\">2</span>  <span class=\"comment\"># input layer dimensionality</span></span><br><span class=\"line\">    nn_output_dim = <span class=\"number\">2</span>  <span class=\"comment\"># output layer dimensionality</span></span><br><span class=\"line\">    <span class=\"comment\"># Gradient descent parameters (I picked these by hand)</span></span><br><span class=\"line\">    epsilon = <span class=\"number\">0.01</span>  <span class=\"comment\"># learning rate for gradient descent</span></span><br><span class=\"line\">    reg_lambda = <span class=\"number\">0.01</span>  <span class=\"comment\"># regularization strength</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">    X, y = datasets.make_moons(<span class=\"number\">200</span>, noise=<span class=\"number\">0.20</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">visualize</span><span class=\"params\">(X, y, model)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)</span></span><br><span class=\"line\">    <span class=\"comment\"># plt.show()</span></span><br><span class=\"line\">    plot_decision_boundary(<span class=\"keyword\">lambda</span> x:predict(model,x), X, y)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Logistic Regression\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(pred_func, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">.5</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">.5</span></span><br><span class=\"line\">    y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">.5</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">.5</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole gid</span></span><br><span class=\"line\">    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure><h2 id=\"计算损失\"><a href=\"#计算损失\" class=\"headerlink\" title=\"计算损失\"></a><font size=\"4\">计算损失</font></h2><ul><li>其中probs即指数归一化后的各类别概率</li><li>probs[range(num_examples), y]即每一个预测结果中实际正确类别的预测概率，用log还原指数并累加即交叉熵损失data_loss,之后再正则化</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Helper function to evaluate the total loss on the dataset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_loss</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    num_examples = len(X)  <span class=\"comment\"># training set size</span></span><br><span class=\"line\">    W1, b1, W2, b2 = model[<span class=\"string\">'W1'</span>], model[<span class=\"string\">'b1'</span>], model[<span class=\"string\">'W2'</span>], model[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation to calculate our predictions</span></span><br><span class=\"line\">    z1 = X.dot(W1) + b1</span><br><span class=\"line\">    a1 = np.tanh(z1)</span><br><span class=\"line\">    z2 = a1.dot(W2) + b2</span><br><span class=\"line\">    exp_scores = np.exp(z2)</span><br><span class=\"line\">    probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Calculating the loss</span></span><br><span class=\"line\">    corect_logprobs = -np.log(probs[range(num_examples), y])</span><br><span class=\"line\">    data_loss = np.sum(corect_logprobs)</span><br><span class=\"line\">    <span class=\"comment\"># Add regulatization term to loss (optional)</span></span><br><span class=\"line\">    data_loss += Config.reg_lambda / <span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / num_examples * data_loss</span><br></pre></td></tr></table></figure><h2 id=\"逆向误差传输\"><a href=\"#逆向误差传输\" class=\"headerlink\" title=\"逆向误差传输\"></a><font size=\"4\">逆向误差传输</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This function learns parameters for the neural network and returns the model.</span></span><br><span class=\"line\"><span class=\"comment\"># - nn_hdim: Number of nodes in the hidden layer</span></span><br><span class=\"line\"><span class=\"comment\"># - num_passes: Number of passes through the training data for gradient descent</span></span><br><span class=\"line\"><span class=\"comment\"># - print_loss: If True, print the loss every 1000 iterations</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build_model</span><span class=\"params\">(X, y, nn_hdim, num_passes=<span class=\"number\">20000</span>, print_loss=False)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the parameters to random values. We need to learn these.</span></span><br><span class=\"line\">    num_examples = len(X)</span><br><span class=\"line\">    np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">    W1 = np.random.randn(Config.nn_input_dim, nn_hdim) / np.sqrt(Config.nn_input_dim)</span><br><span class=\"line\">    b1 = np.zeros((<span class=\"number\">1</span>, nn_hdim))</span><br><span class=\"line\">    W2 = np.random.randn(nn_hdim, Config.nn_output_dim) / np.sqrt(nn_hdim)</span><br><span class=\"line\">    b2 = np.zeros((<span class=\"number\">1</span>, Config.nn_output_dim))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># This is what we return at the end</span></span><br><span class=\"line\">    model = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Gradient descent. For each batch...</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_passes):</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        z1 = X.dot(W1) + b1</span><br><span class=\"line\">        a1 = np.tanh(z1)</span><br><span class=\"line\">        z2 = a1.dot(W2) + b2</span><br><span class=\"line\">        exp_scores = np.exp(z2)</span><br><span class=\"line\">        probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Backpropagation</span></span><br><span class=\"line\">        delta3 = probs</span><br><span class=\"line\">        delta3[range(num_examples), y] -= <span class=\"number\">1</span></span><br><span class=\"line\">        dW2 = (a1.T).dot(delta3)</span><br><span class=\"line\">        db2 = np.sum(delta3, axis=<span class=\"number\">0</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        delta2 = delta3.dot(W2.T) * (<span class=\"number\">1</span> - np.power(a1, <span class=\"number\">2</span>))</span><br><span class=\"line\">        dW1 = np.dot(X.T, delta2)</span><br><span class=\"line\">        db1 = np.sum(delta2, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Add regularization terms (b1 and b2 don't have regularization terms)</span></span><br><span class=\"line\">        dW2 += Config.reg_lambda * W2</span><br><span class=\"line\">        dW1 += Config.reg_lambda * W1</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Gradient descent parameter update</span></span><br><span class=\"line\">        W1 += -Config.epsilon * dW1</span><br><span class=\"line\">        b1 += -Config.epsilon * db1</span><br><span class=\"line\">        W2 += -Config.epsilon * dW2</span><br><span class=\"line\">        b2 += -Config.epsilon * db2</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Assign new parameters to the model</span></span><br><span class=\"line\">        model = &#123;<span class=\"string\">'W1'</span>: W1, <span class=\"string\">'b1'</span>: b1, <span class=\"string\">'W2'</span>: W2, <span class=\"string\">'b2'</span>: b2&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Optionally print the loss.</span></span><br><span class=\"line\">        <span class=\"comment\"># This is expensive because it uses the whole dataset, so we don't want to do it too often.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_loss <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Loss after iteration %i: %f\"</span> % (i, calculate_loss(model, X, y)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure><h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a><font size=\"4\">预测</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(model, x)</span>:</span></span><br><span class=\"line\">    W1, b1, W2, b2 = model[<span class=\"string\">'W1'</span>], model[<span class=\"string\">'b1'</span>], model[<span class=\"string\">'W2'</span>], model[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    z1 = x.dot(W1) + b1</span><br><span class=\"line\">    a1 = np.tanh(z1)</span><br><span class=\"line\">    z2 = a1.dot(W2) + b2</span><br><span class=\"line\">    exp_scores = np.exp(z2)</span><br><span class=\"line\">    probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.argmax(probs, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure><h1 id=\"学习率，正则化\"><a href=\"#学习率，正则化\" class=\"headerlink\" title=\"学习率，正则化\"></a><font size=\"5\">学习率，正则化</font></h1><h2 id=\"梯度下降的学习速率\"><a href=\"#梯度下降的学习速率\" class=\"headerlink\" title=\"梯度下降的学习速率\"></a><font size=\"4\">梯度下降的学习速率</font></h2><h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><h1 id=\"点集分类结果\"><a href=\"#点集分类结果\" class=\"headerlink\" title=\"点集分类结果\"></a><font size=\"5\">点集分类结果</font></h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/223945007.JPG\" alt=\"mark\"></p><h1 id=\"泰坦尼克号预测调参\"><a href=\"#泰坦尼克号预测调参\" class=\"headerlink\" title=\"泰坦尼克号预测调参\"></a><font size=\"5\">泰坦尼克号预测调参</font></h1><h1 id=\"泰坦尼克号预测结果\"><a href=\"#泰坦尼克号预测结果\" class=\"headerlink\" title=\"泰坦尼克号预测结果\"></a><font size=\"5\">泰坦尼克号预测结果</font></h1><h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a><font size=\"5\">参考</font></h1><blockquote><p><a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\" target=\"_blank\" rel=\"noopener\">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION</a><br><a href=\"http://cs231n.github.io/optimization-1/#gd\" target=\"_blank\" rel=\"noopener\">CS231n Convolutional Neural Networks or Visual Recognition—optimization</a><br><a href=\"http://cs231n.github.io/neural-networks-3/#gradcheck\" target=\"_blank\" rel=\"noopener\">CS231n Convolutional Neural Networks or Visual Recognition—neural-networks</a><br><a href=\"http://blog.csdn.net/longxinchen_ml/article/details/50521933\" target=\"_blank\" rel=\"noopener\">龙心尘</a></p></blockquote>","site":{"data":{}},"excerpt":"<hr><p>以简单的点集分类和泰坦尼克号作为例子，学习BP神经网络</p>","more":"<h1 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a><font size=\"5\">模型</font></h1><h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a><font size=\"4\">模型结构</font></h2><ul><li>就以文章头部那张图片为例，机器学习(监督学习部分)是找到一个模型，通过已知的条件和结果，调整模型中各个参数的值，最后得到一个模型，对于某一类问题，将条件输入模型，可以输出(预测)较为正确的结果，神经网络的模型结构就如图所示，左边的input就是条件，右边的output就是结果</li><li>模型至少有2个层，输入层与输出层中间可能有多个层，每一层包含若干个节点，每一个节点是一个函数，函数的输入是上一层所有节点的输出通过路径加权再减去这个节点本身的阈值，输出再作为下一层每一个个节点输入的一部分。如果某个节点有输出，则叫做被激活，节点所代表的函数就是激活函数，路径上的加权系数以及阈值就是网络参数。当一个神经网络构建好且学习好后，输入一组条件，每一层的节点依次被上一层节点激活输出，直到最后一层的节点输出结果。机器学习学习的成果就是正确的网络参数，一个神经网络的数据模型也可以看成一组网络参数。在本例中我们研究分类问题，最后的输出节点输出这个条件下属于某一类的概率。</li></ul><h2 id=\"网络参数\"><a href=\"#网络参数\" class=\"headerlink\" title=\"网络参数\"></a><font size=\"4\">网络参数</font></h2><ul><li><p>节点的输出可以写成<br>$$<br>y=f(\\sum_{i=1}^nw_ix_i-\\theta )<br>$$<br>其中n是连接到这个节点的路径数，$w_i$是路径上的权值，$\\theta$是阈值,$f$是激活函数。阈值的理解:可以看成上一层有一个哑节点，它的输出恒为-1，这个哑节点连接过来的权值$w_{n+1}$就是阈值。阈值和权值统称为网络参数。</p></li><li><p>网络参数的学习其实就是根据这次训练的结果和理想结果对比，将参数增加一个$\\Delta$值，进行调整。<br>$$<br>\\Delta w_i=\\eta (y-y_0)x_i<br>$$<br>上式即最简单的感知机模型(无隐层)的权重调整函数，其中$\\eta$称为学习率。</p></li></ul><h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a><font size=\"4\">激活函数</font></h2><ul><li><p>激活函数有许多种，比如最简单的阶跃函数(加权输入大于阈值则输出1否则无输出)，或者Sigmoid函数、tanh函数。输出节点一般用Softmax函数。</p></li><li><p>Sigmoid函数有一种很好的性质:$f’(x)=f(x)(1-f(x))$</p><p>tanh函数也有一种很好的性质:$tanh’(x)=1-tanh^2(x)$</p><p>可以看到这两种函数都可以通过本身计算出导函数，而在多层前馈神经网络中，我们需要利用负梯度来调整网络参数，计算负梯度需要利用导数，这样的性质能方便推导公式。</p></li></ul><h1 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a><font size=\"5\">损失函数</font></h1><h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a><font size=\"4\">损失评估</font></h2><ul><li>每一次训练后，我们需要知道这一次训练是否降低了与目标之间的误差，这个误差的量化就利用损失函数</li><li>损失评估可以防止BP神经网络过拟合，例如早停策略:将数据分成训练集和验证集，训练集用来计算梯度，更新网络参数，验证机用来评估误差，若训练集误差降低而验证集误差升高，则停止训练以防止过拟合，同时返回具有最小验证集误差的网络参数。</li></ul><h2 id=\"交叉熵损失\"><a href=\"#交叉熵损失\" class=\"headerlink\" title=\"交叉熵损失\"></a><font size=\"4\">交叉熵损失</font></h2><ul><li><p>交叉熵损失的计算公式<br>$$<br>L_i=-log(\\frac {e^f_{yi}} {\\sum_je^{f_j}})<br>$$<br>f是最后一层的节点的输出，可以看到括号里实际上是将最后一层输出的每一类的概率指数函数归一化，再求log。假如某一行训练元素，正确分类到a类，结果输出a类的概率低，则括号里的值接近0,$L_i$就会趋近无穷，即损失太多，反之，括号里的值接近1，$L_i$趋近0，即几乎无损失。</p><p>一般还会加入正则化项<br>$$<br>L=\\frac1N \\sum_iL_I+ \\frac12 \\lambda \\sum_k\\sum_lW_{k,l}^2<br>$$</p></li></ul><h2 id=\"均方误差\"><a href=\"#均方误差\" class=\"headerlink\" title=\"均方误差\"></a><font size=\"4\">均方误差</font></h2><ul><li>均方误差的衡量就很简单暴力<br>$$<br>E_k=\\frac12\\sum_{j=1}^l(y_0j^k-y_j^k)^2<br>$$<br>$\\frac 12$是为了方便做负梯度计算</li></ul><h1 id=\"BP算法\"><a href=\"#BP算法\" class=\"headerlink\" title=\"BP算法\"></a><font size=\"5\">BP算法</font></h1><h2 id=\"负梯度\"><a href=\"#负梯度\" class=\"headerlink\" title=\"负梯度\"></a><font size=\"4\">负梯度</font></h2><ul><li><p>梯度下降法或最速下降法是求解无约束最优化问题的一种常见方法，是迭代算法，每一步需要求解目标函数的梯度向量。</p></li><li><p>BP算法以目标的负梯度方向对网络参数进行调整，目标函数即损失函数，即</p><p>$$<br>\\Delta w=-\\eta \\frac {\\partial L}{\\partial w}<br>$$</p><p>这样求出来的意义是，调整值是损失函数的负梯度乘学习率，即让损失减小的方向调整</p></li><li><p>对于均方误差和交叉熵损失两种损失函数，求出来的负梯度不同，但都可以通过他们的性质优化，以交叉熵损失为例，求出来的负梯度为：</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170213/194245109.jpg\" alt=\"mark\"></p><p>其中$W_1,b_1$是第一层网络参数，$W_2,b_2$是第二层网络参数</p></li></ul><h2 id=\"更新公式\"><a href=\"#更新公式\" class=\"headerlink\" title=\"更新公式\"></a><font size=\"4\">更新公式</font></h2><ul><li>对每一次学习先得到预测结果，再得出损失函数，从而计算出负梯度，乘上学习率并正则化，就是最终这一次学习到的网络参数调整值</li></ul><h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a><font size=\"5\">代码实现</font></h1><h2 id=\"初始化与可视化\"><a href=\"#初始化与可视化\" class=\"headerlink\" title=\"初始化与可视化\"></a><font size=\"4\">初始化与可视化</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Config</span>:</span></span><br><span class=\"line\">    nn_input_dim = <span class=\"number\">2</span>  <span class=\"comment\"># input layer dimensionality</span></span><br><span class=\"line\">    nn_output_dim = <span class=\"number\">2</span>  <span class=\"comment\"># output layer dimensionality</span></span><br><span class=\"line\">    <span class=\"comment\"># Gradient descent parameters (I picked these by hand)</span></span><br><span class=\"line\">    epsilon = <span class=\"number\">0.01</span>  <span class=\"comment\"># learning rate for gradient descent</span></span><br><span class=\"line\">    reg_lambda = <span class=\"number\">0.01</span>  <span class=\"comment\"># regularization strength</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">    X, y = datasets.make_moons(<span class=\"number\">200</span>, noise=<span class=\"number\">0.20</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">visualize</span><span class=\"params\">(X, y, model)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)</span></span><br><span class=\"line\">    <span class=\"comment\"># plt.show()</span></span><br><span class=\"line\">    plot_decision_boundary(<span class=\"keyword\">lambda</span> x:predict(model,x), X, y)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Logistic Regression\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(pred_func, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[:, <span class=\"number\">0</span>].min() - <span class=\"number\">.5</span>, X[:, <span class=\"number\">0</span>].max() + <span class=\"number\">.5</span></span><br><span class=\"line\">    y_min, y_max = X[:, <span class=\"number\">1</span>].min() - <span class=\"number\">.5</span>, X[:, <span class=\"number\">1</span>].max() + <span class=\"number\">.5</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole gid</span></span><br><span class=\"line\">    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure><h2 id=\"计算损失\"><a href=\"#计算损失\" class=\"headerlink\" title=\"计算损失\"></a><font size=\"4\">计算损失</font></h2><ul><li>其中probs即指数归一化后的各类别概率</li><li>probs[range(num_examples), y]即每一个预测结果中实际正确类别的预测概率，用log还原指数并累加即交叉熵损失data_loss,之后再正则化</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Helper function to evaluate the total loss on the dataset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_loss</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    num_examples = len(X)  <span class=\"comment\"># training set size</span></span><br><span class=\"line\">    W1, b1, W2, b2 = model[<span class=\"string\">'W1'</span>], model[<span class=\"string\">'b1'</span>], model[<span class=\"string\">'W2'</span>], model[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation to calculate our predictions</span></span><br><span class=\"line\">    z1 = X.dot(W1) + b1</span><br><span class=\"line\">    a1 = np.tanh(z1)</span><br><span class=\"line\">    z2 = a1.dot(W2) + b2</span><br><span class=\"line\">    exp_scores = np.exp(z2)</span><br><span class=\"line\">    probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Calculating the loss</span></span><br><span class=\"line\">    corect_logprobs = -np.log(probs[range(num_examples), y])</span><br><span class=\"line\">    data_loss = np.sum(corect_logprobs)</span><br><span class=\"line\">    <span class=\"comment\"># Add regulatization term to loss (optional)</span></span><br><span class=\"line\">    data_loss += Config.reg_lambda / <span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / num_examples * data_loss</span><br></pre></td></tr></table></figure><h2 id=\"逆向误差传输\"><a href=\"#逆向误差传输\" class=\"headerlink\" title=\"逆向误差传输\"></a><font size=\"4\">逆向误差传输</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This function learns parameters for the neural network and returns the model.</span></span><br><span class=\"line\"><span class=\"comment\"># - nn_hdim: Number of nodes in the hidden layer</span></span><br><span class=\"line\"><span class=\"comment\"># - num_passes: Number of passes through the training data for gradient descent</span></span><br><span class=\"line\"><span class=\"comment\"># - print_loss: If True, print the loss every 1000 iterations</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">build_model</span><span class=\"params\">(X, y, nn_hdim, num_passes=<span class=\"number\">20000</span>, print_loss=False)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the parameters to random values. We need to learn these.</span></span><br><span class=\"line\">    num_examples = len(X)</span><br><span class=\"line\">    np.random.seed(<span class=\"number\">0</span>)</span><br><span class=\"line\">    W1 = np.random.randn(Config.nn_input_dim, nn_hdim) / np.sqrt(Config.nn_input_dim)</span><br><span class=\"line\">    b1 = np.zeros((<span class=\"number\">1</span>, nn_hdim))</span><br><span class=\"line\">    W2 = np.random.randn(nn_hdim, Config.nn_output_dim) / np.sqrt(nn_hdim)</span><br><span class=\"line\">    b2 = np.zeros((<span class=\"number\">1</span>, Config.nn_output_dim))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># This is what we return at the end</span></span><br><span class=\"line\">    model = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Gradient descent. For each batch...</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_passes):</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        z1 = X.dot(W1) + b1</span><br><span class=\"line\">        a1 = np.tanh(z1)</span><br><span class=\"line\">        z2 = a1.dot(W2) + b2</span><br><span class=\"line\">        exp_scores = np.exp(z2)</span><br><span class=\"line\">        probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Backpropagation</span></span><br><span class=\"line\">        delta3 = probs</span><br><span class=\"line\">        delta3[range(num_examples), y] -= <span class=\"number\">1</span></span><br><span class=\"line\">        dW2 = (a1.T).dot(delta3)</span><br><span class=\"line\">        db2 = np.sum(delta3, axis=<span class=\"number\">0</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        delta2 = delta3.dot(W2.T) * (<span class=\"number\">1</span> - np.power(a1, <span class=\"number\">2</span>))</span><br><span class=\"line\">        dW1 = np.dot(X.T, delta2)</span><br><span class=\"line\">        db1 = np.sum(delta2, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Add regularization terms (b1 and b2 don't have regularization terms)</span></span><br><span class=\"line\">        dW2 += Config.reg_lambda * W2</span><br><span class=\"line\">        dW1 += Config.reg_lambda * W1</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Gradient descent parameter update</span></span><br><span class=\"line\">        W1 += -Config.epsilon * dW1</span><br><span class=\"line\">        b1 += -Config.epsilon * db1</span><br><span class=\"line\">        W2 += -Config.epsilon * dW2</span><br><span class=\"line\">        b2 += -Config.epsilon * db2</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Assign new parameters to the model</span></span><br><span class=\"line\">        model = &#123;<span class=\"string\">'W1'</span>: W1, <span class=\"string\">'b1'</span>: b1, <span class=\"string\">'W2'</span>: W2, <span class=\"string\">'b2'</span>: b2&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Optionally print the loss.</span></span><br><span class=\"line\">        <span class=\"comment\"># This is expensive because it uses the whole dataset, so we don't want to do it too often.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_loss <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Loss after iteration %i: %f\"</span> % (i, calculate_loss(model, X, y)))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> model</span><br></pre></td></tr></table></figure><h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a><font size=\"4\">预测</font></h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(model, x)</span>:</span></span><br><span class=\"line\">    W1, b1, W2, b2 = model[<span class=\"string\">'W1'</span>], model[<span class=\"string\">'b1'</span>], model[<span class=\"string\">'W2'</span>], model[<span class=\"string\">'b2'</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    z1 = x.dot(W1) + b1</span><br><span class=\"line\">    a1 = np.tanh(z1)</span><br><span class=\"line\">    z2 = a1.dot(W2) + b2</span><br><span class=\"line\">    exp_scores = np.exp(z2)</span><br><span class=\"line\">    probs = exp_scores / np.sum(exp_scores, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.argmax(probs, axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure><h1 id=\"学习率，正则化\"><a href=\"#学习率，正则化\" class=\"headerlink\" title=\"学习率，正则化\"></a><font size=\"5\">学习率，正则化</font></h1><h2 id=\"梯度下降的学习速率\"><a href=\"#梯度下降的学习速率\" class=\"headerlink\" title=\"梯度下降的学习速率\"></a><font size=\"4\">梯度下降的学习速率</font></h2><h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><h1 id=\"点集分类结果\"><a href=\"#点集分类结果\" class=\"headerlink\" title=\"点集分类结果\"></a><font size=\"5\">点集分类结果</font></h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/223945007.JPG\" alt=\"mark\"></p><h1 id=\"泰坦尼克号预测调参\"><a href=\"#泰坦尼克号预测调参\" class=\"headerlink\" title=\"泰坦尼克号预测调参\"></a><font size=\"5\">泰坦尼克号预测调参</font></h1><h1 id=\"泰坦尼克号预测结果\"><a href=\"#泰坦尼克号预测结果\" class=\"headerlink\" title=\"泰坦尼克号预测结果\"></a><font size=\"5\">泰坦尼克号预测结果</font></h1><h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a><font size=\"5\">参考</font></h1><blockquote><p><a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\" target=\"_blank\" rel=\"noopener\">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON – AN INTRODUCTION</a><br><a href=\"http://cs231n.github.io/optimization-1/#gd\" target=\"_blank\" rel=\"noopener\">CS231n Convolutional Neural Networks or Visual Recognition—optimization</a><br><a href=\"http://cs231n.github.io/neural-networks-3/#gradcheck\" target=\"_blank\" rel=\"noopener\">CS231n Convolutional Neural Networks or Visual Recognition—neural-networks</a><br><a href=\"http://blog.csdn.net/longxinchen_ml/article/details/50521933\" target=\"_blank\" rel=\"noopener\">龙心尘</a></p></blockquote>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"机器学习入门:神经网络基础","path":"2017/02/10/NeuralNetworks1/","eyeCatchImage":null,"excerpt":"<hr><p>以简单的点集分类和泰坦尼克号作为例子，学习BP神经网络</p>","date":"2017-02-10T04:20:17.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"自然语言处理基础知识","date":"2018-03-07T01:56:23.000Z","author":"Thinkwee","mathjax":true,"_content":"记录入门NLP中seq2seq模型时学习到的一些基础知识。\n\n<!--more-->\n\n# 统计自然语言方法\n-\t读书笔记，待补充\n\n# 前馈神经网络相关\n-\t数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。\n-\t激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。\n-\tSoftmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）\n-\t隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6I6a2Eccce.png?imageslim)\n\t绝对值整流：右边系数为-1\n\t渗漏整流：右边系数固定为一个较小值\n\t参数化整流：系数放到模型中学习\n\n# 反向传播\n-\t反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。\n-\t神经网络中的反向传播：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/4c4HJ2L5Eg.png?imageslim)\n\t初始化梯度表\n\t最后一层输出对输出求梯度，因此初始值为1\n\t从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。\n\t本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。\n\n# RNN循环神经网络\n## RNN\n-\t特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。\n-\t用双曲正切作为隐藏层激活函数\n-\t输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y\n-\t基本结构（展开和非展开）：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6KB34Ef356.png?imageslim)\n-\t几种变式：\n -\t每一个时间步均有输出，隐藏层之间有循环连接：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/FfDA7bJIce.png?imageslim)\n -\t每一个时间步均有输出，输出与隐藏层之间有循环连接：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CDfgfKaH5d.png?imageslim)\n -\t读取整个序列后产生单个输出：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GJ2CBjhGII.png?imageslim)\n-\t普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。\n\t前馈过程：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/kj5bKAgKfc.png?imageslim)\n\t代价函数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/b0dL8g7clF.png?imageslim)\n-\t改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）\n\t导师驱动模型：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jehilKjam9.png?imageslim)\n\n## 双向RNN\n-\t考虑对未来信息的依赖，相当于两类隐藏层结合在一起\n\n## 序列到序列\n-\t采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CdI13aLb2A.png?imageslim)\n\n## 深度RNN\n-\tA.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）\n-\tB.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深\n-\tC.引入跳跃连接来缓解加深网络后导致的路径延长效应\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/clfEmm1Kj5.png?imageslim)\n\n## RNN中的长期依赖问题\n-\t长期依赖问题：模型变深，失去了学习到先前信息的能力\n-\t对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。\n-\t最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）\n\n## 门控RNN\n-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。\n-\t渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。\n\n# LSTM\n-\tLSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）\n-\tLSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6iK5ld6Aij.png?imageslim)\n-\t可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h\n-\t所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。\n-\t三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。\n-\t内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。\n-\t细胞输出h，内部状态过激活函数，由输出门控制。\n-\t另一张更好理解的图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/5f3GlGB9Ek.png?imageslim)\n\n# 双向LSTM\n-\t同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。\n-\t因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：\n -\t直接连接(concat)\t\n -\t求和\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/B4FKeKIc7f.png?imageslim)\n\n# 词嵌入、Word2Vec\n-\t使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：\n-\t分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。\n-\t重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。\n-\t没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：[技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720050&idx=2&sn=9fedc937d3128462c478ef7911e77687&chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&scene=21#wechat_redirect)\n\n# 注意力机制\n-\t在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。\n-\t注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。\n","source":"_posts/NLPBasic.md","raw":"---\ntitle: 自然语言处理基础知识\ndate: 2018-03-07 09:56:23\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 自然语言处理\nauthor: Thinkwee\nmathjax: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180307/D16L2739kI.jpg?imageslim\n---\n记录入门NLP中seq2seq模型时学习到的一些基础知识。\n\n<!--more-->\n\n# 统计自然语言方法\n-\t读书笔记，待补充\n\n# 前馈神经网络相关\n-\t数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。\n-\t激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。\n-\tSoftmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）\n-\t隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6I6a2Eccce.png?imageslim)\n\t绝对值整流：右边系数为-1\n\t渗漏整流：右边系数固定为一个较小值\n\t参数化整流：系数放到模型中学习\n\n# 反向传播\n-\t反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。\n-\t神经网络中的反向传播：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/4c4HJ2L5Eg.png?imageslim)\n\t初始化梯度表\n\t最后一层输出对输出求梯度，因此初始值为1\n\t从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。\n\t本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。\n\n# RNN循环神经网络\n## RNN\n-\t特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。\n-\t用双曲正切作为隐藏层激活函数\n-\t输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y\n-\t基本结构（展开和非展开）：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6KB34Ef356.png?imageslim)\n-\t几种变式：\n -\t每一个时间步均有输出，隐藏层之间有循环连接：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/FfDA7bJIce.png?imageslim)\n -\t每一个时间步均有输出，输出与隐藏层之间有循环连接：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CDfgfKaH5d.png?imageslim)\n -\t读取整个序列后产生单个输出：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GJ2CBjhGII.png?imageslim)\n-\t普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。\n\t前馈过程：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/kj5bKAgKfc.png?imageslim)\n\t代价函数：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/b0dL8g7clF.png?imageslim)\n-\t改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）\n\t导师驱动模型：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jehilKjam9.png?imageslim)\n\n## 双向RNN\n-\t考虑对未来信息的依赖，相当于两类隐藏层结合在一起\n\n## 序列到序列\n-\t采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CdI13aLb2A.png?imageslim)\n\n## 深度RNN\n-\tA.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）\n-\tB.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深\n-\tC.引入跳跃连接来缓解加深网络后导致的路径延长效应\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/clfEmm1Kj5.png?imageslim)\n\n## RNN中的长期依赖问题\n-\t长期依赖问题：模型变深，失去了学习到先前信息的能力\n-\t对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。\n-\t最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）\n\n## 门控RNN\n-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。\n-\t渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。\n\n# LSTM\n-\tLSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）\n-\tLSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6iK5ld6Aij.png?imageslim)\n-\t可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h\n-\t所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。\n-\t三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。\n-\t内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。\n-\t细胞输出h，内部状态过激活函数，由输出门控制。\n-\t另一张更好理解的图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/5f3GlGB9Ek.png?imageslim)\n\n# 双向LSTM\n-\t同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。\n-\t因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：\n -\t直接连接(concat)\t\n -\t求和\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/B4FKeKIc7f.png?imageslim)\n\n# 词嵌入、Word2Vec\n-\t使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：\n-\t分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。\n-\t重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。\n-\t没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：[技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720050&idx=2&sn=9fedc937d3128462c478ef7911e77687&chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&scene=21#wechat_redirect)\n\n# 注意力机制\n-\t在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。\n-\t注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。\n","slug":"NLPBasic","published":1,"updated":"2018-07-23T01:28:38.487Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180307/D16L2739kI.jpg?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072cl000iqcw6gj4y2i1c","content":"<p>记录入门NLP中seq2seq模型时学习到的一些基础知识。</p><a id=\"more\"></a><h1 id=\"统计自然语言方法\"><a href=\"#统计自然语言方法\" class=\"headerlink\" title=\"统计自然语言方法\"></a>统计自然语言方法</h1><ul><li>读书笔记，待补充</li></ul><h1 id=\"前馈神经网络相关\"><a href=\"#前馈神经网络相关\" class=\"headerlink\" title=\"前馈神经网络相关\"></a>前馈神经网络相关</h1><ul><li>数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。</li><li>激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。</li><li>Softmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）</li><li>隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6I6a2Eccce.png?imageslim\" alt=\"mark\"><br>绝对值整流：右边系数为-1<br>渗漏整流：右边系数固定为一个较小值<br>参数化整流：系数放到模型中学习</li></ul><h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><ul><li>反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。</li><li>神经网络中的反向传播：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/4c4HJ2L5Eg.png?imageslim\" alt=\"mark\"><br>初始化梯度表<br>最后一层输出对输出求梯度，因此初始值为1<br>从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。<br>本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。</li></ul><h1 id=\"RNN循环神经网络\"><a href=\"#RNN循环神经网络\" class=\"headerlink\" title=\"RNN循环神经网络\"></a>RNN循环神经网络</h1><h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><ul><li>特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。</li><li>用双曲正切作为隐藏层激活函数</li><li>输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y</li><li>基本结构（展开和非展开）：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6KB34Ef356.png?imageslim\" alt=\"mark\"></li><li>几种变式：<ul><li>每一个时间步均有输出，隐藏层之间有循环连接：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/FfDA7bJIce.png?imageslim\" alt=\"mark\"></li><li>每一个时间步均有输出，输出与隐藏层之间有循环连接：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CDfgfKaH5d.png?imageslim\" alt=\"mark\"></li><li>读取整个序列后产生单个输出：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GJ2CBjhGII.png?imageslim\" alt=\"mark\"></li></ul></li><li>普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。<br>前馈过程：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/kj5bKAgKfc.png?imageslim\" alt=\"mark\"><br>代价函数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/b0dL8g7clF.png?imageslim\" alt=\"mark\"></li><li>改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）<br>导师驱动模型：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jehilKjam9.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h2><ul><li>考虑对未来信息的依赖，相当于两类隐藏层结合在一起</li></ul><h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul><li>采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CdI13aLb2A.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"深度RNN\"><a href=\"#深度RNN\" class=\"headerlink\" title=\"深度RNN\"></a>深度RNN</h2><ul><li>A.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）</li><li>B.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深</li><li>C.引入跳跃连接来缓解加深网络后导致的路径延长效应<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/clfEmm1Kj5.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"RNN中的长期依赖问题\"><a href=\"#RNN中的长期依赖问题\" class=\"headerlink\" title=\"RNN中的长期依赖问题\"></a>RNN中的长期依赖问题</h2><ul><li>长期依赖问题：模型变深，失去了学习到先前信息的能力</li><li>对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。</li><li>最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）</li></ul><h2 id=\"门控RNN\"><a href=\"#门控RNN\" class=\"headerlink\" title=\"门控RNN\"></a>门控RNN</h2><p>-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。</p><ul><li>渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。</li></ul><h1 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h1><ul><li>LSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）</li><li>LSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6iK5ld6Aij.png?imageslim\" alt=\"mark\"></li><li>可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h</li><li>所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。</li><li>三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。</li><li>内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。</li><li>细胞输出h，内部状态过激活函数，由输出门控制。</li><li>另一张更好理解的图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/5f3GlGB9Ek.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"双向LSTM\"><a href=\"#双向LSTM\" class=\"headerlink\" title=\"双向LSTM\"></a>双向LSTM</h1><ul><li>同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。</li><li>因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：<ul><li>直接连接(concat)</li><li>求和<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/B4FKeKIc7f.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"词嵌入、Word2Vec\"><a href=\"#词嵌入、Word2Vec\" class=\"headerlink\" title=\"词嵌入、Word2Vec\"></a>词嵌入、Word2Vec</h1><ul><li>使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：</li><li>分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。</li><li>重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。</li><li>没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650720050&amp;idx=2&amp;sn=9fedc937d3128462c478ef7911e77687&amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</a></li></ul><h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><ul><li>在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。</li><li>注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。</li></ul>","site":{"data":{}},"excerpt":"<p>记录入门NLP中seq2seq模型时学习到的一些基础知识。</p>","more":"<h1 id=\"统计自然语言方法\"><a href=\"#统计自然语言方法\" class=\"headerlink\" title=\"统计自然语言方法\"></a>统计自然语言方法</h1><ul><li>读书笔记，待补充</li></ul><h1 id=\"前馈神经网络相关\"><a href=\"#前馈神经网络相关\" class=\"headerlink\" title=\"前馈神经网络相关\"></a>前馈神经网络相关</h1><ul><li>数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。</li><li>激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。</li><li>Softmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）</li><li>隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6I6a2Eccce.png?imageslim\" alt=\"mark\"><br>绝对值整流：右边系数为-1<br>渗漏整流：右边系数固定为一个较小值<br>参数化整流：系数放到模型中学习</li></ul><h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><ul><li>反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。</li><li>神经网络中的反向传播：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/4c4HJ2L5Eg.png?imageslim\" alt=\"mark\"><br>初始化梯度表<br>最后一层输出对输出求梯度，因此初始值为1<br>从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。<br>本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。</li></ul><h1 id=\"RNN循环神经网络\"><a href=\"#RNN循环神经网络\" class=\"headerlink\" title=\"RNN循环神经网络\"></a>RNN循环神经网络</h1><h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><ul><li>特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。</li><li>用双曲正切作为隐藏层激活函数</li><li>输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y</li><li>基本结构（展开和非展开）：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6KB34Ef356.png?imageslim\" alt=\"mark\"></li><li>几种变式：<ul><li>每一个时间步均有输出，隐藏层之间有循环连接：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/FfDA7bJIce.png?imageslim\" alt=\"mark\"></li><li>每一个时间步均有输出，输出与隐藏层之间有循环连接：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CDfgfKaH5d.png?imageslim\" alt=\"mark\"></li><li>读取整个序列后产生单个输出：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/GJ2CBjhGII.png?imageslim\" alt=\"mark\"></li></ul></li><li>普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。<br>前馈过程：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/kj5bKAgKfc.png?imageslim\" alt=\"mark\"><br>代价函数：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/b0dL8g7clF.png?imageslim\" alt=\"mark\"></li><li>改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）<br>导师驱动模型：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/jehilKjam9.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h2><ul><li>考虑对未来信息的依赖，相当于两类隐藏层结合在一起</li></ul><h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul><li>采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/CdI13aLb2A.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"深度RNN\"><a href=\"#深度RNN\" class=\"headerlink\" title=\"深度RNN\"></a>深度RNN</h2><ul><li>A.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）</li><li>B.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深</li><li>C.引入跳跃连接来缓解加深网络后导致的路径延长效应<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/clfEmm1Kj5.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"RNN中的长期依赖问题\"><a href=\"#RNN中的长期依赖问题\" class=\"headerlink\" title=\"RNN中的长期依赖问题\"></a>RNN中的长期依赖问题</h2><ul><li>长期依赖问题：模型变深，失去了学习到先前信息的能力</li><li>对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。</li><li>最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）</li></ul><h2 id=\"门控RNN\"><a href=\"#门控RNN\" class=\"headerlink\" title=\"门控RNN\"></a>门控RNN</h2><p>-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。</p><ul><li>渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。</li></ul><h1 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h1><ul><li>LSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）</li><li>LSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6iK5ld6Aij.png?imageslim\" alt=\"mark\"></li><li>可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h</li><li>所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。</li><li>三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。</li><li>内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。</li><li>细胞输出h，内部状态过激活函数，由输出门控制。</li><li>另一张更好理解的图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/5f3GlGB9Ek.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"双向LSTM\"><a href=\"#双向LSTM\" class=\"headerlink\" title=\"双向LSTM\"></a>双向LSTM</h1><ul><li>同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。</li><li>因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：<ul><li>直接连接(concat)</li><li>求和<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/B4FKeKIc7f.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"词嵌入、Word2Vec\"><a href=\"#词嵌入、Word2Vec\" class=\"headerlink\" title=\"词嵌入、Word2Vec\"></a>词嵌入、Word2Vec</h1><ul><li>使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：</li><li>分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。</li><li>重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。</li><li>没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650720050&amp;idx=2&amp;sn=9fedc937d3128462c478ef7911e77687&amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</a></li></ul><h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><ul><li>在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。</li><li>注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"自然语言处理基础知识","path":"2018/03/07/NLPBasic/","eyeCatchImage":null,"excerpt":"<p>记录入门NLP中seq2seq模型时学习到的一些基础知识。</p>","date":"2018-03-07T01:56:23.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Python特性拾零","date":"2017-03-28T12:02:39.000Z","_content":"***\nPython的一些特性和语法\n总结一些自己跳过的坑\nPython3.5\n\n<!--more-->\n\n# 对象皆引用\n-\t不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用\n-\tPython中不需要声明变量类型也是基于此\n\t```Python\n\t\ta=3\n\t\tb=a\n\t```\n\ta和b都只是引用一个整型值3，修改b，a的引用值也会变化\n-\t如果要拷贝，可以用b=a[:]\n\n# string是常量\n-\t字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变\n\n# lambda匿名函数\n-\t简化函数书写，lambda 参量:计算式\n-\t主要用于排序或者reduce\n-\tlambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数\n\t```Python\n\t\tx=10\n\t\ta=lambda y,x=x:x+y\n\t\tx=20\n\t\tprint(a(10))\n\t\t\n\t>>> 20\n\t```\n\t如果不设置默认参数，上面的运行结果就是30\n\t\n# 迭代器与生成器\n-\t通过重写对象的__iter__方法实现自定义迭代器，生成器yield实现迭代器的next方法\n\t```Python\n\t\tclass Countdown:\n\t\t\tdef __init__(self, start):\n\t\t\t\tself.start = start\n\t\t\n\t\t\tdef __iter__(self):\n\t\t\t\tn = self.start\n\t\t\t\twhile n > 0:\n\t\t\t\t\tyield n\n\t\t\t\t\tn -= 1\n    \n\t\t\tdef __reversed__(self):\n\t\t\t\tn = 1\n\t\t\t\twhile n <= self.start:\n\t\t\t\t\tn += 1\n    \n    \n\t\tfor rr in (Countdown(3)):\n\t\t\tprint(rr)\n\t\t\n\t>>> 3\n\t    2\n\t    1\n\t```\n\n# enumerate\n-\t同时输出迭代对象和索引，参数为索引开始号\n\t```Python\n\t\tfor idx,val in enumerate(my_list,1):\n\t\t\tprint(idx,val)\n\t```\n\n# 函数\n-\t接收任意个参数\n\t```Python\n\tdef avg(first,*rest):\n\t\treturn (first+sum(rest))/(1+len(rest))\n\t```\n\t\\*接任意数量的位置参数，也可以用\\*\\*接一个字典，代表任意数量的关键字参数，也可以混用\\*和\\*\\*\n\t顺序(任意个位置参数，\\*，最后一个位置参数，其他参数，\\*\\*)\n-\t函数返回多个值\n\t直接return a,b,c，实际上返回的是一个元祖\n\n# 装饰器\n-\t一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数\n\t```Python\n\t\timport time\n\t\tfrom functools import wraps\n\t\t\n\t\t\n\t\tdef timethis(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tstart = time.time()\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tend = time.time()\n\t\t\t\tprint(func.__name__, end - start)\n\t\t\t\treturn result\n\t\t\n\t\t\treturn wrapper\n\t\t\n\t\t@timethis\n\t\tdef loop(n):\n\t\t\twhile n > 0:\n\t\t\t\tn -= 1\n\t\t\n\t\t\n\t\tloop(100000)\n\t\n\t>>> loop 0.03971695899963379\n\t```\n\t在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行\n\t可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果\n-\t可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的__wrapped__属性直接访问到被装饰的函数，用来解除装饰\n\n# 逗号的特殊作用\n-\t输出时换行变空格\n-\t转换类型为元组\n\n# filter\n-\t接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素\n\t```Python\n\t\tdef is_odd(n):\n\t\t\treturn n % 2 == 1\n\n\t\tfilter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])\n\t```\n\n# any\n-\t原型：\n\t```Python\t\t\t\t\n\t\tdef any(iterable):\n\t\t   for element in iterable:\n\t\t\t   if  element:\n\t\t\t\t   return False\n\t\t   return True\n\t```\n\n# yield\n-\t一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。\n\t```Python\n\t\t>>> def g(n):\n\t\t...     for i in range(n):\n\t\t...             yield i **2\n\t\t...\n\t\t>>> for i in g(5):\n\t\t...     print i,\":\",\n\t\t...\n\t\t0 : 1 : 4 : 9 : 16 :\n\t```","source":"_posts/PythonNotes.md","raw":"title: Python特性拾零\ndate: 2017-03-28 20:02:39\ntags:\n-\tcode\n-\tpython\ncategories:\n-\tPython\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/20170328/200539402.png\n---\n***\nPython的一些特性和语法\n总结一些自己跳过的坑\nPython3.5\n\n<!--more-->\n\n# 对象皆引用\n-\t不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用\n-\tPython中不需要声明变量类型也是基于此\n\t```Python\n\t\ta=3\n\t\tb=a\n\t```\n\ta和b都只是引用一个整型值3，修改b，a的引用值也会变化\n-\t如果要拷贝，可以用b=a[:]\n\n# string是常量\n-\t字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变\n\n# lambda匿名函数\n-\t简化函数书写，lambda 参量:计算式\n-\t主要用于排序或者reduce\n-\tlambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数\n\t```Python\n\t\tx=10\n\t\ta=lambda y,x=x:x+y\n\t\tx=20\n\t\tprint(a(10))\n\t\t\n\t>>> 20\n\t```\n\t如果不设置默认参数，上面的运行结果就是30\n\t\n# 迭代器与生成器\n-\t通过重写对象的__iter__方法实现自定义迭代器，生成器yield实现迭代器的next方法\n\t```Python\n\t\tclass Countdown:\n\t\t\tdef __init__(self, start):\n\t\t\t\tself.start = start\n\t\t\n\t\t\tdef __iter__(self):\n\t\t\t\tn = self.start\n\t\t\t\twhile n > 0:\n\t\t\t\t\tyield n\n\t\t\t\t\tn -= 1\n    \n\t\t\tdef __reversed__(self):\n\t\t\t\tn = 1\n\t\t\t\twhile n <= self.start:\n\t\t\t\t\tn += 1\n    \n    \n\t\tfor rr in (Countdown(3)):\n\t\t\tprint(rr)\n\t\t\n\t>>> 3\n\t    2\n\t    1\n\t```\n\n# enumerate\n-\t同时输出迭代对象和索引，参数为索引开始号\n\t```Python\n\t\tfor idx,val in enumerate(my_list,1):\n\t\t\tprint(idx,val)\n\t```\n\n# 函数\n-\t接收任意个参数\n\t```Python\n\tdef avg(first,*rest):\n\t\treturn (first+sum(rest))/(1+len(rest))\n\t```\n\t\\*接任意数量的位置参数，也可以用\\*\\*接一个字典，代表任意数量的关键字参数，也可以混用\\*和\\*\\*\n\t顺序(任意个位置参数，\\*，最后一个位置参数，其他参数，\\*\\*)\n-\t函数返回多个值\n\t直接return a,b,c，实际上返回的是一个元祖\n\n# 装饰器\n-\t一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数\n\t```Python\n\t\timport time\n\t\tfrom functools import wraps\n\t\t\n\t\t\n\t\tdef timethis(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tstart = time.time()\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tend = time.time()\n\t\t\t\tprint(func.__name__, end - start)\n\t\t\t\treturn result\n\t\t\n\t\t\treturn wrapper\n\t\t\n\t\t@timethis\n\t\tdef loop(n):\n\t\t\twhile n > 0:\n\t\t\t\tn -= 1\n\t\t\n\t\t\n\t\tloop(100000)\n\t\n\t>>> loop 0.03971695899963379\n\t```\n\t在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行\n\t可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果\n-\t可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的__wrapped__属性直接访问到被装饰的函数，用来解除装饰\n\n# 逗号的特殊作用\n-\t输出时换行变空格\n-\t转换类型为元组\n\n# filter\n-\t接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素\n\t```Python\n\t\tdef is_odd(n):\n\t\t\treturn n % 2 == 1\n\n\t\tfilter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])\n\t```\n\n# any\n-\t原型：\n\t```Python\t\t\t\t\n\t\tdef any(iterable):\n\t\t   for element in iterable:\n\t\t\t   if  element:\n\t\t\t\t   return False\n\t\t   return True\n\t```\n\n# yield\n-\t一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。\n\t```Python\n\t\t>>> def g(n):\n\t\t...     for i in range(n):\n\t\t...             yield i **2\n\t\t...\n\t\t>>> for i in g(5):\n\t\t...     print i,\":\",\n\t\t...\n\t\t0 : 1 : 4 : 9 : 16 :\n\t```","slug":"PythonNotes","published":1,"updated":"2018-07-23T01:28:38.502Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170328/200539402.png"],"comments":1,"layout":"post","link":"","_id":"cjmd072cm000jqcw667rfquno","content":"<hr><p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p><a id=\"more\"></a><h1 id=\"对象皆引用\"><a href=\"#对象皆引用\" class=\"headerlink\" title=\"对象皆引用\"></a>对象皆引用</h1><ul><li>不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用</li><li><p>Python中不需要声明变量类型也是基于此</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=<span class=\"number\">3</span></span><br><span class=\"line\">b=a</span><br></pre></td></tr></table></figure><p>a和b都只是引用一个整型值3，修改b，a的引用值也会变化</p></li><li>如果要拷贝，可以用b=a[:]</li></ul><h1 id=\"string是常量\"><a href=\"#string是常量\" class=\"headerlink\" title=\"string是常量\"></a>string是常量</h1><ul><li>字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变</li></ul><h1 id=\"lambda匿名函数\"><a href=\"#lambda匿名函数\" class=\"headerlink\" title=\"lambda匿名函数\"></a>lambda匿名函数</h1><ul><li>简化函数书写，lambda 参量:计算式</li><li>主要用于排序或者reduce</li><li><p>lambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tx=<span class=\"number\">10</span></span><br><span class=\"line\">\ta=<span class=\"keyword\">lambda</span> y,x=x:x+y</span><br><span class=\"line\">\tx=<span class=\"number\">20</span></span><br><span class=\"line\">\tprint(a(<span class=\"number\">10</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">20</span></span><br></pre></td></tr></table></figure><p>如果不设置默认参数，上面的运行结果就是30</p></li></ul><h1 id=\"迭代器与生成器\"><a href=\"#迭代器与生成器\" class=\"headerlink\" title=\"迭代器与生成器\"></a>迭代器与生成器</h1><ul><li>通过重写对象的<strong>iter</strong>方法实现自定义迭代器，生成器yield实现迭代器的next方法<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Countdown</span>:</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, start)</span>:</span></span><br><span class=\"line\">\t\t\tself.start = start</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = self.start</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">yield</span> n</span><br><span class=\"line\">\t\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__reversed__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &lt;= self.start:</span><br><span class=\"line\">\t\t\t\tn += <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">   </span><br><span class=\"line\">\t<span class=\"keyword\">for</span> rr <span class=\"keyword\">in</span> (Countdown(<span class=\"number\">3</span>)):</span><br><span class=\"line\">\t\tprint(rr)</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></li></ul><h1 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h1><ul><li>同时输出迭代对象和索引，参数为索引开始号<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx,val <span class=\"keyword\">in</span> enumerate(my_list,<span class=\"number\">1</span>):</span><br><span class=\"line\">\tprint(idx,val)</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><ul><li><p>接收任意个参数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">avg</span><span class=\"params\">(first,*rest)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> (first+sum(rest))/(<span class=\"number\">1</span>+len(rest))</span><br></pre></td></tr></table></figure><p>*接任意数量的位置参数，也可以用**接一个字典，代表任意数量的关键字参数，也可以混用*和**<br>顺序(任意个位置参数，*，最后一个位置参数，其他参数，**)</p></li><li>函数返回多个值<br>直接return a,b,c，实际上返回的是一个元祖</li></ul><h1 id=\"装饰器\"><a href=\"#装饰器\" class=\"headerlink\" title=\"装饰器\"></a>装饰器</h1><ul><li><p>一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"keyword\">import</span> time</span><br><span class=\"line\">\t<span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> wraps</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">timethis</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">\t\t@wraps(func)</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">wrapper</span><span class=\"params\">(*args, **kwargs)</span>:</span></span><br><span class=\"line\">\t\t\tstart = time.time()</span><br><span class=\"line\">\t\t\tresult = func(*args, **kwargs)</span><br><span class=\"line\">\t\t\tend = time.time()</span><br><span class=\"line\">\t\t\tprint(func.__name__, end - start)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> result</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> wrapper</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">\t@timethis</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tloop(<span class=\"number\">100000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>loop <span class=\"number\">0.03971695899963379</span></span><br></pre></td></tr></table></figure><p>在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行<br>可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果</p></li><li>可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的<strong>wrapped</strong>属性直接访问到被装饰的函数，用来解除装饰</li></ul><h1 id=\"逗号的特殊作用\"><a href=\"#逗号的特殊作用\" class=\"headerlink\" title=\"逗号的特殊作用\"></a>逗号的特殊作用</h1><ul><li>输出时换行变空格</li><li>转换类型为元组</li></ul><h1 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h1><ul><li>接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">is_odd</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> n % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">filter(is_odd, [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>])</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"any\"><a href=\"#any\" class=\"headerlink\" title=\"any\"></a>any</h1><ul><li>原型：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">any</span><span class=\"params\">(iterable)</span>:</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> iterable:</span><br><span class=\"line\">\t   <span class=\"keyword\">if</span>  element:</span><br><span class=\"line\">\t\t   <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></li></ul><h1 id=\"yield\"><a href=\"#yield\" class=\"headerlink\" title=\"yield\"></a>yield</h1><ul><li>一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">g</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\"><span class=\"meta\">... </span>            <span class=\"keyword\">yield</span> i **<span class=\"number\">2</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> g(<span class=\"number\">5</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">print</span> i,<span class=\"string\">\":\"</span>,</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"number\">0</span> : <span class=\"number\">1</span> : <span class=\"number\">4</span> : <span class=\"number\">9</span> : <span class=\"number\">16</span> :</span><br></pre></td></tr></table></figure></li></ul>","site":{"data":{}},"excerpt":"<hr><p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p>","more":"<h1 id=\"对象皆引用\"><a href=\"#对象皆引用\" class=\"headerlink\" title=\"对象皆引用\"></a>对象皆引用</h1><ul><li>不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用</li><li><p>Python中不需要声明变量类型也是基于此</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=<span class=\"number\">3</span></span><br><span class=\"line\">b=a</span><br></pre></td></tr></table></figure><p>a和b都只是引用一个整型值3，修改b，a的引用值也会变化</p></li><li>如果要拷贝，可以用b=a[:]</li></ul><h1 id=\"string是常量\"><a href=\"#string是常量\" class=\"headerlink\" title=\"string是常量\"></a>string是常量</h1><ul><li>字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变</li></ul><h1 id=\"lambda匿名函数\"><a href=\"#lambda匿名函数\" class=\"headerlink\" title=\"lambda匿名函数\"></a>lambda匿名函数</h1><ul><li>简化函数书写，lambda 参量:计算式</li><li>主要用于排序或者reduce</li><li><p>lambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tx=<span class=\"number\">10</span></span><br><span class=\"line\">\ta=<span class=\"keyword\">lambda</span> y,x=x:x+y</span><br><span class=\"line\">\tx=<span class=\"number\">20</span></span><br><span class=\"line\">\tprint(a(<span class=\"number\">10</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">20</span></span><br></pre></td></tr></table></figure><p>如果不设置默认参数，上面的运行结果就是30</p></li></ul><h1 id=\"迭代器与生成器\"><a href=\"#迭代器与生成器\" class=\"headerlink\" title=\"迭代器与生成器\"></a>迭代器与生成器</h1><ul><li>通过重写对象的<strong>iter</strong>方法实现自定义迭代器，生成器yield实现迭代器的next方法<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Countdown</span>:</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, start)</span>:</span></span><br><span class=\"line\">\t\t\tself.start = start</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = self.start</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">yield</span> n</span><br><span class=\"line\">\t\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__reversed__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &lt;= self.start:</span><br><span class=\"line\">\t\t\t\tn += <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">   </span><br><span class=\"line\">\t<span class=\"keyword\">for</span> rr <span class=\"keyword\">in</span> (Countdown(<span class=\"number\">3</span>)):</span><br><span class=\"line\">\t\tprint(rr)</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"number\">1</span></span><br></pre></td></tr></table></figure></li></ul><h1 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h1><ul><li>同时输出迭代对象和索引，参数为索引开始号<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx,val <span class=\"keyword\">in</span> enumerate(my_list,<span class=\"number\">1</span>):</span><br><span class=\"line\">\tprint(idx,val)</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><ul><li><p>接收任意个参数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">avg</span><span class=\"params\">(first,*rest)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> (first+sum(rest))/(<span class=\"number\">1</span>+len(rest))</span><br></pre></td></tr></table></figure><p>*接任意数量的位置参数，也可以用**接一个字典，代表任意数量的关键字参数，也可以混用*和**<br>顺序(任意个位置参数，*，最后一个位置参数，其他参数，**)</p></li><li>函数返回多个值<br>直接return a,b,c，实际上返回的是一个元祖</li></ul><h1 id=\"装饰器\"><a href=\"#装饰器\" class=\"headerlink\" title=\"装饰器\"></a>装饰器</h1><ul><li><p>一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"keyword\">import</span> time</span><br><span class=\"line\">\t<span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> wraps</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">timethis</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">\t\t@wraps(func)</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">wrapper</span><span class=\"params\">(*args, **kwargs)</span>:</span></span><br><span class=\"line\">\t\t\tstart = time.time()</span><br><span class=\"line\">\t\t\tresult = func(*args, **kwargs)</span><br><span class=\"line\">\t\t\tend = time.time()</span><br><span class=\"line\">\t\t\tprint(func.__name__, end - start)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> result</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> wrapper</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">\t@timethis</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tloop(<span class=\"number\">100000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>loop <span class=\"number\">0.03971695899963379</span></span><br></pre></td></tr></table></figure><p>在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行<br>可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果</p></li><li>可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的<strong>wrapped</strong>属性直接访问到被装饰的函数，用来解除装饰</li></ul><h1 id=\"逗号的特殊作用\"><a href=\"#逗号的特殊作用\" class=\"headerlink\" title=\"逗号的特殊作用\"></a>逗号的特殊作用</h1><ul><li>输出时换行变空格</li><li>转换类型为元组</li></ul><h1 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h1><ul><li>接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">is_odd</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> n % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">filter(is_odd, [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>])</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"any\"><a href=\"#any\" class=\"headerlink\" title=\"any\"></a>any</h1><ul><li>原型：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">any</span><span class=\"params\">(iterable)</span>:</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> iterable:</span><br><span class=\"line\">\t   <span class=\"keyword\">if</span>  element:</span><br><span class=\"line\">\t\t   <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"keyword\">True</span></span><br></pre></td></tr></table></figure></li></ul><h1 id=\"yield\"><a href=\"#yield\" class=\"headerlink\" title=\"yield\"></a>yield</h1><ul><li>一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">g</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\"><span class=\"meta\">... </span>            <span class=\"keyword\">yield</span> i **<span class=\"number\">2</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> g(<span class=\"number\">5</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">print</span> i,<span class=\"string\">\":\"</span>,</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"number\">0</span> : <span class=\"number\">1</span> : <span class=\"number\">4</span> : <span class=\"number\">9</span> : <span class=\"number\">16</span> :</span><br></pre></td></tr></table></figure></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Python特性拾零","path":"2017/03/28/PythonNotes/","eyeCatchImage":null,"excerpt":"<hr><p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p>","date":"2017-03-28T12:02:39.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"论文阅读笔记2018上半年","date":"2018-03-07T02:20:14.000Z","author":"Thinkwee","mathjax":true,"_content":"\n论文阅读笔记\n主要关注自动文摘方向\n<!--more-->\n\n# Neural Machine Translation By Jointly Learning To Align And Translate\n-\t发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。\n-\t编码器解码器模型，翻译任务。\n-\t其中双向GRU做编码器。编码隐藏层向量由双向连接而成。\n-\t生成每一个单词时有不同的表示。\n-\t权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。\n-\t对所有步编码隐藏层向量加权生成表示。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2a61D9C0bJ.png?imageslim)\n\n# Effective Approaches to Attention-based Neural Machine Translation\n-\t发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。\n-\t其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Jhd4h2ic23.png?imageslim)\n\t之后注意力向量过softmax生成概率分布。\n\t\n## 全局注意力\n-\t文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEl1fi1b3B.png?imageslim)\n-\t之后引入了两种Effective Approaches，即局部注意力和input-feeding。\n\n## 局部注意力\n-\t局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：\n -\t单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。\n -\t预测对齐：训练对齐位置。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2km3e0C7Hl.png?imageslim)\n\t其中ht是第t个生成单词的隐藏层向量\n\tWp和vp都是需要训练的权重\n\tS是输入单词长度,与sigmoid相乘就得到输入句中任意位置\n-\t另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fk5dLHDFIf.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HGb9LdgmKf.png?imageslim)\n\n## Input-feeding\n-\tInput-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。\n-\t实验结果表明使用预测对齐的局部注意力模型表现最好。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6ag84hdbdg.png?imageslim)\n\n# A Neural Attention Model for Abstractive Sentence Summarization\n-\t发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。\n-\t提出了一种注意力编码器，使用普通的NNLM解码器。\n-\t未使用RNN，直接用词向量。\n-\t使用全部输入信息,局部输出信息(yc)构建注意力权重。\n-\t直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。\n-\t模型如下图:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/1ihKc65elf.png?imageslim)\n\t\n# Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\n-\t发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。\n-\t基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。\n-\t改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。\n\n## LVT\n-\t减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。\n\n## Feature-rich Encoder\n-\t不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ekCiaiiCK5.png?imageslim)\n\t\n## Switching Generator-pointer\n-\t解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/8FK4CfIBmm.png?imageslim)\n\tSwitching generator/pointer model\n\t开关为G时就用传统方法生成文摘\n\t开关为P时就从输入中拷贝单词到文摘中\n\t\n## 分层注意力\n-\t传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JfLDj1G2Hb.png?imageslim)\n\n# Recurrent Neural Network Regularization\n-\t本文介绍了如何在循环神经网络中使用dropout来防止过拟合\n-\tDropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。\n-\t对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。\n-\t在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。\n-\t作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息\n-\t效果如图:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ji48kaCe2H.png?imageslim)\n\n# Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n-\t本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。\n-\t训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。\n-\t分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HkHJghaaa0.png?imageslim)\n\t三段式对话包含在两个词水平RNN端到端系统中\n\t中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量\n-\t文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。\n-\t系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度\n-\t论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。\n\n# News Event Summarization Complemented by Micropoints\n-\t这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。\n-\t文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。\n-\t这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。\n-\tMicropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。\n-\t筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。\n-\t将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), ..., p(topic n |t))，最后使用DBSCAN完成主题聚类。\n-\t使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。\n-\t现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。\n-\t得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。\n\n# DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\n-\t更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充\n-\t作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。\n-\t作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。\n-\t作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。\n-\t传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/IkFjGiG4L8.png?imageslim)\n\t右边是多维度注意力\n\t可以看到注意力权重变成了向量，与输入词向量维度数相同\n-\t一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。\n-\t有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/km7mbBdDmC.png?imageslim)\n-\t最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。\n\n# Neural Summarization by Extracting Sentences and Words\n-\t本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。\n-\t与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。\n-\t因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。\n-\t编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。\n-\t句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。\n-\t与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。\n-\t与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。\n-\t抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。\n-\t抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。\n\n# A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\n-\t使用强化学习来优化当前的端到端生成式文摘模型\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIEJhaelI8.png?imageslim)\n-\t解决长文摘生成和重复短语问题\n-\t强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘\n-\t模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励\n-\t编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。\n-\t编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。\n-\t解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。\n-\t在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。\n-\t在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。\n-\t之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。\n-\t因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/I631A6dL8J.png?imageslim)\n\tr是人工评价奖励函数\n\t两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子\n\t目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）\n-\t之后作者将监督学习和强化学习的两种目标函数结合起来：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/bbkcG52m3E.png?imageslim)\n\n# Thumbs up? Sentiment Classification using Machine Learning Techniques\n-\t较早的一篇概述用机器学习方法实现文本的情感分类\n-\t使用电影评论作为数据，因为容易获得标注\n-\t通过实验证明简单的人工添加情感关键词并不能做到很好的分类\n-\t为了证明情感分类是否是文本主题分类的一种二分类特殊情况，使用在文本主题分类中取得很好效果的三种方案来进行情感分类：朴素贝叶斯、最大熵分类、支持向量机\n-\t那时使用人工提取的特征，例如特殊的单词、词组。\n-\t使用朴素贝叶斯的话需假定各个特征在给定输入下条件独立\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JAGd4Fm0md.png?imageslim)\n\td是输入文本，c是分类\n\td的各个特征条件独立下可以将公式化为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/d2eaGg3lIg.png?imageslim)\n\tf是特征，n是特征维度的值，这里是离散的整数值\n-\t因此对文章统计计算最大似然估计，乘以先验概率即可得到后验概率，使用了add-one smoothing（防止分母为0？）。因为是假设的特征独立，所以如果提取的特征独立性好的话贝叶斯能取得较好效果。\n-\t使用最大熵分类，相比朴素贝叶斯，最大熵使用了另一个公式估计后验概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/0KkljD37i9.png?imageslim)\n\t其中Z(d)是规范化因子，F是特征函数，λ是特征权重\n-\t这是用拉格朗日乘子法求解有约束条件的最优化问题得到的解。其中约束条件：特征函数关于联合经验分布的期望与特征函数关于后验概率和边缘经验分布乘积的期望相同。而最优化：使定义在后验概率上的条件熵最大。其隐藏含义是我们应该选择这样的模型：在保持已有数据的一致性上，做出最少的假设（熵最大）。最大熵不需要假设特征相互独立，因此应该有更好的表现。\n-\t支持向量机的思想是找到一个超平面，不仅能够将数据分类，而且分类出来的间隔最大。\n-\t使用均匀分布（一半正一半负）的数据，进行三折交叉验证，将标点符号作为独立语法单位。\n-\t为了解决一种contextual effect of negation（即not very good与good意思恰恰相反），将两个否定词之间的每一个词、否定词后接的第一个标点符号都加上NOT_标签。\n-\t使用语料库中出现最多的前多少个一元、二元词组作为特征。\n-\t在最大熵中特征函数是presence的，即只有1和0的区别，而在svm和贝叶斯中特征值是定量的，假如将svm和贝叶斯中的特征值二值化会怎样呢？结果与主题分类相反，在情感分类任务中，将特征值二值化会使svm和贝叶斯表现提升，svm提升尤为明显。\n-\t作者还尝试为每一个词标注POS信息，但是效果有好有坏，贝叶斯的精确度略有提高，SVM下降，最大熵不变；只使用形容词作为特征，然而效果并不好，使用相同数量的最频繁的单词效果更好；为每一个词标注位置信息（段首，段中，段末）也没有特别改善结果。\n-\t结果：SVM最好，贝叶斯最差，一元单词特征最好，但整体结果比主题分类的效果差。作者指出了他使用的训练集（电影评论）中常出现欲扬先抑的现象，机器学习方法难以识别真正的情感。\n\n# 基于双重注意力模型的微博情感分析方法\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/E9jaE6AcFB.png?imageslim)\n-\t情感符号库和符号集合构建：先构建好情感符号库，包括情感词，表情符号，程度副词，否定词，网络用语。针对训练的每一条微博文本构建情感符号集合，先分词，对每一个词：在情感词库中则直接加入；程度副词加后接的情感词作为整体加入，删除重复情感词；否定词加情感词，否定词加程度副词加情感词，删除重复词组。\n-\t词向量表示层：在词嵌入构造时有一个词向量矩阵，对每一段文本有两个输入序列：原文本和仅包括情感符号的文本，所以对每一段文本，构造两个词向量：普通语义词向量和情感词向量。\n-\t语义获取层：对原文本使用BiLSTM+Attention模型，对情感符号文本采用全连接+attention。通过拼接正反两个方向的LSTM输出构造BiLSTM输出。作者认为微博文本中的情感符号较简单，每个情感符号仅具有情感加强或者减弱的作用，情感符号相互之间不存在较强的语义依赖关系。因此使用全连接网络。与标准Bahdanau Attention不同的是，情感分析是普通分类任务，不具有序列到序列模型，因此在构造注意力权重时参数只有输入端的词向量，作者为此引入了一个随机初始化的上下文向量A：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Ej83dIEagl.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/4jDkmhKDh1.png?imageslim)\n\t其中W、b、A均为训练学习到的参数，对原文本和情感符号文本分别设立A，\n-\t语义合成层：直接拼接两个词向量。\n-\t情感计算层：softmax分类，交叉熵损失，情感标签包括开心、喜欢、讨厌、生气、憎恶等等，使用NLPCC2013、2014数据集，相比非深度学习和无注意力深度学习方法结果有所改进。\n\n# 基于情感词向量的微博情感分类\n-\t本文在w2v训练出来的初始词向量上进行了情感调整，考虑了情感倾向信息。\n-\t首先用w2v训练出原始词向量，再针对每一个情感种子词（已知情感标签的词），使用一个改进的情感CBOW-W2V模型，训练出一个情感向量调整值，对种子词以及种子词的上下文进行调整。\n-\t对负采样CBOW模型进行改进，将输出层的softmax词分类改为情感标签分类，极大化公式为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/A300DIhIDL.png?imageslim)\n-\t其中S是情感标签，取对数之后，针对每一个情感种子词，最大化目标函数可以写成：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/kl5JeLec9b.png?imageslim)\n-\t分别对Xw和THETAs求导，得到上下文词向量的平均调整量和中心词向量的调整量：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/8e88bi8Lh8.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/I0KBEGG94b.png?imageslim)\n-\t之后类似反向传播学习的方式进行学习（调整）：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Le6A1e20bh.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/49aLkb5AH8.png?imageslim)\n-\t改进了词向量之后是完成情感分类的任务。文章使用了3种方案：SVM，logistics回归（情感只分正负类），CNN。\n-\t提到了两种由词向量得到文本向量的方法：使用TF-IDF值进行加权；对词向量序列使用CNN得到多个一维特征，再进行max-pooling下采样。\n\n# 用于微博情感分析的一种情感语义增强的深度学习模型\n-\t本文强调了表情符号比情感词在微博中更能起到指示文本情感的作用，借助词向量表示技术 ,为常用表情符号构建情感空间的特征表示矩阵 ;基于向量的语义合成计算原理 ,通过矩阵与词向量的乘积运算完成词义到情感空间的映射 ;\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/h5b728245A.png?imageslim)\n-\t第一层（词向量输入层）：为所有单词和符号构建词向量，对表情符号和文本词向量分别拼接形成向量序列\n-\t第二层（情感语义增强层）：得到表情符号向量序列Re和词向量序列Rm，基于向量的语义合成原理，将两个矩阵相乘（相同维度为向量维数），得到每条微博在情感空间的特征表示E，并提出了一种算法使得反向传播时词向量的L2范数保持不变。\n-\t第三层（卷积采样层）：对矩阵E进行卷积提取特征（在指定窗口内完成微博文本的语义合成）。再进行下采样。\n-\t第四层（情感得分层）：下采样之后的向量过MLP获得更高层的特征表示（融合各个卷积核卷积采样之后的结果？），过softmax得到情感标签分类概率。\n-\t作者认为词向量是矩阵乘积得到的，而不是简单的拼接增加维度，因此减少了模型复杂度，提高了计算效率。\n\n# Distributed Representations of Words and Phrases and their Compositionality\n-\t介绍了w2v的负采样版本。\n-\t以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。\n-\t用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/FH1960aACj.png?imageslim)\n\t被替换成\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/j1mjCDC8DI.png?imageslim)\n-\t每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。\n-\t对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以下面的概率跳过：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/AgEkIaGCGJ.png?imageslim)\n\t其中f是词频率，t是一个阈值\n-\t这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。","source":"_posts/PaperReading.md","raw":"---\ntitle: 论文阅读笔记2018上半年\ndate: 2018-03-07 10:20:14\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180307/14f9JEiGeI.jpg?imageslim\n---\n\n论文阅读笔记\n主要关注自动文摘方向\n<!--more-->\n\n# Neural Machine Translation By Jointly Learning To Align And Translate\n-\t发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。\n-\t编码器解码器模型，翻译任务。\n-\t其中双向GRU做编码器。编码隐藏层向量由双向连接而成。\n-\t生成每一个单词时有不同的表示。\n-\t权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。\n-\t对所有步编码隐藏层向量加权生成表示。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2a61D9C0bJ.png?imageslim)\n\n# Effective Approaches to Attention-based Neural Machine Translation\n-\t发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。\n-\t其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Jhd4h2ic23.png?imageslim)\n\t之后注意力向量过softmax生成概率分布。\n\t\n## 全局注意力\n-\t文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEl1fi1b3B.png?imageslim)\n-\t之后引入了两种Effective Approaches，即局部注意力和input-feeding。\n\n## 局部注意力\n-\t局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：\n -\t单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。\n -\t预测对齐：训练对齐位置。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2km3e0C7Hl.png?imageslim)\n\t其中ht是第t个生成单词的隐藏层向量\n\tWp和vp都是需要训练的权重\n\tS是输入单词长度,与sigmoid相乘就得到输入句中任意位置\n-\t另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fk5dLHDFIf.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HGb9LdgmKf.png?imageslim)\n\n## Input-feeding\n-\tInput-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。\n-\t实验结果表明使用预测对齐的局部注意力模型表现最好。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6ag84hdbdg.png?imageslim)\n\n# A Neural Attention Model for Abstractive Sentence Summarization\n-\t发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。\n-\t提出了一种注意力编码器，使用普通的NNLM解码器。\n-\t未使用RNN，直接用词向量。\n-\t使用全部输入信息,局部输出信息(yc)构建注意力权重。\n-\t直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。\n-\t模型如下图:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/1ihKc65elf.png?imageslim)\n\t\n# Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\n-\t发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。\n-\t基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。\n-\t改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。\n\n## LVT\n-\t减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。\n\n## Feature-rich Encoder\n-\t不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ekCiaiiCK5.png?imageslim)\n\t\n## Switching Generator-pointer\n-\t解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/8FK4CfIBmm.png?imageslim)\n\tSwitching generator/pointer model\n\t开关为G时就用传统方法生成文摘\n\t开关为P时就从输入中拷贝单词到文摘中\n\t\n## 分层注意力\n-\t传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JfLDj1G2Hb.png?imageslim)\n\n# Recurrent Neural Network Regularization\n-\t本文介绍了如何在循环神经网络中使用dropout来防止过拟合\n-\tDropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。\n-\t对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。\n-\t在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。\n-\t作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息\n-\t效果如图:\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ji48kaCe2H.png?imageslim)\n\n# Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n-\t本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。\n-\t训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。\n-\t分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HkHJghaaa0.png?imageslim)\n\t三段式对话包含在两个词水平RNN端到端系统中\n\t中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量\n-\t文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。\n-\t系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度\n-\t论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。\n\n# News Event Summarization Complemented by Micropoints\n-\t这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。\n-\t文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。\n-\t这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。\n-\tMicropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。\n-\t筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。\n-\t将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), ..., p(topic n |t))，最后使用DBSCAN完成主题聚类。\n-\t使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。\n-\t现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。\n-\t得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。\n\n# DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\n-\t更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充\n-\t作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。\n-\t作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。\n-\t作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。\n-\t传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/IkFjGiG4L8.png?imageslim)\n\t右边是多维度注意力\n\t可以看到注意力权重变成了向量，与输入词向量维度数相同\n-\t一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。\n-\t有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/km7mbBdDmC.png?imageslim)\n-\t最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。\n\n# Neural Summarization by Extracting Sentences and Words\n-\t本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。\n-\t与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。\n-\t因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。\n-\t编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。\n-\t句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。\n-\t与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。\n-\t与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。\n-\t抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。\n-\t抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。\n\n# A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\n-\t使用强化学习来优化当前的端到端生成式文摘模型\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIEJhaelI8.png?imageslim)\n-\t解决长文摘生成和重复短语问题\n-\t强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘\n-\t模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励\n-\t编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。\n-\t编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。\n-\t解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。\n-\t在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。\n-\t在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。\n-\t之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。\n-\t因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/I631A6dL8J.png?imageslim)\n\tr是人工评价奖励函数\n\t两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子\n\t目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）\n-\t之后作者将监督学习和强化学习的两种目标函数结合起来：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/bbkcG52m3E.png?imageslim)\n\n# Thumbs up? Sentiment Classification using Machine Learning Techniques\n-\t较早的一篇概述用机器学习方法实现文本的情感分类\n-\t使用电影评论作为数据，因为容易获得标注\n-\t通过实验证明简单的人工添加情感关键词并不能做到很好的分类\n-\t为了证明情感分类是否是文本主题分类的一种二分类特殊情况，使用在文本主题分类中取得很好效果的三种方案来进行情感分类：朴素贝叶斯、最大熵分类、支持向量机\n-\t那时使用人工提取的特征，例如特殊的单词、词组。\n-\t使用朴素贝叶斯的话需假定各个特征在给定输入下条件独立\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JAGd4Fm0md.png?imageslim)\n\td是输入文本，c是分类\n\td的各个特征条件独立下可以将公式化为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/d2eaGg3lIg.png?imageslim)\n\tf是特征，n是特征维度的值，这里是离散的整数值\n-\t因此对文章统计计算最大似然估计，乘以先验概率即可得到后验概率，使用了add-one smoothing（防止分母为0？）。因为是假设的特征独立，所以如果提取的特征独立性好的话贝叶斯能取得较好效果。\n-\t使用最大熵分类，相比朴素贝叶斯，最大熵使用了另一个公式估计后验概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180307/0KkljD37i9.png?imageslim)\n\t其中Z(d)是规范化因子，F是特征函数，λ是特征权重\n-\t这是用拉格朗日乘子法求解有约束条件的最优化问题得到的解。其中约束条件：特征函数关于联合经验分布的期望与特征函数关于后验概率和边缘经验分布乘积的期望相同。而最优化：使定义在后验概率上的条件熵最大。其隐藏含义是我们应该选择这样的模型：在保持已有数据的一致性上，做出最少的假设（熵最大）。最大熵不需要假设特征相互独立，因此应该有更好的表现。\n-\t支持向量机的思想是找到一个超平面，不仅能够将数据分类，而且分类出来的间隔最大。\n-\t使用均匀分布（一半正一半负）的数据，进行三折交叉验证，将标点符号作为独立语法单位。\n-\t为了解决一种contextual effect of negation（即not very good与good意思恰恰相反），将两个否定词之间的每一个词、否定词后接的第一个标点符号都加上NOT_标签。\n-\t使用语料库中出现最多的前多少个一元、二元词组作为特征。\n-\t在最大熵中特征函数是presence的，即只有1和0的区别，而在svm和贝叶斯中特征值是定量的，假如将svm和贝叶斯中的特征值二值化会怎样呢？结果与主题分类相反，在情感分类任务中，将特征值二值化会使svm和贝叶斯表现提升，svm提升尤为明显。\n-\t作者还尝试为每一个词标注POS信息，但是效果有好有坏，贝叶斯的精确度略有提高，SVM下降，最大熵不变；只使用形容词作为特征，然而效果并不好，使用相同数量的最频繁的单词效果更好；为每一个词标注位置信息（段首，段中，段末）也没有特别改善结果。\n-\t结果：SVM最好，贝叶斯最差，一元单词特征最好，但整体结果比主题分类的效果差。作者指出了他使用的训练集（电影评论）中常出现欲扬先抑的现象，机器学习方法难以识别真正的情感。\n\n# 基于双重注意力模型的微博情感分析方法\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/E9jaE6AcFB.png?imageslim)\n-\t情感符号库和符号集合构建：先构建好情感符号库，包括情感词，表情符号，程度副词，否定词，网络用语。针对训练的每一条微博文本构建情感符号集合，先分词，对每一个词：在情感词库中则直接加入；程度副词加后接的情感词作为整体加入，删除重复情感词；否定词加情感词，否定词加程度副词加情感词，删除重复词组。\n-\t词向量表示层：在词嵌入构造时有一个词向量矩阵，对每一段文本有两个输入序列：原文本和仅包括情感符号的文本，所以对每一段文本，构造两个词向量：普通语义词向量和情感词向量。\n-\t语义获取层：对原文本使用BiLSTM+Attention模型，对情感符号文本采用全连接+attention。通过拼接正反两个方向的LSTM输出构造BiLSTM输出。作者认为微博文本中的情感符号较简单，每个情感符号仅具有情感加强或者减弱的作用，情感符号相互之间不存在较强的语义依赖关系。因此使用全连接网络。与标准Bahdanau Attention不同的是，情感分析是普通分类任务，不具有序列到序列模型，因此在构造注意力权重时参数只有输入端的词向量，作者为此引入了一个随机初始化的上下文向量A：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Ej83dIEagl.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/4jDkmhKDh1.png?imageslim)\n\t其中W、b、A均为训练学习到的参数，对原文本和情感符号文本分别设立A，\n-\t语义合成层：直接拼接两个词向量。\n-\t情感计算层：softmax分类，交叉熵损失，情感标签包括开心、喜欢、讨厌、生气、憎恶等等，使用NLPCC2013、2014数据集，相比非深度学习和无注意力深度学习方法结果有所改进。\n\n# 基于情感词向量的微博情感分类\n-\t本文在w2v训练出来的初始词向量上进行了情感调整，考虑了情感倾向信息。\n-\t首先用w2v训练出原始词向量，再针对每一个情感种子词（已知情感标签的词），使用一个改进的情感CBOW-W2V模型，训练出一个情感向量调整值，对种子词以及种子词的上下文进行调整。\n-\t对负采样CBOW模型进行改进，将输出层的softmax词分类改为情感标签分类，极大化公式为：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/A300DIhIDL.png?imageslim)\n-\t其中S是情感标签，取对数之后，针对每一个情感种子词，最大化目标函数可以写成：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/kl5JeLec9b.png?imageslim)\n-\t分别对Xw和THETAs求导，得到上下文词向量的平均调整量和中心词向量的调整量：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/8e88bi8Lh8.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/I0KBEGG94b.png?imageslim)\n-\t之后类似反向传播学习的方式进行学习（调整）：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Le6A1e20bh.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/49aLkb5AH8.png?imageslim)\n-\t改进了词向量之后是完成情感分类的任务。文章使用了3种方案：SVM，logistics回归（情感只分正负类），CNN。\n-\t提到了两种由词向量得到文本向量的方法：使用TF-IDF值进行加权；对词向量序列使用CNN得到多个一维特征，再进行max-pooling下采样。\n\n# 用于微博情感分析的一种情感语义增强的深度学习模型\n-\t本文强调了表情符号比情感词在微博中更能起到指示文本情感的作用，借助词向量表示技术 ,为常用表情符号构建情感空间的特征表示矩阵 ;基于向量的语义合成计算原理 ,通过矩阵与词向量的乘积运算完成词义到情感空间的映射 ;\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/h5b728245A.png?imageslim)\n-\t第一层（词向量输入层）：为所有单词和符号构建词向量，对表情符号和文本词向量分别拼接形成向量序列\n-\t第二层（情感语义增强层）：得到表情符号向量序列Re和词向量序列Rm，基于向量的语义合成原理，将两个矩阵相乘（相同维度为向量维数），得到每条微博在情感空间的特征表示E，并提出了一种算法使得反向传播时词向量的L2范数保持不变。\n-\t第三层（卷积采样层）：对矩阵E进行卷积提取特征（在指定窗口内完成微博文本的语义合成）。再进行下采样。\n-\t第四层（情感得分层）：下采样之后的向量过MLP获得更高层的特征表示（融合各个卷积核卷积采样之后的结果？），过softmax得到情感标签分类概率。\n-\t作者认为词向量是矩阵乘积得到的，而不是简单的拼接增加维度，因此减少了模型复杂度，提高了计算效率。\n\n# Distributed Representations of Words and Phrases and their Compositionality\n-\t介绍了w2v的负采样版本。\n-\t以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。\n-\t用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/FH1960aACj.png?imageslim)\n\t被替换成\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/j1mjCDC8DI.png?imageslim)\n-\t每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。\n-\t对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以下面的概率跳过：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180324/AgEkIaGCGJ.png?imageslim)\n\t其中f是词频率，t是一个阈值\n-\t这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。","slug":"PaperReading","published":1,"updated":"2018-09-20T13:49:14.711Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180307/14f9JEiGeI.jpg?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072co000nqcw6ejn1yfkb","content":"<p>论文阅读笔记<br>主要关注自动文摘方向<br><a id=\"more\"></a></p><h1 id=\"Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\"><a href=\"#Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\" class=\"headerlink\" title=\"Neural Machine Translation By Jointly Learning To Align And Translate\"></a>Neural Machine Translation By Jointly Learning To Align And Translate</h1><ul><li>发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。</li><li>编码器解码器模型，翻译任务。</li><li>其中双向GRU做编码器。编码隐藏层向量由双向连接而成。</li><li>生成每一个单词时有不同的表示。</li><li>权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。</li><li>对所有步编码隐藏层向量加权生成表示。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2a61D9C0bJ.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Effective-Approaches-to-Attention-based-Neural-Machine-Translation\"><a href=\"#Effective-Approaches-to-Attention-based-Neural-Machine-Translation\" class=\"headerlink\" title=\"Effective Approaches to Attention-based Neural Machine Translation\"></a>Effective Approaches to Attention-based Neural Machine Translation</h1><ul><li>发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。</li><li>其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Jhd4h2ic23.png?imageslim\" alt=\"mark\"><br>之后注意力向量过softmax生成概率分布。</li></ul><h2 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h2><ul><li>文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEl1fi1b3B.png?imageslim\" alt=\"mark\"></li><li>之后引入了两种Effective Approaches，即局部注意力和input-feeding。</li></ul><h2 id=\"局部注意力\"><a href=\"#局部注意力\" class=\"headerlink\" title=\"局部注意力\"></a>局部注意力</h2><ul><li>局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：<ul><li>单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。</li><li>预测对齐：训练对齐位置。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2km3e0C7Hl.png?imageslim\" alt=\"mark\"><br>其中ht是第t个生成单词的隐藏层向量<br>Wp和vp都是需要训练的权重<br>S是输入单词长度,与sigmoid相乘就得到输入句中任意位置</li></ul></li><li>另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fk5dLHDFIf.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HGb9LdgmKf.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"Input-feeding\"><a href=\"#Input-feeding\" class=\"headerlink\" title=\"Input-feeding\"></a>Input-feeding</h2><ul><li>Input-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。</li><li>实验结果表明使用预测对齐的局部注意力模型表现最好。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6ag84hdbdg.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Neural Attention Model for Abstractive Sentence Summarization\"></a>A Neural Attention Model for Abstractive Sentence Summarization</h1><ul><li>发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。</li><li>提出了一种注意力编码器，使用普通的NNLM解码器。</li><li>未使用RNN，直接用词向量。</li><li>使用全部输入信息,局部输出信息(yc)构建注意力权重。</li><li>直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。</li><li>模型如下图:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/1ihKc65elf.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\"><a href=\"#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\" class=\"headerlink\" title=\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\"></a>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</h1><ul><li>发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。</li><li>基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。</li><li>改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。</li></ul><h2 id=\"LVT\"><a href=\"#LVT\" class=\"headerlink\" title=\"LVT\"></a>LVT</h2><ul><li>减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。</li></ul><h2 id=\"Feature-rich-Encoder\"><a href=\"#Feature-rich-Encoder\" class=\"headerlink\" title=\"Feature-rich Encoder\"></a>Feature-rich Encoder</h2><ul><li>不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ekCiaiiCK5.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"Switching-Generator-pointer\"><a href=\"#Switching-Generator-pointer\" class=\"headerlink\" title=\"Switching Generator-pointer\"></a>Switching Generator-pointer</h2><ul><li>解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/8FK4CfIBmm.png?imageslim\" alt=\"mark\"><br>Switching generator/pointer model<br>开关为G时就用传统方法生成文摘<br>开关为P时就从输入中拷贝单词到文摘中</li></ul><h2 id=\"分层注意力\"><a href=\"#分层注意力\" class=\"headerlink\" title=\"分层注意力\"></a>分层注意力</h2><ul><li>传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JfLDj1G2Hb.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Recurrent-Neural-Network-Regularization\"><a href=\"#Recurrent-Neural-Network-Regularization\" class=\"headerlink\" title=\"Recurrent Neural Network Regularization\"></a>Recurrent Neural Network Regularization</h1><ul><li>本文介绍了如何在循环神经网络中使用dropout来防止过拟合</li><li>Dropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。</li><li>对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。</li><li>在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。</li><li>作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息</li><li>效果如图:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ji48kaCe2H.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\"><a href=\"#Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\" class=\"headerlink\" title=\"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\"></a>Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</h1><ul><li>本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。</li><li>训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。</li><li>分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HkHJghaaa0.png?imageslim\" alt=\"mark\"><br>三段式对话包含在两个词水平RNN端到端系统中<br>中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量</li><li>文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。</li><li>系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度</li><li>论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。</li></ul><h1 id=\"News-Event-Summarization-Complemented-by-Micropoints\"><a href=\"#News-Event-Summarization-Complemented-by-Micropoints\" class=\"headerlink\" title=\"News Event Summarization Complemented by Micropoints\"></a>News Event Summarization Complemented by Micropoints</h1><ul><li>这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。</li><li>文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。</li><li>这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。</li><li>Micropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。</li><li>筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。</li><li>将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), …, p(topic n |t))，最后使用DBSCAN完成主题聚类。</li><li>使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。</li><li>现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。</li><li>得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。</li></ul><h1 id=\"DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\"><a href=\"#DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\" class=\"headerlink\" title=\"DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\"></a>DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</h1><ul><li>更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充</li><li>作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。</li><li>作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。</li><li>作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。</li><li>传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/IkFjGiG4L8.png?imageslim\" alt=\"mark\"><br>右边是多维度注意力<br>可以看到注意力权重变成了向量，与输入词向量维度数相同</li><li>一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。</li><li>有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/km7mbBdDmC.png?imageslim\" alt=\"mark\"></li><li>最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。</li></ul><h1 id=\"Neural-Summarization-by-Extracting-Sentences-and-Words\"><a href=\"#Neural-Summarization-by-Extracting-Sentences-and-Words\" class=\"headerlink\" title=\"Neural Summarization by Extracting Sentences and Words\"></a>Neural Summarization by Extracting Sentences and Words</h1><ul><li>本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。</li><li>与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。</li><li>因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。</li><li>编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。</li><li>句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。</li><li>与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。</li><li>与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。</li><li>抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。</li><li>抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。</li></ul><h1 id=\"A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\"><a href=\"#A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\" class=\"headerlink\" title=\"A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\"></a>A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1><ul><li>使用强化学习来优化当前的端到端生成式文摘模型<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIEJhaelI8.png?imageslim\" alt=\"mark\"></li><li>解决长文摘生成和重复短语问题</li><li>强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘</li><li>模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励</li><li>编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。</li><li>编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。</li><li>解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。</li><li>在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。</li><li>在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。</li><li>之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。</li><li>因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/I631A6dL8J.png?imageslim\" alt=\"mark\"><br>r是人工评价奖励函数<br>两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子<br>目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）</li><li>之后作者将监督学习和强化学习的两种目标函数结合起来：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/bbkcG52m3E.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Thumbs-up-Sentiment-Classification-using-Machine-Learning-Techniques\"><a href=\"#Thumbs-up-Sentiment-Classification-using-Machine-Learning-Techniques\" class=\"headerlink\" title=\"Thumbs up? Sentiment Classification using Machine Learning Techniques\"></a>Thumbs up? Sentiment Classification using Machine Learning Techniques</h1><ul><li>较早的一篇概述用机器学习方法实现文本的情感分类</li><li>使用电影评论作为数据，因为容易获得标注</li><li>通过实验证明简单的人工添加情感关键词并不能做到很好的分类</li><li>为了证明情感分类是否是文本主题分类的一种二分类特殊情况，使用在文本主题分类中取得很好效果的三种方案来进行情感分类：朴素贝叶斯、最大熵分类、支持向量机</li><li>那时使用人工提取的特征，例如特殊的单词、词组。</li><li>使用朴素贝叶斯的话需假定各个特征在给定输入下条件独立<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JAGd4Fm0md.png?imageslim\" alt=\"mark\"><br>d是输入文本，c是分类<br>d的各个特征条件独立下可以将公式化为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/d2eaGg3lIg.png?imageslim\" alt=\"mark\"><br>f是特征，n是特征维度的值，这里是离散的整数值</li><li>因此对文章统计计算最大似然估计，乘以先验概率即可得到后验概率，使用了add-one smoothing（防止分母为0？）。因为是假设的特征独立，所以如果提取的特征独立性好的话贝叶斯能取得较好效果。</li><li>使用最大熵分类，相比朴素贝叶斯，最大熵使用了另一个公式估计后验概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/0KkljD37i9.png?imageslim\" alt=\"mark\"><br>其中Z(d)是规范化因子，F是特征函数，λ是特征权重</li><li>这是用拉格朗日乘子法求解有约束条件的最优化问题得到的解。其中约束条件：特征函数关于联合经验分布的期望与特征函数关于后验概率和边缘经验分布乘积的期望相同。而最优化：使定义在后验概率上的条件熵最大。其隐藏含义是我们应该选择这样的模型：在保持已有数据的一致性上，做出最少的假设（熵最大）。最大熵不需要假设特征相互独立，因此应该有更好的表现。</li><li>支持向量机的思想是找到一个超平面，不仅能够将数据分类，而且分类出来的间隔最大。</li><li>使用均匀分布（一半正一半负）的数据，进行三折交叉验证，将标点符号作为独立语法单位。</li><li>为了解决一种contextual effect of negation（即not very good与good意思恰恰相反），将两个否定词之间的每一个词、否定词后接的第一个标点符号都加上NOT_标签。</li><li>使用语料库中出现最多的前多少个一元、二元词组作为特征。</li><li>在最大熵中特征函数是presence的，即只有1和0的区别，而在svm和贝叶斯中特征值是定量的，假如将svm和贝叶斯中的特征值二值化会怎样呢？结果与主题分类相反，在情感分类任务中，将特征值二值化会使svm和贝叶斯表现提升，svm提升尤为明显。</li><li>作者还尝试为每一个词标注POS信息，但是效果有好有坏，贝叶斯的精确度略有提高，SVM下降，最大熵不变；只使用形容词作为特征，然而效果并不好，使用相同数量的最频繁的单词效果更好；为每一个词标注位置信息（段首，段中，段末）也没有特别改善结果。</li><li>结果：SVM最好，贝叶斯最差，一元单词特征最好，但整体结果比主题分类的效果差。作者指出了他使用的训练集（电影评论）中常出现欲扬先抑的现象，机器学习方法难以识别真正的情感。</li></ul><h1 id=\"基于双重注意力模型的微博情感分析方法\"><a href=\"#基于双重注意力模型的微博情感分析方法\" class=\"headerlink\" title=\"基于双重注意力模型的微博情感分析方法\"></a>基于双重注意力模型的微博情感分析方法</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/E9jaE6AcFB.png?imageslim\" alt=\"mark\"></p><ul><li>情感符号库和符号集合构建：先构建好情感符号库，包括情感词，表情符号，程度副词，否定词，网络用语。针对训练的每一条微博文本构建情感符号集合，先分词，对每一个词：在情感词库中则直接加入；程度副词加后接的情感词作为整体加入，删除重复情感词；否定词加情感词，否定词加程度副词加情感词，删除重复词组。</li><li>词向量表示层：在词嵌入构造时有一个词向量矩阵，对每一段文本有两个输入序列：原文本和仅包括情感符号的文本，所以对每一段文本，构造两个词向量：普通语义词向量和情感词向量。</li><li>语义获取层：对原文本使用BiLSTM+Attention模型，对情感符号文本采用全连接+attention。通过拼接正反两个方向的LSTM输出构造BiLSTM输出。作者认为微博文本中的情感符号较简单，每个情感符号仅具有情感加强或者减弱的作用，情感符号相互之间不存在较强的语义依赖关系。因此使用全连接网络。与标准Bahdanau Attention不同的是，情感分析是普通分类任务，不具有序列到序列模型，因此在构造注意力权重时参数只有输入端的词向量，作者为此引入了一个随机初始化的上下文向量A：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Ej83dIEagl.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/4jDkmhKDh1.png?imageslim\" alt=\"mark\"><br>其中W、b、A均为训练学习到的参数，对原文本和情感符号文本分别设立A，</li><li>语义合成层：直接拼接两个词向量。</li><li>情感计算层：softmax分类，交叉熵损失，情感标签包括开心、喜欢、讨厌、生气、憎恶等等，使用NLPCC2013、2014数据集，相比非深度学习和无注意力深度学习方法结果有所改进。</li></ul><h1 id=\"基于情感词向量的微博情感分类\"><a href=\"#基于情感词向量的微博情感分类\" class=\"headerlink\" title=\"基于情感词向量的微博情感分类\"></a>基于情感词向量的微博情感分类</h1><ul><li>本文在w2v训练出来的初始词向量上进行了情感调整，考虑了情感倾向信息。</li><li>首先用w2v训练出原始词向量，再针对每一个情感种子词（已知情感标签的词），使用一个改进的情感CBOW-W2V模型，训练出一个情感向量调整值，对种子词以及种子词的上下文进行调整。</li><li>对负采样CBOW模型进行改进，将输出层的softmax词分类改为情感标签分类，极大化公式为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/A300DIhIDL.png?imageslim\" alt=\"mark\"></li><li>其中S是情感标签，取对数之后，针对每一个情感种子词，最大化目标函数可以写成：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/kl5JeLec9b.png?imageslim\" alt=\"mark\"></li><li>分别对Xw和THETAs求导，得到上下文词向量的平均调整量和中心词向量的调整量：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/8e88bi8Lh8.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/I0KBEGG94b.png?imageslim\" alt=\"mark\"></li><li>之后类似反向传播学习的方式进行学习（调整）：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Le6A1e20bh.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/49aLkb5AH8.png?imageslim\" alt=\"mark\"></li><li>改进了词向量之后是完成情感分类的任务。文章使用了3种方案：SVM，logistics回归（情感只分正负类），CNN。</li><li>提到了两种由词向量得到文本向量的方法：使用TF-IDF值进行加权；对词向量序列使用CNN得到多个一维特征，再进行max-pooling下采样。</li></ul><h1 id=\"用于微博情感分析的一种情感语义增强的深度学习模型\"><a href=\"#用于微博情感分析的一种情感语义增强的深度学习模型\" class=\"headerlink\" title=\"用于微博情感分析的一种情感语义增强的深度学习模型\"></a>用于微博情感分析的一种情感语义增强的深度学习模型</h1><ul><li>本文强调了表情符号比情感词在微博中更能起到指示文本情感的作用，借助词向量表示技术 ,为常用表情符号构建情感空间的特征表示矩阵 ;基于向量的语义合成计算原理 ,通过矩阵与词向量的乘积运算完成词义到情感空间的映射 ;<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/h5b728245A.png?imageslim\" alt=\"mark\"></li><li>第一层（词向量输入层）：为所有单词和符号构建词向量，对表情符号和文本词向量分别拼接形成向量序列</li><li>第二层（情感语义增强层）：得到表情符号向量序列Re和词向量序列Rm，基于向量的语义合成原理，将两个矩阵相乘（相同维度为向量维数），得到每条微博在情感空间的特征表示E，并提出了一种算法使得反向传播时词向量的L2范数保持不变。</li><li>第三层（卷积采样层）：对矩阵E进行卷积提取特征（在指定窗口内完成微博文本的语义合成）。再进行下采样。</li><li>第四层（情感得分层）：下采样之后的向量过MLP获得更高层的特征表示（融合各个卷积核卷积采样之后的结果？），过softmax得到情感标签分类概率。</li><li>作者认为词向量是矩阵乘积得到的，而不是简单的拼接增加维度，因此减少了模型复杂度，提高了计算效率。</li></ul><h1 id=\"Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\"><a href=\"#Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\" class=\"headerlink\" title=\"Distributed Representations of Words and Phrases and their Compositionality\"></a>Distributed Representations of Words and Phrases and their Compositionality</h1><ul><li>介绍了w2v的负采样版本。</li><li>以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。</li><li>用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/FH1960aACj.png?imageslim\" alt=\"mark\"><br>被替换成<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/j1mjCDC8DI.png?imageslim\" alt=\"mark\"></li><li>每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。</li><li>对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以下面的概率跳过：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/AgEkIaGCGJ.png?imageslim\" alt=\"mark\"><br>其中f是词频率，t是一个阈值</li><li>这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。</li></ul>","site":{"data":{}},"excerpt":"<p>论文阅读笔记<br>主要关注自动文摘方向<br>","more":"</p><h1 id=\"Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\"><a href=\"#Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\" class=\"headerlink\" title=\"Neural Machine Translation By Jointly Learning To Align And Translate\"></a>Neural Machine Translation By Jointly Learning To Align And Translate</h1><ul><li>发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。</li><li>编码器解码器模型，翻译任务。</li><li>其中双向GRU做编码器。编码隐藏层向量由双向连接而成。</li><li>生成每一个单词时有不同的表示。</li><li>权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。</li><li>对所有步编码隐藏层向量加权生成表示。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2a61D9C0bJ.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Effective-Approaches-to-Attention-based-Neural-Machine-Translation\"><a href=\"#Effective-Approaches-to-Attention-based-Neural-Machine-Translation\" class=\"headerlink\" title=\"Effective Approaches to Attention-based Neural Machine Translation\"></a>Effective Approaches to Attention-based Neural Machine Translation</h1><ul><li>发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。</li><li>其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Jhd4h2ic23.png?imageslim\" alt=\"mark\"><br>之后注意力向量过softmax生成概率分布。</li></ul><h2 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h2><ul><li>文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/aEl1fi1b3B.png?imageslim\" alt=\"mark\"></li><li>之后引入了两种Effective Approaches，即局部注意力和input-feeding。</li></ul><h2 id=\"局部注意力\"><a href=\"#局部注意力\" class=\"headerlink\" title=\"局部注意力\"></a>局部注意力</h2><ul><li>局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：<ul><li>单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。</li><li>预测对齐：训练对齐位置。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/2km3e0C7Hl.png?imageslim\" alt=\"mark\"><br>其中ht是第t个生成单词的隐藏层向量<br>Wp和vp都是需要训练的权重<br>S是输入单词长度,与sigmoid相乘就得到输入句中任意位置</li></ul></li><li>另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/Fk5dLHDFIf.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HGb9LdgmKf.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"Input-feeding\"><a href=\"#Input-feeding\" class=\"headerlink\" title=\"Input-feeding\"></a>Input-feeding</h2><ul><li>Input-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。</li><li>实验结果表明使用预测对齐的局部注意力模型表现最好。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/6ag84hdbdg.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Neural Attention Model for Abstractive Sentence Summarization\"></a>A Neural Attention Model for Abstractive Sentence Summarization</h1><ul><li>发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。</li><li>提出了一种注意力编码器，使用普通的NNLM解码器。</li><li>未使用RNN，直接用词向量。</li><li>使用全部输入信息,局部输出信息(yc)构建注意力权重。</li><li>直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。</li><li>模型如下图:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/1ihKc65elf.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\"><a href=\"#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\" class=\"headerlink\" title=\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\"></a>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</h1><ul><li>发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。</li><li>基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。</li><li>改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。</li></ul><h2 id=\"LVT\"><a href=\"#LVT\" class=\"headerlink\" title=\"LVT\"></a>LVT</h2><ul><li>减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。</li></ul><h2 id=\"Feature-rich-Encoder\"><a href=\"#Feature-rich-Encoder\" class=\"headerlink\" title=\"Feature-rich Encoder\"></a>Feature-rich Encoder</h2><ul><li>不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ekCiaiiCK5.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"Switching-Generator-pointer\"><a href=\"#Switching-Generator-pointer\" class=\"headerlink\" title=\"Switching Generator-pointer\"></a>Switching Generator-pointer</h2><ul><li>解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/8FK4CfIBmm.png?imageslim\" alt=\"mark\"><br>Switching generator/pointer model<br>开关为G时就用传统方法生成文摘<br>开关为P时就从输入中拷贝单词到文摘中</li></ul><h2 id=\"分层注意力\"><a href=\"#分层注意力\" class=\"headerlink\" title=\"分层注意力\"></a>分层注意力</h2><ul><li>传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JfLDj1G2Hb.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Recurrent-Neural-Network-Regularization\"><a href=\"#Recurrent-Neural-Network-Regularization\" class=\"headerlink\" title=\"Recurrent Neural Network Regularization\"></a>Recurrent Neural Network Regularization</h1><ul><li>本文介绍了如何在循环神经网络中使用dropout来防止过拟合</li><li>Dropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。</li><li>对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。</li><li>在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。</li><li>作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息</li><li>效果如图:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/ji48kaCe2H.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\"><a href=\"#Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\" class=\"headerlink\" title=\"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\"></a>Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</h1><ul><li>本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。</li><li>训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。</li><li>分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HkHJghaaa0.png?imageslim\" alt=\"mark\"><br>三段式对话包含在两个词水平RNN端到端系统中<br>中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量</li><li>文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。</li><li>系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度</li><li>论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。</li></ul><h1 id=\"News-Event-Summarization-Complemented-by-Micropoints\"><a href=\"#News-Event-Summarization-Complemented-by-Micropoints\" class=\"headerlink\" title=\"News Event Summarization Complemented by Micropoints\"></a>News Event Summarization Complemented by Micropoints</h1><ul><li>这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。</li><li>文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。</li><li>这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。</li><li>Micropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。</li><li>筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。</li><li>将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), …, p(topic n |t))，最后使用DBSCAN完成主题聚类。</li><li>使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。</li><li>现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。</li><li>得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。</li></ul><h1 id=\"DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\"><a href=\"#DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\" class=\"headerlink\" title=\"DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\"></a>DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</h1><ul><li>更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充</li><li>作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。</li><li>作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。</li><li>作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。</li><li>传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/IkFjGiG4L8.png?imageslim\" alt=\"mark\"><br>右边是多维度注意力<br>可以看到注意力权重变成了向量，与输入词向量维度数相同</li><li>一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。</li><li>有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/km7mbBdDmC.png?imageslim\" alt=\"mark\"></li><li>最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。</li></ul><h1 id=\"Neural-Summarization-by-Extracting-Sentences-and-Words\"><a href=\"#Neural-Summarization-by-Extracting-Sentences-and-Words\" class=\"headerlink\" title=\"Neural Summarization by Extracting Sentences and Words\"></a>Neural Summarization by Extracting Sentences and Words</h1><ul><li>本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。</li><li>与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。</li><li>因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。</li><li>编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。</li><li>句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。</li><li>与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。</li><li>与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。</li><li>抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。</li><li>抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。</li></ul><h1 id=\"A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\"><a href=\"#A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\" class=\"headerlink\" title=\"A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\"></a>A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1><ul><li>使用强化学习来优化当前的端到端生成式文摘模型<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/HIEJhaelI8.png?imageslim\" alt=\"mark\"></li><li>解决长文摘生成和重复短语问题</li><li>强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘</li><li>模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励</li><li>编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。</li><li>编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。</li><li>解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。</li><li>在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。</li><li>在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。</li><li>之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。</li><li>因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/I631A6dL8J.png?imageslim\" alt=\"mark\"><br>r是人工评价奖励函数<br>两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子<br>目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）</li><li>之后作者将监督学习和强化学习的两种目标函数结合起来：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/bbkcG52m3E.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"Thumbs-up-Sentiment-Classification-using-Machine-Learning-Techniques\"><a href=\"#Thumbs-up-Sentiment-Classification-using-Machine-Learning-Techniques\" class=\"headerlink\" title=\"Thumbs up? Sentiment Classification using Machine Learning Techniques\"></a>Thumbs up? Sentiment Classification using Machine Learning Techniques</h1><ul><li>较早的一篇概述用机器学习方法实现文本的情感分类</li><li>使用电影评论作为数据，因为容易获得标注</li><li>通过实验证明简单的人工添加情感关键词并不能做到很好的分类</li><li>为了证明情感分类是否是文本主题分类的一种二分类特殊情况，使用在文本主题分类中取得很好效果的三种方案来进行情感分类：朴素贝叶斯、最大熵分类、支持向量机</li><li>那时使用人工提取的特征，例如特殊的单词、词组。</li><li>使用朴素贝叶斯的话需假定各个特征在给定输入下条件独立<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/JAGd4Fm0md.png?imageslim\" alt=\"mark\"><br>d是输入文本，c是分类<br>d的各个特征条件独立下可以将公式化为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/d2eaGg3lIg.png?imageslim\" alt=\"mark\"><br>f是特征，n是特征维度的值，这里是离散的整数值</li><li>因此对文章统计计算最大似然估计，乘以先验概率即可得到后验概率，使用了add-one smoothing（防止分母为0？）。因为是假设的特征独立，所以如果提取的特征独立性好的话贝叶斯能取得较好效果。</li><li>使用最大熵分类，相比朴素贝叶斯，最大熵使用了另一个公式估计后验概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180307/0KkljD37i9.png?imageslim\" alt=\"mark\"><br>其中Z(d)是规范化因子，F是特征函数，λ是特征权重</li><li>这是用拉格朗日乘子法求解有约束条件的最优化问题得到的解。其中约束条件：特征函数关于联合经验分布的期望与特征函数关于后验概率和边缘经验分布乘积的期望相同。而最优化：使定义在后验概率上的条件熵最大。其隐藏含义是我们应该选择这样的模型：在保持已有数据的一致性上，做出最少的假设（熵最大）。最大熵不需要假设特征相互独立，因此应该有更好的表现。</li><li>支持向量机的思想是找到一个超平面，不仅能够将数据分类，而且分类出来的间隔最大。</li><li>使用均匀分布（一半正一半负）的数据，进行三折交叉验证，将标点符号作为独立语法单位。</li><li>为了解决一种contextual effect of negation（即not very good与good意思恰恰相反），将两个否定词之间的每一个词、否定词后接的第一个标点符号都加上NOT_标签。</li><li>使用语料库中出现最多的前多少个一元、二元词组作为特征。</li><li>在最大熵中特征函数是presence的，即只有1和0的区别，而在svm和贝叶斯中特征值是定量的，假如将svm和贝叶斯中的特征值二值化会怎样呢？结果与主题分类相反，在情感分类任务中，将特征值二值化会使svm和贝叶斯表现提升，svm提升尤为明显。</li><li>作者还尝试为每一个词标注POS信息，但是效果有好有坏，贝叶斯的精确度略有提高，SVM下降，最大熵不变；只使用形容词作为特征，然而效果并不好，使用相同数量的最频繁的单词效果更好；为每一个词标注位置信息（段首，段中，段末）也没有特别改善结果。</li><li>结果：SVM最好，贝叶斯最差，一元单词特征最好，但整体结果比主题分类的效果差。作者指出了他使用的训练集（电影评论）中常出现欲扬先抑的现象，机器学习方法难以识别真正的情感。</li></ul><h1 id=\"基于双重注意力模型的微博情感分析方法\"><a href=\"#基于双重注意力模型的微博情感分析方法\" class=\"headerlink\" title=\"基于双重注意力模型的微博情感分析方法\"></a>基于双重注意力模型的微博情感分析方法</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/E9jaE6AcFB.png?imageslim\" alt=\"mark\"></p><ul><li>情感符号库和符号集合构建：先构建好情感符号库，包括情感词，表情符号，程度副词，否定词，网络用语。针对训练的每一条微博文本构建情感符号集合，先分词，对每一个词：在情感词库中则直接加入；程度副词加后接的情感词作为整体加入，删除重复情感词；否定词加情感词，否定词加程度副词加情感词，删除重复词组。</li><li>词向量表示层：在词嵌入构造时有一个词向量矩阵，对每一段文本有两个输入序列：原文本和仅包括情感符号的文本，所以对每一段文本，构造两个词向量：普通语义词向量和情感词向量。</li><li>语义获取层：对原文本使用BiLSTM+Attention模型，对情感符号文本采用全连接+attention。通过拼接正反两个方向的LSTM输出构造BiLSTM输出。作者认为微博文本中的情感符号较简单，每个情感符号仅具有情感加强或者减弱的作用，情感符号相互之间不存在较强的语义依赖关系。因此使用全连接网络。与标准Bahdanau Attention不同的是，情感分析是普通分类任务，不具有序列到序列模型，因此在构造注意力权重时参数只有输入端的词向量，作者为此引入了一个随机初始化的上下文向量A：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Ej83dIEagl.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/4jDkmhKDh1.png?imageslim\" alt=\"mark\"><br>其中W、b、A均为训练学习到的参数，对原文本和情感符号文本分别设立A，</li><li>语义合成层：直接拼接两个词向量。</li><li>情感计算层：softmax分类，交叉熵损失，情感标签包括开心、喜欢、讨厌、生气、憎恶等等，使用NLPCC2013、2014数据集，相比非深度学习和无注意力深度学习方法结果有所改进。</li></ul><h1 id=\"基于情感词向量的微博情感分类\"><a href=\"#基于情感词向量的微博情感分类\" class=\"headerlink\" title=\"基于情感词向量的微博情感分类\"></a>基于情感词向量的微博情感分类</h1><ul><li>本文在w2v训练出来的初始词向量上进行了情感调整，考虑了情感倾向信息。</li><li>首先用w2v训练出原始词向量，再针对每一个情感种子词（已知情感标签的词），使用一个改进的情感CBOW-W2V模型，训练出一个情感向量调整值，对种子词以及种子词的上下文进行调整。</li><li>对负采样CBOW模型进行改进，将输出层的softmax词分类改为情感标签分类，极大化公式为：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/A300DIhIDL.png?imageslim\" alt=\"mark\"></li><li>其中S是情感标签，取对数之后，针对每一个情感种子词，最大化目标函数可以写成：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/kl5JeLec9b.png?imageslim\" alt=\"mark\"></li><li>分别对Xw和THETAs求导，得到上下文词向量的平均调整量和中心词向量的调整量：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/8e88bi8Lh8.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/I0KBEGG94b.png?imageslim\" alt=\"mark\"></li><li>之后类似反向传播学习的方式进行学习（调整）：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/Le6A1e20bh.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/49aLkb5AH8.png?imageslim\" alt=\"mark\"></li><li>改进了词向量之后是完成情感分类的任务。文章使用了3种方案：SVM，logistics回归（情感只分正负类），CNN。</li><li>提到了两种由词向量得到文本向量的方法：使用TF-IDF值进行加权；对词向量序列使用CNN得到多个一维特征，再进行max-pooling下采样。</li></ul><h1 id=\"用于微博情感分析的一种情感语义增强的深度学习模型\"><a href=\"#用于微博情感分析的一种情感语义增强的深度学习模型\" class=\"headerlink\" title=\"用于微博情感分析的一种情感语义增强的深度学习模型\"></a>用于微博情感分析的一种情感语义增强的深度学习模型</h1><ul><li>本文强调了表情符号比情感词在微博中更能起到指示文本情感的作用，借助词向量表示技术 ,为常用表情符号构建情感空间的特征表示矩阵 ;基于向量的语义合成计算原理 ,通过矩阵与词向量的乘积运算完成词义到情感空间的映射 ;<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/h5b728245A.png?imageslim\" alt=\"mark\"></li><li>第一层（词向量输入层）：为所有单词和符号构建词向量，对表情符号和文本词向量分别拼接形成向量序列</li><li>第二层（情感语义增强层）：得到表情符号向量序列Re和词向量序列Rm，基于向量的语义合成原理，将两个矩阵相乘（相同维度为向量维数），得到每条微博在情感空间的特征表示E，并提出了一种算法使得反向传播时词向量的L2范数保持不变。</li><li>第三层（卷积采样层）：对矩阵E进行卷积提取特征（在指定窗口内完成微博文本的语义合成）。再进行下采样。</li><li>第四层（情感得分层）：下采样之后的向量过MLP获得更高层的特征表示（融合各个卷积核卷积采样之后的结果？），过softmax得到情感标签分类概率。</li><li>作者认为词向量是矩阵乘积得到的，而不是简单的拼接增加维度，因此减少了模型复杂度，提高了计算效率。</li></ul><h1 id=\"Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\"><a href=\"#Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\" class=\"headerlink\" title=\"Distributed Representations of Words and Phrases and their Compositionality\"></a>Distributed Representations of Words and Phrases and their Compositionality</h1><ul><li>介绍了w2v的负采样版本。</li><li>以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。</li><li>用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/FH1960aACj.png?imageslim\" alt=\"mark\"><br>被替换成<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/j1mjCDC8DI.png?imageslim\" alt=\"mark\"></li><li>每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。</li><li>对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以下面的概率跳过：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180324/AgEkIaGCGJ.png?imageslim\" alt=\"mark\"><br>其中f是词频率，t是一个阈值</li><li>这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Thu Sep 20 2018 21:49:14 GMT+0800 (中国标准时间)","title":"论文阅读笔记2018上半年","path":"2018/03/07/PaperReading/","eyeCatchImage":null,"excerpt":"<p>论文阅读笔记<br>主要关注自动文摘方向<br>","date":"2018-03-07T02:20:14.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:Coding基础与线性回归","mathjax":true,"date":"2017-02-07T13:57:22.000Z","_content":"![](http://ojtdnrpmt.bkt.clouddn.com/17-1-15/62213897-file_1484471472044_c2cc.jpg)\n\n***\n# 简介\n\n2016年11月的时候决定开始入坑机器学习\n首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。\n\n2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression\n\n题目介绍在这：[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n\n下面是数据集表格样式，每个人有12个属性\n\n<!--more-->\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG)\n\n\n***\n# 不是算法的算法\n\n官方示例就是按几个属性分类，比如年龄，性别，票价(.....)\n然后对每个属性内所有人的生还数据（0或者1）加一起求平均。\n英文注释都是官方文档的说明\n我就当入门教程学了，也全打了上去\n代码如下：\n```Python\n    # -*- coding: utf-8 -*-\n    \"\"\"\n    Created on Sun Oct 30 15:28:22 2016\n    \n    @author: thinkwee\n    \"\"\"\n    \n    import csv as csv \n    import numpy as np\n    from glue import qglue\n    \n    \n    test_file=(open(r'文件目录略', 'r'))\n    test_file_object = csv.reader(open(r'文件目录略', 'r'))\n    testheader = next(test_file_object)\n    predictions_file = open(r\"文件目录略\", \"w\")\n    predictions_file_object = csv.writer(predictions_file)\n    p = csv.writer(predictions_file)\n    p.writerow([\"PassengerId\", \"Survived\"])\n    csv_file_object = csv.reader(open(r'文件目录略', 'r')) \n    trainheader = next(csv_file_object)  # The next() command just skips the \n\t\t\t\t\t\t\t\t\t\t # first line which is a header\n    data=[]                          \t # Create a variable called 'data'.\n    for row in csv_file_object:      \t # Run through each row in the csv file,\n        data.append(row)             \t # adding each row to the data variable\n    print(type(data))\n    data = np.array(data) \t         \t # Then convert from a list to an array\n\t\t\t\t\t\t\t\t\t\t # Be aware that each item is currently\n\t\t\t\t\t\t\t\t\t\t # a string in this format\n    \n    number_passengers = np.size(data[0::,1].astype(np.float))\n    number_survived = np.sum(data[0::,1].astype(np.float))\n    proportion_survivors = number_survived / number_passengers\n    \n    women_only_stats = data[0::,4] == \"female\" # This finds where all \n                                               # the elements in the gender\n                                               # column that equals “female”\n    men_only_stats = data[0::,4] != \"female\"   # This finds where all the \n                                               # elements do not equal \n                                               # female (i.e. male)\n                                               \n    # Using the index from above we select the females and males separately\n    women_onboard = data[women_only_stats,1].astype(np.float)     \n    men_onboard = data[men_only_stats,1].astype(np.float)\n    \n    # Then we finds the proportions of them that survived\n    proportion_women_survived = \\\n                           np.sum(women_onboard) / np.size(women_onboard)  \n    proportion_men_survived = \\\n                           np.sum(men_onboard) / np.size(men_onboard) \n    \n    # and then print it out\n    print ('Proportion of women who survived is %s' % proportion_women_survived)\n    print ('Proportion of men who survived is %s' % proportion_men_survived)\n    \n    \n    \n    \n    # The script will systematically will loop through each combination \n    # and use the 'where' function in python to search the passengers that fit that combination of variables. \n    # Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. \n    # The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  \n    # For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. \n    # So then you can set the bins\n    \n    # So we add a ceiling\n    fare_ceiling = 40\n    \n    # then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\n    data[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n    \n    fare_bracket_size = 10\n    number_of_price_brackets = fare_ceiling // fare_bracket_size\n    \n    # Take the length of an array of unique values in column index 2\n    number_of_classes = len(np.unique(data[0::,2]))\n    \n    number_of_age_brackets=8 \n    \n    # Initialize the survival table with all zeros\n    survival_table = np.zeros((2, number_of_classes, \n    \t\t\t\t\t\t   number_of_price_brackets,\n    \t\t\t\t\t\t   number_of_age_brackets))\n    \n    \n    \n    #Now that these are set up, \n    #you can loop through each variable \n    #and find all those passengers that agree with the statements\n    \n    for i in range(number_of_classes):       \t\t#loop through each class\n      for j in range(number_of_price_brackets):   \t#loop through each price bin\n        for k in range(number_of_age_brackets):     #loop through each age bin\n            women_only_stats_plus = data[                 #Which element           \n                            (data[0::,4] == \"female\")     #is a female\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                        \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)#the next bin\n                           \n                              , 1]                        #in the 2nd col                           \n     \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t\n    \n            men_only_stats_plus = data[                   #Which element           \n                             (data[0::,4] != \"female\")    #is a male\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                                       \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)#the next bin\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)\n                              , 1]\n                              \n            survival_table[0,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) \n            survival_table[1,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))\n    \t\t\n    \t\t#if nan then the type will change to string from float so this sentence can set nan to 0. \n            survival_table[ survival_table != survival_table ] = 0.\n    \n    #Notice that  data[ where function, 1]  means \n    #it is finding the Survived column for the conditional criteria which is being called. \n    #As the loop starts with i=0 and j=0, \n    #the first loop will return the Survived values for all the 1st-class females (i + 1) \n    #who paid less than 10 ((j+1)*fare_bracket_size) \n    #and similarly all the 1st-class males who paid less than 10.  \n    #Before resetting to the top of the loop, \n    #we can calculate the proportion of survivors for this particular \n    #combination of criteria and record it to our survival table\n    \n        \n    #官方示例中将概率大于0.5的视为生还，这里我们略过\n    #直接打印详细概率\n    #survival_table[ survival_table < 0.5 ] = 0\n    #survival_table[ survival_table >= 0.5 ] = 1 \n        \n        \n    #Then we can make the prediction\n    \n    for row in test_file_object:                  # We are going to loop\n                                                  # through each passenger\n                                                  # in the test set                     \n      for j in range(number_of_price_brackets):   # For each passenger we\n                                                  # loop thro each price bin\n        try:                                      # Some passengers have no\n                                                  # Fare data so try to make\n          row[8] = float(row[8])                  # a float\n        except:                                   # If fails: no data, so \n          bin_fare = 3 - float(row[1])            # bin the fare according Pclass\n          break                                   # Break from the loop\n        if row[8] > fare_ceiling:              \t  # If there is data see if\n                                                  # it is greater than fare\n                                                  # ceiling we set earlier\n          bin_fare = number_of_price_brackets-1   # If so set to highest bin\n          break                                   # And then break loop\n        if row[8] >= j * fare_bracket_size\\\n           and row[8] < \\\n           (j+1) * fare_bracket_size:             # If passed these tests \n                                                  # then loop through each bin \n          bin_fare = j                            # then assign index\n          break\n      \n      for j in range(number_of_age_brackets): \n                                                 \n        try:                                    \n                                                \n          row[4] = float(row[4])              \n        except:                                   \n          bin_age = -1      \n          break                                  \n                                   \n        if row[4] >= j * 10\\\n           and row[4] < \\\n           (j+1) * 10:             # If passed these tests \n                                   # then loop through each bin \n          bin_age = j              # then assign index\n          break\n      \n      if row[3] == 'female':       #If the passenger is female\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[0, int(row[1])-1, bin_fare,bin_age]*100)])\n      else:                        #else if male\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[1, int(row[1])-1, bin_fare,bin_age]*100)])\n         \n    # Close out the files.\n    test_file.close() \n    predictions_file.close()\n```\n\n\n***\n# 多元线性回归\n    \n之后买了西瓜书，我把这个例题改成了线性回归模型：\n假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，\n我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小\n用均方误差作为性能度量，均方误差是线性系数的函数\n对线性系数w求导，可以得到w最优解的闭式\n\n关键公式是\n\t** $$ w^*=(X^TX)^{-1}X^Ty $$ **\n\n-\tX:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，\n\t  假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)\n-\tw:线性系数\n-\ty:生还结果 $$ y=w^T*x $$\n\n写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220426082.JPG)\n\n代码如下\n\n```Python\n        train1=train.dropna(subset=(['Age']),axis=0)\n    vali1=vali.dropna(subset=(['Age']),axis=0)\n    \n    validata=np.array(vali1)\n    data=np.array(train1)\n    \n    fare_ceiling = 40\n    data[data[0::,9].astype(np.float)>=fare_ceiling,9] = fare_ceiling - 1.0\n    \n    train = np.column_stack((data[0::,9],data[0::,2],data[0::,5],data[0::,4]))\n    predict=np.column_stack((validata[0::,9],validata[0::,2],validata[0::,5],validata[0::,4]))\n    survive = np.column_stack((data[0::,1]))\n    \n    \n    for i in range(train.shape[0]):\n        if (train[i][3]=='male'):\n            train[i][3]=0.00\n        else:\n            train[i][3]=1.00\n    for i in range(predict.shape[0]):\n        if (predict[i][3]=='male'):\n            predict[i][3]=0.00\n        else:\n            predict[i][3]=1.00\n    \n    x0=np.ones((train.shape[0],1))\n    train=np.concatenate((train,x0),axis=1)\n    \n    x0=np.ones((predict.shape[0],1))\n    predict=np.concatenate((predict,x0),axis=1)\n    \n    print('raw data finish')\n    \n    survive=survive.T.astype(np.float)\n    traint=train.T.astype(np.float)\n    w0=traint.dot(train.astype(np.float))\n    w1=(np.linalg.inv(w0))  \n    w2=w1.dot(traint)\n    w=w2.dot(survive)  #w=(Xt*X)^-1*Xt*y\n    print('w calc finish')\n    \n    feature=['Fare','Pclass','Age','Sex','b']\n    for i in zip(feature,w):\n        print(i)\n    \n    \n    valipredict_file_object.writerow([\"PassengerName\", \"Actual Survived\",\"Predict Survived\",\"XO\"])\n    count=0.0\n    for i in range(predict.shape[0]):\n        temp=predict[i,0::].T.astype(float)\n        answer=temp.dot(w)\n        answer=answer[0]\n        if ((answer>0.5 and validata[i][1]==1) or (answer<0.5 and validata[i][1]==0)):\n            flag=\"Correct\"\n            count=count+1.0;\n        else:\n            flag=\"Error\"\n        valipredict_file_object.writerow([validata[i][3],validata[i][1],answer,flag])\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/predict.shape[0]*100))  \n```\n***\n# scikit-learn中的多元线性回归\n试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220441164.JPG)\n\n如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/093538631.JPG)\n\n```Python\n    train=train.dropna(subset=['Age','Embarked'],axis=0)\n    vali=vali.dropna(subset=(['Age','Embarked']),axis=0)\n    \n    train.loc[train[\"Sex\"]==\"male\",\"Sex\"]=0\n    train.loc[train[\"Sex\"]==\"female\",\"Sex\"]=1\n    train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    trainx=train.reindex(index=train.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    vali.loc[vali[\"Sex\"]==\"male\",\"Sex\"]=0\n    vali.loc[vali[\"Sex\"]==\"female\",\"Sex\"]=1\n    vali.loc[vali[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    vali.loc[vali[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    vali.loc[vali[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    vali1=vali.reindex(index=vali.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    survive=vali.reindex(index=vali.index[:],columns=['Survived'])\n    survive=np.array(survive)\n    \n    feature=['Age','Sex','Parch','Fare','Embarked','SibSp']\n    \n    trainy=train.reindex(index=train.index[:],columns=['Survived'])  \n    trainy=trainy.Survived\n    \n    X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=1)\n    \n    \n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    print(model)\n    \n    \n    for i in zip(feature,model.coef_):\n        print(i)\n    \n    predict=model.predict(vali1)\n    \n    count=0\n    for i in range(len(predict)):\n        if (predict[i]>1 and survive[i] == 1) or  (predict[i]<1 and survive [i]== 0 ):\n            count=count+1.0\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/len(predict)*100))\n```","source":"_posts/TitanicLinearRegression.md","raw":"title: '机器学习入门:Coding基础与线性回归'\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\ndate: 2017-02-07 21:57:22\n---\n![](http://ojtdnrpmt.bkt.clouddn.com/17-1-15/62213897-file_1484471472044_c2cc.jpg)\n\n***\n# 简介\n\n2016年11月的时候决定开始入坑机器学习\n首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。\n\n2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression\n\n题目介绍在这：[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n\n下面是数据集表格样式，每个人有12个属性\n\n<!--more-->\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG)\n\n\n***\n# 不是算法的算法\n\n官方示例就是按几个属性分类，比如年龄，性别，票价(.....)\n然后对每个属性内所有人的生还数据（0或者1）加一起求平均。\n英文注释都是官方文档的说明\n我就当入门教程学了，也全打了上去\n代码如下：\n```Python\n    # -*- coding: utf-8 -*-\n    \"\"\"\n    Created on Sun Oct 30 15:28:22 2016\n    \n    @author: thinkwee\n    \"\"\"\n    \n    import csv as csv \n    import numpy as np\n    from glue import qglue\n    \n    \n    test_file=(open(r'文件目录略', 'r'))\n    test_file_object = csv.reader(open(r'文件目录略', 'r'))\n    testheader = next(test_file_object)\n    predictions_file = open(r\"文件目录略\", \"w\")\n    predictions_file_object = csv.writer(predictions_file)\n    p = csv.writer(predictions_file)\n    p.writerow([\"PassengerId\", \"Survived\"])\n    csv_file_object = csv.reader(open(r'文件目录略', 'r')) \n    trainheader = next(csv_file_object)  # The next() command just skips the \n\t\t\t\t\t\t\t\t\t\t # first line which is a header\n    data=[]                          \t # Create a variable called 'data'.\n    for row in csv_file_object:      \t # Run through each row in the csv file,\n        data.append(row)             \t # adding each row to the data variable\n    print(type(data))\n    data = np.array(data) \t         \t # Then convert from a list to an array\n\t\t\t\t\t\t\t\t\t\t # Be aware that each item is currently\n\t\t\t\t\t\t\t\t\t\t # a string in this format\n    \n    number_passengers = np.size(data[0::,1].astype(np.float))\n    number_survived = np.sum(data[0::,1].astype(np.float))\n    proportion_survivors = number_survived / number_passengers\n    \n    women_only_stats = data[0::,4] == \"female\" # This finds where all \n                                               # the elements in the gender\n                                               # column that equals “female”\n    men_only_stats = data[0::,4] != \"female\"   # This finds where all the \n                                               # elements do not equal \n                                               # female (i.e. male)\n                                               \n    # Using the index from above we select the females and males separately\n    women_onboard = data[women_only_stats,1].astype(np.float)     \n    men_onboard = data[men_only_stats,1].astype(np.float)\n    \n    # Then we finds the proportions of them that survived\n    proportion_women_survived = \\\n                           np.sum(women_onboard) / np.size(women_onboard)  \n    proportion_men_survived = \\\n                           np.sum(men_onboard) / np.size(men_onboard) \n    \n    # and then print it out\n    print ('Proportion of women who survived is %s' % proportion_women_survived)\n    print ('Proportion of men who survived is %s' % proportion_men_survived)\n    \n    \n    \n    \n    # The script will systematically will loop through each combination \n    # and use the 'where' function in python to search the passengers that fit that combination of variables. \n    # Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. \n    # The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  \n    # For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. \n    # So then you can set the bins\n    \n    # So we add a ceiling\n    fare_ceiling = 40\n    \n    # then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\n    data[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n    \n    fare_bracket_size = 10\n    number_of_price_brackets = fare_ceiling // fare_bracket_size\n    \n    # Take the length of an array of unique values in column index 2\n    number_of_classes = len(np.unique(data[0::,2]))\n    \n    number_of_age_brackets=8 \n    \n    # Initialize the survival table with all zeros\n    survival_table = np.zeros((2, number_of_classes, \n    \t\t\t\t\t\t   number_of_price_brackets,\n    \t\t\t\t\t\t   number_of_age_brackets))\n    \n    \n    \n    #Now that these are set up, \n    #you can loop through each variable \n    #and find all those passengers that agree with the statements\n    \n    for i in range(number_of_classes):       \t\t#loop through each class\n      for j in range(number_of_price_brackets):   \t#loop through each price bin\n        for k in range(number_of_age_brackets):     #loop through each age bin\n            women_only_stats_plus = data[                 #Which element           \n                            (data[0::,4] == \"female\")     #is a female\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                        \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)#the next bin\n                           \n                              , 1]                        #in the 2nd col                           \n     \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t\n    \n            men_only_stats_plus = data[                   #Which element           \n                             (data[0::,4] != \"female\")    #is a male\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                                       \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)#the next bin\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)\n                              , 1]\n                              \n            survival_table[0,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) \n            survival_table[1,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))\n    \t\t\n    \t\t#if nan then the type will change to string from float so this sentence can set nan to 0. \n            survival_table[ survival_table != survival_table ] = 0.\n    \n    #Notice that  data[ where function, 1]  means \n    #it is finding the Survived column for the conditional criteria which is being called. \n    #As the loop starts with i=0 and j=0, \n    #the first loop will return the Survived values for all the 1st-class females (i + 1) \n    #who paid less than 10 ((j+1)*fare_bracket_size) \n    #and similarly all the 1st-class males who paid less than 10.  \n    #Before resetting to the top of the loop, \n    #we can calculate the proportion of survivors for this particular \n    #combination of criteria and record it to our survival table\n    \n        \n    #官方示例中将概率大于0.5的视为生还，这里我们略过\n    #直接打印详细概率\n    #survival_table[ survival_table < 0.5 ] = 0\n    #survival_table[ survival_table >= 0.5 ] = 1 \n        \n        \n    #Then we can make the prediction\n    \n    for row in test_file_object:                  # We are going to loop\n                                                  # through each passenger\n                                                  # in the test set                     \n      for j in range(number_of_price_brackets):   # For each passenger we\n                                                  # loop thro each price bin\n        try:                                      # Some passengers have no\n                                                  # Fare data so try to make\n          row[8] = float(row[8])                  # a float\n        except:                                   # If fails: no data, so \n          bin_fare = 3 - float(row[1])            # bin the fare according Pclass\n          break                                   # Break from the loop\n        if row[8] > fare_ceiling:              \t  # If there is data see if\n                                                  # it is greater than fare\n                                                  # ceiling we set earlier\n          bin_fare = number_of_price_brackets-1   # If so set to highest bin\n          break                                   # And then break loop\n        if row[8] >= j * fare_bracket_size\\\n           and row[8] < \\\n           (j+1) * fare_bracket_size:             # If passed these tests \n                                                  # then loop through each bin \n          bin_fare = j                            # then assign index\n          break\n      \n      for j in range(number_of_age_brackets): \n                                                 \n        try:                                    \n                                                \n          row[4] = float(row[4])              \n        except:                                   \n          bin_age = -1      \n          break                                  \n                                   \n        if row[4] >= j * 10\\\n           and row[4] < \\\n           (j+1) * 10:             # If passed these tests \n                                   # then loop through each bin \n          bin_age = j              # then assign index\n          break\n      \n      if row[3] == 'female':       #If the passenger is female\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[0, int(row[1])-1, bin_fare,bin_age]*100)])\n      else:                        #else if male\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[1, int(row[1])-1, bin_fare,bin_age]*100)])\n         \n    # Close out the files.\n    test_file.close() \n    predictions_file.close()\n```\n\n\n***\n# 多元线性回归\n    \n之后买了西瓜书，我把这个例题改成了线性回归模型：\n假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，\n我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小\n用均方误差作为性能度量，均方误差是线性系数的函数\n对线性系数w求导，可以得到w最优解的闭式\n\n关键公式是\n\t** $$ w^*=(X^TX)^{-1}X^Ty $$ **\n\n-\tX:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，\n\t  假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)\n-\tw:线性系数\n-\ty:生还结果 $$ y=w^T*x $$\n\n写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220426082.JPG)\n\n代码如下\n\n```Python\n        train1=train.dropna(subset=(['Age']),axis=0)\n    vali1=vali.dropna(subset=(['Age']),axis=0)\n    \n    validata=np.array(vali1)\n    data=np.array(train1)\n    \n    fare_ceiling = 40\n    data[data[0::,9].astype(np.float)>=fare_ceiling,9] = fare_ceiling - 1.0\n    \n    train = np.column_stack((data[0::,9],data[0::,2],data[0::,5],data[0::,4]))\n    predict=np.column_stack((validata[0::,9],validata[0::,2],validata[0::,5],validata[0::,4]))\n    survive = np.column_stack((data[0::,1]))\n    \n    \n    for i in range(train.shape[0]):\n        if (train[i][3]=='male'):\n            train[i][3]=0.00\n        else:\n            train[i][3]=1.00\n    for i in range(predict.shape[0]):\n        if (predict[i][3]=='male'):\n            predict[i][3]=0.00\n        else:\n            predict[i][3]=1.00\n    \n    x0=np.ones((train.shape[0],1))\n    train=np.concatenate((train,x0),axis=1)\n    \n    x0=np.ones((predict.shape[0],1))\n    predict=np.concatenate((predict,x0),axis=1)\n    \n    print('raw data finish')\n    \n    survive=survive.T.astype(np.float)\n    traint=train.T.astype(np.float)\n    w0=traint.dot(train.astype(np.float))\n    w1=(np.linalg.inv(w0))  \n    w2=w1.dot(traint)\n    w=w2.dot(survive)  #w=(Xt*X)^-1*Xt*y\n    print('w calc finish')\n    \n    feature=['Fare','Pclass','Age','Sex','b']\n    for i in zip(feature,w):\n        print(i)\n    \n    \n    valipredict_file_object.writerow([\"PassengerName\", \"Actual Survived\",\"Predict Survived\",\"XO\"])\n    count=0.0\n    for i in range(predict.shape[0]):\n        temp=predict[i,0::].T.astype(float)\n        answer=temp.dot(w)\n        answer=answer[0]\n        if ((answer>0.5 and validata[i][1]==1) or (answer<0.5 and validata[i][1]==0)):\n            flag=\"Correct\"\n            count=count+1.0;\n        else:\n            flag=\"Error\"\n        valipredict_file_object.writerow([validata[i][3],validata[i][1],answer,flag])\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/predict.shape[0]*100))  \n```\n***\n# scikit-learn中的多元线性回归\n试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220441164.JPG)\n\n如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/093538631.JPG)\n\n```Python\n    train=train.dropna(subset=['Age','Embarked'],axis=0)\n    vali=vali.dropna(subset=(['Age','Embarked']),axis=0)\n    \n    train.loc[train[\"Sex\"]==\"male\",\"Sex\"]=0\n    train.loc[train[\"Sex\"]==\"female\",\"Sex\"]=1\n    train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    trainx=train.reindex(index=train.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    vali.loc[vali[\"Sex\"]==\"male\",\"Sex\"]=0\n    vali.loc[vali[\"Sex\"]==\"female\",\"Sex\"]=1\n    vali.loc[vali[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    vali.loc[vali[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    vali.loc[vali[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    vali1=vali.reindex(index=vali.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    survive=vali.reindex(index=vali.index[:],columns=['Survived'])\n    survive=np.array(survive)\n    \n    feature=['Age','Sex','Parch','Fare','Embarked','SibSp']\n    \n    trainy=train.reindex(index=train.index[:],columns=['Survived'])  \n    trainy=trainy.Survived\n    \n    X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=1)\n    \n    \n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    print(model)\n    \n    \n    for i in zip(feature,model.coef_):\n        print(i)\n    \n    predict=model.predict(vali1)\n    \n    count=0\n    for i in range(len(predict)):\n        if (predict[i]>1 and survive[i] == 1) or  (predict[i]<1 and survive [i]== 0 ):\n            count=count+1.0\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/len(predict)*100))\n```","slug":"TitanicLinearRegression","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072cp000oqcw65ode1hfz","content":"<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-15/62213897-file_1484471472044_c2cc.jpg\" alt=\"\"></p><hr><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p><p>2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression</p><p>题目介绍在这：<a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener\">Titanic: Machine Learning from Disaster</a></p><p>下面是数据集表格样式，每个人有12个属性</p><a id=\"more\"></a><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG\" alt=\"mark\"></p><hr><h1 id=\"不是算法的算法\"><a href=\"#不是算法的算法\" class=\"headerlink\" title=\"不是算法的算法\"></a>不是算法的算法</h1><p>官方示例就是按几个属性分类，比如年龄，性别，票价(…..)<br>然后对每个属性内所有人的生还数据（0或者1）加一起求平均。<br>英文注释都是官方文档的说明<br>我就当入门教程学了，也全打了上去<br>代码如下：<br></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">Created on Sun Oct 30 15:28:22 2016</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@author: thinkwee</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> glue <span class=\"keyword\">import</span> qglue</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_file=(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">test_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">testheader = next(test_file_object)</span><br><span class=\"line\">predictions_file = open(<span class=\"string\">r\"文件目录略\"</span>, <span class=\"string\">\"w\"</span>)</span><br><span class=\"line\">predictions_file_object = csv.writer(predictions_file)</span><br><span class=\"line\">p = csv.writer(predictions_file)</span><br><span class=\"line\">p.writerow([<span class=\"string\">\"PassengerId\"</span>, <span class=\"string\">\"Survived\"</span>])</span><br><span class=\"line\">csv_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>)) </span><br><span class=\"line\">trainheader = next(csv_file_object)  <span class=\"comment\"># The next() command just skips the </span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># first line which is a header</span></span><br><span class=\"line\">data=[]                          \t <span class=\"comment\"># Create a variable called 'data'.</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> csv_file_object:      \t <span class=\"comment\"># Run through each row in the csv file,</span></span><br><span class=\"line\">    data.append(row)             \t <span class=\"comment\"># adding each row to the data variable</span></span><br><span class=\"line\">print(type(data))</span><br><span class=\"line\">data = np.array(data) \t         \t <span class=\"comment\"># Then convert from a list to an array</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># Be aware that each item is currently</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># a string in this format</span></span><br><span class=\"line\"></span><br><span class=\"line\">number_passengers = np.size(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">number_survived = np.sum(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">proportion_survivors = number_survived / number_passengers</span><br><span class=\"line\"></span><br><span class=\"line\">women_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span> <span class=\"comment\"># This finds where all </span></span><br><span class=\"line\">                                           <span class=\"comment\"># the elements in the gender</span></span><br><span class=\"line\">                                           <span class=\"comment\"># column that equals “female”</span></span><br><span class=\"line\">men_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>   <span class=\"comment\"># This finds where all the </span></span><br><span class=\"line\">                                           <span class=\"comment\"># elements do not equal </span></span><br><span class=\"line\">                                           <span class=\"comment\"># female (i.e. male)</span></span><br><span class=\"line\">                                           </span><br><span class=\"line\"><span class=\"comment\"># Using the index from above we select the females and males separately</span></span><br><span class=\"line\">women_onboard = data[women_only_stats,<span class=\"number\">1</span>].astype(np.float)     </span><br><span class=\"line\">men_onboard = data[men_only_stats,<span class=\"number\">1</span>].astype(np.float)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Then we finds the proportions of them that survived</span></span><br><span class=\"line\">proportion_women_survived = \\</span><br><span class=\"line\">                       np.sum(women_onboard) / np.size(women_onboard)  </span><br><span class=\"line\">proportion_men_survived = \\</span><br><span class=\"line\">                       np.sum(men_onboard) / np.size(men_onboard) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># and then print it out</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of women who survived is %s'</span> % proportion_women_survived)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of men who survived is %s'</span> % proportion_men_survived)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The script will systematically will loop through each combination </span></span><br><span class=\"line\"><span class=\"comment\"># and use the 'where' function in python to search the passengers that fit that combination of variables. </span></span><br><span class=\"line\"><span class=\"comment\"># Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. </span></span><br><span class=\"line\"><span class=\"comment\"># The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  </span></span><br><span class=\"line\"><span class=\"comment\"># For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. </span></span><br><span class=\"line\"><span class=\"comment\"># So then you can set the bins</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># So we add a ceiling</span></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling</span></span><br><span class=\"line\">data[ data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float) &gt;= fare_ceiling, <span class=\"number\">9</span> ] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">fare_bracket_size = <span class=\"number\">10</span></span><br><span class=\"line\">number_of_price_brackets = fare_ceiling // fare_bracket_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Take the length of an array of unique values in column index 2</span></span><br><span class=\"line\">number_of_classes = len(np.unique(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">number_of_age_brackets=<span class=\"number\">8</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the survival table with all zeros</span></span><br><span class=\"line\">survival_table = np.zeros((<span class=\"number\">2</span>, number_of_classes, </span><br><span class=\"line\">\t\t\t\t\t\t   number_of_price_brackets,</span><br><span class=\"line\">\t\t\t\t\t\t   number_of_age_brackets))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Now that these are set up, </span></span><br><span class=\"line\"><span class=\"comment\">#you can loop through each variable </span></span><br><span class=\"line\"><span class=\"comment\">#and find all those passengers that agree with the statements</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(number_of_classes):       \t\t<span class=\"comment\">#loop through each class</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   \t<span class=\"comment\">#loop through each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(number_of_age_brackets):     <span class=\"comment\">#loop through each age bin</span></span><br><span class=\"line\">        women_only_stats_plus = data[                 <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                        (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span>)     <span class=\"comment\">#is a female</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                        </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       </span><br><span class=\"line\">                          , <span class=\"number\">1</span>]                        <span class=\"comment\">#in the 2nd col                           </span></span><br><span class=\"line\"> \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">        men_only_stats_plus = data[                   <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                         (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>)    <span class=\"comment\">#is a male</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                                       </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)</span><br><span class=\"line\">                          , <span class=\"number\">1</span>]</span><br><span class=\"line\">                          </span><br><span class=\"line\">        survival_table[<span class=\"number\">0</span>,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) </span><br><span class=\"line\">        survival_table[<span class=\"number\">1</span>,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">#if nan then the type will change to string from float so this sentence can set nan to 0. </span></span><br><span class=\"line\">        survival_table[ survival_table != survival_table ] = <span class=\"number\">0.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Notice that  data[ where function, 1]  means </span></span><br><span class=\"line\"><span class=\"comment\">#it is finding the Survived column for the conditional criteria which is being called. </span></span><br><span class=\"line\"><span class=\"comment\">#As the loop starts with i=0 and j=0, </span></span><br><span class=\"line\"><span class=\"comment\">#the first loop will return the Survived values for all the 1st-class females (i + 1) </span></span><br><span class=\"line\"><span class=\"comment\">#who paid less than 10 ((j+1)*fare_bracket_size) </span></span><br><span class=\"line\"><span class=\"comment\">#and similarly all the 1st-class males who paid less than 10.  </span></span><br><span class=\"line\"><span class=\"comment\">#Before resetting to the top of the loop, </span></span><br><span class=\"line\"><span class=\"comment\">#we can calculate the proportion of survivors for this particular </span></span><br><span class=\"line\"><span class=\"comment\">#combination of criteria and record it to our survival table</span></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#官方示例中将概率大于0.5的视为生还，这里我们略过</span></span><br><span class=\"line\"><span class=\"comment\">#直接打印详细概率</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &lt; 0.5 ] = 0</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &gt;= 0.5 ] = 1 </span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#Then we can make the prediction</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> test_file_object:                  <span class=\"comment\"># We are going to loop</span></span><br><span class=\"line\">                                              <span class=\"comment\"># through each passenger</span></span><br><span class=\"line\">                                              <span class=\"comment\"># in the test set                     </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   <span class=\"comment\"># For each passenger we</span></span><br><span class=\"line\">                                              <span class=\"comment\"># loop thro each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                      <span class=\"comment\"># Some passengers have no</span></span><br><span class=\"line\">                                              <span class=\"comment\"># Fare data so try to make</span></span><br><span class=\"line\">      row[<span class=\"number\">8</span>] = float(row[<span class=\"number\">8</span>])                  <span class=\"comment\"># a float</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   <span class=\"comment\"># If fails: no data, so </span></span><br><span class=\"line\">      bin_fare = <span class=\"number\">3</span> - float(row[<span class=\"number\">1</span>])            <span class=\"comment\"># bin the fare according Pclass</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># Break from the loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt; fare_ceiling:              \t  <span class=\"comment\"># If there is data see if</span></span><br><span class=\"line\">                                              <span class=\"comment\"># it is greater than fare</span></span><br><span class=\"line\">                                              <span class=\"comment\"># ceiling we set earlier</span></span><br><span class=\"line\">      bin_fare = number_of_price_brackets<span class=\"number\">-1</span>   <span class=\"comment\"># If so set to highest bin</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># And then break loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt;= j * fare_bracket_size\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">8</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * fare_bracket_size:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                                              <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_fare = j                            <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_age_brackets): </span><br><span class=\"line\">                                             </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                    </span><br><span class=\"line\">                                            </span><br><span class=\"line\">      row[<span class=\"number\">4</span>] = float(row[<span class=\"number\">4</span>])              </span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   </span><br><span class=\"line\">      bin_age = <span class=\"number\">-1</span>      </span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                  </span><br><span class=\"line\">                               </span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">4</span>] &gt;= j * <span class=\"number\">10</span>\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">4</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * <span class=\"number\">10</span>:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                               <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_age = j              <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> row[<span class=\"number\">3</span>] == <span class=\"string\">'female'</span>:       <span class=\"comment\">#If the passenger is female</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">0</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">else</span>:                        <span class=\"comment\">#else if male</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">1</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"comment\"># Close out the files.</span></span><br><span class=\"line\">test_file.close() </span><br><span class=\"line\">predictions_file.close()</span><br></pre></td></tr></table></figure><p></p><hr><h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>之后买了西瓜书，我把这个例题改成了线性回归模型：<br>假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，<br>我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小<br>用均方误差作为性能度量，均方误差是线性系数的函数<br>对线性系数w求导，可以得到w最优解的闭式</p><p>关键公式是<br><strong>$$ w^*=(X^TX)^{-1}X^Ty $$</strong></p><ul><li>X:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，<br>假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)</li><li>w:线性系数</li><li>y:生还结果 $$ y=w^T*x $$</li></ul><p>写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220426082.JPG\" alt=\"mark\"></p><p>代码如下</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    train1=train.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali1=vali.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">validata=np.array(vali1)</span><br><span class=\"line\">data=np.array(train1)</span><br><span class=\"line\"></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\">data[data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float)&gt;=fare_ceiling,<span class=\"number\">9</span>] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">train = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">predict=np.column_stack((validata[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">survive = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(train.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (train[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((train.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">train=np.concatenate((train,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((predict.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">predict=np.concatenate((predict,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'raw data finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">survive=survive.T.astype(np.float)</span><br><span class=\"line\">traint=train.T.astype(np.float)</span><br><span class=\"line\">w0=traint.dot(train.astype(np.float))</span><br><span class=\"line\">w1=(np.linalg.inv(w0))  </span><br><span class=\"line\">w2=w1.dot(traint)</span><br><span class=\"line\">w=w2.dot(survive)  <span class=\"comment\">#w=(Xt*X)^-1*Xt*y</span></span><br><span class=\"line\">print(<span class=\"string\">'w calc finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Fare'</span>,<span class=\"string\">'Pclass'</span>,<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'b'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,w):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">valipredict_file_object.writerow([<span class=\"string\">\"PassengerName\"</span>, <span class=\"string\">\"Actual Survived\"</span>,<span class=\"string\">\"Predict Survived\"</span>,<span class=\"string\">\"XO\"</span>])</span><br><span class=\"line\">count=<span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    temp=predict[i,<span class=\"number\">0</span>::].T.astype(float)</span><br><span class=\"line\">    answer=temp.dot(w)</span><br><span class=\"line\">    answer=answer[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((answer&gt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">1</span>) <span class=\"keyword\">or</span> (answer&lt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">0</span>)):</span><br><span class=\"line\">        flag=<span class=\"string\">\"Correct\"</span></span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        flag=<span class=\"string\">\"Error\"</span></span><br><span class=\"line\">    valipredict_file_object.writerow([validata[i][<span class=\"number\">3</span>],validata[i][<span class=\"number\">1</span>],answer,flag])</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/predict.shape[<span class=\"number\">0</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure><hr><h1 id=\"scikit-learn中的多元线性回归\"><a href=\"#scikit-learn中的多元线性回归\" class=\"headerlink\" title=\"scikit-learn中的多元线性回归\"></a>scikit-learn中的多元线性回归</h1><p>试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220441164.JPG\" alt=\"mark\"></p><p>如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/093538631.JPG\" alt=\"mark\"></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train=train.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>],axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali=vali.dropna(subset=([<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">trainx=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">vali1=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">survive=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Survived'</span>])</span><br><span class=\"line\">survive=np.array(survive)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'Parch'</span>,<span class=\"string\">'Fare'</span>,<span class=\"string\">'Embarked'</span>,<span class=\"string\">'SibSp'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">trainy=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Survived'</span>])  </span><br><span class=\"line\">trainy=trainy.Survived</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(X_train,y_train)</span><br><span class=\"line\">print(model)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,model.coef_):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\">predict=model.predict(vali1)</span><br><span class=\"line\"></span><br><span class=\"line\">count=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(predict)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i]&gt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive[i] == <span class=\"number\">1</span>) <span class=\"keyword\">or</span>  (predict[i]&lt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive [i]== <span class=\"number\">0</span> ):</span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/len(predict)*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-15/62213897-file_1484471472044_c2cc.jpg\" alt=\"\"></p><hr><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p><p>2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression</p><p>题目介绍在这：<a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener\">Titanic: Machine Learning from Disaster</a></p><p>下面是数据集表格样式，每个人有12个属性</p>","more":"<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG\" alt=\"mark\"></p><hr><h1 id=\"不是算法的算法\"><a href=\"#不是算法的算法\" class=\"headerlink\" title=\"不是算法的算法\"></a>不是算法的算法</h1><p>官方示例就是按几个属性分类，比如年龄，性别，票价(…..)<br>然后对每个属性内所有人的生还数据（0或者1）加一起求平均。<br>英文注释都是官方文档的说明<br>我就当入门教程学了，也全打了上去<br>代码如下：<br></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">Created on Sun Oct 30 15:28:22 2016</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@author: thinkwee</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> glue <span class=\"keyword\">import</span> qglue</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_file=(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">test_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">testheader = next(test_file_object)</span><br><span class=\"line\">predictions_file = open(<span class=\"string\">r\"文件目录略\"</span>, <span class=\"string\">\"w\"</span>)</span><br><span class=\"line\">predictions_file_object = csv.writer(predictions_file)</span><br><span class=\"line\">p = csv.writer(predictions_file)</span><br><span class=\"line\">p.writerow([<span class=\"string\">\"PassengerId\"</span>, <span class=\"string\">\"Survived\"</span>])</span><br><span class=\"line\">csv_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>)) </span><br><span class=\"line\">trainheader = next(csv_file_object)  <span class=\"comment\"># The next() command just skips the </span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># first line which is a header</span></span><br><span class=\"line\">data=[]                          \t <span class=\"comment\"># Create a variable called 'data'.</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> csv_file_object:      \t <span class=\"comment\"># Run through each row in the csv file,</span></span><br><span class=\"line\">    data.append(row)             \t <span class=\"comment\"># adding each row to the data variable</span></span><br><span class=\"line\">print(type(data))</span><br><span class=\"line\">data = np.array(data) \t         \t <span class=\"comment\"># Then convert from a list to an array</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># Be aware that each item is currently</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># a string in this format</span></span><br><span class=\"line\"></span><br><span class=\"line\">number_passengers = np.size(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">number_survived = np.sum(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">proportion_survivors = number_survived / number_passengers</span><br><span class=\"line\"></span><br><span class=\"line\">women_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span> <span class=\"comment\"># This finds where all </span></span><br><span class=\"line\">                                           <span class=\"comment\"># the elements in the gender</span></span><br><span class=\"line\">                                           <span class=\"comment\"># column that equals “female”</span></span><br><span class=\"line\">men_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>   <span class=\"comment\"># This finds where all the </span></span><br><span class=\"line\">                                           <span class=\"comment\"># elements do not equal </span></span><br><span class=\"line\">                                           <span class=\"comment\"># female (i.e. male)</span></span><br><span class=\"line\">                                           </span><br><span class=\"line\"><span class=\"comment\"># Using the index from above we select the females and males separately</span></span><br><span class=\"line\">women_onboard = data[women_only_stats,<span class=\"number\">1</span>].astype(np.float)     </span><br><span class=\"line\">men_onboard = data[men_only_stats,<span class=\"number\">1</span>].astype(np.float)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Then we finds the proportions of them that survived</span></span><br><span class=\"line\">proportion_women_survived = \\</span><br><span class=\"line\">                       np.sum(women_onboard) / np.size(women_onboard)  </span><br><span class=\"line\">proportion_men_survived = \\</span><br><span class=\"line\">                       np.sum(men_onboard) / np.size(men_onboard) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># and then print it out</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of women who survived is %s'</span> % proportion_women_survived)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of men who survived is %s'</span> % proportion_men_survived)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The script will systematically will loop through each combination </span></span><br><span class=\"line\"><span class=\"comment\"># and use the 'where' function in python to search the passengers that fit that combination of variables. </span></span><br><span class=\"line\"><span class=\"comment\"># Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. </span></span><br><span class=\"line\"><span class=\"comment\"># The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  </span></span><br><span class=\"line\"><span class=\"comment\"># For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. </span></span><br><span class=\"line\"><span class=\"comment\"># So then you can set the bins</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># So we add a ceiling</span></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling</span></span><br><span class=\"line\">data[ data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float) &gt;= fare_ceiling, <span class=\"number\">9</span> ] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">fare_bracket_size = <span class=\"number\">10</span></span><br><span class=\"line\">number_of_price_brackets = fare_ceiling // fare_bracket_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Take the length of an array of unique values in column index 2</span></span><br><span class=\"line\">number_of_classes = len(np.unique(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">number_of_age_brackets=<span class=\"number\">8</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the survival table with all zeros</span></span><br><span class=\"line\">survival_table = np.zeros((<span class=\"number\">2</span>, number_of_classes, </span><br><span class=\"line\">\t\t\t\t\t\t   number_of_price_brackets,</span><br><span class=\"line\">\t\t\t\t\t\t   number_of_age_brackets))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Now that these are set up, </span></span><br><span class=\"line\"><span class=\"comment\">#you can loop through each variable </span></span><br><span class=\"line\"><span class=\"comment\">#and find all those passengers that agree with the statements</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(number_of_classes):       \t\t<span class=\"comment\">#loop through each class</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   \t<span class=\"comment\">#loop through each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(number_of_age_brackets):     <span class=\"comment\">#loop through each age bin</span></span><br><span class=\"line\">        women_only_stats_plus = data[                 <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                        (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span>)     <span class=\"comment\">#is a female</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                        </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       </span><br><span class=\"line\">                          , <span class=\"number\">1</span>]                        <span class=\"comment\">#in the 2nd col                           </span></span><br><span class=\"line\"> \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">        men_only_stats_plus = data[                   <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                         (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>)    <span class=\"comment\">#is a male</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                                       </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)</span><br><span class=\"line\">                          , <span class=\"number\">1</span>]</span><br><span class=\"line\">                          </span><br><span class=\"line\">        survival_table[<span class=\"number\">0</span>,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) </span><br><span class=\"line\">        survival_table[<span class=\"number\">1</span>,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">#if nan then the type will change to string from float so this sentence can set nan to 0. </span></span><br><span class=\"line\">        survival_table[ survival_table != survival_table ] = <span class=\"number\">0.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Notice that  data[ where function, 1]  means </span></span><br><span class=\"line\"><span class=\"comment\">#it is finding the Survived column for the conditional criteria which is being called. </span></span><br><span class=\"line\"><span class=\"comment\">#As the loop starts with i=0 and j=0, </span></span><br><span class=\"line\"><span class=\"comment\">#the first loop will return the Survived values for all the 1st-class females (i + 1) </span></span><br><span class=\"line\"><span class=\"comment\">#who paid less than 10 ((j+1)*fare_bracket_size) </span></span><br><span class=\"line\"><span class=\"comment\">#and similarly all the 1st-class males who paid less than 10.  </span></span><br><span class=\"line\"><span class=\"comment\">#Before resetting to the top of the loop, </span></span><br><span class=\"line\"><span class=\"comment\">#we can calculate the proportion of survivors for this particular </span></span><br><span class=\"line\"><span class=\"comment\">#combination of criteria and record it to our survival table</span></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#官方示例中将概率大于0.5的视为生还，这里我们略过</span></span><br><span class=\"line\"><span class=\"comment\">#直接打印详细概率</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &lt; 0.5 ] = 0</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &gt;= 0.5 ] = 1 </span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#Then we can make the prediction</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> test_file_object:                  <span class=\"comment\"># We are going to loop</span></span><br><span class=\"line\">                                              <span class=\"comment\"># through each passenger</span></span><br><span class=\"line\">                                              <span class=\"comment\"># in the test set                     </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   <span class=\"comment\"># For each passenger we</span></span><br><span class=\"line\">                                              <span class=\"comment\"># loop thro each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                      <span class=\"comment\"># Some passengers have no</span></span><br><span class=\"line\">                                              <span class=\"comment\"># Fare data so try to make</span></span><br><span class=\"line\">      row[<span class=\"number\">8</span>] = float(row[<span class=\"number\">8</span>])                  <span class=\"comment\"># a float</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   <span class=\"comment\"># If fails: no data, so </span></span><br><span class=\"line\">      bin_fare = <span class=\"number\">3</span> - float(row[<span class=\"number\">1</span>])            <span class=\"comment\"># bin the fare according Pclass</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># Break from the loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt; fare_ceiling:              \t  <span class=\"comment\"># If there is data see if</span></span><br><span class=\"line\">                                              <span class=\"comment\"># it is greater than fare</span></span><br><span class=\"line\">                                              <span class=\"comment\"># ceiling we set earlier</span></span><br><span class=\"line\">      bin_fare = number_of_price_brackets<span class=\"number\">-1</span>   <span class=\"comment\"># If so set to highest bin</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># And then break loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt;= j * fare_bracket_size\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">8</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * fare_bracket_size:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                                              <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_fare = j                            <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_age_brackets): </span><br><span class=\"line\">                                             </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                    </span><br><span class=\"line\">                                            </span><br><span class=\"line\">      row[<span class=\"number\">4</span>] = float(row[<span class=\"number\">4</span>])              </span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   </span><br><span class=\"line\">      bin_age = <span class=\"number\">-1</span>      </span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                  </span><br><span class=\"line\">                               </span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">4</span>] &gt;= j * <span class=\"number\">10</span>\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">4</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * <span class=\"number\">10</span>:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                               <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_age = j              <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> row[<span class=\"number\">3</span>] == <span class=\"string\">'female'</span>:       <span class=\"comment\">#If the passenger is female</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">0</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">else</span>:                        <span class=\"comment\">#else if male</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">1</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"comment\"># Close out the files.</span></span><br><span class=\"line\">test_file.close() </span><br><span class=\"line\">predictions_file.close()</span><br></pre></td></tr></table></figure><p></p><hr><h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>之后买了西瓜书，我把这个例题改成了线性回归模型：<br>假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，<br>我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小<br>用均方误差作为性能度量，均方误差是线性系数的函数<br>对线性系数w求导，可以得到w最优解的闭式</p><p>关键公式是<br><strong>$$ w^*=(X^TX)^{-1}X^Ty $$</strong></p><ul><li>X:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，<br>假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)</li><li>w:线性系数</li><li>y:生还结果 $$ y=w^T*x $$</li></ul><p>写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220426082.JPG\" alt=\"mark\"></p><p>代码如下</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    train1=train.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali1=vali.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">validata=np.array(vali1)</span><br><span class=\"line\">data=np.array(train1)</span><br><span class=\"line\"></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\">data[data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float)&gt;=fare_ceiling,<span class=\"number\">9</span>] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">train = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">predict=np.column_stack((validata[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">survive = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(train.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (train[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((train.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">train=np.concatenate((train,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((predict.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">predict=np.concatenate((predict,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'raw data finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">survive=survive.T.astype(np.float)</span><br><span class=\"line\">traint=train.T.astype(np.float)</span><br><span class=\"line\">w0=traint.dot(train.astype(np.float))</span><br><span class=\"line\">w1=(np.linalg.inv(w0))  </span><br><span class=\"line\">w2=w1.dot(traint)</span><br><span class=\"line\">w=w2.dot(survive)  <span class=\"comment\">#w=(Xt*X)^-1*Xt*y</span></span><br><span class=\"line\">print(<span class=\"string\">'w calc finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Fare'</span>,<span class=\"string\">'Pclass'</span>,<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'b'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,w):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">valipredict_file_object.writerow([<span class=\"string\">\"PassengerName\"</span>, <span class=\"string\">\"Actual Survived\"</span>,<span class=\"string\">\"Predict Survived\"</span>,<span class=\"string\">\"XO\"</span>])</span><br><span class=\"line\">count=<span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    temp=predict[i,<span class=\"number\">0</span>::].T.astype(float)</span><br><span class=\"line\">    answer=temp.dot(w)</span><br><span class=\"line\">    answer=answer[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((answer&gt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">1</span>) <span class=\"keyword\">or</span> (answer&lt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">0</span>)):</span><br><span class=\"line\">        flag=<span class=\"string\">\"Correct\"</span></span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        flag=<span class=\"string\">\"Error\"</span></span><br><span class=\"line\">    valipredict_file_object.writerow([validata[i][<span class=\"number\">3</span>],validata[i][<span class=\"number\">1</span>],answer,flag])</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/predict.shape[<span class=\"number\">0</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure><hr><h1 id=\"scikit-learn中的多元线性回归\"><a href=\"#scikit-learn中的多元线性回归\" class=\"headerlink\" title=\"scikit-learn中的多元线性回归\"></a>scikit-learn中的多元线性回归</h1><p>试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170207/220441164.JPG\" alt=\"mark\"></p><p>如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览</p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170210/093538631.JPG\" alt=\"mark\"></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train=train.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>],axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali=vali.dropna(subset=([<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">trainx=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">vali1=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">survive=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Survived'</span>])</span><br><span class=\"line\">survive=np.array(survive)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'Parch'</span>,<span class=\"string\">'Fare'</span>,<span class=\"string\">'Embarked'</span>,<span class=\"string\">'SibSp'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">trainy=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Survived'</span>])  </span><br><span class=\"line\">trainy=trainy.Survived</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(X_train,y_train)</span><br><span class=\"line\">print(model)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,model.coef_):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\">predict=model.predict(vali1)</span><br><span class=\"line\"></span><br><span class=\"line\">count=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(predict)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i]&gt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive[i] == <span class=\"number\">1</span>) <span class=\"keyword\">or</span>  (predict[i]&lt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive [i]== <span class=\"number\">0</span> ):</span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/len(predict)*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"机器学习入门:Coding基础与线性回归","path":"2017/02/07/TitanicLinearRegression/","eyeCatchImage":null,"excerpt":"<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-15/62213897-file_1484471472044_c2cc.jpg\" alt=\"\"></p><hr><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p><p>2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression</p><p>题目介绍在这：<a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener\">Titanic: Machine Learning from Disaster</a></p><p>下面是数据集表格样式，每个人有12个属性</p>","date":"2017-02-07T13:57:22.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"论文阅读笔记2018下半年待填坑","date":"2018-07-03T07:18:52.000Z","author":"Thinkwee","mathjax":true,"html":true,"_content":"\n下半年待填坑列表\n主要关注图表示学习、文本生成和文本摘要、注意力机制、一些数学基础和模型的深入理解。\n好像又有一大批贝叶斯网络的坑要来了。\n读论文是不可能读完的，这辈子都不可能读完的。\n有时间再补上阅读笔记。\n\n<!--more-->\n\n# TODO列表\n- [ ]\tself-critical sequence training for image captioning\n- [ ]\trecurrent highway networks\n- [ ]\trecurrent convolutional neural networks for scene labeling\n- [ ]\tneural word embedding as implicit matrix factorization\n- [ ]\thierarchically-attentive rnn for album summarization and storytelling\n- [ ]\tlearning phrase representations using rnn encoder-decoder for statistical machine translation\n- [x]\tneural headline generation with sentence-wise optimization\n- [x]\tlda数学八卦\n- [ ]\tincorporating copying mechanism in sequence-to-sequence learning\n- [ ]\tgenerating sentences from a continuous space\n- [x]\tgenerating news headlines with recurrent neural networks\n- [ ]\tcalculus on computational graphs: backpropagation\n- [ ]\tconvolutional sequence to sequence learning\n- [ ]\tcutting-off redundant repeating generations for neural abstractive summarization\n- [ ]\tattention and augmented recurrent neural networks\n- [x]\tattention is all you need\n- [ ]\tadmissible stopping in viterbi beam search for unit selection in concatenative speech synthesis\n- [ ]\tabstractive document summarization with a graph-based attentional neural model\n- [ ]\taddressing the rare word problem in neural machine translation\n- [ ]\tabstractive sentence summarization with attentive recurrent neural networks\n- [ ]\tunsupervised machine translation using monolingual corpora only\n- [x]\tthe nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies\n- [ ] \tneural architecture search with reinforcement learning\n- [ ] \tan introduction to conditional random fields\n\n# 一些文摘方面的SOTA\n-\t结果从上到下依次是ROUGE-1,ROUGE-2,ROUGE-L\n-\t强调是pyrouge是因为相比原作者用Perl写的系统，pyrouge测试值偏高\n\n<table style=\"border-collapse:collapse;border-spacing:0;border-color:#ccc\" class=\"tg\"><tr><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">论文</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">日期</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">数据集</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">结果</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">是否用pyrouge</th></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">Deep Communicating Agents for Abstractive Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2018.8</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.69<br>19.47<br>37.92</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">48.08<br>31.19<br>42.33</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Document Modeling with External Attention for Sentence Extraction</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN news highlights</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">54.2<br>21.6<br>48.1</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Multi-Reward Reinforced Summarization with Saliency and Entailment</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.5</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.43<br>18.00<br>37.10</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Ranking Sentences for Extractive Summarization with Reinforcement Learning</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.4</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.0<br>18.2<br>36.6</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">A Deep Reinforced Model for Abstractive Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2017.11</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.16<br>15.82<br>39.08</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">47.22<br>30.72<br>43.27</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.8</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">38.1<br>13.9<br>34.0</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Get To The Point: Summarization with Pointer-Generator Networks</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.4</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.53<br>17.82<br>36.38</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.11</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.6<br>16.2<br>35.3</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Distraction-Based Neural Networks for Document Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.1</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">27.1<br>8.2<br>18.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Neural Summarization by Extracting Sentences and Words</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">500 DailyMail</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">21.2<br>8.3<br>12.0</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr></table>\n\n# Distraction-Based Neural Networks for Document Summarization\n-\t不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/H6IBk7f0bk.png?imageslim)\n-\t在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LjAaidg72e.png?imageslim)\n-\t这个控制层捕捉$s_t^{'}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。\n-\t三种注意力分散模型\n\t-\tM1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^'，减去了历史上下文得到，类似coverage机制\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/j3ijmk54kj.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/3dCkK5ie7F.png?imageslim)\n\t-\tM2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/Ei4m9iKA9D.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JbHf12J8gG.png?imageslim)\n\t-\tM3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/IJ3Li6AA3m.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/HIfge59Ji8.png?imageslim)\n\n# Document Modeling with External Attention for Sentence Extraction\n-\t构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。\n在文摘任务中，外部信息是图片配字和文档标题。\n-\t通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/I5GHHELAdb.png?imageslim)\n\n-\t句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。\n-\t文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。\n-\t句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{'}$：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k8hHm3A3CL.png?imageslim)\n\n# Get To The Point: Summarization with Pointer-Generator Networks\n-\t介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题\n-\tPointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k707Ael05k.png?imageslim)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/5idDIiI9ik.png?imageslim)\n\n-\t指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JhmIg667i7.png?imageslim)\n\n-\tCoverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力\n-\t普通注意力计算\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/gjFeh5GHm9.png?imageslim)\n\n-\t维护一个coverage向量，表示每个词在此之前获得了多少注意力:\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/deabGEiC00.png?imageslim)\n\n-\t然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/099iBDA88m.png?imageslim)\n\n-\t并在损失函数里加上coverage损失\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/36LFjHbch5.png?imageslim)\n\n-\t使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小\n\n# SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/F08I4Ll22f.png?imageslim)\n\n-\t用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法\n-\t将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。\n-\t用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/a77EbCH3Eg.png?imageslim)\n\n-\t其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码\n-\t之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/eG9Ka8G1lA.png?imageslim)\n\n-\t其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/f2Fg71Hjdg.png?imageslim)\n\n-\t第一行：参数为当前句子编码，表示当前句子的内容\n-\t第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性\n-\t第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）\n-\t第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）\n-\t最后对整个模型做最大似然估计:\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/KBeh4fEfIh.png?imageslim)\n\n-\t作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。\n-\t还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。\n-\t因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/J1C5aaef5j.png?imageslim)\n\n# Attention Is All You Need\n-\t抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/g1BhFElcff.png?imageslim)\n\n-\t完全使用attention的一个好处就是快。\n-\t为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512\n-\t编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。\n-\t解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。\n-\t编码与解码的6个块都是堆叠的(stack)，\n-\tAttention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/DbhA9j0iba.png?imageslim)\n\n-\tMulti-head attention由多个scaled dot-product attention并行组成。\n-\tScaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/mJFA1DJ8DC.png?imageslim)\n\n-\tMulti-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/0IBA23dbEJ.png?imageslim)\n\n-\t论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64\n-\t这种multi-head attention用在了模型的三个地方：\n\t-\t编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置\n\t-\t编码端块与块之间的自注意力\n\t-\t解码端块与块之间的自注意力\n-\t在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/AKjaJ4KLLj.png?imageslim)\n\n-\t完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LELGH5F9ge.png?imageslim)","source":"_posts/PaperReading2.md","raw":"---\ntitle: 论文阅读笔记2018下半年待填坑\ndate: 2018-07-03 15:18:52\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\nhtml: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180804/E1lIhI4li1.jpg?imageslim\n---\n\n下半年待填坑列表\n主要关注图表示学习、文本生成和文本摘要、注意力机制、一些数学基础和模型的深入理解。\n好像又有一大批贝叶斯网络的坑要来了。\n读论文是不可能读完的，这辈子都不可能读完的。\n有时间再补上阅读笔记。\n\n<!--more-->\n\n# TODO列表\n- [ ]\tself-critical sequence training for image captioning\n- [ ]\trecurrent highway networks\n- [ ]\trecurrent convolutional neural networks for scene labeling\n- [ ]\tneural word embedding as implicit matrix factorization\n- [ ]\thierarchically-attentive rnn for album summarization and storytelling\n- [ ]\tlearning phrase representations using rnn encoder-decoder for statistical machine translation\n- [x]\tneural headline generation with sentence-wise optimization\n- [x]\tlda数学八卦\n- [ ]\tincorporating copying mechanism in sequence-to-sequence learning\n- [ ]\tgenerating sentences from a continuous space\n- [x]\tgenerating news headlines with recurrent neural networks\n- [ ]\tcalculus on computational graphs: backpropagation\n- [ ]\tconvolutional sequence to sequence learning\n- [ ]\tcutting-off redundant repeating generations for neural abstractive summarization\n- [ ]\tattention and augmented recurrent neural networks\n- [x]\tattention is all you need\n- [ ]\tadmissible stopping in viterbi beam search for unit selection in concatenative speech synthesis\n- [ ]\tabstractive document summarization with a graph-based attentional neural model\n- [ ]\taddressing the rare word problem in neural machine translation\n- [ ]\tabstractive sentence summarization with attentive recurrent neural networks\n- [ ]\tunsupervised machine translation using monolingual corpora only\n- [x]\tthe nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies\n- [ ] \tneural architecture search with reinforcement learning\n- [ ] \tan introduction to conditional random fields\n\n# 一些文摘方面的SOTA\n-\t结果从上到下依次是ROUGE-1,ROUGE-2,ROUGE-L\n-\t强调是pyrouge是因为相比原作者用Perl写的系统，pyrouge测试值偏高\n\n<table style=\"border-collapse:collapse;border-spacing:0;border-color:#ccc\" class=\"tg\"><tr><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">论文</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">日期</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">数据集</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">结果</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">是否用pyrouge</th></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">Deep Communicating Agents for Abstractive Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2018.8</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.69<br>19.47<br>37.92</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">48.08<br>31.19<br>42.33</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Document Modeling with External Attention for Sentence Extraction</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN news highlights</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">54.2<br>21.6<br>48.1</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Multi-Reward Reinforced Summarization with Saliency and Entailment</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.5</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.43<br>18.00<br>37.10</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Ranking Sentences for Extractive Summarization with Reinforcement Learning</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.4</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.0<br>18.2<br>36.6</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">A Deep Reinforced Model for Abstractive Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2017.11</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.16<br>15.82<br>39.08</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">47.22<br>30.72<br>43.27</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.8</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">38.1<br>13.9<br>34.0</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Get To The Point: Summarization with Pointer-Generator Networks</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.4</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.53<br>17.82<br>36.38</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.11</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.6<br>16.2<br>35.3</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Distraction-Based Neural Networks for Document Summarization</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.1</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">27.1<br>8.2<br>18.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Neural Summarization by Extracting Sentences and Words</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.7</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">500 DailyMail</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">21.2<br>8.3<br>12.0</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr></table>\n\n# Distraction-Based Neural Networks for Document Summarization\n-\t不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/H6IBk7f0bk.png?imageslim)\n-\t在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LjAaidg72e.png?imageslim)\n-\t这个控制层捕捉$s_t^{'}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。\n-\t三种注意力分散模型\n\t-\tM1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^'，减去了历史上下文得到，类似coverage机制\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/j3ijmk54kj.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/3dCkK5ie7F.png?imageslim)\n\t-\tM2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/Ei4m9iKA9D.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JbHf12J8gG.png?imageslim)\n\t-\tM3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/IJ3Li6AA3m.png?imageslim)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/HIfge59Ji8.png?imageslim)\n\n# Document Modeling with External Attention for Sentence Extraction\n-\t构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。\n在文摘任务中，外部信息是图片配字和文档标题。\n-\t通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/I5GHHELAdb.png?imageslim)\n\n-\t句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。\n-\t文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。\n-\t句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{'}$：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k8hHm3A3CL.png?imageslim)\n\n# Get To The Point: Summarization with Pointer-Generator Networks\n-\t介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题\n-\tPointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k707Ael05k.png?imageslim)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/5idDIiI9ik.png?imageslim)\n\n-\t指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JhmIg667i7.png?imageslim)\n\n-\tCoverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力\n-\t普通注意力计算\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/gjFeh5GHm9.png?imageslim)\n\n-\t维护一个coverage向量，表示每个词在此之前获得了多少注意力:\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/deabGEiC00.png?imageslim)\n\n-\t然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/099iBDA88m.png?imageslim)\n\n-\t并在损失函数里加上coverage损失\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/36LFjHbch5.png?imageslim)\n\n-\t使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小\n\n# SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/F08I4Ll22f.png?imageslim)\n\n-\t用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法\n-\t将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。\n-\t用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/a77EbCH3Eg.png?imageslim)\n\n-\t其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码\n-\t之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/eG9Ka8G1lA.png?imageslim)\n\n-\t其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/f2Fg71Hjdg.png?imageslim)\n\n-\t第一行：参数为当前句子编码，表示当前句子的内容\n-\t第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性\n-\t第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）\n-\t第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）\n-\t最后对整个模型做最大似然估计:\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/KBeh4fEfIh.png?imageslim)\n\n-\t作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。\n-\t还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。\n-\t因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/J1C5aaef5j.png?imageslim)\n\n# Attention Is All You Need\n-\t抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/g1BhFElcff.png?imageslim)\n\n-\t完全使用attention的一个好处就是快。\n-\t为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512\n-\t编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。\n-\t解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。\n-\t编码与解码的6个块都是堆叠的(stack)，\n-\tAttention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/DbhA9j0iba.png?imageslim)\n\n-\tMulti-head attention由多个scaled dot-product attention并行组成。\n-\tScaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/mJFA1DJ8DC.png?imageslim)\n\n-\tMulti-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/0IBA23dbEJ.png?imageslim)\n\n-\t论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64\n-\t这种multi-head attention用在了模型的三个地方：\n\t-\t编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置\n\t-\t编码端块与块之间的自注意力\n\t-\t解码端块与块之间的自注意力\n-\t在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/AKjaJ4KLLj.png?imageslim)\n\n-\t完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LELGH5F9ge.png?imageslim)","slug":"PaperReading2","published":1,"updated":"2018-09-08T13:00:40.145Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180804/E1lIhI4li1.jpg?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072cr000sqcw6ptpwtvbc","content":"<p>下半年待填坑列表<br>主要关注图表示学习、文本生成和文本摘要、注意力机制、一些数学基础和模型的深入理解。<br>好像又有一大批贝叶斯网络的坑要来了。<br>读论文是不可能读完的，这辈子都不可能读完的。<br>有时间再补上阅读笔记。</p><a id=\"more\"></a><h1 id=\"TODO列表\"><a href=\"#TODO列表\" class=\"headerlink\" title=\"TODO列表\"></a>TODO列表</h1><ul><li style=\"list-style:none\"><input type=\"checkbox\">self-critical sequence training for image captioning</li><li style=\"list-style:none\"><input type=\"checkbox\">recurrent highway networks</li><li style=\"list-style:none\"><input type=\"checkbox\">recurrent convolutional neural networks for scene labeling</li><li style=\"list-style:none\"><input type=\"checkbox\">neural word embedding as implicit matrix factorization</li><li style=\"list-style:none\"><input type=\"checkbox\">hierarchically-attentive rnn for album summarization and storytelling</li><li style=\"list-style:none\"><input type=\"checkbox\">learning phrase representations using rnn encoder-decoder for statistical machine translation</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>neural headline generation with sentence-wise optimization</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>lda数学八卦</li><li style=\"list-style:none\"><input type=\"checkbox\">incorporating copying mechanism in sequence-to-sequence learning</li><li style=\"list-style:none\"><input type=\"checkbox\">generating sentences from a continuous space</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>generating news headlines with recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\">calculus on computational graphs: backpropagation</li><li style=\"list-style:none\"><input type=\"checkbox\">convolutional sequence to sequence learning</li><li style=\"list-style:none\"><input type=\"checkbox\">cutting-off redundant repeating generations for neural abstractive summarization</li><li style=\"list-style:none\"><input type=\"checkbox\">attention and augmented recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>attention is all you need</li><li style=\"list-style:none\"><input type=\"checkbox\">admissible stopping in viterbi beam search for unit selection in concatenative speech synthesis</li><li style=\"list-style:none\"><input type=\"checkbox\">abstractive document summarization with a graph-based attentional neural model</li><li style=\"list-style:none\"><input type=\"checkbox\">addressing the rare word problem in neural machine translation</li><li style=\"list-style:none\"><input type=\"checkbox\">abstractive sentence summarization with attentive recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\">unsupervised machine translation using monolingual corpora only</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>the nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</li><li style=\"list-style:none\"><input type=\"checkbox\">neural architecture search with reinforcement learning</li><li style=\"list-style:none\"><input type=\"checkbox\">an introduction to conditional random fields</li></ul><h1 id=\"一些文摘方面的SOTA\"><a href=\"#一些文摘方面的SOTA\" class=\"headerlink\" title=\"一些文摘方面的SOTA\"></a>一些文摘方面的SOTA</h1><ul><li>结果从上到下依次是ROUGE-1,ROUGE-2,ROUGE-L</li><li>强调是pyrouge是因为相比原作者用Perl写的系统，pyrouge测试值偏高</li></ul><table style=\"border-collapse:collapse;border-spacing:0;border-color:#ccc\" class=\"tg\"><tr><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">论文</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">日期</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">数据集</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">结果</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">是否用pyrouge</th></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">Deep Communicating Agents for Abstractive Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2018.8</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.69<br>19.47<br>37.92</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">48.08<br>31.19<br>42.33</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Document Modeling with External Attention for Sentence Extraction</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN news highlights</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">54.2<br>21.6<br>48.1</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Multi-Reward Reinforced Summarization with Saliency and Entailment</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.5</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.43<br>18.00<br>37.10</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Ranking Sentences for Extractive Summarization with Reinforcement Learning</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.4</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.0<br>18.2<br>36.6</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">A Deep Reinforced Model for Abstractive Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2017.11</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.16<br>15.82<br>39.08</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">47.22<br>30.72<br>43.27</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.8</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">38.1<br>13.9<br>34.0</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Get To The Point: Summarization with Pointer-Generator Networks</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.4</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.53<br>17.82<br>36.38</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.11</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.6<br>16.2<br>35.3</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Distraction-Based Neural Networks for Document Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.1</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">27.1<br>8.2<br>18.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Neural Summarization by Extracting Sentences and Words</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">500 DailyMail</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">21.2<br>8.3<br>12.0</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr></table><h1 id=\"Distraction-Based-Neural-Networks-for-Document-Summarization\"><a href=\"#Distraction-Based-Neural-Networks-for-Document-Summarization\" class=\"headerlink\" title=\"Distraction-Based Neural Networks for Document Summarization\"></a>Distraction-Based Neural Networks for Document Summarization</h1><ul><li>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/H6IBk7f0bk.png?imageslim\" alt=\"mark\"></li><li>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LjAaidg72e.png?imageslim\" alt=\"mark\"></li><li>这个控制层捕捉$s_t^{‘}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。</li><li>三种注意力分散模型<ul><li>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^’，减去了历史上下文得到，类似coverage机制<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/j3ijmk54kj.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/3dCkK5ie7F.png?imageslim\" alt=\"mark\"></li><li>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/Ei4m9iKA9D.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JbHf12J8gG.png?imageslim\" alt=\"mark\"></li><li>M3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/IJ3Li6AA3m.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/HIfge59Ji8.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"Document-Modeling-with-External-Attention-for-Sentence-Extraction\"><a href=\"#Document-Modeling-with-External-Attention-for-Sentence-Extraction\" class=\"headerlink\" title=\"Document Modeling with External Attention for Sentence Extraction\"></a>Document Modeling with External Attention for Sentence Extraction</h1><ul><li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。<br>在文摘任务中，外部信息是图片配字和文档标题。</li><li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/I5GHHELAdb.png?imageslim\" alt=\"mark\"></p><ul><li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li><li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li><li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{‘}$：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k8hHm3A3CL.png?imageslim\" alt=\"mark\"></p><h1 id=\"Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\"><a href=\"#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\" class=\"headerlink\" title=\"Get To The Point: Summarization with Pointer-Generator Networks\"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><ul><li>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</li><li>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k707Ael05k.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/5idDIiI9ik.png?imageslim\" alt=\"mark\"></p><ul><li>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JhmIg667i7.png?imageslim\" alt=\"mark\"></p><ul><li>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</li><li>普通注意力计算</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/gjFeh5GHm9.png?imageslim\" alt=\"mark\"></p><ul><li>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/deabGEiC00.png?imageslim\" alt=\"mark\"></p><ul><li>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/099iBDA88m.png?imageslim\" alt=\"mark\"></p><ul><li>并在损失函数里加上coverage损失</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/36LFjHbch5.png?imageslim\" alt=\"mark\"></p><ul><li>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</li></ul><h1 id=\"SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\"><a href=\"#SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\" class=\"headerlink\" title=\"SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\"></a>SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/F08I4Ll22f.png?imageslim\" alt=\"mark\"></p><ul><li>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</li><li>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</li><li>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/a77EbCH3Eg.png?imageslim\" alt=\"mark\"></p><ul><li>其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码</li><li>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/eG9Ka8G1lA.png?imageslim\" alt=\"mark\"></p><ul><li>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/f2Fg71Hjdg.png?imageslim\" alt=\"mark\"></p><ul><li>第一行：参数为当前句子编码，表示当前句子的内容</li><li>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</li><li>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）</li><li>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）</li><li>最后对整个模型做最大似然估计:</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/KBeh4fEfIh.png?imageslim\" alt=\"mark\"></p><ul><li>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</li><li>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</li><li>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/J1C5aaef5j.png?imageslim\" alt=\"mark\"></p><h1 id=\"Attention-Is-All-You-Need\"><a href=\"#Attention-Is-All-You-Need\" class=\"headerlink\" title=\"Attention Is All You Need\"></a>Attention Is All You Need</h1><ul><li>抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/g1BhFElcff.png?imageslim\" alt=\"mark\"></p><ul><li>完全使用attention的一个好处就是快。</li><li>为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512</li><li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li><li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li><li>编码与解码的6个块都是堆叠的(stack)，</li><li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/DbhA9j0iba.png?imageslim\" alt=\"mark\"></p><ul><li>Multi-head attention由多个scaled dot-product attention并行组成。</li><li>Scaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/mJFA1DJ8DC.png?imageslim\" alt=\"mark\"></p><ul><li>Multi-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/0IBA23dbEJ.png?imageslim\" alt=\"mark\"></p><ul><li>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</li><li>这种multi-head attention用在了模型的三个地方：<ul><li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置</li><li>编码端块与块之间的自注意力</li><li>解码端块与块之间的自注意力</li></ul></li><li>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/AKjaJ4KLLj.png?imageslim\" alt=\"mark\"></p><ul><li>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LELGH5F9ge.png?imageslim\" alt=\"mark\"></p>","site":{"data":{}},"excerpt":"<p>下半年待填坑列表<br>主要关注图表示学习、文本生成和文本摘要、注意力机制、一些数学基础和模型的深入理解。<br>好像又有一大批贝叶斯网络的坑要来了。<br>读论文是不可能读完的，这辈子都不可能读完的。<br>有时间再补上阅读笔记。</p>","more":"<h1 id=\"TODO列表\"><a href=\"#TODO列表\" class=\"headerlink\" title=\"TODO列表\"></a>TODO列表</h1><ul><li style=\"list-style:none\"><input type=\"checkbox\">self-critical sequence training for image captioning</li><li style=\"list-style:none\"><input type=\"checkbox\">recurrent highway networks</li><li style=\"list-style:none\"><input type=\"checkbox\">recurrent convolutional neural networks for scene labeling</li><li style=\"list-style:none\"><input type=\"checkbox\">neural word embedding as implicit matrix factorization</li><li style=\"list-style:none\"><input type=\"checkbox\">hierarchically-attentive rnn for album summarization and storytelling</li><li style=\"list-style:none\"><input type=\"checkbox\">learning phrase representations using rnn encoder-decoder for statistical machine translation</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>neural headline generation with sentence-wise optimization</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>lda数学八卦</li><li style=\"list-style:none\"><input type=\"checkbox\">incorporating copying mechanism in sequence-to-sequence learning</li><li style=\"list-style:none\"><input type=\"checkbox\">generating sentences from a continuous space</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>generating news headlines with recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\">calculus on computational graphs: backpropagation</li><li style=\"list-style:none\"><input type=\"checkbox\">convolutional sequence to sequence learning</li><li style=\"list-style:none\"><input type=\"checkbox\">cutting-off redundant repeating generations for neural abstractive summarization</li><li style=\"list-style:none\"><input type=\"checkbox\">attention and augmented recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>attention is all you need</li><li style=\"list-style:none\"><input type=\"checkbox\">admissible stopping in viterbi beam search for unit selection in concatenative speech synthesis</li><li style=\"list-style:none\"><input type=\"checkbox\">abstractive document summarization with a graph-based attentional neural model</li><li style=\"list-style:none\"><input type=\"checkbox\">addressing the rare word problem in neural machine translation</li><li style=\"list-style:none\"><input type=\"checkbox\">abstractive sentence summarization with attentive recurrent neural networks</li><li style=\"list-style:none\"><input type=\"checkbox\">unsupervised machine translation using monolingual corpora only</li><li style=\"list-style:none\"><input type=\"checkbox\" checked>the nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</li><li style=\"list-style:none\"><input type=\"checkbox\">neural architecture search with reinforcement learning</li><li style=\"list-style:none\"><input type=\"checkbox\">an introduction to conditional random fields</li></ul><h1 id=\"一些文摘方面的SOTA\"><a href=\"#一些文摘方面的SOTA\" class=\"headerlink\" title=\"一些文摘方面的SOTA\"></a>一些文摘方面的SOTA</h1><ul><li>结果从上到下依次是ROUGE-1,ROUGE-2,ROUGE-L</li><li>强调是pyrouge是因为相比原作者用Perl写的系统，pyrouge测试值偏高</li></ul><table style=\"border-collapse:collapse;border-spacing:0;border-color:#ccc\" class=\"tg\"><tr><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">论文</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">日期</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">数据集</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">结果</th><th style=\"font-family:Arial,sans-serif;font-size:14px;font-weight:700;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#f0f0f0;text-align:center\">是否用pyrouge</th></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">Deep Communicating Agents for Abstractive Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2018.8</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.69<br>19.47<br>37.92</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">48.08<br>31.19<br>42.33</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Document Modeling with External Attention for Sentence Extraction</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN news highlights</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">54.2<br>21.6<br>48.1</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Multi-Reward Reinforced Summarization with Saliency and Entailment</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.5</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.43<br>18.00<br>37.10</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Ranking Sentences for Extractive Summarization with Reinforcement Learning</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2018.4</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">40.0<br>18.2<br>36.6</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">A Deep Reinforced Model for Abstractive Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\">2017.11</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">41.16<br>15.82<br>39.08</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\" rowspan=\"2\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">NYT</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">47.22<br>30.72<br>43.27</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.8</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">38.1<br>13.9<br>34.0</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Get To The Point: Summarization with Pointer-Generator Networks</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2017.4</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.53<br>17.82<br>36.38</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">是</td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.11</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">39.6<br>16.2<br>35.3</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Distraction-Based Neural Networks for Document Summarization</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.1</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">CNN/DM</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">27.1<br>8.2<br>18.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr><tr><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">Neural Summarization by Extracting Sentences and Words</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">2016.7</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">500 DailyMail</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\">21.2<br>8.3<br>12.0</td><td style=\"font-family:Arial,sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:inherit;color:#333;background-color:#fff;text-align:center\"></td></tr></table><h1 id=\"Distraction-Based-Neural-Networks-for-Document-Summarization\"><a href=\"#Distraction-Based-Neural-Networks-for-Document-Summarization\" class=\"headerlink\" title=\"Distraction-Based Neural Networks for Document Summarization\"></a>Distraction-Based Neural Networks for Document Summarization</h1><ul><li>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/H6IBk7f0bk.png?imageslim\" alt=\"mark\"></li><li>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LjAaidg72e.png?imageslim\" alt=\"mark\"></li><li>这个控制层捕捉$s_t^{‘}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。</li><li>三种注意力分散模型<ul><li>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^’，减去了历史上下文得到，类似coverage机制<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/j3ijmk54kj.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/3dCkK5ie7F.png?imageslim\" alt=\"mark\"></li><li>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/Ei4m9iKA9D.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JbHf12J8gG.png?imageslim\" alt=\"mark\"></li><li>M3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/IJ3Li6AA3m.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/HIfge59Ji8.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"Document-Modeling-with-External-Attention-for-Sentence-Extraction\"><a href=\"#Document-Modeling-with-External-Attention-for-Sentence-Extraction\" class=\"headerlink\" title=\"Document Modeling with External Attention for Sentence Extraction\"></a>Document Modeling with External Attention for Sentence Extraction</h1><ul><li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。<br>在文摘任务中，外部信息是图片配字和文档标题。</li><li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/I5GHHELAdb.png?imageslim\" alt=\"mark\"></p><ul><li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li><li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li><li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{‘}$：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k8hHm3A3CL.png?imageslim\" alt=\"mark\"></p><h1 id=\"Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\"><a href=\"#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\" class=\"headerlink\" title=\"Get To The Point: Summarization with Pointer-Generator Networks\"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><ul><li>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</li><li>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/k707Ael05k.png?imageslim\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/5idDIiI9ik.png?imageslim\" alt=\"mark\"></p><ul><li>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/JhmIg667i7.png?imageslim\" alt=\"mark\"></p><ul><li>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</li><li>普通注意力计算</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/gjFeh5GHm9.png?imageslim\" alt=\"mark\"></p><ul><li>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/deabGEiC00.png?imageslim\" alt=\"mark\"></p><ul><li>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/099iBDA88m.png?imageslim\" alt=\"mark\"></p><ul><li>并在损失函数里加上coverage损失</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/36LFjHbch5.png?imageslim\" alt=\"mark\"></p><ul><li>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</li></ul><h1 id=\"SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\"><a href=\"#SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\" class=\"headerlink\" title=\"SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\"></a>SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/F08I4Ll22f.png?imageslim\" alt=\"mark\"></p><ul><li>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</li><li>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</li><li>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/a77EbCH3Eg.png?imageslim\" alt=\"mark\"></p><ul><li>其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码</li><li>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/eG9Ka8G1lA.png?imageslim\" alt=\"mark\"></p><ul><li>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/f2Fg71Hjdg.png?imageslim\" alt=\"mark\"></p><ul><li>第一行：参数为当前句子编码，表示当前句子的内容</li><li>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</li><li>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）</li><li>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）</li><li>最后对整个模型做最大似然估计:</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/KBeh4fEfIh.png?imageslim\" alt=\"mark\"></p><ul><li>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</li><li>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</li><li>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/J1C5aaef5j.png?imageslim\" alt=\"mark\"></p><h1 id=\"Attention-Is-All-You-Need\"><a href=\"#Attention-Is-All-You-Need\" class=\"headerlink\" title=\"Attention Is All You Need\"></a>Attention Is All You Need</h1><ul><li>抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/g1BhFElcff.png?imageslim\" alt=\"mark\"></p><ul><li>完全使用attention的一个好处就是快。</li><li>为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512</li><li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li><li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li><li>编码与解码的6个块都是堆叠的(stack)，</li><li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/DbhA9j0iba.png?imageslim\" alt=\"mark\"></p><ul><li>Multi-head attention由多个scaled dot-product attention并行组成。</li><li>Scaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/mJFA1DJ8DC.png?imageslim\" alt=\"mark\"></p><ul><li>Multi-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/0IBA23dbEJ.png?imageslim\" alt=\"mark\"></p><ul><li>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</li><li>这种multi-head attention用在了模型的三个地方：<ul><li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置</li><li>编码端块与块之间的自注意力</li><li>解码端块与块之间的自注意力</li></ul></li><li>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/AKjaJ4KLLj.png?imageslim\" alt=\"mark\"></p><ul><li>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180908/LELGH5F9ge.png?imageslim\" alt=\"mark\"></p>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sat Sep 08 2018 21:00:40 GMT+0800 (中国标准时间)","title":"论文阅读笔记2018下半年待填坑","path":"2018/07/03/PaperReading2/","eyeCatchImage":null,"excerpt":"<p>下半年待填坑列表<br>主要关注图表示学习、文本生成和文本摘要、注意力机制、一些数学基础和模型的深入理解。<br>好像又有一大批贝叶斯网络的坑要来了。<br>读论文是不可能读完的，这辈子都不可能读完的。<br>有时间再补上阅读笔记。</p>","date":"2018-07-03T07:18:52.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"凸优化笔记","date":"2018-08-04T02:07:26.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180804/Hk49mE6DDC.jpg?imageslim"],"html":true,"_content":"***\n今日开一大坑，看毕业之前能不能填完。\nConvex Optimization By Stephen Boyd & Lieven Vandenberghe\n\n<!--more-->\n\n# 简介\n\n# 理论\n\n## 凸集\n\n## 凸函数\n\n## 凸优化问题\n\n## 对偶性\n\n# 应用\n\n## 近似和拟合\n\n## 统计估计\n\n## 几何问题\n\n# 算法\n\n## 无约束最小化\n\n## 等式约束最小化\n\n## 内点方法 ","source":"_posts/convex-optimization.md","raw":"---\ntitle: 凸优化笔记\ndate: 2018-08-04 10:07:26\ncategories: 数学\ntags:\n  - convex optimization\n  - math\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180804/Hk49mE6DDC.jpg?imageslim\nhtml: true\n---\n***\n今日开一大坑，看毕业之前能不能填完。\nConvex Optimization By Stephen Boyd & Lieven Vandenberghe\n\n<!--more-->\n\n# 简介\n\n# 理论\n\n## 凸集\n\n## 凸函数\n\n## 凸优化问题\n\n## 对偶性\n\n# 应用\n\n## 近似和拟合\n\n## 统计估计\n\n## 几何问题\n\n# 算法\n\n## 无约束最小化\n\n## 等式约束最小化\n\n## 内点方法 ","slug":"convex-optimization","published":1,"updated":"2018-08-04T02:14:48.460Z","comments":1,"layout":"post","link":"","_id":"cjmd072cs000uqcw6wnw7fvpf","content":"<hr><p>今日开一大坑，看毕业之前能不能填完。<br>Convex Optimization By Stephen Boyd &amp; Lieven Vandenberghe</p><a id=\"more\"></a><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><h1 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h1><h2 id=\"凸集\"><a href=\"#凸集\" class=\"headerlink\" title=\"凸集\"></a>凸集</h2><h2 id=\"凸函数\"><a href=\"#凸函数\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h2><h2 id=\"凸优化问题\"><a href=\"#凸优化问题\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h2><h2 id=\"对偶性\"><a href=\"#对偶性\" class=\"headerlink\" title=\"对偶性\"></a>对偶性</h2><h1 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h1><h2 id=\"近似和拟合\"><a href=\"#近似和拟合\" class=\"headerlink\" title=\"近似和拟合\"></a>近似和拟合</h2><h2 id=\"统计估计\"><a href=\"#统计估计\" class=\"headerlink\" title=\"统计估计\"></a>统计估计</h2><h2 id=\"几何问题\"><a href=\"#几何问题\" class=\"headerlink\" title=\"几何问题\"></a>几何问题</h2><h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><h2 id=\"无约束最小化\"><a href=\"#无约束最小化\" class=\"headerlink\" title=\"无约束最小化\"></a>无约束最小化</h2><h2 id=\"等式约束最小化\"><a href=\"#等式约束最小化\" class=\"headerlink\" title=\"等式约束最小化\"></a>等式约束最小化</h2><h2 id=\"内点方法\"><a href=\"#内点方法\" class=\"headerlink\" title=\"内点方法\"></a>内点方法</h2>","site":{"data":{}},"excerpt":"<hr><p>今日开一大坑，看毕业之前能不能填完。<br>Convex Optimization By Stephen Boyd &amp; Lieven Vandenberghe</p>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><h1 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h1><h2 id=\"凸集\"><a href=\"#凸集\" class=\"headerlink\" title=\"凸集\"></a>凸集</h2><h2 id=\"凸函数\"><a href=\"#凸函数\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h2><h2 id=\"凸优化问题\"><a href=\"#凸优化问题\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h2><h2 id=\"对偶性\"><a href=\"#对偶性\" class=\"headerlink\" title=\"对偶性\"></a>对偶性</h2><h1 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h1><h2 id=\"近似和拟合\"><a href=\"#近似和拟合\" class=\"headerlink\" title=\"近似和拟合\"></a>近似和拟合</h2><h2 id=\"统计估计\"><a href=\"#统计估计\" class=\"headerlink\" title=\"统计估计\"></a>统计估计</h2><h2 id=\"几何问题\"><a href=\"#几何问题\" class=\"headerlink\" title=\"几何问题\"></a>几何问题</h2><h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><h2 id=\"无约束最小化\"><a href=\"#无约束最小化\" class=\"headerlink\" title=\"无约束最小化\"></a>无约束最小化</h2><h2 id=\"等式约束最小化\"><a href=\"#等式约束最小化\" class=\"headerlink\" title=\"等式约束最小化\"></a>等式约束最小化</h2><h2 id=\"内点方法\"><a href=\"#内点方法\" class=\"headerlink\" title=\"内点方法\"></a>内点方法</h2>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sat Aug 04 2018 10:14:48 GMT+0800 (中国标准时间)","title":"凸优化笔记","path":"2018/08/04/convex-optimization/","eyeCatchImage":null,"excerpt":"<hr><p>今日开一大坑，看毕业之前能不能填完。<br>Convex Optimization By Stephen Boyd &amp; Lieven Vandenberghe</p>","date":"2018-08-04T02:07:26.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","convex optimization"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"凸优化概论笔记","date":"2018-07-03T07:17:27.000Z","mathjax":true,"html":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180703/15D61BF7mL.png?imageslim"],"_content":"***\n凸优化笔记\nConvex Optimization Overview\nZico Kolter (updated by Honglak Lee)\n\n<!--more-->\n\n# 引入\n-\t机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解\n\n# 凸集\n-\t定义\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/e78fGmcAgj.png?imageslim)\n-\t直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合\n-\t常见的二维形式即图中两点连线依然在图中，不会越界\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/m18774d3Gk.png?imageslim)\n\n## 例子\n-\t所有的实数n维空间\n-\t非负象限\n-\t范数域\n-\t仿射子空间和多面体\n-\t凸集的交集，注意并集一般不成立\n-\t半正定矩阵\n-\t以上这些例子，他们元素的凸组合依然符合原始集合的性质\n\n# 凸函数\n-\t定义\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/De00K79cF6.png?imageslim)\n-\t直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方\n\t-\t如果严格在直线下方而不是会有相切，则为严格凸性\n\t-\t如果在直线上方则为凹性\n\t-\t严格凹性同理\n\n## 凸性一阶条件\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C4cAECJLhb.png?imageslim)\n-\t前提时函数可微\n-\t即在函数上任意一点做切线，切线在函数的下方\n\n## 凸性二阶条件\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/elbdAFA9aH.png?imageslim)\n-\t前提函数二阶可微，即Hessian矩阵在所有定义域内存在\n\n## Jensen不等式\n-\t由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/AE34lI2FFj.png?imageslim)\n-\t从概率密度的角度改写为\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/c2GgbjigdA.png?imageslim)\n-\t即Jensen不等式\n\n## 分段集\n-\t一种特别的凸集称为$\\alpha$分段集，定义如下\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/L3fjcKl5if.png?imageslim)\n-\t可以证明该集合也是凸集\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/8ff5831B1d.png?imageslim)\n\n## 凸函数例子\n-\t指数函数\n-\t负对数函数\n-\t线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。\n-\t二次函数\n-\t范数\n-\t权值非负情况下，凸函数的加权和\n\n# 凸优化问题\n-\t变量属于凸集，调整变量使得凸函数值最小。\n-\t变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/hD8Ka52AcG.png?imageslim)\n-\t凸函数的最小值即最优值，最优值可以取正负无穷\n\n## 凸问题中的全局最优性\n-\t可行点的局部最优条件和全局最优条件，略过\n-\t对于凸优化问题，所有的局部最优点都是全局最优点\n\t证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/DCd7l9lDbl.png?imageslim)\n\t则可以证明z在x的邻域内\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/j4D5hg8l4J.png?imageslim)\n\t并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C0l3BC192a.png?imageslim)\n\n\n## 凸优化问题的特殊情况\n-\t对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题\n-\t线性编程（LP）：f和g都是线性函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Ci58g01eF5.png?imageslim)\n-\t二次编程（QP）：g均为线性函数，f为凸二次函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/HkiCClD4eh.png?imageslim)\n-\t二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/l45kgmJ8j1.png?imageslim)\n-\t半定编程（SDP）\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Kce3mdA3FG.png?imageslim)\n-\t这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例\n\n## 例子\n-\tSVM\n-\t约束最小二乘法\n-\t罗杰斯特回归的最大似然估计\n\n# 待续\n","source":"_posts/cvxopt.md","raw":"---\ntitle: 凸优化概论笔记\ndate: 2018-07-03 15:17:27\ntags: [convex optimization ,math]\ncategories: 数学\nmathjax: true\nhtml: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180703/15D61BF7mL.png?imageslim\n---\n***\n凸优化笔记\nConvex Optimization Overview\nZico Kolter (updated by Honglak Lee)\n\n<!--more-->\n\n# 引入\n-\t机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解\n\n# 凸集\n-\t定义\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/e78fGmcAgj.png?imageslim)\n-\t直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合\n-\t常见的二维形式即图中两点连线依然在图中，不会越界\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/m18774d3Gk.png?imageslim)\n\n## 例子\n-\t所有的实数n维空间\n-\t非负象限\n-\t范数域\n-\t仿射子空间和多面体\n-\t凸集的交集，注意并集一般不成立\n-\t半正定矩阵\n-\t以上这些例子，他们元素的凸组合依然符合原始集合的性质\n\n# 凸函数\n-\t定义\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/De00K79cF6.png?imageslim)\n-\t直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方\n\t-\t如果严格在直线下方而不是会有相切，则为严格凸性\n\t-\t如果在直线上方则为凹性\n\t-\t严格凹性同理\n\n## 凸性一阶条件\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C4cAECJLhb.png?imageslim)\n-\t前提时函数可微\n-\t即在函数上任意一点做切线，切线在函数的下方\n\n## 凸性二阶条件\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/elbdAFA9aH.png?imageslim)\n-\t前提函数二阶可微，即Hessian矩阵在所有定义域内存在\n\n## Jensen不等式\n-\t由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/AE34lI2FFj.png?imageslim)\n-\t从概率密度的角度改写为\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/c2GgbjigdA.png?imageslim)\n-\t即Jensen不等式\n\n## 分段集\n-\t一种特别的凸集称为$\\alpha$分段集，定义如下\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/L3fjcKl5if.png?imageslim)\n-\t可以证明该集合也是凸集\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/8ff5831B1d.png?imageslim)\n\n## 凸函数例子\n-\t指数函数\n-\t负对数函数\n-\t线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。\n-\t二次函数\n-\t范数\n-\t权值非负情况下，凸函数的加权和\n\n# 凸优化问题\n-\t变量属于凸集，调整变量使得凸函数值最小。\n-\t变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/hD8Ka52AcG.png?imageslim)\n-\t凸函数的最小值即最优值，最优值可以取正负无穷\n\n## 凸问题中的全局最优性\n-\t可行点的局部最优条件和全局最优条件，略过\n-\t对于凸优化问题，所有的局部最优点都是全局最优点\n\t证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/DCd7l9lDbl.png?imageslim)\n\t则可以证明z在x的邻域内\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/j4D5hg8l4J.png?imageslim)\n\t并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C0l3BC192a.png?imageslim)\n\n\n## 凸优化问题的特殊情况\n-\t对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题\n-\t线性编程（LP）：f和g都是线性函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Ci58g01eF5.png?imageslim)\n-\t二次编程（QP）：g均为线性函数，f为凸二次函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/HkiCClD4eh.png?imageslim)\n-\t二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/l45kgmJ8j1.png?imageslim)\n-\t半定编程（SDP）\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Kce3mdA3FG.png?imageslim)\n-\t这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例\n\n## 例子\n-\tSVM\n-\t约束最小二乘法\n-\t罗杰斯特回归的最大似然估计\n\n# 待续\n","slug":"cvxopt","published":1,"updated":"2018-07-06T07:37:11.346Z","comments":1,"layout":"post","link":"","_id":"cjmd072cu000xqcw6l4m00fyh","content":"<hr><p>凸优化笔记<br>Convex Optimization Overview<br>Zico Kolter (updated by Honglak Lee)</p><a id=\"more\"></a><h1 id=\"引入\"><a href=\"#引入\" class=\"headerlink\" title=\"引入\"></a>引入</h1><ul><li>机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解</li></ul><h1 id=\"凸集\"><a href=\"#凸集\" class=\"headerlink\" title=\"凸集\"></a>凸集</h1><ul><li>定义<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/e78fGmcAgj.png?imageslim\" alt=\"mark\"></li><li>直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合</li><li>常见的二维形式即图中两点连线依然在图中，不会越界<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/m18774d3Gk.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><ul><li>所有的实数n维空间</li><li>非负象限</li><li>范数域</li><li>仿射子空间和多面体</li><li>凸集的交集，注意并集一般不成立</li><li>半正定矩阵</li><li>以上这些例子，他们元素的凸组合依然符合原始集合的性质</li></ul><h1 id=\"凸函数\"><a href=\"#凸函数\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h1><ul><li>定义<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/De00K79cF6.png?imageslim\" alt=\"mark\"></li><li>直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方<ul><li>如果严格在直线下方而不是会有相切，则为严格凸性</li><li>如果在直线上方则为凹性</li><li>严格凹性同理</li></ul></li></ul><h2 id=\"凸性一阶条件\"><a href=\"#凸性一阶条件\" class=\"headerlink\" title=\"凸性一阶条件\"></a>凸性一阶条件</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C4cAECJLhb.png?imageslim\" alt=\"mark\"></p><ul><li>前提时函数可微</li><li>即在函数上任意一点做切线，切线在函数的下方</li></ul><h2 id=\"凸性二阶条件\"><a href=\"#凸性二阶条件\" class=\"headerlink\" title=\"凸性二阶条件\"></a>凸性二阶条件</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/elbdAFA9aH.png?imageslim\" alt=\"mark\"></p><ul><li>前提函数二阶可微，即Hessian矩阵在所有定义域内存在</li></ul><h2 id=\"Jensen不等式\"><a href=\"#Jensen不等式\" class=\"headerlink\" title=\"Jensen不等式\"></a>Jensen不等式</h2><ul><li>由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/AE34lI2FFj.png?imageslim\" alt=\"mark\"></li><li>从概率密度的角度改写为<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/c2GgbjigdA.png?imageslim\" alt=\"mark\"></li><li>即Jensen不等式</li></ul><h2 id=\"分段集\"><a href=\"#分段集\" class=\"headerlink\" title=\"分段集\"></a>分段集</h2><ul><li>一种特别的凸集称为$\\alpha$分段集，定义如下<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/L3fjcKl5if.png?imageslim\" alt=\"mark\"></li><li>可以证明该集合也是凸集<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/8ff5831B1d.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"凸函数例子\"><a href=\"#凸函数例子\" class=\"headerlink\" title=\"凸函数例子\"></a>凸函数例子</h2><ul><li>指数函数</li><li>负对数函数</li><li>线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。</li><li>二次函数</li><li>范数</li><li>权值非负情况下，凸函数的加权和</li></ul><h1 id=\"凸优化问题\"><a href=\"#凸优化问题\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h1><ul><li>变量属于凸集，调整变量使得凸函数值最小。</li><li>变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/hD8Ka52AcG.png?imageslim\" alt=\"mark\"></li><li>凸函数的最小值即最优值，最优值可以取正负无穷</li></ul><h2 id=\"凸问题中的全局最优性\"><a href=\"#凸问题中的全局最优性\" class=\"headerlink\" title=\"凸问题中的全局最优性\"></a>凸问题中的全局最优性</h2><ul><li>可行点的局部最优条件和全局最优条件，略过</li><li>对于凸优化问题，所有的局部最优点都是全局最优点<br>证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/DCd7l9lDbl.png?imageslim\" alt=\"mark\"><br>则可以证明z在x的邻域内<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/j4D5hg8l4J.png?imageslim\" alt=\"mark\"><br>并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C0l3BC192a.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"凸优化问题的特殊情况\"><a href=\"#凸优化问题的特殊情况\" class=\"headerlink\" title=\"凸优化问题的特殊情况\"></a>凸优化问题的特殊情况</h2><ul><li>对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题</li><li>线性编程（LP）：f和g都是线性函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Ci58g01eF5.png?imageslim\" alt=\"mark\"></li><li>二次编程（QP）：g均为线性函数，f为凸二次函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/HkiCClD4eh.png?imageslim\" alt=\"mark\"></li><li>二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/l45kgmJ8j1.png?imageslim\" alt=\"mark\"></li><li>半定编程（SDP）<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Kce3mdA3FG.png?imageslim\" alt=\"mark\"></li><li>这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例</li></ul><h2 id=\"例子-1\"><a href=\"#例子-1\" class=\"headerlink\" title=\"例子\"></a>例子</h2><ul><li>SVM</li><li>约束最小二乘法</li><li>罗杰斯特回归的最大似然估计</li></ul><h1 id=\"待续\"><a href=\"#待续\" class=\"headerlink\" title=\"待续\"></a>待续</h1>","site":{"data":{}},"excerpt":"<hr><p>凸优化笔记<br>Convex Optimization Overview<br>Zico Kolter (updated by Honglak Lee)</p>","more":"<h1 id=\"引入\"><a href=\"#引入\" class=\"headerlink\" title=\"引入\"></a>引入</h1><ul><li>机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解</li></ul><h1 id=\"凸集\"><a href=\"#凸集\" class=\"headerlink\" title=\"凸集\"></a>凸集</h1><ul><li>定义<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/e78fGmcAgj.png?imageslim\" alt=\"mark\"></li><li>直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合</li><li>常见的二维形式即图中两点连线依然在图中，不会越界<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/m18774d3Gk.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><ul><li>所有的实数n维空间</li><li>非负象限</li><li>范数域</li><li>仿射子空间和多面体</li><li>凸集的交集，注意并集一般不成立</li><li>半正定矩阵</li><li>以上这些例子，他们元素的凸组合依然符合原始集合的性质</li></ul><h1 id=\"凸函数\"><a href=\"#凸函数\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h1><ul><li>定义<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/De00K79cF6.png?imageslim\" alt=\"mark\"></li><li>直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方<ul><li>如果严格在直线下方而不是会有相切，则为严格凸性</li><li>如果在直线上方则为凹性</li><li>严格凹性同理</li></ul></li></ul><h2 id=\"凸性一阶条件\"><a href=\"#凸性一阶条件\" class=\"headerlink\" title=\"凸性一阶条件\"></a>凸性一阶条件</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C4cAECJLhb.png?imageslim\" alt=\"mark\"></p><ul><li>前提时函数可微</li><li>即在函数上任意一点做切线，切线在函数的下方</li></ul><h2 id=\"凸性二阶条件\"><a href=\"#凸性二阶条件\" class=\"headerlink\" title=\"凸性二阶条件\"></a>凸性二阶条件</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/elbdAFA9aH.png?imageslim\" alt=\"mark\"></p><ul><li>前提函数二阶可微，即Hessian矩阵在所有定义域内存在</li></ul><h2 id=\"Jensen不等式\"><a href=\"#Jensen不等式\" class=\"headerlink\" title=\"Jensen不等式\"></a>Jensen不等式</h2><ul><li>由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/AE34lI2FFj.png?imageslim\" alt=\"mark\"></li><li>从概率密度的角度改写为<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/c2GgbjigdA.png?imageslim\" alt=\"mark\"></li><li>即Jensen不等式</li></ul><h2 id=\"分段集\"><a href=\"#分段集\" class=\"headerlink\" title=\"分段集\"></a>分段集</h2><ul><li>一种特别的凸集称为$\\alpha$分段集，定义如下<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/L3fjcKl5if.png?imageslim\" alt=\"mark\"></li><li>可以证明该集合也是凸集<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/8ff5831B1d.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"凸函数例子\"><a href=\"#凸函数例子\" class=\"headerlink\" title=\"凸函数例子\"></a>凸函数例子</h2><ul><li>指数函数</li><li>负对数函数</li><li>线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。</li><li>二次函数</li><li>范数</li><li>权值非负情况下，凸函数的加权和</li></ul><h1 id=\"凸优化问题\"><a href=\"#凸优化问题\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h1><ul><li>变量属于凸集，调整变量使得凸函数值最小。</li><li>变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/hD8Ka52AcG.png?imageslim\" alt=\"mark\"></li><li>凸函数的最小值即最优值，最优值可以取正负无穷</li></ul><h2 id=\"凸问题中的全局最优性\"><a href=\"#凸问题中的全局最优性\" class=\"headerlink\" title=\"凸问题中的全局最优性\"></a>凸问题中的全局最优性</h2><ul><li>可行点的局部最优条件和全局最优条件，略过</li><li>对于凸优化问题，所有的局部最优点都是全局最优点<br>证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/DCd7l9lDbl.png?imageslim\" alt=\"mark\"><br>则可以证明z在x的邻域内<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/j4D5hg8l4J.png?imageslim\" alt=\"mark\"><br>并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/C0l3BC192a.png?imageslim\" alt=\"mark\"></li></ul><h2 id=\"凸优化问题的特殊情况\"><a href=\"#凸优化问题的特殊情况\" class=\"headerlink\" title=\"凸优化问题的特殊情况\"></a>凸优化问题的特殊情况</h2><ul><li>对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题</li><li>线性编程（LP）：f和g都是线性函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Ci58g01eF5.png?imageslim\" alt=\"mark\"></li><li>二次编程（QP）：g均为线性函数，f为凸二次函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/HkiCClD4eh.png?imageslim\" alt=\"mark\"></li><li>二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/l45kgmJ8j1.png?imageslim\" alt=\"mark\"></li><li>半定编程（SDP）<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180703/Kce3mdA3FG.png?imageslim\" alt=\"mark\"></li><li>这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例</li></ul><h2 id=\"例子-1\"><a href=\"#例子-1\" class=\"headerlink\" title=\"例子\"></a>例子</h2><ul><li>SVM</li><li>约束最小二乘法</li><li>罗杰斯特回归的最大似然估计</li></ul><h1 id=\"待续\"><a href=\"#待续\" class=\"headerlink\" title=\"待续\"></a>待续</h1>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Fri Jul 06 2018 15:37:11 GMT+0800 (中国标准时间)","title":"凸优化概论笔记","path":"2018/07/03/cvxopt/","eyeCatchImage":null,"excerpt":"<hr><p>凸优化笔记<br>Convex Optimization Overview<br>Zico Kolter (updated by Honglak Lee)</p>","date":"2018-07-03T07:17:27.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","convex optimization"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Android:Melodia客户端","date":"2017-03-09T09:19:53.000Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170309/172109957.gif"],"_content":"***\n学校大创项目简单的app\n实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。\n封面图使用[qiao](https://github.com/qiao)的midi在线可视化工具[euphony](https://github.com/qiao/euphony)\n\n<!--more-->\n\n# midi播放\n调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放\n-\tMediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放\n-\t从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)\n-\tUri或者外部存储读取new->setDataSource->prepare->start\n\n# 录制声音并重放\n参考[android中AudioRecord使用](http://blog.csdn.net/jiangliloveyou/article/details/11218555)\n```Java\n\tprivate class RecordTask extends AsyncTask<Void, Integer, Void> {\n\t\t@Override\n\t\tprotected Void doInBackground(Void... arg0) {\n\t\t\tisRecording = true;\n\t\t\ttry {\n\t\t\t\t//开通输出流到指定的文件\n\t\t\t\tDataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(pcmFile)));\n\t\t\t\t//根据定义好的几个配置，来获取合适的缓冲大小\n\t\t\t\tint bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);\n\t\t\t\t//实例化AudioRecord\n\t\t\t\tAudioRecord record = new AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);\n\t\t\t\t//定义缓冲\n\t\t\t\tshort[] buffer = new short[bufferSize];\n\n\t\t\t\t//开始录制\n\t\t\t\trecord.startRecording();\n\n\t\t\t\tint r = 0; //存储录制进度\n\t\t\t\t//定义循环，根据isRecording的值来判断是否继续录制\n\t\t\t\twhile (isRecording) {\n\t\t\t\t\t//从bufferSize中读取字节，返回读取的short个数\n\t\t\t\t\t//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决\n\t\t\t\t\tint bufferReadResult = record.read(buffer, 0, buffer.length);\n\t\t\t\t\t//循环将buffer中的音频数据写入到OutputStream中\n\t\t\t\t\tfor (int i = 0; i < bufferReadResult; i++) {\n\t\t\t\t\t\tdos.writeShort(buffer[i]);\n\t\t\t\t\t}\n\t\t\t\t\tpublishProgress(new Integer(r)); //向UI线程报告当前进度\n\t\t\t\t\tr++; //自增进度值\n\t\t\t\t}\n\t\t\t\t//录制结束\n\t\t\t\trecord.stop();\n\t\t\t\tconvertWaveFile();\n\t\t\t\tdos.close();\n\t\t\t} catch (Exception e) {\n\t\t\t\t// TODO: handle exception\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n```\n\n# pcm写头文件转成wav\n因为录制的是裸文件，pcm格式，需要自己加上wav头\n```Java\n\tprivate void WriteWaveFileHeader(FileOutputStream out, long totalAudioLen, long totalDataLen, long longSampleRate,\n\t\t\t\t\t\t\t\t\t int channels, long byteRate) throws IOException {\n\t\tbyte[] header = new byte[45];\n\t\theader[0] = 'R'; // RIFF\n\t\theader[1] = 'I';\n\t\theader[2] = 'F';\n\t\theader[3] = 'F';\n\t\theader[4] = (byte) (totalDataLen & 0xff);//数据大小\n\t\theader[5] = (byte) ((totalDataLen >> 8) & 0xff);\n\t\theader[6] = (byte) ((totalDataLen >> 16) & 0xff);\n\t\theader[7] = (byte) ((totalDataLen >> 24) & 0xff);\n\t\theader[8] = 'W';//WAVE\n\t\theader[9] = 'A';\n\t\theader[10] = 'V';\n\t\theader[11] = 'E';\n\t\t//FMT Chunk\n\t\theader[12] = 'f'; // 'fmt '\n\t\theader[13] = 'm';\n\t\theader[14] = 't';\n\t\theader[15] = ' ';//过渡字节\n\t\t//数据大小\n\t\theader[16] = 16; // 4 bytes: size of 'fmt ' chunk\n\t\theader[17] = 0;\n\t\theader[18] = 0;\n\t\theader[19] = 0;\n\t\t//编码方式 10H为PCM编码格式\n\t\theader[20] = 1; // format = 1\n\t\theader[21] = 0;\n\t\t//通道数\n\t\theader[22] = (byte) channels;\n\t\theader[23] = 0;\n\t\t//采样率，每个通道的播放速度\n\t\theader[24] = (byte) (longSampleRate & 0xff);\n\t\theader[25] = (byte) ((longSampleRate >> 8) & 0xff);\n\t\theader[26] = (byte) ((longSampleRate >> 16) & 0xff);\n\t\theader[27] = (byte) ((longSampleRate >> 24) & 0xff);\n\t\t//音频数据传送速率,采样率*通道数*采样深度/8\n\t\theader[28] = (byte) (byteRate & 0xff);\n\t\theader[29] = (byte) ((byteRate >> 8) & 0xff);\n\t\theader[30] = (byte) ((byteRate >> 16) & 0xff);\n\t\theader[31] = (byte) ((byteRate >> 24) & 0xff);\n\t\t// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数\n\t\theader[32] = (byte) (1 * 16 / 8);\n\t\theader[33] = 0;\n\t\t//每个样本的数据位数\n\t\theader[34] = 16;\n\t\theader[35] = 0;\n\t\t//Data chunk\n\t\theader[36] = 'd';//data\n\t\theader[37] = 'a';\n\t\theader[38] = 't';\n\t\theader[39] = 'a';\n\t\theader[40] = (byte) (totalAudioLen & 0xff);\n\t\theader[41] = (byte) ((totalAudioLen >> 8) & 0xff);\n\t\theader[42] = (byte) ((totalAudioLen >> 16) & 0xff);\n\t\theader[43] = (byte) ((totalAudioLen >> 24) & 0xff);\n\t\theader[44] = 0;\n\t\tout.write(header, 0, 45);\n\t}\n```\n\n# json收发\n根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中\njson发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件\n```Java\n    private JSONObject makejson(int request, String identifycode, String data) {\n        if (identifycode == \"a\") {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                JSONObject config = new JSONObject();\n                config.put(\"n\", lowf);\n                config.put(\"m\", highf);\n                config.put(\"w\", interval);\n                pack.put(\"config\", config);\n                pack.put(\"data\", data);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n        } else {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                pack.put(\"config\", \"\");\n                pack.put(\"data\", identifycode);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n\n        }\n        return null;\n    }\n```\n# socket通信\n单开一个线程用于启动socket，再开一个线程写两次json收发\n注意收发json时将json字符串用base64解码编码，java自己的string会存在错误\n另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，\"endbidou\"，不要问我是什么意思，做转换算法的兄弟想的\n```Java\nprivate class MsgThread extends Thread {\n        @Override\n        public void run() {\n            File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.wav\");\n            FileInputStream reader = null;\n            try {\n                reader = new FileInputStream(file);\n                int len = reader.available();\n                byte[] buff = new byte[len];\n                reader.read(buff);\n                String data = Base64.encodeToString(buff, Base64.DEFAULT);\n                String senda = makejson(1, \"a\", data).toString();\n                Log.i(TAG, \"request1: \" + senda);\n                OutputStream os = null;\n                InputStream is = null;\n                DataInputStream in = null;\n                try {\n                    os = soc.getOutputStream();\n                    BufferedReader bra = null;\n                    os.write(senda.getBytes());\n                    os.write(\"endbidou1\".getBytes());\n                    os.flush();\n                    Log.i(TAG, \"request1 send successful\");\n                    if (soc.isConnected()) {\n                        is = soc.getInputStream();\n                        bra = new BufferedReader(new InputStreamReader(is));\n                        md5 = bra.readLine();\n                        Log.i(TAG, \"md5: \" + md5);\n                        bra.close();\n                    } else\n                        Log.i(TAG, \"socket closed while reading\");\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n                soc.close();\n                startflag = 1;\n\n                StartThread st = new StartThread();\n                st.start();\n\n                while (soc.isClosed()) ;\n\n                String sendb = makejson(2, md5, \"request2\").toString();\n                Log.i(TAG, \"request2: \" + sendb);\n                os = soc.getOutputStream();\n                os.write(sendb.getBytes());\n                os.write(\"endbidou1\".getBytes());\n                os.flush();\n                Log.i(TAG, \"request2 send successful\");\n\n                is = soc.getInputStream();\n                byte buffer[] = new byte[1024 * 100];\n                is.read(buffer);\n                Log.i(TAG, \"midifilecontent: \" + buffer.toString());\n                soc.close();\n                File filemid = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.mid\");\n                FileOutputStream writer = null;\n                writer = new FileOutputStream(filemid);\n                writer.write(buffer);\n                writer.close();\n                Message msg = myhandler.obtainMessage();\n                msg.what = 1;\n                myhandler.sendMessage(msg);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n\n\n        }\n    }\n```\n\n# 录音特效\n录音图像动画效果来自Github：[ShineButton](https://github.com/ChadCSong/ShineButton)\n另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消\n```Java\n\tfabrecord.setOnTouchListener(new View.OnTouchListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onTouch(View v, MotionEvent event) {\n\t\t\t\t\tswitch (event.getAction()) {\n\t\t\t\t\t\tcase MotionEvent.ACTION_DOWN:\n\t\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);\n\t\t\t\t\t\t\tif (isUploadingIcon) {\n\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_DOWN\");\n\t\t\t\t\t\t\tif (!shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tox = event.getX();\n\t\t\t\t\t\t\toy = event.getY();\n\n\t\t\t\t\t\t\tisRecording = true;\n\t\t\t\t\t\t\trecLen = 0;\n\t\t\t\t\t\t\trecTime = 0;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"开始录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\t\t\trecorder = new RecordTask();\n\t\t\t\t\t\t\trecorder.execute();\n\t\t\t\t\t\t\thandler.postDelayed(runrecord, 0);\n\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_UP:\n\t\t\t\t\t\t\thandler.removeCallbacks(runrecord);\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_UP\");\n\t\t\t\t\t\t\tif (shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfloat x1 = event.getX();\n\t\t\t\t\t\t\tfloat y1 = event.getY();\n\t\t\t\t\t\t\tfloat dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);\n\n\t\t\t\t\t\t\tisRecording = false;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);\n\t\t\t\t\t\t\tif (dis1 > 30000) {\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"取消录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tif (!isUploadingIcon) {\n\t\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);\n\t\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t\t} else {\n\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"录音完成\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t\thandler.postDelayed(runreplay, 0);\n\t\t\t\t\t\t\t\treplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_MOVE:\n\t\t\t\t\t\t\tfloat x2 = event.getX();\n\t\t\t\t\t\t\tfloat y2 = event.getY();\n\t\t\t\t\t\t\tfloat dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);\n\t\t\t\t\t\t\tif (dis2 > 30000) {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```\n# 展示乐谱\n-\t本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享\n```Java\n    public void init() {\n        md5 = getArguments().getString(\"md5\");\n        final String imageUri = \"服务器地址\" + md5 + \"_1.png\";\n        Log.i(\"play\", \"pngfile: \" + imageUri);\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                imageLoader.displayImage(imageUri, showpic);\n            }\n        }, 2000);\n\n    }\n```\n\n# 与电子琴通信\n-\t类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接\n```Java\n\tpianobt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tif (!isconnected) {\n\t\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\t\tparam[0] = 0x30;\n\t\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\t\tst.start();\n\t\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\t\tms.start();\n\t\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tisconnected = false;\n\t\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\t\t\t\t\t}\n\n\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tsamplebt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\tparam[0] = 0x31;\n\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\tst.start();\n\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\tms.start();\n\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tisconnected = false;\n\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\n\t\t\t\t}\n\t\t\t});\n\n\n\t\t}\n\n\t\tprivate class StartThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tsoc = new Socket(pianoaddr, pianoport);\n\t\t\t\t\tif (soc.isConnected()) {//成功连接获取soc对象则发送成功消息\n\t\t\t\t\t\tLog.i(\"piano\", \"piano is Connected\");\n\t\t\t\t\t\tif (!isconnected)\n\t\t\t\t\t\t\tisconnected = !isconnected;\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tprivate class MsgThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tOutputStream os = soc.getOutputStream();\n\t\t\t\t\tos.write(param);\n\t\t\t\t\tos.flush();\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful\");\n\t\t\t\t\tSnackbar.make(pianobt, \"正在启动启动电子琴教学\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\tsoc.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful failed\");\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n\n# 乐谱分享\n-\t显示乐谱的是Github上一个魔改的ImageView:[PinchImageView](https://github.com/boycy815/PinchImageView)\n-\t定义其长按事件，触发一个分享的intent\n```Java\n\tshowpic.setOnLongClickListener(new View.OnLongClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onLongClick(View v) {\n\t\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);\n\t\t\t\t\tif (drawingCache == null) {\n\t\t\t\t\t\tLog.i(\"play\", \"no img to save\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFile imageFile = new File(Environment.getExternalStorageDirectory(), \"saveImageview.jpg\");\n\t\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),\n\t\t\t\t\t\t\t\t\t\"\", Toast.LENGTH_LONG);\n\t\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, 0, 200);\n\t\t\t\t\t\t\ttoast.setText(\"分享图片\");\n\t\t\t\t\t\t\ttoast.show();\n\t\t\t\t\t\t\tFileOutputStream outStream;\n\t\t\t\t\t\t\toutStream = new FileOutputStream(imageFile);\n\t\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, 100, outStream);\n\t\t\t\t\t\t\toutStream.flush();\n\t\t\t\t\t\t\toutStream.close();\n\n\t\t\t\t\t\t\tIntent sendIntent = new Intent();\n\t\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);\n\t\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));\n\t\t\t\t\t\t\tsendIntent.setType(\"image/png\");\n\t\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, \"分享到\"));\n\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\tLog.i(\"play\", \"share img wrong\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```","source":"_posts/dachuang.md","raw":"---\ntitle: Android:Melodia客户端\ndate: 2017-03-09 17:19:53\ntags: [code,android]\ncategories: Android\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170309/172109957.gif\n---\n***\n学校大创项目简单的app\n实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。\n封面图使用[qiao](https://github.com/qiao)的midi在线可视化工具[euphony](https://github.com/qiao/euphony)\n\n<!--more-->\n\n# midi播放\n调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放\n-\tMediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放\n-\t从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)\n-\tUri或者外部存储读取new->setDataSource->prepare->start\n\n# 录制声音并重放\n参考[android中AudioRecord使用](http://blog.csdn.net/jiangliloveyou/article/details/11218555)\n```Java\n\tprivate class RecordTask extends AsyncTask<Void, Integer, Void> {\n\t\t@Override\n\t\tprotected Void doInBackground(Void... arg0) {\n\t\t\tisRecording = true;\n\t\t\ttry {\n\t\t\t\t//开通输出流到指定的文件\n\t\t\t\tDataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(pcmFile)));\n\t\t\t\t//根据定义好的几个配置，来获取合适的缓冲大小\n\t\t\t\tint bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);\n\t\t\t\t//实例化AudioRecord\n\t\t\t\tAudioRecord record = new AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);\n\t\t\t\t//定义缓冲\n\t\t\t\tshort[] buffer = new short[bufferSize];\n\n\t\t\t\t//开始录制\n\t\t\t\trecord.startRecording();\n\n\t\t\t\tint r = 0; //存储录制进度\n\t\t\t\t//定义循环，根据isRecording的值来判断是否继续录制\n\t\t\t\twhile (isRecording) {\n\t\t\t\t\t//从bufferSize中读取字节，返回读取的short个数\n\t\t\t\t\t//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决\n\t\t\t\t\tint bufferReadResult = record.read(buffer, 0, buffer.length);\n\t\t\t\t\t//循环将buffer中的音频数据写入到OutputStream中\n\t\t\t\t\tfor (int i = 0; i < bufferReadResult; i++) {\n\t\t\t\t\t\tdos.writeShort(buffer[i]);\n\t\t\t\t\t}\n\t\t\t\t\tpublishProgress(new Integer(r)); //向UI线程报告当前进度\n\t\t\t\t\tr++; //自增进度值\n\t\t\t\t}\n\t\t\t\t//录制结束\n\t\t\t\trecord.stop();\n\t\t\t\tconvertWaveFile();\n\t\t\t\tdos.close();\n\t\t\t} catch (Exception e) {\n\t\t\t\t// TODO: handle exception\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n```\n\n# pcm写头文件转成wav\n因为录制的是裸文件，pcm格式，需要自己加上wav头\n```Java\n\tprivate void WriteWaveFileHeader(FileOutputStream out, long totalAudioLen, long totalDataLen, long longSampleRate,\n\t\t\t\t\t\t\t\t\t int channels, long byteRate) throws IOException {\n\t\tbyte[] header = new byte[45];\n\t\theader[0] = 'R'; // RIFF\n\t\theader[1] = 'I';\n\t\theader[2] = 'F';\n\t\theader[3] = 'F';\n\t\theader[4] = (byte) (totalDataLen & 0xff);//数据大小\n\t\theader[5] = (byte) ((totalDataLen >> 8) & 0xff);\n\t\theader[6] = (byte) ((totalDataLen >> 16) & 0xff);\n\t\theader[7] = (byte) ((totalDataLen >> 24) & 0xff);\n\t\theader[8] = 'W';//WAVE\n\t\theader[9] = 'A';\n\t\theader[10] = 'V';\n\t\theader[11] = 'E';\n\t\t//FMT Chunk\n\t\theader[12] = 'f'; // 'fmt '\n\t\theader[13] = 'm';\n\t\theader[14] = 't';\n\t\theader[15] = ' ';//过渡字节\n\t\t//数据大小\n\t\theader[16] = 16; // 4 bytes: size of 'fmt ' chunk\n\t\theader[17] = 0;\n\t\theader[18] = 0;\n\t\theader[19] = 0;\n\t\t//编码方式 10H为PCM编码格式\n\t\theader[20] = 1; // format = 1\n\t\theader[21] = 0;\n\t\t//通道数\n\t\theader[22] = (byte) channels;\n\t\theader[23] = 0;\n\t\t//采样率，每个通道的播放速度\n\t\theader[24] = (byte) (longSampleRate & 0xff);\n\t\theader[25] = (byte) ((longSampleRate >> 8) & 0xff);\n\t\theader[26] = (byte) ((longSampleRate >> 16) & 0xff);\n\t\theader[27] = (byte) ((longSampleRate >> 24) & 0xff);\n\t\t//音频数据传送速率,采样率*通道数*采样深度/8\n\t\theader[28] = (byte) (byteRate & 0xff);\n\t\theader[29] = (byte) ((byteRate >> 8) & 0xff);\n\t\theader[30] = (byte) ((byteRate >> 16) & 0xff);\n\t\theader[31] = (byte) ((byteRate >> 24) & 0xff);\n\t\t// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数\n\t\theader[32] = (byte) (1 * 16 / 8);\n\t\theader[33] = 0;\n\t\t//每个样本的数据位数\n\t\theader[34] = 16;\n\t\theader[35] = 0;\n\t\t//Data chunk\n\t\theader[36] = 'd';//data\n\t\theader[37] = 'a';\n\t\theader[38] = 't';\n\t\theader[39] = 'a';\n\t\theader[40] = (byte) (totalAudioLen & 0xff);\n\t\theader[41] = (byte) ((totalAudioLen >> 8) & 0xff);\n\t\theader[42] = (byte) ((totalAudioLen >> 16) & 0xff);\n\t\theader[43] = (byte) ((totalAudioLen >> 24) & 0xff);\n\t\theader[44] = 0;\n\t\tout.write(header, 0, 45);\n\t}\n```\n\n# json收发\n根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中\njson发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件\n```Java\n    private JSONObject makejson(int request, String identifycode, String data) {\n        if (identifycode == \"a\") {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                JSONObject config = new JSONObject();\n                config.put(\"n\", lowf);\n                config.put(\"m\", highf);\n                config.put(\"w\", interval);\n                pack.put(\"config\", config);\n                pack.put(\"data\", data);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n        } else {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                pack.put(\"config\", \"\");\n                pack.put(\"data\", identifycode);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n\n        }\n        return null;\n    }\n```\n# socket通信\n单开一个线程用于启动socket，再开一个线程写两次json收发\n注意收发json时将json字符串用base64解码编码，java自己的string会存在错误\n另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，\"endbidou\"，不要问我是什么意思，做转换算法的兄弟想的\n```Java\nprivate class MsgThread extends Thread {\n        @Override\n        public void run() {\n            File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.wav\");\n            FileInputStream reader = null;\n            try {\n                reader = new FileInputStream(file);\n                int len = reader.available();\n                byte[] buff = new byte[len];\n                reader.read(buff);\n                String data = Base64.encodeToString(buff, Base64.DEFAULT);\n                String senda = makejson(1, \"a\", data).toString();\n                Log.i(TAG, \"request1: \" + senda);\n                OutputStream os = null;\n                InputStream is = null;\n                DataInputStream in = null;\n                try {\n                    os = soc.getOutputStream();\n                    BufferedReader bra = null;\n                    os.write(senda.getBytes());\n                    os.write(\"endbidou1\".getBytes());\n                    os.flush();\n                    Log.i(TAG, \"request1 send successful\");\n                    if (soc.isConnected()) {\n                        is = soc.getInputStream();\n                        bra = new BufferedReader(new InputStreamReader(is));\n                        md5 = bra.readLine();\n                        Log.i(TAG, \"md5: \" + md5);\n                        bra.close();\n                    } else\n                        Log.i(TAG, \"socket closed while reading\");\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n                soc.close();\n                startflag = 1;\n\n                StartThread st = new StartThread();\n                st.start();\n\n                while (soc.isClosed()) ;\n\n                String sendb = makejson(2, md5, \"request2\").toString();\n                Log.i(TAG, \"request2: \" + sendb);\n                os = soc.getOutputStream();\n                os.write(sendb.getBytes());\n                os.write(\"endbidou1\".getBytes());\n                os.flush();\n                Log.i(TAG, \"request2 send successful\");\n\n                is = soc.getInputStream();\n                byte buffer[] = new byte[1024 * 100];\n                is.read(buffer);\n                Log.i(TAG, \"midifilecontent: \" + buffer.toString());\n                soc.close();\n                File filemid = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.mid\");\n                FileOutputStream writer = null;\n                writer = new FileOutputStream(filemid);\n                writer.write(buffer);\n                writer.close();\n                Message msg = myhandler.obtainMessage();\n                msg.what = 1;\n                myhandler.sendMessage(msg);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n\n\n        }\n    }\n```\n\n# 录音特效\n录音图像动画效果来自Github：[ShineButton](https://github.com/ChadCSong/ShineButton)\n另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消\n```Java\n\tfabrecord.setOnTouchListener(new View.OnTouchListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onTouch(View v, MotionEvent event) {\n\t\t\t\t\tswitch (event.getAction()) {\n\t\t\t\t\t\tcase MotionEvent.ACTION_DOWN:\n\t\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);\n\t\t\t\t\t\t\tif (isUploadingIcon) {\n\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_DOWN\");\n\t\t\t\t\t\t\tif (!shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tox = event.getX();\n\t\t\t\t\t\t\toy = event.getY();\n\n\t\t\t\t\t\t\tisRecording = true;\n\t\t\t\t\t\t\trecLen = 0;\n\t\t\t\t\t\t\trecTime = 0;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"开始录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\t\t\trecorder = new RecordTask();\n\t\t\t\t\t\t\trecorder.execute();\n\t\t\t\t\t\t\thandler.postDelayed(runrecord, 0);\n\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_UP:\n\t\t\t\t\t\t\thandler.removeCallbacks(runrecord);\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_UP\");\n\t\t\t\t\t\t\tif (shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfloat x1 = event.getX();\n\t\t\t\t\t\t\tfloat y1 = event.getY();\n\t\t\t\t\t\t\tfloat dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);\n\n\t\t\t\t\t\t\tisRecording = false;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);\n\t\t\t\t\t\t\tif (dis1 > 30000) {\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"取消录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tif (!isUploadingIcon) {\n\t\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);\n\t\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t\t} else {\n\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"录音完成\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t\thandler.postDelayed(runreplay, 0);\n\t\t\t\t\t\t\t\treplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_MOVE:\n\t\t\t\t\t\t\tfloat x2 = event.getX();\n\t\t\t\t\t\t\tfloat y2 = event.getY();\n\t\t\t\t\t\t\tfloat dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);\n\t\t\t\t\t\t\tif (dis2 > 30000) {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```\n# 展示乐谱\n-\t本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享\n```Java\n    public void init() {\n        md5 = getArguments().getString(\"md5\");\n        final String imageUri = \"服务器地址\" + md5 + \"_1.png\";\n        Log.i(\"play\", \"pngfile: \" + imageUri);\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                imageLoader.displayImage(imageUri, showpic);\n            }\n        }, 2000);\n\n    }\n```\n\n# 与电子琴通信\n-\t类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接\n```Java\n\tpianobt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tif (!isconnected) {\n\t\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\t\tparam[0] = 0x30;\n\t\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\t\tst.start();\n\t\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\t\tms.start();\n\t\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tisconnected = false;\n\t\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\t\t\t\t\t}\n\n\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tsamplebt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\tparam[0] = 0x31;\n\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\tst.start();\n\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\tms.start();\n\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tisconnected = false;\n\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\n\t\t\t\t}\n\t\t\t});\n\n\n\t\t}\n\n\t\tprivate class StartThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tsoc = new Socket(pianoaddr, pianoport);\n\t\t\t\t\tif (soc.isConnected()) {//成功连接获取soc对象则发送成功消息\n\t\t\t\t\t\tLog.i(\"piano\", \"piano is Connected\");\n\t\t\t\t\t\tif (!isconnected)\n\t\t\t\t\t\t\tisconnected = !isconnected;\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tprivate class MsgThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tOutputStream os = soc.getOutputStream();\n\t\t\t\t\tos.write(param);\n\t\t\t\t\tos.flush();\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful\");\n\t\t\t\t\tSnackbar.make(pianobt, \"正在启动启动电子琴教学\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\tsoc.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful failed\");\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n\n# 乐谱分享\n-\t显示乐谱的是Github上一个魔改的ImageView:[PinchImageView](https://github.com/boycy815/PinchImageView)\n-\t定义其长按事件，触发一个分享的intent\n```Java\n\tshowpic.setOnLongClickListener(new View.OnLongClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onLongClick(View v) {\n\t\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);\n\t\t\t\t\tif (drawingCache == null) {\n\t\t\t\t\t\tLog.i(\"play\", \"no img to save\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFile imageFile = new File(Environment.getExternalStorageDirectory(), \"saveImageview.jpg\");\n\t\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),\n\t\t\t\t\t\t\t\t\t\"\", Toast.LENGTH_LONG);\n\t\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, 0, 200);\n\t\t\t\t\t\t\ttoast.setText(\"分享图片\");\n\t\t\t\t\t\t\ttoast.show();\n\t\t\t\t\t\t\tFileOutputStream outStream;\n\t\t\t\t\t\t\toutStream = new FileOutputStream(imageFile);\n\t\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, 100, outStream);\n\t\t\t\t\t\t\toutStream.flush();\n\t\t\t\t\t\t\toutStream.close();\n\n\t\t\t\t\t\t\tIntent sendIntent = new Intent();\n\t\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);\n\t\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));\n\t\t\t\t\t\t\tsendIntent.setType(\"image/png\");\n\t\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, \"分享到\"));\n\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\tLog.i(\"play\", \"share img wrong\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```","slug":"dachuang","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","link":"","_id":"cjmd072cv0011qcw6rzw7215r","content":"<hr><p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。<br>封面图使用<a href=\"https://github.com/qiao\" target=\"_blank\" rel=\"noopener\">qiao</a>的midi在线可视化工具<a href=\"https://github.com/qiao/euphony\" target=\"_blank\" rel=\"noopener\">euphony</a></p><a id=\"more\"></a><h1 id=\"midi播放\"><a href=\"#midi播放\" class=\"headerlink\" title=\"midi播放\"></a>midi播放</h1><p>调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放</p><ul><li>MediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放</li><li>从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)</li><li>Uri或者外部存储读取new-&gt;setDataSource-&gt;prepare-&gt;start</li></ul><h1 id=\"录制声音并重放\"><a href=\"#录制声音并重放\" class=\"headerlink\" title=\"录制声音并重放\"></a>录制声音并重放</h1><p>参考<a href=\"http://blog.csdn.net/jiangliloveyou/article/details/11218555\" target=\"_blank\" rel=\"noopener\">android中AudioRecord使用</a><br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">AsyncTask</span>&lt;<span class=\"title\">Void</span>, <span class=\"title\">Integer</span>, <span class=\"title\">Void</span>&gt; </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">protected</span> Void <span class=\"title\">doInBackground</span><span class=\"params\">(Void... arg0)</span> </span>&#123;</span><br><span class=\"line\">\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开通输出流到指定的文件</span></span><br><span class=\"line\">\t\t\tDataOutputStream dos = <span class=\"keyword\">new</span> DataOutputStream(<span class=\"keyword\">new</span> BufferedOutputStream(<span class=\"keyword\">new</span> FileOutputStream(pcmFile)));</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//根据定义好的几个配置，来获取合适的缓冲大小</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//实例化AudioRecord</span></span><br><span class=\"line\">\t\t\tAudioRecord record = <span class=\"keyword\">new</span> AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义缓冲</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">short</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">short</span>[bufferSize];</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开始录制</span></span><br><span class=\"line\">\t\t\trecord.startRecording();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> r = <span class=\"number\">0</span>; <span class=\"comment\">//存储录制进度</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义循环，根据isRecording的值来判断是否继续录制</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (isRecording) &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//从bufferSize中读取字节，返回读取的short个数</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">int</span> bufferReadResult = record.read(buffer, <span class=\"number\">0</span>, buffer.length);</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//循环将buffer中的音频数据写入到OutputStream中</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; bufferReadResult; i++) &#123;</span><br><span class=\"line\">\t\t\t\t\tdos.writeShort(buffer[i]);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tpublishProgress(<span class=\"keyword\">new</span> Integer(r)); <span class=\"comment\">//向UI线程报告当前进度</span></span><br><span class=\"line\">\t\t\t\tr++; <span class=\"comment\">//自增进度值</span></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//录制结束</span></span><br><span class=\"line\">\t\t\trecord.stop();</span><br><span class=\"line\">\t\t\tconvertWaveFile();</span><br><span class=\"line\">\t\t\tdos.close();</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"pcm写头文件转成wav\"><a href=\"#pcm写头文件转成wav\" class=\"headerlink\" title=\"pcm写头文件转成wav\"></a>pcm写头文件转成wav</h1><p>因为录制的是裸文件，pcm格式，需要自己加上wav头<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">WriteWaveFileHeader</span><span class=\"params\">(FileOutputStream out, <span class=\"keyword\">long</span> totalAudioLen, <span class=\"keyword\">long</span> totalDataLen, <span class=\"keyword\">long</span> longSampleRate,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\t\t\t\t\t\t\t <span class=\"keyword\">int</span> channels, <span class=\"keyword\">long</span> byteRate)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] header = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">45</span>];</span><br><span class=\"line\">\theader[<span class=\"number\">0</span>] = <span class=\"string\">'R'</span>; <span class=\"comment\">// RIFF</span></span><br><span class=\"line\">\theader[<span class=\"number\">1</span>] = <span class=\"string\">'I'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">2</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">3</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">4</span>] = (<span class=\"keyword\">byte</span>) (totalDataLen &amp; <span class=\"number\">0xff</span>);<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">5</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">6</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">7</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">8</span>] = <span class=\"string\">'W'</span>;<span class=\"comment\">//WAVE</span></span><br><span class=\"line\">\theader[<span class=\"number\">9</span>] = <span class=\"string\">'A'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">10</span>] = <span class=\"string\">'V'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">11</span>] = <span class=\"string\">'E'</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//FMT Chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">12</span>] = <span class=\"string\">'f'</span>; <span class=\"comment\">// 'fmt '</span></span><br><span class=\"line\">\theader[<span class=\"number\">13</span>] = <span class=\"string\">'m'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">14</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">15</span>] = <span class=\"string\">' '</span>;<span class=\"comment\">//过渡字节</span></span><br><span class=\"line\">\t<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">16</span>] = <span class=\"number\">16</span>; <span class=\"comment\">// 4 bytes: size of 'fmt ' chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">17</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">18</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">19</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//编码方式 10H为PCM编码格式</span></span><br><span class=\"line\">\theader[<span class=\"number\">20</span>] = <span class=\"number\">1</span>; <span class=\"comment\">// format = 1</span></span><br><span class=\"line\">\theader[<span class=\"number\">21</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//通道数</span></span><br><span class=\"line\">\theader[<span class=\"number\">22</span>] = (<span class=\"keyword\">byte</span>) channels;</span><br><span class=\"line\">\theader[<span class=\"number\">23</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//采样率，每个通道的播放速度</span></span><br><span class=\"line\">\theader[<span class=\"number\">24</span>] = (<span class=\"keyword\">byte</span>) (longSampleRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">25</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">26</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">27</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">//音频数据传送速率,采样率*通道数*采样深度/8</span></span><br><span class=\"line\">\theader[<span class=\"number\">28</span>] = (<span class=\"keyword\">byte</span>) (byteRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">29</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">30</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">31</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">32</span>] = (<span class=\"keyword\">byte</span>) (<span class=\"number\">1</span> * <span class=\"number\">16</span> / <span class=\"number\">8</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">33</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//每个样本的数据位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">34</span>] = <span class=\"number\">16</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">35</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//Data chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">36</span>] = <span class=\"string\">'d'</span>;<span class=\"comment\">//data</span></span><br><span class=\"line\">\theader[<span class=\"number\">37</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">38</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">39</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">40</span>] = (<span class=\"keyword\">byte</span>) (totalAudioLen &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">41</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">42</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">43</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">44</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tout.write(header, <span class=\"number\">0</span>, <span class=\"number\">45</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"json收发\"><a href=\"#json收发\" class=\"headerlink\" title=\"json收发\"></a>json收发</h1><p>根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中<br>json发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> JSONObject <span class=\"title\">makejson</span><span class=\"params\">(<span class=\"keyword\">int</span> request, String identifycode, String data)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (identifycode == <span class=\"string\">\"a\"</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            JSONObject config = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            config.put(<span class=\"string\">\"n\"</span>, lowf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"m\"</span>, highf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"w\"</span>, interval);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, config);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, data);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, identifycode);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"socket通信\"><a href=\"#socket通信\" class=\"headerlink\" title=\"socket通信\"></a>socket通信</h1><p>单开一个线程用于启动socket，再开一个线程写两次json收发<br>注意收发json时将json字符串用base64解码编码，java自己的string会存在错误<br>另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，”endbidou”，不要问我是什么意思，做转换算法的兄弟想的<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            File file = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.wav\"</span>);</span><br><span class=\"line\">            FileInputStream reader = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                reader = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> len = reader.available();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span>[] buff = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[len];</span><br><span class=\"line\">                reader.read(buff);</span><br><span class=\"line\">                String data = Base64.encodeToString(buff, Base64.DEFAULT);</span><br><span class=\"line\">                String senda = makejson(<span class=\"number\">1</span>, <span class=\"string\">\"a\"</span>, data).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request1: \"</span> + senda);</span><br><span class=\"line\">                OutputStream os = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                InputStream is = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                DataInputStream in = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    os = soc.getOutputStream();</span><br><span class=\"line\">                    BufferedReader bra = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                    os.write(senda.getBytes());</span><br><span class=\"line\">                    os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                    os.flush();</span><br><span class=\"line\">                    Log.i(TAG, <span class=\"string\">\"request1 send successful\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (soc.isConnected()) &#123;</span><br><span class=\"line\">                        is = soc.getInputStream();</span><br><span class=\"line\">                        bra = <span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(is));</span><br><span class=\"line\">                        md5 = bra.readLine();</span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"md5: \"</span> + md5);</span><br><span class=\"line\">                        bra.close();</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"socket closed while reading\"</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    e.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                startflag = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                StartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">                st.start();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">while</span> (soc.isClosed()) ;</span><br><span class=\"line\"></span><br><span class=\"line\">                String sendb = makejson(<span class=\"number\">2</span>, md5, <span class=\"string\">\"request2\"</span>).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2: \"</span> + sendb);</span><br><span class=\"line\">                os = soc.getOutputStream();</span><br><span class=\"line\">                os.write(sendb.getBytes());</span><br><span class=\"line\">                os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                os.flush();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2 send successful\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                is = soc.getInputStream();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span> buffer[] = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024</span> * <span class=\"number\">100</span>];</span><br><span class=\"line\">                is.read(buffer);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"midifilecontent: \"</span> + buffer.toString());</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                File filemid = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.mid\"</span>);</span><br><span class=\"line\">                FileOutputStream writer = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                writer = <span class=\"keyword\">new</span> FileOutputStream(filemid);</span><br><span class=\"line\">                writer.write(buffer);</span><br><span class=\"line\">                writer.close();</span><br><span class=\"line\">                Message msg = myhandler.obtainMessage();</span><br><span class=\"line\">                msg.what = <span class=\"number\">1</span>;</span><br><span class=\"line\">                myhandler.sendMessage(msg);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"录音特效\"><a href=\"#录音特效\" class=\"headerlink\" title=\"录音特效\"></a>录音特效</h1><p>录音图像动画效果来自Github：<a href=\"https://github.com/ChadCSong/ShineButton\" target=\"_blank\" rel=\"noopener\">ShineButton</a><br>另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fabrecord.setOnTouchListener(<span class=\"keyword\">new</span> View.OnTouchListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onTouch</span><span class=\"params\">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">switch</span> (event.getAction()) &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_DOWN:</span><br><span class=\"line\">\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_DOWN\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (!shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tox = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\toy = event.getY();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecTime = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"开始录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\trecorder = <span class=\"keyword\">new</span> RecordTask();</span><br><span class=\"line\">\t\t\t\t\t\trecorder.execute();</span><br><span class=\"line\">\t\t\t\t\t\thandler.postDelayed(runrecord, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_UP:</span><br><span class=\"line\">\t\t\t\t\t\thandler.removeCallbacks(runrecord);</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_UP\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x1 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y1 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis1 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"取消录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">if</span> (!isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"录音完成\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t\thandler.postDelayed(runreplay, <span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\t\treplay();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_MOVE:</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x2 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y2 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis2 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure><p></p><h1 id=\"展示乐谱\"><a href=\"#展示乐谱\" class=\"headerlink\" title=\"展示乐谱\"></a>展示乐谱</h1><ul><li>本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    md5 = getArguments().getString(<span class=\"string\">\"md5\"</span>);</span><br><span class=\"line\">    <span class=\"keyword\">final</span> String imageUri = <span class=\"string\">\"服务器地址\"</span> + md5 + <span class=\"string\">\"_1.png\"</span>;</span><br><span class=\"line\">    Log.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"pngfile: \"</span> + imageUri);</span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            imageLoader.displayImage(imageUri, showpic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"与电子琴通信\"><a href=\"#与电子琴通信\" class=\"headerlink\" title=\"与电子琴通信\"></a>与电子琴通信</h1><ul><li>类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pianobt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isconnected) &#123;</span><br><span class=\"line\">\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x30</span>;</span><br><span class=\"line\">\t\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsamplebt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x31</span>;</span><br><span class=\"line\">\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StartThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tsoc = <span class=\"keyword\">new</span> Socket(pianoaddr, pianoport);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (soc.isConnected()) &#123;<span class=\"comment\">//成功连接获取soc对象则发送成功消息</span></span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano is Connected\"</span>);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> (!isconnected)</span><br><span class=\"line\">\t\t\t\t\t\tisconnected = !isconnected;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tOutputStream os = soc.getOutputStream();</span><br><span class=\"line\">\t\t\t\tos.write(param);</span><br><span class=\"line\">\t\t\t\tos.flush();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"正在启动启动电子琴教学\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful failed\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"乐谱分享\"><a href=\"#乐谱分享\" class=\"headerlink\" title=\"乐谱分享\"></a>乐谱分享</h1><ul><li>显示乐谱的是Github上一个魔改的ImageView:<a href=\"https://github.com/boycy815/PinchImageView\" target=\"_blank\" rel=\"noopener\">PinchImageView</a></li><li>定义其长按事件，触发一个分享的intent<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">showpic.setOnLongClickListener(<span class=\"keyword\">new</span> View.OnLongClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onLongClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (drawingCache == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"no img to save\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tFile imageFile = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory(), <span class=\"string\">\"saveImageview.jpg\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t<span class=\"string\">\"\"</span>, Toast.LENGTH_LONG);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, <span class=\"number\">0</span>, <span class=\"number\">200</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setText(<span class=\"string\">\"分享图片\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.show();</span><br><span class=\"line\">\t\t\t\t\t\tFileOutputStream outStream;</span><br><span class=\"line\">\t\t\t\t\t\toutStream = <span class=\"keyword\">new</span> FileOutputStream(imageFile);</span><br><span class=\"line\">\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, <span class=\"number\">100</span>, outStream);</span><br><span class=\"line\">\t\t\t\t\t\toutStream.flush();</span><br><span class=\"line\">\t\t\t\t\t\toutStream.close();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tIntent sendIntent = <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setType(<span class=\"string\">\"image/png\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, <span class=\"string\">\"分享到\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"share img wrong\"</span>);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></li></ul>","site":{"data":{}},"excerpt":"<hr><p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。<br>封面图使用<a href=\"https://github.com/qiao\" target=\"_blank\" rel=\"noopener\">qiao</a>的midi在线可视化工具<a href=\"https://github.com/qiao/euphony\" target=\"_blank\" rel=\"noopener\">euphony</a></p>","more":"<h1 id=\"midi播放\"><a href=\"#midi播放\" class=\"headerlink\" title=\"midi播放\"></a>midi播放</h1><p>调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放</p><ul><li>MediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放</li><li>从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)</li><li>Uri或者外部存储读取new-&gt;setDataSource-&gt;prepare-&gt;start</li></ul><h1 id=\"录制声音并重放\"><a href=\"#录制声音并重放\" class=\"headerlink\" title=\"录制声音并重放\"></a>录制声音并重放</h1><p>参考<a href=\"http://blog.csdn.net/jiangliloveyou/article/details/11218555\" target=\"_blank\" rel=\"noopener\">android中AudioRecord使用</a><br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">AsyncTask</span>&lt;<span class=\"title\">Void</span>, <span class=\"title\">Integer</span>, <span class=\"title\">Void</span>&gt; </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">protected</span> Void <span class=\"title\">doInBackground</span><span class=\"params\">(Void... arg0)</span> </span>&#123;</span><br><span class=\"line\">\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开通输出流到指定的文件</span></span><br><span class=\"line\">\t\t\tDataOutputStream dos = <span class=\"keyword\">new</span> DataOutputStream(<span class=\"keyword\">new</span> BufferedOutputStream(<span class=\"keyword\">new</span> FileOutputStream(pcmFile)));</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//根据定义好的几个配置，来获取合适的缓冲大小</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//实例化AudioRecord</span></span><br><span class=\"line\">\t\t\tAudioRecord record = <span class=\"keyword\">new</span> AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义缓冲</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">short</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">short</span>[bufferSize];</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开始录制</span></span><br><span class=\"line\">\t\t\trecord.startRecording();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> r = <span class=\"number\">0</span>; <span class=\"comment\">//存储录制进度</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义循环，根据isRecording的值来判断是否继续录制</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (isRecording) &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//从bufferSize中读取字节，返回读取的short个数</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">int</span> bufferReadResult = record.read(buffer, <span class=\"number\">0</span>, buffer.length);</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//循环将buffer中的音频数据写入到OutputStream中</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; bufferReadResult; i++) &#123;</span><br><span class=\"line\">\t\t\t\t\tdos.writeShort(buffer[i]);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tpublishProgress(<span class=\"keyword\">new</span> Integer(r)); <span class=\"comment\">//向UI线程报告当前进度</span></span><br><span class=\"line\">\t\t\t\tr++; <span class=\"comment\">//自增进度值</span></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//录制结束</span></span><br><span class=\"line\">\t\t\trecord.stop();</span><br><span class=\"line\">\t\t\tconvertWaveFile();</span><br><span class=\"line\">\t\t\tdos.close();</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"pcm写头文件转成wav\"><a href=\"#pcm写头文件转成wav\" class=\"headerlink\" title=\"pcm写头文件转成wav\"></a>pcm写头文件转成wav</h1><p>因为录制的是裸文件，pcm格式，需要自己加上wav头<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">WriteWaveFileHeader</span><span class=\"params\">(FileOutputStream out, <span class=\"keyword\">long</span> totalAudioLen, <span class=\"keyword\">long</span> totalDataLen, <span class=\"keyword\">long</span> longSampleRate,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\t\t\t\t\t\t\t <span class=\"keyword\">int</span> channels, <span class=\"keyword\">long</span> byteRate)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] header = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">45</span>];</span><br><span class=\"line\">\theader[<span class=\"number\">0</span>] = <span class=\"string\">'R'</span>; <span class=\"comment\">// RIFF</span></span><br><span class=\"line\">\theader[<span class=\"number\">1</span>] = <span class=\"string\">'I'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">2</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">3</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">4</span>] = (<span class=\"keyword\">byte</span>) (totalDataLen &amp; <span class=\"number\">0xff</span>);<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">5</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">6</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">7</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">8</span>] = <span class=\"string\">'W'</span>;<span class=\"comment\">//WAVE</span></span><br><span class=\"line\">\theader[<span class=\"number\">9</span>] = <span class=\"string\">'A'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">10</span>] = <span class=\"string\">'V'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">11</span>] = <span class=\"string\">'E'</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//FMT Chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">12</span>] = <span class=\"string\">'f'</span>; <span class=\"comment\">// 'fmt '</span></span><br><span class=\"line\">\theader[<span class=\"number\">13</span>] = <span class=\"string\">'m'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">14</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">15</span>] = <span class=\"string\">' '</span>;<span class=\"comment\">//过渡字节</span></span><br><span class=\"line\">\t<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">16</span>] = <span class=\"number\">16</span>; <span class=\"comment\">// 4 bytes: size of 'fmt ' chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">17</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">18</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">19</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//编码方式 10H为PCM编码格式</span></span><br><span class=\"line\">\theader[<span class=\"number\">20</span>] = <span class=\"number\">1</span>; <span class=\"comment\">// format = 1</span></span><br><span class=\"line\">\theader[<span class=\"number\">21</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//通道数</span></span><br><span class=\"line\">\theader[<span class=\"number\">22</span>] = (<span class=\"keyword\">byte</span>) channels;</span><br><span class=\"line\">\theader[<span class=\"number\">23</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//采样率，每个通道的播放速度</span></span><br><span class=\"line\">\theader[<span class=\"number\">24</span>] = (<span class=\"keyword\">byte</span>) (longSampleRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">25</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">26</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">27</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">//音频数据传送速率,采样率*通道数*采样深度/8</span></span><br><span class=\"line\">\theader[<span class=\"number\">28</span>] = (<span class=\"keyword\">byte</span>) (byteRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">29</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">30</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">31</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">32</span>] = (<span class=\"keyword\">byte</span>) (<span class=\"number\">1</span> * <span class=\"number\">16</span> / <span class=\"number\">8</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">33</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//每个样本的数据位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">34</span>] = <span class=\"number\">16</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">35</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//Data chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">36</span>] = <span class=\"string\">'d'</span>;<span class=\"comment\">//data</span></span><br><span class=\"line\">\theader[<span class=\"number\">37</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">38</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">39</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">40</span>] = (<span class=\"keyword\">byte</span>) (totalAudioLen &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">41</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">42</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">43</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">44</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tout.write(header, <span class=\"number\">0</span>, <span class=\"number\">45</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"json收发\"><a href=\"#json收发\" class=\"headerlink\" title=\"json收发\"></a>json收发</h1><p>根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中<br>json发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> JSONObject <span class=\"title\">makejson</span><span class=\"params\">(<span class=\"keyword\">int</span> request, String identifycode, String data)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (identifycode == <span class=\"string\">\"a\"</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            JSONObject config = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            config.put(<span class=\"string\">\"n\"</span>, lowf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"m\"</span>, highf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"w\"</span>, interval);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, config);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, data);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, identifycode);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"socket通信\"><a href=\"#socket通信\" class=\"headerlink\" title=\"socket通信\"></a>socket通信</h1><p>单开一个线程用于启动socket，再开一个线程写两次json收发<br>注意收发json时将json字符串用base64解码编码，java自己的string会存在错误<br>另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，”endbidou”，不要问我是什么意思，做转换算法的兄弟想的<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            File file = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.wav\"</span>);</span><br><span class=\"line\">            FileInputStream reader = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                reader = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> len = reader.available();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span>[] buff = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[len];</span><br><span class=\"line\">                reader.read(buff);</span><br><span class=\"line\">                String data = Base64.encodeToString(buff, Base64.DEFAULT);</span><br><span class=\"line\">                String senda = makejson(<span class=\"number\">1</span>, <span class=\"string\">\"a\"</span>, data).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request1: \"</span> + senda);</span><br><span class=\"line\">                OutputStream os = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                InputStream is = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                DataInputStream in = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    os = soc.getOutputStream();</span><br><span class=\"line\">                    BufferedReader bra = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                    os.write(senda.getBytes());</span><br><span class=\"line\">                    os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                    os.flush();</span><br><span class=\"line\">                    Log.i(TAG, <span class=\"string\">\"request1 send successful\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (soc.isConnected()) &#123;</span><br><span class=\"line\">                        is = soc.getInputStream();</span><br><span class=\"line\">                        bra = <span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(is));</span><br><span class=\"line\">                        md5 = bra.readLine();</span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"md5: \"</span> + md5);</span><br><span class=\"line\">                        bra.close();</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"socket closed while reading\"</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    e.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                startflag = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                StartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">                st.start();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">while</span> (soc.isClosed()) ;</span><br><span class=\"line\"></span><br><span class=\"line\">                String sendb = makejson(<span class=\"number\">2</span>, md5, <span class=\"string\">\"request2\"</span>).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2: \"</span> + sendb);</span><br><span class=\"line\">                os = soc.getOutputStream();</span><br><span class=\"line\">                os.write(sendb.getBytes());</span><br><span class=\"line\">                os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                os.flush();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2 send successful\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                is = soc.getInputStream();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span> buffer[] = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024</span> * <span class=\"number\">100</span>];</span><br><span class=\"line\">                is.read(buffer);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"midifilecontent: \"</span> + buffer.toString());</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                File filemid = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.mid\"</span>);</span><br><span class=\"line\">                FileOutputStream writer = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                writer = <span class=\"keyword\">new</span> FileOutputStream(filemid);</span><br><span class=\"line\">                writer.write(buffer);</span><br><span class=\"line\">                writer.close();</span><br><span class=\"line\">                Message msg = myhandler.obtainMessage();</span><br><span class=\"line\">                msg.what = <span class=\"number\">1</span>;</span><br><span class=\"line\">                myhandler.sendMessage(msg);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure><p></p><h1 id=\"录音特效\"><a href=\"#录音特效\" class=\"headerlink\" title=\"录音特效\"></a>录音特效</h1><p>录音图像动画效果来自Github：<a href=\"https://github.com/ChadCSong/ShineButton\" target=\"_blank\" rel=\"noopener\">ShineButton</a><br>另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消<br></p><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fabrecord.setOnTouchListener(<span class=\"keyword\">new</span> View.OnTouchListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onTouch</span><span class=\"params\">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">switch</span> (event.getAction()) &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_DOWN:</span><br><span class=\"line\">\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_DOWN\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (!shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tox = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\toy = event.getY();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecTime = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"开始录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\trecorder = <span class=\"keyword\">new</span> RecordTask();</span><br><span class=\"line\">\t\t\t\t\t\trecorder.execute();</span><br><span class=\"line\">\t\t\t\t\t\thandler.postDelayed(runrecord, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_UP:</span><br><span class=\"line\">\t\t\t\t\t\thandler.removeCallbacks(runrecord);</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_UP\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x1 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y1 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis1 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"取消录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">if</span> (!isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"录音完成\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t\thandler.postDelayed(runreplay, <span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\t\treplay();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_MOVE:</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x2 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y2 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis2 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure><p></p><h1 id=\"展示乐谱\"><a href=\"#展示乐谱\" class=\"headerlink\" title=\"展示乐谱\"></a>展示乐谱</h1><ul><li>本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    md5 = getArguments().getString(<span class=\"string\">\"md5\"</span>);</span><br><span class=\"line\">    <span class=\"keyword\">final</span> String imageUri = <span class=\"string\">\"服务器地址\"</span> + md5 + <span class=\"string\">\"_1.png\"</span>;</span><br><span class=\"line\">    Log.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"pngfile: \"</span> + imageUri);</span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            imageLoader.displayImage(imageUri, showpic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"与电子琴通信\"><a href=\"#与电子琴通信\" class=\"headerlink\" title=\"与电子琴通信\"></a>与电子琴通信</h1><ul><li>类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pianobt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isconnected) &#123;</span><br><span class=\"line\">\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x30</span>;</span><br><span class=\"line\">\t\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsamplebt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x31</span>;</span><br><span class=\"line\">\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StartThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tsoc = <span class=\"keyword\">new</span> Socket(pianoaddr, pianoport);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (soc.isConnected()) &#123;<span class=\"comment\">//成功连接获取soc对象则发送成功消息</span></span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano is Connected\"</span>);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> (!isconnected)</span><br><span class=\"line\">\t\t\t\t\t\tisconnected = !isconnected;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tOutputStream os = soc.getOutputStream();</span><br><span class=\"line\">\t\t\t\tos.write(param);</span><br><span class=\"line\">\t\t\t\tos.flush();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"正在启动启动电子琴教学\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful failed\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"乐谱分享\"><a href=\"#乐谱分享\" class=\"headerlink\" title=\"乐谱分享\"></a>乐谱分享</h1><ul><li>显示乐谱的是Github上一个魔改的ImageView:<a href=\"https://github.com/boycy815/PinchImageView\" target=\"_blank\" rel=\"noopener\">PinchImageView</a></li><li>定义其长按事件，触发一个分享的intent<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">showpic.setOnLongClickListener(<span class=\"keyword\">new</span> View.OnLongClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onLongClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (drawingCache == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"no img to save\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tFile imageFile = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory(), <span class=\"string\">\"saveImageview.jpg\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t<span class=\"string\">\"\"</span>, Toast.LENGTH_LONG);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, <span class=\"number\">0</span>, <span class=\"number\">200</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setText(<span class=\"string\">\"分享图片\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.show();</span><br><span class=\"line\">\t\t\t\t\t\tFileOutputStream outStream;</span><br><span class=\"line\">\t\t\t\t\t\toutStream = <span class=\"keyword\">new</span> FileOutputStream(imageFile);</span><br><span class=\"line\">\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, <span class=\"number\">100</span>, outStream);</span><br><span class=\"line\">\t\t\t\t\t\toutStream.flush();</span><br><span class=\"line\">\t\t\t\t\t\toutStream.close();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tIntent sendIntent = <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setType(<span class=\"string\">\"image/png\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, <span class=\"string\">\"分享到\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"share img wrong\"</span>);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Android:Melodia客户端","path":"2017/03/09/dachuang/","eyeCatchImage":null,"excerpt":"<hr><p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。<br>封面图使用<a href=\"https://github.com/qiao\" target=\"_blank\" rel=\"noopener\">qiao</a>的midi在线可视化工具<a href=\"https://github.com/qiao/euphony\" target=\"_blank\" rel=\"noopener\">euphony</a></p>","date":"2017-03-09T09:19:53.000Z","pv":0,"totalPV":0,"categories":"Android","tags":["code","android"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Android:BuptRoom总结","date":"2017-01-16T03:56:39.000Z","_content":"***\n\n![](http://ojtdnrpmt.bkt.clouddn.com/17-1-16/24437570-file_1484550130084_2fc4.png)\n\n# 简介\n写了一个查询学校空闲教室的APP\n拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的\n毕竟第一次写android，什么都想尝试一下\n点这下载：[BuptRoom](https://fir.im/buptroom)\nrepository地址:[一个简单的北邮自习室查询系统](https://github.com/thinkwee/BuptRoom)\n完成第一个版本大概是3个周末\n之后花了1个月陆陆续续更新了杂七杂八的\n很多东西写的不规范，也是临时查到了就用上\n总结一下这次写App的经过:\n\n<!--more-->\n\n# 整体结构\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170123/205417119.JPG)\n\n# 学习的内容\n-\tAndroid基本架构，组件，生命周期\n-\tFragment的使用\n-\tJava库与库之间的调用\n-\tGithub的使用\n-\t部署app\n-\t图像处理的一些方法\n-\t一个愚蠢的拉取网页内容的方式\n-\tGitHub第三方库的利用\n-\t颜色方面的知识\n-\tAndroid Material Design\n-\t简单的优化\n-\t多线程与Handler\n\n# 解决的问题\n主要解决了这么几个问题\n\n-\tAndroid6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:\n\n```Java\n    <uses-permission android:name=\"android.permission.INTERNET\"></uses-permission>\n    <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"></uses-permission>\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"></uses-permission>\n```\n\n-\t网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中\n\n```Java\n    final class MyWebViewClient extends WebViewClient {\n        public boolean shouldOverrideUrlLoading(WebView view, String url) {\n            view.loadUrl(url);\n            return true;\n        }\n        public void onPageStarted(WebView view, String url, Bitmap favicon) {\n            Log.d(\"WebView\",\"onPageStarted\");\n            super.onPageStarted(view, url, favicon);\n        }\n        public void onPageFinished(WebView view, String url) {\n            Log.d(\"WebView\",\"onPageFinished \");\n            view.loadUrl(\"javascript:window.handler.getContent(document.body.innerHTML);\");\n            super.onPageFinished(view, url);\n        }\n    }\n```\n\n-\t写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容\n\n```Java\n    final  class JavascriptHandler{\n        @JavascriptInterface\n        public void getContent(String htmlContent){\n            Log.i(Tag,\"html content: \"+htmlContent);\n            document= Jsoup.parse(htmlContent);\n            htmlstring=htmlContent;\n            content=document.getElementsByTag(\"body\").text();\n            Toast.makeText(MainActivity.this,\"加载完成\",Toast.LENGTH_SHORT).show();\n        }\n    }\n```\n\n\n\n-\t之后是字符串处理，根据教务处给的格式精简分类\n\n```Java\n    去逗号\n    String contenttemp=content;\n    content=\"\";\n    String[] contentstemp=contenttemp.split(\",\");\n    for (String temp:contentstemp){\n        content=content+temp;\n    }\n    \n    分组\n    contents=content.split(\" |:\");\n    String showcontent=\"\";\n    count=0;\n    int tsgflag=0;\n    int cishu=0;\n    j12.clear();\n    j34.clear();\n    j56.clear();\n    j78.clear();\n    j9.clear();\n    j1011.clear();\n    if (keyword.contains(\"图书馆\")) tsgflag=1;\n    for (String temp:contents){\n        if (temp.contains(keyword)){\n            cishu++;\n            SaveBuidlingInfo(count,cishu,tsgflag);\n        }\n        count++;\n    }\n    \n    SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....\n    while (1 == 1) {\n        if (contents[k].contains(\"楼\") || contents[k].contains(\"节\") || contents[k].contains(\"图\"))\n            break;\n        ;\n        switch (c) {\n            case 1:\n                j12.add(contents[k]);\n                break;\n            case 2:\n                j34.add(contents[k]);\n                break;\n            case 3:\n                j56.add(contents[k]);\n                break;\n            case 4:\n                j78.add(contents[k]);\n                break;\n            case 5:\n                j9.add(contents[k]);\n                break;\n            case 6:\n                j1011.add(contents[k]);\n                break;\n            default:\n                break;\n        }\n        k++;\n    }\n```\n\n-\t界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面\n\n\n-\t尝试了MaterialDesign组件，加入一点系统时间方面的东西\n\n```Java\n    final Calendar c = Calendar.getInstance();\n     c.setTimeZone(TimeZone.getTimeZone(\"GMT+8:00\"));\n     mYear = String.valueOf(c.get(Calendar.YEAR)); // 获取当前年份\n     mMonth = String.valueOf(c.get(Calendar.MONTH) + 1);// 获取当前月份\n     mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));// 获取当前月份的日期号码\n     mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));\n     mHour= c.get(Calendar.HOUR_OF_DAY);\n     mMinute= c.get(Calendar.MINUTE);\n    \n     if (mHour>=8&&mHour<10){\n         nowtime=\"现在是一二节课\";\n     }else\n     if (mHour>=10&&mHour<12){\n         nowtime=\"现在是三四节课\";\n     }else\n     if ((mHour==13&&mMinute>=30)||(mHour==14)||(mHour==15&&mMinute<30)){\n         nowtime=\"现在是五六节课\";\n     }else\n     if ((mHour==15&&mMinute>=30)||(mHour==16)||(mHour==17&&mMinute<30)){\n         nowtime=\"现在是七八节课\";\n     }else\n     if ((mHour==17&&mMinute>=30)||(mHour==18&&mMinute<30)){\n         nowtime=\"现在是第九节课\";\n     }else\n     if ((mHour==18&&mMinute>=30)||(mHour==19)||(mHour==20&&mMinute<30)){\n         nowtime=\"现在是十、十一节课\";\n     }else\n    nowtime=\"现在是休息时间\";\n    \n     if(\"1\".equals(mWay)){\n         mWay =\"天\";\n         daycount=6;\n     }else if(\"2\".equals(mWay)){\n         mWay =\"一\";\n         daycount=0;\n     }else if(\"3\".equals(mWay)){\n         mWay =\"二\";\n         daycount=1;\n     }else if(\"4\".equals(mWay)){\n         mWay =\"三\";\n         daycount=2;\n     }else if(\"5\".equals(mWay)){\n         mWay =\"四\";\n         daycount=3;\n     }else if(\"6\".equals(mWay)){\n         mWay =\"五\";\n         daycount=4;\n     }else if(\"7\".equals(mWay)){\n         mWay =\"六\";\n         daycount=5;\n     }\n     Timestring=mYear + \"年\" + mMonth + \"月\" + mDay+\"日\"+\"星期\"+mWay;\n    \n     FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);\n     fab.setOnClickListener(new View.OnClickListener() {\n         @Override\n         public void onClick(View view) {\n             Snackbar.make(view, \"今天是\"+Timestring+\"\\n\"+nowtime+\"  \"+interesting[daycount], Snackbar.LENGTH_SHORT)\n                     .setAction(\"Action\", null).show();\n         }\n     });\n```\n\n# 在GitHub上学到的\n此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等\n部分GitHub repository链接在这里\n-\t滑动卡片界面：[Android-SwipeToDismiss](https://github.com/romannurik/Android-SwipeToDismiss)\n-\tfir更新模块:[UpdateDemo](https://github.com/hugeterry/UpdateDemo)\n\n还有一些直接写在代码里了，忘记原地址了....\n-\t摇一摇的传感器调用\n```Java\npublic class ShakeService extends Service {\n    public static final String TAG = \"ShakeService\";\n    private SensorManager mSensorManager;\n    public boolean flag=false;\n    private ShakeBinder shakebinder= new ShakeBinder();\n    private String htmlbody=\"\";\n\n    @Override\n    public void onCreate() {\n        // TODO Auto-generated method stub\n        super.onCreate();\n        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);\n        Log.i(TAG,\"Shake Service Create\");\n    }\n\n    @Override\n    public void onDestroy() {\n        // TODO Auto-generated method stub\n        flag=false;\n        super.onDestroy();\n        mSensorManager.unregisterListener(mShakeListener);\n    }\n\n    @Override\n    public void onStart(Intent intent, int startId) {\n        // TODO Auto-generated method stub\n        super.onStart(intent, startId);\n        Log.i(TAG,\"Shake Service Start\");\n    }\n\n    @Override\n    public int onStartCommand(Intent intent, int flags, int startId) {\n        // TODO Auto-generated method stub\n        mSensorManager.registerListener(mShakeListener,\n                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),\n                //SensorManager.SENSOR_DELAY_GAME,\n                50 * 1000); //batch every 50 milliseconds\n        htmlbody=intent.getStringExtra(\"htmlbody\");\n\n        return super.onStartCommand(intent, flags, startId);\n    }\n\n    private final SensorEventListener mShakeListener = new SensorEventListener() {\n        private static final float SENSITIVITY = 10;\n        private static final int BUFFER = 5;\n        private float[] gravity = new float[3];\n        private float average = 0;\n        private int fill = 0;\n\n        @Override\n        public void onAccuracyChanged(Sensor sensor, int acc) {\n        }\n\n        public void onSensorChanged(SensorEvent event) {\n            final float alpha = 0.8F;\n\n            for (int i = 0; i < 3; i++) {\n                gravity[i] = alpha * gravity[i] + (1 - alpha) * event.values[i];\n            }\n\n            float x = event.values[0] - gravity[0];\n            float y = event.values[1] - gravity[1];\n            float z = event.values[2] - gravity[2];\n\n            if (fill <= BUFFER) {\n                average += Math.abs(x) + Math.abs(y) + Math.abs(z);\n                fill++;\n            } else {\n                Log.i(TAG, \"average:\"+average);\n                Log.i(TAG, \"average / BUFFER:\"+(average / BUFFER));\n                if (average / BUFFER >= SENSITIVITY) {\n                    handleShakeAction();//如果达到阈值则处理摇一摇响应\n                }\n                average = 0;\n                fill = 0;\n            }\n        }\n    };\n\n    protected void handleShakeAction() {\n        // TODO Auto-generated method stub\n        flag=true;\n        Toast.makeText(getApplicationContext(), \"摇一摇成功\", Toast.LENGTH_SHORT).show();\n        Intent intent= new Intent();\n        intent.putExtra(\"htmlbody\",htmlbody);\n        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);\n        intent.setClassName(this,\"thinkwee.buptroom.ShakeTestActivity\");\n        startActivity(intent);\n    }\n\n    @Override\n    public IBinder onBind(Intent intent) {\n        // TODO Auto-generated method stub\n        return shakebinder;\n    }\n    class ShakeBinder extends Binder{\n\n    }\n}\n\n```\n\n# 独立网络拉取，并使用多线程\n-\t在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用\n-\t然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新\n```Java\n        webget = new Webget();\n        webget.init(webView);\n        HaveNetFlag = webget.WebInit();\n\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                ImageView img = (ImageView) findViewById(R.id.welcomeimg);\n                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);\n                animation.setFillAfter(true);\n                img.startAnimation(animation);\n            }\n        }, 50);\n        \n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                WrongNet = webget.getWrongnet();\n                HaveNetFlag = webget.getHaveNetFlag();\n                htmlbody = webget.getHtmlbody();\n                Log.i(\"welcome\", \"2HaveNetFlag: \" + HaveNetFlag);\n                Log.i(\"welcome\", \"2Wrongnet: \" + WrongNet);\n                Log.i(\"welcome\", \"2html: \" + htmlbody);\n            }\n        }, 2000);\n\n        new Handler().postDelayed(new Runnable() {\n\n            @Override\n            public void run() {\n                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);\n                intent.putExtra(\"WrongNet\", WrongNet);\n                intent.putExtra(\"HtmlBody\", htmlbody);\n                startActivity(intent);\n                WelcomeActivity.this.finish();\n\n            }\n\n        }, 2500);\n    }\n```\n\n\n\n\n\n\n\n\n","source":"_posts/buptroomreview.md","raw":"---\ntitle: Android:BuptRoom总结\ndate: 2017-01-16 11:56:39\ntags: [code,android]\ncategories: Android\n---\n***\n\n![](http://ojtdnrpmt.bkt.clouddn.com/17-1-16/24437570-file_1484550130084_2fc4.png)\n\n# 简介\n写了一个查询学校空闲教室的APP\n拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的\n毕竟第一次写android，什么都想尝试一下\n点这下载：[BuptRoom](https://fir.im/buptroom)\nrepository地址:[一个简单的北邮自习室查询系统](https://github.com/thinkwee/BuptRoom)\n完成第一个版本大概是3个周末\n之后花了1个月陆陆续续更新了杂七杂八的\n很多东西写的不规范，也是临时查到了就用上\n总结一下这次写App的经过:\n\n<!--more-->\n\n# 整体结构\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170123/205417119.JPG)\n\n# 学习的内容\n-\tAndroid基本架构，组件，生命周期\n-\tFragment的使用\n-\tJava库与库之间的调用\n-\tGithub的使用\n-\t部署app\n-\t图像处理的一些方法\n-\t一个愚蠢的拉取网页内容的方式\n-\tGitHub第三方库的利用\n-\t颜色方面的知识\n-\tAndroid Material Design\n-\t简单的优化\n-\t多线程与Handler\n\n# 解决的问题\n主要解决了这么几个问题\n\n-\tAndroid6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:\n\n```Java\n    <uses-permission android:name=\"android.permission.INTERNET\"></uses-permission>\n    <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"></uses-permission>\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"></uses-permission>\n```\n\n-\t网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中\n\n```Java\n    final class MyWebViewClient extends WebViewClient {\n        public boolean shouldOverrideUrlLoading(WebView view, String url) {\n            view.loadUrl(url);\n            return true;\n        }\n        public void onPageStarted(WebView view, String url, Bitmap favicon) {\n            Log.d(\"WebView\",\"onPageStarted\");\n            super.onPageStarted(view, url, favicon);\n        }\n        public void onPageFinished(WebView view, String url) {\n            Log.d(\"WebView\",\"onPageFinished \");\n            view.loadUrl(\"javascript:window.handler.getContent(document.body.innerHTML);\");\n            super.onPageFinished(view, url);\n        }\n    }\n```\n\n-\t写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容\n\n```Java\n    final  class JavascriptHandler{\n        @JavascriptInterface\n        public void getContent(String htmlContent){\n            Log.i(Tag,\"html content: \"+htmlContent);\n            document= Jsoup.parse(htmlContent);\n            htmlstring=htmlContent;\n            content=document.getElementsByTag(\"body\").text();\n            Toast.makeText(MainActivity.this,\"加载完成\",Toast.LENGTH_SHORT).show();\n        }\n    }\n```\n\n\n\n-\t之后是字符串处理，根据教务处给的格式精简分类\n\n```Java\n    去逗号\n    String contenttemp=content;\n    content=\"\";\n    String[] contentstemp=contenttemp.split(\",\");\n    for (String temp:contentstemp){\n        content=content+temp;\n    }\n    \n    分组\n    contents=content.split(\" |:\");\n    String showcontent=\"\";\n    count=0;\n    int tsgflag=0;\n    int cishu=0;\n    j12.clear();\n    j34.clear();\n    j56.clear();\n    j78.clear();\n    j9.clear();\n    j1011.clear();\n    if (keyword.contains(\"图书馆\")) tsgflag=1;\n    for (String temp:contents){\n        if (temp.contains(keyword)){\n            cishu++;\n            SaveBuidlingInfo(count,cishu,tsgflag);\n        }\n        count++;\n    }\n    \n    SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....\n    while (1 == 1) {\n        if (contents[k].contains(\"楼\") || contents[k].contains(\"节\") || contents[k].contains(\"图\"))\n            break;\n        ;\n        switch (c) {\n            case 1:\n                j12.add(contents[k]);\n                break;\n            case 2:\n                j34.add(contents[k]);\n                break;\n            case 3:\n                j56.add(contents[k]);\n                break;\n            case 4:\n                j78.add(contents[k]);\n                break;\n            case 5:\n                j9.add(contents[k]);\n                break;\n            case 6:\n                j1011.add(contents[k]);\n                break;\n            default:\n                break;\n        }\n        k++;\n    }\n```\n\n-\t界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面\n\n\n-\t尝试了MaterialDesign组件，加入一点系统时间方面的东西\n\n```Java\n    final Calendar c = Calendar.getInstance();\n     c.setTimeZone(TimeZone.getTimeZone(\"GMT+8:00\"));\n     mYear = String.valueOf(c.get(Calendar.YEAR)); // 获取当前年份\n     mMonth = String.valueOf(c.get(Calendar.MONTH) + 1);// 获取当前月份\n     mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));// 获取当前月份的日期号码\n     mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));\n     mHour= c.get(Calendar.HOUR_OF_DAY);\n     mMinute= c.get(Calendar.MINUTE);\n    \n     if (mHour>=8&&mHour<10){\n         nowtime=\"现在是一二节课\";\n     }else\n     if (mHour>=10&&mHour<12){\n         nowtime=\"现在是三四节课\";\n     }else\n     if ((mHour==13&&mMinute>=30)||(mHour==14)||(mHour==15&&mMinute<30)){\n         nowtime=\"现在是五六节课\";\n     }else\n     if ((mHour==15&&mMinute>=30)||(mHour==16)||(mHour==17&&mMinute<30)){\n         nowtime=\"现在是七八节课\";\n     }else\n     if ((mHour==17&&mMinute>=30)||(mHour==18&&mMinute<30)){\n         nowtime=\"现在是第九节课\";\n     }else\n     if ((mHour==18&&mMinute>=30)||(mHour==19)||(mHour==20&&mMinute<30)){\n         nowtime=\"现在是十、十一节课\";\n     }else\n    nowtime=\"现在是休息时间\";\n    \n     if(\"1\".equals(mWay)){\n         mWay =\"天\";\n         daycount=6;\n     }else if(\"2\".equals(mWay)){\n         mWay =\"一\";\n         daycount=0;\n     }else if(\"3\".equals(mWay)){\n         mWay =\"二\";\n         daycount=1;\n     }else if(\"4\".equals(mWay)){\n         mWay =\"三\";\n         daycount=2;\n     }else if(\"5\".equals(mWay)){\n         mWay =\"四\";\n         daycount=3;\n     }else if(\"6\".equals(mWay)){\n         mWay =\"五\";\n         daycount=4;\n     }else if(\"7\".equals(mWay)){\n         mWay =\"六\";\n         daycount=5;\n     }\n     Timestring=mYear + \"年\" + mMonth + \"月\" + mDay+\"日\"+\"星期\"+mWay;\n    \n     FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);\n     fab.setOnClickListener(new View.OnClickListener() {\n         @Override\n         public void onClick(View view) {\n             Snackbar.make(view, \"今天是\"+Timestring+\"\\n\"+nowtime+\"  \"+interesting[daycount], Snackbar.LENGTH_SHORT)\n                     .setAction(\"Action\", null).show();\n         }\n     });\n```\n\n# 在GitHub上学到的\n此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等\n部分GitHub repository链接在这里\n-\t滑动卡片界面：[Android-SwipeToDismiss](https://github.com/romannurik/Android-SwipeToDismiss)\n-\tfir更新模块:[UpdateDemo](https://github.com/hugeterry/UpdateDemo)\n\n还有一些直接写在代码里了，忘记原地址了....\n-\t摇一摇的传感器调用\n```Java\npublic class ShakeService extends Service {\n    public static final String TAG = \"ShakeService\";\n    private SensorManager mSensorManager;\n    public boolean flag=false;\n    private ShakeBinder shakebinder= new ShakeBinder();\n    private String htmlbody=\"\";\n\n    @Override\n    public void onCreate() {\n        // TODO Auto-generated method stub\n        super.onCreate();\n        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);\n        Log.i(TAG,\"Shake Service Create\");\n    }\n\n    @Override\n    public void onDestroy() {\n        // TODO Auto-generated method stub\n        flag=false;\n        super.onDestroy();\n        mSensorManager.unregisterListener(mShakeListener);\n    }\n\n    @Override\n    public void onStart(Intent intent, int startId) {\n        // TODO Auto-generated method stub\n        super.onStart(intent, startId);\n        Log.i(TAG,\"Shake Service Start\");\n    }\n\n    @Override\n    public int onStartCommand(Intent intent, int flags, int startId) {\n        // TODO Auto-generated method stub\n        mSensorManager.registerListener(mShakeListener,\n                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),\n                //SensorManager.SENSOR_DELAY_GAME,\n                50 * 1000); //batch every 50 milliseconds\n        htmlbody=intent.getStringExtra(\"htmlbody\");\n\n        return super.onStartCommand(intent, flags, startId);\n    }\n\n    private final SensorEventListener mShakeListener = new SensorEventListener() {\n        private static final float SENSITIVITY = 10;\n        private static final int BUFFER = 5;\n        private float[] gravity = new float[3];\n        private float average = 0;\n        private int fill = 0;\n\n        @Override\n        public void onAccuracyChanged(Sensor sensor, int acc) {\n        }\n\n        public void onSensorChanged(SensorEvent event) {\n            final float alpha = 0.8F;\n\n            for (int i = 0; i < 3; i++) {\n                gravity[i] = alpha * gravity[i] + (1 - alpha) * event.values[i];\n            }\n\n            float x = event.values[0] - gravity[0];\n            float y = event.values[1] - gravity[1];\n            float z = event.values[2] - gravity[2];\n\n            if (fill <= BUFFER) {\n                average += Math.abs(x) + Math.abs(y) + Math.abs(z);\n                fill++;\n            } else {\n                Log.i(TAG, \"average:\"+average);\n                Log.i(TAG, \"average / BUFFER:\"+(average / BUFFER));\n                if (average / BUFFER >= SENSITIVITY) {\n                    handleShakeAction();//如果达到阈值则处理摇一摇响应\n                }\n                average = 0;\n                fill = 0;\n            }\n        }\n    };\n\n    protected void handleShakeAction() {\n        // TODO Auto-generated method stub\n        flag=true;\n        Toast.makeText(getApplicationContext(), \"摇一摇成功\", Toast.LENGTH_SHORT).show();\n        Intent intent= new Intent();\n        intent.putExtra(\"htmlbody\",htmlbody);\n        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);\n        intent.setClassName(this,\"thinkwee.buptroom.ShakeTestActivity\");\n        startActivity(intent);\n    }\n\n    @Override\n    public IBinder onBind(Intent intent) {\n        // TODO Auto-generated method stub\n        return shakebinder;\n    }\n    class ShakeBinder extends Binder{\n\n    }\n}\n\n```\n\n# 独立网络拉取，并使用多线程\n-\t在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用\n-\t然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新\n```Java\n        webget = new Webget();\n        webget.init(webView);\n        HaveNetFlag = webget.WebInit();\n\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                ImageView img = (ImageView) findViewById(R.id.welcomeimg);\n                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);\n                animation.setFillAfter(true);\n                img.startAnimation(animation);\n            }\n        }, 50);\n        \n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                WrongNet = webget.getWrongnet();\n                HaveNetFlag = webget.getHaveNetFlag();\n                htmlbody = webget.getHtmlbody();\n                Log.i(\"welcome\", \"2HaveNetFlag: \" + HaveNetFlag);\n                Log.i(\"welcome\", \"2Wrongnet: \" + WrongNet);\n                Log.i(\"welcome\", \"2html: \" + htmlbody);\n            }\n        }, 2000);\n\n        new Handler().postDelayed(new Runnable() {\n\n            @Override\n            public void run() {\n                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);\n                intent.putExtra(\"WrongNet\", WrongNet);\n                intent.putExtra(\"HtmlBody\", htmlbody);\n                startActivity(intent);\n                WelcomeActivity.this.finish();\n\n            }\n\n        }, 2500);\n    }\n```\n\n\n\n\n\n\n\n\n","slug":"buptroomreview","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072cx0015qcw6cz5y0k9y","content":"<hr><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-16/24437570-file_1484550130084_2fc4.png\" alt=\"\"></p><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p><a id=\"more\"></a><h1 id=\"整体结构\"><a href=\"#整体结构\" class=\"headerlink\" title=\"整体结构\"></a>整体结构</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170123/205417119.JPG\" alt=\"mark\"></p><h1 id=\"学习的内容\"><a href=\"#学习的内容\" class=\"headerlink\" title=\"学习的内容\"></a>学习的内容</h1><ul><li>Android基本架构，组件，生命周期</li><li>Fragment的使用</li><li>Java库与库之间的调用</li><li>Github的使用</li><li>部署app</li><li>图像处理的一些方法</li><li>一个愚蠢的拉取网页内容的方式</li><li>GitHub第三方库的利用</li><li>颜色方面的知识</li><li>Android Material Design</li><li>简单的优化</li><li>多线程与Handler</li></ul><h1 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h1><p>主要解决了这么几个问题</p><ul><li>Android6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;uses-permission android:name=\"android.permission.INTERNET\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure><ul><li>网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWebViewClient</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebViewClient</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">shouldOverrideUrlLoading</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        view.loadUrl(url);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageStarted</span><span class=\"params\">(WebView view, String url, Bitmap favicon)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageStarted\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageStarted(view, url, favicon);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageFinished</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageFinished \"</span>);</span><br><span class=\"line\">        view.loadUrl(<span class=\"string\">\"javascript:window.handler.getContent(document.body.innerHTML);\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageFinished(view, url);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span>  <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavascriptHandler</span></span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@JavascriptInterface</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getContent</span><span class=\"params\">(String htmlContent)</span></span>&#123;</span><br><span class=\"line\">        Log.i(Tag,<span class=\"string\">\"html content: \"</span>+htmlContent);</span><br><span class=\"line\">        document= Jsoup.parse(htmlContent);</span><br><span class=\"line\">        htmlstring=htmlContent;</span><br><span class=\"line\">        content=document.getElementsByTag(<span class=\"string\">\"body\"</span>).text();</span><br><span class=\"line\">        Toast.makeText(MainActivity.<span class=\"keyword\">this</span>,<span class=\"string\">\"加载完成\"</span>,Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>之后是字符串处理，根据教务处给的格式精简分类</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">去逗号</span><br><span class=\"line\">String contenttemp=content;</span><br><span class=\"line\">content=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">String[] contentstemp=contenttemp.split(<span class=\"string\">\",\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contentstemp)&#123;</span><br><span class=\"line\">    content=content+temp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">分组</span><br><span class=\"line\">contents=content.split(<span class=\"string\">\" |:\"</span>);</span><br><span class=\"line\">String showcontent=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">count=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> tsgflag=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> cishu=<span class=\"number\">0</span>;</span><br><span class=\"line\">j12.clear();</span><br><span class=\"line\">j34.clear();</span><br><span class=\"line\">j56.clear();</span><br><span class=\"line\">j78.clear();</span><br><span class=\"line\">j9.clear();</span><br><span class=\"line\">j1011.clear();</span><br><span class=\"line\"><span class=\"keyword\">if</span> (keyword.contains(<span class=\"string\">\"图书馆\"</span>)) tsgflag=<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contents)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (temp.contains(keyword))&#123;</span><br><span class=\"line\">        cishu++;</span><br><span class=\"line\">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    count++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....</span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"number\">1</span> == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (contents[k].contains(<span class=\"string\">\"楼\"</span>) || contents[k].contains(<span class=\"string\">\"节\"</span>) || contents[k].contains(<span class=\"string\">\"图\"</span>))</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ;</span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (c) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">1</span>:</span><br><span class=\"line\">            j12.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">2</span>:</span><br><span class=\"line\">            j34.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">3</span>:</span><br><span class=\"line\">            j56.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">4</span>:</span><br><span class=\"line\">            j78.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">5</span>:</span><br><span class=\"line\">            j9.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">6</span>:</span><br><span class=\"line\">            j1011.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    k++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面</li></ul><ul><li>尝试了MaterialDesign组件，加入一点系统时间方面的东西</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Calendar c = Calendar.getInstance();</span><br><span class=\"line\"> c.setTimeZone(TimeZone.getTimeZone(<span class=\"string\">\"GMT+8:00\"</span>));</span><br><span class=\"line\"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class=\"comment\">// 获取当前年份</span></span><br><span class=\"line\"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class=\"number\">1</span>);<span class=\"comment\">// 获取当前月份</span></span><br><span class=\"line\"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class=\"comment\">// 获取当前月份的日期号码</span></span><br><span class=\"line\"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class=\"line\"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class=\"line\"> mMinute= c.get(Calendar.MINUTE);</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">8</span>&amp;&amp;mHour&lt;<span class=\"number\">10</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是一二节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">10</span>&amp;&amp;mHour&lt;<span class=\"number\">12</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是三四节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">13</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">14</span>)||(mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是五六节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">16</span>)||(mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是七八节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是第九节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">19</span>)||(mHour==<span class=\"number\">20</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是十、十一节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">nowtime=<span class=\"string\">\"现在是休息时间\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span>(<span class=\"string\">\"1\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"天\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">6</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"2\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"一\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">0</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"3\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"二\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">1</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"4\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"三\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">2</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"5\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"四\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">3</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"6\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"五\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">4</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"7\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"六\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">5</span>;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> Timestring=mYear + <span class=\"string\">\"年\"</span> + mMonth + <span class=\"string\">\"月\"</span> + mDay+<span class=\"string\">\"日\"</span>+<span class=\"string\">\"星期\"</span>+mWay;</span><br><span class=\"line\"></span><br><span class=\"line\"> FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);</span><br><span class=\"line\"> fab.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View view)</span> </span>&#123;</span><br><span class=\"line\">         Snackbar.make(view, <span class=\"string\">\"今天是\"</span>+Timestring+<span class=\"string\">\"\\n\"</span>+nowtime+<span class=\"string\">\"  \"</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">                 .setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure><h1 id=\"在GitHub上学到的\"><a href=\"#在GitHub上学到的\" class=\"headerlink\" title=\"在GitHub上学到的\"></a>在GitHub上学到的</h1><p>此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等<br>部分GitHub repository链接在这里</p><ul><li>滑动卡片界面：<a href=\"https://github.com/romannurik/Android-SwipeToDismiss\" target=\"_blank\" rel=\"noopener\">Android-SwipeToDismiss</a></li><li>fir更新模块:<a href=\"https://github.com/hugeterry/UpdateDemo\" target=\"_blank\" rel=\"noopener\">UpdateDemo</a></li></ul><p>还有一些直接写在代码里了，忘记原地址了….</p><ul><li>摇一摇的传感器调用<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeService</span> <span class=\"keyword\">extends</span> <span class=\"title\">Service</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> String TAG = <span class=\"string\">\"ShakeService\"</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SensorManager mSensorManager;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> ShakeBinder shakebinder= <span class=\"keyword\">new</span> ShakeBinder();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String htmlbody=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate();</span><br><span class=\"line\">        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Create\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onDestroy</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onDestroy();</span><br><span class=\"line\">        mSensorManager.unregisterListener(mShakeListener);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onStart</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onStart(intent, startId);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Start\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">onStartCommand</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> flags, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        mSensorManager.registerListener(mShakeListener,</span><br><span class=\"line\">                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class=\"line\">                <span class=\"comment\">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class=\"line\">                <span class=\"number\">50</span> * <span class=\"number\">1000</span>); <span class=\"comment\">//batch every 50 milliseconds</span></span><br><span class=\"line\">        htmlbody=intent.getStringExtra(<span class=\"string\">\"htmlbody\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.onStartCommand(intent, flags, startId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SensorEventListener mShakeListener = <span class=\"keyword\">new</span> SensorEventListener() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> SENSITIVITY = <span class=\"number\">10</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BUFFER = <span class=\"number\">5</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span>[] gravity = <span class=\"keyword\">new</span> <span class=\"keyword\">float</span>[<span class=\"number\">3</span>];</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span> average = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> fill = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onAccuracyChanged</span><span class=\"params\">(Sensor sensor, <span class=\"keyword\">int</span> acc)</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onSensorChanged</span><span class=\"params\">(SensorEvent event)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> alpha = <span class=\"number\">0.8F</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; i++) &#123;</span><br><span class=\"line\">                gravity[i] = alpha * gravity[i] + (<span class=\"number\">1</span> - alpha) * event.values[i];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">float</span> x = event.values[<span class=\"number\">0</span>] - gravity[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> y = event.values[<span class=\"number\">1</span>] - gravity[<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> z = event.values[<span class=\"number\">2</span>] - gravity[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class=\"line\">                average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class=\"line\">                fill++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average:\"</span>+average);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average / BUFFER:\"</span>+(average / BUFFER));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class=\"line\">                    handleShakeAction();<span class=\"comment\">//如果达到阈值则处理摇一摇响应</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                average = <span class=\"number\">0</span>;</span><br><span class=\"line\">                fill = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">handleShakeAction</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">true</span>;</span><br><span class=\"line\">        Toast.makeText(getApplicationContext(), <span class=\"string\">\"摇一摇成功\"</span>, Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">        Intent intent= <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">        intent.putExtra(<span class=\"string\">\"htmlbody\"</span>,htmlbody);</span><br><span class=\"line\">        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class=\"line\">        intent.setClassName(<span class=\"keyword\">this</span>,<span class=\"string\">\"thinkwee.buptroom.ShakeTestActivity\"</span>);</span><br><span class=\"line\">        startActivity(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> IBinder <span class=\"title\">onBind</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> shakebinder;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeBinder</span> <span class=\"keyword\">extends</span> <span class=\"title\">Binder</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"独立网络拉取，并使用多线程\"><a href=\"#独立网络拉取，并使用多线程\" class=\"headerlink\" title=\"独立网络拉取，并使用多线程\"></a>独立网络拉取，并使用多线程</h1><ul><li>在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用</li><li>然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    webget = <span class=\"keyword\">new</span> Webget();</span><br><span class=\"line\">    webget.init(webView);</span><br><span class=\"line\">    HaveNetFlag = webget.WebInit();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class=\"line\">            Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.<span class=\"keyword\">this</span>, R.anim.enlarge);</span><br><span class=\"line\">            animation.setFillAfter(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">            img.startAnimation(animation);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">50</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            WrongNet = webget.getWrongnet();</span><br><span class=\"line\">            HaveNetFlag = webget.getHaveNetFlag();</span><br><span class=\"line\">            htmlbody = webget.getHtmlbody();</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2HaveNetFlag: \"</span> + HaveNetFlag);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2Wrongnet: \"</span> + WrongNet);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2html: \"</span> + htmlbody);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            Intent intent = <span class=\"keyword\">new</span> Intent(WelcomeActivity.<span class=\"keyword\">this</span>, MainActivity.class);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"WrongNet\"</span>, WrongNet);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"HtmlBody\"</span>, htmlbody);</span><br><span class=\"line\">            startActivity(intent);</span><br><span class=\"line\">            WelcomeActivity.<span class=\"keyword\">this</span>.finish();</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;, <span class=\"number\">2500</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul>","site":{"data":{}},"excerpt":"<hr><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-16/24437570-file_1484550130084_2fc4.png\" alt=\"\"></p><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p>","more":"<h1 id=\"整体结构\"><a href=\"#整体结构\" class=\"headerlink\" title=\"整体结构\"></a>整体结构</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170123/205417119.JPG\" alt=\"mark\"></p><h1 id=\"学习的内容\"><a href=\"#学习的内容\" class=\"headerlink\" title=\"学习的内容\"></a>学习的内容</h1><ul><li>Android基本架构，组件，生命周期</li><li>Fragment的使用</li><li>Java库与库之间的调用</li><li>Github的使用</li><li>部署app</li><li>图像处理的一些方法</li><li>一个愚蠢的拉取网页内容的方式</li><li>GitHub第三方库的利用</li><li>颜色方面的知识</li><li>Android Material Design</li><li>简单的优化</li><li>多线程与Handler</li></ul><h1 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h1><p>主要解决了这么几个问题</p><ul><li>Android6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;uses-permission android:name=\"android.permission.INTERNET\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure><ul><li>网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWebViewClient</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebViewClient</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">shouldOverrideUrlLoading</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        view.loadUrl(url);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageStarted</span><span class=\"params\">(WebView view, String url, Bitmap favicon)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageStarted\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageStarted(view, url, favicon);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageFinished</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageFinished \"</span>);</span><br><span class=\"line\">        view.loadUrl(<span class=\"string\">\"javascript:window.handler.getContent(document.body.innerHTML);\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageFinished(view, url);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span>  <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavascriptHandler</span></span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@JavascriptInterface</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getContent</span><span class=\"params\">(String htmlContent)</span></span>&#123;</span><br><span class=\"line\">        Log.i(Tag,<span class=\"string\">\"html content: \"</span>+htmlContent);</span><br><span class=\"line\">        document= Jsoup.parse(htmlContent);</span><br><span class=\"line\">        htmlstring=htmlContent;</span><br><span class=\"line\">        content=document.getElementsByTag(<span class=\"string\">\"body\"</span>).text();</span><br><span class=\"line\">        Toast.makeText(MainActivity.<span class=\"keyword\">this</span>,<span class=\"string\">\"加载完成\"</span>,Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>之后是字符串处理，根据教务处给的格式精简分类</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">去逗号</span><br><span class=\"line\">String contenttemp=content;</span><br><span class=\"line\">content=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">String[] contentstemp=contenttemp.split(<span class=\"string\">\",\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contentstemp)&#123;</span><br><span class=\"line\">    content=content+temp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">分组</span><br><span class=\"line\">contents=content.split(<span class=\"string\">\" |:\"</span>);</span><br><span class=\"line\">String showcontent=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">count=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> tsgflag=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> cishu=<span class=\"number\">0</span>;</span><br><span class=\"line\">j12.clear();</span><br><span class=\"line\">j34.clear();</span><br><span class=\"line\">j56.clear();</span><br><span class=\"line\">j78.clear();</span><br><span class=\"line\">j9.clear();</span><br><span class=\"line\">j1011.clear();</span><br><span class=\"line\"><span class=\"keyword\">if</span> (keyword.contains(<span class=\"string\">\"图书馆\"</span>)) tsgflag=<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contents)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (temp.contains(keyword))&#123;</span><br><span class=\"line\">        cishu++;</span><br><span class=\"line\">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    count++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....</span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"number\">1</span> == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (contents[k].contains(<span class=\"string\">\"楼\"</span>) || contents[k].contains(<span class=\"string\">\"节\"</span>) || contents[k].contains(<span class=\"string\">\"图\"</span>))</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ;</span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (c) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">1</span>:</span><br><span class=\"line\">            j12.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">2</span>:</span><br><span class=\"line\">            j34.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">3</span>:</span><br><span class=\"line\">            j56.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">4</span>:</span><br><span class=\"line\">            j78.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">5</span>:</span><br><span class=\"line\">            j9.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">6</span>:</span><br><span class=\"line\">            j1011.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    k++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><ul><li>界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面</li></ul><ul><li>尝试了MaterialDesign组件，加入一点系统时间方面的东西</li></ul><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Calendar c = Calendar.getInstance();</span><br><span class=\"line\"> c.setTimeZone(TimeZone.getTimeZone(<span class=\"string\">\"GMT+8:00\"</span>));</span><br><span class=\"line\"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class=\"comment\">// 获取当前年份</span></span><br><span class=\"line\"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class=\"number\">1</span>);<span class=\"comment\">// 获取当前月份</span></span><br><span class=\"line\"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class=\"comment\">// 获取当前月份的日期号码</span></span><br><span class=\"line\"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class=\"line\"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class=\"line\"> mMinute= c.get(Calendar.MINUTE);</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">8</span>&amp;&amp;mHour&lt;<span class=\"number\">10</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是一二节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">10</span>&amp;&amp;mHour&lt;<span class=\"number\">12</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是三四节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">13</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">14</span>)||(mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是五六节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">16</span>)||(mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是七八节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是第九节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">19</span>)||(mHour==<span class=\"number\">20</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是十、十一节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">nowtime=<span class=\"string\">\"现在是休息时间\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span>(<span class=\"string\">\"1\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"天\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">6</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"2\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"一\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">0</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"3\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"二\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">1</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"4\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"三\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">2</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"5\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"四\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">3</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"6\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"五\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">4</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"7\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"六\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">5</span>;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> Timestring=mYear + <span class=\"string\">\"年\"</span> + mMonth + <span class=\"string\">\"月\"</span> + mDay+<span class=\"string\">\"日\"</span>+<span class=\"string\">\"星期\"</span>+mWay;</span><br><span class=\"line\"></span><br><span class=\"line\"> FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);</span><br><span class=\"line\"> fab.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View view)</span> </span>&#123;</span><br><span class=\"line\">         Snackbar.make(view, <span class=\"string\">\"今天是\"</span>+Timestring+<span class=\"string\">\"\\n\"</span>+nowtime+<span class=\"string\">\"  \"</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">                 .setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure><h1 id=\"在GitHub上学到的\"><a href=\"#在GitHub上学到的\" class=\"headerlink\" title=\"在GitHub上学到的\"></a>在GitHub上学到的</h1><p>此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等<br>部分GitHub repository链接在这里</p><ul><li>滑动卡片界面：<a href=\"https://github.com/romannurik/Android-SwipeToDismiss\" target=\"_blank\" rel=\"noopener\">Android-SwipeToDismiss</a></li><li>fir更新模块:<a href=\"https://github.com/hugeterry/UpdateDemo\" target=\"_blank\" rel=\"noopener\">UpdateDemo</a></li></ul><p>还有一些直接写在代码里了，忘记原地址了….</p><ul><li>摇一摇的传感器调用<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeService</span> <span class=\"keyword\">extends</span> <span class=\"title\">Service</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> String TAG = <span class=\"string\">\"ShakeService\"</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SensorManager mSensorManager;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> ShakeBinder shakebinder= <span class=\"keyword\">new</span> ShakeBinder();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String htmlbody=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate();</span><br><span class=\"line\">        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Create\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onDestroy</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onDestroy();</span><br><span class=\"line\">        mSensorManager.unregisterListener(mShakeListener);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onStart</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onStart(intent, startId);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Start\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">onStartCommand</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> flags, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        mSensorManager.registerListener(mShakeListener,</span><br><span class=\"line\">                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class=\"line\">                <span class=\"comment\">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class=\"line\">                <span class=\"number\">50</span> * <span class=\"number\">1000</span>); <span class=\"comment\">//batch every 50 milliseconds</span></span><br><span class=\"line\">        htmlbody=intent.getStringExtra(<span class=\"string\">\"htmlbody\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.onStartCommand(intent, flags, startId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SensorEventListener mShakeListener = <span class=\"keyword\">new</span> SensorEventListener() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> SENSITIVITY = <span class=\"number\">10</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BUFFER = <span class=\"number\">5</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span>[] gravity = <span class=\"keyword\">new</span> <span class=\"keyword\">float</span>[<span class=\"number\">3</span>];</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span> average = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> fill = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onAccuracyChanged</span><span class=\"params\">(Sensor sensor, <span class=\"keyword\">int</span> acc)</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onSensorChanged</span><span class=\"params\">(SensorEvent event)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> alpha = <span class=\"number\">0.8F</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; i++) &#123;</span><br><span class=\"line\">                gravity[i] = alpha * gravity[i] + (<span class=\"number\">1</span> - alpha) * event.values[i];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">float</span> x = event.values[<span class=\"number\">0</span>] - gravity[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> y = event.values[<span class=\"number\">1</span>] - gravity[<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> z = event.values[<span class=\"number\">2</span>] - gravity[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class=\"line\">                average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class=\"line\">                fill++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average:\"</span>+average);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average / BUFFER:\"</span>+(average / BUFFER));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class=\"line\">                    handleShakeAction();<span class=\"comment\">//如果达到阈值则处理摇一摇响应</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                average = <span class=\"number\">0</span>;</span><br><span class=\"line\">                fill = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">handleShakeAction</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">true</span>;</span><br><span class=\"line\">        Toast.makeText(getApplicationContext(), <span class=\"string\">\"摇一摇成功\"</span>, Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">        Intent intent= <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">        intent.putExtra(<span class=\"string\">\"htmlbody\"</span>,htmlbody);</span><br><span class=\"line\">        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class=\"line\">        intent.setClassName(<span class=\"keyword\">this</span>,<span class=\"string\">\"thinkwee.buptroom.ShakeTestActivity\"</span>);</span><br><span class=\"line\">        startActivity(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> IBinder <span class=\"title\">onBind</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> shakebinder;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeBinder</span> <span class=\"keyword\">extends</span> <span class=\"title\">Binder</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"独立网络拉取，并使用多线程\"><a href=\"#独立网络拉取，并使用多线程\" class=\"headerlink\" title=\"独立网络拉取，并使用多线程\"></a>独立网络拉取，并使用多线程</h1><ul><li>在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用</li><li>然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    webget = <span class=\"keyword\">new</span> Webget();</span><br><span class=\"line\">    webget.init(webView);</span><br><span class=\"line\">    HaveNetFlag = webget.WebInit();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class=\"line\">            Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.<span class=\"keyword\">this</span>, R.anim.enlarge);</span><br><span class=\"line\">            animation.setFillAfter(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">            img.startAnimation(animation);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">50</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            WrongNet = webget.getWrongnet();</span><br><span class=\"line\">            HaveNetFlag = webget.getHaveNetFlag();</span><br><span class=\"line\">            htmlbody = webget.getHtmlbody();</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2HaveNetFlag: \"</span> + HaveNetFlag);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2Wrongnet: \"</span> + WrongNet);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2html: \"</span> + htmlbody);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            Intent intent = <span class=\"keyword\">new</span> Intent(WelcomeActivity.<span class=\"keyword\">this</span>, MainActivity.class);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"WrongNet\"</span>, WrongNet);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"HtmlBody\"</span>, htmlbody);</span><br><span class=\"line\">            startActivity(intent);</span><br><span class=\"line\">            WelcomeActivity.<span class=\"keyword\">this</span>.finish();</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;, <span class=\"number\">2500</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Android:BuptRoom总结","path":"2017/01/16/buptroomreview/","eyeCatchImage":null,"excerpt":"<hr><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/17-1-16/24437570-file_1484550130084_2fc4.png\" alt=\"\"></p><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p>","date":"2017-01-16T03:56:39.000Z","pv":0,"totalPV":0,"categories":"Android","tags":["code","android"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Melodia服务器搭建","date":"2017-05-26T11:18:01.000Z","_content":"***\n-\t大创项目的服务器端，大创以及客户端介绍见[Melodia客户端](http://thinkwee.top/2017/03/09/dachuang/)\n-\t我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端\n\n<!--more-->\n\n# 功能\n-\t从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程\n-\t客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端\n\n# 代码\n```Python\n\t# -*- coding:utf-8 -*- \n\t# ! usr/bin/python\n\n\tfrom socket import *\n\timport time\n\timport threading\n\timport os\n\timport md5\n\timport warnings\n\n\tHost = ''\n\tPort = 2017\n\tAddr = (Host, Port)\n\tmidi_dict = {}\n\n\twarnings.filterwarnings(\"ignore\")\n\n\n\tdef md5_encode(src):\n\t\tm1 = md5.new()\n\t\tm1.update(src)\n\t\treturn m1.hexdigest()\n\n\n\tdef tcplink(sock, addr):\n\t\tsessnum = 0\n\t\tmusic_data = ''\n\t\twhile True:\n\t\t\tdata = sock.recv(1480)\n\t\t\tif data[-9:]=='endbidou1':\n\t\t\t\tprint 'wav recv finished'\n\t\t\t\tmusic_data+=data\n\t\t\t\tmusic_data=music_data[:-9]\n\t\t\t\tmidi_data = eval(music_data)\n\t\t\tsessnum = midi_data['request']  \n\t\t\t\tif midi_data['request'] == 1:\n\t\t\t\t\tflag_md5 = md5_encode(str(time.time()))\n\t\t\t\t\tprint 'md5: ', flag_md5\n\t\t\t\t\twav_name = flag_md5 + '.wav'\n\t\t\t\t\twith open(wav_name, 'w+') as f:\n\t\t\t\t\t\tf.write(midi_data['data'].decode('base64'))\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tn = midi_data['config']['n'];\n\t\t\t\t\tm = midi_data['config']['m'];\n\t\t\t\t\tw = midi_data['config']['w'];\n\t\t\t\t\tmidi_name = flag_md5 + '.mid'\n\t\t\t\t\twith open(midi_name, 'w') as f:\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tshellmid = '../mldm/hum2midi.py -n '+str(n)+' -m '+str(m)+' -w '+str(w)+' -o ' + midi_name + ' ' + wav_name\n\t\t\tprint \"running wav2midi shell\"\n\t\t\t\t\tretmid = os.system(shellmid)\n\t\t\t\t\tretmid >= 8\n\t\t\t\t\tif retmid == 0:\n\t\t\t\tprint 'generate midi successful'\n\t\t\t\tshellpng = 'mono ../mlds/sheet '+midi_name+' '+flag_md5\n\t\t\t\tretpng = os.system(shellpng)\n\t\t\t\tif retpng == 0:\n\t\t\t\t\t\t\tsock.send(flag_md5.encode())\n\t\t\t\t\t\t\tprint 'generate png successful'\n\t\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\tprint 'generate png error'\n\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'generate midi error'\n\t\t\t\t\t\tbreak\n\t\t\t\telif midi_data['request'] == 2:\n\t\t\t\t\tflag = midi_data['data']\n\t\t\t\t\tif flag in midi_dict.keys():\n\t\t\t\t\t\tfo = open(flag+'.mid', 'rb')\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tfiledata = fo.read(1024)\n\t\t\t\t\t\t\tif not filedata:\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\tsock.send(filedata)\n\t\t\t\tprint 'midi file sent'\n\t\t\t\t\t\tfo.close()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'can not find midi'\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tprint 'json error'\n\t\t\telse:\n\t\t\t\tmusic_data += data\n\t\tsock.close()\n\t\tprint 'session '+str(sessnum)+' for '+str(addr)+' finished'\n\n\ttcpSerSock = socket(AF_INET, SOCK_STREAM)\n\ttcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n\ttcpSerSock.bind(Addr)\n\ttcpSerSock.listen(5)\n\n\twhile True:\n\t\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()\n\t\tprint 'add ', tcpCliAddr\n\t\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))\n\t\tt.start()\n\ttcpSerSock.close()\n```","source":"_posts/dachuangserver.md","raw":"---\ntitle: Melodia服务器搭建\ndate: 2017-05-26 19:18:01\ntags: [code,server,linux]\ncategories: Python\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/20170526/191951436.jpg\n---\n***\n-\t大创项目的服务器端，大创以及客户端介绍见[Melodia客户端](http://thinkwee.top/2017/03/09/dachuang/)\n-\t我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端\n\n<!--more-->\n\n# 功能\n-\t从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程\n-\t客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端\n\n# 代码\n```Python\n\t# -*- coding:utf-8 -*- \n\t# ! usr/bin/python\n\n\tfrom socket import *\n\timport time\n\timport threading\n\timport os\n\timport md5\n\timport warnings\n\n\tHost = ''\n\tPort = 2017\n\tAddr = (Host, Port)\n\tmidi_dict = {}\n\n\twarnings.filterwarnings(\"ignore\")\n\n\n\tdef md5_encode(src):\n\t\tm1 = md5.new()\n\t\tm1.update(src)\n\t\treturn m1.hexdigest()\n\n\n\tdef tcplink(sock, addr):\n\t\tsessnum = 0\n\t\tmusic_data = ''\n\t\twhile True:\n\t\t\tdata = sock.recv(1480)\n\t\t\tif data[-9:]=='endbidou1':\n\t\t\t\tprint 'wav recv finished'\n\t\t\t\tmusic_data+=data\n\t\t\t\tmusic_data=music_data[:-9]\n\t\t\t\tmidi_data = eval(music_data)\n\t\t\tsessnum = midi_data['request']  \n\t\t\t\tif midi_data['request'] == 1:\n\t\t\t\t\tflag_md5 = md5_encode(str(time.time()))\n\t\t\t\t\tprint 'md5: ', flag_md5\n\t\t\t\t\twav_name = flag_md5 + '.wav'\n\t\t\t\t\twith open(wav_name, 'w+') as f:\n\t\t\t\t\t\tf.write(midi_data['data'].decode('base64'))\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tn = midi_data['config']['n'];\n\t\t\t\t\tm = midi_data['config']['m'];\n\t\t\t\t\tw = midi_data['config']['w'];\n\t\t\t\t\tmidi_name = flag_md5 + '.mid'\n\t\t\t\t\twith open(midi_name, 'w') as f:\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tshellmid = '../mldm/hum2midi.py -n '+str(n)+' -m '+str(m)+' -w '+str(w)+' -o ' + midi_name + ' ' + wav_name\n\t\t\tprint \"running wav2midi shell\"\n\t\t\t\t\tretmid = os.system(shellmid)\n\t\t\t\t\tretmid >= 8\n\t\t\t\t\tif retmid == 0:\n\t\t\t\tprint 'generate midi successful'\n\t\t\t\tshellpng = 'mono ../mlds/sheet '+midi_name+' '+flag_md5\n\t\t\t\tretpng = os.system(shellpng)\n\t\t\t\tif retpng == 0:\n\t\t\t\t\t\t\tsock.send(flag_md5.encode())\n\t\t\t\t\t\t\tprint 'generate png successful'\n\t\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\tprint 'generate png error'\n\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'generate midi error'\n\t\t\t\t\t\tbreak\n\t\t\t\telif midi_data['request'] == 2:\n\t\t\t\t\tflag = midi_data['data']\n\t\t\t\t\tif flag in midi_dict.keys():\n\t\t\t\t\t\tfo = open(flag+'.mid', 'rb')\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tfiledata = fo.read(1024)\n\t\t\t\t\t\t\tif not filedata:\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\tsock.send(filedata)\n\t\t\t\tprint 'midi file sent'\n\t\t\t\t\t\tfo.close()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'can not find midi'\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tprint 'json error'\n\t\t\telse:\n\t\t\t\tmusic_data += data\n\t\tsock.close()\n\t\tprint 'session '+str(sessnum)+' for '+str(addr)+' finished'\n\n\ttcpSerSock = socket(AF_INET, SOCK_STREAM)\n\ttcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n\ttcpSerSock.bind(Addr)\n\ttcpSerSock.listen(5)\n\n\twhile True:\n\t\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()\n\t\tprint 'add ', tcpCliAddr\n\t\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))\n\t\tt.start()\n\ttcpSerSock.close()\n```","slug":"dachuangserver","published":1,"updated":"2018-07-23T01:28:38.502Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170526/191951436.jpg"],"comments":1,"layout":"post","link":"","_id":"cjmd072cy0019qcw6dcpt8kzu","content":"<hr><ul><li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li><li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li></ul><a id=\"more\"></a><h1 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h1><ul><li>从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程</li><li>客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端</li></ul><h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*- </span></span><br><span class=\"line\"><span class=\"comment\"># ! usr/bin/python</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> socket <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> threading</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> md5</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">Host = <span class=\"string\">''</span></span><br><span class=\"line\">Port = <span class=\"number\">2017</span></span><br><span class=\"line\">Addr = (Host, Port)</span><br><span class=\"line\">midi_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">\"ignore\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">md5_encode</span><span class=\"params\">(src)</span>:</span></span><br><span class=\"line\">\tm1 = md5.new()</span><br><span class=\"line\">\tm1.update(src)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> m1.hexdigest()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tcplink</span><span class=\"params\">(sock, addr)</span>:</span></span><br><span class=\"line\">\tsessnum = <span class=\"number\">0</span></span><br><span class=\"line\">\tmusic_data = <span class=\"string\">''</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\tdata = sock.recv(<span class=\"number\">1480</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> data[<span class=\"number\">-9</span>:]==<span class=\"string\">'endbidou1'</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'wav recv finished'</span></span><br><span class=\"line\">\t\t\tmusic_data+=data</span><br><span class=\"line\">\t\t\tmusic_data=music_data[:<span class=\"number\">-9</span>]</span><br><span class=\"line\">\t\t\tmidi_data = eval(music_data)</span><br><span class=\"line\">\t\tsessnum = midi_data[<span class=\"string\">'request'</span>]  </span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\t\t\tflag_md5 = md5_encode(str(time.time()))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'md5: '</span>, flag_md5</span><br><span class=\"line\">\t\t\t\twav_name = flag_md5 + <span class=\"string\">'.wav'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(wav_name, <span class=\"string\">'w+'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.write(midi_data[<span class=\"string\">'data'</span>].decode(<span class=\"string\">'base64'</span>))</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tn = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'n'</span>];</span><br><span class=\"line\">\t\t\t\tm = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'m'</span>];</span><br><span class=\"line\">\t\t\t\tw = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'w'</span>];</span><br><span class=\"line\">\t\t\t\tmidi_name = flag_md5 + <span class=\"string\">'.mid'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(midi_name, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tshellmid = <span class=\"string\">'../mldm/hum2midi.py -n '</span>+str(n)+<span class=\"string\">' -m '</span>+str(m)+<span class=\"string\">' -w '</span>+str(w)+<span class=\"string\">' -o '</span> + midi_name + <span class=\"string\">' '</span> + wav_name</span><br><span class=\"line\">\t\t<span class=\"keyword\">print</span> <span class=\"string\">\"running wav2midi shell\"</span></span><br><span class=\"line\">\t\t\t\tretmid = os.system(shellmid)</span><br><span class=\"line\">\t\t\t\tretmid &gt;= <span class=\"number\">8</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> retmid == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi successful'</span></span><br><span class=\"line\">\t\t\tshellpng = <span class=\"string\">'mono ../mlds/sheet '</span>+midi_name+<span class=\"string\">' '</span>+flag_md5</span><br><span class=\"line\">\t\t\tretpng = os.system(shellpng)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> retpng == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\t\tsock.send(flag_md5.encode())</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png successful'</span></span><br><span class=\"line\">\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png error'</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi error'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">2</span>:</span><br><span class=\"line\">\t\t\t\tflag = midi_data[<span class=\"string\">'data'</span>]</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> flag <span class=\"keyword\">in</span> midi_dict.keys():</span><br><span class=\"line\">\t\t\t\t\tfo = open(flag+<span class=\"string\">'.mid'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t\t\t\t\tfiledata = fo.read(<span class=\"number\">1024</span>)</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> filedata:</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t\t\tsock.send(filedata)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'midi file sent'</span></span><br><span class=\"line\">\t\t\t\t\tfo.close()</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'can not find midi'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'json error'</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\tmusic_data += data</span><br><span class=\"line\">\tsock.close()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'session '</span>+str(sessnum)+<span class=\"string\">' for '</span>+str(addr)+<span class=\"string\">' finished'</span></span><br><span class=\"line\"></span><br><span class=\"line\">tcpSerSock = socket(AF_INET, SOCK_STREAM)</span><br><span class=\"line\">tcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, <span class=\"number\">1</span>)</span><br><span class=\"line\">tcpSerSock.bind(Addr)</span><br><span class=\"line\">tcpSerSock.listen(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'add '</span>, tcpCliAddr</span><br><span class=\"line\">\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))</span><br><span class=\"line\">\tt.start()</span><br><span class=\"line\">tcpSerSock.close()</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<hr><ul><li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li><li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li></ul>","more":"<h1 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h1><ul><li>从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程</li><li>客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端</li></ul><h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*- </span></span><br><span class=\"line\"><span class=\"comment\"># ! usr/bin/python</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> socket <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> threading</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> md5</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">Host = <span class=\"string\">''</span></span><br><span class=\"line\">Port = <span class=\"number\">2017</span></span><br><span class=\"line\">Addr = (Host, Port)</span><br><span class=\"line\">midi_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">\"ignore\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">md5_encode</span><span class=\"params\">(src)</span>:</span></span><br><span class=\"line\">\tm1 = md5.new()</span><br><span class=\"line\">\tm1.update(src)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> m1.hexdigest()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tcplink</span><span class=\"params\">(sock, addr)</span>:</span></span><br><span class=\"line\">\tsessnum = <span class=\"number\">0</span></span><br><span class=\"line\">\tmusic_data = <span class=\"string\">''</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\tdata = sock.recv(<span class=\"number\">1480</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> data[<span class=\"number\">-9</span>:]==<span class=\"string\">'endbidou1'</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'wav recv finished'</span></span><br><span class=\"line\">\t\t\tmusic_data+=data</span><br><span class=\"line\">\t\t\tmusic_data=music_data[:<span class=\"number\">-9</span>]</span><br><span class=\"line\">\t\t\tmidi_data = eval(music_data)</span><br><span class=\"line\">\t\tsessnum = midi_data[<span class=\"string\">'request'</span>]  </span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\t\t\tflag_md5 = md5_encode(str(time.time()))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'md5: '</span>, flag_md5</span><br><span class=\"line\">\t\t\t\twav_name = flag_md5 + <span class=\"string\">'.wav'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(wav_name, <span class=\"string\">'w+'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.write(midi_data[<span class=\"string\">'data'</span>].decode(<span class=\"string\">'base64'</span>))</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tn = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'n'</span>];</span><br><span class=\"line\">\t\t\t\tm = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'m'</span>];</span><br><span class=\"line\">\t\t\t\tw = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'w'</span>];</span><br><span class=\"line\">\t\t\t\tmidi_name = flag_md5 + <span class=\"string\">'.mid'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(midi_name, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tshellmid = <span class=\"string\">'../mldm/hum2midi.py -n '</span>+str(n)+<span class=\"string\">' -m '</span>+str(m)+<span class=\"string\">' -w '</span>+str(w)+<span class=\"string\">' -o '</span> + midi_name + <span class=\"string\">' '</span> + wav_name</span><br><span class=\"line\">\t\t<span class=\"keyword\">print</span> <span class=\"string\">\"running wav2midi shell\"</span></span><br><span class=\"line\">\t\t\t\tretmid = os.system(shellmid)</span><br><span class=\"line\">\t\t\t\tretmid &gt;= <span class=\"number\">8</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> retmid == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi successful'</span></span><br><span class=\"line\">\t\t\tshellpng = <span class=\"string\">'mono ../mlds/sheet '</span>+midi_name+<span class=\"string\">' '</span>+flag_md5</span><br><span class=\"line\">\t\t\tretpng = os.system(shellpng)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> retpng == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\t\tsock.send(flag_md5.encode())</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png successful'</span></span><br><span class=\"line\">\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png error'</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi error'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">2</span>:</span><br><span class=\"line\">\t\t\t\tflag = midi_data[<span class=\"string\">'data'</span>]</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> flag <span class=\"keyword\">in</span> midi_dict.keys():</span><br><span class=\"line\">\t\t\t\t\tfo = open(flag+<span class=\"string\">'.mid'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\t\t\t\t\t\tfiledata = fo.read(<span class=\"number\">1024</span>)</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> filedata:</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t\t\tsock.send(filedata)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'midi file sent'</span></span><br><span class=\"line\">\t\t\t\t\tfo.close()</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'can not find midi'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'json error'</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\tmusic_data += data</span><br><span class=\"line\">\tsock.close()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'session '</span>+str(sessnum)+<span class=\"string\">' for '</span>+str(addr)+<span class=\"string\">' finished'</span></span><br><span class=\"line\"></span><br><span class=\"line\">tcpSerSock = socket(AF_INET, SOCK_STREAM)</span><br><span class=\"line\">tcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, <span class=\"number\">1</span>)</span><br><span class=\"line\">tcpSerSock.bind(Addr)</span><br><span class=\"line\">tcpSerSock.listen(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> <span class=\"keyword\">True</span>:</span><br><span class=\"line\">\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'add '</span>, tcpCliAddr</span><br><span class=\"line\">\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))</span><br><span class=\"line\">\tt.start()</span><br><span class=\"line\">tcpSerSock.close()</span><br></pre></td></tr></table></figure>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Melodia服务器搭建","path":"2017/05/26/dachuangserver/","eyeCatchImage":null,"excerpt":"<hr><ul><li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li><li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li></ul>","date":"2017-05-26T11:18:01.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","server","linux"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"行列式点过程学习笔记","date":"2018-09-14T12:17:29.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180914/hJk7lgJGJb.JPG"],"html":true,"_content":"研究一下行列式点过程，这是一种广泛应用的确保diversity的一种数学方法\n估计从此以后封面就用这种老E灵魂画风\nDPP是结合了实分析、矩阵计算和概率计算的一种有效、优雅的算法，最广为流传的是Ben Taskar在2012作出的Determinantal point processes for machine learning，在videoslectures.net有一段半小时的视频，另外还有120页的pdf数学推导和一份250页的tutorial，可惜大神在2013年英年早逝，2017年在youtube上Wray Buntine教授根据这些材料有一段讲课视频，教授研究领域也是自动文摘，这份视频也值得推荐。\n参考：\n-\tDeterminantal point processes for machine learning ppt&pdf (Alex Kulesza , Ben Taskar)\n-\tDeterminantal point processes (Laurent Decreusefond , Ian Flint , Nicolas Privault)\n-\tDeterminantal Point Process and its Time-varying model (A/Prof Richard Yi Da Xu)\n-\tk-DPPs: Fixed-Size Determinantal Point Processes (Alex Kulesza , Ben Taskar)\n-\tOn adding a list of numbers (and other one-dependent determinantal processes) (Alexei Borodin , Persi Diaconis , Jason Fulman)\n-\tDeterminantal point processes (Alexei Borodin)\n\n<!--more-->\n\n# 一个例子：单依赖行列式点过程\n\n## 例子\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180914/6k9chBkjFc.png?imageslim)\n-\t考虑上图所示一个简单的问题，52是左边一列数字相加之和，右边是对应累加到此行时和的个位数，例如(7+9)%10=6，所以9的右边是6，加点代表着从此行到下一行累加和会进位。\n-\t我们每次只加0到9这个范围内的数，那么可以得到一系列进位的序列，例如上图，在第1，2，4，7，8次加法时进位。考虑每次我们加的数是均匀分布在[0,9]的，那么这些进位是如何分布的？一般n次加法里会有几次进位？至少有两点我们从直觉上可以确定：\n\t-\t一般n次加法里有一半的进位\n\t-\t如果本次进位了，那么下一次就不太可能进位\n-\t我们进一步观察上图中的例子：\n\t-\t如果某一行有点，也就是即将进位，那么此行右边的数到下一行会变少，也就是发生了下降（descent）\n\t-\t如果左边一列的数字是相互独立且均匀分布的，那么右边一列也是独立且均匀分布\n\t-\t综上，我们可以知道能从进位的分布推测出一个随机数字序列的下降模式（descent pattern）\n\n## 从概率来描述进位和下降\n-\t定义集合$B={0,1,2,....,b-1}$，从中独立同分布抽取一个序列$B_1,B_2,...,B_n$，假如$B_i > B_{i+1}$，就说在位置i发生了下降，对应的$X_i$置为1，否则为0，所有下降的位置组成集合$D=\\{i:X_i=1\\}$\n-\tfact 1\n$$\n\\forall i \\in [n-1] \\\\\nP(X_i=1) = \\frac 12 - \\frac{1}{2b} \\\\\n= \\frac {C_b^2}{b^2} \\\\\nVar(X_i)=\\frac 14 - \\frac{1}{4b^2} \\\\\n$$\n-\tfact 2\n$$\n\\forall i,j \\ with \\ 1 \\leq i < i+j \\leq n \\\\\nP(X_i=X_{i+1}=...=X_{i+j-1}=1) = \\frac {C_b^{j+1}}{b^{j+1}} \\\\ \nCov(X_i,X_{i+1})=-\\frac {1}{12}(1-\\frac{1}{b^2}) \\\\\n$$\n-\tfact 3，${X_i}$的分布是稳定单依赖的\n\t-\t稳定：是因为在有效范围内，$X_i$和$X_{i+j}$的分布一样，也就是与其位置无关。\n\t-\t单依赖：只要不是相邻（距离大于1），则${X_i}$之间是独立的。\n-\t利用以上结论，我们可以发现进位的分布只和进位的基相关，且可以计算进位总个数的均值和方差\n-\tfact 4，k点相关性：在[n-1]里取一个大小为k的子集，则在这个子集内的$P(X_i=1)$为点过程$X_1,...,X_{n-1}$的k点相关性。k点相关性是描述一般点过程的基本单元,记作$\\rho (A)$:\n$$\n\\rho (A) = \\prod _1 ^k [\\frac{C_b^{a_{i+1}}}{b^{a_{i+1}}}] \n$$\n-\tfact 5，重要的来了，所有的$X_i$的联合分布可以用一个行列式乘一个系数表示，假如总共有k个$X_i$是1，那么行列式是k+1行k+1列的矩阵的行列式，假设这k个1的位置是$s_1,...,s_k$，且定义$s_0=0,s_{k+1}=n$，则矩阵中第i行第j列的值为：\n$$\nC_{s_{j+1}-s_i+b-1}^{b-1}\n$$\n-\t系数是$\\frac{1}{b^n}$，即\n$$\np(X) = \\frac{1}{b^n} det(C_{s_{j+1}-s_i+b-1}^{b-1})\n$$\n-\t一个例子如下，进位的基为2，加8个数，在第一和第五个位置发生下降的情况的概率为0.03516：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180915/IGF2j3CjAj.JPG)\n\n## 来吧，行列式点过程\n-\t令X为一个有限集，在X上的一个点过程是指X上的$2^{|X|}$个子集的概率测度P，例如，$X={1,2,...,n-1}$，那么点过程就是记录在加上n个以b为基的数的过程中何时出现进位，也就是上一节中我们讨论的。P我们也可以通过相关函数$\\rho$来明确：\n$$\n\\rho (A) = P\\{S:A \\in S\\}\n$$\n-\t当一个点过程可以用核方法$K(x,y)$来表示时，就说这个点过程是行列式的：\n$$\n\\rho (A) = det(K(x,y))_{x,y \\in A}\n$$\n-\t行列式是一个$|A|x|A|$大小的矩阵的行列式。\n## 稳定单依赖过程-进位和下降模式，对称函数理论\n## 混合例子\n## 泛化：从加法到乘法\n## 单依赖行列式家族","source":"_posts/dpp.md","raw":"---\ntitle: 行列式点过程学习笔记\ndate: 2018-09-14 20:17:29\ncategories: 数学\ntags:\n  - dpp\n  - math\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180914/hJk7lgJGJb.JPG\nhtml: true\n---\n研究一下行列式点过程，这是一种广泛应用的确保diversity的一种数学方法\n估计从此以后封面就用这种老E灵魂画风\nDPP是结合了实分析、矩阵计算和概率计算的一种有效、优雅的算法，最广为流传的是Ben Taskar在2012作出的Determinantal point processes for machine learning，在videoslectures.net有一段半小时的视频，另外还有120页的pdf数学推导和一份250页的tutorial，可惜大神在2013年英年早逝，2017年在youtube上Wray Buntine教授根据这些材料有一段讲课视频，教授研究领域也是自动文摘，这份视频也值得推荐。\n参考：\n-\tDeterminantal point processes for machine learning ppt&pdf (Alex Kulesza , Ben Taskar)\n-\tDeterminantal point processes (Laurent Decreusefond , Ian Flint , Nicolas Privault)\n-\tDeterminantal Point Process and its Time-varying model (A/Prof Richard Yi Da Xu)\n-\tk-DPPs: Fixed-Size Determinantal Point Processes (Alex Kulesza , Ben Taskar)\n-\tOn adding a list of numbers (and other one-dependent determinantal processes) (Alexei Borodin , Persi Diaconis , Jason Fulman)\n-\tDeterminantal point processes (Alexei Borodin)\n\n<!--more-->\n\n# 一个例子：单依赖行列式点过程\n\n## 例子\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180914/6k9chBkjFc.png?imageslim)\n-\t考虑上图所示一个简单的问题，52是左边一列数字相加之和，右边是对应累加到此行时和的个位数，例如(7+9)%10=6，所以9的右边是6，加点代表着从此行到下一行累加和会进位。\n-\t我们每次只加0到9这个范围内的数，那么可以得到一系列进位的序列，例如上图，在第1，2，4，7，8次加法时进位。考虑每次我们加的数是均匀分布在[0,9]的，那么这些进位是如何分布的？一般n次加法里会有几次进位？至少有两点我们从直觉上可以确定：\n\t-\t一般n次加法里有一半的进位\n\t-\t如果本次进位了，那么下一次就不太可能进位\n-\t我们进一步观察上图中的例子：\n\t-\t如果某一行有点，也就是即将进位，那么此行右边的数到下一行会变少，也就是发生了下降（descent）\n\t-\t如果左边一列的数字是相互独立且均匀分布的，那么右边一列也是独立且均匀分布\n\t-\t综上，我们可以知道能从进位的分布推测出一个随机数字序列的下降模式（descent pattern）\n\n## 从概率来描述进位和下降\n-\t定义集合$B={0,1,2,....,b-1}$，从中独立同分布抽取一个序列$B_1,B_2,...,B_n$，假如$B_i > B_{i+1}$，就说在位置i发生了下降，对应的$X_i$置为1，否则为0，所有下降的位置组成集合$D=\\{i:X_i=1\\}$\n-\tfact 1\n$$\n\\forall i \\in [n-1] \\\\\nP(X_i=1) = \\frac 12 - \\frac{1}{2b} \\\\\n= \\frac {C_b^2}{b^2} \\\\\nVar(X_i)=\\frac 14 - \\frac{1}{4b^2} \\\\\n$$\n-\tfact 2\n$$\n\\forall i,j \\ with \\ 1 \\leq i < i+j \\leq n \\\\\nP(X_i=X_{i+1}=...=X_{i+j-1}=1) = \\frac {C_b^{j+1}}{b^{j+1}} \\\\ \nCov(X_i,X_{i+1})=-\\frac {1}{12}(1-\\frac{1}{b^2}) \\\\\n$$\n-\tfact 3，${X_i}$的分布是稳定单依赖的\n\t-\t稳定：是因为在有效范围内，$X_i$和$X_{i+j}$的分布一样，也就是与其位置无关。\n\t-\t单依赖：只要不是相邻（距离大于1），则${X_i}$之间是独立的。\n-\t利用以上结论，我们可以发现进位的分布只和进位的基相关，且可以计算进位总个数的均值和方差\n-\tfact 4，k点相关性：在[n-1]里取一个大小为k的子集，则在这个子集内的$P(X_i=1)$为点过程$X_1,...,X_{n-1}$的k点相关性。k点相关性是描述一般点过程的基本单元,记作$\\rho (A)$:\n$$\n\\rho (A) = \\prod _1 ^k [\\frac{C_b^{a_{i+1}}}{b^{a_{i+1}}}] \n$$\n-\tfact 5，重要的来了，所有的$X_i$的联合分布可以用一个行列式乘一个系数表示，假如总共有k个$X_i$是1，那么行列式是k+1行k+1列的矩阵的行列式，假设这k个1的位置是$s_1,...,s_k$，且定义$s_0=0,s_{k+1}=n$，则矩阵中第i行第j列的值为：\n$$\nC_{s_{j+1}-s_i+b-1}^{b-1}\n$$\n-\t系数是$\\frac{1}{b^n}$，即\n$$\np(X) = \\frac{1}{b^n} det(C_{s_{j+1}-s_i+b-1}^{b-1})\n$$\n-\t一个例子如下，进位的基为2，加8个数，在第一和第五个位置发生下降的情况的概率为0.03516：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180915/IGF2j3CjAj.JPG)\n\n## 来吧，行列式点过程\n-\t令X为一个有限集，在X上的一个点过程是指X上的$2^{|X|}$个子集的概率测度P，例如，$X={1,2,...,n-1}$，那么点过程就是记录在加上n个以b为基的数的过程中何时出现进位，也就是上一节中我们讨论的。P我们也可以通过相关函数$\\rho$来明确：\n$$\n\\rho (A) = P\\{S:A \\in S\\}\n$$\n-\t当一个点过程可以用核方法$K(x,y)$来表示时，就说这个点过程是行列式的：\n$$\n\\rho (A) = det(K(x,y))_{x,y \\in A}\n$$\n-\t行列式是一个$|A|x|A|$大小的矩阵的行列式。\n## 稳定单依赖过程-进位和下降模式，对称函数理论\n## 混合例子\n## 泛化：从加法到乘法\n## 单依赖行列式家族","slug":"dpp","published":1,"updated":"2018-09-15T13:42:35.426Z","comments":1,"layout":"post","link":"","_id":"cjmd072d0001dqcw6c7l4tbo5","content":"<p>研究一下行列式点过程，这是一种广泛应用的确保diversity的一种数学方法<br>估计从此以后封面就用这种老E灵魂画风<br>DPP是结合了实分析、矩阵计算和概率计算的一种有效、优雅的算法，最广为流传的是Ben Taskar在2012作出的Determinantal point processes for machine learning，在videoslectures.net有一段半小时的视频，另外还有120页的pdf数学推导和一份250页的tutorial，可惜大神在2013年英年早逝，2017年在youtube上Wray Buntine教授根据这些材料有一段讲课视频，教授研究领域也是自动文摘，这份视频也值得推荐。<br>参考：</p><ul><li>Determinantal point processes for machine learning ppt&amp;pdf (Alex Kulesza , Ben Taskar)</li><li>Determinantal point processes (Laurent Decreusefond , Ian Flint , Nicolas Privault)</li><li>Determinantal Point Process and its Time-varying model (A/Prof Richard Yi Da Xu)</li><li>k-DPPs: Fixed-Size Determinantal Point Processes (Alex Kulesza , Ben Taskar)</li><li>On adding a list of numbers (and other one-dependent determinantal processes) (Alexei Borodin , Persi Diaconis , Jason Fulman)</li><li>Determinantal point processes (Alexei Borodin)</li></ul><a id=\"more\"></a><h1 id=\"一个例子：单依赖行列式点过程\"><a href=\"#一个例子：单依赖行列式点过程\" class=\"headerlink\" title=\"一个例子：单依赖行列式点过程\"></a>一个例子：单依赖行列式点过程</h1><h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180914/6k9chBkjFc.png?imageslim\" alt=\"mark\"></p><ul><li>考虑上图所示一个简单的问题，52是左边一列数字相加之和，右边是对应累加到此行时和的个位数，例如(7+9)%10=6，所以9的右边是6，加点代表着从此行到下一行累加和会进位。</li><li>我们每次只加0到9这个范围内的数，那么可以得到一系列进位的序列，例如上图，在第1，2，4，7，8次加法时进位。考虑每次我们加的数是均匀分布在[0,9]的，那么这些进位是如何分布的？一般n次加法里会有几次进位？至少有两点我们从直觉上可以确定：<ul><li>一般n次加法里有一半的进位</li><li>如果本次进位了，那么下一次就不太可能进位</li></ul></li><li>我们进一步观察上图中的例子：<ul><li>如果某一行有点，也就是即将进位，那么此行右边的数到下一行会变少，也就是发生了下降（descent）</li><li>如果左边一列的数字是相互独立且均匀分布的，那么右边一列也是独立且均匀分布</li><li>综上，我们可以知道能从进位的分布推测出一个随机数字序列的下降模式（descent pattern）</li></ul></li></ul><h2 id=\"从概率来描述进位和下降\"><a href=\"#从概率来描述进位和下降\" class=\"headerlink\" title=\"从概率来描述进位和下降\"></a>从概率来描述进位和下降</h2><ul><li>定义集合$B={0,1,2,….,b-1}$，从中独立同分布抽取一个序列$B_1,B_2,…,B_n$，假如$B_i &gt; B_{i+1}$，就说在位置i发生了下降，对应的$X_i$置为1，否则为0，所有下降的位置组成集合$D=\\{i:X_i=1\\}$</li><li>fact 1<br>$$<br>\\forall i \\in [n-1] \\\\<br>P(X_i=1) = \\frac 12 - \\frac{1}{2b} \\\\<br>= \\frac {C_b^2}{b^2} \\\\<br>Var(X_i)=\\frac 14 - \\frac{1}{4b^2} \\\\<br>$$</li><li>fact 2<br>$$<br>\\forall i,j \\ with \\ 1 \\leq i &lt; i+j \\leq n \\\\<br>P(X_i=X_{i+1}=…=X_{i+j-1}=1) = \\frac {C_b^{j+1}}{b^{j+1}} \\\\<br>Cov(X_i,X_{i+1})=-\\frac {1}{12}(1-\\frac{1}{b^2}) \\\\<br>$$</li><li>fact 3，${X_i}$的分布是稳定单依赖的<ul><li>稳定：是因为在有效范围内，$X_i$和$X_{i+j}$的分布一样，也就是与其位置无关。</li><li>单依赖：只要不是相邻（距离大于1），则${X_i}$之间是独立的。</li></ul></li><li>利用以上结论，我们可以发现进位的分布只和进位的基相关，且可以计算进位总个数的均值和方差</li><li>fact 4，k点相关性：在[n-1]里取一个大小为k的子集，则在这个子集内的$P(X_i=1)$为点过程$X_1,…,X_{n-1}$的k点相关性。k点相关性是描述一般点过程的基本单元,记作$\\rho (A)$:<br>$$<br>\\rho (A) = \\prod _1 ^k [\\frac{C_b^{a_{i+1}}}{b^{a_{i+1}}}]<br>$$</li><li>fact 5，重要的来了，所有的$X_i$的联合分布可以用一个行列式乘一个系数表示，假如总共有k个$X_i$是1，那么行列式是k+1行k+1列的矩阵的行列式，假设这k个1的位置是$s_1,…,s_k$，且定义$s_0=0,s_{k+1}=n$，则矩阵中第i行第j列的值为：<br>$$<br>C_{s_{j+1}-s_i+b-1}^{b-1}<br>$$</li><li>系数是$\\frac{1}{b^n}$，即<br>$$<br>p(X) = \\frac{1}{b^n} det(C_{s_{j+1}-s_i+b-1}^{b-1})<br>$$</li><li>一个例子如下，进位的基为2，加8个数，在第一和第五个位置发生下降的情况的概率为0.03516：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180915/IGF2j3CjAj.JPG\" alt=\"mark\"></li></ul><h2 id=\"来吧，行列式点过程\"><a href=\"#来吧，行列式点过程\" class=\"headerlink\" title=\"来吧，行列式点过程\"></a>来吧，行列式点过程</h2><ul><li>令X为一个有限集，在X上的一个点过程是指X上的$2^{|X|}$个子集的概率测度P，例如，$X={1,2,…,n-1}$，那么点过程就是记录在加上n个以b为基的数的过程中何时出现进位，也就是上一节中我们讨论的。P我们也可以通过相关函数$\\rho$来明确：<br>$$<br>\\rho (A) = P\\{S:A \\in S\\}<br>$$</li><li>当一个点过程可以用核方法$K(x,y)$来表示时，就说这个点过程是行列式的：<br>$$<br>\\rho (A) = det(K(x,y))_{x,y \\in A}<br>$$</li><li>行列式是一个$|A|x|A|$大小的矩阵的行列式。<h2 id=\"稳定单依赖过程-进位和下降模式，对称函数理论\"><a href=\"#稳定单依赖过程-进位和下降模式，对称函数理论\" class=\"headerlink\" title=\"稳定单依赖过程-进位和下降模式，对称函数理论\"></a>稳定单依赖过程-进位和下降模式，对称函数理论</h2><h2 id=\"混合例子\"><a href=\"#混合例子\" class=\"headerlink\" title=\"混合例子\"></a>混合例子</h2><h2 id=\"泛化：从加法到乘法\"><a href=\"#泛化：从加法到乘法\" class=\"headerlink\" title=\"泛化：从加法到乘法\"></a>泛化：从加法到乘法</h2><h2 id=\"单依赖行列式家族\"><a href=\"#单依赖行列式家族\" class=\"headerlink\" title=\"单依赖行列式家族\"></a>单依赖行列式家族</h2></li></ul>","site":{"data":{}},"excerpt":"<p>研究一下行列式点过程，这是一种广泛应用的确保diversity的一种数学方法<br>估计从此以后封面就用这种老E灵魂画风<br>DPP是结合了实分析、矩阵计算和概率计算的一种有效、优雅的算法，最广为流传的是Ben Taskar在2012作出的Determinantal point processes for machine learning，在videoslectures.net有一段半小时的视频，另外还有120页的pdf数学推导和一份250页的tutorial，可惜大神在2013年英年早逝，2017年在youtube上Wray Buntine教授根据这些材料有一段讲课视频，教授研究领域也是自动文摘，这份视频也值得推荐。<br>参考：</p><ul><li>Determinantal point processes for machine learning ppt&amp;pdf (Alex Kulesza , Ben Taskar)</li><li>Determinantal point processes (Laurent Decreusefond , Ian Flint , Nicolas Privault)</li><li>Determinantal Point Process and its Time-varying model (A/Prof Richard Yi Da Xu)</li><li>k-DPPs: Fixed-Size Determinantal Point Processes (Alex Kulesza , Ben Taskar)</li><li>On adding a list of numbers (and other one-dependent determinantal processes) (Alexei Borodin , Persi Diaconis , Jason Fulman)</li><li>Determinantal point processes (Alexei Borodin)</li></ul>","more":"<h1 id=\"一个例子：单依赖行列式点过程\"><a href=\"#一个例子：单依赖行列式点过程\" class=\"headerlink\" title=\"一个例子：单依赖行列式点过程\"></a>一个例子：单依赖行列式点过程</h1><h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180914/6k9chBkjFc.png?imageslim\" alt=\"mark\"></p><ul><li>考虑上图所示一个简单的问题，52是左边一列数字相加之和，右边是对应累加到此行时和的个位数，例如(7+9)%10=6，所以9的右边是6，加点代表着从此行到下一行累加和会进位。</li><li>我们每次只加0到9这个范围内的数，那么可以得到一系列进位的序列，例如上图，在第1，2，4，7，8次加法时进位。考虑每次我们加的数是均匀分布在[0,9]的，那么这些进位是如何分布的？一般n次加法里会有几次进位？至少有两点我们从直觉上可以确定：<ul><li>一般n次加法里有一半的进位</li><li>如果本次进位了，那么下一次就不太可能进位</li></ul></li><li>我们进一步观察上图中的例子：<ul><li>如果某一行有点，也就是即将进位，那么此行右边的数到下一行会变少，也就是发生了下降（descent）</li><li>如果左边一列的数字是相互独立且均匀分布的，那么右边一列也是独立且均匀分布</li><li>综上，我们可以知道能从进位的分布推测出一个随机数字序列的下降模式（descent pattern）</li></ul></li></ul><h2 id=\"从概率来描述进位和下降\"><a href=\"#从概率来描述进位和下降\" class=\"headerlink\" title=\"从概率来描述进位和下降\"></a>从概率来描述进位和下降</h2><ul><li>定义集合$B={0,1,2,….,b-1}$，从中独立同分布抽取一个序列$B_1,B_2,…,B_n$，假如$B_i &gt; B_{i+1}$，就说在位置i发生了下降，对应的$X_i$置为1，否则为0，所有下降的位置组成集合$D=\\{i:X_i=1\\}$</li><li>fact 1<br>$$<br>\\forall i \\in [n-1] \\\\<br>P(X_i=1) = \\frac 12 - \\frac{1}{2b} \\\\<br>= \\frac {C_b^2}{b^2} \\\\<br>Var(X_i)=\\frac 14 - \\frac{1}{4b^2} \\\\<br>$$</li><li>fact 2<br>$$<br>\\forall i,j \\ with \\ 1 \\leq i &lt; i+j \\leq n \\\\<br>P(X_i=X_{i+1}=…=X_{i+j-1}=1) = \\frac {C_b^{j+1}}{b^{j+1}} \\\\<br>Cov(X_i,X_{i+1})=-\\frac {1}{12}(1-\\frac{1}{b^2}) \\\\<br>$$</li><li>fact 3，${X_i}$的分布是稳定单依赖的<ul><li>稳定：是因为在有效范围内，$X_i$和$X_{i+j}$的分布一样，也就是与其位置无关。</li><li>单依赖：只要不是相邻（距离大于1），则${X_i}$之间是独立的。</li></ul></li><li>利用以上结论，我们可以发现进位的分布只和进位的基相关，且可以计算进位总个数的均值和方差</li><li>fact 4，k点相关性：在[n-1]里取一个大小为k的子集，则在这个子集内的$P(X_i=1)$为点过程$X_1,…,X_{n-1}$的k点相关性。k点相关性是描述一般点过程的基本单元,记作$\\rho (A)$:<br>$$<br>\\rho (A) = \\prod _1 ^k [\\frac{C_b^{a_{i+1}}}{b^{a_{i+1}}}]<br>$$</li><li>fact 5，重要的来了，所有的$X_i$的联合分布可以用一个行列式乘一个系数表示，假如总共有k个$X_i$是1，那么行列式是k+1行k+1列的矩阵的行列式，假设这k个1的位置是$s_1,…,s_k$，且定义$s_0=0,s_{k+1}=n$，则矩阵中第i行第j列的值为：<br>$$<br>C_{s_{j+1}-s_i+b-1}^{b-1}<br>$$</li><li>系数是$\\frac{1}{b^n}$，即<br>$$<br>p(X) = \\frac{1}{b^n} det(C_{s_{j+1}-s_i+b-1}^{b-1})<br>$$</li><li>一个例子如下，进位的基为2，加8个数，在第一和第五个位置发生下降的情况的概率为0.03516：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180915/IGF2j3CjAj.JPG\" alt=\"mark\"></li></ul><h2 id=\"来吧，行列式点过程\"><a href=\"#来吧，行列式点过程\" class=\"headerlink\" title=\"来吧，行列式点过程\"></a>来吧，行列式点过程</h2><ul><li>令X为一个有限集，在X上的一个点过程是指X上的$2^{|X|}$个子集的概率测度P，例如，$X={1,2,…,n-1}$，那么点过程就是记录在加上n个以b为基的数的过程中何时出现进位，也就是上一节中我们讨论的。P我们也可以通过相关函数$\\rho$来明确：<br>$$<br>\\rho (A) = P\\{S:A \\in S\\}<br>$$</li><li>当一个点过程可以用核方法$K(x,y)$来表示时，就说这个点过程是行列式的：<br>$$<br>\\rho (A) = det(K(x,y))_{x,y \\in A}<br>$$</li><li>行列式是一个$|A|x|A|$大小的矩阵的行列式。<h2 id=\"稳定单依赖过程-进位和下降模式，对称函数理论\"><a href=\"#稳定单依赖过程-进位和下降模式，对称函数理论\" class=\"headerlink\" title=\"稳定单依赖过程-进位和下降模式，对称函数理论\"></a>稳定单依赖过程-进位和下降模式，对称函数理论</h2><h2 id=\"混合例子\"><a href=\"#混合例子\" class=\"headerlink\" title=\"混合例子\"></a>混合例子</h2><h2 id=\"泛化：从加法到乘法\"><a href=\"#泛化：从加法到乘法\" class=\"headerlink\" title=\"泛化：从加法到乘法\"></a>泛化：从加法到乘法</h2><h2 id=\"单依赖行列式家族\"><a href=\"#单依赖行列式家族\" class=\"headerlink\" title=\"单依赖行列式家族\"></a>单依赖行列式家族</h2></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sat Sep 15 2018 21:42:35 GMT+0800 (中国标准时间)","title":"行列式点过程学习笔记","path":"2018/09/14/dpp/","eyeCatchImage":null,"excerpt":"<p>研究一下行列式点过程，这是一种广泛应用的确保diversity的一种数学方法<br>估计从此以后封面就用这种老E灵魂画风<br>DPP是结合了实分析、矩阵计算和概率计算的一种有效、优雅的算法，最广为流传的是Ben Taskar在2012作出的Determinantal point processes for machine learning，在videoslectures.net有一段半小时的视频，另外还有120页的pdf数学推导和一份250页的tutorial，可惜大神在2013年英年早逝，2017年在youtube上Wray Buntine教授根据这些材料有一段讲课视频，教授研究领域也是自动文摘，这份视频也值得推荐。<br>参考：</p><ul><li>Determinantal point processes for machine learning ppt&amp;pdf (Alex Kulesza , Ben Taskar)</li><li>Determinantal point processes (Laurent Decreusefond , Ian Flint , Nicolas Privault)</li><li>Determinantal Point Process and its Time-varying model (A/Prof Richard Yi Da Xu)</li><li>k-DPPs: Fixed-Size Determinantal Point Processes (Alex Kulesza , Ben Taskar)</li><li>On adding a list of numbers (and other one-dependent determinantal processes) (Alexei Borodin , Persi Diaconis , Jason Fulman)</li><li>Determinantal point processes (Alexei Borodin)</li></ul>","date":"2018-09-14T12:17:29.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","dpp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"HLDA学习笔记","date":"2018-09-03T09:26:25.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180903/J0jk0eb7Gh.PNG"],"html":true,"_content":"***\nHierarchical Latent Dirichlet Allocation 层次文档主题生成模型学习笔记\n还有一些小地方不清楚，等弄清楚了再补全。\n\n<!--more-->\n\n# Dirichlet Process\n\n# Stick-breaking process and GEM Distribution\n\n# Chinese Restaurant Process\n\n# Nested Chinese Restaurant Process\n\n# Hierarchical Latent Dirichlet Allocation\n\n# Hyper Parameters","source":"_posts/hlda.md","raw":"---\ntitle: HLDA学习笔记\ndate: 2018-09-03 17:26:25\ncategories: 机器学习\ntags:\n  - lda\n  - math\n  -\tmcmc\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180903/J0jk0eb7Gh.PNG\nhtml: true\n---\n***\nHierarchical Latent Dirichlet Allocation 层次文档主题生成模型学习笔记\n还有一些小地方不清楚，等弄清楚了再补全。\n\n<!--more-->\n\n# Dirichlet Process\n\n# Stick-breaking process and GEM Distribution\n\n# Chinese Restaurant Process\n\n# Nested Chinese Restaurant Process\n\n# Hierarchical Latent Dirichlet Allocation\n\n# Hyper Parameters","slug":"hlda","published":1,"updated":"2018-09-03T09:39:39.248Z","comments":1,"layout":"post","link":"","_id":"cjmd072d1001hqcw6sweam9cm","content":"<hr><p>Hierarchical Latent Dirichlet Allocation 层次文档主题生成模型学习笔记<br>还有一些小地方不清楚，等弄清楚了再补全。</p><a id=\"more\"></a><h1 id=\"Dirichlet-Process\"><a href=\"#Dirichlet-Process\" class=\"headerlink\" title=\"Dirichlet Process\"></a>Dirichlet Process</h1><h1 id=\"Stick-breaking-process-and-GEM-Distribution\"><a href=\"#Stick-breaking-process-and-GEM-Distribution\" class=\"headerlink\" title=\"Stick-breaking process and GEM Distribution\"></a>Stick-breaking process and GEM Distribution</h1><h1 id=\"Chinese-Restaurant-Process\"><a href=\"#Chinese-Restaurant-Process\" class=\"headerlink\" title=\"Chinese Restaurant Process\"></a>Chinese Restaurant Process</h1><h1 id=\"Nested-Chinese-Restaurant-Process\"><a href=\"#Nested-Chinese-Restaurant-Process\" class=\"headerlink\" title=\"Nested Chinese Restaurant Process\"></a>Nested Chinese Restaurant Process</h1><h1 id=\"Hierarchical-Latent-Dirichlet-Allocation\"><a href=\"#Hierarchical-Latent-Dirichlet-Allocation\" class=\"headerlink\" title=\"Hierarchical Latent Dirichlet Allocation\"></a>Hierarchical Latent Dirichlet Allocation</h1><h1 id=\"Hyper-Parameters\"><a href=\"#Hyper-Parameters\" class=\"headerlink\" title=\"Hyper Parameters\"></a>Hyper Parameters</h1>","site":{"data":{}},"excerpt":"<hr><p>Hierarchical Latent Dirichlet Allocation 层次文档主题生成模型学习笔记<br>还有一些小地方不清楚，等弄清楚了再补全。</p>","more":"<h1 id=\"Dirichlet-Process\"><a href=\"#Dirichlet-Process\" class=\"headerlink\" title=\"Dirichlet Process\"></a>Dirichlet Process</h1><h1 id=\"Stick-breaking-process-and-GEM-Distribution\"><a href=\"#Stick-breaking-process-and-GEM-Distribution\" class=\"headerlink\" title=\"Stick-breaking process and GEM Distribution\"></a>Stick-breaking process and GEM Distribution</h1><h1 id=\"Chinese-Restaurant-Process\"><a href=\"#Chinese-Restaurant-Process\" class=\"headerlink\" title=\"Chinese Restaurant Process\"></a>Chinese Restaurant Process</h1><h1 id=\"Nested-Chinese-Restaurant-Process\"><a href=\"#Nested-Chinese-Restaurant-Process\" class=\"headerlink\" title=\"Nested Chinese Restaurant Process\"></a>Nested Chinese Restaurant Process</h1><h1 id=\"Hierarchical-Latent-Dirichlet-Allocation\"><a href=\"#Hierarchical-Latent-Dirichlet-Allocation\" class=\"headerlink\" title=\"Hierarchical Latent Dirichlet Allocation\"></a>Hierarchical Latent Dirichlet Allocation</h1><h1 id=\"Hyper-Parameters\"><a href=\"#Hyper-Parameters\" class=\"headerlink\" title=\"Hyper Parameters\"></a>Hyper Parameters</h1>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Sep 03 2018 17:39:39 GMT+0800 (中国标准时间)","title":"HLDA学习笔记","path":"2018/09/03/hlda/","eyeCatchImage":null,"excerpt":"<hr><p>Hierarchical Latent Dirichlet Allocation 层次文档主题生成模型学习笔记<br>还有一些小地方不清楚，等弄清楚了再补全。</p>","date":"2018-09-03T09:26:25.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","lda","mcmc"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"深度贝叶斯习题","date":"2018-09-22T02:26:48.000Z","mathjax":true,"html":true,"_content":"\nDeep-Bayes 2018 Summer Camp\n习题\n<!--more-->\n\n# Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm\n\nOne of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.\n\nLet's find the prankster!\n\nWhen you recognize [him or her](http://deepbayes.ru/#speakers), send:\n* name\n* reconstructed photo\n* this notebook with your code (doesn't matter how awful it is :)\n\n__privately__ to [Nadia Chirkova](https://www.facebook.com/nadiinchi) at Facebook or to info@deepbayes.ru. The first three participants will receive a present. Do not make spoilers to other participants!\n\nPlease, note that you have only __one attempt__ to send a message!\n\n\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n```\n\n\n```python\nDATA_FILE = \"data_em\"\nw = 73 # face_width\n```\n\n### Data\n\nWe are given a set of $K$ images with shape $H \\times W$.\n\nIt is represented by a numpy-array with shape $H \\times W \\times K$:\n\n\n```python\nX = np.load(DATA_FILE)\n```\n\n\n```python\nX.shape # H, W, K\n```\n\n\n\n\n    (100, 200, 1000)\n\n\n\nExample of noisy image:\n\n\n```python\nplt.imshow(X[:, :, 0], cmap=\"Greys_r\")\nplt.axis(\"off\")\nprint(X[1,:,0])\n```\n\n    [255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n       0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n     148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n       0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n       0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n       0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n       0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n       0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n     255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n     233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n     226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n       0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n     255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n     255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n     102.  25.  30.  73.]\n    \n\n\n![png](task_em_files/task_em_9_1.png)\n\n\n### Goal and plan\n\nOur goal is to find face $F$ ($H \\times w$).\n\nAlso, we will find:\n* $B$: background  ($H \\times W$)\n* $s$: noise standard deviation (float)\n* $a$: discrete prior over face positions ($W-w+1$)\n* $q(d)$: discrete posterior over face positions for each image  (($W-w+1$) x $K$)\n\nImplementation plan:\n1. calculating $log\\, p(X  \\mid d,\\,F,\\,B,\\,s)$\n1. calculating objective\n1. E-step: finding $q(d)$\n1. M-step: estimating $F,\\, B, \\,s, \\,a$\n1. composing EM-algorithm from E- and M-step\n\n\n### Implementation\n\n\n```python\n### Variables to test implementation\ntH, tW, tw, tK = 2, 3, 1, 2\ntX = np.arange(tH*tW*tK).reshape(tH, tW, tK)\ntF = np.arange(tH*tw).reshape(tH, tw)\ntB = np.arange(tH*tW).reshape(tH, tW)\nts = 0.1\nta = np.arange(1, (tW-tw+1)+1)\nta = ta / ta.sum()\ntq = np.arange(1, (tW-tw+1)*tK+1).reshape(tW-tw+1, tK)\ntq = tq / tq.sum(axis=0)[np.newaxis, :]\n```\n\n#### 1. Implement calculate_log_probability\nFor $k$-th image $X_k$ and some face position $d_k$:\n$$p(X_k  \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}\n    \\begin{cases} \n    \t\\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2), \n    \t& \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\\n    \t\\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), & \\text{else}\n    \\end{cases}$$\n\nImportant notes:\n* Do not forget about logarithm!\n* This implementation should use no more than 1 cycle!\n\n\n```python\ndef calculate_log_probability(X, F, B, s):\n    \"\"\"\n    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and\n    all possible face position d_k.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n\n    Returns\n    -------\n    ll : array, shape(W-w+1, K)\n        ll[dw, k] - log-likelihood of observing image X_k given\n        that the prankster's face F is located at position dw\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[-3541.69812064, -5541.69812064],\n       [-4541.69812064, -6741.69812064],\n       [-6141.69812064, -8541.69812064]])\nactual = calculate_log_probability(tX, tF, tB, ts)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-8-7e147f60fb2e> in <module>()\n          4        [-6141.69812064, -8541.69812064]])\n          5 actual = calculate_log_probability(tX, tF, tB, ts)\n    ----> 6 assert np.allclose(actual, expected)\n          7 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 2. Implement calculate_lower_bound\n$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s) + \n    \\log p( d_k  \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$\n    \nImportant notes:\n* Use already implemented calculate_log_probability! \n* Note that distributions $q( d_k)$ and $p( d_k  \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.\n* This implementation should not use cycles!\n\n\n```python\ndef calculate_lower_bound(X, F, B, s, a, q):\n    \"\"\"\n    Calculates the lower bound L(q, F, B, s, a) for \n    the marginal log likelihood.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    q : array\n        q[dw, k] - estimate of posterior \n                   of position dw\n                   of prankster's face given image Xk\n\n    Returns\n    -------\n    L : float\n        The lower bound L(q, F, B, s, a) \n        for the marginal log likelihood.\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = -12761.1875\nactual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-10-eb49d6900e27> in <module>()\n          2 expected = -12761.1875\n          3 actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\n    ----> 4 assert np.allclose(actual, expected)\n          5 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 3. Implement E-step\n$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) = \n\\frac {p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}\n{\\sum_{d'_k} p(  X_{k}  \\mid d'_k , \\,F,\\,B,\\,s) \\,p(d'_k \\mid a)}$$\n\nImportant notes:\n* Use already implemented calculate_log_probability!\n* For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:\n$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n\t\\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} = \n\t\\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$\n* This implementation should not use cycles!\n\n\n```python\ndef run_e_step(X, F, B, s, a):\n    \"\"\"\n    Given the current esitmate of the parameters, for each image Xk\n    esitmates the probability p(d_k|X_k, F, B, s, a).\n\n    Parameters\n    ----------\n    X : array, shape(H, W, K)\n        K images of size H x W.\n    F  : array_like, shape(H, w)\n        Estimate of prankster's face.\n    B : array shape(H, W)\n        Estimate of background.\n    s : float\n        Eestimate of standard deviation of Gaussian noise.\n    a : array, shape(W-w+1)\n        Estimate of prior on face position in any image.\n\n    Returns\n    -------\n    q : array\n        shape (W-w+1, K)\n        q[dw, k] - estimate of posterior of position dw\n        of prankster's face given image Xk\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[ 1.,  1.],\n                   [ 0.,  0.],\n                   [ 0.,  0.]])\nactual = run_e_step(tX, tF, tB, ts, ta)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-12-98d2b4c2edac> in <module>()\n          4                    [ 0.,  0.]])\n          5 actual = run_e_step(tX, tF, tB, ts, ta)\n    ----> 6 assert np.allclose(actual, expected)\n          7 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 4. Implement M-step\n$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j'}  \\sum_{k'} q( d_{k'} = j')}$$\n$$F[i, m] = \\frac 1 K  \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$\n$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]} \n\t  \t{\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$\n$$s^2 = \\frac 1 {HWK}   \\sum_k \\sum_{d_k} q(d_k)\n\t  \t\\sum_{i,\\, j}  (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$\n\nwhere $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.\n\nImportant notes:\n* Update parameters in the following order: $a$, $F$, $B$, $s$.\n* When the parameter is updated, its __new__ value is used to update other parameters.\n* This implementation should use no more than 3 cycles and no embedded cycles!\n\n\n```python\ndef run_m_step(X, q, w):\n    \"\"\"\n    Estimates F, B, s, a given esitmate of posteriors defined by q.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    q  :\n        q[dw, k] - estimate of posterior of position dw\n                   of prankster's face given image Xk\n    w : int\n        Face mask width.\n\n    Returns\n    -------\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = [np.array([[ 3.27777778],\n                      [ 9.27777778]]),\n np.array([[  0.48387097,   2.5       ,   4.52941176],\n           [  6.48387097,   8.5       ,  10.52941176]]),\n  0.94868,\n np.array([ 0.13888889,  0.33333333,  0.52777778])]\nactual = run_m_step(tX, tq, tw)\nfor a, e in zip(actual, expected):\n    assert np.allclose(a, e)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-14-e44e2bfe2fd1> in <module>()\n          7  np.array([ 0.13888889,  0.33333333,  0.52777778])]\n          8 actual = run_m_step(tX, tq, tw)\n    ----> 9 for a, e in zip(actual, expected):\n         10     assert np.allclose(a, e)\n         11 print(\"OK\")\n    \n\n    TypeError: zip argument #1 must support iteration\n\n\n#### 5. Implement EM_algorithm\nInitialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.\n\nPlease note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.\n\n\n```python\ndef run_EM(X, w, F=None, B=None, s=None, a=None, tolerance=0.001,\n           max_iter=50):\n    \"\"\"\n    Runs EM loop until the likelihood of observing X given current\n    estimate of parameters is idempotent as defined by a fixed\n    tolerance.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    w : int\n        Face mask width.\n    F : array, shape (H, w), optional\n        Initial estimate of prankster's face.\n    B : array, shape (H, W), optional\n        Initial estimate of background.\n    s : float, optional\n        Initial estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1), optional\n        Initial estimate of prior on position of face in any image.\n    tolerance : float, optional\n        Parameter for stopping criterion.\n    max_iter  : int, optional\n        Maximum number of iterations.\n\n    Returns\n    -------\n    F, B, s, a : trained parameters.\n    LL : array, shape(number_of_iters + 2,)\n        L(q, F, B, s, a) at initial guess, \n        after each EM iteration and after\n        final estimate of posteriors;\n        number_of_iters is actual number of iterations that was done.\n    \"\"\"\n    H, W, N = X.shape\n    if F is None:\n        F = np.random.randint(0, 255, (H, w))\n    if B is None:\n        B = np.random.randint(0, 255, (H, W))\n    if a is None:\n        a = np.ones(W - w + 1)\n        a /= np.sum(a)\n    if s is None:\n        s = np.random.rand()*pow(64,2)\n    # your code here\n    \n```\n\n\n```python\n# run this cell to test your implementation\nres = run_EM(tX, tw, max_iter=3)\nLL = res[-1]\nassert np.alltrue(LL[1:] - LL[:-1] > 0)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-16-6b1860dc5143> in <module>()\n          1 # run this cell to test your implementation\n          2 res = run_EM(tX, tw, max_iter=3)\n    ----> 3 LL = res[-1]\n          4 assert np.alltrue(LL[1:] - LL[:-1] > 0)\n          5 print(\"OK\")\n    \n\n    TypeError: 'NoneType' object is not subscriptable\n\n\n### Who is the prankster?\n\nTo speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.\n\nIf everything is implemented correctly, you will recognize the prankster (remember he is the one from [DeepBayes team](http://deepbayes.ru/#speakers)).\n\nRun EM-algorithm:\n\n\n```python\ndef show(F, i=1, n=1):\n    \"\"\"\n    shows face F at subplot i out of n\n    \"\"\"\n    plt.subplot(1, n, i)\n    plt.imshow(F, cmap=\"Greys_r\")\n    plt.axis(\"off\")\n```\n\n\n```python\nF, B, s, a = [None] * 4\nLL = []\nlens = [50, 100, 300, 500, 1000]\niters = [5, 1, 1, 1, 1]\nplt.figure(figsize=(20, 5))\nfor i, (l, it) in enumerate(zip(lens, iters)):\n    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n    show(F, i+1, 5)\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-18-5c3ec9450804> in <module>()\n          5 plt.figure(figsize=(20, 5))\n          6 for i, (l, it) in enumerate(zip(lens, iters)):\n    ----> 7     F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n          8     show(F, i+1, 5)\n    \n\n    TypeError: 'NoneType' object is not iterable\n\n\n\n    <Figure size 1440x360 with 0 Axes>\n\n\nAnd this is the background:\n\n\n```python\nshow(B)\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-19-ba7381968102> in <module>()\n    ----> 1 show(B)\n    \n\n    <ipython-input-17-1c6656dd6e56> in show(F, i, n)\n          4     \"\"\"\n          5     plt.subplot(1, n, i)\n    ----> 6     plt.imshow(F, cmap=\"Greys_r\")\n          7     plt.axis(\"off\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\n       3203                         filternorm=filternorm, filterrad=filterrad,\n       3204                         imlim=imlim, resample=resample, url=url, data=data,\n    -> 3205                         **kwargs)\n       3206     finally:\n       3207         ax._hold = washold\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py in inner(ax, *args, **kwargs)\n       1853                         \"the Matplotlib list!)\" % (label_namer, func.__name__),\n       1854                         RuntimeWarning, stacklevel=2)\n    -> 1855             return func(ax, *args, **kwargs)\n       1856 \n       1857         inner.__doc__ = _add_data_doc(inner.__doc__,\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\n       5485                               resample=resample, **kwargs)\n       5486 \n    -> 5487         im.set_data(X)\n       5488         im.set_alpha(alpha)\n       5489         if im.get_clip_path() is None:\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py in set_data(self, A)\n        647         if (self._A.dtype != np.uint8 and\n        648                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n    --> 649             raise TypeError(\"Image data cannot be converted to float\")\n        650 \n        651         if not (self._A.ndim == 2\n    \n\n    TypeError: Image data cannot be converted to float\n\n\n\n![png](task_em_files/task_em_35_1.png)\n\n\n### Optional part: hard-EM\n\nIf you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:\n$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$\nThis modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.\n\nIf you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:\n* calculate_lower_bound\n* run_e_step\n* run_m_step\n* run_EM\n\nAfter implementation, compare overall computation time for EM and hard-EM till recognizable F.\n","source":"_posts/deepbayes2018.md","raw":"---\ntitle: 深度贝叶斯习题\ndate: 2018-09-22 10:26:48\ntags: [bayes,math,machinelearning]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\nDeep-Bayes 2018 Summer Camp\n习题\n<!--more-->\n\n# Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm\n\nOne of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.\n\nLet's find the prankster!\n\nWhen you recognize [him or her](http://deepbayes.ru/#speakers), send:\n* name\n* reconstructed photo\n* this notebook with your code (doesn't matter how awful it is :)\n\n__privately__ to [Nadia Chirkova](https://www.facebook.com/nadiinchi) at Facebook or to info@deepbayes.ru. The first three participants will receive a present. Do not make spoilers to other participants!\n\nPlease, note that you have only __one attempt__ to send a message!\n\n\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n```\n\n\n```python\nDATA_FILE = \"data_em\"\nw = 73 # face_width\n```\n\n### Data\n\nWe are given a set of $K$ images with shape $H \\times W$.\n\nIt is represented by a numpy-array with shape $H \\times W \\times K$:\n\n\n```python\nX = np.load(DATA_FILE)\n```\n\n\n```python\nX.shape # H, W, K\n```\n\n\n\n\n    (100, 200, 1000)\n\n\n\nExample of noisy image:\n\n\n```python\nplt.imshow(X[:, :, 0], cmap=\"Greys_r\")\nplt.axis(\"off\")\nprint(X[1,:,0])\n```\n\n    [255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n       0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n     148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n       0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n       0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n       0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n       0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n       0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n     255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n     233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n     226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n       0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n     255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n     255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n     102.  25.  30.  73.]\n    \n\n\n![png](task_em_files/task_em_9_1.png)\n\n\n### Goal and plan\n\nOur goal is to find face $F$ ($H \\times w$).\n\nAlso, we will find:\n* $B$: background  ($H \\times W$)\n* $s$: noise standard deviation (float)\n* $a$: discrete prior over face positions ($W-w+1$)\n* $q(d)$: discrete posterior over face positions for each image  (($W-w+1$) x $K$)\n\nImplementation plan:\n1. calculating $log\\, p(X  \\mid d,\\,F,\\,B,\\,s)$\n1. calculating objective\n1. E-step: finding $q(d)$\n1. M-step: estimating $F,\\, B, \\,s, \\,a$\n1. composing EM-algorithm from E- and M-step\n\n\n### Implementation\n\n\n```python\n### Variables to test implementation\ntH, tW, tw, tK = 2, 3, 1, 2\ntX = np.arange(tH*tW*tK).reshape(tH, tW, tK)\ntF = np.arange(tH*tw).reshape(tH, tw)\ntB = np.arange(tH*tW).reshape(tH, tW)\nts = 0.1\nta = np.arange(1, (tW-tw+1)+1)\nta = ta / ta.sum()\ntq = np.arange(1, (tW-tw+1)*tK+1).reshape(tW-tw+1, tK)\ntq = tq / tq.sum(axis=0)[np.newaxis, :]\n```\n\n#### 1. Implement calculate_log_probability\nFor $k$-th image $X_k$ and some face position $d_k$:\n$$p(X_k  \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}\n    \\begin{cases} \n    \t\\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2), \n    \t& \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\\n    \t\\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), & \\text{else}\n    \\end{cases}$$\n\nImportant notes:\n* Do not forget about logarithm!\n* This implementation should use no more than 1 cycle!\n\n\n```python\ndef calculate_log_probability(X, F, B, s):\n    \"\"\"\n    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and\n    all possible face position d_k.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n\n    Returns\n    -------\n    ll : array, shape(W-w+1, K)\n        ll[dw, k] - log-likelihood of observing image X_k given\n        that the prankster's face F is located at position dw\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[-3541.69812064, -5541.69812064],\n       [-4541.69812064, -6741.69812064],\n       [-6141.69812064, -8541.69812064]])\nactual = calculate_log_probability(tX, tF, tB, ts)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-8-7e147f60fb2e> in <module>()\n          4        [-6141.69812064, -8541.69812064]])\n          5 actual = calculate_log_probability(tX, tF, tB, ts)\n    ----> 6 assert np.allclose(actual, expected)\n          7 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 2. Implement calculate_lower_bound\n$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s) + \n    \\log p( d_k  \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$\n    \nImportant notes:\n* Use already implemented calculate_log_probability! \n* Note that distributions $q( d_k)$ and $p( d_k  \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.\n* This implementation should not use cycles!\n\n\n```python\ndef calculate_lower_bound(X, F, B, s, a, q):\n    \"\"\"\n    Calculates the lower bound L(q, F, B, s, a) for \n    the marginal log likelihood.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    q : array\n        q[dw, k] - estimate of posterior \n                   of position dw\n                   of prankster's face given image Xk\n\n    Returns\n    -------\n    L : float\n        The lower bound L(q, F, B, s, a) \n        for the marginal log likelihood.\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = -12761.1875\nactual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-10-eb49d6900e27> in <module>()\n          2 expected = -12761.1875\n          3 actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\n    ----> 4 assert np.allclose(actual, expected)\n          5 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 3. Implement E-step\n$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) = \n\\frac {p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}\n{\\sum_{d'_k} p(  X_{k}  \\mid d'_k , \\,F,\\,B,\\,s) \\,p(d'_k \\mid a)}$$\n\nImportant notes:\n* Use already implemented calculate_log_probability!\n* For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:\n$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n\t\\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} = \n\t\\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$\n* This implementation should not use cycles!\n\n\n```python\ndef run_e_step(X, F, B, s, a):\n    \"\"\"\n    Given the current esitmate of the parameters, for each image Xk\n    esitmates the probability p(d_k|X_k, F, B, s, a).\n\n    Parameters\n    ----------\n    X : array, shape(H, W, K)\n        K images of size H x W.\n    F  : array_like, shape(H, w)\n        Estimate of prankster's face.\n    B : array shape(H, W)\n        Estimate of background.\n    s : float\n        Eestimate of standard deviation of Gaussian noise.\n    a : array, shape(W-w+1)\n        Estimate of prior on face position in any image.\n\n    Returns\n    -------\n    q : array\n        shape (W-w+1, K)\n        q[dw, k] - estimate of posterior of position dw\n        of prankster's face given image Xk\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[ 1.,  1.],\n                   [ 0.,  0.],\n                   [ 0.,  0.]])\nactual = run_e_step(tX, tF, tB, ts, ta)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-12-98d2b4c2edac> in <module>()\n          4                    [ 0.,  0.]])\n          5 actual = run_e_step(tX, tF, tB, ts, ta)\n    ----> 6 assert np.allclose(actual, expected)\n          7 print(\"OK\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n       2254 \n       2255     \"\"\"\n    -> 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n       2257     return bool(res)\n       2258 \n    \n\n    ~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n       2330     y = array(y, dtype=dt, copy=False, subok=True)\n       2331 \n    -> 2332     xfin = isfinite(x)\n       2333     yfin = isfinite(y)\n       2334     if all(xfin) and all(yfin):\n    \n\n    TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n#### 4. Implement M-step\n$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j'}  \\sum_{k'} q( d_{k'} = j')}$$\n$$F[i, m] = \\frac 1 K  \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$\n$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]} \n\t  \t{\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$\n$$s^2 = \\frac 1 {HWK}   \\sum_k \\sum_{d_k} q(d_k)\n\t  \t\\sum_{i,\\, j}  (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$\n\nwhere $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.\n\nImportant notes:\n* Update parameters in the following order: $a$, $F$, $B$, $s$.\n* When the parameter is updated, its __new__ value is used to update other parameters.\n* This implementation should use no more than 3 cycles and no embedded cycles!\n\n\n```python\ndef run_m_step(X, q, w):\n    \"\"\"\n    Estimates F, B, s, a given esitmate of posteriors defined by q.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    q  :\n        q[dw, k] - estimate of posterior of position dw\n                   of prankster's face given image Xk\n    w : int\n        Face mask width.\n\n    Returns\n    -------\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    \"\"\"\n    # your code here\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = [np.array([[ 3.27777778],\n                      [ 9.27777778]]),\n np.array([[  0.48387097,   2.5       ,   4.52941176],\n           [  6.48387097,   8.5       ,  10.52941176]]),\n  0.94868,\n np.array([ 0.13888889,  0.33333333,  0.52777778])]\nactual = run_m_step(tX, tq, tw)\nfor a, e in zip(actual, expected):\n    assert np.allclose(a, e)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-14-e44e2bfe2fd1> in <module>()\n          7  np.array([ 0.13888889,  0.33333333,  0.52777778])]\n          8 actual = run_m_step(tX, tq, tw)\n    ----> 9 for a, e in zip(actual, expected):\n         10     assert np.allclose(a, e)\n         11 print(\"OK\")\n    \n\n    TypeError: zip argument #1 must support iteration\n\n\n#### 5. Implement EM_algorithm\nInitialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.\n\nPlease note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.\n\n\n```python\ndef run_EM(X, w, F=None, B=None, s=None, a=None, tolerance=0.001,\n           max_iter=50):\n    \"\"\"\n    Runs EM loop until the likelihood of observing X given current\n    estimate of parameters is idempotent as defined by a fixed\n    tolerance.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    w : int\n        Face mask width.\n    F : array, shape (H, w), optional\n        Initial estimate of prankster's face.\n    B : array, shape (H, W), optional\n        Initial estimate of background.\n    s : float, optional\n        Initial estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1), optional\n        Initial estimate of prior on position of face in any image.\n    tolerance : float, optional\n        Parameter for stopping criterion.\n    max_iter  : int, optional\n        Maximum number of iterations.\n\n    Returns\n    -------\n    F, B, s, a : trained parameters.\n    LL : array, shape(number_of_iters + 2,)\n        L(q, F, B, s, a) at initial guess, \n        after each EM iteration and after\n        final estimate of posteriors;\n        number_of_iters is actual number of iterations that was done.\n    \"\"\"\n    H, W, N = X.shape\n    if F is None:\n        F = np.random.randint(0, 255, (H, w))\n    if B is None:\n        B = np.random.randint(0, 255, (H, W))\n    if a is None:\n        a = np.ones(W - w + 1)\n        a /= np.sum(a)\n    if s is None:\n        s = np.random.rand()*pow(64,2)\n    # your code here\n    \n```\n\n\n```python\n# run this cell to test your implementation\nres = run_EM(tX, tw, max_iter=3)\nLL = res[-1]\nassert np.alltrue(LL[1:] - LL[:-1] > 0)\nprint(\"OK\")\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-16-6b1860dc5143> in <module>()\n          1 # run this cell to test your implementation\n          2 res = run_EM(tX, tw, max_iter=3)\n    ----> 3 LL = res[-1]\n          4 assert np.alltrue(LL[1:] - LL[:-1] > 0)\n          5 print(\"OK\")\n    \n\n    TypeError: 'NoneType' object is not subscriptable\n\n\n### Who is the prankster?\n\nTo speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.\n\nIf everything is implemented correctly, you will recognize the prankster (remember he is the one from [DeepBayes team](http://deepbayes.ru/#speakers)).\n\nRun EM-algorithm:\n\n\n```python\ndef show(F, i=1, n=1):\n    \"\"\"\n    shows face F at subplot i out of n\n    \"\"\"\n    plt.subplot(1, n, i)\n    plt.imshow(F, cmap=\"Greys_r\")\n    plt.axis(\"off\")\n```\n\n\n```python\nF, B, s, a = [None] * 4\nLL = []\nlens = [50, 100, 300, 500, 1000]\niters = [5, 1, 1, 1, 1]\nplt.figure(figsize=(20, 5))\nfor i, (l, it) in enumerate(zip(lens, iters)):\n    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n    show(F, i+1, 5)\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-18-5c3ec9450804> in <module>()\n          5 plt.figure(figsize=(20, 5))\n          6 for i, (l, it) in enumerate(zip(lens, iters)):\n    ----> 7     F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n          8     show(F, i+1, 5)\n    \n\n    TypeError: 'NoneType' object is not iterable\n\n\n\n    <Figure size 1440x360 with 0 Axes>\n\n\nAnd this is the background:\n\n\n```python\nshow(B)\n```\n\n\n    ---------------------------------------------------------------------------\n\n    TypeError                                 Traceback (most recent call last)\n\n    <ipython-input-19-ba7381968102> in <module>()\n    ----> 1 show(B)\n    \n\n    <ipython-input-17-1c6656dd6e56> in show(F, i, n)\n          4     \"\"\"\n          5     plt.subplot(1, n, i)\n    ----> 6     plt.imshow(F, cmap=\"Greys_r\")\n          7     plt.axis(\"off\")\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\n       3203                         filternorm=filternorm, filterrad=filterrad,\n       3204                         imlim=imlim, resample=resample, url=url, data=data,\n    -> 3205                         **kwargs)\n       3206     finally:\n       3207         ax._hold = washold\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py in inner(ax, *args, **kwargs)\n       1853                         \"the Matplotlib list!)\" % (label_namer, func.__name__),\n       1854                         RuntimeWarning, stacklevel=2)\n    -> 1855             return func(ax, *args, **kwargs)\n       1856 \n       1857         inner.__doc__ = _add_data_doc(inner.__doc__,\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\n       5485                               resample=resample, **kwargs)\n       5486 \n    -> 5487         im.set_data(X)\n       5488         im.set_alpha(alpha)\n       5489         if im.get_clip_path() is None:\n    \n\n    ~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py in set_data(self, A)\n        647         if (self._A.dtype != np.uint8 and\n        648                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n    --> 649             raise TypeError(\"Image data cannot be converted to float\")\n        650 \n        651         if not (self._A.ndim == 2\n    \n\n    TypeError: Image data cannot be converted to float\n\n\n\n![png](task_em_files/task_em_35_1.png)\n\n\n### Optional part: hard-EM\n\nIf you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:\n$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$\nThis modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.\n\nIf you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:\n* calculate_lower_bound\n* run_e_step\n* run_m_step\n* run_EM\n\nAfter implementation, compare overall computation time for EM and hard-EM till recognizable F.\n","slug":"deepbayes2018","published":1,"updated":"2018-09-22T05:41:51.054Z","_id":"cjmd072d2001lqcw6y1vijoci","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Deep-Bayes 2018 Summer Camp<br>习题<br><a id=\"more\"></a></p><h1 id=\"Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\"><a href=\"#Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\" class=\"headerlink\" title=\"Deep|Bayes summer school. Practical session on EM algorithm\"></a>Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm</h1><p>One of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.</p><p>Let’s find the prankster!</p><p>When you recognize <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">him or her</a>, send:</p><ul><li>name</li><li>reconstructed photo</li><li>this notebook with your code (doesn’t matter how awful it is :)</li></ul><p><strong>privately</strong> to <a href=\"https://www.facebook.com/nadiinchi\" target=\"_blank\" rel=\"noopener\">Nadia Chirkova</a> at Facebook or to <a href=\"mailto:info@deepbayes.ru\" target=\"_blank\" rel=\"noopener\">info@deepbayes.ru</a>. The first three participants will receive a present. Do not make spoilers to other participants!</p><p>Please, note that you have only <strong>one attempt</strong> to send a message!</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DATA_FILE = <span class=\"string\">\"data_em\"</span></span><br><span class=\"line\">w = <span class=\"number\">73</span> <span class=\"comment\"># face_width</span></span><br></pre></td></tr></table></figure><h3 id=\"Data\"><a href=\"#Data\" class=\"headerlink\" title=\"Data\"></a>Data</h3><p>We are given a set of $K$ images with shape $H \\times W$.</p><p>It is represented by a numpy-array with shape $H \\times W \\times K$:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.load(DATA_FILE)</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.shape <span class=\"comment\"># H, W, K</span></span><br></pre></td></tr></table></figure><pre><code>(100, 200, 1000)\n</code></pre><p>Example of noisy image:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.imshow(X[:, :, <span class=\"number\">0</span>], cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">print(X[<span class=\"number\">1</span>,:,<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n   0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n 148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n   0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n   0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n   0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n   0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n   0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n 255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n 233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n 226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n   0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n 255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n 255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n 102.  25.  30.  73.]\n</code></pre><p><img src=\"task_em_files/task_em_9_1.png\" alt=\"png\"></p><h3 id=\"Goal-and-plan\"><a href=\"#Goal-and-plan\" class=\"headerlink\" title=\"Goal and plan\"></a>Goal and plan</h3><p>Our goal is to find face $F$ ($H \\times w$).</p><p>Also, we will find:</p><ul><li>$B$: background ($H \\times W$)</li><li>$s$: noise standard deviation (float)</li><li>$a$: discrete prior over face positions ($W-w+1$)</li><li>$q(d)$: discrete posterior over face positions for each image (($W-w+1$) x $K$)</li></ul><p>Implementation plan:</p><ol><li>calculating $log\\, p(X \\mid d,\\,F,\\,B,\\,s)$</li><li>calculating objective</li><li>E-step: finding $q(d)$</li><li>M-step: estimating $F,\\, B, \\,s, \\,a$</li><li>composing EM-algorithm from E- and M-step</li></ol><h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### Variables to test implementation</span></span><br><span class=\"line\">tH, tW, tw, tK = <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span></span><br><span class=\"line\">tX = np.arange(tH*tW*tK).reshape(tH, tW, tK)</span><br><span class=\"line\">tF = np.arange(tH*tw).reshape(tH, tw)</span><br><span class=\"line\">tB = np.arange(tH*tW).reshape(tH, tW)</span><br><span class=\"line\">ts = <span class=\"number\">0.1</span></span><br><span class=\"line\">ta = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)+<span class=\"number\">1</span>)</span><br><span class=\"line\">ta = ta / ta.sum()</span><br><span class=\"line\">tq = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)*tK+<span class=\"number\">1</span>).reshape(tW-tw+<span class=\"number\">1</span>, tK)</span><br><span class=\"line\">tq = tq / tq.sum(axis=<span class=\"number\">0</span>)[np.newaxis, :]</span><br></pre></td></tr></table></figure><h4 id=\"1-Implement-calculate-log-probability\"><a href=\"#1-Implement-calculate-log-probability\" class=\"headerlink\" title=\"1. Implement calculate_log_probability\"></a>1. Implement calculate_log_probability</h4><p>For $k$-th image $X_k$ and some face position $d_k$:<br>$$p(X_k \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}<br>\\begin{cases}<br>\\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2),<br>&amp; \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\<br>\\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), &amp; \\text{else}<br>\\end{cases}$$</p><p>Important notes:</p><ul><li>Do not forget about logarithm!</li><li>This implementation should use no more than 1 cycle!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_log_probability</span><span class=\"params\">(X, F, B, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and</span></span><br><span class=\"line\"><span class=\"string\">    all possible face position d_k.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    ll : array, shape(W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        ll[dw, k] - log-likelihood of observing image X_k given</span></span><br><span class=\"line\"><span class=\"string\">        that the prankster's face F is located at position dw</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[<span class=\"number\">-3541.69812064</span>, <span class=\"number\">-5541.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-4541.69812064</span>, <span class=\"number\">-6741.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-6141.69812064</span>, <span class=\"number\">-8541.69812064</span>]])</span><br><span class=\"line\">actual = calculate_log_probability(tX, tF, tB, ts)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-8-7e147f60fb2e&gt; in &lt;module&gt;()\n      4        [-6141.69812064, -8541.69812064]])\n      5 actual = calculate_log_probability(tX, tF, tB, ts)\n----&gt; 6 assert np.allclose(actual, expected)\n      7 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"2-Implement-calculate-lower-bound\"><a href=\"#2-Implement-calculate-lower-bound\" class=\"headerlink\" title=\"2. Implement calculate_lower_bound\"></a>2. Implement calculate_lower_bound</h4><p>$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p( X_{k} \\mid {d}_{k} , \\,F,\\,B,\\,s) +<br>\\log p( d_k \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$</p><p>Important notes:</p><ul><li>Use already implemented calculate_log_probability!</li><li>Note that distributions $q( d_k)$ and $p( d_k \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.</li><li>This implementation should not use cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_lower_bound</span><span class=\"params\">(X, F, B, s, a, q)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates the lower bound L(q, F, B, s, a) for </span></span><br><span class=\"line\"><span class=\"string\">    the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior </span></span><br><span class=\"line\"><span class=\"string\">                   of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    L : float</span></span><br><span class=\"line\"><span class=\"string\">        The lower bound L(q, F, B, s, a) </span></span><br><span class=\"line\"><span class=\"string\">        for the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = <span class=\"number\">-12761.1875</span></span><br><span class=\"line\">actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-10-eb49d6900e27&gt; in &lt;module&gt;()\n      2 expected = -12761.1875\n      3 actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\n----&gt; 4 assert np.allclose(actual, expected)\n      5 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"3-Implement-E-step\"><a href=\"#3-Implement-E-step\" class=\"headerlink\" title=\"3. Implement E-step\"></a>3. Implement E-step</h4><p>$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) =<br>\\frac {p( X_{k} \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}<br>{\\sum_{d’_k} p( X_{k} \\mid d’_k , \\,F,\\,B,\\,s) \\,p(d’_k \\mid a)}$$</p><p>Important notes:</p><ul><li>Use already implemented calculate_log_probability!</li><li>For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:<br>$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad<br>\\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} =<br>\\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$</li><li>This implementation should not use cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_e_step</span><span class=\"params\">(X, F, B, s, a)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Given the current esitmate of the parameters, for each image Xk</span></span><br><span class=\"line\"><span class=\"string\">    esitmates the probability p(d_k|X_k, F, B, s, a).</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape(H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F  : array_like, shape(H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array shape(H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Eestimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape(W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on face position in any image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        shape (W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">        of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>]])</span><br><span class=\"line\">actual = run_e_step(tX, tF, tB, ts, ta)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-12-98d2b4c2edac&gt; in &lt;module&gt;()\n      4                    [ 0.,  0.]])\n      5 actual = run_e_step(tX, tF, tB, ts, ta)\n----&gt; 6 assert np.allclose(actual, expected)\n      7 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"4-Implement-M-step\"><a href=\"#4-Implement-M-step\" class=\"headerlink\" title=\"4. Implement M-step\"></a>4. Implement M-step</h4><p>$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j’} \\sum_{k’} q( d_{k’} = j’)}$$<br>$$F[i, m] = \\frac 1 K \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$<br>$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]}<br>{\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$<br>$$s^2 = \\frac 1 {HWK} \\sum_k \\sum_{d_k} q(d_k)<br>\\sum_{i,\\, j} (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$</p><p>where $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.</p><p>Important notes:</p><ul><li>Update parameters in the following order: $a$, $F$, $B$, $s$.</li><li>When the parameter is updated, its <strong>new</strong> value is used to update other parameters.</li><li>This implementation should use no more than 3 cycles and no embedded cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_m_step</span><span class=\"params\">(X, q, w)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Estimates F, B, s, a given esitmate of posteriors defined by q.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    q  :</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = [np.array([[ <span class=\"number\">3.27777778</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">9.27777778</span>]]),</span><br><span class=\"line\"> np.array([[  <span class=\"number\">0.48387097</span>,   <span class=\"number\">2.5</span>       ,   <span class=\"number\">4.52941176</span>],</span><br><span class=\"line\">           [  <span class=\"number\">6.48387097</span>,   <span class=\"number\">8.5</span>       ,  <span class=\"number\">10.52941176</span>]]),</span><br><span class=\"line\">  <span class=\"number\">0.94868</span>,</span><br><span class=\"line\"> np.array([ <span class=\"number\">0.13888889</span>,  <span class=\"number\">0.33333333</span>,  <span class=\"number\">0.52777778</span>])]</span><br><span class=\"line\">actual = run_m_step(tX, tq, tw)</span><br><span class=\"line\"><span class=\"keyword\">for</span> a, e <span class=\"keyword\">in</span> zip(actual, expected):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> np.allclose(a, e)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-14-e44e2bfe2fd1&gt; in &lt;module&gt;()\n      7  np.array([ 0.13888889,  0.33333333,  0.52777778])]\n      8 actual = run_m_step(tX, tq, tw)\n----&gt; 9 for a, e in zip(actual, expected):\n     10     assert np.allclose(a, e)\n     11 print(&quot;OK&quot;)\n\n\nTypeError: zip argument #1 must support iteration\n</code></pre><h4 id=\"5-Implement-EM-algorithm\"><a href=\"#5-Implement-EM-algorithm\" class=\"headerlink\" title=\"5. Implement EM_algorithm\"></a>5. Implement EM_algorithm</h4><p>Initialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.</p><p>Please note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_EM</span><span class=\"params\">(X, w, F=None, B=None, s=None, a=None, tolerance=<span class=\"number\">0.001</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           max_iter=<span class=\"number\">50</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Runs EM loop until the likelihood of observing X given current</span></span><br><span class=\"line\"><span class=\"string\">    estimate of parameters is idempotent as defined by a fixed</span></span><br><span class=\"line\"><span class=\"string\">    tolerance.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    tolerance : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Parameter for stopping criterion.</span></span><br><span class=\"line\"><span class=\"string\">    max_iter  : int, optional</span></span><br><span class=\"line\"><span class=\"string\">        Maximum number of iterations.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F, B, s, a : trained parameters.</span></span><br><span class=\"line\"><span class=\"string\">    LL : array, shape(number_of_iters + 2,)</span></span><br><span class=\"line\"><span class=\"string\">        L(q, F, B, s, a) at initial guess, </span></span><br><span class=\"line\"><span class=\"string\">        after each EM iteration and after</span></span><br><span class=\"line\"><span class=\"string\">        final estimate of posteriors;</span></span><br><span class=\"line\"><span class=\"string\">        number_of_iters is actual number of iterations that was done.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    H, W, N = X.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> F <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        F = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, w))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> B <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        B = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, W))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> a <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        a = np.ones(W - w + <span class=\"number\">1</span>)</span><br><span class=\"line\">        a /= np.sum(a)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> s <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        s = np.random.rand()*pow(<span class=\"number\">64</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">res = run_EM(tX, tw, max_iter=<span class=\"number\">3</span>)</span><br><span class=\"line\">LL = res[<span class=\"number\">-1</span>]</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.alltrue(LL[<span class=\"number\">1</span>:] - LL[:<span class=\"number\">-1</span>] &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-16-6b1860dc5143&gt; in &lt;module&gt;()\n      1 # run this cell to test your implementation\n      2 res = run_EM(tX, tw, max_iter=3)\n----&gt; 3 LL = res[-1]\n      4 assert np.alltrue(LL[1:] - LL[:-1] &gt; 0)\n      5 print(&quot;OK&quot;)\n\n\nTypeError: &apos;NoneType&apos; object is not subscriptable\n</code></pre><h3 id=\"Who-is-the-prankster\"><a href=\"#Who-is-the-prankster\" class=\"headerlink\" title=\"Who is the prankster?\"></a>Who is the prankster?</h3><p>To speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.</p><p>If everything is implemented correctly, you will recognize the prankster (remember he is the one from <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">DeepBayes team</a>).</p><p>Run EM-algorithm:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show</span><span class=\"params\">(F, i=<span class=\"number\">1</span>, n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    shows face F at subplot i out of n</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, n, i)</span><br><span class=\"line\">    plt.imshow(F, cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F, B, s, a = [<span class=\"keyword\">None</span>] * <span class=\"number\">4</span></span><br><span class=\"line\">LL = []</span><br><span class=\"line\">lens = [<span class=\"number\">50</span>, <span class=\"number\">100</span>, <span class=\"number\">300</span>, <span class=\"number\">500</span>, <span class=\"number\">1000</span>]</span><br><span class=\"line\">iters = [<span class=\"number\">5</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">20</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, (l, it) <span class=\"keyword\">in</span> enumerate(zip(lens, iters)):</span><br><span class=\"line\">    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)</span><br><span class=\"line\">    show(F, i+<span class=\"number\">1</span>, <span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-18-5c3ec9450804&gt; in &lt;module&gt;()\n      5 plt.figure(figsize=(20, 5))\n      6 for i, (l, it) in enumerate(zip(lens, iters)):\n----&gt; 7     F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n      8     show(F, i+1, 5)\n\n\nTypeError: &apos;NoneType&apos; object is not iterable\n\n\n\n&lt;Figure size 1440x360 with 0 Axes&gt;\n</code></pre><p>And this is the background:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show(B)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-19-ba7381968102&gt; in &lt;module&gt;()\n----&gt; 1 show(B)\n\n\n&lt;ipython-input-17-1c6656dd6e56&gt; in show(F, i, n)\n      4     &quot;&quot;&quot;\n      5     plt.subplot(1, n, i)\n----&gt; 6     plt.imshow(F, cmap=&quot;Greys_r&quot;)\n      7     plt.axis(&quot;off&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\n   3203                         filternorm=filternorm, filterrad=filterrad,\n   3204                         imlim=imlim, resample=resample, url=url, data=data,\n-&gt; 3205                         **kwargs)\n   3206     finally:\n   3207         ax._hold = washold\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py in inner(ax, *args, **kwargs)\n   1853                         &quot;the Matplotlib list!)&quot; % (label_namer, func.__name__),\n   1854                         RuntimeWarning, stacklevel=2)\n-&gt; 1855             return func(ax, *args, **kwargs)\n   1856 \n   1857         inner.__doc__ = _add_data_doc(inner.__doc__,\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\n   5485                               resample=resample, **kwargs)\n   5486 \n-&gt; 5487         im.set_data(X)\n   5488         im.set_alpha(alpha)\n   5489         if im.get_clip_path() is None:\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py in set_data(self, A)\n    647         if (self._A.dtype != np.uint8 and\n    648                 not np.can_cast(self._A.dtype, float, &quot;same_kind&quot;)):\n--&gt; 649             raise TypeError(&quot;Image data cannot be converted to float&quot;)\n    650 \n    651         if not (self._A.ndim == 2\n\n\nTypeError: Image data cannot be converted to float\n</code></pre><p><img src=\"task_em_files/task_em_35_1.png\" alt=\"png\"></p><h3 id=\"Optional-part-hard-EM\"><a href=\"#Optional-part-hard-EM\" class=\"headerlink\" title=\"Optional part: hard-EM\"></a>Optional part: hard-EM</h3><p>If you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:<br>$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$<br>This modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.</p><p>If you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:</p><ul><li>calculate_lower_bound</li><li>run_e_step</li><li>run_m_step</li><li>run_EM</li></ul><p>After implementation, compare overall computation time for EM and hard-EM till recognizable F.</p>","site":{"data":{}},"excerpt":"<p>Deep-Bayes 2018 Summer Camp<br>习题<br>","more":"</p><h1 id=\"Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\"><a href=\"#Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\" class=\"headerlink\" title=\"Deep|Bayes summer school. Practical session on EM algorithm\"></a>Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm</h1><p>One of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.</p><p>Let’s find the prankster!</p><p>When you recognize <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">him or her</a>, send:</p><ul><li>name</li><li>reconstructed photo</li><li>this notebook with your code (doesn’t matter how awful it is :)</li></ul><p><strong>privately</strong> to <a href=\"https://www.facebook.com/nadiinchi\" target=\"_blank\" rel=\"noopener\">Nadia Chirkova</a> at Facebook or to <a href=\"mailto:info@deepbayes.ru\" target=\"_blank\" rel=\"noopener\">info@deepbayes.ru</a>. The first three participants will receive a present. Do not make spoilers to other participants!</p><p>Please, note that you have only <strong>one attempt</strong> to send a message!</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DATA_FILE = <span class=\"string\">\"data_em\"</span></span><br><span class=\"line\">w = <span class=\"number\">73</span> <span class=\"comment\"># face_width</span></span><br></pre></td></tr></table></figure><h3 id=\"Data\"><a href=\"#Data\" class=\"headerlink\" title=\"Data\"></a>Data</h3><p>We are given a set of $K$ images with shape $H \\times W$.</p><p>It is represented by a numpy-array with shape $H \\times W \\times K$:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.load(DATA_FILE)</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.shape <span class=\"comment\"># H, W, K</span></span><br></pre></td></tr></table></figure><pre><code>(100, 200, 1000)\n</code></pre><p>Example of noisy image:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.imshow(X[:, :, <span class=\"number\">0</span>], cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">print(X[<span class=\"number\">1</span>,:,<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n   0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n 148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n   0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n   0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n   0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n   0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n   0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n 255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n 233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n 226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n   0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n 255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n 255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n 102.  25.  30.  73.]\n</code></pre><p><img src=\"task_em_files/task_em_9_1.png\" alt=\"png\"></p><h3 id=\"Goal-and-plan\"><a href=\"#Goal-and-plan\" class=\"headerlink\" title=\"Goal and plan\"></a>Goal and plan</h3><p>Our goal is to find face $F$ ($H \\times w$).</p><p>Also, we will find:</p><ul><li>$B$: background ($H \\times W$)</li><li>$s$: noise standard deviation (float)</li><li>$a$: discrete prior over face positions ($W-w+1$)</li><li>$q(d)$: discrete posterior over face positions for each image (($W-w+1$) x $K$)</li></ul><p>Implementation plan:</p><ol><li>calculating $log\\, p(X \\mid d,\\,F,\\,B,\\,s)$</li><li>calculating objective</li><li>E-step: finding $q(d)$</li><li>M-step: estimating $F,\\, B, \\,s, \\,a$</li><li>composing EM-algorithm from E- and M-step</li></ol><h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### Variables to test implementation</span></span><br><span class=\"line\">tH, tW, tw, tK = <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span></span><br><span class=\"line\">tX = np.arange(tH*tW*tK).reshape(tH, tW, tK)</span><br><span class=\"line\">tF = np.arange(tH*tw).reshape(tH, tw)</span><br><span class=\"line\">tB = np.arange(tH*tW).reshape(tH, tW)</span><br><span class=\"line\">ts = <span class=\"number\">0.1</span></span><br><span class=\"line\">ta = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)+<span class=\"number\">1</span>)</span><br><span class=\"line\">ta = ta / ta.sum()</span><br><span class=\"line\">tq = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)*tK+<span class=\"number\">1</span>).reshape(tW-tw+<span class=\"number\">1</span>, tK)</span><br><span class=\"line\">tq = tq / tq.sum(axis=<span class=\"number\">0</span>)[np.newaxis, :]</span><br></pre></td></tr></table></figure><h4 id=\"1-Implement-calculate-log-probability\"><a href=\"#1-Implement-calculate-log-probability\" class=\"headerlink\" title=\"1. Implement calculate_log_probability\"></a>1. Implement calculate_log_probability</h4><p>For $k$-th image $X_k$ and some face position $d_k$:<br>$$p(X_k \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}<br>\\begin{cases}<br>\\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2),<br>&amp; \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\<br>\\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), &amp; \\text{else}<br>\\end{cases}$$</p><p>Important notes:</p><ul><li>Do not forget about logarithm!</li><li>This implementation should use no more than 1 cycle!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_log_probability</span><span class=\"params\">(X, F, B, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and</span></span><br><span class=\"line\"><span class=\"string\">    all possible face position d_k.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    ll : array, shape(W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        ll[dw, k] - log-likelihood of observing image X_k given</span></span><br><span class=\"line\"><span class=\"string\">        that the prankster's face F is located at position dw</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[<span class=\"number\">-3541.69812064</span>, <span class=\"number\">-5541.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-4541.69812064</span>, <span class=\"number\">-6741.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-6141.69812064</span>, <span class=\"number\">-8541.69812064</span>]])</span><br><span class=\"line\">actual = calculate_log_probability(tX, tF, tB, ts)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-8-7e147f60fb2e&gt; in &lt;module&gt;()\n      4        [-6141.69812064, -8541.69812064]])\n      5 actual = calculate_log_probability(tX, tF, tB, ts)\n----&gt; 6 assert np.allclose(actual, expected)\n      7 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"2-Implement-calculate-lower-bound\"><a href=\"#2-Implement-calculate-lower-bound\" class=\"headerlink\" title=\"2. Implement calculate_lower_bound\"></a>2. Implement calculate_lower_bound</h4><p>$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p( X_{k} \\mid {d}_{k} , \\,F,\\,B,\\,s) +<br>\\log p( d_k \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$</p><p>Important notes:</p><ul><li>Use already implemented calculate_log_probability!</li><li>Note that distributions $q( d_k)$ and $p( d_k \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.</li><li>This implementation should not use cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_lower_bound</span><span class=\"params\">(X, F, B, s, a, q)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates the lower bound L(q, F, B, s, a) for </span></span><br><span class=\"line\"><span class=\"string\">    the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior </span></span><br><span class=\"line\"><span class=\"string\">                   of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    L : float</span></span><br><span class=\"line\"><span class=\"string\">        The lower bound L(q, F, B, s, a) </span></span><br><span class=\"line\"><span class=\"string\">        for the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = <span class=\"number\">-12761.1875</span></span><br><span class=\"line\">actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-10-eb49d6900e27&gt; in &lt;module&gt;()\n      2 expected = -12761.1875\n      3 actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\n----&gt; 4 assert np.allclose(actual, expected)\n      5 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"3-Implement-E-step\"><a href=\"#3-Implement-E-step\" class=\"headerlink\" title=\"3. Implement E-step\"></a>3. Implement E-step</h4><p>$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) =<br>\\frac {p( X_{k} \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}<br>{\\sum_{d’_k} p( X_{k} \\mid d’_k , \\,F,\\,B,\\,s) \\,p(d’_k \\mid a)}$$</p><p>Important notes:</p><ul><li>Use already implemented calculate_log_probability!</li><li>For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:<br>$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad<br>\\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} =<br>\\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$</li><li>This implementation should not use cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_e_step</span><span class=\"params\">(X, F, B, s, a)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Given the current esitmate of the parameters, for each image Xk</span></span><br><span class=\"line\"><span class=\"string\">    esitmates the probability p(d_k|X_k, F, B, s, a).</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape(H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F  : array_like, shape(H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array shape(H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Eestimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape(W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on face position in any image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        shape (W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">        of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>]])</span><br><span class=\"line\">actual = run_e_step(tX, tF, tB, ts, ta)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-12-98d2b4c2edac&gt; in &lt;module&gt;()\n      4                    [ 0.,  0.]])\n      5 actual = run_e_step(tX, tF, tB, ts, ta)\n----&gt; 6 assert np.allclose(actual, expected)\n      7 print(&quot;OK&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2254 \n   2255     &quot;&quot;&quot;\n-&gt; 2256     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2257     return bool(res)\n   2258 \n\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2330     y = array(y, dtype=dt, copy=False, subok=True)\n   2331 \n-&gt; 2332     xfin = isfinite(x)\n   2333     yfin = isfinite(y)\n   2334     if all(xfin) and all(yfin):\n\n\nTypeError: ufunc &apos;isfinite&apos; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &apos;&apos;safe&apos;&apos;\n</code></pre><h4 id=\"4-Implement-M-step\"><a href=\"#4-Implement-M-step\" class=\"headerlink\" title=\"4. Implement M-step\"></a>4. Implement M-step</h4><p>$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j’} \\sum_{k’} q( d_{k’} = j’)}$$<br>$$F[i, m] = \\frac 1 K \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$<br>$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]}<br>{\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$<br>$$s^2 = \\frac 1 {HWK} \\sum_k \\sum_{d_k} q(d_k)<br>\\sum_{i,\\, j} (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$</p><p>where $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.</p><p>Important notes:</p><ul><li>Update parameters in the following order: $a$, $F$, $B$, $s$.</li><li>When the parameter is updated, its <strong>new</strong> value is used to update other parameters.</li><li>This implementation should use no more than 3 cycles and no embedded cycles!</li></ul><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_m_step</span><span class=\"params\">(X, q, w)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Estimates F, B, s, a given esitmate of posteriors defined by q.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    q  :</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = [np.array([[ <span class=\"number\">3.27777778</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">9.27777778</span>]]),</span><br><span class=\"line\"> np.array([[  <span class=\"number\">0.48387097</span>,   <span class=\"number\">2.5</span>       ,   <span class=\"number\">4.52941176</span>],</span><br><span class=\"line\">           [  <span class=\"number\">6.48387097</span>,   <span class=\"number\">8.5</span>       ,  <span class=\"number\">10.52941176</span>]]),</span><br><span class=\"line\">  <span class=\"number\">0.94868</span>,</span><br><span class=\"line\"> np.array([ <span class=\"number\">0.13888889</span>,  <span class=\"number\">0.33333333</span>,  <span class=\"number\">0.52777778</span>])]</span><br><span class=\"line\">actual = run_m_step(tX, tq, tw)</span><br><span class=\"line\"><span class=\"keyword\">for</span> a, e <span class=\"keyword\">in</span> zip(actual, expected):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> np.allclose(a, e)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-14-e44e2bfe2fd1&gt; in &lt;module&gt;()\n      7  np.array([ 0.13888889,  0.33333333,  0.52777778])]\n      8 actual = run_m_step(tX, tq, tw)\n----&gt; 9 for a, e in zip(actual, expected):\n     10     assert np.allclose(a, e)\n     11 print(&quot;OK&quot;)\n\n\nTypeError: zip argument #1 must support iteration\n</code></pre><h4 id=\"5-Implement-EM-algorithm\"><a href=\"#5-Implement-EM-algorithm\" class=\"headerlink\" title=\"5. Implement EM_algorithm\"></a>5. Implement EM_algorithm</h4><p>Initialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.</p><p>Please note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_EM</span><span class=\"params\">(X, w, F=None, B=None, s=None, a=None, tolerance=<span class=\"number\">0.001</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           max_iter=<span class=\"number\">50</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Runs EM loop until the likelihood of observing X given current</span></span><br><span class=\"line\"><span class=\"string\">    estimate of parameters is idempotent as defined by a fixed</span></span><br><span class=\"line\"><span class=\"string\">    tolerance.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    tolerance : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Parameter for stopping criterion.</span></span><br><span class=\"line\"><span class=\"string\">    max_iter  : int, optional</span></span><br><span class=\"line\"><span class=\"string\">        Maximum number of iterations.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F, B, s, a : trained parameters.</span></span><br><span class=\"line\"><span class=\"string\">    LL : array, shape(number_of_iters + 2,)</span></span><br><span class=\"line\"><span class=\"string\">        L(q, F, B, s, a) at initial guess, </span></span><br><span class=\"line\"><span class=\"string\">        after each EM iteration and after</span></span><br><span class=\"line\"><span class=\"string\">        final estimate of posteriors;</span></span><br><span class=\"line\"><span class=\"string\">        number_of_iters is actual number of iterations that was done.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    H, W, N = X.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> F <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        F = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, w))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> B <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        B = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, W))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> a <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        a = np.ones(W - w + <span class=\"number\">1</span>)</span><br><span class=\"line\">        a /= np.sum(a)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> s <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        s = np.random.rand()*pow(<span class=\"number\">64</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">res = run_EM(tX, tw, max_iter=<span class=\"number\">3</span>)</span><br><span class=\"line\">LL = res[<span class=\"number\">-1</span>]</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.alltrue(LL[<span class=\"number\">1</span>:] - LL[:<span class=\"number\">-1</span>] &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-16-6b1860dc5143&gt; in &lt;module&gt;()\n      1 # run this cell to test your implementation\n      2 res = run_EM(tX, tw, max_iter=3)\n----&gt; 3 LL = res[-1]\n      4 assert np.alltrue(LL[1:] - LL[:-1] &gt; 0)\n      5 print(&quot;OK&quot;)\n\n\nTypeError: &apos;NoneType&apos; object is not subscriptable\n</code></pre><h3 id=\"Who-is-the-prankster\"><a href=\"#Who-is-the-prankster\" class=\"headerlink\" title=\"Who is the prankster?\"></a>Who is the prankster?</h3><p>To speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.</p><p>If everything is implemented correctly, you will recognize the prankster (remember he is the one from <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">DeepBayes team</a>).</p><p>Run EM-algorithm:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show</span><span class=\"params\">(F, i=<span class=\"number\">1</span>, n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    shows face F at subplot i out of n</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, n, i)</span><br><span class=\"line\">    plt.imshow(F, cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br></pre></td></tr></table></figure><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F, B, s, a = [<span class=\"keyword\">None</span>] * <span class=\"number\">4</span></span><br><span class=\"line\">LL = []</span><br><span class=\"line\">lens = [<span class=\"number\">50</span>, <span class=\"number\">100</span>, <span class=\"number\">300</span>, <span class=\"number\">500</span>, <span class=\"number\">1000</span>]</span><br><span class=\"line\">iters = [<span class=\"number\">5</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">20</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, (l, it) <span class=\"keyword\">in</span> enumerate(zip(lens, iters)):</span><br><span class=\"line\">    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)</span><br><span class=\"line\">    show(F, i+<span class=\"number\">1</span>, <span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-18-5c3ec9450804&gt; in &lt;module&gt;()\n      5 plt.figure(figsize=(20, 5))\n      6 for i, (l, it) in enumerate(zip(lens, iters)):\n----&gt; 7     F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n      8     show(F, i+1, 5)\n\n\nTypeError: &apos;NoneType&apos; object is not iterable\n\n\n\n&lt;Figure size 1440x360 with 0 Axes&gt;\n</code></pre><p>And this is the background:</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show(B)</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-19-ba7381968102&gt; in &lt;module&gt;()\n----&gt; 1 show(B)\n\n\n&lt;ipython-input-17-1c6656dd6e56&gt; in show(F, i, n)\n      4     &quot;&quot;&quot;\n      5     plt.subplot(1, n, i)\n----&gt; 6     plt.imshow(F, cmap=&quot;Greys_r&quot;)\n      7     plt.axis(&quot;off&quot;)\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\n   3203                         filternorm=filternorm, filterrad=filterrad,\n   3204                         imlim=imlim, resample=resample, url=url, data=data,\n-&gt; 3205                         **kwargs)\n   3206     finally:\n   3207         ax._hold = washold\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py in inner(ax, *args, **kwargs)\n   1853                         &quot;the Matplotlib list!)&quot; % (label_namer, func.__name__),\n   1854                         RuntimeWarning, stacklevel=2)\n-&gt; 1855             return func(ax, *args, **kwargs)\n   1856 \n   1857         inner.__doc__ = _add_data_doc(inner.__doc__,\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\n   5485                               resample=resample, **kwargs)\n   5486 \n-&gt; 5487         im.set_data(X)\n   5488         im.set_alpha(alpha)\n   5489         if im.get_clip_path() is None:\n\n\n~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py in set_data(self, A)\n    647         if (self._A.dtype != np.uint8 and\n    648                 not np.can_cast(self._A.dtype, float, &quot;same_kind&quot;)):\n--&gt; 649             raise TypeError(&quot;Image data cannot be converted to float&quot;)\n    650 \n    651         if not (self._A.ndim == 2\n\n\nTypeError: Image data cannot be converted to float\n</code></pre><p><img src=\"task_em_files/task_em_35_1.png\" alt=\"png\"></p><h3 id=\"Optional-part-hard-EM\"><a href=\"#Optional-part-hard-EM\" class=\"headerlink\" title=\"Optional part: hard-EM\"></a>Optional part: hard-EM</h3><p>If you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:<br>$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$<br>This modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.</p><p>If you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:</p><ul><li>calculate_lower_bound</li><li>run_e_step</li><li>run_m_step</li><li>run_EM</li></ul><p>After implementation, compare overall computation time for EM and hard-EM till recognizable F.</p>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sat Sep 22 2018 13:41:51 GMT+0800 (中国标准时间)","title":"深度贝叶斯习题","path":"2018/09/22/deepbayes2018/","eyeCatchImage":null,"excerpt":"<p>Deep-Bayes 2018 Summer Camp<br>习题<br>","date":"2018-09-22T02:26:48.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","machinelearning","bayes"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:K-Means和KNN","date":"2017-03-16T07:51:11.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170306/215446798.jpg"],"html":true,"_content":"-\t以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较\n-   标题本来是K-Means&KNN，把&改成了和，因为标题中出现特殊符号&会导致我的sitemap生成错误......\n***\n\n<!--more-->\n# 简介\n-   K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复\n-   KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别\n\n# K-means++\n-\tk-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点\n-\t算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点\n\t-\t计算每个点$c_i$到已经选出的中心点$k_1,k_2...$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远\n\t-\t将$c_1,c_2,c_3......$的距离归一化，并排成一条线\n\t-\t这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大\n\t-\t在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点\n-\t可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求\n\n# K-Means代码实现\n\n## 数据检视\n-\tIris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143719952.JPG)\n\n\n\n## 初始化数据\n-\t初始化数据\n```Python\n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n```\n\n## k-means++初始化k个点\n-\tD2是每个点的距离(即之前定义的里其他中心点至少有多远)\n-\tprobs归一化\n-\tcumprobs将归一化的概率累加，排列成一条线\n```Python\n    def initk(X_train, k):\n        C = [X_train[0]]\n        for i in range(1, k):\n            D2 = scipy.array([min([scipy.inner(c - x, c - x) for c in C]) for x in X_train])\n            probs = D2 / D2.sum()\n            cumprobs = probs.cumsum()\n            r = scipy.rand()\n            for j, p in enumerate(cumprobs):\n                if r < p:\n                    i = j\n                    break\n            C.append(X_train[i])\n        return C\n    \n```\n\n## 损失评估\n-\t在这里用每个类内点到中心点距离平方和的总和作为损失评估\n```Python\n    def evaluate(C, X_train, y_predict):\n        sum = 0\n        for i in range(len(X_train)):\n            c = C[y_predict[i]]\n            sum += scipy.inner(c - X_train[i], c - X_train[i])\n        return sum\n```\n\n## 聚类\n-\t初始化k个中心点后，所有的点就可以分类\n-\t重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标\n```Python\n    def cluster(C, X_train, y_predict, k):\n        sum = [0, 0, 0, 0] * k\n        count = [0] * k\n        newC = []\n        for i in range(len(X_train)):\n            min = 32768\n            minj = -1\n            for j in range(k):\n                if scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) < min:\n                    min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])\n                    minj = j\n            y_predict[i] = (minj + 1) % k\n        for i in range(len(X_train)):\n            sum[y_predict[i]] += X_train[i]\n            count[y_predict[i]] += 1\n        for i in range(k):\n            newC.append(sum[i] / count[i])\n        return y_predict, newC\n```\n\n## 主函数\n-\t计算损失，更新k个中心点，再站队(聚类)一次\n-\t重复，直到损失变化小于10%\n-\t每次迭代显示新旧损失，显示损失变化\n-\t最后输出分类结果\n```Python\n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        k = 3\n        total = len(y_train)\n        y_predict = [0] * total\n        C = initk(X_train, k)\n        oldeval = evaluate(C, X_train, y_predict)\n        while (1):\n            y_predict, C = cluster(C, X_train, y_predict, k)\n            neweval = evaluate(C, X_train, y_predict)\n            ratio = (oldeval - neweval) / oldeval * 100\n            print(oldeval, \" -> \", neweval, \"%f %%\" % ratio)\n            oldeval = neweval\n            if ratio < 0.1:\n                break\n    \n        print(y_train)\n        print(y_predict)\n        n = 0\n        m = 0\n        for i in range(len(y_train)):\n            m += 1\n            if y_train[i] == y_predict[i]:\n                n += 1\n        print(n / m)\n        print(classification_report(y_train, y_predict, target_names=iris.target_names))\n    \n```\n\n# KNN代码\n-\t直接使用了sklearn中的KNeighborsClassifier\n```Python\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    \n    \n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n    \n    def KNN(X_train, X_test, y_train, y_test, iris):\n        knc = KNeighborsClassifier()\n        knc.fit(X_train, y_train)\n        y_predict = knc.predict(X_test)\n        print(knc.score(X_test, y_test))\n        print(classification_report(y_test, y_predict, target_names=iris.target_names))\n    \n    \n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        KNN(X_train, X_test, y_train, y_test, iris)\n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 预测结果\n-\t指标说明\n\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n\t$$\n\t精确率:P=\\frac{TP}{TP+FP} \\\\\n\t召回率:R=\\frac{TP}{TP+FN} \\\\\n\t1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n\t$$\n-\tK-Means程序输出\n\t预测正确率:88.39%\n\t平均精确率:89%\n\t召回率:0.88\n\tF1指标:0.88\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143801503.JPG)\n\n-\tKNN程序输出\n\t预测正确率:71.05%\n\t平均精确率:86%\n\t召回率:0.71\n\tF1指标:0.70\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143752944.JPG)\n\n-\t原始分类\n\t可以看到这个数据集本身在空间上就比较方便聚类划分\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203100875.gif)\n\n-\t预测分类\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203121208.gif)\n\n\n\n# 改进\n## 未知k的情况\n-\t以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?\n-\t一种方式是canopy算法\n-\t待补充\n\n## 空类的处理\n-\t待补充\n\n## 不同距离计算方式\n-\t待补充\n\n## ANN算法\n","source":"_posts/kmeans.md","raw":"---\ntitle: 机器学习入门:K-Means和KNN\ndate: 2017-03-016 15:51:11\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170306/215446798.jpg\nhtml: true\n---\n-\t以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较\n-   标题本来是K-Means&KNN，把&改成了和，因为标题中出现特殊符号&会导致我的sitemap生成错误......\n***\n\n<!--more-->\n# 简介\n-   K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复\n-   KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别\n\n# K-means++\n-\tk-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点\n-\t算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点\n\t-\t计算每个点$c_i$到已经选出的中心点$k_1,k_2...$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远\n\t-\t将$c_1,c_2,c_3......$的距离归一化，并排成一条线\n\t-\t这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大\n\t-\t在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点\n-\t可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求\n\n# K-Means代码实现\n\n## 数据检视\n-\tIris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143719952.JPG)\n\n\n\n## 初始化数据\n-\t初始化数据\n```Python\n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n```\n\n## k-means++初始化k个点\n-\tD2是每个点的距离(即之前定义的里其他中心点至少有多远)\n-\tprobs归一化\n-\tcumprobs将归一化的概率累加，排列成一条线\n```Python\n    def initk(X_train, k):\n        C = [X_train[0]]\n        for i in range(1, k):\n            D2 = scipy.array([min([scipy.inner(c - x, c - x) for c in C]) for x in X_train])\n            probs = D2 / D2.sum()\n            cumprobs = probs.cumsum()\n            r = scipy.rand()\n            for j, p in enumerate(cumprobs):\n                if r < p:\n                    i = j\n                    break\n            C.append(X_train[i])\n        return C\n    \n```\n\n## 损失评估\n-\t在这里用每个类内点到中心点距离平方和的总和作为损失评估\n```Python\n    def evaluate(C, X_train, y_predict):\n        sum = 0\n        for i in range(len(X_train)):\n            c = C[y_predict[i]]\n            sum += scipy.inner(c - X_train[i], c - X_train[i])\n        return sum\n```\n\n## 聚类\n-\t初始化k个中心点后，所有的点就可以分类\n-\t重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标\n```Python\n    def cluster(C, X_train, y_predict, k):\n        sum = [0, 0, 0, 0] * k\n        count = [0] * k\n        newC = []\n        for i in range(len(X_train)):\n            min = 32768\n            minj = -1\n            for j in range(k):\n                if scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) < min:\n                    min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])\n                    minj = j\n            y_predict[i] = (minj + 1) % k\n        for i in range(len(X_train)):\n            sum[y_predict[i]] += X_train[i]\n            count[y_predict[i]] += 1\n        for i in range(k):\n            newC.append(sum[i] / count[i])\n        return y_predict, newC\n```\n\n## 主函数\n-\t计算损失，更新k个中心点，再站队(聚类)一次\n-\t重复，直到损失变化小于10%\n-\t每次迭代显示新旧损失，显示损失变化\n-\t最后输出分类结果\n```Python\n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        k = 3\n        total = len(y_train)\n        y_predict = [0] * total\n        C = initk(X_train, k)\n        oldeval = evaluate(C, X_train, y_predict)\n        while (1):\n            y_predict, C = cluster(C, X_train, y_predict, k)\n            neweval = evaluate(C, X_train, y_predict)\n            ratio = (oldeval - neweval) / oldeval * 100\n            print(oldeval, \" -> \", neweval, \"%f %%\" % ratio)\n            oldeval = neweval\n            if ratio < 0.1:\n                break\n    \n        print(y_train)\n        print(y_predict)\n        n = 0\n        m = 0\n        for i in range(len(y_train)):\n            m += 1\n            if y_train[i] == y_predict[i]:\n                n += 1\n        print(n / m)\n        print(classification_report(y_train, y_predict, target_names=iris.target_names))\n    \n```\n\n# KNN代码\n-\t直接使用了sklearn中的KNeighborsClassifier\n```Python\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    \n    \n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n    \n    def KNN(X_train, X_test, y_train, y_test, iris):\n        knc = KNeighborsClassifier()\n        knc.fit(X_train, y_train)\n        y_predict = knc.predict(X_test)\n        print(knc.score(X_test, y_test))\n        print(classification_report(y_test, y_predict, target_names=iris.target_names))\n    \n    \n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        KNN(X_train, X_test, y_train, y_test, iris)\n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 预测结果\n-\t指标说明\n\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n\t$$\n\t精确率:P=\\frac{TP}{TP+FP} \\\\\n\t召回率:R=\\frac{TP}{TP+FN} \\\\\n\t1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n\t$$\n-\tK-Means程序输出\n\t预测正确率:88.39%\n\t平均精确率:89%\n\t召回率:0.88\n\tF1指标:0.88\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143801503.JPG)\n\n-\tKNN程序输出\n\t预测正确率:71.05%\n\t平均精确率:86%\n\t召回率:0.71\n\tF1指标:0.70\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143752944.JPG)\n\n-\t原始分类\n\t可以看到这个数据集本身在空间上就比较方便聚类划分\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203100875.gif)\n\n-\t预测分类\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203121208.gif)\n\n\n\n# 改进\n## 未知k的情况\n-\t以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?\n-\t一种方式是canopy算法\n-\t待补充\n\n## 空类的处理\n-\t待补充\n\n## 不同距离计算方式\n-\t待补充\n\n## ANN算法\n","slug":"kmeans","published":1,"updated":"2018-09-02T09:16:00.160Z","comments":1,"layout":"post","link":"","_id":"cjmd072d4001pqcw6vvk1cc8m","content":"<ul><li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li><li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li></ul><hr><a id=\"more\"></a><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><ul><li>K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复</li><li>KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别</li></ul><h1 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means++\"></a>K-means++</h1><ul><li>k-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点</li><li>算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点<ul><li>计算每个点$c_i$到已经选出的中心点$k_1,k_2…$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远</li><li>将$c_1,c_2,c_3……$的距离归一化，并排成一条线</li><li>这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大</li><li>在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点</li></ul></li><li>可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求</li></ul><h1 id=\"K-Means代码实现\"><a href=\"#K-Means代码实现\" class=\"headerlink\" title=\"K-Means代码实现\"></a>K-Means代码实现</h1><h2 id=\"数据检视\"><a href=\"#数据检视\" class=\"headerlink\" title=\"数据检视\"></a>数据检视</h2><ul><li>Iris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143719952.JPG\" alt=\"mark\"></li></ul><h2 id=\"初始化数据\"><a href=\"#初始化数据\" class=\"headerlink\" title=\"初始化数据\"></a>初始化数据</h2><ul><li>初始化数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"k-means-初始化k个点\"><a href=\"#k-means-初始化k个点\" class=\"headerlink\" title=\"k-means++初始化k个点\"></a>k-means++初始化k个点</h2><ul><li>D2是每个点的距离(即之前定义的里其他中心点至少有多远)</li><li>probs归一化</li><li>cumprobs将归一化的概率累加，排列成一条线<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initk</span><span class=\"params\">(X_train, k)</span>:</span></span><br><span class=\"line\">    C = [X_train[<span class=\"number\">0</span>]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, k):</span><br><span class=\"line\">        D2 = scipy.array([min([scipy.inner(c - x, c - x) <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> C]) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X_train])</span><br><span class=\"line\">        probs = D2 / D2.sum()</span><br><span class=\"line\">        cumprobs = probs.cumsum()</span><br><span class=\"line\">        r = scipy.rand()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, p <span class=\"keyword\">in</span> enumerate(cumprobs):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> r &lt; p:</span><br><span class=\"line\">                i = j</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        C.append(X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a>损失评估</h2><ul><li>在这里用每个类内点到中心点距离平方和的总和作为损失评估<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate</span><span class=\"params\">(C, X_train, y_predict)</span>:</span></span><br><span class=\"line\">    sum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        c = C[y_predict[i]]</span><br><span class=\"line\">        sum += scipy.inner(c - X_train[i], c - X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><ul><li>初始化k个中心点后，所有的点就可以分类</li><li>重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cluster</span><span class=\"params\">(C, X_train, y_predict, k)</span>:</span></span><br><span class=\"line\">    sum = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>] * k</span><br><span class=\"line\">    count = [<span class=\"number\">0</span>] * k</span><br><span class=\"line\">    newC = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        min = <span class=\"number\">32768</span></span><br><span class=\"line\">        minj = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) &lt; min:</span><br><span class=\"line\">                min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])</span><br><span class=\"line\">                minj = j</span><br><span class=\"line\">        y_predict[i] = (minj + <span class=\"number\">1</span>) % k</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        sum[y_predict[i]] += X_train[i]</span><br><span class=\"line\">        count[y_predict[i]] += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">        newC.append(sum[i] / count[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_predict, newC</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"主函数\"><a href=\"#主函数\" class=\"headerlink\" title=\"主函数\"></a>主函数</h2><ul><li>计算损失，更新k个中心点，再站队(聚类)一次</li><li>重复，直到损失变化小于10%</li><li>每次迭代显示新旧损失，显示损失变化</li><li>最后输出分类结果<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    k = <span class=\"number\">3</span></span><br><span class=\"line\">    total = len(y_train)</span><br><span class=\"line\">    y_predict = [<span class=\"number\">0</span>] * total</span><br><span class=\"line\">    C = initk(X_train, k)</span><br><span class=\"line\">    oldeval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_predict, C = cluster(C, X_train, y_predict, k)</span><br><span class=\"line\">        neweval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">        ratio = (oldeval - neweval) / oldeval * <span class=\"number\">100</span></span><br><span class=\"line\">        print(oldeval, <span class=\"string\">\" -&gt; \"</span>, neweval, <span class=\"string\">\"%f %%\"</span> % ratio)</span><br><span class=\"line\">        oldeval = neweval</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ratio &lt; <span class=\"number\">0.1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    print(y_train)</span><br><span class=\"line\">    print(y_predict)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    m = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(y_train)):</span><br><span class=\"line\">        m += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> y_train[i] == y_predict[i]:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">    print(n / m)</span><br><span class=\"line\">    print(classification_report(y_train, y_predict, target_names=iris.target_names))</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"KNN代码\"><a href=\"#KNN代码\" class=\"headerlink\" title=\"KNN代码\"></a>KNN代码</h1><ul><li>直接使用了sklearn中的KNeighborsClassifier<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KNN</span><span class=\"params\">(X_train, X_test, y_train, y_test, iris)</span>:</span></span><br><span class=\"line\">    knc = KNeighborsClassifier()</span><br><span class=\"line\">    knc.fit(X_train, y_train)</span><br><span class=\"line\">    y_predict = knc.predict(X_test)</span><br><span class=\"line\">    print(knc.score(X_test, y_test))</span><br><span class=\"line\">    print(classification_report(y_test, y_predict, target_names=iris.target_names))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    KNN(X_train, X_test, y_train, y_test, iris)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h1><ul><li>指标说明<br>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<br>$$<br>精确率:P=\\frac{TP}{TP+FP} \\\\<br>召回率:R=\\frac{TP}{TP+FN} \\\\<br>1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\<br>$$</li><li><p>K-Means程序输出<br>预测正确率:88.39%<br>平均精确率:89%<br>召回率:0.88<br>F1指标:0.88<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143801503.JPG\" alt=\"mark\"></p></li><li><p>KNN程序输出<br>预测正确率:71.05%<br>平均精确率:86%<br>召回率:0.71<br>F1指标:0.70<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143752944.JPG\" alt=\"mark\"></p></li><li><p>原始分类<br>可以看到这个数据集本身在空间上就比较方便聚类划分<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203100875.gif\" alt=\"mark\"></p></li><li><p>预测分类<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203121208.gif\" alt=\"mark\"></p></li></ul><h1 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h1><h2 id=\"未知k的情况\"><a href=\"#未知k的情况\" class=\"headerlink\" title=\"未知k的情况\"></a>未知k的情况</h2><ul><li>以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?</li><li>一种方式是canopy算法</li><li>待补充</li></ul><h2 id=\"空类的处理\"><a href=\"#空类的处理\" class=\"headerlink\" title=\"空类的处理\"></a>空类的处理</h2><ul><li>待补充</li></ul><h2 id=\"不同距离计算方式\"><a href=\"#不同距离计算方式\" class=\"headerlink\" title=\"不同距离计算方式\"></a>不同距离计算方式</h2><ul><li>待补充</li></ul><h2 id=\"ANN算法\"><a href=\"#ANN算法\" class=\"headerlink\" title=\"ANN算法\"></a>ANN算法</h2>","site":{"data":{}},"excerpt":"<ul><li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li><li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li></ul><hr>","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><ul><li>K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复</li><li>KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别</li></ul><h1 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means++\"></a>K-means++</h1><ul><li>k-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点</li><li>算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点<ul><li>计算每个点$c_i$到已经选出的中心点$k_1,k_2…$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远</li><li>将$c_1,c_2,c_3……$的距离归一化，并排成一条线</li><li>这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大</li><li>在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点</li></ul></li><li>可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求</li></ul><h1 id=\"K-Means代码实现\"><a href=\"#K-Means代码实现\" class=\"headerlink\" title=\"K-Means代码实现\"></a>K-Means代码实现</h1><h2 id=\"数据检视\"><a href=\"#数据检视\" class=\"headerlink\" title=\"数据检视\"></a>数据检视</h2><ul><li>Iris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143719952.JPG\" alt=\"mark\"></li></ul><h2 id=\"初始化数据\"><a href=\"#初始化数据\" class=\"headerlink\" title=\"初始化数据\"></a>初始化数据</h2><ul><li>初始化数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"k-means-初始化k个点\"><a href=\"#k-means-初始化k个点\" class=\"headerlink\" title=\"k-means++初始化k个点\"></a>k-means++初始化k个点</h2><ul><li>D2是每个点的距离(即之前定义的里其他中心点至少有多远)</li><li>probs归一化</li><li>cumprobs将归一化的概率累加，排列成一条线<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initk</span><span class=\"params\">(X_train, k)</span>:</span></span><br><span class=\"line\">    C = [X_train[<span class=\"number\">0</span>]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, k):</span><br><span class=\"line\">        D2 = scipy.array([min([scipy.inner(c - x, c - x) <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> C]) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X_train])</span><br><span class=\"line\">        probs = D2 / D2.sum()</span><br><span class=\"line\">        cumprobs = probs.cumsum()</span><br><span class=\"line\">        r = scipy.rand()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, p <span class=\"keyword\">in</span> enumerate(cumprobs):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> r &lt; p:</span><br><span class=\"line\">                i = j</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        C.append(X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a>损失评估</h2><ul><li>在这里用每个类内点到中心点距离平方和的总和作为损失评估<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate</span><span class=\"params\">(C, X_train, y_predict)</span>:</span></span><br><span class=\"line\">    sum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        c = C[y_predict[i]]</span><br><span class=\"line\">        sum += scipy.inner(c - X_train[i], c - X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><ul><li>初始化k个中心点后，所有的点就可以分类</li><li>重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cluster</span><span class=\"params\">(C, X_train, y_predict, k)</span>:</span></span><br><span class=\"line\">    sum = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>] * k</span><br><span class=\"line\">    count = [<span class=\"number\">0</span>] * k</span><br><span class=\"line\">    newC = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        min = <span class=\"number\">32768</span></span><br><span class=\"line\">        minj = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) &lt; min:</span><br><span class=\"line\">                min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])</span><br><span class=\"line\">                minj = j</span><br><span class=\"line\">        y_predict[i] = (minj + <span class=\"number\">1</span>) % k</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        sum[y_predict[i]] += X_train[i]</span><br><span class=\"line\">        count[y_predict[i]] += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">        newC.append(sum[i] / count[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_predict, newC</span><br></pre></td></tr></table></figure></li></ul><h2 id=\"主函数\"><a href=\"#主函数\" class=\"headerlink\" title=\"主函数\"></a>主函数</h2><ul><li>计算损失，更新k个中心点，再站队(聚类)一次</li><li>重复，直到损失变化小于10%</li><li>每次迭代显示新旧损失，显示损失变化</li><li>最后输出分类结果<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    k = <span class=\"number\">3</span></span><br><span class=\"line\">    total = len(y_train)</span><br><span class=\"line\">    y_predict = [<span class=\"number\">0</span>] * total</span><br><span class=\"line\">    C = initk(X_train, k)</span><br><span class=\"line\">    oldeval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_predict, C = cluster(C, X_train, y_predict, k)</span><br><span class=\"line\">        neweval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">        ratio = (oldeval - neweval) / oldeval * <span class=\"number\">100</span></span><br><span class=\"line\">        print(oldeval, <span class=\"string\">\" -&gt; \"</span>, neweval, <span class=\"string\">\"%f %%\"</span> % ratio)</span><br><span class=\"line\">        oldeval = neweval</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ratio &lt; <span class=\"number\">0.1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    print(y_train)</span><br><span class=\"line\">    print(y_predict)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    m = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(y_train)):</span><br><span class=\"line\">        m += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> y_train[i] == y_predict[i]:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">    print(n / m)</span><br><span class=\"line\">    print(classification_report(y_train, y_predict, target_names=iris.target_names))</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"KNN代码\"><a href=\"#KNN代码\" class=\"headerlink\" title=\"KNN代码\"></a>KNN代码</h1><ul><li>直接使用了sklearn中的KNeighborsClassifier<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KNN</span><span class=\"params\">(X_train, X_test, y_train, y_test, iris)</span>:</span></span><br><span class=\"line\">    knc = KNeighborsClassifier()</span><br><span class=\"line\">    knc.fit(X_train, y_train)</span><br><span class=\"line\">    y_predict = knc.predict(X_test)</span><br><span class=\"line\">    print(knc.score(X_test, y_test))</span><br><span class=\"line\">    print(classification_report(y_test, y_predict, target_names=iris.target_names))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    KNN(X_train, X_test, y_train, y_test, iris)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h1><ul><li>指标说明<br>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<br>$$<br>精确率:P=\\frac{TP}{TP+FP} \\\\<br>召回率:R=\\frac{TP}{TP+FN} \\\\<br>1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\<br>$$</li><li><p>K-Means程序输出<br>预测正确率:88.39%<br>平均精确率:89%<br>召回率:0.88<br>F1指标:0.88<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143801503.JPG\" alt=\"mark\"></p></li><li><p>KNN程序输出<br>预测正确率:71.05%<br>平均精确率:86%<br>召回率:0.71<br>F1指标:0.70<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/143752944.JPG\" alt=\"mark\"></p></li><li><p>原始分类<br>可以看到这个数据集本身在空间上就比较方便聚类划分<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203100875.gif\" alt=\"mark\"></p></li><li><p>预测分类<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170307/203121208.gif\" alt=\"mark\"></p></li></ul><h1 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h1><h2 id=\"未知k的情况\"><a href=\"#未知k的情况\" class=\"headerlink\" title=\"未知k的情况\"></a>未知k的情况</h2><ul><li>以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?</li><li>一种方式是canopy算法</li><li>待补充</li></ul><h2 id=\"空类的处理\"><a href=\"#空类的处理\" class=\"headerlink\" title=\"空类的处理\"></a>空类的处理</h2><ul><li>待补充</li></ul><h2 id=\"不同距离计算方式\"><a href=\"#不同距离计算方式\" class=\"headerlink\" title=\"不同距离计算方式\"></a>不同距离计算方式</h2><ul><li>待补充</li></ul><h2 id=\"ANN算法\"><a href=\"#ANN算法\" class=\"headerlink\" title=\"ANN算法\"></a>ANN算法</h2>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Sun Sep 02 2018 17:16:00 GMT+0800 (中国标准时间)","title":"机器学习入门:K-Means和KNN","path":"2017/03/16/kmeans/","eyeCatchImage":null,"excerpt":"<ul><li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li><li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li></ul><hr>","date":"2017-03-16T07:51:11.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:Mnist(SVM,PCA)","mathjax":true,"date":"2017-03-16T02:35:04.000Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170308/184051988.JPG"],"_content":"在mnist手写数字识别集上测试特征降维前后用SVM进行识别的准确度和时间\n使用主成分分析方法进行特征降维\n\n<!--more-->\n\n# Mnist\n-\tMnist的训练集有60000条数据，每条数据是8*8点阵图像，代表一个手写数字，因此有64维，在线性SVM中训练需要很长时间，如果通过PCA降维可以在损失少量精确度的情况下大大缩短训练时间\n\n# SVM\n\n# PCA\n\n# 代码\n```Python\n    from tools.data_util import DataUtils\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.svm import LinearSVC\n    from sklearn.metrics import classification_report\n    from sklearn.decomposition import PCA\n    from matplotlib import pyplot as plt\n    import numpy as np\n    \n    \n    def init():\n        trainfile_X = r'E:\\Machine Learning\\MLData\\mnist\\train-images.idx3-ubyte'\n        trainfile_y = r'E:\\Machine Learning\\MLData\\mnist\\train-labels.idx1-ubyte'\n        testfile_X = r'E:\\Machine Learning\\MLData\\mnist\\t10k-images.idx3-ubyte'\n        testfile_y = r'E:\\Machine Learning\\MLData\\mnist\\t10k-labels.idx1-ubyte'\n        train_X = DataUtils(filename=trainfile_X).getImage()\n        train_y = DataUtils(filename=trainfile_y).getLabel()\n        test_X = DataUtils(testfile_X).getImage()\n        test_y = DataUtils(testfile_y).getLabel()\n        ss = StandardScaler()\n        train_X = ss.fit_transform(train_X)\n        test_X = ss.transform(test_X)\n        return train_X, train_y, test_X, test_y\n    \n    \n    def LSVC(train_X, train_y, test_X, test_y):\n        lsvc = LinearSVC()\n        lsvc.fit(train_X, train_y)\n        predict_y = lsvc.predict(test_X)\n    \n        print(lsvc.score(test_X, test_y))\n        print(classification_report(test_y, predict_y))\n    \n    \n    def PrincipalComponentAnalysis(train_X, train_y, test_X, test_y):\n        estimator = PCA(n_components=20)\n    \n        train_X_pca = estimator.fit_transform(train_X)\n        test_X_pca = estimator.transform(test_X)\n        lsvc = LinearSVC()\n        lsvc.fit(train_X_pca, train_y)\n        predict_y = lsvc.predict(test_X_pca)\n    \n        print(lsvc.score(test_X_pca, test_y))\n        print(classification_report(test_y, predict_y))\n    \n    \n    def main():\n        train_X, train_y, test_X, test_y = init()\n        print(train_X.shape[0])\n        # LSVC(train_X, train_y, test_X, test_y)\n        # PrincipalComponentAnalysis(train_X, train_y, test_X, test_y)\n    \n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 结果\n-\t降维前\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155543898.JPG)\n-\t降维后\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155614953.JPG)\n-\t原始64维的数据在我的机器上跑了将近20分钟，降到20维后3分钟就输出了结果，对比一下各项性能指标，下降了4%左右\n\t\n","source":"_posts/mnist.md","raw":"---\ntitle: 机器学习入门:Mnist(SVM,PCA)\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\ndate: 2017-03-16 10:35:04\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/20170308/184051988.JPG\t\n---\n在mnist手写数字识别集上测试特征降维前后用SVM进行识别的准确度和时间\n使用主成分分析方法进行特征降维\n\n<!--more-->\n\n# Mnist\n-\tMnist的训练集有60000条数据，每条数据是8*8点阵图像，代表一个手写数字，因此有64维，在线性SVM中训练需要很长时间，如果通过PCA降维可以在损失少量精确度的情况下大大缩短训练时间\n\n# SVM\n\n# PCA\n\n# 代码\n```Python\n    from tools.data_util import DataUtils\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.svm import LinearSVC\n    from sklearn.metrics import classification_report\n    from sklearn.decomposition import PCA\n    from matplotlib import pyplot as plt\n    import numpy as np\n    \n    \n    def init():\n        trainfile_X = r'E:\\Machine Learning\\MLData\\mnist\\train-images.idx3-ubyte'\n        trainfile_y = r'E:\\Machine Learning\\MLData\\mnist\\train-labels.idx1-ubyte'\n        testfile_X = r'E:\\Machine Learning\\MLData\\mnist\\t10k-images.idx3-ubyte'\n        testfile_y = r'E:\\Machine Learning\\MLData\\mnist\\t10k-labels.idx1-ubyte'\n        train_X = DataUtils(filename=trainfile_X).getImage()\n        train_y = DataUtils(filename=trainfile_y).getLabel()\n        test_X = DataUtils(testfile_X).getImage()\n        test_y = DataUtils(testfile_y).getLabel()\n        ss = StandardScaler()\n        train_X = ss.fit_transform(train_X)\n        test_X = ss.transform(test_X)\n        return train_X, train_y, test_X, test_y\n    \n    \n    def LSVC(train_X, train_y, test_X, test_y):\n        lsvc = LinearSVC()\n        lsvc.fit(train_X, train_y)\n        predict_y = lsvc.predict(test_X)\n    \n        print(lsvc.score(test_X, test_y))\n        print(classification_report(test_y, predict_y))\n    \n    \n    def PrincipalComponentAnalysis(train_X, train_y, test_X, test_y):\n        estimator = PCA(n_components=20)\n    \n        train_X_pca = estimator.fit_transform(train_X)\n        test_X_pca = estimator.transform(test_X)\n        lsvc = LinearSVC()\n        lsvc.fit(train_X_pca, train_y)\n        predict_y = lsvc.predict(test_X_pca)\n    \n        print(lsvc.score(test_X_pca, test_y))\n        print(classification_report(test_y, predict_y))\n    \n    \n    def main():\n        train_X, train_y, test_X, test_y = init()\n        print(train_X.shape[0])\n        # LSVC(train_X, train_y, test_X, test_y)\n        # PrincipalComponentAnalysis(train_X, train_y, test_X, test_y)\n    \n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 结果\n-\t降维前\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155543898.JPG)\n-\t降维后\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155614953.JPG)\n-\t原始64维的数据在我的机器上跑了将近20分钟，降到20维后3分钟就输出了结果，对比一下各项性能指标，下降了4%左右\n\t\n","slug":"mnist","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","link":"","_id":"cjmd072d5001rqcw6f90w9tcn","content":"<p>在mnist手写数字识别集上测试特征降维前后用SVM进行识别的准确度和时间<br>使用主成分分析方法进行特征降维</p><a id=\"more\"></a><h1 id=\"Mnist\"><a href=\"#Mnist\" class=\"headerlink\" title=\"Mnist\"></a>Mnist</h1><ul><li>Mnist的训练集有60000条数据，每条数据是8*8点阵图像，代表一个手写数字，因此有64维，在线性SVM中训练需要很长时间，如果通过PCA降维可以在损失少量精确度的情况下大大缩短训练时间</li></ul><h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tools.data_util <span class=\"keyword\">import</span> DataUtils</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    trainfile_X = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\train-images.idx3-ubyte'</span></span><br><span class=\"line\">    trainfile_y = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\train-labels.idx1-ubyte'</span></span><br><span class=\"line\">    testfile_X = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\t10k-images.idx3-ubyte'</span></span><br><span class=\"line\">    testfile_y = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\t10k-labels.idx1-ubyte'</span></span><br><span class=\"line\">    train_X = DataUtils(filename=trainfile_X).getImage()</span><br><span class=\"line\">    train_y = DataUtils(filename=trainfile_y).getLabel()</span><br><span class=\"line\">    test_X = DataUtils(testfile_X).getImage()</span><br><span class=\"line\">    test_y = DataUtils(testfile_y).getLabel()</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    train_X = ss.fit_transform(train_X)</span><br><span class=\"line\">    test_X = ss.transform(test_X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_X, train_y, test_X, test_y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LSVC</span><span class=\"params\">(train_X, train_y, test_X, test_y)</span>:</span></span><br><span class=\"line\">    lsvc = LinearSVC()</span><br><span class=\"line\">    lsvc.fit(train_X, train_y)</span><br><span class=\"line\">    predict_y = lsvc.predict(test_X)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(lsvc.score(test_X, test_y))</span><br><span class=\"line\">    print(classification_report(test_y, predict_y))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrincipalComponentAnalysis</span><span class=\"params\">(train_X, train_y, test_X, test_y)</span>:</span></span><br><span class=\"line\">    estimator = PCA(n_components=<span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    train_X_pca = estimator.fit_transform(train_X)</span><br><span class=\"line\">    test_X_pca = estimator.transform(test_X)</span><br><span class=\"line\">    lsvc = LinearSVC()</span><br><span class=\"line\">    lsvc.fit(train_X_pca, train_y)</span><br><span class=\"line\">    predict_y = lsvc.predict(test_X_pca)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(lsvc.score(test_X_pca, test_y))</span><br><span class=\"line\">    print(classification_report(test_y, predict_y))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_X, train_y, test_X, test_y = init()</span><br><span class=\"line\">    print(train_X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    <span class=\"comment\"># LSVC(train_X, train_y, test_X, test_y)</span></span><br><span class=\"line\">    <span class=\"comment\"># PrincipalComponentAnalysis(train_X, train_y, test_X, test_y)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure><h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul><li>降维前<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155543898.JPG\" alt=\"mark\"></li><li>降维后<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155614953.JPG\" alt=\"mark\"></li><li>原始64维的数据在我的机器上跑了将近20分钟，降到20维后3分钟就输出了结果，对比一下各项性能指标，下降了4%左右</li></ul>","site":{"data":{}},"excerpt":"<p>在mnist手写数字识别集上测试特征降维前后用SVM进行识别的准确度和时间<br>使用主成分分析方法进行特征降维</p>","more":"<h1 id=\"Mnist\"><a href=\"#Mnist\" class=\"headerlink\" title=\"Mnist\"></a>Mnist</h1><ul><li>Mnist的训练集有60000条数据，每条数据是8*8点阵图像，代表一个手写数字，因此有64维，在线性SVM中训练需要很长时间，如果通过PCA降维可以在损失少量精确度的情况下大大缩短训练时间</li></ul><h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tools.data_util <span class=\"keyword\">import</span> DataUtils</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> LinearSVC</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    trainfile_X = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\train-images.idx3-ubyte'</span></span><br><span class=\"line\">    trainfile_y = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\train-labels.idx1-ubyte'</span></span><br><span class=\"line\">    testfile_X = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\t10k-images.idx3-ubyte'</span></span><br><span class=\"line\">    testfile_y = <span class=\"string\">r'E:\\Machine Learning\\MLData\\mnist\\t10k-labels.idx1-ubyte'</span></span><br><span class=\"line\">    train_X = DataUtils(filename=trainfile_X).getImage()</span><br><span class=\"line\">    train_y = DataUtils(filename=trainfile_y).getLabel()</span><br><span class=\"line\">    test_X = DataUtils(testfile_X).getImage()</span><br><span class=\"line\">    test_y = DataUtils(testfile_y).getLabel()</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    train_X = ss.fit_transform(train_X)</span><br><span class=\"line\">    test_X = ss.transform(test_X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_X, train_y, test_X, test_y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">LSVC</span><span class=\"params\">(train_X, train_y, test_X, test_y)</span>:</span></span><br><span class=\"line\">    lsvc = LinearSVC()</span><br><span class=\"line\">    lsvc.fit(train_X, train_y)</span><br><span class=\"line\">    predict_y = lsvc.predict(test_X)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(lsvc.score(test_X, test_y))</span><br><span class=\"line\">    print(classification_report(test_y, predict_y))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">PrincipalComponentAnalysis</span><span class=\"params\">(train_X, train_y, test_X, test_y)</span>:</span></span><br><span class=\"line\">    estimator = PCA(n_components=<span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    train_X_pca = estimator.fit_transform(train_X)</span><br><span class=\"line\">    test_X_pca = estimator.transform(test_X)</span><br><span class=\"line\">    lsvc = LinearSVC()</span><br><span class=\"line\">    lsvc.fit(train_X_pca, train_y)</span><br><span class=\"line\">    predict_y = lsvc.predict(test_X_pca)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(lsvc.score(test_X_pca, test_y))</span><br><span class=\"line\">    print(classification_report(test_y, predict_y))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_X, train_y, test_X, test_y = init()</span><br><span class=\"line\">    print(train_X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">    <span class=\"comment\"># LSVC(train_X, train_y, test_X, test_y)</span></span><br><span class=\"line\">    <span class=\"comment\"># PrincipalComponentAnalysis(train_X, train_y, test_X, test_y)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure><h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul><li>降维前<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155543898.JPG\" alt=\"mark\"></li><li>降维后<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170316/155614953.JPG\" alt=\"mark\"></li><li>原始64维的数据在我的机器上跑了将近20分钟，降到20维后3分钟就输出了结果，对比一下各项性能指标，下降了4%左右</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"机器学习入门:Mnist(SVM,PCA)","path":"2017/03/16/mnist/","eyeCatchImage":null,"excerpt":"<p>在mnist手写数字识别集上测试特征降维前后用SVM进行识别的准确度和时间<br>使用主成分分析方法进行特征降维</p>","date":"2017-03-16T02:35:04.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Numpy Cookbook","date":"2017-01-23T12:12:40.000Z","_content":"Cookbook网址：[Numpy Cookbook](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\nNumpy的一些语法查询和总结\n持续更新\n\n<!--more-->\n\n- shape()\nshape是numpy函数库中的方法，用于查看矩阵或者数组的维素\nshape(array) 若矩阵有m行n列，则返回(m,n)\narray.shape[0] 返回矩阵的行数m，参数为1的话返回列数n\n\n- tile()\ntile是numpy函数库中的方法，用法如下:\ntile(A,(m,n))  将数组A作为元素构造出m行n列的数组\n\n- sum()\nsum()是numpy函数库中的方法\narray.sum(axis=1)按行累加，axis=0为按列累加\n\n- argsort()\nargsort()是numpy中的方法，得到矩阵中每个元素的排序序号 \nA=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标\n\n- dict.get(key,x)\nPython中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0\n\n- sorted()\n\n- numpy中有min()、max()方法，用法如下\narray.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值\narray.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值\n\n-\tlistdir('str')\nstrlist=listdir('str')  读取目录str下的所有文件名，返回一个字符串列表\n\n-\tsplit()\npython中的方法，切片函数\nstring.split('str')以字符str为分隔符切片，返回list\n\n-\tzeros\na=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵","source":"_posts/numpycookbook.md","raw":"---\ntitle: Numpy Cookbook\ndate: 2017-01-23 20:12:40\ntags: [math,machinelearning,python,code]\ncategories: Python\n---\nCookbook网址：[Numpy Cookbook](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\nNumpy的一些语法查询和总结\n持续更新\n\n<!--more-->\n\n- shape()\nshape是numpy函数库中的方法，用于查看矩阵或者数组的维素\nshape(array) 若矩阵有m行n列，则返回(m,n)\narray.shape[0] 返回矩阵的行数m，参数为1的话返回列数n\n\n- tile()\ntile是numpy函数库中的方法，用法如下:\ntile(A,(m,n))  将数组A作为元素构造出m行n列的数组\n\n- sum()\nsum()是numpy函数库中的方法\narray.sum(axis=1)按行累加，axis=0为按列累加\n\n- argsort()\nargsort()是numpy中的方法，得到矩阵中每个元素的排序序号 \nA=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标\n\n- dict.get(key,x)\nPython中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0\n\n- sorted()\n\n- numpy中有min()、max()方法，用法如下\narray.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值\narray.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值\n\n-\tlistdir('str')\nstrlist=listdir('str')  读取目录str下的所有文件名，返回一个字符串列表\n\n-\tsplit()\npython中的方法，切片函数\nstring.split('str')以字符str为分隔符切片，返回list\n\n-\tzeros\na=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵","slug":"numpycookbook","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072d7001wqcw6iml78k1e","content":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p><a id=\"more\"></a><ul><li><p>shape()<br>shape是numpy函数库中的方法，用于查看矩阵或者数组的维素<br>shape(array) 若矩阵有m行n列，则返回(m,n)<br>array.shape[0] 返回矩阵的行数m，参数为1的话返回列数n</p></li><li><p>tile()<br>tile是numpy函数库中的方法，用法如下:<br>tile(A,(m,n)) 将数组A作为元素构造出m行n列的数组</p></li><li><p>sum()<br>sum()是numpy函数库中的方法<br>array.sum(axis=1)按行累加，axis=0为按列累加</p></li><li><p>argsort()<br>argsort()是numpy中的方法，得到矩阵中每个元素的排序序号<br>A=array.argsort() A[0]表示排序后 排在第一个的那个数在原来数组中的下标</p></li><li><p>dict.get(key,x)<br>Python中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0</p></li><li><p>sorted()</p></li><li><p>numpy中有min()、max()方法，用法如下<br>array.min(0) 返回一个数组，数组中每个数都是它所在列的所有数的最小值<br>array.min(1) 返回一个数组，数组中每个数都是它所在行的所有数的最小值</p></li><li><p>listdir(‘str’)<br>strlist=listdir(‘str’) 读取目录str下的所有文件名，返回一个字符串列表</p></li><li><p>split()<br>python中的方法，切片函数<br>string.split(‘str’)以字符str为分隔符切片，返回list</p></li><li><p>zeros<br>a=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵</p></li></ul>","site":{"data":{}},"excerpt":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p>","more":"<ul><li><p>shape()<br>shape是numpy函数库中的方法，用于查看矩阵或者数组的维素<br>shape(array) 若矩阵有m行n列，则返回(m,n)<br>array.shape[0] 返回矩阵的行数m，参数为1的话返回列数n</p></li><li><p>tile()<br>tile是numpy函数库中的方法，用法如下:<br>tile(A,(m,n)) 将数组A作为元素构造出m行n列的数组</p></li><li><p>sum()<br>sum()是numpy函数库中的方法<br>array.sum(axis=1)按行累加，axis=0为按列累加</p></li><li><p>argsort()<br>argsort()是numpy中的方法，得到矩阵中每个元素的排序序号<br>A=array.argsort() A[0]表示排序后 排在第一个的那个数在原来数组中的下标</p></li><li><p>dict.get(key,x)<br>Python中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0</p></li><li><p>sorted()</p></li><li><p>numpy中有min()、max()方法，用法如下<br>array.min(0) 返回一个数组，数组中每个数都是它所在列的所有数的最小值<br>array.min(1) 返回一个数组，数组中每个数都是它所在行的所有数的最小值</p></li><li><p>listdir(‘str’)<br>strlist=listdir(‘str’) 读取目录str下的所有文件名，返回一个字符串列表</p></li><li><p>split()<br>python中的方法，切片函数<br>string.split(‘str’)以字符str为分隔符切片，返回list</p></li><li><p>zeros<br>a=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵</p></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Numpy Cookbook","path":"2017/01/23/numpycookbook/","eyeCatchImage":null,"excerpt":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p>","date":"2017-01-23T12:12:40.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["math","machinelearning","code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"LDA学习笔记","date":"2018-07-23T01:56:41.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim"],"html":true,"_content":"***\nLatent Dirichlet Allocation 文档主题生成模型学习笔记\n主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。\n\n<!--more-->\n\n# 数学基础\n## Gamma函数\n-\t定义\n$$\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt\n$$\n-\t因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数\n-\tBohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足\n\t-\t$f(1)=1$\n\t-\t$f(x+1)=xf(x)$\n\t-\t$log f(x)$是凸函数\n\t那么$f(x)=\\Gamma(x)$\n-\tDigamma函数\n$$\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}\n$$\n\t其具有以下性质\n$$\n\\psi (x+1)=\\psi (x)+\\frac 1x\n$$\n-\t在用变分推断对LDA进行推断时结果就是digamma函数的形式\n\n## Gamma分布\n-\t将上式变换\n$$\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1\n$$\n\t因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：\n$$\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}\n$$\n-\t指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。\n-\tGamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。\n$$\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}\n$$\n-\t令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。\n\n## Beta分布\n-\t背景：\n\t-\t现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？\n\t-\t为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$\n\t-\t将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为\n$$\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n    &=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}\n$$\n\t-\t若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数\n$$\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]\n$$\n\t用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:\n$$\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n    &=Beta(p|k,n-k+1) \\\\\n    &=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}\n$$\n\t这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。\n-\tBeta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。\n-\t因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。\n\n## Beta-Binomial共轭\n-\t这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。\n-\t假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$\n-\t补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$\n\n| 先验      |  数据知识 | 后验  |\n| :-------: | :-------:| :--: |\n| Beta分布  | 二项分布 |   Beta分布   |\n-\t因此我们可以得到Beta-Binomial共轭\n$$\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)\n$$\n\t即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。\n-\t可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$\n-\t通过这个共轭，我们可以推出关于二项分布的一个重要公式：\n$$\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)\n$$\n\t现在可以证明如下：\n\t-\t式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分\n\t-\t取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$\n\t-\t此时可以得到$P(C \\leq k)=P(X_{k+1}>p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个\n\t-\t这时利用我们对第k大数的概率密度计算出为Beta分布，带入有\n$$\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n\t\t\t&=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n\t\t\t&=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}\n$$\n\t即证\n-\t通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。\n-\t在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。\n\n## Dirichlet-Multinomial共轭\n-\t假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？\n-\t同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}\n$$\n-\t整理一下可以写成\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}\n$$\n\t这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。\n-\t同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布\n$$\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)\n$$\n\t上式中的参数均是向量，对应多维情况。\n-\t无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。\n\n## 总结\n-\t总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。\n-\tLDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。\n-\t为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180828/B5IBgB1kbK.PNG)\n\n-\t$\\alpha$控制了多项式分布参数的mean shape和sparsity。\n-\t最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。\n-\t中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。\n-\t最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）\n-\t因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。\n-\t当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。\n-\t再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：\n{% blockquote Dirichlet_distribution https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters Intuitive interpretations of the parameters %}\nThe concentration parameter\nDirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how \"concentrated\" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.\n{% endblockquote %}\n-\t当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。\n-\t在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。\n-\t对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。\n\n# 马卡洛夫蒙特卡洛和吉步斯采样\n## 随机模拟\n-\t即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本。\n-\t马卡洛夫链是一种蒙特卡洛方法依赖的原理\n-\tMCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。\n\n## 马氏链\n-\t马氏链即状态转移的概率只依赖于前一个状态\n-\t因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果\n-\t矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布\n-\t关于马氏链收敛的定义\n\t-\t如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$\n\t-\t$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$\n\t-\t$\\pi$是方程$\\pi P = \\pi $的唯一非负解\n\t-\t$\\pi$称为马氏链的平稳分布\n\n## Markov Chain Monte Carlo\n-\t回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。\n-\t因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。\n-\t设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：\n$$\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)\n$$\n\t为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。\n-\t如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。\n-\t因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。\n-\t这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。\n\n## Gibbs Sampling\n-\t之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。\n-\t对二维概率分布$p(x,y)$，易得到\n$$\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}\n$$\n-\t从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：\n\t-\t若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$\n\t-\t若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$\n\t-\t若两点连线既不垂直也水平，则$Q=0$\n\t这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二位平面上的马氏链将收敛到$p(x,y)$。\n-\t因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，等到马氏链收敛之后形成的状态序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。\n-\t同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。\n\n## 总结\n-\t首先明确，MCMC方法是产生已知复杂分布的样本，但是实际应用时用的是转移分布，因此当复杂分布未知，但转移分布已知时，我们依然可以采样，而且可以根据采样推断这个复杂分布的参数。\n-\t应用于LDA中，我们知道复杂分布的形式$p(z_i=k|\\mathop{w}^{\\rightarrow})$，但不知道参数，而转移分布$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$实际上是可以计算出来的，因此我们可以用gibbs采样做LDA模型的推断。\n-\tgibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。\n-\t更新被采样单词的主题，不仅更新了gibbs采样的转移矩阵，同时也更新整个模型收敛。事实上这种更新非常巧妙：我只是将每个单词属于哪个主题改变了，也就是矩阵中某些项加1，某些项减1，但是这样一来最后得到的矩阵就是收敛之后的主题到词分布，也就是我们想要的复杂分布参数；同时单词，在数据里就是按文章组织起来的，自然文章到主题的分布也在更新，最后我们也可以得到这个分布用于模型推断未知文章；下文会讲到gibbs采样的公式，也仅仅只用主题-单词矩阵统计一下就计算出来了，因此gibbs采样本身也可以通过这种方法迭代更新！\n-\t既然使用了gibbs采样，那么只要保证能正确更新迭代，则初始状态不会影响到模型收敛，因此在采样前每个单词的主题可以随机初始化。这也说明了在LDA中gibbs采样不是鸡生蛋，蛋生鸡的矛盾，我们并不是从正确的数据中用初始的错误的转移矩阵采样，再用采样出来的错误样本点更新模型，虽然一开始的转移矩阵是错的，但是只要满足gibbs采样的公式，则转移矩阵会不断更新迭代直到正确，至于采样出来的样本点，一开始是错误的，但随着转移矩阵更新，最后也会正确采样。转移矩阵最终收敛不会受错误样本点影响，而是由其计算公式决定。\n-\t如何推断？当我们采样到词对应的主题之后，就将主题更新，这样转移分布就会改变，然后重新采样直到收敛。主题不断更新的过程中隐变量也在不断逼近正确值，当我们用困惑度或者其他方式衡量模型已经收敛到合适的程度时，拿到的隐变量就是推断的结果（实际上主要需要的就是两个多项式分布，这些本身在代码中是以矩阵预设并保存的，可以直接读取）。\n\n# 文本建模\n-\t接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：\n\t-\t模型是怎样的？\n\t-\t各个词的生成概率或者说模型参数是多少？\n\n## Unigram模型\n-\t模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率，贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。\n-\t也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：\n$$\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}\n$$\n-\t按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：\n$$\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}\n$$\n-\t既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：\n$$\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}\n$$\n\t其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：\n$$\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}\n$$\n-\t得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：\n$$\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}\n$$\n\t这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。\n-\t现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：\n$$\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}\n$$\n\t将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。\n$$\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}\n$$\n\t其中$\\Delta$是归一化因子：\n$$\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}\n$$\n\n## PLSA模型\n-\tPLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。\n-\t事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。\n-\tPLSA模型可以用EM算法迭代学习到参数。\n\n## 总结\n-\t现在整理一下，Unigram模型中主要包含两部分\n\t-\t词生成概率分布\n\t-\t词生成概率分布的分布\n-\tPLSA模型主要包含两部分\n\t-\t词生成概率分布\n\t-\t主题生成概率分布\n-\tUnigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了\n-\tPLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。\n\n# LDA文本建模\n## 模型概述\n-\tLDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设\n\t-\t词生成概率分布（暂记A）\n\t-\t词生成概率分布的分布（暂记B）\n\t-\t主题生成概率分布（暂记C）\n\t-\t主题生成概率分布的分布（暂记D）\n-\t这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：\n\t-\t先在B分布条件下抽样得到K个A分布\n\t-\t对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：\n\t\t-\t从C分布中抽样得到一个主题z\n\t\t-\t选择K个A分布中第z个，从这个A分布中抽样得到一个单词\n-\t假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：\n\n-\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180725/lb2HJm3i59.PNG)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim)\n\n## 建立分布\n-\t现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：\n$$\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}\n$$\n-\t最终得到词与主题的联合分布:\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n\n\n## 采样\n-\t首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。\n-\t在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。\n\t-\t其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。\n\t-\t采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。\n-\t公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导\n-\t基于共轭关系推导如下：\n-\t采样的对象是词所对应的主题，概率为：\n$$\np(z_i=k|\\mathop{w}^{\\rightarrow})\n$$\n-\t使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})\n$$\n-\t由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：\n$$\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})\n$$\n-\t把这个公式按主题分布和词分布展开：\n$$\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t由于所有的共轭都是独立的，上式可以写成：\n$$\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：\n$$\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：\n$$\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\\n$$\n-\t而根据共轭关系，有\n$$\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\\n$$\n-\t因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，因此最后的概率计算出来就是（注意是正比于）：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t我们再看看基于联合分布如何推导\n-\t之前我们已经得到词和主题的联合分布：\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n-\t根据贝叶斯公式有\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\\n$$\n-\t因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：\n$$\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}\n$$\n-\t将$\\Delta$的表达式带入计算，也可以得到：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180829/31jGjg0idC.png?imageslim)\n\n-\t可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：\n$$\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}\n$$\n## 训练与推断\n-\t接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：\n\t-\t迭代什么？词对应的主题\n\t-\t根据什么迭代？通过Gibbs采样得到词的主题\n\t-\t迭代之后的效果？根据Dirichlet-Multinomial共轭更新Gibbs采样的概率\n\t-\t迭代到什么时候为止？Gibbs采样收敛，即采样前后的主题相同(很难达到），或者根据困惑度等指标来衡量模型收敛的程度。\n-\t训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。\n-\tLDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。\n\n# LDA in Gensim\n-\tGensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：\n{% blockquote Gensim https://radimrehurek.com/gensim/models/ldamodel.html models.ldamodel – Latent Dirichlet Allocation %}\nalpha ({numpy.ndarray, str}, optional) –\nCan be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n’default’: Learns an asymmetric prior from the corpus.\n{% endblockquote %}\n-\t具体分析待补充","source":"_posts/lda.md","raw":"---\ntitle: LDA学习笔记\ndate: 2018-07-23 09:56:41\ncategories: 机器学习\ntags:\n  - lda\n  - math\n  -\tmcmc\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim\nhtml: true\n---\n***\nLatent Dirichlet Allocation 文档主题生成模型学习笔记\n主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。\n\n<!--more-->\n\n# 数学基础\n## Gamma函数\n-\t定义\n$$\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt\n$$\n-\t因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数\n-\tBohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足\n\t-\t$f(1)=1$\n\t-\t$f(x+1)=xf(x)$\n\t-\t$log f(x)$是凸函数\n\t那么$f(x)=\\Gamma(x)$\n-\tDigamma函数\n$$\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}\n$$\n\t其具有以下性质\n$$\n\\psi (x+1)=\\psi (x)+\\frac 1x\n$$\n-\t在用变分推断对LDA进行推断时结果就是digamma函数的形式\n\n## Gamma分布\n-\t将上式变换\n$$\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1\n$$\n\t因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：\n$$\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}\n$$\n-\t指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。\n-\tGamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。\n$$\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}\n$$\n-\t令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。\n\n## Beta分布\n-\t背景：\n\t-\t现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？\n\t-\t为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$\n\t-\t将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为\n$$\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n    &=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}\n$$\n\t-\t若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数\n$$\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]\n$$\n\t用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:\n$$\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n    &=Beta(p|k,n-k+1) \\\\\n    &=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}\n$$\n\t这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。\n-\tBeta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。\n-\t因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。\n\n## Beta-Binomial共轭\n-\t这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。\n-\t假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$\n-\t补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$\n\n| 先验      |  数据知识 | 后验  |\n| :-------: | :-------:| :--: |\n| Beta分布  | 二项分布 |   Beta分布   |\n-\t因此我们可以得到Beta-Binomial共轭\n$$\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)\n$$\n\t即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。\n-\t可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$\n-\t通过这个共轭，我们可以推出关于二项分布的一个重要公式：\n$$\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)\n$$\n\t现在可以证明如下：\n\t-\t式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分\n\t-\t取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$\n\t-\t此时可以得到$P(C \\leq k)=P(X_{k+1}>p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个\n\t-\t这时利用我们对第k大数的概率密度计算出为Beta分布，带入有\n$$\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n\t\t\t&=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n\t\t\t&=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}\n$$\n\t即证\n-\t通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。\n-\t在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。\n\n## Dirichlet-Multinomial共轭\n-\t假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？\n-\t同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}\n$$\n-\t整理一下可以写成\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}\n$$\n\t这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。\n-\t同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布\n$$\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)\n$$\n\t上式中的参数均是向量，对应多维情况。\n-\t无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。\n\n## 总结\n-\t总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。\n-\tLDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。\n-\t为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180828/B5IBgB1kbK.PNG)\n\n-\t$\\alpha$控制了多项式分布参数的mean shape和sparsity。\n-\t最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。\n-\t中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。\n-\t最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）\n-\t因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。\n-\t当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。\n-\t再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：\n{% blockquote Dirichlet_distribution https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters Intuitive interpretations of the parameters %}\nThe concentration parameter\nDirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how \"concentrated\" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.\n{% endblockquote %}\n-\t当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。\n-\t在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。\n-\t对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。\n\n# 马卡洛夫蒙特卡洛和吉步斯采样\n## 随机模拟\n-\t即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本。\n-\t马卡洛夫链是一种蒙特卡洛方法依赖的原理\n-\tMCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。\n\n## 马氏链\n-\t马氏链即状态转移的概率只依赖于前一个状态\n-\t因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果\n-\t矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布\n-\t关于马氏链收敛的定义\n\t-\t如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$\n\t-\t$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$\n\t-\t$\\pi$是方程$\\pi P = \\pi $的唯一非负解\n\t-\t$\\pi$称为马氏链的平稳分布\n\n## Markov Chain Monte Carlo\n-\t回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。\n-\t因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。\n-\t设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：\n$$\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)\n$$\n\t为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。\n-\t如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。\n-\t因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。\n-\t这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。\n\n## Gibbs Sampling\n-\t之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。\n-\t对二维概率分布$p(x,y)$，易得到\n$$\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}\n$$\n-\t从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：\n\t-\t若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$\n\t-\t若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$\n\t-\t若两点连线既不垂直也水平，则$Q=0$\n\t这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二位平面上的马氏链将收敛到$p(x,y)$。\n-\t因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，等到马氏链收敛之后形成的状态序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。\n-\t同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。\n\n## 总结\n-\t首先明确，MCMC方法是产生已知复杂分布的样本，但是实际应用时用的是转移分布，因此当复杂分布未知，但转移分布已知时，我们依然可以采样，而且可以根据采样推断这个复杂分布的参数。\n-\t应用于LDA中，我们知道复杂分布的形式$p(z_i=k|\\mathop{w}^{\\rightarrow})$，但不知道参数，而转移分布$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$实际上是可以计算出来的，因此我们可以用gibbs采样做LDA模型的推断。\n-\tgibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。\n-\t更新被采样单词的主题，不仅更新了gibbs采样的转移矩阵，同时也更新整个模型收敛。事实上这种更新非常巧妙：我只是将每个单词属于哪个主题改变了，也就是矩阵中某些项加1，某些项减1，但是这样一来最后得到的矩阵就是收敛之后的主题到词分布，也就是我们想要的复杂分布参数；同时单词，在数据里就是按文章组织起来的，自然文章到主题的分布也在更新，最后我们也可以得到这个分布用于模型推断未知文章；下文会讲到gibbs采样的公式，也仅仅只用主题-单词矩阵统计一下就计算出来了，因此gibbs采样本身也可以通过这种方法迭代更新！\n-\t既然使用了gibbs采样，那么只要保证能正确更新迭代，则初始状态不会影响到模型收敛，因此在采样前每个单词的主题可以随机初始化。这也说明了在LDA中gibbs采样不是鸡生蛋，蛋生鸡的矛盾，我们并不是从正确的数据中用初始的错误的转移矩阵采样，再用采样出来的错误样本点更新模型，虽然一开始的转移矩阵是错的，但是只要满足gibbs采样的公式，则转移矩阵会不断更新迭代直到正确，至于采样出来的样本点，一开始是错误的，但随着转移矩阵更新，最后也会正确采样。转移矩阵最终收敛不会受错误样本点影响，而是由其计算公式决定。\n-\t如何推断？当我们采样到词对应的主题之后，就将主题更新，这样转移分布就会改变，然后重新采样直到收敛。主题不断更新的过程中隐变量也在不断逼近正确值，当我们用困惑度或者其他方式衡量模型已经收敛到合适的程度时，拿到的隐变量就是推断的结果（实际上主要需要的就是两个多项式分布，这些本身在代码中是以矩阵预设并保存的，可以直接读取）。\n\n# 文本建模\n-\t接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：\n\t-\t模型是怎样的？\n\t-\t各个词的生成概率或者说模型参数是多少？\n\n## Unigram模型\n-\t模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率，贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。\n-\t也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：\n$$\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}\n$$\n-\t按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：\n$$\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}\n$$\n-\t既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：\n$$\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}\n$$\n\t其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：\n$$\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}\n$$\n-\t得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：\n$$\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}\n$$\n\t这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。\n-\t现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：\n$$\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}\n$$\n\t将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。\n$$\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}\n$$\n\t其中$\\Delta$是归一化因子：\n$$\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}\n$$\n\n## PLSA模型\n-\tPLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。\n-\t事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。\n-\tPLSA模型可以用EM算法迭代学习到参数。\n\n## 总结\n-\t现在整理一下，Unigram模型中主要包含两部分\n\t-\t词生成概率分布\n\t-\t词生成概率分布的分布\n-\tPLSA模型主要包含两部分\n\t-\t词生成概率分布\n\t-\t主题生成概率分布\n-\tUnigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了\n-\tPLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。\n\n# LDA文本建模\n## 模型概述\n-\tLDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设\n\t-\t词生成概率分布（暂记A）\n\t-\t词生成概率分布的分布（暂记B）\n\t-\t主题生成概率分布（暂记C）\n\t-\t主题生成概率分布的分布（暂记D）\n-\t这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：\n\t-\t先在B分布条件下抽样得到K个A分布\n\t-\t对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：\n\t\t-\t从C分布中抽样得到一个主题z\n\t\t-\t选择K个A分布中第z个，从这个A分布中抽样得到一个单词\n-\t假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：\n\n-\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180725/lb2HJm3i59.PNG)\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim)\n\n## 建立分布\n-\t现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：\n$$\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}\n$$\n-\t最终得到词与主题的联合分布:\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n\n\n## 采样\n-\t首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。\n-\t在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。\n\t-\t其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。\n\t-\t采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。\n-\t公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导\n-\t基于共轭关系推导如下：\n-\t采样的对象是词所对应的主题，概率为：\n$$\np(z_i=k|\\mathop{w}^{\\rightarrow})\n$$\n-\t使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})\n$$\n-\t由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：\n$$\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})\n$$\n-\t把这个公式按主题分布和词分布展开：\n$$\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t由于所有的共轭都是独立的，上式可以写成：\n$$\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：\n$$\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：\n$$\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\\n$$\n-\t而根据共轭关系，有\n$$\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\\n$$\n-\t因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，因此最后的概率计算出来就是（注意是正比于）：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t我们再看看基于联合分布如何推导\n-\t之前我们已经得到词和主题的联合分布：\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n-\t根据贝叶斯公式有\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\\n$$\n-\t因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：\n$$\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}\n$$\n-\t将$\\Delta$的表达式带入计算，也可以得到：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180829/31jGjg0idC.png?imageslim)\n\n-\t可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：\n$$\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}\n$$\n## 训练与推断\n-\t接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：\n\t-\t迭代什么？词对应的主题\n\t-\t根据什么迭代？通过Gibbs采样得到词的主题\n\t-\t迭代之后的效果？根据Dirichlet-Multinomial共轭更新Gibbs采样的概率\n\t-\t迭代到什么时候为止？Gibbs采样收敛，即采样前后的主题相同(很难达到），或者根据困惑度等指标来衡量模型收敛的程度。\n-\t训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。\n-\tLDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。\n\n# LDA in Gensim\n-\tGensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：\n{% blockquote Gensim https://radimrehurek.com/gensim/models/ldamodel.html models.ldamodel – Latent Dirichlet Allocation %}\nalpha ({numpy.ndarray, str}, optional) –\nCan be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n’default’: Learns an asymmetric prior from the corpus.\n{% endblockquote %}\n-\t具体分析待补充","slug":"lda","published":1,"updated":"2018-09-03T02:31:22.157Z","comments":1,"layout":"post","link":"","_id":"cjmd072d8001zqcw67w7br38j","content":"<hr><p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。</p><a id=\"more\"></a><h1 id=\"数学基础\"><a href=\"#数学基础\" class=\"headerlink\" title=\"数学基础\"></a>数学基础</h1><h2 id=\"Gamma函数\"><a href=\"#Gamma函数\" class=\"headerlink\" title=\"Gamma函数\"></a>Gamma函数</h2><ul><li>定义<br>$$<br>\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt<br>$$</li><li>因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数</li><li>Bohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足<ul><li>$f(1)=1$</li><li>$f(x+1)=xf(x)$</li><li>$log f(x)$是凸函数<br>那么$f(x)=\\Gamma(x)$</li></ul></li><li>Digamma函数<br>$$<br>\\psi (x)=\\frac{d log \\Gamma(x)}{dx}<br>$$<br>其具有以下性质<br>$$<br>\\psi (x+1)=\\psi (x)+\\frac 1x<br>$$</li><li>在用变分推断对LDA进行推断时结果就是digamma函数的形式</li></ul><h2 id=\"Gamma分布\"><a href=\"#Gamma分布\" class=\"headerlink\" title=\"Gamma分布\"></a>Gamma分布</h2><ul><li>将上式变换<br>$$<br>\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1<br>$$<br>因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：<br>$$<br>Gamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}<br>$$</li><li>指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。</li><li>Gamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。<br>$$<br>Poisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}<br>$$</li><li>令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。</li></ul><h2 id=\"Beta分布\"><a href=\"#Beta分布\" class=\"headerlink\" title=\"Beta分布\"></a>Beta分布</h2><ul><li>背景：<ul><li>现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？</li><li>为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$</li><li>将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为<br>$$<br>\\begin{aligned}<br>P(E)&amp;=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\<br>&amp;=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\<br>\\end{aligned}<br>$$</li><li>若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数<br>$$<br>f(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]<br>$$<br>用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:<br>$$<br>\\begin{aligned}<br>f(x)&amp;=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\<br>&amp;=Beta(p|k,n-k+1) \\\\<br>&amp;=Beta(\\alpha,\\beta) \\\\<br>\\end{aligned}<br>$$<br>这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。</li></ul></li><li>Beta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。</li><li>因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。</li></ul><h2 id=\"Beta-Binomial共轭\"><a href=\"#Beta-Binomial共轭\" class=\"headerlink\" title=\"Beta-Binomial共轭\"></a>Beta-Binomial共轭</h2><ul><li>这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。</li><li>假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$</li><li>补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$</li></ul><table><thead><tr><th style=\"text-align:center\">先验</th><th style=\"text-align:center\">数据知识</th><th style=\"text-align:center\">后验</th></tr></thead><tbody><tr><td style=\"text-align:center\">Beta分布</td><td style=\"text-align:center\">二项分布</td><td style=\"text-align:center\">Beta分布</td></tr></tbody></table><ul><li>因此我们可以得到Beta-Binomial共轭<br>$$<br>Beta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)<br>$$<br>即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。</li><li>可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$</li><li>通过这个共轭，我们可以推出关于二项分布的一个重要公式：<br>$$<br>P(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)<br>$$<br>现在可以证明如下：<ul><li>式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分</li><li>取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$</li><li>此时可以得到$P(C \\leq k)=P(X_{k+1}&gt;p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个</li><li>这时利用我们对第k大数的概率密度计算出为Beta分布，带入有<br>$$<br>\\begin{aligned}<br>P(C \\leq k) &amp;=P(X_{k+1} &gt; p) \\\\<br>&amp;=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\<br>&amp;=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\<br>\\end{aligned}<br>$$<br>即证</li></ul></li><li>通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。</li><li>在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。</li></ul><h2 id=\"Dirichlet-Multinomial共轭\"><a href=\"#Dirichlet-Multinomial共轭\" class=\"headerlink\" title=\"Dirichlet-Multinomial共轭\"></a>Dirichlet-Multinomial共轭</h2><ul><li>假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？</li><li>同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到<br>$$<br>f(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>$$</li><li>整理一下可以写成<br>$$<br>f(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}<br>$$<br>这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。</li><li>同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布<br>$$<br>Dir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)<br>$$<br>上式中的参数均是向量，对应多维情况。</li><li>无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。</li><li>LDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。</li><li>为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180828/B5IBgB1kbK.PNG\" alt=\"mark\"></p><ul><li>$\\alpha$控制了多项式分布参数的mean shape和sparsity。</li><li>最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。</li><li>中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。</li><li>最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）</li><li>因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。</li><li>当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。</li><li>再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：<blockquote><p>The concentration parameter<br>Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how “concentrated” the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.</p><footer><strong>Dirichlet_distribution</strong><cite><a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters\" target=\"_blank\" rel=\"noopener\">Intuitive interpretations of the parameters</a></cite></footer></blockquote></li><li>当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。</li><li>在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。</li><li>对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。</li></ul><h1 id=\"马卡洛夫蒙特卡洛和吉步斯采样\"><a href=\"#马卡洛夫蒙特卡洛和吉步斯采样\" class=\"headerlink\" title=\"马卡洛夫蒙特卡洛和吉步斯采样\"></a>马卡洛夫蒙特卡洛和吉步斯采样</h1><h2 id=\"随机模拟\"><a href=\"#随机模拟\" class=\"headerlink\" title=\"随机模拟\"></a>随机模拟</h2><ul><li>即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本。</li><li>马卡洛夫链是一种蒙特卡洛方法依赖的原理</li><li>MCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。</li></ul><h2 id=\"马氏链\"><a href=\"#马氏链\" class=\"headerlink\" title=\"马氏链\"></a>马氏链</h2><ul><li>马氏链即状态转移的概率只依赖于前一个状态</li><li>因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果</li><li>矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布</li><li>关于马氏链收敛的定义<ul><li>如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$</li><li>$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$</li><li>$\\pi$是方程$\\pi P = \\pi $的唯一非负解</li><li>$\\pi$称为马氏链的平稳分布</li></ul></li></ul><h2 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h2><ul><li>回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。</li><li>因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。</li><li>设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：<br>$$<br>p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)<br>$$<br>为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。</li><li>如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。</li><li>因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。</li><li>这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。</li></ul><h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul><li>之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。</li><li>对二维概率分布$p(x,y)$，易得到<br>$$<br>\\begin{aligned}<br>p(x_1,y_1)p(y_2|x_1) &amp; =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\<br>&amp; =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\<br>&amp; =p(x_1,y_2)p(y_1|x_1) \\\\<br>\\end{aligned}<br>$$</li><li>从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：<ul><li>若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$</li><li>若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$</li><li>若两点连线既不垂直也水平，则$Q=0$<br>这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二位平面上的马氏链将收敛到$p(x,y)$。</li></ul></li><li>因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，等到马氏链收敛之后形成的状态序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。</li><li>同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。</li></ul><h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>首先明确，MCMC方法是产生已知复杂分布的样本，但是实际应用时用的是转移分布，因此当复杂分布未知，但转移分布已知时，我们依然可以采样，而且可以根据采样推断这个复杂分布的参数。</li><li>应用于LDA中，我们知道复杂分布的形式$p(z_i=k|\\mathop{w}^{\\rightarrow})$，但不知道参数，而转移分布$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$实际上是可以计算出来的，因此我们可以用gibbs采样做LDA模型的推断。</li><li>gibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。</li><li>更新被采样单词的主题，不仅更新了gibbs采样的转移矩阵，同时也更新整个模型收敛。事实上这种更新非常巧妙：我只是将每个单词属于哪个主题改变了，也就是矩阵中某些项加1，某些项减1，但是这样一来最后得到的矩阵就是收敛之后的主题到词分布，也就是我们想要的复杂分布参数；同时单词，在数据里就是按文章组织起来的，自然文章到主题的分布也在更新，最后我们也可以得到这个分布用于模型推断未知文章；下文会讲到gibbs采样的公式，也仅仅只用主题-单词矩阵统计一下就计算出来了，因此gibbs采样本身也可以通过这种方法迭代更新！</li><li>既然使用了gibbs采样，那么只要保证能正确更新迭代，则初始状态不会影响到模型收敛，因此在采样前每个单词的主题可以随机初始化。这也说明了在LDA中gibbs采样不是鸡生蛋，蛋生鸡的矛盾，我们并不是从正确的数据中用初始的错误的转移矩阵采样，再用采样出来的错误样本点更新模型，虽然一开始的转移矩阵是错的，但是只要满足gibbs采样的公式，则转移矩阵会不断更新迭代直到正确，至于采样出来的样本点，一开始是错误的，但随着转移矩阵更新，最后也会正确采样。转移矩阵最终收敛不会受错误样本点影响，而是由其计算公式决定。</li><li>如何推断？当我们采样到词对应的主题之后，就将主题更新，这样转移分布就会改变，然后重新采样直到收敛。主题不断更新的过程中隐变量也在不断逼近正确值，当我们用困惑度或者其他方式衡量模型已经收敛到合适的程度时，拿到的隐变量就是推断的结果（实际上主要需要的就是两个多项式分布，这些本身在代码中是以矩阵预设并保存的，可以直接读取）。</li></ul><h1 id=\"文本建模\"><a href=\"#文本建模\" class=\"headerlink\" title=\"文本建模\"></a>文本建模</h1><ul><li>接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：<ul><li>模型是怎样的？</li><li>各个词的生成概率或者说模型参数是多少？</li></ul></li></ul><h2 id=\"Unigram模型\"><a href=\"#Unigram模型\" class=\"headerlink\" title=\"Unigram模型\"></a>Unigram模型</h2><ul><li>模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率，贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。</li><li>也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：<br>$$<br>p(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}<br>$$</li><li>按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：<br>$$<br>\\begin{aligned}<br>p(\\mathop{n}^{\\rightarrow}) &amp;= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\<br>&amp;= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\<br>\\end{aligned}<br>$$</li><li>既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：<br>$$<br>Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}<br>$$<br>其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：<br>$$<br>\\begin{aligned}<br>p(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &amp;= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\<br>&amp;= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\<br>\\end{aligned}<br>$$</li><li>得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：<br>$$<br>\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}<br>$$<br>这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。</li><li>现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：<br>$$<br>p(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}<br>$$<br>将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。<br>$$<br>p(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}<br>$$<br>其中$\\Delta$是归一化因子：<br>$$<br>\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}<br>$$</li></ul><h2 id=\"PLSA模型\"><a href=\"#PLSA模型\" class=\"headerlink\" title=\"PLSA模型\"></a>PLSA模型</h2><ul><li>PLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。</li><li>事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。</li><li>PLSA模型可以用EM算法迭代学习到参数。</li></ul><h2 id=\"总结-2\"><a href=\"#总结-2\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>现在整理一下，Unigram模型中主要包含两部分<ul><li>词生成概率分布</li><li>词生成概率分布的分布</li></ul></li><li>PLSA模型主要包含两部分<ul><li>词生成概率分布</li><li>主题生成概率分布</li></ul></li><li>Unigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了</li><li>PLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。</li></ul><h1 id=\"LDA文本建模\"><a href=\"#LDA文本建模\" class=\"headerlink\" title=\"LDA文本建模\"></a>LDA文本建模</h1><h2 id=\"模型概述\"><a href=\"#模型概述\" class=\"headerlink\" title=\"模型概述\"></a>模型概述</h2><ul><li>LDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设<ul><li>词生成概率分布（暂记A）</li><li>词生成概率分布的分布（暂记B）</li><li>主题生成概率分布（暂记C）</li><li>主题生成概率分布的分布（暂记D）</li></ul></li><li>这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：<ul><li>先在B分布条件下抽样得到K个A分布</li><li>对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：<ul><li>从C分布中抽样得到一个主题z</li><li>选择K个A分布中第z个，从这个A分布中抽样得到一个单词</li></ul></li></ul></li><li><p>假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：</p></li><li><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180725/lb2HJm3i59.PNG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim\" alt=\"mark\"></p></li></ul><h2 id=\"建立分布\"><a href=\"#建立分布\" class=\"headerlink\" title=\"建立分布\"></a>建立分布</h2><ul><li>现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：<br>$$<br>\\begin{aligned}<br>p(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\<br>p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\<br>\\end{aligned}<br>$$</li><li>最终得到词与主题的联合分布:<br>$$<br>p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}<br>$$</li></ul><h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul><li>首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。</li><li>在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。<ul><li>其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。</li><li>采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。</li></ul></li><li>公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导</li><li>基于共轭关系推导如下：</li><li>采样的对象是词所对应的主题，概率为：<br>$$<br>p(z_i=k|\\mathop{w}^{\\rightarrow})<br>$$</li><li>使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})<br>$$</li><li>由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：<br>$$<br>p(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})<br>$$</li><li>把这个公式按主题分布和词分布展开：<br>$$<br>\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>由于所有的共轭都是独立的，上式可以写成：<br>$$<br>\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：<br>$$<br>\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：<br>$$<br>p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\<br>p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\<br>$$</li><li>而根据共轭关系，有<br>$$<br>p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\<br>p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\<br>$$</li><li>因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，因此最后的概率计算出来就是（注意是正比于）：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>我们再看看基于联合分布如何推导</li><li>之前我们已经得到词和主题的联合分布：<br>$$<br>p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}<br>$$</li><li>根据贝叶斯公式有<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\<br>=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\<br>$$</li><li>因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：<br>$$<br>∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}<br>$$</li><li>将$\\Delta$的表达式带入计算，也可以得到：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180829/31jGjg0idC.png?imageslim\" alt=\"mark\"></p><ul><li>可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：<br>$$<br>\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\<br>\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}<br>$$<h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2></li><li>接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：<ul><li>迭代什么？词对应的主题</li><li>根据什么迭代？通过Gibbs采样得到词的主题</li><li>迭代之后的效果？根据Dirichlet-Multinomial共轭更新Gibbs采样的概率</li><li>迭代到什么时候为止？Gibbs采样收敛，即采样前后的主题相同(很难达到），或者根据困惑度等指标来衡量模型收敛的程度。</li></ul></li><li>训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。</li><li>LDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。</li></ul><h1 id=\"LDA-in-Gensim\"><a href=\"#LDA-in-Gensim\" class=\"headerlink\" title=\"LDA in Gensim\"></a>LDA in Gensim</h1><ul><li>Gensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：<blockquote><p>alpha ({numpy.ndarray, str}, optional) –<br>Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:<br>’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.<br>’default’: Learns an asymmetric prior from the corpus.</p><footer><strong>Gensim</strong><cite><a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\" target=\"_blank\" rel=\"noopener\">models.ldamodel – Latent Dirichlet Allocation</a></cite></footer></blockquote></li><li>具体分析待补充</li></ul>","site":{"data":{}},"excerpt":"<hr><p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。</p>","more":"<h1 id=\"数学基础\"><a href=\"#数学基础\" class=\"headerlink\" title=\"数学基础\"></a>数学基础</h1><h2 id=\"Gamma函数\"><a href=\"#Gamma函数\" class=\"headerlink\" title=\"Gamma函数\"></a>Gamma函数</h2><ul><li>定义<br>$$<br>\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt<br>$$</li><li>因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数</li><li>Bohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足<ul><li>$f(1)=1$</li><li>$f(x+1)=xf(x)$</li><li>$log f(x)$是凸函数<br>那么$f(x)=\\Gamma(x)$</li></ul></li><li>Digamma函数<br>$$<br>\\psi (x)=\\frac{d log \\Gamma(x)}{dx}<br>$$<br>其具有以下性质<br>$$<br>\\psi (x+1)=\\psi (x)+\\frac 1x<br>$$</li><li>在用变分推断对LDA进行推断时结果就是digamma函数的形式</li></ul><h2 id=\"Gamma分布\"><a href=\"#Gamma分布\" class=\"headerlink\" title=\"Gamma分布\"></a>Gamma分布</h2><ul><li>将上式变换<br>$$<br>\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1<br>$$<br>因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：<br>$$<br>Gamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}<br>$$</li><li>指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。</li><li>Gamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。<br>$$<br>Poisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}<br>$$</li><li>令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。</li></ul><h2 id=\"Beta分布\"><a href=\"#Beta分布\" class=\"headerlink\" title=\"Beta分布\"></a>Beta分布</h2><ul><li>背景：<ul><li>现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？</li><li>为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$</li><li>将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为<br>$$<br>\\begin{aligned}<br>P(E)&amp;=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\<br>&amp;=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\<br>\\end{aligned}<br>$$</li><li>若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数<br>$$<br>f(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]<br>$$<br>用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:<br>$$<br>\\begin{aligned}<br>f(x)&amp;=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\<br>&amp;=Beta(p|k,n-k+1) \\\\<br>&amp;=Beta(\\alpha,\\beta) \\\\<br>\\end{aligned}<br>$$<br>这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。</li></ul></li><li>Beta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。</li><li>因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。</li></ul><h2 id=\"Beta-Binomial共轭\"><a href=\"#Beta-Binomial共轭\" class=\"headerlink\" title=\"Beta-Binomial共轭\"></a>Beta-Binomial共轭</h2><ul><li>这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。</li><li>假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$</li><li>补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$</li></ul><table><thead><tr><th style=\"text-align:center\">先验</th><th style=\"text-align:center\">数据知识</th><th style=\"text-align:center\">后验</th></tr></thead><tbody><tr><td style=\"text-align:center\">Beta分布</td><td style=\"text-align:center\">二项分布</td><td style=\"text-align:center\">Beta分布</td></tr></tbody></table><ul><li>因此我们可以得到Beta-Binomial共轭<br>$$<br>Beta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)<br>$$<br>即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。</li><li>可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$</li><li>通过这个共轭，我们可以推出关于二项分布的一个重要公式：<br>$$<br>P(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)<br>$$<br>现在可以证明如下：<ul><li>式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分</li><li>取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$</li><li>此时可以得到$P(C \\leq k)=P(X_{k+1}&gt;p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个</li><li>这时利用我们对第k大数的概率密度计算出为Beta分布，带入有<br>$$<br>\\begin{aligned}<br>P(C \\leq k) &amp;=P(X_{k+1} &gt; p) \\\\<br>&amp;=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\<br>&amp;=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\<br>\\end{aligned}<br>$$<br>即证</li></ul></li><li>通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。</li><li>在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。</li></ul><h2 id=\"Dirichlet-Multinomial共轭\"><a href=\"#Dirichlet-Multinomial共轭\" class=\"headerlink\" title=\"Dirichlet-Multinomial共轭\"></a>Dirichlet-Multinomial共轭</h2><ul><li>假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？</li><li>同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到<br>$$<br>f(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>$$</li><li>整理一下可以写成<br>$$<br>f(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}<br>$$<br>这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。</li><li>同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布<br>$$<br>Dir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)<br>$$<br>上式中的参数均是向量，对应多维情况。</li><li>无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。</li></ul><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。</li><li>LDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。</li><li>为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180828/B5IBgB1kbK.PNG\" alt=\"mark\"></p><ul><li>$\\alpha$控制了多项式分布参数的mean shape和sparsity。</li><li>最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。</li><li>中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。</li><li>最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）</li><li>因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。</li><li>当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。</li><li>再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：<blockquote><p>The concentration parameter<br>Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how “concentrated” the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.</p><footer><strong>Dirichlet_distribution</strong><cite><a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters\" target=\"_blank\" rel=\"noopener\">Intuitive interpretations of the parameters</a></cite></footer></blockquote></li><li>当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。</li><li>在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。</li><li>对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。</li></ul><h1 id=\"马卡洛夫蒙特卡洛和吉步斯采样\"><a href=\"#马卡洛夫蒙特卡洛和吉步斯采样\" class=\"headerlink\" title=\"马卡洛夫蒙特卡洛和吉步斯采样\"></a>马卡洛夫蒙特卡洛和吉步斯采样</h1><h2 id=\"随机模拟\"><a href=\"#随机模拟\" class=\"headerlink\" title=\"随机模拟\"></a>随机模拟</h2><ul><li>即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本。</li><li>马卡洛夫链是一种蒙特卡洛方法依赖的原理</li><li>MCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。</li></ul><h2 id=\"马氏链\"><a href=\"#马氏链\" class=\"headerlink\" title=\"马氏链\"></a>马氏链</h2><ul><li>马氏链即状态转移的概率只依赖于前一个状态</li><li>因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果</li><li>矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布</li><li>关于马氏链收敛的定义<ul><li>如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$</li><li>$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$</li><li>$\\pi$是方程$\\pi P = \\pi $的唯一非负解</li><li>$\\pi$称为马氏链的平稳分布</li></ul></li></ul><h2 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h2><ul><li>回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。</li><li>因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。</li><li>设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：<br>$$<br>p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)<br>$$<br>为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。</li><li>如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。</li><li>因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。</li><li>这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。</li></ul><h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul><li>之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。</li><li>对二维概率分布$p(x,y)$，易得到<br>$$<br>\\begin{aligned}<br>p(x_1,y_1)p(y_2|x_1) &amp; =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\<br>&amp; =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\<br>&amp; =p(x_1,y_2)p(y_1|x_1) \\\\<br>\\end{aligned}<br>$$</li><li>从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：<ul><li>若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$</li><li>若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$</li><li>若两点连线既不垂直也水平，则$Q=0$<br>这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二位平面上的马氏链将收敛到$p(x,y)$。</li></ul></li><li>因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，等到马氏链收敛之后形成的状态序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。</li><li>同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。</li></ul><h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>首先明确，MCMC方法是产生已知复杂分布的样本，但是实际应用时用的是转移分布，因此当复杂分布未知，但转移分布已知时，我们依然可以采样，而且可以根据采样推断这个复杂分布的参数。</li><li>应用于LDA中，我们知道复杂分布的形式$p(z_i=k|\\mathop{w}^{\\rightarrow})$，但不知道参数，而转移分布$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$实际上是可以计算出来的，因此我们可以用gibbs采样做LDA模型的推断。</li><li>gibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。</li><li>更新被采样单词的主题，不仅更新了gibbs采样的转移矩阵，同时也更新整个模型收敛。事实上这种更新非常巧妙：我只是将每个单词属于哪个主题改变了，也就是矩阵中某些项加1，某些项减1，但是这样一来最后得到的矩阵就是收敛之后的主题到词分布，也就是我们想要的复杂分布参数；同时单词，在数据里就是按文章组织起来的，自然文章到主题的分布也在更新，最后我们也可以得到这个分布用于模型推断未知文章；下文会讲到gibbs采样的公式，也仅仅只用主题-单词矩阵统计一下就计算出来了，因此gibbs采样本身也可以通过这种方法迭代更新！</li><li>既然使用了gibbs采样，那么只要保证能正确更新迭代，则初始状态不会影响到模型收敛，因此在采样前每个单词的主题可以随机初始化。这也说明了在LDA中gibbs采样不是鸡生蛋，蛋生鸡的矛盾，我们并不是从正确的数据中用初始的错误的转移矩阵采样，再用采样出来的错误样本点更新模型，虽然一开始的转移矩阵是错的，但是只要满足gibbs采样的公式，则转移矩阵会不断更新迭代直到正确，至于采样出来的样本点，一开始是错误的，但随着转移矩阵更新，最后也会正确采样。转移矩阵最终收敛不会受错误样本点影响，而是由其计算公式决定。</li><li>如何推断？当我们采样到词对应的主题之后，就将主题更新，这样转移分布就会改变，然后重新采样直到收敛。主题不断更新的过程中隐变量也在不断逼近正确值，当我们用困惑度或者其他方式衡量模型已经收敛到合适的程度时，拿到的隐变量就是推断的结果（实际上主要需要的就是两个多项式分布，这些本身在代码中是以矩阵预设并保存的，可以直接读取）。</li></ul><h1 id=\"文本建模\"><a href=\"#文本建模\" class=\"headerlink\" title=\"文本建模\"></a>文本建模</h1><ul><li>接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：<ul><li>模型是怎样的？</li><li>各个词的生成概率或者说模型参数是多少？</li></ul></li></ul><h2 id=\"Unigram模型\"><a href=\"#Unigram模型\" class=\"headerlink\" title=\"Unigram模型\"></a>Unigram模型</h2><ul><li>模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率，贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。</li><li>也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：<br>$$<br>p(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}<br>$$</li><li>按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：<br>$$<br>\\begin{aligned}<br>p(\\mathop{n}^{\\rightarrow}) &amp;= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\<br>&amp;= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\<br>\\end{aligned}<br>$$</li><li>既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：<br>$$<br>Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}<br>$$<br>其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：<br>$$<br>\\begin{aligned}<br>p(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &amp;= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\<br>&amp;= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\<br>\\end{aligned}<br>$$</li><li>得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：<br>$$<br>\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}<br>$$<br>这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。</li><li>现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：<br>$$<br>p(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}<br>$$<br>将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。<br>$$<br>p(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}<br>$$<br>其中$\\Delta$是归一化因子：<br>$$<br>\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}<br>$$</li></ul><h2 id=\"PLSA模型\"><a href=\"#PLSA模型\" class=\"headerlink\" title=\"PLSA模型\"></a>PLSA模型</h2><ul><li>PLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。</li><li>事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。</li><li>PLSA模型可以用EM算法迭代学习到参数。</li></ul><h2 id=\"总结-2\"><a href=\"#总结-2\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul><li>现在整理一下，Unigram模型中主要包含两部分<ul><li>词生成概率分布</li><li>词生成概率分布的分布</li></ul></li><li>PLSA模型主要包含两部分<ul><li>词生成概率分布</li><li>主题生成概率分布</li></ul></li><li>Unigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了</li><li>PLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。</li></ul><h1 id=\"LDA文本建模\"><a href=\"#LDA文本建模\" class=\"headerlink\" title=\"LDA文本建模\"></a>LDA文本建模</h1><h2 id=\"模型概述\"><a href=\"#模型概述\" class=\"headerlink\" title=\"模型概述\"></a>模型概述</h2><ul><li>LDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设<ul><li>词生成概率分布（暂记A）</li><li>词生成概率分布的分布（暂记B）</li><li>主题生成概率分布（暂记C）</li><li>主题生成概率分布的分布（暂记D）</li></ul></li><li>这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：<ul><li>先在B分布条件下抽样得到K个A分布</li><li>对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：<ul><li>从C分布中抽样得到一个主题z</li><li>选择K个A分布中第z个，从这个A分布中抽样得到一个单词</li></ul></li></ul></li><li><p>假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：</p></li><li><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180725/lb2HJm3i59.PNG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180723/6740GFbJ1a.png?imageslim\" alt=\"mark\"></p></li></ul><h2 id=\"建立分布\"><a href=\"#建立分布\" class=\"headerlink\" title=\"建立分布\"></a>建立分布</h2><ul><li>现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：<br>$$<br>\\begin{aligned}<br>p(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\<br>p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\<br>\\end{aligned}<br>$$</li><li>最终得到词与主题的联合分布:<br>$$<br>p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}<br>$$</li></ul><h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul><li>首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。</li><li>在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。<ul><li>其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。</li><li>采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。</li></ul></li><li>公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导</li><li>基于共轭关系推导如下：</li><li>采样的对象是词所对应的主题，概率为：<br>$$<br>p(z_i=k|\\mathop{w}^{\\rightarrow})<br>$$</li><li>使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})<br>$$</li><li>由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：<br>$$<br>p(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})<br>$$</li><li>把这个公式按主题分布和词分布展开：<br>$$<br>\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>由于所有的共轭都是独立的，上式可以写成：<br>$$<br>\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：<br>$$<br>\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}<br>$$</li><li>已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：<br>$$<br>p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\<br>p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\<br>$$</li><li>而根据共轭关系，有<br>$$<br>p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\<br>p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\<br>$$</li><li>因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，因此最后的概率计算出来就是（注意是正比于）：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>我们再看看基于联合分布如何推导</li><li>之前我们已经得到词和主题的联合分布：<br>$$<br>p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}<br>$$</li><li>根据贝叶斯公式有<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\<br>=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\<br>$$</li><li>因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：<br>$$<br>∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}<br>$$</li><li>将$\\Delta$的表达式带入计算，也可以得到：<br>$$<br>p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}<br>$$</li><li>最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180829/31jGjg0idC.png?imageslim\" alt=\"mark\"></p><ul><li>可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：<br>$$<br>\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\<br>\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}<br>$$<h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2></li><li>接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：<ul><li>迭代什么？词对应的主题</li><li>根据什么迭代？通过Gibbs采样得到词的主题</li><li>迭代之后的效果？根据Dirichlet-Multinomial共轭更新Gibbs采样的概率</li><li>迭代到什么时候为止？Gibbs采样收敛，即采样前后的主题相同(很难达到），或者根据困惑度等指标来衡量模型收敛的程度。</li></ul></li><li>训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。</li><li>LDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。</li></ul><h1 id=\"LDA-in-Gensim\"><a href=\"#LDA-in-Gensim\" class=\"headerlink\" title=\"LDA in Gensim\"></a>LDA in Gensim</h1><ul><li>Gensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：<blockquote><p>alpha ({numpy.ndarray, str}, optional) –<br>Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:<br>’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.<br>’default’: Learns an asymmetric prior from the corpus.</p><footer><strong>Gensim</strong><cite><a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\" target=\"_blank\" rel=\"noopener\">models.ldamodel – Latent Dirichlet Allocation</a></cite></footer></blockquote></li><li>具体分析待补充</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Sep 03 2018 10:31:22 GMT+0800 (中国标准时间)","title":"LDA学习笔记","path":"2018/07/23/lda/","eyeCatchImage":null,"excerpt":"<hr><p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。</p>","date":"2018-07-23T01:56:41.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","lda","mcmc"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"推断算法笔记","date":"2018-08-28T01:55:10.000Z","mathjax":true,"photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180828/BA94mibfCf.png?imageslim"],"_content":"\n记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。\n很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。\n徐老师的课程讲义地址：[roboticcam/machine-learning-notes](https://github.com/roboticcam/machine-learning-notes)，如果不额外说明，一些截图和代码均来自徐老师的讲义。\n其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。\n\n<!--more-->\n\n# Bayesian Inference\n-\t在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）\n-\t统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计\n-\t在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$\n-\t后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布\n-\t在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和\n-\t后验实际上是在最大似然估计和先验之间权衡\n-\t当数据非常多时，后验渐渐不再依赖于先验\n-\t很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布\n-\t有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计\n\n# Markov Chain Monte Carlo\n-\tMCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数\n-\t最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布\n-\t蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量\n-\t蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）\n\n## Sample\n-\t直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量\n-\t在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数\n-\t最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：\n$$\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\\n$$\n\n## rejection sampling\n-\t但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling\n-\t对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/ildAC764ke.JPG)\n-\t我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接\n-\t显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：\n```\ni=0\nwhile i!= N\n\tx(i)~q(x) and u~U(0,1)\n\tif u< p(x(i))/Mq(x(i)) then\n\t\taccept x(i)\n\t\ti=i+1\n\telse\n\t\treject x(i)\n\tend\nend\n```\n-\trejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。\n\n## adaptive rejection sampling\n-\t当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高\n-\t基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域\n-\t但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/2gcFgBDJLa.JPG)\n\n## Importance Sampling\n-\t上面提到的采样算法是从简单分布采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布\n-\timportance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。\n-\t例如我们希望通过采样得到某个分布的期望\n$$\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\\n$$\n-\tp(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。\n-\t这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。\n\n## MCMC&MH\n-\tmcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关\n-\t不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。\n-\t我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{'} | x)$，从样本$x$转移到样本$x^{'}$。\n-\t在马尔可夫链中，有如下Chapman-Kologronvo等式：\n$$\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx\n$$\n-\t这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：\n$$\n\\pi _t (x) = \\pi _{t-1} (x)\n$$\n-\t实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：\n$$\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})\n$$\n-\t由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。\n-\t当满足细致平稳条件时，马氏链是收敛的\n-\t在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/6C57aKcD1d.JPG)\n-\t尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。\n-\t而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。\n\n## Hybrid Metropolis-Hasting\n-\t待补充\n\n## Gibbs Sampling\n-\t吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布\n-\t吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/CLgH680c1a.JPG)\n-\t工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的：\n```\nu~p(u|x,y,z)\nx,y,z~p(x,y,z|u)\n=p(x|u)p(y|u)p(z|u)\n```\n-\t上面关于x,y,z的三个条件概率可以并行计算。\n-\t现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率\n$$\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})\n$$\n-\t在gibbs中\n$$\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\\n$$\n-\t而且实际上从$x_{¬i}$到$x_{¬i}^{'}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此\n$$\nx_{¬i}^{'}=x_{¬i}\n$$\n-\t接下来看看gibbs的接受率\n$$\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\\n$$\n\n\n# Expectation Maximization\n## 更新\n-\t看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。\n\n## 公式\n-\t对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：\n\n$$\n\\theta=\\mathop{argmax}_{\\theta} L(\\theta | X) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\\n$$\n\n-\t之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导\n-\t这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。\n-\t每一次迭代分为两个部分，E和M，也就求期望和最大化\n\t-\t求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数\n\t-\t最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果\n-\t合起来就得到EM算法的公式：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t)}) dz\n$$\n## 为何有效\n-\t也就是证明，每次迭代后最大似然会增加\n-\t要证明：\n$$\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})\n$$\n-\t先改写对数似然\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n$$\n-\t两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：\n$$\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\\n$$\n-\t其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量\n-\t迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？\n-\t我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$\n$$\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\\n$$\n-\t其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入\tH,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性\n\n## 从ELBO的角度理解\n-\t我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式\n-\t在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：\n$$\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\\n$$\n-\t也就是\n$$\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))\n$$\n-\t其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然\n-\t可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤\n-\t我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：\n$$\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))\n$$\n-\tq取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：\n$$\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\\n$$\n-\t其中第二项与$\\theta$无关，因此：\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\\n$$\n-\t代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz\n$$\n-\t同样得到了EM算法的迭代公式\n-\t下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180830/dCj203m7jB.PNG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180830/6k4kbJH3bc.PNG)\n-\t剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入\n\n## 从假设隐变量为可观察的角度理解\n-\t这种理解来自Chuong B Do & Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?\n-\tEM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。\n-\tEM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。\n-\t猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。\n-\t所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。\n\n## EM算法与K-means\n-\tK-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。\n\n## 广义EM算法\n\n## Wake-Sleep算法\n\n## 广义EM算法与吉布斯采样\n\n# Variational Inference\n## ELBO\n-\t接下来介绍变分推断，可以看到，EM算法可以推广到变分推断\n-\t重新推出ELBO与对数似然的关系：\n$$\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\\n$$\n-\t两边对隐分布$q(z)$求期望\n$$\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\\n$$\n-\t我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。\n-\t接下来就是讨论如何使得ELBO最大化\n\n## 任意分布上的变分推断\n-\t对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量\n-\t我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：\n$$\nq(z)=\\prod _{i=1}^M q_i(z_i)\n$$\n-\t因此ELBO可以写成两部分\n$$\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\\n$$\n-\t其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成\n$$\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\\n$$\n-\t在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：\n$$\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n$$\n-\t这样part1用伪分布的形式可以改写成\n$$\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\\n$$\n-\tpart2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：\n$$\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n$$\n-\t再把part1和part2合起来，得到ELBO关于分量j的形式：\n$$\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\\n$$\n-\t也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度\n-\t何时这个KL散度最小？也就是：\n$$\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\\n$$\n-\t到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。\n\n## 指数家族分布\n-\t定义指数家族分布：\n$$\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\\n$$\n-\t其中\n\t-\t$T(x)$:sufficient statistics\n\t-\t$\\theta$:parameter of the family\n\t-\t$\\eta$:natural parameter\n\t-\t$h(x)$:underlying measure\n\t-\t$A(\\theta)$:log normalizer / partition function\n-\t注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：\n$$\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))\n$$\n-\t当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：\n$$\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\\n$$\n-\t继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：\n$$\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n$$\n-\t举个例子，高斯分布写成指数家族分布形式：\n$$\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )\n$$\n-\t用自然参数去替代方差和均值，写成指数家族分布形式：\n$$\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))\n$$\n-\t其中：\n\t-\t$T(x)$:$[x \\ x^2]$\n\t-\t$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$\n\t-\t$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$\n-\t接下来我们利用指数家族的性质来快速计算均值和方差\n$$\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\\n$$\n-\t为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：\n$$\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\\n$$\n-\t下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：\n$$\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\\n$$\n-\t用向量组的方式改写：\n$$\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\\n$$\n- 原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：\n$$\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\\n$$\n-\t我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：\n$$\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\\n$$\n-\t这样先验和后验形式一致，也就是共轭\n-\t这样我们用统一的形式写下似然和先验\n$$\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\\n$$\n-\t这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：\n$$\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\\n$$\n-\t上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。\n\n## 指数族分布下的变分推断\n-\t接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁\n-\t我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$\n-\t我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大\n-\t首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：\n$$\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\\n$$\n-\t其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：\n$$\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\\n$$\n-\t现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：\n$$\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\\n$$\n-\t代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：\n$$\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda) \n$$\n-\t利用之前log normalizer关于参数求导的结论，有:\n$$\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)\n$$\n-\t对上式求导，令其为0，有：\n$$\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\\n$$\n-\t我们就得到了$\\lambda$的迭代式！同理可以得到：\n$$\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\\n$$\n-\t写完整应该是：\n$$\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\\n$$\n-\t观察这两个迭代式，变量更新的路径是:\n$$\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda\n$$","source":"_posts/inference-algorithm.md","raw":"---\ntitle: 推断算法笔记\ndate: 2018-08-28 09:55:10\ncategories: 机器学习\ntags:\n  - inference\n  - math\n  -\tmcmc\n  - variational inference\n  - em\nmathjax: true\nphotos: http://ojtdnrpmt.bkt.clouddn.com/blog/180828/BA94mibfCf.png?imageslim\n---\n\n记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。\n很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。\n徐老师的课程讲义地址：[roboticcam/machine-learning-notes](https://github.com/roboticcam/machine-learning-notes)，如果不额外说明，一些截图和代码均来自徐老师的讲义。\n其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。\n\n<!--more-->\n\n# Bayesian Inference\n-\t在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）\n-\t统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计\n-\t在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$\n-\t后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布\n-\t在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和\n-\t后验实际上是在最大似然估计和先验之间权衡\n-\t当数据非常多时，后验渐渐不再依赖于先验\n-\t很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布\n-\t有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计\n\n# Markov Chain Monte Carlo\n-\tMCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数\n-\t最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布\n-\t蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量\n-\t蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）\n\n## Sample\n-\t直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量\n-\t在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数\n-\t最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：\n$$\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\\n$$\n\n## rejection sampling\n-\t但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling\n-\t对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/ildAC764ke.JPG)\n-\t我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接\n-\t显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：\n```\ni=0\nwhile i!= N\n\tx(i)~q(x) and u~U(0,1)\n\tif u< p(x(i))/Mq(x(i)) then\n\t\taccept x(i)\n\t\ti=i+1\n\telse\n\t\treject x(i)\n\tend\nend\n```\n-\trejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。\n\n## adaptive rejection sampling\n-\t当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高\n-\t基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域\n-\t但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/2gcFgBDJLa.JPG)\n\n## Importance Sampling\n-\t上面提到的采样算法是从简单分布采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布\n-\timportance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。\n-\t例如我们希望通过采样得到某个分布的期望\n$$\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\\n$$\n-\tp(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。\n-\t这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。\n\n## MCMC&MH\n-\tmcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关\n-\t不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。\n-\t我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{'} | x)$，从样本$x$转移到样本$x^{'}$。\n-\t在马尔可夫链中，有如下Chapman-Kologronvo等式：\n$$\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx\n$$\n-\t这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：\n$$\n\\pi _t (x) = \\pi _{t-1} (x)\n$$\n-\t实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：\n$$\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})\n$$\n-\t由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。\n-\t当满足细致平稳条件时，马氏链是收敛的\n-\t在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/6C57aKcD1d.JPG)\n-\t尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。\n-\t而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。\n\n## Hybrid Metropolis-Hasting\n-\t待补充\n\n## Gibbs Sampling\n-\t吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布\n-\t吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180913/CLgH680c1a.JPG)\n-\t工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的：\n```\nu~p(u|x,y,z)\nx,y,z~p(x,y,z|u)\n=p(x|u)p(y|u)p(z|u)\n```\n-\t上面关于x,y,z的三个条件概率可以并行计算。\n-\t现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率\n$$\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})\n$$\n-\t在gibbs中\n$$\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\\n$$\n-\t而且实际上从$x_{¬i}$到$x_{¬i}^{'}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此\n$$\nx_{¬i}^{'}=x_{¬i}\n$$\n-\t接下来看看gibbs的接受率\n$$\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\\n$$\n\n\n# Expectation Maximization\n## 更新\n-\t看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。\n\n## 公式\n-\t对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：\n\n$$\n\\theta=\\mathop{argmax}_{\\theta} L(\\theta | X) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\\n$$\n\n-\t之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导\n-\t这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。\n-\t每一次迭代分为两个部分，E和M，也就求期望和最大化\n\t-\t求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数\n\t-\t最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果\n-\t合起来就得到EM算法的公式：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t)}) dz\n$$\n## 为何有效\n-\t也就是证明，每次迭代后最大似然会增加\n-\t要证明：\n$$\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})\n$$\n-\t先改写对数似然\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n$$\n-\t两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：\n$$\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\\n$$\n-\t其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量\n-\t迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？\n-\t我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$\n$$\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\\n$$\n-\t其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入\tH,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性\n\n## 从ELBO的角度理解\n-\t我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式\n-\t在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：\n$$\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\\n$$\n-\t也就是\n$$\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))\n$$\n-\t其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然\n-\t可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤\n-\t我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：\n$$\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))\n$$\n-\tq取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：\n$$\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\\n$$\n-\t其中第二项与$\\theta$无关，因此：\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\\n$$\n-\t代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz\n$$\n-\t同样得到了EM算法的迭代公式\n-\t下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180830/dCj203m7jB.PNG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180830/6k4kbJH3bc.PNG)\n-\t剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入\n\n## 从假设隐变量为可观察的角度理解\n-\t这种理解来自Chuong B Do & Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?\n-\tEM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。\n-\tEM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。\n-\t猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。\n-\t所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。\n\n## EM算法与K-means\n-\tK-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。\n\n## 广义EM算法\n\n## Wake-Sleep算法\n\n## 广义EM算法与吉布斯采样\n\n# Variational Inference\n## ELBO\n-\t接下来介绍变分推断，可以看到，EM算法可以推广到变分推断\n-\t重新推出ELBO与对数似然的关系：\n$$\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\\n$$\n-\t两边对隐分布$q(z)$求期望\n$$\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\\n$$\n-\t我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。\n-\t接下来就是讨论如何使得ELBO最大化\n\n## 任意分布上的变分推断\n-\t对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量\n-\t我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：\n$$\nq(z)=\\prod _{i=1}^M q_i(z_i)\n$$\n-\t因此ELBO可以写成两部分\n$$\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\\n$$\n-\t其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成\n$$\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\\n$$\n-\t在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：\n$$\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n$$\n-\t这样part1用伪分布的形式可以改写成\n$$\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\\n$$\n-\tpart2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：\n$$\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n$$\n-\t再把part1和part2合起来，得到ELBO关于分量j的形式：\n$$\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\\n$$\n-\t也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度\n-\t何时这个KL散度最小？也就是：\n$$\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\\n$$\n-\t到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。\n\n## 指数家族分布\n-\t定义指数家族分布：\n$$\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\\n$$\n-\t其中\n\t-\t$T(x)$:sufficient statistics\n\t-\t$\\theta$:parameter of the family\n\t-\t$\\eta$:natural parameter\n\t-\t$h(x)$:underlying measure\n\t-\t$A(\\theta)$:log normalizer / partition function\n-\t注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：\n$$\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))\n$$\n-\t当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：\n$$\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\\n$$\n-\t继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：\n$$\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n$$\n-\t举个例子，高斯分布写成指数家族分布形式：\n$$\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )\n$$\n-\t用自然参数去替代方差和均值，写成指数家族分布形式：\n$$\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))\n$$\n-\t其中：\n\t-\t$T(x)$:$[x \\ x^2]$\n\t-\t$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$\n\t-\t$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$\n-\t接下来我们利用指数家族的性质来快速计算均值和方差\n$$\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\\n$$\n-\t为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：\n$$\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\\n$$\n-\t下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：\n$$\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\\n$$\n-\t用向量组的方式改写：\n$$\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\\n$$\n- 原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：\n$$\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\\n$$\n-\t我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：\n$$\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\\n$$\n-\t这样先验和后验形式一致，也就是共轭\n-\t这样我们用统一的形式写下似然和先验\n$$\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\\n$$\n-\t这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：\n$$\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\\n$$\n-\t上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。\n\n## 指数族分布下的变分推断\n-\t接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁\n-\t我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$\n-\t我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大\n-\t首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：\n$$\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\\n$$\n-\t其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：\n$$\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\\n$$\n-\t现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：\n$$\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\\n$$\n-\t代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：\n$$\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda) \n$$\n-\t利用之前log normalizer关于参数求导的结论，有:\n$$\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)\n$$\n-\t对上式求导，令其为0，有：\n$$\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\\n$$\n-\t我们就得到了$\\lambda$的迭代式！同理可以得到：\n$$\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\\n$$\n-\t写完整应该是：\n$$\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\\n$$\n-\t观察这两个迭代式，变量更新的路径是:\n$$\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda\n$$","slug":"inference-algorithm","published":1,"updated":"2018-09-21T07:45:23.962Z","comments":1,"layout":"post","link":"","_id":"cjmd072da0024qcw6k88en422","content":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p><a id=\"more\"></a><h1 id=\"Bayesian-Inference\"><a href=\"#Bayesian-Inference\" class=\"headerlink\" title=\"Bayesian Inference\"></a>Bayesian Inference</h1><ul><li>在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）</li><li>统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计</li><li>在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$</li><li>后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布</li><li>在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和</li><li>后验实际上是在最大似然估计和先验之间权衡</li><li>当数据非常多时，后验渐渐不再依赖于先验</li><li>很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布</li><li>有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计</li></ul><h1 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h1><ul><li>MCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数</li><li>最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布</li><li>蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量</li><li>蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）</li></ul><h2 id=\"Sample\"><a href=\"#Sample\" class=\"headerlink\" title=\"Sample\"></a>Sample</h2><ul><li>直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量</li><li>在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数</li><li>最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：<br>$$<br>u = U(0,1) \\\\<br>x= cdf ^{-1} (u) \\\\<br>$$</li></ul><h2 id=\"rejection-sampling\"><a href=\"#rejection-sampling\" class=\"headerlink\" title=\"rejection sampling\"></a>rejection sampling</h2><ul><li>但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling</li><li>对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/ildAC764ke.JPG\" alt=\"mark\"></li><li>我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接</li><li><p>显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=0</span><br><span class=\"line\">while i!= N</span><br><span class=\"line\">\tx(i)~q(x) and u~U(0,1)</span><br><span class=\"line\">\tif u&lt; p(x(i))/Mq(x(i)) then</span><br><span class=\"line\">\t\taccept x(i)</span><br><span class=\"line\">\t\ti=i+1</span><br><span class=\"line\">\telse</span><br><span class=\"line\">\t\treject x(i)</span><br><span class=\"line\">\tend</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></li><li><p>rejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。</p></li></ul><h2 id=\"adaptive-rejection-sampling\"><a href=\"#adaptive-rejection-sampling\" class=\"headerlink\" title=\"adaptive rejection sampling\"></a>adaptive rejection sampling</h2><ul><li>当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高</li><li>基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域</li><li>但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/2gcFgBDJLa.JPG\" alt=\"mark\"></li></ul><h2 id=\"Importance-Sampling\"><a href=\"#Importance-Sampling\" class=\"headerlink\" title=\"Importance Sampling\"></a>Importance Sampling</h2><ul><li>上面提到的采样算法是从简单分布采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布</li><li>importance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。</li><li>例如我们希望通过采样得到某个分布的期望<br>$$<br>E_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\<br>E_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\<br>E_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\<br>$$</li><li>p(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。</li><li>这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。</li></ul><h2 id=\"MCMC-amp-MH\"><a href=\"#MCMC-amp-MH\" class=\"headerlink\" title=\"MCMC&amp;MH\"></a>MCMC&amp;MH</h2><ul><li>mcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关</li><li>不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。</li><li>我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{‘} | x)$，从样本$x$转移到样本$x^{‘}$。</li><li>在马尔可夫链中，有如下Chapman-Kologronvo等式：<br>$$<br>\\pi _t (x^{‘}) = \\int _x \\pi _{t-1}(x) k(x^{‘} | x) dx<br>$$</li><li>这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：<br>$$<br>\\pi _t (x) = \\pi _{t-1} (x)<br>$$</li><li>实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：<br>$$<br>\\pi (x) k(x^{‘} | x) = \\pi (x^{‘}) k(x | x^{‘})<br>$$</li><li>由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。</li><li>当满足细致平稳条件时，马氏链是收敛的</li><li>在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/6C57aKcD1d.JPG\" alt=\"mark\"></li><li>尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。</li><li>而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。</li></ul><h2 id=\"Hybrid-Metropolis-Hasting\"><a href=\"#Hybrid-Metropolis-Hasting\" class=\"headerlink\" title=\"Hybrid Metropolis-Hasting\"></a>Hybrid Metropolis-Hasting</h2><ul><li>待补充</li></ul><h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul><li>吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布</li><li>吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/CLgH680c1a.JPG\" alt=\"mark\"></li><li><p>工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u~p(u|x,y,z)</span><br><span class=\"line\">x,y,z~p(x,y,z|u)</span><br><span class=\"line\">=p(x|u)p(y|u)p(z|u)</span><br></pre></td></tr></table></figure></li><li><p>上面关于x,y,z的三个条件概率可以并行计算。</p></li><li>现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率<br>$$<br>\\alpha = min(1,\\frac{\\pi (x^{‘}),q(x| x^{‘})}{\\pi (x) q(x^{‘} | x)})<br>$$</li><li>在gibbs中<br>$$<br>q(x|x^{‘})=\\pi (x_i | x_{¬i}^{‘}) \\\\<br>q(x^{‘}|x)=\\pi (x_i ^{‘} | x_{¬i}) \\\\<br>$$</li><li>而且实际上从$x_{¬i}$到$x_{¬i}^{‘}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此<br>$$<br>x_{¬i}^{‘}=x_{¬i}<br>$$</li><li>接下来看看gibbs的接受率<br>$$<br>\\alpha _{gibbs} = min(1,\\frac{\\pi (x^{‘}) \\pi (x_i | x_{¬i}^{‘})}{\\pi (x) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘} | x_{¬i}^{‘}) \\pi( x_{¬i}^{‘}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘} | x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,1) \\\\<br>= 1 \\\\<br>$$</li></ul><h1 id=\"Expectation-Maximization\"><a href=\"#Expectation-Maximization\" class=\"headerlink\" title=\"Expectation Maximization\"></a>Expectation Maximization</h1><h2 id=\"更新\"><a href=\"#更新\" class=\"headerlink\" title=\"更新\"></a>更新</h2><ul><li>看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。</li></ul><h2 id=\"公式\"><a href=\"#公式\" class=\"headerlink\" title=\"公式\"></a>公式</h2><ul><li>对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：</li></ul><p>$$<br>\\theta=\\mathop{argmax}_{\\theta} L(\\theta | X) \\\\<br>=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\<br>=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\<br>$$</p><ul><li>之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导</li><li>这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。</li><li>每一次迭代分为两个部分，E和M，也就求期望和最大化<ul><li>求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数</li><li>最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果</li></ul></li><li>合起来就得到EM算法的公式：<br>$$<br>\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t)}) dz<br>$$<h2 id=\"为何有效\"><a href=\"#为何有效\" class=\"headerlink\" title=\"为何有效\"></a>为何有效</h2></li><li>也就是证明，每次迭代后最大似然会增加</li><li>要证明：<br>$$<br>\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})<br>$$</li><li>先改写对数似然<br>$$<br>\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\<br>$$</li><li>两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：<br>$$<br>\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\<br>=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\<br>$$</li><li>其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量</li><li>迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？</li><li>我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$<br>$$<br>H(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\<br>\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\<br>= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>\\geq - \\log \\int _z (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>= - \\log 1 \\\\<br>= 0 \\\\<br>$$</li><li>其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入 H,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性</li></ul><h2 id=\"从ELBO的角度理解\"><a href=\"#从ELBO的角度理解\" class=\"headerlink\" title=\"从ELBO的角度理解\"></a>从ELBO的角度理解</h2><ul><li>我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式</li><li>在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：<br>$$<br>KL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\<br>= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\<br>$$</li><li>也就是<br>$$<br>\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))<br>$$</li><li>其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然</li><li>可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤</li><li>我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：<br>$$<br>ELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))<br>$$</li><li>q取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：<br>$$<br>ELBO = - KL(q(z)||p(x,z|\\theta)) \\\\<br>= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\<br>= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\<br>$$</li><li>其中第二项与$\\theta$无关，因此：<br>$$<br>\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\<br>$$</li><li>代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到<br>$$<br>\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz<br>$$</li><li>同样得到了EM算法的迭代公式</li><li>下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180830/dCj203m7jB.PNG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180830/6k4kbJH3bc.PNG\" alt=\"mark\"></li><li>剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入</li></ul><h2 id=\"从假设隐变量为可观察的角度理解\"><a href=\"#从假设隐变量为可观察的角度理解\" class=\"headerlink\" title=\"从假设隐变量为可观察的角度理解\"></a>从假设隐变量为可观察的角度理解</h2><ul><li>这种理解来自Chuong B Do &amp; Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?</li><li>EM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。</li><li>EM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。</li><li>猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。</li><li>所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。</li></ul><h2 id=\"EM算法与K-means\"><a href=\"#EM算法与K-means\" class=\"headerlink\" title=\"EM算法与K-means\"></a>EM算法与K-means</h2><ul><li>K-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。</li></ul><h2 id=\"广义EM算法\"><a href=\"#广义EM算法\" class=\"headerlink\" title=\"广义EM算法\"></a>广义EM算法</h2><h2 id=\"Wake-Sleep算法\"><a href=\"#Wake-Sleep算法\" class=\"headerlink\" title=\"Wake-Sleep算法\"></a>Wake-Sleep算法</h2><h2 id=\"广义EM算法与吉布斯采样\"><a href=\"#广义EM算法与吉布斯采样\" class=\"headerlink\" title=\"广义EM算法与吉布斯采样\"></a>广义EM算法与吉布斯采样</h2><h1 id=\"Variational-Inference\"><a href=\"#Variational-Inference\" class=\"headerlink\" title=\"Variational Inference\"></a>Variational Inference</h1><h2 id=\"ELBO\"><a href=\"#ELBO\" class=\"headerlink\" title=\"ELBO\"></a>ELBO</h2><ul><li>接下来介绍变分推断，可以看到，EM算法可以推广到变分推断</li><li>重新推出ELBO与对数似然的关系：<br>$$<br>\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\<br>= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\<br>= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\<br>$$</li><li>两边对隐分布$q(z)$求期望<br>$$<br>\\log p(x) = \\\\<br>[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\<br>= ELBO+KL(q||p(z|x)) \\\\<br>$$</li><li>我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。</li><li>接下来就是讨论如何使得ELBO最大化</li></ul><h2 id=\"任意分布上的变分推断\"><a href=\"#任意分布上的变分推断\" class=\"headerlink\" title=\"任意分布上的变分推断\"></a>任意分布上的变分推断</h2><ul><li>对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量</li><li>我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：<br>$$<br>q(z)=\\prod _{i=1}^M q_i(z_i)<br>$$</li><li>因此ELBO可以写成两部分<br>$$<br>ELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\<br>=part1-part2 \\\\<br>$$</li><li>其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成<br>$$<br>part1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\<br>= \\int _{z_1} \\int _{z_2} … \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , … ,d z_M \\\\<br>= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\<br>= \\int _{z_j} q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\<br>$$</li><li>在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：<br>$$<br>p_j(z_j) = \\int _{i \\neq j} p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>p_j^{‘}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>\\log p_j^{‘}(z_j) = \\int _{i \\neq j} \\log p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>$$</li><li>这样part1用伪分布的形式可以改写成<br>$$<br>part1= \\int _{z_j} q_j(z_j) \\log p_j^{‘}(x,z_j) \\\\<br>$$</li><li>part2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：<br>$$<br>part2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\<br>= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\<br>= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\<br>$$</li><li>再把part1和part2合起来，得到ELBO关于分量j的形式：<br>$$<br>ELBO = \\int _{z_j} \\log \\log p_j^{‘}(x,z_j) - \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\<br>= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{‘}(x,z_j)}{q_j(z_j)} + const \\\\<br>= - KL(p_j^{‘}(x,z_j) || q_j(z_j)) + const\\\\<br>$$</li><li>也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度</li><li>何时这个KL散度最小？也就是：<br>$$<br>q_j(z_j) = p_j^{‘}(x,z_j) \\\\<br>\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\<br>$$</li><li>到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。</li></ul><h2 id=\"指数家族分布\"><a href=\"#指数家族分布\" class=\"headerlink\" title=\"指数家族分布\"></a>指数家族分布</h2><ul><li>定义指数家族分布：<br>$$<br>p(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\<br>$$</li><li>其中<ul><li>$T(x)$:sufficient statistics</li><li>$\\theta$:parameter of the family</li><li>$\\eta$:natural parameter</li><li>$h(x)$:underlying measure</li><li>$A(\\theta)$:log normalizer / partition function</li></ul></li><li>注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：<br>$$<br>p(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))<br>$$</li><li>当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：<br>$$<br>\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\<br>= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\<br>$$</li><li>继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：<br>$$<br>\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{‘}(\\eta) =0 \\\\<br>A^{‘}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\<br>$$</li><li>举个例子，高斯分布写成指数家族分布形式：<br>$$<br>p(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\<br>=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )<br>$$</li><li>用自然参数去替代方差和均值，写成指数家族分布形式：<br>$$<br>p(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))<br>$$</li><li>其中：<ul><li>$T(x)$:$[x \\ x^2]$</li><li>$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$</li><li>$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$</li></ul></li><li>接下来我们利用指数家族的性质来快速计算均值和方差<br>$$<br>A^{‘}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\<br>[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\<br>= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\<br>= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\<br>$$</li><li>为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：<br>$$<br>\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\<br>A(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\<br>$$</li><li>下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：<br>$$<br>p(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\<br>∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\<br>$$</li><li>用向量组的方式改写：<br>$$<br>T(\\beta) = [\\beta \\ -g(\\beta)] \\\\<br>\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\<br>$$</li><li>原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：<br>$$<br>∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\<br>$$</li><li>我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：<br>$$<br>∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\<br>∝ h(\\beta) exp(\\alpha _1 ^{‘} \\beta - \\alpha _2 ^{‘} A_l (\\beta)) \\\\<br>$$</li><li>这样先验和后验形式一致，也就是共轭</li><li>这样我们用统一的形式写下似然和先验<br>$$<br>p(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\<br>∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\<br>$$</li><li>这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：<br>$$<br>\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\<br>= E_{p(x|\\beta)} [T(x)] \\\\<br>$$</li><li>上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。</li></ul><h2 id=\"指数族分布下的变分推断\"><a href=\"#指数族分布下的变分推断\" class=\"headerlink\" title=\"指数族分布下的变分推断\"></a>指数族分布下的变分推断</h2><ul><li>接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁</li><li>我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$</li><li>我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大</li><li>首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：<br>$$<br>ELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\<br>= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\<br>$$</li><li>其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：<br>$$<br>p(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\<br>\\approx q(\\beta | \\lambda) \\\\<br>= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\<br>p(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\<br>\\approx q(\\beta | \\phi) \\\\<br>= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\<br>$$</li><li>现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：<br>$$<br>ELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\<br>$$</li><li>代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：<br>$$<br>ELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)] -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda)<br>$$</li><li>利用之前log normalizer关于参数求导的结论，有:<br>$$<br>ELBO_{\\lambda} = A_g^{‘}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{‘}(\\lambda) ^T + A_g (\\lambda)<br>$$</li><li>对上式求导，令其为0，有：<br>$$<br>A_g^{‘’}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{‘}(\\lambda)-\\lambda A_g^{‘’}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\<br>\\lambda = E_{q(z)}[\\eta(z,x)] \\\\<br>$$</li><li>我们就得到了$\\lambda$的迭代式！同理可以得到：<br>$$<br>\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\<br>$$</li><li>写完整应该是：<br>$$<br>\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\<br>\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\<br>$$</li><li>观察这两个迭代式，变量更新的路径是:<br>$$<br>\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda<br>$$</li></ul>","site":{"data":{}},"excerpt":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p>","more":"<h1 id=\"Bayesian-Inference\"><a href=\"#Bayesian-Inference\" class=\"headerlink\" title=\"Bayesian Inference\"></a>Bayesian Inference</h1><ul><li>在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）</li><li>统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计</li><li>在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$</li><li>后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布</li><li>在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和</li><li>后验实际上是在最大似然估计和先验之间权衡</li><li>当数据非常多时，后验渐渐不再依赖于先验</li><li>很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布</li><li>有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计</li></ul><h1 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h1><ul><li>MCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数</li><li>最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布</li><li>蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量</li><li>蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）</li></ul><h2 id=\"Sample\"><a href=\"#Sample\" class=\"headerlink\" title=\"Sample\"></a>Sample</h2><ul><li>直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量</li><li>在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数</li><li>最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：<br>$$<br>u = U(0,1) \\\\<br>x= cdf ^{-1} (u) \\\\<br>$$</li></ul><h2 id=\"rejection-sampling\"><a href=\"#rejection-sampling\" class=\"headerlink\" title=\"rejection sampling\"></a>rejection sampling</h2><ul><li>但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling</li><li>对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/ildAC764ke.JPG\" alt=\"mark\"></li><li>我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接</li><li><p>显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=0</span><br><span class=\"line\">while i!= N</span><br><span class=\"line\">\tx(i)~q(x) and u~U(0,1)</span><br><span class=\"line\">\tif u&lt; p(x(i))/Mq(x(i)) then</span><br><span class=\"line\">\t\taccept x(i)</span><br><span class=\"line\">\t\ti=i+1</span><br><span class=\"line\">\telse</span><br><span class=\"line\">\t\treject x(i)</span><br><span class=\"line\">\tend</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></li><li><p>rejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。</p></li></ul><h2 id=\"adaptive-rejection-sampling\"><a href=\"#adaptive-rejection-sampling\" class=\"headerlink\" title=\"adaptive rejection sampling\"></a>adaptive rejection sampling</h2><ul><li>当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高</li><li>基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域</li><li>但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/2gcFgBDJLa.JPG\" alt=\"mark\"></li></ul><h2 id=\"Importance-Sampling\"><a href=\"#Importance-Sampling\" class=\"headerlink\" title=\"Importance Sampling\"></a>Importance Sampling</h2><ul><li>上面提到的采样算法是从简单分布采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布</li><li>importance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。</li><li>例如我们希望通过采样得到某个分布的期望<br>$$<br>E_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\<br>E_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\<br>E_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\<br>$$</li><li>p(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。</li><li>这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。</li></ul><h2 id=\"MCMC-amp-MH\"><a href=\"#MCMC-amp-MH\" class=\"headerlink\" title=\"MCMC&amp;MH\"></a>MCMC&amp;MH</h2><ul><li>mcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关</li><li>不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。</li><li>我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{‘} | x)$，从样本$x$转移到样本$x^{‘}$。</li><li>在马尔可夫链中，有如下Chapman-Kologronvo等式：<br>$$<br>\\pi _t (x^{‘}) = \\int _x \\pi _{t-1}(x) k(x^{‘} | x) dx<br>$$</li><li>这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：<br>$$<br>\\pi _t (x) = \\pi _{t-1} (x)<br>$$</li><li>实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：<br>$$<br>\\pi (x) k(x^{‘} | x) = \\pi (x^{‘}) k(x | x^{‘})<br>$$</li><li>由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。</li><li>当满足细致平稳条件时，马氏链是收敛的</li><li>在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/6C57aKcD1d.JPG\" alt=\"mark\"></li><li>尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。</li><li>而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。</li></ul><h2 id=\"Hybrid-Metropolis-Hasting\"><a href=\"#Hybrid-Metropolis-Hasting\" class=\"headerlink\" title=\"Hybrid Metropolis-Hasting\"></a>Hybrid Metropolis-Hasting</h2><ul><li>待补充</li></ul><h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul><li>吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布</li><li>吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180913/CLgH680c1a.JPG\" alt=\"mark\"></li><li><p>工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u~p(u|x,y,z)</span><br><span class=\"line\">x,y,z~p(x,y,z|u)</span><br><span class=\"line\">=p(x|u)p(y|u)p(z|u)</span><br></pre></td></tr></table></figure></li><li><p>上面关于x,y,z的三个条件概率可以并行计算。</p></li><li>现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率<br>$$<br>\\alpha = min(1,\\frac{\\pi (x^{‘}),q(x| x^{‘})}{\\pi (x) q(x^{‘} | x)})<br>$$</li><li>在gibbs中<br>$$<br>q(x|x^{‘})=\\pi (x_i | x_{¬i}^{‘}) \\\\<br>q(x^{‘}|x)=\\pi (x_i ^{‘} | x_{¬i}) \\\\<br>$$</li><li>而且实际上从$x_{¬i}$到$x_{¬i}^{‘}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此<br>$$<br>x_{¬i}^{‘}=x_{¬i}<br>$$</li><li>接下来看看gibbs的接受率<br>$$<br>\\alpha _{gibbs} = min(1,\\frac{\\pi (x^{‘}) \\pi (x_i | x_{¬i}^{‘})}{\\pi (x) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘} | x_{¬i}^{‘}) \\pi( x_{¬i}^{‘}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,\\frac{\\pi (x^{‘} | x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{‘} | x_{¬i})}) \\\\<br>= min(1,1) \\\\<br>= 1 \\\\<br>$$</li></ul><h1 id=\"Expectation-Maximization\"><a href=\"#Expectation-Maximization\" class=\"headerlink\" title=\"Expectation Maximization\"></a>Expectation Maximization</h1><h2 id=\"更新\"><a href=\"#更新\" class=\"headerlink\" title=\"更新\"></a>更新</h2><ul><li>看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。</li></ul><h2 id=\"公式\"><a href=\"#公式\" class=\"headerlink\" title=\"公式\"></a>公式</h2><ul><li>对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：</li></ul><p>$$<br>\\theta=\\mathop{argmax}_{\\theta} L(\\theta | X) \\\\<br>=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\<br>=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\<br>$$</p><ul><li>之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导</li><li>这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。</li><li>每一次迭代分为两个部分，E和M，也就求期望和最大化<ul><li>求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数</li><li>最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果</li></ul></li><li>合起来就得到EM算法的公式：<br>$$<br>\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t)}) dz<br>$$<h2 id=\"为何有效\"><a href=\"#为何有效\" class=\"headerlink\" title=\"为何有效\"></a>为何有效</h2></li><li>也就是证明，每次迭代后最大似然会增加</li><li>要证明：<br>$$<br>\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})<br>$$</li><li>先改写对数似然<br>$$<br>\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\<br>$$</li><li>两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：<br>$$<br>\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\<br>=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\<br>$$</li><li>其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量</li><li>迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？</li><li>我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$<br>$$<br>H(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\<br>\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\<br>= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>\\geq - \\log \\int _z (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\<br>= - \\log 1 \\\\<br>= 0 \\\\<br>$$</li><li>其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入 H,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性</li></ul><h2 id=\"从ELBO的角度理解\"><a href=\"#从ELBO的角度理解\" class=\"headerlink\" title=\"从ELBO的角度理解\"></a>从ELBO的角度理解</h2><ul><li>我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式</li><li>在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：<br>$$<br>KL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\<br>= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\<br>= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\<br>$$</li><li>也就是<br>$$<br>\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))<br>$$</li><li>其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然</li><li>可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤</li><li>我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：<br>$$<br>ELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))<br>$$</li><li>q取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：<br>$$<br>ELBO = - KL(q(z)||p(x,z|\\theta)) \\\\<br>= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\<br>= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\<br>$$</li><li>其中第二项与$\\theta$无关，因此：<br>$$<br>\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\<br>$$</li><li>代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到<br>$$<br>\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz<br>$$</li><li>同样得到了EM算法的迭代公式</li><li>下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180830/dCj203m7jB.PNG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180830/6k4kbJH3bc.PNG\" alt=\"mark\"></li><li>剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入</li></ul><h2 id=\"从假设隐变量为可观察的角度理解\"><a href=\"#从假设隐变量为可观察的角度理解\" class=\"headerlink\" title=\"从假设隐变量为可观察的角度理解\"></a>从假设隐变量为可观察的角度理解</h2><ul><li>这种理解来自Chuong B Do &amp; Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?</li><li>EM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。</li><li>EM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。</li><li>猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。</li><li>所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。</li></ul><h2 id=\"EM算法与K-means\"><a href=\"#EM算法与K-means\" class=\"headerlink\" title=\"EM算法与K-means\"></a>EM算法与K-means</h2><ul><li>K-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。</li></ul><h2 id=\"广义EM算法\"><a href=\"#广义EM算法\" class=\"headerlink\" title=\"广义EM算法\"></a>广义EM算法</h2><h2 id=\"Wake-Sleep算法\"><a href=\"#Wake-Sleep算法\" class=\"headerlink\" title=\"Wake-Sleep算法\"></a>Wake-Sleep算法</h2><h2 id=\"广义EM算法与吉布斯采样\"><a href=\"#广义EM算法与吉布斯采样\" class=\"headerlink\" title=\"广义EM算法与吉布斯采样\"></a>广义EM算法与吉布斯采样</h2><h1 id=\"Variational-Inference\"><a href=\"#Variational-Inference\" class=\"headerlink\" title=\"Variational Inference\"></a>Variational Inference</h1><h2 id=\"ELBO\"><a href=\"#ELBO\" class=\"headerlink\" title=\"ELBO\"></a>ELBO</h2><ul><li>接下来介绍变分推断，可以看到，EM算法可以推广到变分推断</li><li>重新推出ELBO与对数似然的关系：<br>$$<br>\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\<br>= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\<br>= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\<br>$$</li><li>两边对隐分布$q(z)$求期望<br>$$<br>\\log p(x) = \\\\<br>[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\<br>= ELBO+KL(q||p(z|x)) \\\\<br>$$</li><li>我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。</li><li>接下来就是讨论如何使得ELBO最大化</li></ul><h2 id=\"任意分布上的变分推断\"><a href=\"#任意分布上的变分推断\" class=\"headerlink\" title=\"任意分布上的变分推断\"></a>任意分布上的变分推断</h2><ul><li>对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量</li><li>我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：<br>$$<br>q(z)=\\prod _{i=1}^M q_i(z_i)<br>$$</li><li>因此ELBO可以写成两部分<br>$$<br>ELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\<br>=part1-part2 \\\\<br>$$</li><li>其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成<br>$$<br>part1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\<br>= \\int _{z_1} \\int _{z_2} … \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , … ,d z_M \\\\<br>= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\<br>= \\int _{z_j} q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\<br>$$</li><li>在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：<br>$$<br>p_j(z_j) = \\int _{i \\neq j} p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>p_j^{‘}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>\\log p_j^{‘}(z_j) = \\int _{i \\neq j} \\log p(z_1,…,z_i) d z_1 , d z_2 ,…, d z_i \\\\<br>$$</li><li>这样part1用伪分布的形式可以改写成<br>$$<br>part1= \\int _{z_j} q_j(z_j) \\log p_j^{‘}(x,z_j) \\\\<br>$$</li><li>part2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：<br>$$<br>part2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\<br>= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\<br>= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\<br>$$</li><li>再把part1和part2合起来，得到ELBO关于分量j的形式：<br>$$<br>ELBO = \\int _{z_j} \\log \\log p_j^{‘}(x,z_j) - \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\<br>= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{‘}(x,z_j)}{q_j(z_j)} + const \\\\<br>= - KL(p_j^{‘}(x,z_j) || q_j(z_j)) + const\\\\<br>$$</li><li>也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度</li><li>何时这个KL散度最小？也就是：<br>$$<br>q_j(z_j) = p_j^{‘}(x,z_j) \\\\<br>\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\<br>$$</li><li>到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。</li></ul><h2 id=\"指数家族分布\"><a href=\"#指数家族分布\" class=\"headerlink\" title=\"指数家族分布\"></a>指数家族分布</h2><ul><li>定义指数家族分布：<br>$$<br>p(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\<br>$$</li><li>其中<ul><li>$T(x)$:sufficient statistics</li><li>$\\theta$:parameter of the family</li><li>$\\eta$:natural parameter</li><li>$h(x)$:underlying measure</li><li>$A(\\theta)$:log normalizer / partition function</li></ul></li><li>注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：<br>$$<br>p(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))<br>$$</li><li>当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：<br>$$<br>\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\<br>= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\<br>= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\<br>$$</li><li>继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：<br>$$<br>\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{‘}(\\eta) =0 \\\\<br>A^{‘}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\<br>$$</li><li>举个例子，高斯分布写成指数家族分布形式：<br>$$<br>p(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\<br>=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )<br>$$</li><li>用自然参数去替代方差和均值，写成指数家族分布形式：<br>$$<br>p(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))<br>$$</li><li>其中：<ul><li>$T(x)$:$[x \\ x^2]$</li><li>$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$</li><li>$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$</li></ul></li><li>接下来我们利用指数家族的性质来快速计算均值和方差<br>$$<br>A^{‘}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\<br>[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\<br>= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\<br>= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\<br>$$</li><li>为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：<br>$$<br>\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\<br>A(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\<br>$$</li><li>下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：<br>$$<br>p(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\<br>∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\<br>$$</li><li>用向量组的方式改写：<br>$$<br>T(\\beta) = [\\beta \\ -g(\\beta)] \\\\<br>\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\<br>$$</li><li>原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：<br>$$<br>∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\<br>$$</li><li>我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：<br>$$<br>∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\<br>∝ h(\\beta) exp(\\alpha _1 ^{‘} \\beta - \\alpha _2 ^{‘} A_l (\\beta)) \\\\<br>$$</li><li>这样先验和后验形式一致，也就是共轭</li><li>这样我们用统一的形式写下似然和先验<br>$$<br>p(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\<br>∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\<br>$$</li><li>这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：<br>$$<br>\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\<br>= E_{p(x|\\beta)} [T(x)] \\\\<br>$$</li><li>上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。</li></ul><h2 id=\"指数族分布下的变分推断\"><a href=\"#指数族分布下的变分推断\" class=\"headerlink\" title=\"指数族分布下的变分推断\"></a>指数族分布下的变分推断</h2><ul><li>接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁</li><li>我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$</li><li>我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大</li><li>首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：<br>$$<br>ELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\<br>= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\<br>$$</li><li>其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：<br>$$<br>p(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\<br>\\approx q(\\beta | \\lambda) \\\\<br>= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\<br>p(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\<br>\\approx q(\\beta | \\phi) \\\\<br>= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\<br>$$</li><li>现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：<br>$$<br>ELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\<br>$$</li><li>代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：<br>$$<br>ELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)] -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda)<br>$$</li><li>利用之前log normalizer关于参数求导的结论，有:<br>$$<br>ELBO_{\\lambda} = A_g^{‘}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{‘}(\\lambda) ^T + A_g (\\lambda)<br>$$</li><li>对上式求导，令其为0，有：<br>$$<br>A_g^{‘’}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{‘}(\\lambda)-\\lambda A_g^{‘’}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\<br>\\lambda = E_{q(z)}[\\eta(z,x)] \\\\<br>$$</li><li>我们就得到了$\\lambda$的迭代式！同理可以得到：<br>$$<br>\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\<br>$$</li><li>写完整应该是：<br>$$<br>\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\<br>\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\<br>$$</li><li>观察这两个迭代式，变量更新的路径是:<br>$$<br>\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda<br>$$</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Fri Sep 21 2018 15:45:23 GMT+0800 (中国标准时间)","title":"推断算法笔记","path":"2018/08/28/inference-algorithm/","eyeCatchImage":null,"excerpt":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p>","date":"2018-08-28T01:55:10.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","mcmc","inference","variational inference","em"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"pandas 数据处理基础","date":"2017-02-04T13:02:39.000Z","_content":"***\n以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。\n数据在这：\n# 引入库\n```python\nimport csv as csv \nimport pandas as pd\nimport numpy as np\n```\n# 读取文件\n```python\ntrain = pd.read_csv(r\"文件目录\") \n```\n此时数据的样式是：\n![](http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG)\n\n\n<!--more-->\n\n# 数据概览\n-\tdescribe 显示整体数据常见属性\n```python\nprint(train.describe())\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222443347.JPG)\n-\thead tail 显示首尾一些数据\n```python\nprint(train.head(5))\nprint(train.tail(3))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222504633.JPG)\n-\tindex：索引，默认自建整型索引；columns：列；values：数据数值\n```python\nprint(train.index)\nprint(train.columns)\nprint(train.values)\n```\n# 数据操作\n-\tT：数据的转置\n```python\nprint(train.T)\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222544174.JPG)\n-\tsort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序\n```python\nprint(train.sort_index(axis=0,ascending=True))\nprint(train.sort_values(by=\"Age\",ascending=False))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222551913.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222617681.JPG)\n\n# 数据选择\n-\t按照标签选择，选择列，行切片\n```python\nprint(train['Age'])\nprint(train[0:9])\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222634103.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222640068.JPG)\n-\t利用loc自由选择某些行某些列，可以用at替代\n```python\nprint(train.loc[train.index[4:6]])\nprint(train.loc[:,['Age','Fare']])\nprint(train.loc[3:5,['Age','Fare']])\nprint(train.loc[4,'Age'])\nprint(train.at[4,'Age'])\n```\n-\t利用iloc按照位置进行选择\n```python\nprint(train.iloc[5])\nprint(train.iloc[3:5,2:4])\nprint(train.iloc[[1,2,4],[2,5]])\nprint(train.iloc[3,3])\n```\n-\t布尔选择\n```python\nprint( train[ (train['Age']>40) & (train['Age']<50) ] )\nprint(train[train['Parch'].isin([1,2])])\nprint(train[pd.isnull(train['Age'])==True])\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222658806.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222706679.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222715106.JPG)\n\n# 缺失值处理\n-\t利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()\n```python\ntrain1=train.reindex(index=train.index[0:5],columns=['PassengerId']+['Age']+['Sex'])#选择前5行，只取选定的三列\nprint(train1)\nprint(train1.dropna(axis=0)) #删除存在nan值的行\nprint(train1.dropna(subset=['Age','Sex'])) #删除年龄性别列中存在nan值的行\nprint(pd.isnull(train1)) #nan值改为true，其余值改为false\nprint(train1.fillna(value=2333)) #缺失值替换为2333\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222723080.JPG)\n# 应用函数\n-\t可以自己写函数并应用到数据的行或者列，通过axis参数选择行列\n```python\n#写函数统计包含nan值的行数\ndef null_count(column):\n    column_null=pd.isnull(column)\n    null=column[column_null == True]\n    return len(null)\nprint(train.apply(null_count))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222730409.JPG)\n```python\n#写函数对年龄列进行分类\ndef judge(row):\n    if pd.isnull(row['Age']) ==True:\n        return 'unknown'\n    return 'youngth' if row['Age']<18 else 'adult'\nprint(train.apply(judge,axis=1))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222741327.JPG)\n# 数据透视表\n-\t自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值\n```python\nprint(train.pivot_table(index=[\"Pclass\",\"Sex\"], values=[\"Age\", \"Fare\"], aggfunc=np.mean))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222751776.JPG)\n\n\n\n# 数据合并\n\n- 数据合并的一些操作，待补全\n\n  ```Python\n  import pandas as pd\n  data1 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber':[1,3,5,7]})\n   \n  data2=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber':[2,3,6,10]})\n\n  print(\"merge:\\n\",pd.merge(data1,data2),\"\\n\")\n\n  data3 = pd.DataFrame({'level1':['a','b','c','d'],\n                   'numeber1':[1,3,5,7]})\n  data4 = pd.DataFrame({'level2':['a','b','c','e'],\n                   'numeber2':[2,3,6,10]})\n  print(\"merge with left_on,right_on: \\n\",pd.merge(data3,data4,left_on='level1',right_on='level2'),\"\\n\")\n\n  print(\"concat: \\n\",pd.concat([data1,data2]),\"\\n\")\n\n  data3 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber1':[1,3,5,np.nan]})\n  data4=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber2':[2,np.nan,6,10]})\n  print(\"combine: \\n\",data3.combine_first(data4),\"\\n\")\n  ```\n  ![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/214643504.JPG)\n","source":"_posts/pandas-skill.md","raw":"---\ntitle: pandas 数据处理基础\ndate: 2017-02-04 21:02:39\ntags: [math,machinelearning,python,code]\ncategories: Python\n---\n***\n以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。\n数据在这：\n# 引入库\n```python\nimport csv as csv \nimport pandas as pd\nimport numpy as np\n```\n# 读取文件\n```python\ntrain = pd.read_csv(r\"文件目录\") \n```\n此时数据的样式是：\n![](http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG)\n\n\n<!--more-->\n\n# 数据概览\n-\tdescribe 显示整体数据常见属性\n```python\nprint(train.describe())\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222443347.JPG)\n-\thead tail 显示首尾一些数据\n```python\nprint(train.head(5))\nprint(train.tail(3))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222504633.JPG)\n-\tindex：索引，默认自建整型索引；columns：列；values：数据数值\n```python\nprint(train.index)\nprint(train.columns)\nprint(train.values)\n```\n# 数据操作\n-\tT：数据的转置\n```python\nprint(train.T)\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222544174.JPG)\n-\tsort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序\n```python\nprint(train.sort_index(axis=0,ascending=True))\nprint(train.sort_values(by=\"Age\",ascending=False))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222551913.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222617681.JPG)\n\n# 数据选择\n-\t按照标签选择，选择列，行切片\n```python\nprint(train['Age'])\nprint(train[0:9])\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222634103.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222640068.JPG)\n-\t利用loc自由选择某些行某些列，可以用at替代\n```python\nprint(train.loc[train.index[4:6]])\nprint(train.loc[:,['Age','Fare']])\nprint(train.loc[3:5,['Age','Fare']])\nprint(train.loc[4,'Age'])\nprint(train.at[4,'Age'])\n```\n-\t利用iloc按照位置进行选择\n```python\nprint(train.iloc[5])\nprint(train.iloc[3:5,2:4])\nprint(train.iloc[[1,2,4],[2,5]])\nprint(train.iloc[3,3])\n```\n-\t布尔选择\n```python\nprint( train[ (train['Age']>40) & (train['Age']<50) ] )\nprint(train[train['Parch'].isin([1,2])])\nprint(train[pd.isnull(train['Age'])==True])\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222658806.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222706679.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222715106.JPG)\n\n# 缺失值处理\n-\t利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()\n```python\ntrain1=train.reindex(index=train.index[0:5],columns=['PassengerId']+['Age']+['Sex'])#选择前5行，只取选定的三列\nprint(train1)\nprint(train1.dropna(axis=0)) #删除存在nan值的行\nprint(train1.dropna(subset=['Age','Sex'])) #删除年龄性别列中存在nan值的行\nprint(pd.isnull(train1)) #nan值改为true，其余值改为false\nprint(train1.fillna(value=2333)) #缺失值替换为2333\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222723080.JPG)\n# 应用函数\n-\t可以自己写函数并应用到数据的行或者列，通过axis参数选择行列\n```python\n#写函数统计包含nan值的行数\ndef null_count(column):\n    column_null=pd.isnull(column)\n    null=column[column_null == True]\n    return len(null)\nprint(train.apply(null_count))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222730409.JPG)\n```python\n#写函数对年龄列进行分类\ndef judge(row):\n    if pd.isnull(row['Age']) ==True:\n        return 'unknown'\n    return 'youngth' if row['Age']<18 else 'adult'\nprint(train.apply(judge,axis=1))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222741327.JPG)\n# 数据透视表\n-\t自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值\n```python\nprint(train.pivot_table(index=[\"Pclass\",\"Sex\"], values=[\"Age\", \"Fare\"], aggfunc=np.mean))\n```\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222751776.JPG)\n\n\n\n# 数据合并\n\n- 数据合并的一些操作，待补全\n\n  ```Python\n  import pandas as pd\n  data1 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber':[1,3,5,7]})\n   \n  data2=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber':[2,3,6,10]})\n\n  print(\"merge:\\n\",pd.merge(data1,data2),\"\\n\")\n\n  data3 = pd.DataFrame({'level1':['a','b','c','d'],\n                   'numeber1':[1,3,5,7]})\n  data4 = pd.DataFrame({'level2':['a','b','c','e'],\n                   'numeber2':[2,3,6,10]})\n  print(\"merge with left_on,right_on: \\n\",pd.merge(data3,data4,left_on='level1',right_on='level2'),\"\\n\")\n\n  print(\"concat: \\n\",pd.concat([data1,data2]),\"\\n\")\n\n  data3 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber1':[1,3,5,np.nan]})\n  data4=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber2':[2,np.nan,6,10]})\n  print(\"combine: \\n\",data3.combine_first(data4),\"\\n\")\n  ```\n  ![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/214643504.JPG)\n","slug":"pandas-skill","published":1,"updated":"2018-07-23T01:28:38.518Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072db0027qcw6aj6tcfif","content":"<hr><p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。<br>数据在这：</p><h1 id=\"引入库\"><a href=\"#引入库\" class=\"headerlink\" title=\"引入库\"></a>引入库</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure><h1 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = pd.read_csv(<span class=\"string\">r\"文件目录\"</span>)</span><br></pre></td></tr></table></figure><p>此时数据的样式是：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG\" alt=\"\"></p><a id=\"more\"></a><h1 id=\"数据概览\"><a href=\"#数据概览\" class=\"headerlink\" title=\"数据概览\"></a>数据概览</h1><ul><li>describe 显示整体数据常见属性<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.describe())</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222443347.JPG\" alt=\"mark\"></p><ul><li>head tail 显示首尾一些数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.head(<span class=\"number\">5</span>))</span><br><span class=\"line\">print(train.tail(<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222504633.JPG\" alt=\"mark\"></p><ul><li>index：索引，默认自建整型索引；columns：列；values：数据数值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.index)</span><br><span class=\"line\">print(train.columns)</span><br><span class=\"line\">print(train.values)</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h1><ul><li>T：数据的转置<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.T)</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222544174.JPG\" alt=\"mark\"></p><ul><li>sort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.sort_index(axis=<span class=\"number\">0</span>,ascending=<span class=\"keyword\">True</span>))</span><br><span class=\"line\">print(train.sort_values(by=<span class=\"string\">\"Age\"</span>,ascending=<span class=\"keyword\">False</span>))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222551913.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222617681.JPG\" alt=\"mark\"></p><h1 id=\"数据选择\"><a href=\"#数据选择\" class=\"headerlink\" title=\"数据选择\"></a>数据选择</h1><ul><li>按照标签选择，选择列，行切片<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train[<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train[<span class=\"number\">0</span>:<span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222634103.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222640068.JPG\" alt=\"mark\"></p><ul><li><p>利用loc自由选择某些行某些列，可以用at替代</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.loc[train.index[<span class=\"number\">4</span>:<span class=\"number\">6</span>]])</span><br><span class=\"line\">print(train.loc[:,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train.at[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br></pre></td></tr></table></figure></li><li><p>利用iloc按照位置进行选择</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.iloc[<span class=\"number\">5</span>])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">2</span>:<span class=\"number\">4</span>])</span><br><span class=\"line\">print(train.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">5</span>]])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure></li><li><p>布尔选择</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print( train[ (train[<span class=\"string\">'Age'</span>]&gt;<span class=\"number\">40</span>) &amp; (train[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">50</span>) ] )</span><br><span class=\"line\">print(train[train[<span class=\"string\">'Parch'</span>].isin([<span class=\"number\">1</span>,<span class=\"number\">2</span>])])</span><br><span class=\"line\">print(train[pd.isnull(train[<span class=\"string\">'Age'</span>])==<span class=\"keyword\">True</span>])</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222658806.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222706679.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222715106.JPG\" alt=\"mark\"></p><h1 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h1><ul><li>利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train1=train.reindex(index=train.index[<span class=\"number\">0</span>:<span class=\"number\">5</span>],columns=[<span class=\"string\">'PassengerId'</span>]+[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>])<span class=\"comment\">#选择前5行，只取选定的三列</span></span><br><span class=\"line\">print(train1)</span><br><span class=\"line\">print(train1.dropna(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#删除存在nan值的行</span></span><br><span class=\"line\">print(train1.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>])) <span class=\"comment\">#删除年龄性别列中存在nan值的行</span></span><br><span class=\"line\">print(pd.isnull(train1)) <span class=\"comment\">#nan值改为true，其余值改为false</span></span><br><span class=\"line\">print(train1.fillna(value=<span class=\"number\">2333</span>)) <span class=\"comment\">#缺失值替换为2333</span></span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222723080.JPG\" alt=\"mark\"></p><h1 id=\"应用函数\"><a href=\"#应用函数\" class=\"headerlink\" title=\"应用函数\"></a>应用函数</h1><ul><li>可以自己写函数并应用到数据的行或者列，通过axis参数选择行列<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数统计包含nan值的行数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">null_count</span><span class=\"params\">(column)</span>:</span></span><br><span class=\"line\">    column_null=pd.isnull(column)</span><br><span class=\"line\">    null=column[column_null == <span class=\"keyword\">True</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> len(null)</span><br><span class=\"line\">print(train.apply(null_count))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222730409.JPG\" alt=\"mark\"><br></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数对年龄列进行分类</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">judge</span><span class=\"params\">(row)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pd.isnull(row[<span class=\"string\">'Age'</span>]) ==<span class=\"keyword\">True</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'unknown'</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">'youngth'</span> <span class=\"keyword\">if</span> row[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">18</span> <span class=\"keyword\">else</span> <span class=\"string\">'adult'</span></span><br><span class=\"line\">print(train.apply(judge,axis=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure><p></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222741327.JPG\" alt=\"mark\"></p><h1 id=\"数据透视表\"><a href=\"#数据透视表\" class=\"headerlink\" title=\"数据透视表\"></a>数据透视表</h1><ul><li>自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.pivot_table(index=[<span class=\"string\">\"Pclass\"</span>,<span class=\"string\">\"Sex\"</span>], values=[<span class=\"string\">\"Age\"</span>, <span class=\"string\">\"Fare\"</span>], aggfunc=np.mean))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222751776.JPG\" alt=\"mark\"></p><h1 id=\"数据合并\"><a href=\"#数据合并\" class=\"headerlink\" title=\"数据合并\"></a>数据合并</h1><ul><li><p>数据合并的一些操作，待补全</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data1 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\"> </span><br><span class=\"line\">data2=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"merge:\\n\"</span>,pd.merge(data1,data2),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level1'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\">data4 = pd.DataFrame(&#123;<span class=\"string\">'level2'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"merge with left_on,right_on: \\n\"</span>,pd.merge(data3,data4,left_on=<span class=\"string\">'level1'</span>,right_on=<span class=\"string\">'level2'</span>),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"concat: \\n\"</span>,pd.concat([data1,data2]),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,np.nan]&#125;)</span><br><span class=\"line\">data4=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,np.nan,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"combine: \\n\"</span>,data3.combine_first(data4),<span class=\"string\">\"\\n\"</span>)</span><br></pre></td></tr></table></figure><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/214643504.JPG\" alt=\"mark\"></p></li></ul>","site":{"data":{}},"excerpt":"<hr><p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。<br>数据在这：</p><h1 id=\"引入库\"><a href=\"#引入库\" class=\"headerlink\" title=\"引入库\"></a>引入库</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure><h1 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = pd.read_csv(<span class=\"string\">r\"文件目录\"</span>)</span><br></pre></td></tr></table></figure><p>此时数据的样式是：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG\" alt=\"\"></p>","more":"<h1 id=\"数据概览\"><a href=\"#数据概览\" class=\"headerlink\" title=\"数据概览\"></a>数据概览</h1><ul><li>describe 显示整体数据常见属性<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.describe())</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222443347.JPG\" alt=\"mark\"></p><ul><li>head tail 显示首尾一些数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.head(<span class=\"number\">5</span>))</span><br><span class=\"line\">print(train.tail(<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222504633.JPG\" alt=\"mark\"></p><ul><li>index：索引，默认自建整型索引；columns：列；values：数据数值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.index)</span><br><span class=\"line\">print(train.columns)</span><br><span class=\"line\">print(train.values)</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h1><ul><li>T：数据的转置<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.T)</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222544174.JPG\" alt=\"mark\"></p><ul><li>sort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.sort_index(axis=<span class=\"number\">0</span>,ascending=<span class=\"keyword\">True</span>))</span><br><span class=\"line\">print(train.sort_values(by=<span class=\"string\">\"Age\"</span>,ascending=<span class=\"keyword\">False</span>))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222551913.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222617681.JPG\" alt=\"mark\"></p><h1 id=\"数据选择\"><a href=\"#数据选择\" class=\"headerlink\" title=\"数据选择\"></a>数据选择</h1><ul><li>按照标签选择，选择列，行切片<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train[<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train[<span class=\"number\">0</span>:<span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222634103.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222640068.JPG\" alt=\"mark\"></p><ul><li><p>利用loc自由选择某些行某些列，可以用at替代</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.loc[train.index[<span class=\"number\">4</span>:<span class=\"number\">6</span>]])</span><br><span class=\"line\">print(train.loc[:,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train.at[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br></pre></td></tr></table></figure></li><li><p>利用iloc按照位置进行选择</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.iloc[<span class=\"number\">5</span>])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">2</span>:<span class=\"number\">4</span>])</span><br><span class=\"line\">print(train.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">5</span>]])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure></li><li><p>布尔选择</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print( train[ (train[<span class=\"string\">'Age'</span>]&gt;<span class=\"number\">40</span>) &amp; (train[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">50</span>) ] )</span><br><span class=\"line\">print(train[train[<span class=\"string\">'Parch'</span>].isin([<span class=\"number\">1</span>,<span class=\"number\">2</span>])])</span><br><span class=\"line\">print(train[pd.isnull(train[<span class=\"string\">'Age'</span>])==<span class=\"keyword\">True</span>])</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222658806.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222706679.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222715106.JPG\" alt=\"mark\"></p><h1 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h1><ul><li>利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train1=train.reindex(index=train.index[<span class=\"number\">0</span>:<span class=\"number\">5</span>],columns=[<span class=\"string\">'PassengerId'</span>]+[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>])<span class=\"comment\">#选择前5行，只取选定的三列</span></span><br><span class=\"line\">print(train1)</span><br><span class=\"line\">print(train1.dropna(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#删除存在nan值的行</span></span><br><span class=\"line\">print(train1.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>])) <span class=\"comment\">#删除年龄性别列中存在nan值的行</span></span><br><span class=\"line\">print(pd.isnull(train1)) <span class=\"comment\">#nan值改为true，其余值改为false</span></span><br><span class=\"line\">print(train1.fillna(value=<span class=\"number\">2333</span>)) <span class=\"comment\">#缺失值替换为2333</span></span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222723080.JPG\" alt=\"mark\"></p><h1 id=\"应用函数\"><a href=\"#应用函数\" class=\"headerlink\" title=\"应用函数\"></a>应用函数</h1><ul><li>可以自己写函数并应用到数据的行或者列，通过axis参数选择行列<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数统计包含nan值的行数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">null_count</span><span class=\"params\">(column)</span>:</span></span><br><span class=\"line\">    column_null=pd.isnull(column)</span><br><span class=\"line\">    null=column[column_null == <span class=\"keyword\">True</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> len(null)</span><br><span class=\"line\">print(train.apply(null_count))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222730409.JPG\" alt=\"mark\"><br></p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数对年龄列进行分类</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">judge</span><span class=\"params\">(row)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pd.isnull(row[<span class=\"string\">'Age'</span>]) ==<span class=\"keyword\">True</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'unknown'</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">'youngth'</span> <span class=\"keyword\">if</span> row[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">18</span> <span class=\"keyword\">else</span> <span class=\"string\">'adult'</span></span><br><span class=\"line\">print(train.apply(judge,axis=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure><p></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222741327.JPG\" alt=\"mark\"></p><h1 id=\"数据透视表\"><a href=\"#数据透视表\" class=\"headerlink\" title=\"数据透视表\"></a>数据透视表</h1><ul><li>自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.pivot_table(index=[<span class=\"string\">\"Pclass\"</span>,<span class=\"string\">\"Sex\"</span>], values=[<span class=\"string\">\"Age\"</span>, <span class=\"string\">\"Fare\"</span>], aggfunc=np.mean))</span><br></pre></td></tr></table></figure></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170204/222751776.JPG\" alt=\"mark\"></p><h1 id=\"数据合并\"><a href=\"#数据合并\" class=\"headerlink\" title=\"数据合并\"></a>数据合并</h1><ul><li><p>数据合并的一些操作，待补全</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data1 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\"> </span><br><span class=\"line\">data2=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"merge:\\n\"</span>,pd.merge(data1,data2),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level1'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\">data4 = pd.DataFrame(&#123;<span class=\"string\">'level2'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"merge with left_on,right_on: \\n\"</span>,pd.merge(data3,data4,left_on=<span class=\"string\">'level1'</span>,right_on=<span class=\"string\">'level2'</span>),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"concat: \\n\"</span>,pd.concat([data1,data2]),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,np.nan]&#125;)</span><br><span class=\"line\">data4=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,np.nan,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"combine: \\n\"</span>,data3.combine_first(data4),<span class=\"string\">\"\\n\"</span>)</span><br></pre></td></tr></table></figure><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170212/214643504.JPG\" alt=\"mark\"></p></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"pandas 数据处理基础","path":"2017/02/04/pandas-skill/","eyeCatchImage":null,"excerpt":"<hr><p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。<br>数据在这：</p><h1 id=\"引入库\"><a href=\"#引入库\" class=\"headerlink\" title=\"引入库\"></a>引入库</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure><h1 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = pd.read_csv(<span class=\"string\">r\"文件目录\"</span>)</span><br></pre></td></tr></table></figure><p>此时数据的样式是：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170205/103436937.JPG\" alt=\"\"></p>","date":"2017-02-04T13:02:39.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["math","machinelearning","code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Algorithm 题解目录","date":"2017-03-27T11:47:54.000Z","mathjax":true,"_content":"\n***\n算法刷题目录，方便自己查找回忆复习\n<!--more-->\n\n\n# Leetcode\n-\t[Python源码](https://github.com/thinkwee/Algorithm_Python)\n-\t[Leetcode题目列表](https://leetcode.com/list/wqzcris/)\n\n## 排序\n-\t75:给出一个只包含0，1，2的数列，排序形成[0,0,...,0,1,...,1,2,...,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。\n-\t164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。\n-\t179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序\n\n## 堆、栈、队列\n-\t23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中\n-\t150:算术表达式，栈，python的switch写法\n-\t224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1\n-\t373:在2对有序数组中找出有最小和的k组数对，优先队列\n\n## 组合数学\n-\t47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重\n\n## 搜索与查找\n-\t74:在一个有有特定顺序的矩阵中查找，两次二分查找\n-\t78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归\n-\t79:在一个字母矩阵中查找单词，深搜\n-\t89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称\n-\t140:将一个字符串按给定词典加空格成句子，记忆化搜索\n-\t153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找\n-\t154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个\n-\t240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法\n\n## 树\n-\t98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈\n-\t101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本\n-\t106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可\n-\t107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs\n-\t108:构造一棵二叉查找树，递归\n-\t114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点\n-\t144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点\n-\t515:求二叉树每一层的最大值，BFS\n\n## 图论\n-\t130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic\n\n## 数学题\n-\t4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] <= A[i] and A[i-1] <= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。\n-\t57：给出一组区间，插入一个新区间并合并。模拟\n-\t122:水，一行代码，找一个序列中的最大相邻元素差\n-\t142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点\n-\t166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复\n-\t172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数\n-\t202:快慢指针，求循环的一种方法\n-\t263:数学题\n-\t264:数学题\n-\t313:数学题\n\n## 字符串\n-\t131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码\n-\t242:水题，python字典的使用\n-\t301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况\n-\t451:按频率统计字母，哈希，python字典\n-\t541:部分字符串翻转，模拟，注意python切片的使用\n\n## 贪心\n-\t134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1\n-\t402:从一个大数中移除k个数字，使得新的数最小，栈，贪心\n-\t452:重叠区间问题，贪心\n\n## 动态规划\n-\t70:经典问题爬梯子，斐波那契数列，dp\n-\t96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left\\*right，即f(i,n)=ans(i-1)\\*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) \\* ans(n-1) + ans(1) \\* ans(n-2) + … + ans(n-1) \\* ans(0)，边界ans(0)=ans(1)=1。 \n-\t139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)\n-\t174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前\n-\t312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]\\*num[i]\\*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。\n-\t338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n) & x=2n \\\\\n\tf(n)+1 & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n-\t397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n)+1 & x=2n \\\\\n\tmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n\t奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时\n\t可以将方程进一步化简\n\tIf n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.\n\tIf n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.\n\t[证明在这里](https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof)\t\n-\t472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见[Python中实现字典树](http://thinkwee.top/2017/05/02/trie/#more)\n\t\n## 分治\n-\t247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法\n\n# Poj\n-\t[C++源码](https://github.com/thinkwee/Poj_Test)\n-\t1062:Dijkstra\n-\t1068:模拟\n-\t1094:拓扑排序\n-\t1328:贪心，换求解变量\n-\t1753:枚举，位运算\n-\t1789:Prim，优先队列\n-\t1860:bellman-ford\n-\t2109:贪心，高精度乘法\n-\t2965:枚举，位运算\n-\t3259:建模，bellman-ford\n-\t3295:模拟，栈\n\n# 校内赛\n-\t[2017pre](http://code.bupt.edu.cn/contest/650/)\n-\tA,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题\n-\tD,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断\n-\tF,题目骗人，不需要回溯，递推公式可以写出特征方程求解\n-\tG,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs\n-\tH,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1\n-\tI,高中物理，求重力加速度，方程不好解，二分法逼近答案\t\n\n# hiho\n-\thiho[^1](基本上是offer收割赛的题目)\n-\t1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i < j, p < q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 \n-\t1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1][j](1.0-a[i]);a[i]为第i次正面朝上的概率，注意对j=0进行特判。\n-\t1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。\n-\t1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。\n","source":"_posts/oj.md","raw":"---\ntitle: Algorithm 题解目录\ndate: 2017-03-27 19:47:54\ntags:\n-\tcode\n-\tpython\n-\tc++\n-\talgorithm\ncategories:\n-\t算法\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg\nmathjax: true\n---\n\n***\n算法刷题目录，方便自己查找回忆复习\n<!--more-->\n\n\n# Leetcode\n-\t[Python源码](https://github.com/thinkwee/Algorithm_Python)\n-\t[Leetcode题目列表](https://leetcode.com/list/wqzcris/)\n\n## 排序\n-\t75:给出一个只包含0，1，2的数列，排序形成[0,0,...,0,1,...,1,2,...,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。\n-\t164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。\n-\t179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序\n\n## 堆、栈、队列\n-\t23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中\n-\t150:算术表达式，栈，python的switch写法\n-\t224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1\n-\t373:在2对有序数组中找出有最小和的k组数对，优先队列\n\n## 组合数学\n-\t47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重\n\n## 搜索与查找\n-\t74:在一个有有特定顺序的矩阵中查找，两次二分查找\n-\t78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归\n-\t79:在一个字母矩阵中查找单词，深搜\n-\t89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称\n-\t140:将一个字符串按给定词典加空格成句子，记忆化搜索\n-\t153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找\n-\t154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个\n-\t240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法\n\n## 树\n-\t98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈\n-\t101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本\n-\t106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可\n-\t107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs\n-\t108:构造一棵二叉查找树，递归\n-\t114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点\n-\t144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点\n-\t515:求二叉树每一层的最大值，BFS\n\n## 图论\n-\t130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic\n\n## 数学题\n-\t4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] <= A[i] and A[i-1] <= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。\n-\t57：给出一组区间，插入一个新区间并合并。模拟\n-\t122:水，一行代码，找一个序列中的最大相邻元素差\n-\t142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点\n-\t166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复\n-\t172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数\n-\t202:快慢指针，求循环的一种方法\n-\t263:数学题\n-\t264:数学题\n-\t313:数学题\n\n## 字符串\n-\t131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码\n-\t242:水题，python字典的使用\n-\t301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况\n-\t451:按频率统计字母，哈希，python字典\n-\t541:部分字符串翻转，模拟，注意python切片的使用\n\n## 贪心\n-\t134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1\n-\t402:从一个大数中移除k个数字，使得新的数最小，栈，贪心\n-\t452:重叠区间问题，贪心\n\n## 动态规划\n-\t70:经典问题爬梯子，斐波那契数列，dp\n-\t96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left\\*right，即f(i,n)=ans(i-1)\\*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) \\* ans(n-1) + ans(1) \\* ans(n-2) + … + ans(n-1) \\* ans(0)，边界ans(0)=ans(1)=1。 \n-\t139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)\n-\t174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前\n-\t312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]\\*num[i]\\*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。\n-\t338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n) & x=2n \\\\\n\tf(n)+1 & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n-\t397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n)+1 & x=2n \\\\\n\tmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n\t奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时\n\t可以将方程进一步化简\n\tIf n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.\n\tIf n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.\n\t[证明在这里](https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof)\t\n-\t472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见[Python中实现字典树](http://thinkwee.top/2017/05/02/trie/#more)\n\t\n## 分治\n-\t247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法\n\n# Poj\n-\t[C++源码](https://github.com/thinkwee/Poj_Test)\n-\t1062:Dijkstra\n-\t1068:模拟\n-\t1094:拓扑排序\n-\t1328:贪心，换求解变量\n-\t1753:枚举，位运算\n-\t1789:Prim，优先队列\n-\t1860:bellman-ford\n-\t2109:贪心，高精度乘法\n-\t2965:枚举，位运算\n-\t3259:建模，bellman-ford\n-\t3295:模拟，栈\n\n# 校内赛\n-\t[2017pre](http://code.bupt.edu.cn/contest/650/)\n-\tA,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题\n-\tD,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断\n-\tF,题目骗人，不需要回溯，递推公式可以写出特征方程求解\n-\tG,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs\n-\tH,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1\n-\tI,高中物理，求重力加速度，方程不好解，二分法逼近答案\t\n\n# hiho\n-\thiho[^1](基本上是offer收割赛的题目)\n-\t1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i < j, p < q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 \n-\t1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1][j](1.0-a[i]);a[i]为第i次正面朝上的概率，注意对j=0进行特判。\n-\t1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。\n-\t1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。\n","slug":"oj","published":1,"updated":"2018-07-23T01:28:38.502Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg"],"comments":1,"layout":"post","link":"","_id":"cjmd072dd002bqcw60d7j3mpd","content":"<hr><p>算法刷题目录，方便自己查找回忆复习<br><a id=\"more\"></a></p><h1 id=\"Leetcode\"><a href=\"#Leetcode\" class=\"headerlink\" title=\"Leetcode\"></a>Leetcode</h1><ul><li><a href=\"https://github.com/thinkwee/Algorithm_Python\" target=\"_blank\" rel=\"noopener\">Python源码</a></li><li><a href=\"https://leetcode.com/list/wqzcris/\" target=\"_blank\" rel=\"noopener\">Leetcode题目列表</a></li></ul><h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><ul><li>75:给出一个只包含0，1，2的数列，排序形成[0,0,…,0,1,…,1,2,…,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。</li><li>164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。</li><li>179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序</li></ul><h2 id=\"堆、栈、队列\"><a href=\"#堆、栈、队列\" class=\"headerlink\" title=\"堆、栈、队列\"></a>堆、栈、队列</h2><ul><li>23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中</li><li>150:算术表达式，栈，python的switch写法</li><li>224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1</li><li>373:在2对有序数组中找出有最小和的k组数对，优先队列</li></ul><h2 id=\"组合数学\"><a href=\"#组合数学\" class=\"headerlink\" title=\"组合数学\"></a>组合数学</h2><ul><li>47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重</li></ul><h2 id=\"搜索与查找\"><a href=\"#搜索与查找\" class=\"headerlink\" title=\"搜索与查找\"></a>搜索与查找</h2><ul><li>74:在一个有有特定顺序的矩阵中查找，两次二分查找</li><li>78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归</li><li>79:在一个字母矩阵中查找单词，深搜</li><li>89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称</li><li>140:将一个字符串按给定词典加空格成句子，记忆化搜索</li><li>153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找</li><li>154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个</li><li>240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法</li></ul><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><ul><li>98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈</li><li>101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本</li><li>106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可</li><li>107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs</li><li>108:构造一棵二叉查找树，递归</li><li>114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点</li><li>144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点</li><li>515:求二叉树每一层的最大值，BFS</li></ul><h2 id=\"图论\"><a href=\"#图论\" class=\"headerlink\" title=\"图论\"></a>图论</h2><ul><li>130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic</li></ul><h2 id=\"数学题\"><a href=\"#数学题\" class=\"headerlink\" title=\"数学题\"></a>数学题</h2><ul><li>4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。</li><li>57：给出一组区间，插入一个新区间并合并。模拟</li><li>122:水，一行代码，找一个序列中的最大相邻元素差</li><li>142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点</li><li>166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复</li><li>172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数</li><li>202:快慢指针，求循环的一种方法</li><li>263:数学题</li><li>264:数学题</li><li>313:数学题</li></ul><h2 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h2><ul><li>131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码</li><li>242:水题，python字典的使用</li><li>301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况</li><li>451:按频率统计字母，哈希，python字典</li><li>541:部分字符串翻转，模拟，注意python切片的使用</li></ul><h2 id=\"贪心\"><a href=\"#贪心\" class=\"headerlink\" title=\"贪心\"></a>贪心</h2><ul><li>134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1</li><li>402:从一个大数中移除k个数字，使得新的数最小，栈，贪心</li><li>452:重叠区间问题，贪心</li></ul><h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><ul><li>70:经典问题爬梯子，斐波那契数列，dp</li><li>96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left*right，即f(i,n)=ans(i-1)*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) * ans(n-1) + ans(1) * ans(n-2) + … + ans(n-1) * ans(0)，边界ans(0)=ans(1)=1。</li><li>139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)</li><li>174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前</li><li>312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]*num[i]*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。</li><li>338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程<br>$$<br>f(x)=<br>\\begin{cases}<br>f(n) &amp; x=2n \\\\<br>f(n)+1 &amp; x=2n+1 \\\\<br>\\end{cases}<br>$$</li><li>397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:<br>$$<br>f(x)=<br>\\begin{cases}<br>f(n)+1 &amp; x=2n \\\\<br>min(f(2n+2)+1,f(2n)+1) &amp; x=2n+1 \\\\<br>\\end{cases}<br>$$<br>奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时<br>可以将方程进一步化简<br>If n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.<br>If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.<br><a href=\"https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof\" target=\"_blank\" rel=\"noopener\">证明在这里</a></li><li>472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见<a href=\"http://thinkwee.top/2017/05/02/trie/#more\">Python中实现字典树</a></li></ul><h2 id=\"分治\"><a href=\"#分治\" class=\"headerlink\" title=\"分治\"></a>分治</h2><ul><li>247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法</li></ul><h1 id=\"Poj\"><a href=\"#Poj\" class=\"headerlink\" title=\"Poj\"></a>Poj</h1><ul><li><a href=\"https://github.com/thinkwee/Poj_Test\" target=\"_blank\" rel=\"noopener\">C++源码</a></li><li>1062:Dijkstra</li><li>1068:模拟</li><li>1094:拓扑排序</li><li>1328:贪心，换求解变量</li><li>1753:枚举，位运算</li><li>1789:Prim，优先队列</li><li>1860:bellman-ford</li><li>2109:贪心，高精度乘法</li><li>2965:枚举，位运算</li><li>3259:建模，bellman-ford</li><li>3295:模拟，栈</li></ul><h1 id=\"校内赛\"><a href=\"#校内赛\" class=\"headerlink\" title=\"校内赛\"></a>校内赛</h1><ul><li><a href=\"http://code.bupt.edu.cn/contest/650/\" target=\"_blank\" rel=\"noopener\">2017pre</a></li><li>A,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题</li><li>D,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断</li><li>F,题目骗人，不需要回溯，递推公式可以写出特征方程求解</li><li>G,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs</li><li>H,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1</li><li>I,高中物理，求重力加速度，方程不好解，二分法逼近答案</li></ul><h1 id=\"hiho\"><a href=\"#hiho\" class=\"headerlink\" title=\"hiho\"></a>hiho</h1><ul><li>hiho<a href=\"基本上是offer收割赛的题目\">^1</a></li><li>1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i &lt; j, p &lt; q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。</li><li>1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1]<a href=\"1.0-a[i]\">j</a>;a[i]为第i次正面朝上的概率，注意对j=0进行特判。</li><li>1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。</li><li>1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。</li></ul>","site":{"data":{}},"excerpt":"<hr><p>算法刷题目录，方便自己查找回忆复习<br>","more":"</p><h1 id=\"Leetcode\"><a href=\"#Leetcode\" class=\"headerlink\" title=\"Leetcode\"></a>Leetcode</h1><ul><li><a href=\"https://github.com/thinkwee/Algorithm_Python\" target=\"_blank\" rel=\"noopener\">Python源码</a></li><li><a href=\"https://leetcode.com/list/wqzcris/\" target=\"_blank\" rel=\"noopener\">Leetcode题目列表</a></li></ul><h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><ul><li>75:给出一个只包含0，1，2的数列，排序形成[0,0,…,0,1,…,1,2,…,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。</li><li>164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。</li><li>179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序</li></ul><h2 id=\"堆、栈、队列\"><a href=\"#堆、栈、队列\" class=\"headerlink\" title=\"堆、栈、队列\"></a>堆、栈、队列</h2><ul><li>23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中</li><li>150:算术表达式，栈，python的switch写法</li><li>224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1</li><li>373:在2对有序数组中找出有最小和的k组数对，优先队列</li></ul><h2 id=\"组合数学\"><a href=\"#组合数学\" class=\"headerlink\" title=\"组合数学\"></a>组合数学</h2><ul><li>47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重</li></ul><h2 id=\"搜索与查找\"><a href=\"#搜索与查找\" class=\"headerlink\" title=\"搜索与查找\"></a>搜索与查找</h2><ul><li>74:在一个有有特定顺序的矩阵中查找，两次二分查找</li><li>78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归</li><li>79:在一个字母矩阵中查找单词，深搜</li><li>89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称</li><li>140:将一个字符串按给定词典加空格成句子，记忆化搜索</li><li>153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找</li><li>154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个</li><li>240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法</li></ul><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><ul><li>98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈</li><li>101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本</li><li>106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可</li><li>107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs</li><li>108:构造一棵二叉查找树，递归</li><li>114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点</li><li>144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点</li><li>515:求二叉树每一层的最大值，BFS</li></ul><h2 id=\"图论\"><a href=\"#图论\" class=\"headerlink\" title=\"图论\"></a>图论</h2><ul><li>130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic</li></ul><h2 id=\"数学题\"><a href=\"#数学题\" class=\"headerlink\" title=\"数学题\"></a>数学题</h2><ul><li>4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。</li><li>57：给出一组区间，插入一个新区间并合并。模拟</li><li>122:水，一行代码，找一个序列中的最大相邻元素差</li><li>142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点</li><li>166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复</li><li>172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数</li><li>202:快慢指针，求循环的一种方法</li><li>263:数学题</li><li>264:数学题</li><li>313:数学题</li></ul><h2 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h2><ul><li>131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码</li><li>242:水题，python字典的使用</li><li>301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况</li><li>451:按频率统计字母，哈希，python字典</li><li>541:部分字符串翻转，模拟，注意python切片的使用</li></ul><h2 id=\"贪心\"><a href=\"#贪心\" class=\"headerlink\" title=\"贪心\"></a>贪心</h2><ul><li>134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1</li><li>402:从一个大数中移除k个数字，使得新的数最小，栈，贪心</li><li>452:重叠区间问题，贪心</li></ul><h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><ul><li>70:经典问题爬梯子，斐波那契数列，dp</li><li>96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left*right，即f(i,n)=ans(i-1)*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) * ans(n-1) + ans(1) * ans(n-2) + … + ans(n-1) * ans(0)，边界ans(0)=ans(1)=1。</li><li>139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)</li><li>174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前</li><li>312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]*num[i]*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。</li><li>338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程<br>$$<br>f(x)=<br>\\begin{cases}<br>f(n) &amp; x=2n \\\\<br>f(n)+1 &amp; x=2n+1 \\\\<br>\\end{cases}<br>$$</li><li>397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:<br>$$<br>f(x)=<br>\\begin{cases}<br>f(n)+1 &amp; x=2n \\\\<br>min(f(2n+2)+1,f(2n)+1) &amp; x=2n+1 \\\\<br>\\end{cases}<br>$$<br>奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时<br>可以将方程进一步化简<br>If n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.<br>If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.<br><a href=\"https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof\" target=\"_blank\" rel=\"noopener\">证明在这里</a></li><li>472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见<a href=\"http://thinkwee.top/2017/05/02/trie/#more\">Python中实现字典树</a></li></ul><h2 id=\"分治\"><a href=\"#分治\" class=\"headerlink\" title=\"分治\"></a>分治</h2><ul><li>247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法</li></ul><h1 id=\"Poj\"><a href=\"#Poj\" class=\"headerlink\" title=\"Poj\"></a>Poj</h1><ul><li><a href=\"https://github.com/thinkwee/Poj_Test\" target=\"_blank\" rel=\"noopener\">C++源码</a></li><li>1062:Dijkstra</li><li>1068:模拟</li><li>1094:拓扑排序</li><li>1328:贪心，换求解变量</li><li>1753:枚举，位运算</li><li>1789:Prim，优先队列</li><li>1860:bellman-ford</li><li>2109:贪心，高精度乘法</li><li>2965:枚举，位运算</li><li>3259:建模，bellman-ford</li><li>3295:模拟，栈</li></ul><h1 id=\"校内赛\"><a href=\"#校内赛\" class=\"headerlink\" title=\"校内赛\"></a>校内赛</h1><ul><li><a href=\"http://code.bupt.edu.cn/contest/650/\" target=\"_blank\" rel=\"noopener\">2017pre</a></li><li>A,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题</li><li>D,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断</li><li>F,题目骗人，不需要回溯，递推公式可以写出特征方程求解</li><li>G,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs</li><li>H,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1</li><li>I,高中物理，求重力加速度，方程不好解，二分法逼近答案</li></ul><h1 id=\"hiho\"><a href=\"#hiho\" class=\"headerlink\" title=\"hiho\"></a>hiho</h1><ul><li>hiho<a href=\"基本上是offer收割赛的题目\">^1</a></li><li>1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i &lt; j, p &lt; q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。</li><li>1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1]<a href=\"1.0-a[i]\">j</a>;a[i]为第i次正面朝上的概率，注意对j=0进行特判。</li><li>1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。</li><li>1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Algorithm 题解目录","path":"2017/03/27/oj/","eyeCatchImage":null,"excerpt":"<hr><p>算法刷题目录，方便自己查找回忆复习<br>","date":"2017-03-27T11:47:54.000Z","pv":0,"totalPV":0,"categories":"算法","tags":["code","python","c++","algorithm"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"搭站总结","author":"Thinkwee","date":"2017-01-16T04:01:00.000Z","_content":"***\n\n一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器\n后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管\n而且本来官方说明也推荐用这个写博客，于是就开始试试\n大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站\nHexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository\n为了备份，我们将在repository中建立两个branch\n一个master用于让hexo上传静态网页文件\n一个hexo用于保存本地hexo项目\n下面分享一些经验和踩到的坑\n-\t\t2017.2.8更新md写作软件\n-\t\t2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)\n-\t\t2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)\n-\t\t2017.3.30更新置顶说明原文地址\n-\t\t2017.12.27更新异地恢复\n-\t\t2018.7.6更新一个比较全面的参考网址\n<!--more-->\n\n# 前提\n-\t安装好Node.js\n-\t安装好git\n-\t安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。\n-\t可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)\n\n# GitHub&Hexo初始化\n选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些\n最后的过程是这样的\n-\t注册github账号，这个就省略了\n-\t新建一个repository，命名必须为“你的账户名.github.io\"\n-\t对这个repository新建一个hexo分支并把hexo设置为默认分支\n-\tgit bash cd到你本地新建的一个文件夹（用于存放博客项目）\n-\t依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git\n-\t此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文\n-\t此时你的网址就是：https://你的账户名.github.io/\n-\t现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件\n\n\n\n**之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支**\n\n# Hexo配置\nHexo博客的目录格式如下\n\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n\n-\tconfig.yml在这里设置博客的总体参数，博客名，url，git什么的\n-\t_posts里面存放你的文章\n-\ttheme顾名思义，存放博客界面主题\n-\t在config.yml中关键配置以下几个参数\n *\ttitle: 博客网站标题\n *\tsubtitle: \t副标题\n *\tdescription: \t一句话简介\n *\tauthor: \t作者名\n *\tlanguage: zh-Hans\t语言根据你选择的主题看，在主题目录的language里查看支持哪些语言\n *\ttimezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区\n *\turl: https://你的用户名.github.io/\n *\ttype: git\n *\trepo: https://github.com/你的用户名/你的用户名.github.io.git\n *\t**branch: master**\n\n# 更新Hexo项目\n依次执行\n-\tgit add .  (检查所有文件是否更新)\n-\tgit commit -m \"更新报告\" (提交commit)\n-\tgit push origin hexo (上传更新到github)\n**第一次更新可能出现一些错误**\n-\t第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错\n-\tpermission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：https://www.github.com/你的用户名/你的用户名.github.io.git\n-\trefusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo --allow-unrelated-histories，之后再ush\n**所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的**\n\n\n# Hexo 写博客及更新网页\n主要命令如下\n-\thexo clean 清除缓存和静态文件\n-\thexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate\n-\thexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭\n-\thexo new\"文件名\" 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了\n-\thexo d或者hexo deploy，写好博客，clean&generate之后，部署博客到GitHub Pages上，就更新了博客网页\n**其中有几点要注意**\n-\t第一次新建目录后执行#npm install hexo-deployer-git --save安装git分发，否则上传不了\n-\thexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格\n-\t我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s\n\n# Hexo写作的几种选择(Windows)\n-\t**马克飞象**:这里推荐用马克飞象的chrome app版本，可以离线打开。**优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年**,它长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192945202.jpg)\n-\t**hexo admin插件**:安装hexo-admin插件，可视化管理博客，安装方法：\n```Github\nnpm install --save hexo-admin\nhexo server -d\nopen http://localhost:4000/admin/\n```\n*http://localhost:4000/admin/*就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，**优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好**,它长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192926032.jpg)\n-\t推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。\n-\t**Typora**:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：**Typora**。**优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。**他长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170208/204701798.jpg)\n-\tMac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。\n-\t常备一个notepad++,出现什么问题没有notepad++解决不了的。\n\n\n\n# 主题设置\n-\t一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可\n-\t每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改\n\n# 写作语法\n-\t语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明\n\n# 插入图片\n-\t插入图片分本地和在线链接插入，推荐使用在线链接\n-\t我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。\n\n# 插入公式\n-\t用一对$$将公式围起来，语法支持latex\n-\t必须在文章首部中声明如mathjax: true\n-\t因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法\n-\t建议更换cdn以加速:\n```\nmathjax:\n  enable: true\n  per_page: true\n  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n  ```\n\n# 标签与分类\n-\t标签与分类的区别在于1篇文章归于一类，但可能有多个标签\n-\t比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签\n-\t分类需在首部加上categories: 分类名\n-\t标签需在首部加上tags: 标签名或者tags: [标签1,标签2....],**注意别打成中文逗号**\t\n\n# 其他美化\n-\t我用的是next主题，可以设置背景之类的，百度一下方法就能找到\n-\t更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network\n\n# 首页文章数量设置\n- 安装插件\n```Git\n\tnpm install --save hexo-generator-index\n\tnpm install --save hexo-generator-archive\n\tnpm install --save hexo-generator-tag\n```\n-\t在站点配置文件config.yml中添加如下字段\n```Markdown\n\tindex_generator:\n\tper_page: 5\n\n\tarchive_generator:\n\tper_page: 20\n\tyearly: true\n\tmonthly: true\n\n\ttag_generator:\n\tper_page: 10\n```\n-\tindex, archive及tag开头分表代表主页，归档页面和标签页面。\n\n# 添加评论功能\n-\t~~视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可~~\n-\t现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论\n\n# 优化插件\n-\thexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率\n\t[hexo-all-minifier](https://github.com/chenzhutian/hexo-all-minifier)\n\n# 置顶\n-\t感谢Netcan_Space提供解决方案，希望官方theme加入此功能：[添加Hexo置顶功能](http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/)\n\n# cnpm\n-\t使用淘宝镜像安装插件提速，详情百度cnpm安装\n\n# RSS\n-\t使用hexo-generator-feed使用rss:[hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed)\n\n# 完成之后的目录示意图\n\nhexo分支:\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113838583.JPG)\n\nmaster分支\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113915677.JPG)\n\n# 异地恢复\n-\t最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下\n *\tclone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml\n *\t这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除\n *\t记得安装hexo-deployer-git\n *\tnext更新了，但是新功能的一些依赖还是需要看注释，自己npm install\n *\t发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作\n *\t上面那2张图已经过时了，可以去我的github里看博客的文件目录\n *\t有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱\n\n# 这位大佬靠谱\n-\t网址在这：[HEXO建站备忘录](https://www.vincentqin.tech/posts/build-a-website-using-hexo/)\n-\t里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍[hexo-git-backup](https://github.com/coneycode/hexo-git-backup)\n\n","source":"_posts/setupmywebsite.md","raw":"title: 搭站总结\ntags:\n  - web\n  - hexo\n  - github\ncategories:\n  - 瞎折腾\nauthor: Thinkwee\ndate: 2017-01-16 12:01:00\nphoto: http://ojtdnrpmt.bkt.clouddn.com/17-1-16/64914668-file_1484548577493_a7af.png\n---\n***\n\n一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器\n后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管\n而且本来官方说明也推荐用这个写博客，于是就开始试试\n大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站\nHexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository\n为了备份，我们将在repository中建立两个branch\n一个master用于让hexo上传静态网页文件\n一个hexo用于保存本地hexo项目\n下面分享一些经验和踩到的坑\n-\t\t2017.2.8更新md写作软件\n-\t\t2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)\n-\t\t2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)\n-\t\t2017.3.30更新置顶说明原文地址\n-\t\t2017.12.27更新异地恢复\n-\t\t2018.7.6更新一个比较全面的参考网址\n<!--more-->\n\n# 前提\n-\t安装好Node.js\n-\t安装好git\n-\t安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。\n-\t可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)\n\n# GitHub&Hexo初始化\n选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些\n最后的过程是这样的\n-\t注册github账号，这个就省略了\n-\t新建一个repository，命名必须为“你的账户名.github.io\"\n-\t对这个repository新建一个hexo分支并把hexo设置为默认分支\n-\tgit bash cd到你本地新建的一个文件夹（用于存放博客项目）\n-\t依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git\n-\t此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文\n-\t此时你的网址就是：https://你的账户名.github.io/\n-\t现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件\n\n\n\n**之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支**\n\n# Hexo配置\nHexo博客的目录格式如下\n\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n\n-\tconfig.yml在这里设置博客的总体参数，博客名，url，git什么的\n-\t_posts里面存放你的文章\n-\ttheme顾名思义，存放博客界面主题\n-\t在config.yml中关键配置以下几个参数\n *\ttitle: 博客网站标题\n *\tsubtitle: \t副标题\n *\tdescription: \t一句话简介\n *\tauthor: \t作者名\n *\tlanguage: zh-Hans\t语言根据你选择的主题看，在主题目录的language里查看支持哪些语言\n *\ttimezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区\n *\turl: https://你的用户名.github.io/\n *\ttype: git\n *\trepo: https://github.com/你的用户名/你的用户名.github.io.git\n *\t**branch: master**\n\n# 更新Hexo项目\n依次执行\n-\tgit add .  (检查所有文件是否更新)\n-\tgit commit -m \"更新报告\" (提交commit)\n-\tgit push origin hexo (上传更新到github)\n**第一次更新可能出现一些错误**\n-\t第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错\n-\tpermission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：https://www.github.com/你的用户名/你的用户名.github.io.git\n-\trefusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo --allow-unrelated-histories，之后再ush\n**所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的**\n\n\n# Hexo 写博客及更新网页\n主要命令如下\n-\thexo clean 清除缓存和静态文件\n-\thexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate\n-\thexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭\n-\thexo new\"文件名\" 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了\n-\thexo d或者hexo deploy，写好博客，clean&generate之后，部署博客到GitHub Pages上，就更新了博客网页\n**其中有几点要注意**\n-\t第一次新建目录后执行#npm install hexo-deployer-git --save安装git分发，否则上传不了\n-\thexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格\n-\t我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s\n\n# Hexo写作的几种选择(Windows)\n-\t**马克飞象**:这里推荐用马克飞象的chrome app版本，可以离线打开。**优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年**,它长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192945202.jpg)\n-\t**hexo admin插件**:安装hexo-admin插件，可视化管理博客，安装方法：\n```Github\nnpm install --save hexo-admin\nhexo server -d\nopen http://localhost:4000/admin/\n```\n*http://localhost:4000/admin/*就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，**优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好**,它长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192926032.jpg)\n-\t推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。\n-\t**Typora**:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：**Typora**。**优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。**他长这个样子：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170208/204701798.jpg)\n-\tMac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。\n-\t常备一个notepad++,出现什么问题没有notepad++解决不了的。\n\n\n\n# 主题设置\n-\t一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可\n-\t每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改\n\n# 写作语法\n-\t语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明\n\n# 插入图片\n-\t插入图片分本地和在线链接插入，推荐使用在线链接\n-\t我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。\n\n# 插入公式\n-\t用一对$$将公式围起来，语法支持latex\n-\t必须在文章首部中声明如mathjax: true\n-\t因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法\n-\t建议更换cdn以加速:\n```\nmathjax:\n  enable: true\n  per_page: true\n  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n  ```\n\n# 标签与分类\n-\t标签与分类的区别在于1篇文章归于一类，但可能有多个标签\n-\t比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签\n-\t分类需在首部加上categories: 分类名\n-\t标签需在首部加上tags: 标签名或者tags: [标签1,标签2....],**注意别打成中文逗号**\t\n\n# 其他美化\n-\t我用的是next主题，可以设置背景之类的，百度一下方法就能找到\n-\t更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network\n\n# 首页文章数量设置\n- 安装插件\n```Git\n\tnpm install --save hexo-generator-index\n\tnpm install --save hexo-generator-archive\n\tnpm install --save hexo-generator-tag\n```\n-\t在站点配置文件config.yml中添加如下字段\n```Markdown\n\tindex_generator:\n\tper_page: 5\n\n\tarchive_generator:\n\tper_page: 20\n\tyearly: true\n\tmonthly: true\n\n\ttag_generator:\n\tper_page: 10\n```\n-\tindex, archive及tag开头分表代表主页，归档页面和标签页面。\n\n# 添加评论功能\n-\t~~视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可~~\n-\t现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论\n\n# 优化插件\n-\thexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率\n\t[hexo-all-minifier](https://github.com/chenzhutian/hexo-all-minifier)\n\n# 置顶\n-\t感谢Netcan_Space提供解决方案，希望官方theme加入此功能：[添加Hexo置顶功能](http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/)\n\n# cnpm\n-\t使用淘宝镜像安装插件提速，详情百度cnpm安装\n\n# RSS\n-\t使用hexo-generator-feed使用rss:[hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed)\n\n# 完成之后的目录示意图\n\nhexo分支:\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113838583.JPG)\n\nmaster分支\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113915677.JPG)\n\n# 异地恢复\n-\t最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下\n *\tclone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml\n *\t这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除\n *\t记得安装hexo-deployer-git\n *\tnext更新了，但是新功能的一些依赖还是需要看注释，自己npm install\n *\t发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作\n *\t上面那2张图已经过时了，可以去我的github里看博客的文件目录\n *\t有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱\n\n# 这位大佬靠谱\n-\t网址在这：[HEXO建站备忘录](https://www.vincentqin.tech/posts/build-a-website-using-hexo/)\n-\t里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍[hexo-git-backup](https://github.com/coneycode/hexo-git-backup)\n\n","slug":"setupmywebsite","published":1,"updated":"2018-07-23T01:28:38.518Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/17-1-16/64914668-file_1484548577493_a7af.png"],"comments":1,"layout":"post","link":"","_id":"cjmd072de002eqcw6hub4nmxr","content":"<hr><p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p><ul><li>2017.2.8更新md写作软件</li><li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li><li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li><li>2017.3.30更新置顶说明原文地址</li><li>2017.12.27更新异地恢复</li><li>2018.7.6更新一个比较全面的参考网址<a id=\"more\"></a></li></ul><h1 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h1><ul><li>安装好Node.js</li><li>安装好git</li><li>安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。</li><li>可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)</li></ul><h1 id=\"GitHub-amp-Hexo初始化\"><a href=\"#GitHub-amp-Hexo初始化\" class=\"headerlink\" title=\"GitHub&amp;Hexo初始化\"></a>GitHub&amp;Hexo初始化</h1><p>选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些<br>最后的过程是这样的</p><ul><li>注册github账号，这个就省略了</li><li>新建一个repository，命名必须为“你的账户名.github.io”</li><li>对这个repository新建一个hexo分支并把hexo设置为默认分支</li><li>git bash cd到你本地新建的一个文件夹（用于存放博客项目）</li><li>依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git</li><li>此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到 _config.yml，修改deploy参数为master, 见后文</li><li>此时你的网址就是：https://你的账户名.github.io/</li><li>现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件</li></ul><p><strong>之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支</strong></p><h1 id=\"Hexo配置\"><a href=\"#Hexo配置\" class=\"headerlink\" title=\"Hexo配置\"></a>Hexo配置</h1><p>Hexo博客的目录格式如下</p><p>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br>| ├── _drafts<br>| └── _posts<br>└── themes</p><ul><li>config.yml在这里设置博客的总体参数，博客名，url，git什么的</li><li>_posts里面存放你的文章</li><li>theme顾名思义，存放博客界面主题</li><li>在config.yml中关键配置以下几个参数<ul><li>title: 博客网站标题</li><li>subtitle: 副标题</li><li>description: 一句话简介</li><li>author: 作者名</li><li>language: zh-Hans 语言根据你选择的主题看，在主题目录的language里查看支持哪些语言</li><li>timezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区</li><li>url: https://你的用户名.github.io/</li><li>type: git</li><li>repo: <a href=\"https://github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://github.com/你的用户名/你的用户名.github.io.git</a></li><li><strong>branch: master</strong></li></ul></li></ul><h1 id=\"更新Hexo项目\"><a href=\"#更新Hexo项目\" class=\"headerlink\" title=\"更新Hexo项目\"></a>更新Hexo项目</h1><p>依次执行</p><ul><li>git add . (检查所有文件是否更新)</li><li>git commit -m “更新报告” (提交commit)</li><li>git push origin hexo (上传更新到github)<br><strong>第一次更新可能出现一些错误</strong></li><li>第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错</li><li>permission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：<a href=\"https://www.github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://www.github.com/你的用户名/你的用户名.github.io.git</a></li><li>refusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo –allow-unrelated-histories，之后再ush<br><strong>所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的</strong></li></ul><h1 id=\"Hexo-写博客及更新网页\"><a href=\"#Hexo-写博客及更新网页\" class=\"headerlink\" title=\"Hexo 写博客及更新网页\"></a>Hexo 写博客及更新网页</h1><p>主要命令如下</p><ul><li>hexo clean 清除缓存和静态文件</li><li>hexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate</li><li>hexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭</li><li>hexo new”文件名” 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了</li><li>hexo d或者hexo deploy，写好博客，clean&amp;generate之后，部署博客到GitHub Pages上，就更新了博客网页<br><strong>其中有几点要注意</strong></li><li>第一次新建目录后执行#npm install hexo-deployer-git –save安装git分发，否则上传不了</li><li>hexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格</li><li>我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s</li></ul><h1 id=\"Hexo写作的几种选择-Windows\"><a href=\"#Hexo写作的几种选择-Windows\" class=\"headerlink\" title=\"Hexo写作的几种选择(Windows)\"></a>Hexo写作的几种选择(Windows)</h1><ul><li><strong>马克飞象</strong>:这里推荐用马克飞象的chrome app版本，可以离线打开。<strong>优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年</strong>,它长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192945202.jpg\" alt=\"mark\"></li><li><strong>hexo admin插件</strong>:安装hexo-admin插件，可视化管理博客，安装方法：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-admin</span><br><span class=\"line\">hexo server -d</span><br><span class=\"line\">open http://localhost:4000/admin/</span><br></pre></td></tr></table></figure></li></ul><p><em><a href=\"http://localhost:4000/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/admin/</a></em>就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，<strong>优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好</strong>,它长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192926032.jpg\" alt=\"mark\"></p><ul><li>推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。</li><li><strong>Typora</strong>:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：<strong>Typora</strong>。<strong>优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。</strong>他长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170208/204701798.jpg\" alt=\"mark\"></li><li>Mac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。</li><li>常备一个notepad++,出现什么问题没有notepad++解决不了的。</li></ul><h1 id=\"主题设置\"><a href=\"#主题设置\" class=\"headerlink\" title=\"主题设置\"></a>主题设置</h1><ul><li>一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可</li><li>每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改</li></ul><h1 id=\"写作语法\"><a href=\"#写作语法\" class=\"headerlink\" title=\"写作语法\"></a>写作语法</h1><ul><li>语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明</li></ul><h1 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h1><ul><li>插入图片分本地和在线链接插入，推荐使用在线链接</li><li>我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。</li></ul><h1 id=\"插入公式\"><a href=\"#插入公式\" class=\"headerlink\" title=\"插入公式\"></a>插入公式</h1><ul><li>用一对$$将公式围起来，语法支持latex</li><li>必须在文章首部中声明如mathjax: true</li><li>因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法</li><li>建议更换cdn以加速:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mathjax:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  per_page: true</span><br><span class=\"line\">  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"标签与分类\"><a href=\"#标签与分类\" class=\"headerlink\" title=\"标签与分类\"></a>标签与分类</h1><ul><li>标签与分类的区别在于1篇文章归于一类，但可能有多个标签</li><li>比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签</li><li>分类需在首部加上categories: 分类名</li><li>标签需在首部加上tags: 标签名或者tags: [标签1,标签2….],<strong>注意别打成中文逗号</strong></li></ul><h1 id=\"其他美化\"><a href=\"#其他美化\" class=\"headerlink\" title=\"其他美化\"></a>其他美化</h1><ul><li>我用的是next主题，可以设置背景之类的，百度一下方法就能找到</li><li>更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network</li></ul><h1 id=\"首页文章数量设置\"><a href=\"#首页文章数量设置\" class=\"headerlink\" title=\"首页文章数量设置\"></a>首页文章数量设置</h1><ul><li><p>安装插件</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-generator-index</span><br><span class=\"line\">npm install --save hexo-generator-archive</span><br><span class=\"line\">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure></li><li><p>在站点配置文件config.yml中添加如下字段</p><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index_generator:</span><br><span class=\"line\">per_page: 5</span><br><span class=\"line\"></span><br><span class=\"line\">archive_generator:</span><br><span class=\"line\">per_page: 20</span><br><span class=\"line\">yearly: true</span><br><span class=\"line\">monthly: true</span><br><span class=\"line\"></span><br><span class=\"line\">tag_generator:</span><br><span class=\"line\">per_page: 10</span><br></pre></td></tr></table></figure></li><li><p>index, archive及tag开头分表代表主页，归档页面和标签页面。</p></li></ul><h1 id=\"添加评论功能\"><a href=\"#添加评论功能\" class=\"headerlink\" title=\"添加评论功能\"></a>添加评论功能</h1><ul><li><del>视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可</del></li><li>现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论</li></ul><h1 id=\"优化插件\"><a href=\"#优化插件\" class=\"headerlink\" title=\"优化插件\"></a>优化插件</h1><ul><li>hexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率<br><a href=\"https://github.com/chenzhutian/hexo-all-minifier\" target=\"_blank\" rel=\"noopener\">hexo-all-minifier</a></li></ul><h1 id=\"置顶\"><a href=\"#置顶\" class=\"headerlink\" title=\"置顶\"></a>置顶</h1><ul><li>感谢Netcan_Space提供解决方案，希望官方theme加入此功能：<a href=\"http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">添加Hexo置顶功能</a></li></ul><h1 id=\"cnpm\"><a href=\"#cnpm\" class=\"headerlink\" title=\"cnpm\"></a>cnpm</h1><ul><li>使用淘宝镜像安装插件提速，详情百度cnpm安装</li></ul><h1 id=\"RSS\"><a href=\"#RSS\" class=\"headerlink\" title=\"RSS\"></a>RSS</h1><ul><li>使用hexo-generator-feed使用rss:<a href=\"https://github.com/hexojs/hexo-generator-feed\" target=\"_blank\" rel=\"noopener\">hexo-generator-feed</a></li></ul><h1 id=\"完成之后的目录示意图\"><a href=\"#完成之后的目录示意图\" class=\"headerlink\" title=\"完成之后的目录示意图\"></a>完成之后的目录示意图</h1><p>hexo分支:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113838583.JPG\" alt=\"mark\"></p><p>master分支<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113915677.JPG\" alt=\"mark\"></p><h1 id=\"异地恢复\"><a href=\"#异地恢复\" class=\"headerlink\" title=\"异地恢复\"></a>异地恢复</h1><ul><li>最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下<ul><li>clone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml</li><li>这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除</li><li>记得安装hexo-deployer-git</li><li>next更新了，但是新功能的一些依赖还是需要看注释，自己npm install</li><li>发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作</li><li>上面那2张图已经过时了，可以去我的github里看博客的文件目录</li><li>有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱</li></ul></li></ul><h1 id=\"这位大佬靠谱\"><a href=\"#这位大佬靠谱\" class=\"headerlink\" title=\"这位大佬靠谱\"></a>这位大佬靠谱</h1><ul><li>网址在这：<a href=\"https://www.vincentqin.tech/posts/build-a-website-using-hexo/\" target=\"_blank\" rel=\"noopener\">HEXO建站备忘录</a></li><li>里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍<a href=\"https://github.com/coneycode/hexo-git-backup\" target=\"_blank\" rel=\"noopener\">hexo-git-backup</a></li></ul>","site":{"data":{}},"excerpt":"<hr><p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p><ul><li>2017.2.8更新md写作软件</li><li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li><li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li><li>2017.3.30更新置顶说明原文地址</li><li>2017.12.27更新异地恢复</li><li>2018.7.6更新一个比较全面的参考网址","more":"</li></ul><h1 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h1><ul><li>安装好Node.js</li><li>安装好git</li><li>安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。</li><li>可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)</li></ul><h1 id=\"GitHub-amp-Hexo初始化\"><a href=\"#GitHub-amp-Hexo初始化\" class=\"headerlink\" title=\"GitHub&amp;Hexo初始化\"></a>GitHub&amp;Hexo初始化</h1><p>选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些<br>最后的过程是这样的</p><ul><li>注册github账号，这个就省略了</li><li>新建一个repository，命名必须为“你的账户名.github.io”</li><li>对这个repository新建一个hexo分支并把hexo设置为默认分支</li><li>git bash cd到你本地新建的一个文件夹（用于存放博客项目）</li><li>依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git</li><li>此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到 _config.yml，修改deploy参数为master, 见后文</li><li>此时你的网址就是：https://你的账户名.github.io/</li><li>现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件</li></ul><p><strong>之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支</strong></p><h1 id=\"Hexo配置\"><a href=\"#Hexo配置\" class=\"headerlink\" title=\"Hexo配置\"></a>Hexo配置</h1><p>Hexo博客的目录格式如下</p><p>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br>| ├── _drafts<br>| └── _posts<br>└── themes</p><ul><li>config.yml在这里设置博客的总体参数，博客名，url，git什么的</li><li>_posts里面存放你的文章</li><li>theme顾名思义，存放博客界面主题</li><li>在config.yml中关键配置以下几个参数<ul><li>title: 博客网站标题</li><li>subtitle: 副标题</li><li>description: 一句话简介</li><li>author: 作者名</li><li>language: zh-Hans 语言根据你选择的主题看，在主题目录的language里查看支持哪些语言</li><li>timezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区</li><li>url: https://你的用户名.github.io/</li><li>type: git</li><li>repo: <a href=\"https://github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://github.com/你的用户名/你的用户名.github.io.git</a></li><li><strong>branch: master</strong></li></ul></li></ul><h1 id=\"更新Hexo项目\"><a href=\"#更新Hexo项目\" class=\"headerlink\" title=\"更新Hexo项目\"></a>更新Hexo项目</h1><p>依次执行</p><ul><li>git add . (检查所有文件是否更新)</li><li>git commit -m “更新报告” (提交commit)</li><li>git push origin hexo (上传更新到github)<br><strong>第一次更新可能出现一些错误</strong></li><li>第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错</li><li>permission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：<a href=\"https://www.github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://www.github.com/你的用户名/你的用户名.github.io.git</a></li><li>refusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo –allow-unrelated-histories，之后再ush<br><strong>所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的</strong></li></ul><h1 id=\"Hexo-写博客及更新网页\"><a href=\"#Hexo-写博客及更新网页\" class=\"headerlink\" title=\"Hexo 写博客及更新网页\"></a>Hexo 写博客及更新网页</h1><p>主要命令如下</p><ul><li>hexo clean 清除缓存和静态文件</li><li>hexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate</li><li>hexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭</li><li>hexo new”文件名” 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了</li><li>hexo d或者hexo deploy，写好博客，clean&amp;generate之后，部署博客到GitHub Pages上，就更新了博客网页<br><strong>其中有几点要注意</strong></li><li>第一次新建目录后执行#npm install hexo-deployer-git –save安装git分发，否则上传不了</li><li>hexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格</li><li>我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s</li></ul><h1 id=\"Hexo写作的几种选择-Windows\"><a href=\"#Hexo写作的几种选择-Windows\" class=\"headerlink\" title=\"Hexo写作的几种选择(Windows)\"></a>Hexo写作的几种选择(Windows)</h1><ul><li><strong>马克飞象</strong>:这里推荐用马克飞象的chrome app版本，可以离线打开。<strong>优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年</strong>,它长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192945202.jpg\" alt=\"mark\"></li><li><strong>hexo admin插件</strong>:安装hexo-admin插件，可视化管理博客，安装方法：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-admin</span><br><span class=\"line\">hexo server -d</span><br><span class=\"line\">open http://localhost:4000/admin/</span><br></pre></td></tr></table></figure></li></ul><p><em><a href=\"http://localhost:4000/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/admin/</a></em>就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，<strong>优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好</strong>,它长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170203/192926032.jpg\" alt=\"mark\"></p><ul><li>推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。</li><li><strong>Typora</strong>:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：<strong>Typora</strong>。<strong>优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。</strong>他长这个样子：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170208/204701798.jpg\" alt=\"mark\"></li><li>Mac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。</li><li>常备一个notepad++,出现什么问题没有notepad++解决不了的。</li></ul><h1 id=\"主题设置\"><a href=\"#主题设置\" class=\"headerlink\" title=\"主题设置\"></a>主题设置</h1><ul><li>一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可</li><li>每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改</li></ul><h1 id=\"写作语法\"><a href=\"#写作语法\" class=\"headerlink\" title=\"写作语法\"></a>写作语法</h1><ul><li>语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明</li></ul><h1 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h1><ul><li>插入图片分本地和在线链接插入，推荐使用在线链接</li><li>我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。</li></ul><h1 id=\"插入公式\"><a href=\"#插入公式\" class=\"headerlink\" title=\"插入公式\"></a>插入公式</h1><ul><li>用一对$$将公式围起来，语法支持latex</li><li>必须在文章首部中声明如mathjax: true</li><li>因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法</li><li>建议更换cdn以加速:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mathjax:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  per_page: true</span><br><span class=\"line\">  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure></li></ul><h1 id=\"标签与分类\"><a href=\"#标签与分类\" class=\"headerlink\" title=\"标签与分类\"></a>标签与分类</h1><ul><li>标签与分类的区别在于1篇文章归于一类，但可能有多个标签</li><li>比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签</li><li>分类需在首部加上categories: 分类名</li><li>标签需在首部加上tags: 标签名或者tags: [标签1,标签2….],<strong>注意别打成中文逗号</strong></li></ul><h1 id=\"其他美化\"><a href=\"#其他美化\" class=\"headerlink\" title=\"其他美化\"></a>其他美化</h1><ul><li>我用的是next主题，可以设置背景之类的，百度一下方法就能找到</li><li>更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network</li></ul><h1 id=\"首页文章数量设置\"><a href=\"#首页文章数量设置\" class=\"headerlink\" title=\"首页文章数量设置\"></a>首页文章数量设置</h1><ul><li><p>安装插件</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-generator-index</span><br><span class=\"line\">npm install --save hexo-generator-archive</span><br><span class=\"line\">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure></li><li><p>在站点配置文件config.yml中添加如下字段</p><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index_generator:</span><br><span class=\"line\">per_page: 5</span><br><span class=\"line\"></span><br><span class=\"line\">archive_generator:</span><br><span class=\"line\">per_page: 20</span><br><span class=\"line\">yearly: true</span><br><span class=\"line\">monthly: true</span><br><span class=\"line\"></span><br><span class=\"line\">tag_generator:</span><br><span class=\"line\">per_page: 10</span><br></pre></td></tr></table></figure></li><li><p>index, archive及tag开头分表代表主页，归档页面和标签页面。</p></li></ul><h1 id=\"添加评论功能\"><a href=\"#添加评论功能\" class=\"headerlink\" title=\"添加评论功能\"></a>添加评论功能</h1><ul><li><del>视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可</del></li><li>现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论</li></ul><h1 id=\"优化插件\"><a href=\"#优化插件\" class=\"headerlink\" title=\"优化插件\"></a>优化插件</h1><ul><li>hexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率<br><a href=\"https://github.com/chenzhutian/hexo-all-minifier\" target=\"_blank\" rel=\"noopener\">hexo-all-minifier</a></li></ul><h1 id=\"置顶\"><a href=\"#置顶\" class=\"headerlink\" title=\"置顶\"></a>置顶</h1><ul><li>感谢Netcan_Space提供解决方案，希望官方theme加入此功能：<a href=\"http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">添加Hexo置顶功能</a></li></ul><h1 id=\"cnpm\"><a href=\"#cnpm\" class=\"headerlink\" title=\"cnpm\"></a>cnpm</h1><ul><li>使用淘宝镜像安装插件提速，详情百度cnpm安装</li></ul><h1 id=\"RSS\"><a href=\"#RSS\" class=\"headerlink\" title=\"RSS\"></a>RSS</h1><ul><li>使用hexo-generator-feed使用rss:<a href=\"https://github.com/hexojs/hexo-generator-feed\" target=\"_blank\" rel=\"noopener\">hexo-generator-feed</a></li></ul><h1 id=\"完成之后的目录示意图\"><a href=\"#完成之后的目录示意图\" class=\"headerlink\" title=\"完成之后的目录示意图\"></a>完成之后的目录示意图</h1><p>hexo分支:<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113838583.JPG\" alt=\"mark\"></p><p>master分支<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/113915677.JPG\" alt=\"mark\"></p><h1 id=\"异地恢复\"><a href=\"#异地恢复\" class=\"headerlink\" title=\"异地恢复\"></a>异地恢复</h1><ul><li>最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下<ul><li>clone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml</li><li>这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除</li><li>记得安装hexo-deployer-git</li><li>next更新了，但是新功能的一些依赖还是需要看注释，自己npm install</li><li>发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作</li><li>上面那2张图已经过时了，可以去我的github里看博客的文件目录</li><li>有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱</li></ul></li></ul><h1 id=\"这位大佬靠谱\"><a href=\"#这位大佬靠谱\" class=\"headerlink\" title=\"这位大佬靠谱\"></a>这位大佬靠谱</h1><ul><li>网址在这：<a href=\"https://www.vincentqin.tech/posts/build-a-website-using-hexo/\" target=\"_blank\" rel=\"noopener\">HEXO建站备忘录</a></li><li>里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍<a href=\"https://github.com/coneycode/hexo-git-backup\" target=\"_blank\" rel=\"noopener\">hexo-git-backup</a></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"搭站总结","path":"2017/01/16/setupmywebsite/","eyeCatchImage":null,"excerpt":"<hr><p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p><ul><li>2017.2.8更新md写作软件</li><li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li><li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li><li>2017.3.30更新置顶说明原文地址</li><li>2017.12.27更新异地恢复</li><li>2018.7.6更新一个比较全面的参考网址","date":"2017-01-16T04:01:00.000Z","pv":0,"totalPV":0,"categories":"瞎折腾","tags":["web","hexo","github"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"统计学习方法手写版笔记","date":"2018-08-09T02:03:46.000Z","mathjax":true,"html":true,"_content":"***\n把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）\n现在只有算法本身的流程，以后如果有什么新的理解再补充\n字太丑，自己都看不下去，发上来纯粹做个备份\n\n<!--more-->\n# 概论\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/1lg8jI0Bfe.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/dg79KfJ6LG.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/BiH0Ae7hf3.jpg?imageslim)\n# 感知机\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/5mH2Lc8EKA.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/GEebi0EGfA.jpg?imageslim)\n# k近邻\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lBbBE0iAbd.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/diCeKIIKjd.jpg?imageslim)\n# 朴素贝叶斯\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/LDadChj6HF.jpg?imageslim)\n# 决策树\n-\tGBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/6Hee6Cgh7I.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/D4agk9eK5e.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/mF6BD8AFm8.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Kb0chj4hBI.jpg?imageslim)\n# 逻辑斯蒂回归、最大熵\n-\t待补充最大熵和逻辑斯蒂回归之间的相互推导\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ADGijmf5lh.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/j2eFgF2JL9.jpg?imageslim)\n# 支持向量机\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/g2H4e4kfKL.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/2G6eFkEAF2.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/iBafKl6LLc.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/0eckEhF2ID.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/9A8mffeIe5.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lk4KHEA8f4.jpg?imageslim)\n# 提升方法\n-\t待补充XGBoost\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/AAFA9K0D8g.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/a39G37jjCH.jpg?imageslim)\n# EM算法\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/4EaGEHG3fC.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/fi8EcEa2Li.jpg?imageslim)\n\n-\t用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。\n-\t在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。\n\n# 隐马尔可夫\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/B8a4fcL3F8.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/KfH43akdkI.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ggL7E50ckI.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/E9EFhCgL5E.jpg?imageslim)\n# 条件随机场\n-\t待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Bk5Bj6dfEg.jpg?imageslim)\n","source":"_posts/statistical-handwriting.md","raw":"---\ntitle: 统计学习方法手写版笔记\ndate: 2018-08-09 10:03:46\ncategories: 机器学习\ntags:\n- code\n- machine learning\n- statistical learning\n- math\n\nmathjax: true\nhtml: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180809/8mFd81eem2.PNG\n---\n***\n把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）\n现在只有算法本身的流程，以后如果有什么新的理解再补充\n字太丑，自己都看不下去，发上来纯粹做个备份\n\n<!--more-->\n# 概论\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/1lg8jI0Bfe.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/dg79KfJ6LG.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/BiH0Ae7hf3.jpg?imageslim)\n# 感知机\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/5mH2Lc8EKA.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/GEebi0EGfA.jpg?imageslim)\n# k近邻\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lBbBE0iAbd.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/diCeKIIKjd.jpg?imageslim)\n# 朴素贝叶斯\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/LDadChj6HF.jpg?imageslim)\n# 决策树\n-\tGBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/6Hee6Cgh7I.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/D4agk9eK5e.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/mF6BD8AFm8.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Kb0chj4hBI.jpg?imageslim)\n# 逻辑斯蒂回归、最大熵\n-\t待补充最大熵和逻辑斯蒂回归之间的相互推导\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ADGijmf5lh.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/j2eFgF2JL9.jpg?imageslim)\n# 支持向量机\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/g2H4e4kfKL.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/2G6eFkEAF2.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/iBafKl6LLc.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/0eckEhF2ID.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/9A8mffeIe5.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lk4KHEA8f4.jpg?imageslim)\n# 提升方法\n-\t待补充XGBoost\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/AAFA9K0D8g.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/a39G37jjCH.jpg?imageslim)\n# EM算法\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/4EaGEHG3fC.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/fi8EcEa2Li.jpg?imageslim)\n\n-\t用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。\n-\t在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。\n\n# 隐马尔可夫\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/B8a4fcL3F8.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/KfH43akdkI.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ggL7E50ckI.jpg?imageslim)\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/E9EFhCgL5E.jpg?imageslim)\n# 条件随机场\n-\t待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Bk5Bj6dfEg.jpg?imageslim)\n","slug":"statistical-handwriting","published":1,"updated":"2018-08-28T09:43:30.992Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180809/8mFd81eem2.PNG"],"comments":1,"layout":"post","link":"","_id":"cjmd072dg002jqcw6br15sstp","content":"<hr><p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p><a id=\"more\"></a><h1 id=\"概论\"><a href=\"#概论\" class=\"headerlink\" title=\"概论\"></a>概论</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/1lg8jI0Bfe.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/dg79KfJ6LG.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/BiH0Ae7hf3.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/5mH2Lc8EKA.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/GEebi0EGfA.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"k近邻\"><a href=\"#k近邻\" class=\"headerlink\" title=\"k近邻\"></a>k近邻</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lBbBE0iAbd.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/diCeKIIKjd.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/LDadChj6HF.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><ul><li>GBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/6Hee6Cgh7I.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/D4agk9eK5e.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/mF6BD8AFm8.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Kb0chj4hBI.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"逻辑斯蒂回归、最大熵\"><a href=\"#逻辑斯蒂回归、最大熵\" class=\"headerlink\" title=\"逻辑斯蒂回归、最大熵\"></a>逻辑斯蒂回归、最大熵</h1><ul><li>待补充最大熵和逻辑斯蒂回归之间的相互推导<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ADGijmf5lh.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/j2eFgF2JL9.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/g2H4e4kfKL.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/2G6eFkEAF2.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/iBafKl6LLc.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/0eckEhF2ID.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/9A8mffeIe5.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lk4KHEA8f4.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"提升方法\"><a href=\"#提升方法\" class=\"headerlink\" title=\"提升方法\"></a>提升方法</h1><ul><li>待补充XGBoost<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/AAFA9K0D8g.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/a39G37jjCH.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"EM算法\"><a href=\"#EM算法\" class=\"headerlink\" title=\"EM算法\"></a>EM算法</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/4EaGEHG3fC.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/fi8EcEa2Li.jpg?imageslim\" alt=\"mark\"></p><ul><li>用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。</li><li>在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。</li></ul><h1 id=\"隐马尔可夫\"><a href=\"#隐马尔可夫\" class=\"headerlink\" title=\"隐马尔可夫\"></a>隐马尔可夫</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/B8a4fcL3F8.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/KfH43akdkI.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ggL7E50ckI.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/E9EFhCgL5E.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"条件随机场\"><a href=\"#条件随机场\" class=\"headerlink\" title=\"条件随机场\"></a>条件随机场</h1><ul><li>待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Bk5Bj6dfEg.jpg?imageslim\" alt=\"mark\"></li></ul>","site":{"data":{}},"excerpt":"<hr><p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p>","more":"<h1 id=\"概论\"><a href=\"#概论\" class=\"headerlink\" title=\"概论\"></a>概论</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/1lg8jI0Bfe.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/dg79KfJ6LG.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/BiH0Ae7hf3.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/5mH2Lc8EKA.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/GEebi0EGfA.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"k近邻\"><a href=\"#k近邻\" class=\"headerlink\" title=\"k近邻\"></a>k近邻</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lBbBE0iAbd.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/diCeKIIKjd.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/LDadChj6HF.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><ul><li>GBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/6Hee6Cgh7I.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/D4agk9eK5e.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/mF6BD8AFm8.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Kb0chj4hBI.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"逻辑斯蒂回归、最大熵\"><a href=\"#逻辑斯蒂回归、最大熵\" class=\"headerlink\" title=\"逻辑斯蒂回归、最大熵\"></a>逻辑斯蒂回归、最大熵</h1><ul><li>待补充最大熵和逻辑斯蒂回归之间的相互推导<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ADGijmf5lh.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/j2eFgF2JL9.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/g2H4e4kfKL.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/2G6eFkEAF2.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/iBafKl6LLc.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/0eckEhF2ID.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/9A8mffeIe5.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/lk4KHEA8f4.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"提升方法\"><a href=\"#提升方法\" class=\"headerlink\" title=\"提升方法\"></a>提升方法</h1><ul><li>待补充XGBoost<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/AAFA9K0D8g.jpg?imageslim\" alt=\"mark\"></li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/a39G37jjCH.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"EM算法\"><a href=\"#EM算法\" class=\"headerlink\" title=\"EM算法\"></a>EM算法</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/4EaGEHG3fC.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/fi8EcEa2Li.jpg?imageslim\" alt=\"mark\"></p><ul><li>用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。</li><li>在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。</li></ul><h1 id=\"隐马尔可夫\"><a href=\"#隐马尔可夫\" class=\"headerlink\" title=\"隐马尔可夫\"></a>隐马尔可夫</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/B8a4fcL3F8.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/KfH43akdkI.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/ggL7E50ckI.jpg?imageslim\" alt=\"mark\"></p><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/E9EFhCgL5E.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"条件随机场\"><a href=\"#条件随机场\" class=\"headerlink\" title=\"条件随机场\"></a>条件随机场</h1><ul><li>待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180809/Bk5Bj6dfEg.jpg?imageslim\" alt=\"mark\"></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Tue Aug 28 2018 17:43:30 GMT+0800 (中国标准时间)","title":"统计学习方法手写版笔记","path":"2018/08/09/statistical-handwriting/","eyeCatchImage":null,"excerpt":"<hr><p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p>","date":"2018-08-09T02:03:46.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","code","machine learning","statistical learning"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"基于端到端模型的生成式自动文摘研究","date":"2018-07-04T07:58:59.000Z","mathjax":true,"html":true,"_content":"\n本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制\n现在对整个模型做一个简单的总结\n\n<!--more-->\n\n# 任务\n-\t自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。\n-\t自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/H60Ak44CGg.jpg?imageslim)\n\n# 预备知识\n##\t循环神经网络\n-\t循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。\n-\t不展开时形式如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EJdEd3A25h.jpg?imageslim)\n-\t按时间步展开之后：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lEfFHL4Ie3.jpg?imageslim)\n\n##\tLSTM和GRU\n-\t循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。\n-\tLSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jAe0lJ2KeD.jpg?imageslim)\n-\tGRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jLgI1a8LcJ.jpg?imageslim)\n\n##\t词嵌入\n-\t深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。\n-\t词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维\n度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。\n-\t下图展示了word2vec中的skipgram模型：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekGC6f2lmi.jpg?imageslim)\n-\t得到的词嵌入矩阵如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/mk1I5cG56K.jpg?imageslim)\n-\tMikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。\n-\tWord2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，\n-\t而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。\n\n##\t注意力\n-\t在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8HFdB4k4i3.jpg?imageslim)\n\n##\t序列到序列\n-\tseq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GGL6KGmj64.jpg?imageslim)\n-\t对于序列到序列模型的一些个人理解：\n\t-\t（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。\n\t-\t（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。\n\t-\t（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。\n\n## 序列损失\n-\t解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。\n\n# 基本模型\n-\t预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lJ0kcHJgl8.jpg?imageslim)\n-\t训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/7HLld3059D.jpg?imageslim)\n-\t特征输入编码器得到中间表示\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8I2DG7I38e.png?imageslim)\n-\t拿到中间表示和输出文摘(相当于label)，输入解码器进行解码\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GHe6CliiHJ.png?imageslim)\n-\t加入注意力机制后完整的序列到序列模型结构如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CBaaBiFb64.jpg?imageslim)\n\n# 情感融合机制\n-\t情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。\n-\t先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/j1J086Ikb8.jpg?imageslim)\n-\t查找词典得到情感向量（即情感特征）\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/6FD34B5ALc.jpg?imageslim)\n-\t将情感特征直接拼接在中间表示之后，输入解码器\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/b776dIjkI5.jpg?imageslim)\n\n# 结果\n-\t结果由ROUGE-F1值形式记录，情感语料下各种方法对比\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EI71b1h7J1.png?imageslim)\n-\t普通语料下情感融合方案对比\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jkdJI4gmk1.png?imageslim)\n-\t情感分类准确率，作为参考，之前训练的情感分类器准确率为74%\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/d54gL3IlGD.png?imageslim)\n-\t因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/J8cB6f566i.png?imageslim)\n\n# 问题\n-\tunk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/204g2F7645.png?imageslim)\n-\t语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“...... 周三...... 黄金价格上涨”和“......周四......黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：\n\t-\t不去重：保留原文本，不去重，进行训练和测试\n\t-\t去重：删除语料中所有重复文摘对应的的短文本-短文摘对。\n\t-\t训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。\n\t-\t测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。\n\t重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Em35i3Ah6J.png?imageslim)\n\n# 实现环境\n-\t这里是github地址：-\t[Abstract_Summarization_Tensorflow](https://github.com/thinkwee/Abstract_Summarization_Tensorflow)\n-\tUbuntu 16.04\n-\tTensorflow 1.6\n-\tCUDA 9.0\n-\tCudnn 7.1.2\n-\tGigawords数据集，训练了部分数据，约30万\n-\tGTX1066，训练时间3到4个小时\n\n# 参考文献\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CEglkLLcD2.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/BAbBd73m7L.JPG)\n\n","source":"_posts/seq2seq-summarization.md","raw":"---\ntitle: 基于端到端模型的生成式自动文摘研究\ndate: 2018-07-04 15:58:59\ntags:\n  - abstractive summarization\n  - seq2seq\n  - machinelearning\n  -\trnn\n  -\tnlp\n  - lstm\n  - gru\ncategories:\n  - 自然语言处理\nmathjax: true\nhtml: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180704/27AhFDi986.png?imageslim\n---\n\n本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制\n现在对整个模型做一个简单的总结\n\n<!--more-->\n\n# 任务\n-\t自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。\n-\t自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/H60Ak44CGg.jpg?imageslim)\n\n# 预备知识\n##\t循环神经网络\n-\t循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。\n-\t不展开时形式如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EJdEd3A25h.jpg?imageslim)\n-\t按时间步展开之后：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lEfFHL4Ie3.jpg?imageslim)\n\n##\tLSTM和GRU\n-\t循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。\n-\tLSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jAe0lJ2KeD.jpg?imageslim)\n-\tGRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jLgI1a8LcJ.jpg?imageslim)\n\n##\t词嵌入\n-\t深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。\n-\t词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维\n度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。\n-\t下图展示了word2vec中的skipgram模型：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekGC6f2lmi.jpg?imageslim)\n-\t得到的词嵌入矩阵如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/mk1I5cG56K.jpg?imageslim)\n-\tMikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。\n-\tWord2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，\n-\t而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。\n\n##\t注意力\n-\t在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8HFdB4k4i3.jpg?imageslim)\n\n##\t序列到序列\n-\tseq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GGL6KGmj64.jpg?imageslim)\n-\t对于序列到序列模型的一些个人理解：\n\t-\t（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。\n\t-\t（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。\n\t-\t（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。\n\n## 序列损失\n-\t解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。\n\n# 基本模型\n-\t预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lJ0kcHJgl8.jpg?imageslim)\n-\t训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/7HLld3059D.jpg?imageslim)\n-\t特征输入编码器得到中间表示\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8I2DG7I38e.png?imageslim)\n-\t拿到中间表示和输出文摘(相当于label)，输入解码器进行解码\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GHe6CliiHJ.png?imageslim)\n-\t加入注意力机制后完整的序列到序列模型结构如下：\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CBaaBiFb64.jpg?imageslim)\n\n# 情感融合机制\n-\t情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。\n-\t先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/j1J086Ikb8.jpg?imageslim)\n-\t查找词典得到情感向量（即情感特征）\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/6FD34B5ALc.jpg?imageslim)\n-\t将情感特征直接拼接在中间表示之后，输入解码器\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/b776dIjkI5.jpg?imageslim)\n\n# 结果\n-\t结果由ROUGE-F1值形式记录，情感语料下各种方法对比\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EI71b1h7J1.png?imageslim)\n-\t普通语料下情感融合方案对比\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jkdJI4gmk1.png?imageslim)\n-\t情感分类准确率，作为参考，之前训练的情感分类器准确率为74%\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/d54gL3IlGD.png?imageslim)\n-\t因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/J8cB6f566i.png?imageslim)\n\n# 问题\n-\tunk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/204g2F7645.png?imageslim)\n-\t语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“...... 周三...... 黄金价格上涨”和“......周四......黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：\n\t-\t不去重：保留原文本，不去重，进行训练和测试\n\t-\t去重：删除语料中所有重复文摘对应的的短文本-短文摘对。\n\t-\t训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。\n\t-\t测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。\n\t重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Em35i3Ah6J.png?imageslim)\n\n# 实现环境\n-\t这里是github地址：-\t[Abstract_Summarization_Tensorflow](https://github.com/thinkwee/Abstract_Summarization_Tensorflow)\n-\tUbuntu 16.04\n-\tTensorflow 1.6\n-\tCUDA 9.0\n-\tCudnn 7.1.2\n-\tGigawords数据集，训练了部分数据，约30万\n-\tGTX1066，训练时间3到4个小时\n\n# 参考文献\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CEglkLLcD2.JPG)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/180704/BAbBd73m7L.JPG)\n\n","slug":"seq2seq-summarization","published":1,"updated":"2018-07-23T01:28:38.518Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180704/27AhFDi986.png?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072dh002mqcw6vp0xmfap","content":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p><a id=\"more\"></a><h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul><li>自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。</li><li>自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/H60Ak44CGg.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h1><h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2><ul><li>循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。</li><li>不展开时形式如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EJdEd3A25h.jpg?imageslim\" alt=\"mark\"></li><li>按时间步展开之后：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lEfFHL4Ie3.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"LSTM和GRU\"><a href=\"#LSTM和GRU\" class=\"headerlink\" title=\"LSTM和GRU\"></a>LSTM和GRU</h2><ul><li>循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。</li><li>LSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jAe0lJ2KeD.jpg?imageslim\" alt=\"mark\"></li><li>GRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jLgI1a8LcJ.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"词嵌入\"><a href=\"#词嵌入\" class=\"headerlink\" title=\"词嵌入\"></a>词嵌入</h2><ul><li>深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。</li><li>词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维<br>度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。</li><li>下图展示了word2vec中的skipgram模型：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekGC6f2lmi.jpg?imageslim\" alt=\"mark\"></li><li>得到的词嵌入矩阵如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/mk1I5cG56K.jpg?imageslim\" alt=\"mark\"></li><li>Mikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。</li><li>Word2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，</li><li>而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。</li></ul><h2 id=\"注意力\"><a href=\"#注意力\" class=\"headerlink\" title=\"注意力\"></a>注意力</h2><ul><li>在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8HFdB4k4i3.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul><li>seq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GGL6KGmj64.jpg?imageslim\" alt=\"mark\"></li><li>对于序列到序列模型的一些个人理解：<ul><li>（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。</li><li>（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。</li><li>（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。</li></ul></li></ul><h2 id=\"序列损失\"><a href=\"#序列损失\" class=\"headerlink\" title=\"序列损失\"></a>序列损失</h2><ul><li>解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。</li></ul><h1 id=\"基本模型\"><a href=\"#基本模型\" class=\"headerlink\" title=\"基本模型\"></a>基本模型</h1><ul><li>预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lJ0kcHJgl8.jpg?imageslim\" alt=\"mark\"></li><li>训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/7HLld3059D.jpg?imageslim\" alt=\"mark\"></li><li>特征输入编码器得到中间表示<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8I2DG7I38e.png?imageslim\" alt=\"mark\"></li><li>拿到中间表示和输出文摘(相当于label)，输入解码器进行解码<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GHe6CliiHJ.png?imageslim\" alt=\"mark\"></li><li>加入注意力机制后完整的序列到序列模型结构如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CBaaBiFb64.jpg?imageslim\" alt=\"mark\"></li></ul><h1 id=\"情感融合机制\"><a href=\"#情感融合机制\" class=\"headerlink\" title=\"情感融合机制\"></a>情感融合机制</h1><ul><li>情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。</li><li>先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/j1J086Ikb8.jpg?imageslim\" alt=\"mark\"></li><li>查找词典得到情感向量（即情感特征）<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/6FD34B5ALc.jpg?imageslim\" alt=\"mark\"></li><li>将情感特征直接拼接在中间表示之后，输入解码器<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/b776dIjkI5.jpg?imageslim\" alt=\"mark\"></li></ul><h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul><li>结果由ROUGE-F1值形式记录，情感语料下各种方法对比<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EI71b1h7J1.png?imageslim\" alt=\"mark\"></li><li>普通语料下情感融合方案对比<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jkdJI4gmk1.png?imageslim\" alt=\"mark\"></li><li>情感分类准确率，作为参考，之前训练的情感分类器准确率为74%<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/d54gL3IlGD.png?imageslim\" alt=\"mark\"></li><li>因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/J8cB6f566i.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><ul><li>unk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/204g2F7645.png?imageslim\" alt=\"mark\"></li><li>语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“…… 周三…… 黄金价格上涨”和“……周四……黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：<ul><li>不去重：保留原文本，不去重，进行训练和测试</li><li>去重：删除语料中所有重复文摘对应的的短文本-短文摘对。</li><li>训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。</li><li>测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。<br>重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Em35i3Ah6J.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"实现环境\"><a href=\"#实现环境\" class=\"headerlink\" title=\"实现环境\"></a>实现环境</h1><ul><li>这里是github地址：- <a href=\"https://github.com/thinkwee/Abstract_Summarization_Tensorflow\" target=\"_blank\" rel=\"noopener\">Abstract_Summarization_Tensorflow</a></li><li>Ubuntu 16.04</li><li>Tensorflow 1.6</li><li>CUDA 9.0</li><li>Cudnn 7.1.2</li><li>Gigawords数据集，训练了部分数据，约30万</li><li>GTX1066，训练时间3到4个小时</li></ul><h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CEglkLLcD2.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/BAbBd73m7L.JPG\" alt=\"mark\"></p>","site":{"data":{}},"excerpt":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p>","more":"<h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul><li>自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。</li><li>自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。</li></ul><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/H60Ak44CGg.jpg?imageslim\" alt=\"mark\"></p><h1 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h1><h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2><ul><li>循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。</li><li>不展开时形式如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EJdEd3A25h.jpg?imageslim\" alt=\"mark\"></li><li>按时间步展开之后：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lEfFHL4Ie3.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"LSTM和GRU\"><a href=\"#LSTM和GRU\" class=\"headerlink\" title=\"LSTM和GRU\"></a>LSTM和GRU</h2><ul><li>循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。</li><li>LSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jAe0lJ2KeD.jpg?imageslim\" alt=\"mark\"></li><li>GRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jLgI1a8LcJ.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"词嵌入\"><a href=\"#词嵌入\" class=\"headerlink\" title=\"词嵌入\"></a>词嵌入</h2><ul><li>深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。</li><li>词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维<br>度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。</li><li>下图展示了word2vec中的skipgram模型：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/ekGC6f2lmi.jpg?imageslim\" alt=\"mark\"></li><li>得到的词嵌入矩阵如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/mk1I5cG56K.jpg?imageslim\" alt=\"mark\"></li><li>Mikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。</li><li>Word2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，</li><li>而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。</li></ul><h2 id=\"注意力\"><a href=\"#注意力\" class=\"headerlink\" title=\"注意力\"></a>注意力</h2><ul><li>在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8HFdB4k4i3.jpg?imageslim\" alt=\"mark\"></li></ul><h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul><li>seq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GGL6KGmj64.jpg?imageslim\" alt=\"mark\"></li><li>对于序列到序列模型的一些个人理解：<ul><li>（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。</li><li>（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。</li><li>（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。</li></ul></li></ul><h2 id=\"序列损失\"><a href=\"#序列损失\" class=\"headerlink\" title=\"序列损失\"></a>序列损失</h2><ul><li>解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。</li></ul><h1 id=\"基本模型\"><a href=\"#基本模型\" class=\"headerlink\" title=\"基本模型\"></a>基本模型</h1><ul><li>预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/lJ0kcHJgl8.jpg?imageslim\" alt=\"mark\"></li><li>训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/7HLld3059D.jpg?imageslim\" alt=\"mark\"></li><li>特征输入编码器得到中间表示<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/8I2DG7I38e.png?imageslim\" alt=\"mark\"></li><li>拿到中间表示和输出文摘(相当于label)，输入解码器进行解码<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/GHe6CliiHJ.png?imageslim\" alt=\"mark\"></li><li>加入注意力机制后完整的序列到序列模型结构如下：<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CBaaBiFb64.jpg?imageslim\" alt=\"mark\"></li></ul><h1 id=\"情感融合机制\"><a href=\"#情感融合机制\" class=\"headerlink\" title=\"情感融合机制\"></a>情感融合机制</h1><ul><li>情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。</li><li>先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/j1J086Ikb8.jpg?imageslim\" alt=\"mark\"></li><li>查找词典得到情感向量（即情感特征）<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/6FD34B5ALc.jpg?imageslim\" alt=\"mark\"></li><li>将情感特征直接拼接在中间表示之后，输入解码器<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/b776dIjkI5.jpg?imageslim\" alt=\"mark\"></li></ul><h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul><li>结果由ROUGE-F1值形式记录，情感语料下各种方法对比<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/EI71b1h7J1.png?imageslim\" alt=\"mark\"></li><li>普通语料下情感融合方案对比<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/jkdJI4gmk1.png?imageslim\" alt=\"mark\"></li><li>情感分类准确率，作为参考，之前训练的情感分类器准确率为74%<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/d54gL3IlGD.png?imageslim\" alt=\"mark\"></li><li>因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/J8cB6f566i.png?imageslim\" alt=\"mark\"></li></ul><h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><ul><li>unk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/204g2F7645.png?imageslim\" alt=\"mark\"></li><li>语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“…… 周三…… 黄金价格上涨”和“……周四……黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：<ul><li>不去重：保留原文本，不去重，进行训练和测试</li><li>去重：删除语料中所有重复文摘对应的的短文本-短文摘对。</li><li>训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。</li><li>测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。<br>重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/Em35i3Ah6J.png?imageslim\" alt=\"mark\"></li></ul></li></ul><h1 id=\"实现环境\"><a href=\"#实现环境\" class=\"headerlink\" title=\"实现环境\"></a>实现环境</h1><ul><li>这里是github地址：- <a href=\"https://github.com/thinkwee/Abstract_Summarization_Tensorflow\" target=\"_blank\" rel=\"noopener\">Abstract_Summarization_Tensorflow</a></li><li>Ubuntu 16.04</li><li>Tensorflow 1.6</li><li>CUDA 9.0</li><li>Cudnn 7.1.2</li><li>Gigawords数据集，训练了部分数据，约30万</li><li>GTX1066，训练时间3到4个小时</li></ul><h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/CEglkLLcD2.JPG\" alt=\"mark\"><br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/180704/BAbBd73m7L.JPG\" alt=\"mark\"></p>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"基于端到端模型的生成式自动文摘研究","path":"2018/07/04/seq2seq-summarization/","eyeCatchImage":null,"excerpt":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p>","date":"2018-07-04T07:58:59.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","machinelearning","nlp","seq2seq","rnn","lstm","gru"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"用Python实现字典树","date":"2017-05-02T02:09:19.000Z","_content":"***\n在Python中有字典这一数据结构，因此用Python实现字典树很方便\n\n<!--more-->\n\n# 字典树(trie)\n-\t字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。\n-\t字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找\n-\t一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身\n\n# 例子\n-\t以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成\n-\t初始化trie\n\t```Python\n\t\ttrie = {}\n\t```\n-\ttrie中插入单词\n\t```Python\n\t\tdef insert(trie, w):\n\t\t\tfor c in w:\n\t\t\t\ttrie = trie.setdefault(c, {})  # if c not in trie then set trie[c]={}\n\t\t\ttrie[None] = w\n\t```\n\t对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点\n\t遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数\n\t这样一棵树最终完成时就如标题图所示，其中前四行是加入[\"cat\", \"cats\",\"dog\", \"rat\"]之后树的内容\n-\ttrie中查找某一个单词\n\t```Python\n\t\tdef prefixs(trie, w, lo):\n\t\t\tfor i in range(lo, len(w)):\n\t\t\t\ttrie = trie.get(w[i])\n\t\t\t\tif trie is None:\n\t\t\t\t\tbreak\n\t\t\t\tprefix = trie.get(None)\n\t\t\tif prefix:\n\t\t\t\tyield i + 1, prefix\n\t```\n\t因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同\n\t特别说明的是，trie的一大优势便是支持插入与查找同时进行\n\t\n# 后缀树\n-\t如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。\n-\t简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化\n-\t待续\n\t\t\t\t","source":"_posts/trie.md","raw":"---\ntitle: 用Python实现字典树\ndate: 2017-05-02 10:09:19\ntags:\n-\tcode\n-\tpython\ncategories:\n-\tPython\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/20170502/101155629.JPG\n---\n***\n在Python中有字典这一数据结构，因此用Python实现字典树很方便\n\n<!--more-->\n\n# 字典树(trie)\n-\t字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。\n-\t字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找\n-\t一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身\n\n# 例子\n-\t以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成\n-\t初始化trie\n\t```Python\n\t\ttrie = {}\n\t```\n-\ttrie中插入单词\n\t```Python\n\t\tdef insert(trie, w):\n\t\t\tfor c in w:\n\t\t\t\ttrie = trie.setdefault(c, {})  # if c not in trie then set trie[c]={}\n\t\t\ttrie[None] = w\n\t```\n\t对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点\n\t遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数\n\t这样一棵树最终完成时就如标题图所示，其中前四行是加入[\"cat\", \"cats\",\"dog\", \"rat\"]之后树的内容\n-\ttrie中查找某一个单词\n\t```Python\n\t\tdef prefixs(trie, w, lo):\n\t\t\tfor i in range(lo, len(w)):\n\t\t\t\ttrie = trie.get(w[i])\n\t\t\t\tif trie is None:\n\t\t\t\t\tbreak\n\t\t\t\tprefix = trie.get(None)\n\t\t\tif prefix:\n\t\t\t\tyield i + 1, prefix\n\t```\n\t因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同\n\t特别说明的是，trie的一大优势便是支持插入与查找同时进行\n\t\n# 后缀树\n-\t如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。\n-\t简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化\n-\t待续\n\t\t\t\t","slug":"trie","published":1,"updated":"2018-07-23T01:28:38.518Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/20170502/101155629.JPG"],"comments":1,"layout":"post","link":"","_id":"cjmd072dj002pqcw61zo6vmeo","content":"<hr><p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p><a id=\"more\"></a><h1 id=\"字典树-trie\"><a href=\"#字典树-trie\" class=\"headerlink\" title=\"字典树(trie)\"></a>字典树(trie)</h1><ul><li>字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。</li><li>字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找</li><li>一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身</li></ul><h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><ul><li>以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成</li><li><p>初始化trie</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trie = &#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>trie中插入单词</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert</span><span class=\"params\">(trie, w)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> w:</span><br><span class=\"line\">\t\ttrie = trie.setdefault(c, &#123;&#125;)  <span class=\"comment\"># if c not in trie then set trie[c]=&#123;&#125;</span></span><br><span class=\"line\">\ttrie[<span class=\"keyword\">None</span>] = w</span><br></pre></td></tr></table></figure><p>对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点<br>遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数<br>这样一棵树最终完成时就如标题图所示，其中前四行是加入[“cat”, “cats”,”dog”, “rat”]之后树的内容</p></li><li><p>trie中查找某一个单词</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">prefixs</span><span class=\"params\">(trie, w, lo)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(lo, len(w)):</span><br><span class=\"line\">\t\ttrie = trie.get(w[i])</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> trie <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\tprefix = trie.get(<span class=\"keyword\">None</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> prefix:</span><br><span class=\"line\">\t\t<span class=\"keyword\">yield</span> i + <span class=\"number\">1</span>, prefix</span><br></pre></td></tr></table></figure><p>因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同<br>特别说明的是，trie的一大优势便是支持插入与查找同时进行</p></li></ul><h1 id=\"后缀树\"><a href=\"#后缀树\" class=\"headerlink\" title=\"后缀树\"></a>后缀树</h1><ul><li>如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。</li><li>简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化</li><li>待续</li></ul>","site":{"data":{}},"excerpt":"<hr><p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p>","more":"<h1 id=\"字典树-trie\"><a href=\"#字典树-trie\" class=\"headerlink\" title=\"字典树(trie)\"></a>字典树(trie)</h1><ul><li>字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。</li><li>字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找</li><li>一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身</li></ul><h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><ul><li>以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成</li><li><p>初始化trie</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trie = &#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>trie中插入单词</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert</span><span class=\"params\">(trie, w)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> w:</span><br><span class=\"line\">\t\ttrie = trie.setdefault(c, &#123;&#125;)  <span class=\"comment\"># if c not in trie then set trie[c]=&#123;&#125;</span></span><br><span class=\"line\">\ttrie[<span class=\"keyword\">None</span>] = w</span><br></pre></td></tr></table></figure><p>对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点<br>遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数<br>这样一棵树最终完成时就如标题图所示，其中前四行是加入[“cat”, “cats”,”dog”, “rat”]之后树的内容</p></li><li><p>trie中查找某一个单词</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">prefixs</span><span class=\"params\">(trie, w, lo)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(lo, len(w)):</span><br><span class=\"line\">\t\ttrie = trie.get(w[i])</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> trie <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\tprefix = trie.get(<span class=\"keyword\">None</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> prefix:</span><br><span class=\"line\">\t\t<span class=\"keyword\">yield</span> i + <span class=\"number\">1</span>, prefix</span><br></pre></td></tr></table></figure><p>因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同<br>特别说明的是，trie的一大优势便是支持插入与查找同时进行</p></li></ul><h1 id=\"后缀树\"><a href=\"#后缀树\" class=\"headerlink\" title=\"后缀树\"></a>后缀树</h1><ul><li>如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。</li><li>简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化</li><li>待续</li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"用Python实现字典树","path":"2017/05/02/trie/","eyeCatchImage":null,"excerpt":"<hr><p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p>","date":"2017-05-02T02:09:19.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Tensorflow学习","date":"2018-01-10T01:15:39.000Z","mathjax":true,"_content":"\n-\ttensorflow 学习\n-\t应用于深度学习\n-\tStanford CS20SI\n\n<!--more-->\n\n# CS20SI\n-\t课程代码：[Tensorflow_Learn](https://github.com/thinkwee/Tensorflow_Learn)\n-\t主要是讲解session、op、graph等基本概念\n-\t介绍了如何处理数据，计算梯度，使用summary记录中间数据并在tensorboard中可视化\n-\tB站上有17年的视频：[CS20SI - Tensorflow for Deep Learning Research](https://www.bilibili.com/video/av15898988/)\n-\t有一些关于图像风格迁移的略过了，待补充\n\n# TF中的坑\n-\t记录自己错过的地方\n-\tint() argument must be a string, a bytes-like object or a number, not 'Tensor'\n -\t假如一些函数的参数指定为整型,不能为tensor，则传整型的tensor也不行\n -\t解决：调用eval传值：tensor.Variable.eval()\n-\tAttempting to use uninitialized value\n -\trun模型之前先sess.run(tf.global_variables_initializer())\n -\t如果是回复模型出现错误，说明恢复之前先build graph了，且其中包含了有初始值的tf.variable，最好是恢复整个图和张量，而不是建一个一样的图再恢复张量\n-\t提示nested tuple相关错误\n -\t搜索tf.unstack()查看其用法\n -\t用for i in variable展开\n-\ttensor object is not iterable\n -\ttensor对象是不可遍历的，不能用for i in tensor 或者tensor[i]\n -\t解决：尝试调用其函数或者使用np.function()获取可遍历的值\n-\t在seq2seq中使用LuongAttention报错\n -\t因为我是从无注意力的seq2seq中直接加了attention_wrapper才会报错\n -\t解码端的初始状态应先置0，再复制成编码端最终输出的状态\n ```Python\n decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state) \n ```\n\n\n","source":"_posts/tensorflowtutorial.md","raw":"---\ntitle: Tensorflow学习\ndate: 2018-01-10 09:15:39\ntags:\n\t- tensorflow\n\t- machinelearning\n\t- code\ncategories:\n\t- 机器学习\nmathjax: true\nphoto: http://ojtdnrpmt.bkt.clouddn.com/blog/180307/el4J0a52f0.jpg?imageslim\n---\n\n-\ttensorflow 学习\n-\t应用于深度学习\n-\tStanford CS20SI\n\n<!--more-->\n\n# CS20SI\n-\t课程代码：[Tensorflow_Learn](https://github.com/thinkwee/Tensorflow_Learn)\n-\t主要是讲解session、op、graph等基本概念\n-\t介绍了如何处理数据，计算梯度，使用summary记录中间数据并在tensorboard中可视化\n-\tB站上有17年的视频：[CS20SI - Tensorflow for Deep Learning Research](https://www.bilibili.com/video/av15898988/)\n-\t有一些关于图像风格迁移的略过了，待补充\n\n# TF中的坑\n-\t记录自己错过的地方\n-\tint() argument must be a string, a bytes-like object or a number, not 'Tensor'\n -\t假如一些函数的参数指定为整型,不能为tensor，则传整型的tensor也不行\n -\t解决：调用eval传值：tensor.Variable.eval()\n-\tAttempting to use uninitialized value\n -\trun模型之前先sess.run(tf.global_variables_initializer())\n -\t如果是回复模型出现错误，说明恢复之前先build graph了，且其中包含了有初始值的tf.variable，最好是恢复整个图和张量，而不是建一个一样的图再恢复张量\n-\t提示nested tuple相关错误\n -\t搜索tf.unstack()查看其用法\n -\t用for i in variable展开\n-\ttensor object is not iterable\n -\ttensor对象是不可遍历的，不能用for i in tensor 或者tensor[i]\n -\t解决：尝试调用其函数或者使用np.function()获取可遍历的值\n-\t在seq2seq中使用LuongAttention报错\n -\t因为我是从无注意力的seq2seq中直接加了attention_wrapper才会报错\n -\t解码端的初始状态应先置0，再复制成编码端最终输出的状态\n ```Python\n decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state) \n ```\n\n\n","slug":"tensorflowtutorial","published":1,"updated":"2018-07-23T01:28:38.518Z","photos":["http://ojtdnrpmt.bkt.clouddn.com/blog/180307/el4J0a52f0.jpg?imageslim"],"comments":1,"layout":"post","link":"","_id":"cjmd072dk002tqcw6gtkb4urj","content":"<ul><li>tensorflow 学习</li><li>应用于深度学习</li><li>Stanford CS20SI</li></ul><a id=\"more\"></a><h1 id=\"CS20SI\"><a href=\"#CS20SI\" class=\"headerlink\" title=\"CS20SI\"></a>CS20SI</h1><ul><li>课程代码：<a href=\"https://github.com/thinkwee/Tensorflow_Learn\" target=\"_blank\" rel=\"noopener\">Tensorflow_Learn</a></li><li>主要是讲解session、op、graph等基本概念</li><li>介绍了如何处理数据，计算梯度，使用summary记录中间数据并在tensorboard中可视化</li><li>B站上有17年的视频：<a href=\"https://www.bilibili.com/video/av15898988/\" target=\"_blank\" rel=\"noopener\">CS20SI - Tensorflow for Deep Learning Research</a></li><li>有一些关于图像风格迁移的略过了，待补充</li></ul><h1 id=\"TF中的坑\"><a href=\"#TF中的坑\" class=\"headerlink\" title=\"TF中的坑\"></a>TF中的坑</h1><ul><li>记录自己错过的地方</li><li>int() argument must be a string, a bytes-like object or a number, not ‘Tensor’<ul><li>假如一些函数的参数指定为整型,不能为tensor，则传整型的tensor也不行</li><li>解决：调用eval传值：tensor.Variable.eval()</li></ul></li><li>Attempting to use uninitialized value<ul><li>run模型之前先sess.run(tf.global_variables_initializer())</li><li>如果是回复模型出现错误，说明恢复之前先build graph了，且其中包含了有初始值的tf.variable，最好是恢复整个图和张量，而不是建一个一样的图再恢复张量</li></ul></li><li>提示nested tuple相关错误<ul><li>搜索tf.unstack()查看其用法</li><li>用for i in variable展开</li></ul></li><li>tensor object is not iterable<ul><li>tensor对象是不可遍历的，不能用for i in tensor 或者tensor[i]</li><li>解决：尝试调用其函数或者使用np.function()获取可遍历的值</li></ul></li><li>在seq2seq中使用LuongAttention报错<ul><li>因为我是从无注意力的seq2seq中直接加了attention_wrapper才会报错</li><li>解码端的初始状态应先置0，再复制成编码端最终输出的状态<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state)</span><br></pre></td></tr></table></figure></li></ul></li></ul>","site":{"data":{}},"excerpt":"<ul><li>tensorflow 学习</li><li>应用于深度学习</li><li>Stanford CS20SI</li></ul>","more":"<h1 id=\"CS20SI\"><a href=\"#CS20SI\" class=\"headerlink\" title=\"CS20SI\"></a>CS20SI</h1><ul><li>课程代码：<a href=\"https://github.com/thinkwee/Tensorflow_Learn\" target=\"_blank\" rel=\"noopener\">Tensorflow_Learn</a></li><li>主要是讲解session、op、graph等基本概念</li><li>介绍了如何处理数据，计算梯度，使用summary记录中间数据并在tensorboard中可视化</li><li>B站上有17年的视频：<a href=\"https://www.bilibili.com/video/av15898988/\" target=\"_blank\" rel=\"noopener\">CS20SI - Tensorflow for Deep Learning Research</a></li><li>有一些关于图像风格迁移的略过了，待补充</li></ul><h1 id=\"TF中的坑\"><a href=\"#TF中的坑\" class=\"headerlink\" title=\"TF中的坑\"></a>TF中的坑</h1><ul><li>记录自己错过的地方</li><li>int() argument must be a string, a bytes-like object or a number, not ‘Tensor’<ul><li>假如一些函数的参数指定为整型,不能为tensor，则传整型的tensor也不行</li><li>解决：调用eval传值：tensor.Variable.eval()</li></ul></li><li>Attempting to use uninitialized value<ul><li>run模型之前先sess.run(tf.global_variables_initializer())</li><li>如果是回复模型出现错误，说明恢复之前先build graph了，且其中包含了有初始值的tf.variable，最好是恢复整个图和张量，而不是建一个一样的图再恢复张量</li></ul></li><li>提示nested tuple相关错误<ul><li>搜索tf.unstack()查看其用法</li><li>用for i in variable展开</li></ul></li><li>tensor object is not iterable<ul><li>tensor对象是不可遍历的，不能用for i in tensor 或者tensor[i]</li><li>解决：尝试调用其函数或者使用np.function()获取可遍历的值</li></ul></li><li>在seq2seq中使用LuongAttention报错<ul><li>因为我是从无注意力的seq2seq中直接加了attention_wrapper才会报错</li><li>解码端的初始状态应先置0，再复制成编码端最终输出的状态<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state)</span><br></pre></td></tr></table></figure></li></ul></li></ul>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"Tensorflow学习","path":"2018/01/10/tensorflowtutorial/","eyeCatchImage":null,"excerpt":"<ul><li>tensorflow 学习</li><li>应用于深度学习</li><li>Stanford CS20SI</li></ul>","date":"2018-01-10T01:15:39.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code","tensorflow"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"DataFrame函数与属性","date":"2017-02-04T13:02:51.000Z","_content":"\n整理了pandas中DataFrame的属性和函数，做成了表格。\n顺便推荐csv转md语法的表格生成网站:[Tables Generator](http://www.tablesgenerator.com/)\n\n<!--more-->\n### 属性\n| 属性 | 说明 |\n|:---:|:---:|\n|T|\tTranspose index and columns at\tFast label-based scalar accessor|\n|axes|\tReturn a list with the row axis labels and column axis labels as the only members.|\n|blocks\t|Internal property, property synonym for as_blocks()|\n|dtypes\t|Return the dtypes in this object.|\n|empty\t|True if NDFrame is entirely empty [no items], meaning any of the axes are of length 0.|\n|ftypes\t|Return the ftypes (indication of sparse/dense and dtype) in this object.|\n|iat|\tFast integer location scalar accessor.|\n|iloc|\tPurely integer-location based indexing for selection by position.|\n|is_copy| |\t\n|ix|\tA primarily label-location based indexer, with integer position fallback.|\n|loc|\tPurely label-location based indexer for selection by label.|\n|ndim|\tNumber of axes / array dimensions|\n|shape|\tReturn a tuple representing the dimensionality of the DataFrame.|\n|size|\tnumber of elements in the NDFrame|\n|style|\tProperty returning a Styler object containing methods for building a styled HTML representation fo the DataFrame.|\n|values|\tNumpy representation of NDFrame|\n\n### 函数\n| 函数名                                            | 作用                                                                                                                                                                                                                               |\n|---------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| abs()                                             | Return an object with absolute value taken�only applicable to objects that are all numeric.                                                                                                                                        |\n| add(other[,?axis,?level,?fill_value])             | Addition of dataframe and other, element-wise (binary operator?add).                                                                                                                                                               |\n| add_prefix(prefix)                                | Concatenate prefix string with panel items names.                                                                                                                                                                                  |\n| add_suffix(suffix)                                | Concatenate suffix string with panel items names.                                                                                                                                                                                  |\n| align(other[,?join,?axis,?level,?copy,?...])      | Align two object on their axes with the                                                                                                                                                                                            |\n| all([axis,?bool_only,?skipna,?level])             | Return whether all elements are True over requested axis                                                                                                                                                                           |\n| any([axis,?bool_only,?skipna,?level])             | Return whether any element is True over requested axis                                                                                                                                                                             |\n| append(other[,?ignore_index,?verify_integrity])   | Append rows of?other?to the end of this frame, returning a new object.                                                                                                                                                             |\n| apply(func[,?axis,?broadcast,?raw,?reduce,?args]) | Applies function along input axis of DataFrame.                                                                                                                                                                                    |\n| applymap(func)                                    | Apply a function to a DataFrame that is intended to operate elementwise, i.e.                                                                                                                                                      |\n| as_blocks([copy])                                 | Convert the frame to a dict of dtype -> Constructor Types that each has a homogeneous dtype.                                                                                                                                       |\n| as_matrix([columns])                              | Convert the frame to its Numpy-array representation.                                                                                                                                                                               |\n| asfreq(freq[,?method,?how,?normalize])            | Convert TimeSeries to specified frequency.                                                                                                                                                                                         |\n| asof(where[,?subset])                             | The last row without any NaN is taken (or the last row without                                                                                                                                                                     |\n| assign(\\*\\*kwargs)                                | Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.                                                                                                      |\n| astype(dtype[,?copy,?raise_on_error])             | Cast object to input numpy.dtype                                                                                                                                                                                                   |\n| at_time(time[,?asof])                             | Select values at particular time of day (e.g.                                                                                                                                                                                      |\n| between_time(start_time,?end_time[,?...])         | Select values between particular times of the day (e.g., 9:00-9:30 AM).                                                                                                                                                            |\n| bfill([axis,?inplace,?limit,?downcast])           | Synonym for NDFrame.fillna(method=’bfill’)                                                                                                                                                                                         |\n| bool()                                            | Return the bool of a single element PandasObject.                                                                                                                                                                                  |\n| boxplot([column,?by,?ax,?fontsize,?rot,?...])     | Make a box plot from DataFrame column optionally grouped by some columns or                                                                                                                                                        |\n| clip([lower,?upper,?axis])                        | Trim values at input threshold(s).                                                                                                                                                                                                 |\n| clip_lower(threshold[,?axis])                     | Return copy of the input with values below given value(s) truncated.                                                                                                                                                               |\n| clip_upper(threshold[,?axis])                     | Return copy of input with values above given value(s) truncated.                                                                                                                                                                   |\n| combine(other,?func[,?fill_value,?overwrite])     | Add two DataFrame objects and do not propagate NaN values, so if for a                                                                                                                                                             |\n| combineAdd(other)                                 | DEPRECATED.                                                                                                                                                                                                                        |\n| combineMult(other)                                | DEPRECATED.                                                                                                                                                                                                                        |\n| combine_first(other)                              | Combine two DataFrame objects and default to non-null values in frame calling the method.                                                                                                                                          |\n| compound([axis,?skipna,?level])                   | Return the compound percentage of the values for the requested axis                                                                                                                                                                |\n| consolidate([inplace])                            | Compute NDFrame with “consolidated” internals (data of each dtype grouped together in a single ndarray).                                                                                                                           |\n| convert_objects([convert_dates,?...])             | Deprecated.                                                                                                                                                                                                                        |\n| copy([deep])                                      | Make a copy of this objects data.                                                                                                                                                                                                  |\n| corr([method,?min_periods])                       | Compute pairwise correlation of columns, excluding NA/null values                                                                                                                                                                  |\n| corrwith(other[,?axis,?drop])                     | Compute pairwise correlation between rows or columns of two DataFrame objects.                                                                                                                                                     |\n| count([axis,?level,?numeric_only])                | Return Series with number of non-NA/null observations over requested axis.                                                                                                                                                         |\n| cov([min_periods])                                | Compute pairwise covariance of columns, excluding NA/null values                                                                                                                                                                   |\n| cummax([axis,?skipna])                            | Return cumulative max over requested axis.                                                                                                                                                                                         |\n| cummin([axis,?skipna])                            | Return cumulative minimum over requested axis.                                                                                                                                                                                     |\n| cumprod([axis,?skipna])                           | Return cumulative product over requested axis.                                                                                                                                                                                     |\n| cumsum([axis,?skipna])                            | Return cumulative sum over requested axis.                                                                                                                                                                                         |\n| describe([percentiles,?include,?exclude])         | Generate various summary statistics, excluding NaN values.                                                                                                                                                                         |\n| diff([periods,?axis])                             | 1st discrete difference of object                                                                                                                                                                                                  |\n| div(other[,?axis,?level,?fill_value])             | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| divide(other[,?axis,?level,?fill_value])          | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| dot(other)                                        | Matrix multiplication with DataFrame or Series objects                                                                                                                                                                             |\n| drop(labels[,?axis,?level,?inplace,?errors])      | Return new object with labels in requested axis removed.                                                                                                                                                                           |\n| drop_duplicates(\\*args,?\\*\\*kwargs)               | Return DataFrame with duplicate rows removed, optionally only                                                                                                                                                                      |\n| dropna([axis,?how,?thresh,?subset,?inplace])      | Return object with labels on given axis omitted where alternately any                                                                                                                                                              |\n| duplicated(\\*args,?\\*\\*kwargs)                    | Return boolean Series denoting duplicate rows, optionally only                                                                                                                                                                     |\n| eq(other[,?axis,?level])                          | Wrapper for flexible comparison methods eq                                                                                                                                                                                         |\n| equals(other)                                     | Determines if two NDFrame objects contain the same elements.                                                                                                                                                                       |\n| eval(expr[,?inplace])                             | Evaluate an expression in the context of the calling DataFrame instance.                                                                                                                                                           |\n| ewm([com,?span,?halflife,?alpha,?...])            | Provides exponential weighted functions                                                                                                                                                                                            |\n| expanding([min_periods,?freq,?center,?axis])      | Provides expanding transformations.                                                                                                                                                                                                |\n| ffill([axis,?inplace,?limit,?downcast])           | Synonym for NDFrame.fillna(method=’ffill’)                                                                                                                                                                                         |\n| fillna([value,?method,?axis,?inplace,?...])       | Fill NA/NaN values using the specified method                                                                                                                                                                                      |\n| filter([items,?like,?regex,?axis])                | Subset rows or columns of dataframe according to labels in the specified index.                                                                                                                                                    |\n| first(offset)                                     | Convenience method for subsetting initial periods of time series data based on a date offset.                                                                                                                                      |\n| first_valid_index()                               | Return label for first non-NA/null value                                                                                                                                                                                           |\n| floordiv(other[,?axis,?level,?fill_value])        | Integer division of dataframe and other, element-wise (binary operator?floordiv).                                                                                                                                                  |\n| from_csv(path[,?header,?sep,?index_col,?...])     | Read CSV file (DISCOURAGED, please use?pandas.read_csv()?instead).                                                                                                                                                                 |\n| from_dict(data[,?orient,?dtype])                  | Construct DataFrame from dict of array-like or dicts                                                                                                                                                                               |\n| from_items(items[,?columns,?orient])              | Convert (key, value) pairs to DataFrame.                                                                                                                                                                                           |\n| from_records(data[,?index,?exclude,?...])         | Convert structured or record ndarray to DataFrame                                                                                                                                                                                  |\n| ge(other[,?axis,?level])                          | Wrapper for flexible comparison methods ge                                                                                                                                                                                         |\n| get(key[,?default])                               | Get item from object for given key (DataFrame column, Panel slice, etc.).                                                                                                                                                          |\n| get_dtype_counts()                                | Return the counts of dtypes in this object.                                                                                                                                                                                        |\n| get_ftype_counts()                                | Return the counts of ftypes in this object.                                                                                                                                                                                        |\n| get_value(index,?col[,?takeable])                 | Quickly retrieve single value at passed column and index                                                                                                                                                                           |\n| get_values()                                      | same as values (but handles sparseness conversions)                                                                                                                                                                                |\n| groupby([by,?axis,?level,?as_index,?sort,?...])   | Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.                                                                                                |\n| gt(other[,?axis,?level])                          | Wrapper for flexible comparison methods gt                                                                                                                                                                                         |\n| head([n])                                         | Returns first n rows                                                                                                                                                                                                               |\n| hist(data[,?column,?by,?grid,?xlabelsize,?...])   | Draw histogram of the DataFrame’s series using matplotlib / pylab.                                                                                                                                                                 |\n| icol(i)                                           | DEPRECATED.                                                                                                                                                                                                                        |\n| idxmax([axis,?skipna])                            | Return index of first occurrence of maximum over requested axis.                                                                                                                                                                   |\n| idxmin([axis,?skipna])                            | Return index of first occurrence of minimum over requested axis.                                                                                                                                                                   |\n| iget_value(i,?j)                                  | DEPRECATED.                                                                                                                                                                                                                        |\n| info([verbose,?buf,?max_cols,?memory_usage,?...]) | Concise summary of a DataFrame.                                                                                                                                                                                                    |\n| insert(loc,?column,?value[,?allow_duplicates])    | Insert column into DataFrame at specified location.                                                                                                                                                                                |\n| interpolate([method,?axis,?limit,?inplace,?...])  | Interpolate values according to different methods.                                                                                                                                                                                 |\n| irow(i[,?copy])                                   | DEPRECATED.                                                                                                                                                                                                                        |\n| isin(values)                                      | Return boolean DataFrame showing whether each element in the DataFrame is contained in values.                                                                                                                                     |\n| isnull()                                          | Return a boolean same-sized object indicating if the values are null.                                                                                                                                                              |\n| iteritems()                                       | Iterator over (column name, Series) pairs.                                                                                                                                                                                         |\n| iterkv(\\*args,?\\*\\*kwargs)                        | iteritems alias used to get around 2to3. Deprecated                                                                                                                                                                                |\n| iterrows()                                        | Iterate over DataFrame rows as (index, Series) pairs.                                                                                                                                                                              |\n| itertuples([index,?name])                         | Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.                                                                                                                                        |\n| join(other[,?on,?how,?lsuffix,?rsuffix,?sort])    | Join columns with other DataFrame either on index or on a key column.                                                                                                                                                              |\n| keys()                                            | Get the ‘info axis’ (see Indexing for more)                                                                                                                                                                                        |\n| kurt([axis,?skipna,?level,?numeric_only])         | Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).                                                                                                                    |\n| kurtosis([axis,?skipna,?level,?numeric_only])     | Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).                                                                                                                    |\n| last(offset)                                      | Convenience method for subsetting final periods of time series data based on a date offset.                                                                                                                                        |\n| last_valid_index()                                | Return label for last non-NA/null value                                                                                                                                                                                            |\n| le(other[,?axis,?level])                          | Wrapper for flexible comparison methods le                                                                                                                                                                                         |\n| lookup(row_labels,?col_labels)                    | Label-based “fancy indexing” function for DataFrame.                                                                                                                                                                               |\n| lt(other[,?axis,?level])                          | Wrapper for flexible comparison methods lt                                                                                                                                                                                         |\n| mad([axis,?skipna,?level])                        | Return the mean absolute deviation of the values for the requested axis                                                                                                                                                            |\n| mask(cond[,?other,?inplace,?axis,?level,?...])    | Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.                                                                                             |\n| max([axis,?skipna,?level,?numeric_only])          | This method returns the maximum of the values in the object.                                                                                                                                                                       |\n| mean([axis,?skipna,?level,?numeric_only])         | Return the mean of the values for the requested axis                                                                                                                                                                               |\n| median([axis,?skipna,?level,?numeric_only])       | Return the median of the values for the requested axis                                                                                                                                                                             |\n| memory_usage([index,?deep])                       | Memory usage of DataFrame columns.                                                                                                                                                                                                 |\n| merge(right[,?how,?on,?left_on,?right_on,?...])   | Merge DataFrame objects by performing a database-style join operation by columns or indexes.                                                                                                                                       |\n| min([axis,?skipna,?level,?numeric_only])          | This method returns the minimum of the values in the object.                                                                                                                                                                       |\n| mod(other[,?axis,?level,?fill_value])             | Modulo of dataframe and other, element-wise (binary operator?mod).                                                                                                                                                                 |\n| mode([axis,?numeric_only])                        | Gets the mode(s) of each element along the axis selected.                                                                                                                                                                          |\n| mul(other[,?axis,?level,?fill_value])             | Multiplication of dataframe and other, element-wise (binary operator?mul).                                                                                                                                                         |\n| multiply(other[,?axis,?level,?fill_value])        | Multiplication of dataframe and other, element-wise (binary operator?mul).                                                                                                                                                         |\n| ne(other[,?axis,?level])                          | Wrapper for flexible comparison methods ne                                                                                                                                                                                         |\n| nlargest(n,?columns[,?keep])                      | Get the rows of a DataFrame sorted by the?n?largest values of?columns.                                                                                                                                                             |\n| notnull()                                         | Return a boolean same-sized object indicating if the values are not null.                                                                                                                                                          |\n| nsmallest(n,?columns[,?keep])                     | Get the rows of a DataFrame sorted by the?n?smallest values of?columns.                                                                                                                                                            |\n| pct_change([periods,?fill_method,?limit,?freq])   | Percent change over given number of periods.                                                                                                                                                                                       |\n| pipe(func,?\\*args,?\\*\\*kwargs)                    | Apply func(self, *args, **kwargs)                                                                                                                                                                                                  |\n| pivot([index,?columns,?values])                   | Reshape data (produce a “pivot” table) based on column values.                                                                                                                                                                     |\n| pivot_table(data[,?values,?index,?columns,?...])  | Create a spreadsheet-style pivot table as a DataFrame.                                                                                                                                                                             |\n| plot                                              | alias of?FramePlotMethods                                                                                                                                                                                                          |\n| pop(item)                                         | Return item and drop from frame.                                                                                                                                                                                                   |\n| pow(other[,?axis,?level,?fill_value])             | Exponential power of dataframe and other, element-wise (binary operator?pow).                                                                                                                                                      |\n| prod([axis,?skipna,?level,?numeric_only])         | Return the product of the values for the requested axis                                                                                                                                                                            |\n| product([axis,?skipna,?level,?numeric_only])      | Return the product of the values for the requested axis                                                                                                                                                                            |\n| quantile([q,?axis,?numeric_only,?interpolation])  | Return values at the given quantile over requested axis, a la numpy.percentile.                                                                                                                                                    |\n| query(expr[,?inplace])                            | Query the columns of a frame with a boolean expression.                                                                                                                                                                            |\n| radd(other[,?axis,?level,?fill_value])            | Addition of dataframe and other, element-wise (binary operator?radd).                                                                                                                                                              |\n| rank([axis,?method,?numeric_only,?...])           | Compute numerical data ranks (1 through n) along axis.                                                                                                                                                                             |\n| rdiv(other[,?axis,?level,?fill_value])            | Floating division of dataframe and other, element-wise (binary operator?rtruediv).                                                                                                                                                 |\n| reindex([index,?columns])                         | Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.                                                                                                     |\n| reindex_axis(labels[,?axis,?method,?level,?...])  | Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.                                                                                                  |\n| reindex_like(other[,?method,?copy,?limit,?...])   | Return an object with matching indices to myself.                                                                                                                                                                                  |\n| rename([index,?columns])                          | Alter axes input function or functions.                                                                                                                                                                                            |\n| rename_axis(mapper[,?axis,?copy,?inplace])        | Alter index and / or columns using input function or functions.                                                                                                                                                                    |\n| reorder_levels(order[,?axis])                     | Rearrange index levels using input order.                                                                                                                                                                                          |\n| replace([to_replace,?value,?inplace,?limit,?...]) | Replace values given in ‘to_replace’ with ‘value’.                                                                                                                                                                                 |\n| resample(rule[,?how,?axis,?fill_method,?...])     | Convenience method for frequency conversion and resampling of time series.                                                                                                                                                         |\n| reset_index([level,?drop,?inplace,?...])          | For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to ‘level_0’, ‘level_1’, etc.                                                                |\n| rfloordiv(other[,?axis,?level,?fill_value])       | Integer division of dataframe and other, element-wise (binary operator?rfloordiv).                                                                                                                                                 |\n| rmod(other[,?axis,?level,?fill_value])            | Modulo of dataframe and other, element-wise (binary operator?rmod).                                                                                                                                                                |\n| rmul(other[,?axis,?level,?fill_value])            | Multiplication of dataframe and other, element-wise (binary operator?rmul).                                                                                                                                                        |\n| rolling(window[,?min_periods,?freq,?center,?...]) | Provides rolling window calculcations.                                                                                                                                                                                             |\n| round([decimals])                                 | Round a DataFrame to a variable number of decimal places.                                                                                                                                                                          |\n| rpow(other[,?axis,?level,?fill_value])            | Exponential power of dataframe and other, element-wise (binary operator?rpow).                                                                                                                                                     |\n| rsub(other[,?axis,?level,?fill_value])            | Subtraction of dataframe and other, element-wise (binary operator?rsub).                                                                                                                                                           |\n| rtruediv(other[,?axis,?level,?fill_value])        | Floating division of dataframe and other, element-wise (binary operator?rtruediv).                                                                                                                                                 |\n| sample([n,?frac,?replace,?weights,?...])          | Returns a random sample of items from an axis of object.                                                                                                                                                                           |\n| select(crit[,?axis])                              | Return data corresponding to axis labels matching criteria                                                                                                                                                                         |\n| select_dtypes([include,?exclude])                 | Return a subset of a DataFrame including/excluding columns based on their?dtype.                                                                                                                                                   |\n| sem([axis,?skipna,?level,?ddof,?numeric_only])    | Return unbiased standard error of the mean over requested axis.                                                                                                                                                                    |\n| set_axis(axis,?labels)                            | public verson of axis assignment                                                                                                                                                                                                   |\n| set_index(keys[,?drop,?append,?inplace,?...])     | Set the DataFrame index (row labels) using one or more existing columns.                                                                                                                                                           |\n| set_value(index,?col,?value[,?takeable])          | Put single value at passed column and index                                                                                                                                                                                        |\n| shift([periods,?freq,?axis])                      | Shift index by desired number of periods with an optional time freq                                                                                                                                                                |\n| skew([axis,?skipna,?level,?numeric_only])         | Return unbiased skew over requested axis                                                                                                                                                                                           |\n| slice_shift([periods,?axis])                      | Equivalent to?shift?without copying data.                                                                                                                                                                                          |\n| sort([columns,?axis,?ascending,?inplace,?...])    | DEPRECATED: use?DataFrame.sort_values()                                                                                                                                                                                            |\n| sort_index([axis,?level,?ascending,?...])         | Sort object by labels (along an axis)                                                                                                                                                                                              |\n| sort_values(by[,?axis,?ascending,?inplace,?...])  | Sort by the values along either axis                                                                                                                                                                                               |\n| sortlevel([level,?axis,?ascending,?inplace,?...]) | Sort multilevel index by chosen axis and primary level.                                                                                                                                                                            |\n| squeeze(\\*\\*kwargs)                               | Squeeze length 1 dimensions.                                                                                                                                                                                                       |\n| stack([level,?dropna])                            | Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels. |\n| std([axis,?skipna,?level,?ddof,?numeric_only])    | Return sample standard deviation over requested axis.                                                                                                                                                                              |\n| sub(other[,?axis,?level,?fill_value])             | Subtraction of dataframe and other, element-wise (binary operator?sub).                                                                                                                                                            |\n| subtract(other[,?axis,?level,?fill_value])        | Subtraction of dataframe and other, element-wise (binary operator?sub).                                                                                                                                                            |\n| sum([axis,?skipna,?level,?numeric_only])          | Return the sum of the values for the requested axis                                                                                                                                                                                |\n| swapaxes(axis1,?axis2[,?copy])                    | Interchange axes and swap values axes appropriately                                                                                                                                                                                |\n| swaplevel([i,?j,?axis])                           | Swap levels i and j in a MultiIndex on a particular axis                                                                                                                                                                           |\n| tail([n])                                         | Returns last n rows                                                                                                                                                                                                                |\n| take(indices[,?axis,?convert,?is_copy])           | Analogous to ndarray.take                                                                                                                                                                                                          |\n| to_clipboard([excel,?sep])                        | Attempt to write text representation of object to the system clipboard This can be pasted into Excel, for example.                                                                                                                 |\n| to_csv([path_or_buf,?sep,?na_rep,?...])           | Write DataFrame to a comma-separated values (csv) file                                                                                                                                                                             |\n| to_dense()                                        | Return dense representation of NDFrame (as opposed to sparse)                                                                                                                                                                      |\n| to_dict([orient])                                 | Convert DataFrame to dictionary.                                                                                                                                                                                                   |\n| to_excel(excel_writer[,?sheet_name,?na_rep,?...]) | Write DataFrame to a excel sheet                                                                                                                                                                                                   |\n| to_gbq(destination_table,?project_id[,?...])      | Write a DataFrame to a Google BigQuery table.                                                                                                                                                                                      |\n| to_hdf(path_or_buf,?key,?\\*\\*kwargs)              | Write the contained data to an HDF5 file using HDFStore.                                                                                                                                                                           |\n| to_html([buf,?columns,?col_space,?header,?...])   | Render a DataFrame as an HTML table.                                                                                                                                                                                               |\n| to_json([path_or_buf,?orient,?date_format,?...])  | Convert the object to a JSON string.                                                                                                                                                                                               |\n| to_latex([buf,?columns,?col_space,?header,?...])  | Render a DataFrame to a tabular environment table.                                                                                                                                                                                 |\n| to_msgpack([path_or_buf,?encoding])               | msgpack (serialize) object to input file path                                                                                                                                                                                      |\n| to_panel()                                        | Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.                                                                                                                                                          |\n| to_period([freq,?axis,?copy])                     | Convert DataFrame from DatetimeIndex to PeriodIndex with desired                                                                                                                                                                   |\n| to_pickle(path)                                   | Pickle (serialize) object to input file path.                                                                                                                                                                                      |\n| to_records([index,?convert_datetime64])           | Convert DataFrame to record array.                                                                                                                                                                                                 |\n| to_sparse([fill_value,?kind])                     | Convert to SparseDataFrame                                                                                                                                                                                                         |\n| to_sql(name,?con[,?flavor,?schema,?...])          | Write records stored in a DataFrame to a SQL database.                                                                                                                                                                             |\n| to_stata(fname[,?convert_dates,?...])             | A class for writing Stata binary dta files from array-like objects                                                                                                                                                                 |\n| to_string([buf,?columns,?col_space,?header,?...]) | Render a DataFrame to a console-friendly tabular output.                                                                                                                                                                           |\n| to_timestamp([freq,?how,?axis,?copy])             | Cast to DatetimeIndex of timestamps, at?beginning?of period                                                                                                                                                                        |\n| to_xarray()                                       | Return an xarray object from the pandas object.                                                                                                                                                                                    |\n| transpose(\\*args,?\\*\\*kwargs)                     | Transpose index and columns                                                                                                                                                                                                        |\n| truediv(other[,?axis,?level,?fill_value])         | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| truncate([before,?after,?axis,?copy])             | Truncates a sorted NDFrame before and/or after some particular index value.                                                                                                                                                        |\n| tshift([periods,?freq,?axis])                     | Shift the time index, using the index’s frequency if available.                                                                                                                                                                    |\n| tz_convert(tz[,?axis,?level,?copy])               | Convert tz-aware axis to target time zone.                                                                                                                                                                                         |\n| tz_localize(\\*args,?\\*\\*kwargs)                   | Localize tz-naive TimeSeries to target time zone.                                                                                                                                                                                  |\n| unstack([level,?fill_value])                      | Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.                                               |\n| update(other[,?join,?overwrite,?...])             | Modify DataFrame in place using non-NA values from passed DataFrame.                                                                                                                                                               |\n| var([axis,?skipna,?level,?ddof,?numeric_only])    | Return unbiased variance over requested axis.                                                                                                                                                                                      |\n| where(cond[,?other,?inplace,?axis,?level,?...])   | Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.                                                                                              |\n| xs(key[,?axis,?level,?drop_level])                | Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.                                                                                                                                                           |","source":"_posts/pandas-func.md","raw":"---\ntitle: DataFrame函数与属性\ndate: 2017-02-04 21:02:51\ntags: [math,machinelearning,python,code]\ncategories: Python\n---\n\n整理了pandas中DataFrame的属性和函数，做成了表格。\n顺便推荐csv转md语法的表格生成网站:[Tables Generator](http://www.tablesgenerator.com/)\n\n<!--more-->\n### 属性\n| 属性 | 说明 |\n|:---:|:---:|\n|T|\tTranspose index and columns at\tFast label-based scalar accessor|\n|axes|\tReturn a list with the row axis labels and column axis labels as the only members.|\n|blocks\t|Internal property, property synonym for as_blocks()|\n|dtypes\t|Return the dtypes in this object.|\n|empty\t|True if NDFrame is entirely empty [no items], meaning any of the axes are of length 0.|\n|ftypes\t|Return the ftypes (indication of sparse/dense and dtype) in this object.|\n|iat|\tFast integer location scalar accessor.|\n|iloc|\tPurely integer-location based indexing for selection by position.|\n|is_copy| |\t\n|ix|\tA primarily label-location based indexer, with integer position fallback.|\n|loc|\tPurely label-location based indexer for selection by label.|\n|ndim|\tNumber of axes / array dimensions|\n|shape|\tReturn a tuple representing the dimensionality of the DataFrame.|\n|size|\tnumber of elements in the NDFrame|\n|style|\tProperty returning a Styler object containing methods for building a styled HTML representation fo the DataFrame.|\n|values|\tNumpy representation of NDFrame|\n\n### 函数\n| 函数名                                            | 作用                                                                                                                                                                                                                               |\n|---------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| abs()                                             | Return an object with absolute value taken�only applicable to objects that are all numeric.                                                                                                                                        |\n| add(other[,?axis,?level,?fill_value])             | Addition of dataframe and other, element-wise (binary operator?add).                                                                                                                                                               |\n| add_prefix(prefix)                                | Concatenate prefix string with panel items names.                                                                                                                                                                                  |\n| add_suffix(suffix)                                | Concatenate suffix string with panel items names.                                                                                                                                                                                  |\n| align(other[,?join,?axis,?level,?copy,?...])      | Align two object on their axes with the                                                                                                                                                                                            |\n| all([axis,?bool_only,?skipna,?level])             | Return whether all elements are True over requested axis                                                                                                                                                                           |\n| any([axis,?bool_only,?skipna,?level])             | Return whether any element is True over requested axis                                                                                                                                                                             |\n| append(other[,?ignore_index,?verify_integrity])   | Append rows of?other?to the end of this frame, returning a new object.                                                                                                                                                             |\n| apply(func[,?axis,?broadcast,?raw,?reduce,?args]) | Applies function along input axis of DataFrame.                                                                                                                                                                                    |\n| applymap(func)                                    | Apply a function to a DataFrame that is intended to operate elementwise, i.e.                                                                                                                                                      |\n| as_blocks([copy])                                 | Convert the frame to a dict of dtype -> Constructor Types that each has a homogeneous dtype.                                                                                                                                       |\n| as_matrix([columns])                              | Convert the frame to its Numpy-array representation.                                                                                                                                                                               |\n| asfreq(freq[,?method,?how,?normalize])            | Convert TimeSeries to specified frequency.                                                                                                                                                                                         |\n| asof(where[,?subset])                             | The last row without any NaN is taken (or the last row without                                                                                                                                                                     |\n| assign(\\*\\*kwargs)                                | Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.                                                                                                      |\n| astype(dtype[,?copy,?raise_on_error])             | Cast object to input numpy.dtype                                                                                                                                                                                                   |\n| at_time(time[,?asof])                             | Select values at particular time of day (e.g.                                                                                                                                                                                      |\n| between_time(start_time,?end_time[,?...])         | Select values between particular times of the day (e.g., 9:00-9:30 AM).                                                                                                                                                            |\n| bfill([axis,?inplace,?limit,?downcast])           | Synonym for NDFrame.fillna(method=’bfill’)                                                                                                                                                                                         |\n| bool()                                            | Return the bool of a single element PandasObject.                                                                                                                                                                                  |\n| boxplot([column,?by,?ax,?fontsize,?rot,?...])     | Make a box plot from DataFrame column optionally grouped by some columns or                                                                                                                                                        |\n| clip([lower,?upper,?axis])                        | Trim values at input threshold(s).                                                                                                                                                                                                 |\n| clip_lower(threshold[,?axis])                     | Return copy of the input with values below given value(s) truncated.                                                                                                                                                               |\n| clip_upper(threshold[,?axis])                     | Return copy of input with values above given value(s) truncated.                                                                                                                                                                   |\n| combine(other,?func[,?fill_value,?overwrite])     | Add two DataFrame objects and do not propagate NaN values, so if for a                                                                                                                                                             |\n| combineAdd(other)                                 | DEPRECATED.                                                                                                                                                                                                                        |\n| combineMult(other)                                | DEPRECATED.                                                                                                                                                                                                                        |\n| combine_first(other)                              | Combine two DataFrame objects and default to non-null values in frame calling the method.                                                                                                                                          |\n| compound([axis,?skipna,?level])                   | Return the compound percentage of the values for the requested axis                                                                                                                                                                |\n| consolidate([inplace])                            | Compute NDFrame with “consolidated” internals (data of each dtype grouped together in a single ndarray).                                                                                                                           |\n| convert_objects([convert_dates,?...])             | Deprecated.                                                                                                                                                                                                                        |\n| copy([deep])                                      | Make a copy of this objects data.                                                                                                                                                                                                  |\n| corr([method,?min_periods])                       | Compute pairwise correlation of columns, excluding NA/null values                                                                                                                                                                  |\n| corrwith(other[,?axis,?drop])                     | Compute pairwise correlation between rows or columns of two DataFrame objects.                                                                                                                                                     |\n| count([axis,?level,?numeric_only])                | Return Series with number of non-NA/null observations over requested axis.                                                                                                                                                         |\n| cov([min_periods])                                | Compute pairwise covariance of columns, excluding NA/null values                                                                                                                                                                   |\n| cummax([axis,?skipna])                            | Return cumulative max over requested axis.                                                                                                                                                                                         |\n| cummin([axis,?skipna])                            | Return cumulative minimum over requested axis.                                                                                                                                                                                     |\n| cumprod([axis,?skipna])                           | Return cumulative product over requested axis.                                                                                                                                                                                     |\n| cumsum([axis,?skipna])                            | Return cumulative sum over requested axis.                                                                                                                                                                                         |\n| describe([percentiles,?include,?exclude])         | Generate various summary statistics, excluding NaN values.                                                                                                                                                                         |\n| diff([periods,?axis])                             | 1st discrete difference of object                                                                                                                                                                                                  |\n| div(other[,?axis,?level,?fill_value])             | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| divide(other[,?axis,?level,?fill_value])          | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| dot(other)                                        | Matrix multiplication with DataFrame or Series objects                                                                                                                                                                             |\n| drop(labels[,?axis,?level,?inplace,?errors])      | Return new object with labels in requested axis removed.                                                                                                                                                                           |\n| drop_duplicates(\\*args,?\\*\\*kwargs)               | Return DataFrame with duplicate rows removed, optionally only                                                                                                                                                                      |\n| dropna([axis,?how,?thresh,?subset,?inplace])      | Return object with labels on given axis omitted where alternately any                                                                                                                                                              |\n| duplicated(\\*args,?\\*\\*kwargs)                    | Return boolean Series denoting duplicate rows, optionally only                                                                                                                                                                     |\n| eq(other[,?axis,?level])                          | Wrapper for flexible comparison methods eq                                                                                                                                                                                         |\n| equals(other)                                     | Determines if two NDFrame objects contain the same elements.                                                                                                                                                                       |\n| eval(expr[,?inplace])                             | Evaluate an expression in the context of the calling DataFrame instance.                                                                                                                                                           |\n| ewm([com,?span,?halflife,?alpha,?...])            | Provides exponential weighted functions                                                                                                                                                                                            |\n| expanding([min_periods,?freq,?center,?axis])      | Provides expanding transformations.                                                                                                                                                                                                |\n| ffill([axis,?inplace,?limit,?downcast])           | Synonym for NDFrame.fillna(method=’ffill’)                                                                                                                                                                                         |\n| fillna([value,?method,?axis,?inplace,?...])       | Fill NA/NaN values using the specified method                                                                                                                                                                                      |\n| filter([items,?like,?regex,?axis])                | Subset rows or columns of dataframe according to labels in the specified index.                                                                                                                                                    |\n| first(offset)                                     | Convenience method for subsetting initial periods of time series data based on a date offset.                                                                                                                                      |\n| first_valid_index()                               | Return label for first non-NA/null value                                                                                                                                                                                           |\n| floordiv(other[,?axis,?level,?fill_value])        | Integer division of dataframe and other, element-wise (binary operator?floordiv).                                                                                                                                                  |\n| from_csv(path[,?header,?sep,?index_col,?...])     | Read CSV file (DISCOURAGED, please use?pandas.read_csv()?instead).                                                                                                                                                                 |\n| from_dict(data[,?orient,?dtype])                  | Construct DataFrame from dict of array-like or dicts                                                                                                                                                                               |\n| from_items(items[,?columns,?orient])              | Convert (key, value) pairs to DataFrame.                                                                                                                                                                                           |\n| from_records(data[,?index,?exclude,?...])         | Convert structured or record ndarray to DataFrame                                                                                                                                                                                  |\n| ge(other[,?axis,?level])                          | Wrapper for flexible comparison methods ge                                                                                                                                                                                         |\n| get(key[,?default])                               | Get item from object for given key (DataFrame column, Panel slice, etc.).                                                                                                                                                          |\n| get_dtype_counts()                                | Return the counts of dtypes in this object.                                                                                                                                                                                        |\n| get_ftype_counts()                                | Return the counts of ftypes in this object.                                                                                                                                                                                        |\n| get_value(index,?col[,?takeable])                 | Quickly retrieve single value at passed column and index                                                                                                                                                                           |\n| get_values()                                      | same as values (but handles sparseness conversions)                                                                                                                                                                                |\n| groupby([by,?axis,?level,?as_index,?sort,?...])   | Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.                                                                                                |\n| gt(other[,?axis,?level])                          | Wrapper for flexible comparison methods gt                                                                                                                                                                                         |\n| head([n])                                         | Returns first n rows                                                                                                                                                                                                               |\n| hist(data[,?column,?by,?grid,?xlabelsize,?...])   | Draw histogram of the DataFrame’s series using matplotlib / pylab.                                                                                                                                                                 |\n| icol(i)                                           | DEPRECATED.                                                                                                                                                                                                                        |\n| idxmax([axis,?skipna])                            | Return index of first occurrence of maximum over requested axis.                                                                                                                                                                   |\n| idxmin([axis,?skipna])                            | Return index of first occurrence of minimum over requested axis.                                                                                                                                                                   |\n| iget_value(i,?j)                                  | DEPRECATED.                                                                                                                                                                                                                        |\n| info([verbose,?buf,?max_cols,?memory_usage,?...]) | Concise summary of a DataFrame.                                                                                                                                                                                                    |\n| insert(loc,?column,?value[,?allow_duplicates])    | Insert column into DataFrame at specified location.                                                                                                                                                                                |\n| interpolate([method,?axis,?limit,?inplace,?...])  | Interpolate values according to different methods.                                                                                                                                                                                 |\n| irow(i[,?copy])                                   | DEPRECATED.                                                                                                                                                                                                                        |\n| isin(values)                                      | Return boolean DataFrame showing whether each element in the DataFrame is contained in values.                                                                                                                                     |\n| isnull()                                          | Return a boolean same-sized object indicating if the values are null.                                                                                                                                                              |\n| iteritems()                                       | Iterator over (column name, Series) pairs.                                                                                                                                                                                         |\n| iterkv(\\*args,?\\*\\*kwargs)                        | iteritems alias used to get around 2to3. Deprecated                                                                                                                                                                                |\n| iterrows()                                        | Iterate over DataFrame rows as (index, Series) pairs.                                                                                                                                                                              |\n| itertuples([index,?name])                         | Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.                                                                                                                                        |\n| join(other[,?on,?how,?lsuffix,?rsuffix,?sort])    | Join columns with other DataFrame either on index or on a key column.                                                                                                                                                              |\n| keys()                                            | Get the ‘info axis’ (see Indexing for more)                                                                                                                                                                                        |\n| kurt([axis,?skipna,?level,?numeric_only])         | Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).                                                                                                                    |\n| kurtosis([axis,?skipna,?level,?numeric_only])     | Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).                                                                                                                    |\n| last(offset)                                      | Convenience method for subsetting final periods of time series data based on a date offset.                                                                                                                                        |\n| last_valid_index()                                | Return label for last non-NA/null value                                                                                                                                                                                            |\n| le(other[,?axis,?level])                          | Wrapper for flexible comparison methods le                                                                                                                                                                                         |\n| lookup(row_labels,?col_labels)                    | Label-based “fancy indexing” function for DataFrame.                                                                                                                                                                               |\n| lt(other[,?axis,?level])                          | Wrapper for flexible comparison methods lt                                                                                                                                                                                         |\n| mad([axis,?skipna,?level])                        | Return the mean absolute deviation of the values for the requested axis                                                                                                                                                            |\n| mask(cond[,?other,?inplace,?axis,?level,?...])    | Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.                                                                                             |\n| max([axis,?skipna,?level,?numeric_only])          | This method returns the maximum of the values in the object.                                                                                                                                                                       |\n| mean([axis,?skipna,?level,?numeric_only])         | Return the mean of the values for the requested axis                                                                                                                                                                               |\n| median([axis,?skipna,?level,?numeric_only])       | Return the median of the values for the requested axis                                                                                                                                                                             |\n| memory_usage([index,?deep])                       | Memory usage of DataFrame columns.                                                                                                                                                                                                 |\n| merge(right[,?how,?on,?left_on,?right_on,?...])   | Merge DataFrame objects by performing a database-style join operation by columns or indexes.                                                                                                                                       |\n| min([axis,?skipna,?level,?numeric_only])          | This method returns the minimum of the values in the object.                                                                                                                                                                       |\n| mod(other[,?axis,?level,?fill_value])             | Modulo of dataframe and other, element-wise (binary operator?mod).                                                                                                                                                                 |\n| mode([axis,?numeric_only])                        | Gets the mode(s) of each element along the axis selected.                                                                                                                                                                          |\n| mul(other[,?axis,?level,?fill_value])             | Multiplication of dataframe and other, element-wise (binary operator?mul).                                                                                                                                                         |\n| multiply(other[,?axis,?level,?fill_value])        | Multiplication of dataframe and other, element-wise (binary operator?mul).                                                                                                                                                         |\n| ne(other[,?axis,?level])                          | Wrapper for flexible comparison methods ne                                                                                                                                                                                         |\n| nlargest(n,?columns[,?keep])                      | Get the rows of a DataFrame sorted by the?n?largest values of?columns.                                                                                                                                                             |\n| notnull()                                         | Return a boolean same-sized object indicating if the values are not null.                                                                                                                                                          |\n| nsmallest(n,?columns[,?keep])                     | Get the rows of a DataFrame sorted by the?n?smallest values of?columns.                                                                                                                                                            |\n| pct_change([periods,?fill_method,?limit,?freq])   | Percent change over given number of periods.                                                                                                                                                                                       |\n| pipe(func,?\\*args,?\\*\\*kwargs)                    | Apply func(self, *args, **kwargs)                                                                                                                                                                                                  |\n| pivot([index,?columns,?values])                   | Reshape data (produce a “pivot” table) based on column values.                                                                                                                                                                     |\n| pivot_table(data[,?values,?index,?columns,?...])  | Create a spreadsheet-style pivot table as a DataFrame.                                                                                                                                                                             |\n| plot                                              | alias of?FramePlotMethods                                                                                                                                                                                                          |\n| pop(item)                                         | Return item and drop from frame.                                                                                                                                                                                                   |\n| pow(other[,?axis,?level,?fill_value])             | Exponential power of dataframe and other, element-wise (binary operator?pow).                                                                                                                                                      |\n| prod([axis,?skipna,?level,?numeric_only])         | Return the product of the values for the requested axis                                                                                                                                                                            |\n| product([axis,?skipna,?level,?numeric_only])      | Return the product of the values for the requested axis                                                                                                                                                                            |\n| quantile([q,?axis,?numeric_only,?interpolation])  | Return values at the given quantile over requested axis, a la numpy.percentile.                                                                                                                                                    |\n| query(expr[,?inplace])                            | Query the columns of a frame with a boolean expression.                                                                                                                                                                            |\n| radd(other[,?axis,?level,?fill_value])            | Addition of dataframe and other, element-wise (binary operator?radd).                                                                                                                                                              |\n| rank([axis,?method,?numeric_only,?...])           | Compute numerical data ranks (1 through n) along axis.                                                                                                                                                                             |\n| rdiv(other[,?axis,?level,?fill_value])            | Floating division of dataframe and other, element-wise (binary operator?rtruediv).                                                                                                                                                 |\n| reindex([index,?columns])                         | Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.                                                                                                     |\n| reindex_axis(labels[,?axis,?method,?level,?...])  | Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.                                                                                                  |\n| reindex_like(other[,?method,?copy,?limit,?...])   | Return an object with matching indices to myself.                                                                                                                                                                                  |\n| rename([index,?columns])                          | Alter axes input function or functions.                                                                                                                                                                                            |\n| rename_axis(mapper[,?axis,?copy,?inplace])        | Alter index and / or columns using input function or functions.                                                                                                                                                                    |\n| reorder_levels(order[,?axis])                     | Rearrange index levels using input order.                                                                                                                                                                                          |\n| replace([to_replace,?value,?inplace,?limit,?...]) | Replace values given in ‘to_replace’ with ‘value’.                                                                                                                                                                                 |\n| resample(rule[,?how,?axis,?fill_method,?...])     | Convenience method for frequency conversion and resampling of time series.                                                                                                                                                         |\n| reset_index([level,?drop,?inplace,?...])          | For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to ‘level_0’, ‘level_1’, etc.                                                                |\n| rfloordiv(other[,?axis,?level,?fill_value])       | Integer division of dataframe and other, element-wise (binary operator?rfloordiv).                                                                                                                                                 |\n| rmod(other[,?axis,?level,?fill_value])            | Modulo of dataframe and other, element-wise (binary operator?rmod).                                                                                                                                                                |\n| rmul(other[,?axis,?level,?fill_value])            | Multiplication of dataframe and other, element-wise (binary operator?rmul).                                                                                                                                                        |\n| rolling(window[,?min_periods,?freq,?center,?...]) | Provides rolling window calculcations.                                                                                                                                                                                             |\n| round([decimals])                                 | Round a DataFrame to a variable number of decimal places.                                                                                                                                                                          |\n| rpow(other[,?axis,?level,?fill_value])            | Exponential power of dataframe and other, element-wise (binary operator?rpow).                                                                                                                                                     |\n| rsub(other[,?axis,?level,?fill_value])            | Subtraction of dataframe and other, element-wise (binary operator?rsub).                                                                                                                                                           |\n| rtruediv(other[,?axis,?level,?fill_value])        | Floating division of dataframe and other, element-wise (binary operator?rtruediv).                                                                                                                                                 |\n| sample([n,?frac,?replace,?weights,?...])          | Returns a random sample of items from an axis of object.                                                                                                                                                                           |\n| select(crit[,?axis])                              | Return data corresponding to axis labels matching criteria                                                                                                                                                                         |\n| select_dtypes([include,?exclude])                 | Return a subset of a DataFrame including/excluding columns based on their?dtype.                                                                                                                                                   |\n| sem([axis,?skipna,?level,?ddof,?numeric_only])    | Return unbiased standard error of the mean over requested axis.                                                                                                                                                                    |\n| set_axis(axis,?labels)                            | public verson of axis assignment                                                                                                                                                                                                   |\n| set_index(keys[,?drop,?append,?inplace,?...])     | Set the DataFrame index (row labels) using one or more existing columns.                                                                                                                                                           |\n| set_value(index,?col,?value[,?takeable])          | Put single value at passed column and index                                                                                                                                                                                        |\n| shift([periods,?freq,?axis])                      | Shift index by desired number of periods with an optional time freq                                                                                                                                                                |\n| skew([axis,?skipna,?level,?numeric_only])         | Return unbiased skew over requested axis                                                                                                                                                                                           |\n| slice_shift([periods,?axis])                      | Equivalent to?shift?without copying data.                                                                                                                                                                                          |\n| sort([columns,?axis,?ascending,?inplace,?...])    | DEPRECATED: use?DataFrame.sort_values()                                                                                                                                                                                            |\n| sort_index([axis,?level,?ascending,?...])         | Sort object by labels (along an axis)                                                                                                                                                                                              |\n| sort_values(by[,?axis,?ascending,?inplace,?...])  | Sort by the values along either axis                                                                                                                                                                                               |\n| sortlevel([level,?axis,?ascending,?inplace,?...]) | Sort multilevel index by chosen axis and primary level.                                                                                                                                                                            |\n| squeeze(\\*\\*kwargs)                               | Squeeze length 1 dimensions.                                                                                                                                                                                                       |\n| stack([level,?dropna])                            | Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels. |\n| std([axis,?skipna,?level,?ddof,?numeric_only])    | Return sample standard deviation over requested axis.                                                                                                                                                                              |\n| sub(other[,?axis,?level,?fill_value])             | Subtraction of dataframe and other, element-wise (binary operator?sub).                                                                                                                                                            |\n| subtract(other[,?axis,?level,?fill_value])        | Subtraction of dataframe and other, element-wise (binary operator?sub).                                                                                                                                                            |\n| sum([axis,?skipna,?level,?numeric_only])          | Return the sum of the values for the requested axis                                                                                                                                                                                |\n| swapaxes(axis1,?axis2[,?copy])                    | Interchange axes and swap values axes appropriately                                                                                                                                                                                |\n| swaplevel([i,?j,?axis])                           | Swap levels i and j in a MultiIndex on a particular axis                                                                                                                                                                           |\n| tail([n])                                         | Returns last n rows                                                                                                                                                                                                                |\n| take(indices[,?axis,?convert,?is_copy])           | Analogous to ndarray.take                                                                                                                                                                                                          |\n| to_clipboard([excel,?sep])                        | Attempt to write text representation of object to the system clipboard This can be pasted into Excel, for example.                                                                                                                 |\n| to_csv([path_or_buf,?sep,?na_rep,?...])           | Write DataFrame to a comma-separated values (csv) file                                                                                                                                                                             |\n| to_dense()                                        | Return dense representation of NDFrame (as opposed to sparse)                                                                                                                                                                      |\n| to_dict([orient])                                 | Convert DataFrame to dictionary.                                                                                                                                                                                                   |\n| to_excel(excel_writer[,?sheet_name,?na_rep,?...]) | Write DataFrame to a excel sheet                                                                                                                                                                                                   |\n| to_gbq(destination_table,?project_id[,?...])      | Write a DataFrame to a Google BigQuery table.                                                                                                                                                                                      |\n| to_hdf(path_or_buf,?key,?\\*\\*kwargs)              | Write the contained data to an HDF5 file using HDFStore.                                                                                                                                                                           |\n| to_html([buf,?columns,?col_space,?header,?...])   | Render a DataFrame as an HTML table.                                                                                                                                                                                               |\n| to_json([path_or_buf,?orient,?date_format,?...])  | Convert the object to a JSON string.                                                                                                                                                                                               |\n| to_latex([buf,?columns,?col_space,?header,?...])  | Render a DataFrame to a tabular environment table.                                                                                                                                                                                 |\n| to_msgpack([path_or_buf,?encoding])               | msgpack (serialize) object to input file path                                                                                                                                                                                      |\n| to_panel()                                        | Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.                                                                                                                                                          |\n| to_period([freq,?axis,?copy])                     | Convert DataFrame from DatetimeIndex to PeriodIndex with desired                                                                                                                                                                   |\n| to_pickle(path)                                   | Pickle (serialize) object to input file path.                                                                                                                                                                                      |\n| to_records([index,?convert_datetime64])           | Convert DataFrame to record array.                                                                                                                                                                                                 |\n| to_sparse([fill_value,?kind])                     | Convert to SparseDataFrame                                                                                                                                                                                                         |\n| to_sql(name,?con[,?flavor,?schema,?...])          | Write records stored in a DataFrame to a SQL database.                                                                                                                                                                             |\n| to_stata(fname[,?convert_dates,?...])             | A class for writing Stata binary dta files from array-like objects                                                                                                                                                                 |\n| to_string([buf,?columns,?col_space,?header,?...]) | Render a DataFrame to a console-friendly tabular output.                                                                                                                                                                           |\n| to_timestamp([freq,?how,?axis,?copy])             | Cast to DatetimeIndex of timestamps, at?beginning?of period                                                                                                                                                                        |\n| to_xarray()                                       | Return an xarray object from the pandas object.                                                                                                                                                                                    |\n| transpose(\\*args,?\\*\\*kwargs)                     | Transpose index and columns                                                                                                                                                                                                        |\n| truediv(other[,?axis,?level,?fill_value])         | Floating division of dataframe and other, element-wise (binary operator?truediv).                                                                                                                                                  |\n| truncate([before,?after,?axis,?copy])             | Truncates a sorted NDFrame before and/or after some particular index value.                                                                                                                                                        |\n| tshift([periods,?freq,?axis])                     | Shift the time index, using the index’s frequency if available.                                                                                                                                                                    |\n| tz_convert(tz[,?axis,?level,?copy])               | Convert tz-aware axis to target time zone.                                                                                                                                                                                         |\n| tz_localize(\\*args,?\\*\\*kwargs)                   | Localize tz-naive TimeSeries to target time zone.                                                                                                                                                                                  |\n| unstack([level,?fill_value])                      | Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.                                               |\n| update(other[,?join,?overwrite,?...])             | Modify DataFrame in place using non-NA values from passed DataFrame.                                                                                                                                                               |\n| var([axis,?skipna,?level,?ddof,?numeric_only])    | Return unbiased variance over requested axis.                                                                                                                                                                                      |\n| where(cond[,?other,?inplace,?axis,?level,?...])   | Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.                                                                                              |\n| xs(key[,?axis,?level,?drop_level])                | Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.                                                                                                                                                           |","slug":"pandas-func","published":1,"updated":"2018-07-23T01:28:38.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmd072dk002xqcw6fhwjjsvj","content":"<p>整理了pandas中DataFrame的属性和函数，做成了表格。<br>顺便推荐csv转md语法的表格生成网站:<a href=\"http://www.tablesgenerator.com/\" target=\"_blank\" rel=\"noopener\">Tables Generator</a></p><a id=\"more\"></a><h3 id=\"属性\"><a href=\"#属性\" class=\"headerlink\" title=\"属性\"></a>属性</h3><table><thead><tr><th style=\"text-align:center\">属性</th><th style=\"text-align:center\">说明</th></tr></thead><tbody><tr><td style=\"text-align:center\">T</td><td style=\"text-align:center\">Transpose index and columns at Fast label-based scalar accessor</td></tr><tr><td style=\"text-align:center\">axes</td><td style=\"text-align:center\">Return a list with the row axis labels and column axis labels as the only members.</td></tr><tr><td style=\"text-align:center\">blocks</td><td style=\"text-align:center\">Internal property, property synonym for as_blocks()</td></tr><tr><td style=\"text-align:center\">dtypes</td><td style=\"text-align:center\">Return the dtypes in this object.</td></tr><tr><td style=\"text-align:center\">empty</td><td style=\"text-align:center\">True if NDFrame is entirely empty [no items], meaning any of the axes are of length 0.</td></tr><tr><td style=\"text-align:center\">ftypes</td><td style=\"text-align:center\">Return the ftypes (indication of sparse/dense and dtype) in this object.</td></tr><tr><td style=\"text-align:center\">iat</td><td style=\"text-align:center\">Fast integer location scalar accessor.</td></tr><tr><td style=\"text-align:center\">iloc</td><td style=\"text-align:center\">Purely integer-location based indexing for selection by position.</td></tr><tr><td style=\"text-align:center\">is_copy</td><td style=\"text-align:center\"></td></tr><tr><td style=\"text-align:center\">ix</td><td style=\"text-align:center\">A primarily label-location based indexer, with integer position fallback.</td></tr><tr><td style=\"text-align:center\">loc</td><td style=\"text-align:center\">Purely label-location based indexer for selection by label.</td></tr><tr><td style=\"text-align:center\">ndim</td><td style=\"text-align:center\">Number of axes / array dimensions</td></tr><tr><td style=\"text-align:center\">shape</td><td style=\"text-align:center\">Return a tuple representing the dimensionality of the DataFrame.</td></tr><tr><td style=\"text-align:center\">size</td><td style=\"text-align:center\">number of elements in the NDFrame</td></tr><tr><td style=\"text-align:center\">style</td><td style=\"text-align:center\">Property returning a Styler object containing methods for building a styled HTML representation fo the DataFrame.</td></tr><tr><td style=\"text-align:center\">values</td><td style=\"text-align:center\">Numpy representation of NDFrame</td></tr></tbody></table><h3 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h3><table><thead><tr><th>函数名</th><th>作用</th></tr></thead><tbody><tr><td>abs()</td><td>Return an object with absolute value taken�only applicable to objects that are all numeric.</td></tr><tr><td>add(other[,?axis,?level,?fill_value])</td><td>Addition of dataframe and other, element-wise (binary operator?add).</td></tr><tr><td>add_prefix(prefix)</td><td>Concatenate prefix string with panel items names.</td></tr><tr><td>add_suffix(suffix)</td><td>Concatenate suffix string with panel items names.</td></tr><tr><td>align(other[,?join,?axis,?level,?copy,?…])</td><td>Align two object on their axes with the</td></tr><tr><td>all([axis,?bool_only,?skipna,?level])</td><td>Return whether all elements are True over requested axis</td></tr><tr><td>any([axis,?bool_only,?skipna,?level])</td><td>Return whether any element is True over requested axis</td></tr><tr><td>append(other[,?ignore_index,?verify_integrity])</td><td>Append rows of?other?to the end of this frame, returning a new object.</td></tr><tr><td>apply(func[,?axis,?broadcast,?raw,?reduce,?args])</td><td>Applies function along input axis of DataFrame.</td></tr><tr><td>applymap(func)</td><td>Apply a function to a DataFrame that is intended to operate elementwise, i.e.</td></tr><tr><td>as_blocks([copy])</td><td>Convert the frame to a dict of dtype -&gt; Constructor Types that each has a homogeneous dtype.</td></tr><tr><td>as_matrix([columns])</td><td>Convert the frame to its Numpy-array representation.</td></tr><tr><td>asfreq(freq[,?method,?how,?normalize])</td><td>Convert TimeSeries to specified frequency.</td></tr><tr><td>asof(where[,?subset])</td><td>The last row without any NaN is taken (or the last row without</td></tr><tr><td>assign(**kwargs)</td><td>Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.</td></tr><tr><td>astype(dtype[,?copy,?raise_on_error])</td><td>Cast object to input numpy.dtype</td></tr><tr><td>at_time(time[,?asof])</td><td>Select values at particular time of day (e.g.</td></tr><tr><td>between_time(start_time,?end_time[,?…])</td><td>Select values between particular times of the day (e.g., 9:00-9:30 AM).</td></tr><tr><td>bfill([axis,?inplace,?limit,?downcast])</td><td>Synonym for NDFrame.fillna(method=’bfill’)</td></tr><tr><td>bool()</td><td>Return the bool of a single element PandasObject.</td></tr><tr><td>boxplot([column,?by,?ax,?fontsize,?rot,?…])</td><td>Make a box plot from DataFrame column optionally grouped by some columns or</td></tr><tr><td>clip([lower,?upper,?axis])</td><td>Trim values at input threshold(s).</td></tr><tr><td>clip_lower(threshold[,?axis])</td><td>Return copy of the input with values below given value(s) truncated.</td></tr><tr><td>clip_upper(threshold[,?axis])</td><td>Return copy of input with values above given value(s) truncated.</td></tr><tr><td>combine(other,?func[,?fill_value,?overwrite])</td><td>Add two DataFrame objects and do not propagate NaN values, so if for a</td></tr><tr><td>combineAdd(other)</td><td>DEPRECATED.</td></tr><tr><td>combineMult(other)</td><td>DEPRECATED.</td></tr><tr><td>combine_first(other)</td><td>Combine two DataFrame objects and default to non-null values in frame calling the method.</td></tr><tr><td>compound([axis,?skipna,?level])</td><td>Return the compound percentage of the values for the requested axis</td></tr><tr><td>consolidate([inplace])</td><td>Compute NDFrame with “consolidated” internals (data of each dtype grouped together in a single ndarray).</td></tr><tr><td>convert_objects([convert_dates,?…])</td><td>Deprecated.</td></tr><tr><td>copy([deep])</td><td>Make a copy of this objects data.</td></tr><tr><td>corr([method,?min_periods])</td><td>Compute pairwise correlation of columns, excluding NA/null values</td></tr><tr><td>corrwith(other[,?axis,?drop])</td><td>Compute pairwise correlation between rows or columns of two DataFrame objects.</td></tr><tr><td>count([axis,?level,?numeric_only])</td><td>Return Series with number of non-NA/null observations over requested axis.</td></tr><tr><td>cov([min_periods])</td><td>Compute pairwise covariance of columns, excluding NA/null values</td></tr><tr><td>cummax([axis,?skipna])</td><td>Return cumulative max over requested axis.</td></tr><tr><td>cummin([axis,?skipna])</td><td>Return cumulative minimum over requested axis.</td></tr><tr><td>cumprod([axis,?skipna])</td><td>Return cumulative product over requested axis.</td></tr><tr><td>cumsum([axis,?skipna])</td><td>Return cumulative sum over requested axis.</td></tr><tr><td>describe([percentiles,?include,?exclude])</td><td>Generate various summary statistics, excluding NaN values.</td></tr><tr><td>diff([periods,?axis])</td><td>1st discrete difference of object</td></tr><tr><td>div(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>divide(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>dot(other)</td><td>Matrix multiplication with DataFrame or Series objects</td></tr><tr><td>drop(labels[,?axis,?level,?inplace,?errors])</td><td>Return new object with labels in requested axis removed.</td></tr><tr><td>drop_duplicates(*args,?**kwargs)</td><td>Return DataFrame with duplicate rows removed, optionally only</td></tr><tr><td>dropna([axis,?how,?thresh,?subset,?inplace])</td><td>Return object with labels on given axis omitted where alternately any</td></tr><tr><td>duplicated(*args,?**kwargs)</td><td>Return boolean Series denoting duplicate rows, optionally only</td></tr><tr><td>eq(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods eq</td></tr><tr><td>equals(other)</td><td>Determines if two NDFrame objects contain the same elements.</td></tr><tr><td>eval(expr[,?inplace])</td><td>Evaluate an expression in the context of the calling DataFrame instance.</td></tr><tr><td>ewm([com,?span,?halflife,?alpha,?…])</td><td>Provides exponential weighted functions</td></tr><tr><td>expanding([min_periods,?freq,?center,?axis])</td><td>Provides expanding transformations.</td></tr><tr><td>ffill([axis,?inplace,?limit,?downcast])</td><td>Synonym for NDFrame.fillna(method=’ffill’)</td></tr><tr><td>fillna([value,?method,?axis,?inplace,?…])</td><td>Fill NA/NaN values using the specified method</td></tr><tr><td>filter([items,?like,?regex,?axis])</td><td>Subset rows or columns of dataframe according to labels in the specified index.</td></tr><tr><td>first(offset)</td><td>Convenience method for subsetting initial periods of time series data based on a date offset.</td></tr><tr><td>first_valid_index()</td><td>Return label for first non-NA/null value</td></tr><tr><td>floordiv(other[,?axis,?level,?fill_value])</td><td>Integer division of dataframe and other, element-wise (binary operator?floordiv).</td></tr><tr><td>from_csv(path[,?header,?sep,?index_col,?…])</td><td>Read CSV file (DISCOURAGED, please use?pandas.read_csv()?instead).</td></tr><tr><td>from_dict(data[,?orient,?dtype])</td><td>Construct DataFrame from dict of array-like or dicts</td></tr><tr><td>from_items(items[,?columns,?orient])</td><td>Convert (key, value) pairs to DataFrame.</td></tr><tr><td>from_records(data[,?index,?exclude,?…])</td><td>Convert structured or record ndarray to DataFrame</td></tr><tr><td>ge(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods ge</td></tr><tr><td>get(key[,?default])</td><td>Get item from object for given key (DataFrame column, Panel slice, etc.).</td></tr><tr><td>get_dtype_counts()</td><td>Return the counts of dtypes in this object.</td></tr><tr><td>get_ftype_counts()</td><td>Return the counts of ftypes in this object.</td></tr><tr><td>get_value(index,?col[,?takeable])</td><td>Quickly retrieve single value at passed column and index</td></tr><tr><td>get_values()</td><td>same as values (but handles sparseness conversions)</td></tr><tr><td>groupby([by,?axis,?level,?as_index,?sort,?…])</td><td>Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.</td></tr><tr><td>gt(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods gt</td></tr><tr><td>head([n])</td><td>Returns first n rows</td></tr><tr><td>hist(data[,?column,?by,?grid,?xlabelsize,?…])</td><td>Draw histogram of the DataFrame’s series using matplotlib / pylab.</td></tr><tr><td>icol(i)</td><td>DEPRECATED.</td></tr><tr><td>idxmax([axis,?skipna])</td><td>Return index of first occurrence of maximum over requested axis.</td></tr><tr><td>idxmin([axis,?skipna])</td><td>Return index of first occurrence of minimum over requested axis.</td></tr><tr><td>iget_value(i,?j)</td><td>DEPRECATED.</td></tr><tr><td>info([verbose,?buf,?max_cols,?memory_usage,?…])</td><td>Concise summary of a DataFrame.</td></tr><tr><td>insert(loc,?column,?value[,?allow_duplicates])</td><td>Insert column into DataFrame at specified location.</td></tr><tr><td>interpolate([method,?axis,?limit,?inplace,?…])</td><td>Interpolate values according to different methods.</td></tr><tr><td>irow(i[,?copy])</td><td>DEPRECATED.</td></tr><tr><td>isin(values)</td><td>Return boolean DataFrame showing whether each element in the DataFrame is contained in values.</td></tr><tr><td>isnull()</td><td>Return a boolean same-sized object indicating if the values are null.</td></tr><tr><td>iteritems()</td><td>Iterator over (column name, Series) pairs.</td></tr><tr><td>iterkv(*args,?**kwargs)</td><td>iteritems alias used to get around 2to3. Deprecated</td></tr><tr><td>iterrows()</td><td>Iterate over DataFrame rows as (index, Series) pairs.</td></tr><tr><td>itertuples([index,?name])</td><td>Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.</td></tr><tr><td>join(other[,?on,?how,?lsuffix,?rsuffix,?sort])</td><td>Join columns with other DataFrame either on index or on a key column.</td></tr><tr><td>keys()</td><td>Get the ‘info axis’ (see Indexing for more)</td></tr><tr><td>kurt([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).</td></tr><tr><td>kurtosis([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).</td></tr><tr><td>last(offset)</td><td>Convenience method for subsetting final periods of time series data based on a date offset.</td></tr><tr><td>last_valid_index()</td><td>Return label for last non-NA/null value</td></tr><tr><td>le(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods le</td></tr><tr><td>lookup(row_labels,?col_labels)</td><td>Label-based “fancy indexing” function for DataFrame.</td></tr><tr><td>lt(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods lt</td></tr><tr><td>mad([axis,?skipna,?level])</td><td>Return the mean absolute deviation of the values for the requested axis</td></tr><tr><td>mask(cond[,?other,?inplace,?axis,?level,?…])</td><td>Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.</td></tr><tr><td>max([axis,?skipna,?level,?numeric_only])</td><td>This method returns the maximum of the values in the object.</td></tr><tr><td>mean([axis,?skipna,?level,?numeric_only])</td><td>Return the mean of the values for the requested axis</td></tr><tr><td>median([axis,?skipna,?level,?numeric_only])</td><td>Return the median of the values for the requested axis</td></tr><tr><td>memory_usage([index,?deep])</td><td>Memory usage of DataFrame columns.</td></tr><tr><td>merge(right[,?how,?on,?left_on,?right_on,?…])</td><td>Merge DataFrame objects by performing a database-style join operation by columns or indexes.</td></tr><tr><td>min([axis,?skipna,?level,?numeric_only])</td><td>This method returns the minimum of the values in the object.</td></tr><tr><td>mod(other[,?axis,?level,?fill_value])</td><td>Modulo of dataframe and other, element-wise (binary operator?mod).</td></tr><tr><td>mode([axis,?numeric_only])</td><td>Gets the mode(s) of each element along the axis selected.</td></tr><tr><td>mul(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?mul).</td></tr><tr><td>multiply(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?mul).</td></tr><tr><td>ne(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods ne</td></tr><tr><td>nlargest(n,?columns[,?keep])</td><td>Get the rows of a DataFrame sorted by the?n?largest values of?columns.</td></tr><tr><td>notnull()</td><td>Return a boolean same-sized object indicating if the values are not null.</td></tr><tr><td>nsmallest(n,?columns[,?keep])</td><td>Get the rows of a DataFrame sorted by the?n?smallest values of?columns.</td></tr><tr><td>pct_change([periods,?fill_method,?limit,?freq])</td><td>Percent change over given number of periods.</td></tr><tr><td>pipe(func,?*args,?**kwargs)</td><td>Apply func(self, <em>args, *</em>kwargs)</td></tr><tr><td>pivot([index,?columns,?values])</td><td>Reshape data (produce a “pivot” table) based on column values.</td></tr><tr><td>pivot_table(data[,?values,?index,?columns,?…])</td><td>Create a spreadsheet-style pivot table as a DataFrame.</td></tr><tr><td>plot</td><td>alias of?FramePlotMethods</td></tr><tr><td>pop(item)</td><td>Return item and drop from frame.</td></tr><tr><td>pow(other[,?axis,?level,?fill_value])</td><td>Exponential power of dataframe and other, element-wise (binary operator?pow).</td></tr><tr><td>prod([axis,?skipna,?level,?numeric_only])</td><td>Return the product of the values for the requested axis</td></tr><tr><td>product([axis,?skipna,?level,?numeric_only])</td><td>Return the product of the values for the requested axis</td></tr><tr><td>quantile([q,?axis,?numeric_only,?interpolation])</td><td>Return values at the given quantile over requested axis, a la numpy.percentile.</td></tr><tr><td>query(expr[,?inplace])</td><td>Query the columns of a frame with a boolean expression.</td></tr><tr><td>radd(other[,?axis,?level,?fill_value])</td><td>Addition of dataframe and other, element-wise (binary operator?radd).</td></tr><tr><td>rank([axis,?method,?numeric_only,?…])</td><td>Compute numerical data ranks (1 through n) along axis.</td></tr><tr><td>rdiv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?rtruediv).</td></tr><tr><td>reindex([index,?columns])</td><td>Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.</td></tr><tr><td>reindex_axis(labels[,?axis,?method,?level,?…])</td><td>Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.</td></tr><tr><td>reindex_like(other[,?method,?copy,?limit,?…])</td><td>Return an object with matching indices to myself.</td></tr><tr><td>rename([index,?columns])</td><td>Alter axes input function or functions.</td></tr><tr><td>rename_axis(mapper[,?axis,?copy,?inplace])</td><td>Alter index and / or columns using input function or functions.</td></tr><tr><td>reorder_levels(order[,?axis])</td><td>Rearrange index levels using input order.</td></tr><tr><td>replace([to_replace,?value,?inplace,?limit,?…])</td><td>Replace values given in ‘to_replace’ with ‘value’.</td></tr><tr><td>resample(rule[,?how,?axis,?fill_method,?…])</td><td>Convenience method for frequency conversion and resampling of time series.</td></tr><tr><td>reset_index([level,?drop,?inplace,?…])</td><td>For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to ‘level_0’, ‘level_1’, etc.</td></tr><tr><td>rfloordiv(other[,?axis,?level,?fill_value])</td><td>Integer division of dataframe and other, element-wise (binary operator?rfloordiv).</td></tr><tr><td>rmod(other[,?axis,?level,?fill_value])</td><td>Modulo of dataframe and other, element-wise (binary operator?rmod).</td></tr><tr><td>rmul(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?rmul).</td></tr><tr><td>rolling(window[,?min_periods,?freq,?center,?…])</td><td>Provides rolling window calculcations.</td></tr><tr><td>round([decimals])</td><td>Round a DataFrame to a variable number of decimal places.</td></tr><tr><td>rpow(other[,?axis,?level,?fill_value])</td><td>Exponential power of dataframe and other, element-wise (binary operator?rpow).</td></tr><tr><td>rsub(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?rsub).</td></tr><tr><td>rtruediv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?rtruediv).</td></tr><tr><td>sample([n,?frac,?replace,?weights,?…])</td><td>Returns a random sample of items from an axis of object.</td></tr><tr><td>select(crit[,?axis])</td><td>Return data corresponding to axis labels matching criteria</td></tr><tr><td>select_dtypes([include,?exclude])</td><td>Return a subset of a DataFrame including/excluding columns based on their?dtype.</td></tr><tr><td>sem([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return unbiased standard error of the mean over requested axis.</td></tr><tr><td>set_axis(axis,?labels)</td><td>public verson of axis assignment</td></tr><tr><td>set_index(keys[,?drop,?append,?inplace,?…])</td><td>Set the DataFrame index (row labels) using one or more existing columns.</td></tr><tr><td>set_value(index,?col,?value[,?takeable])</td><td>Put single value at passed column and index</td></tr><tr><td>shift([periods,?freq,?axis])</td><td>Shift index by desired number of periods with an optional time freq</td></tr><tr><td>skew([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased skew over requested axis</td></tr><tr><td>slice_shift([periods,?axis])</td><td>Equivalent to?shift?without copying data.</td></tr><tr><td>sort([columns,?axis,?ascending,?inplace,?…])</td><td>DEPRECATED: use?DataFrame.sort_values()</td></tr><tr><td>sort_index([axis,?level,?ascending,?…])</td><td>Sort object by labels (along an axis)</td></tr><tr><td>sort_values(by[,?axis,?ascending,?inplace,?…])</td><td>Sort by the values along either axis</td></tr><tr><td>sortlevel([level,?axis,?ascending,?inplace,?…])</td><td>Sort multilevel index by chosen axis and primary level.</td></tr><tr><td>squeeze(**kwargs)</td><td>Squeeze length 1 dimensions.</td></tr><tr><td>stack([level,?dropna])</td><td>Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels.</td></tr><tr><td>std([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return sample standard deviation over requested axis.</td></tr><tr><td>sub(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?sub).</td></tr><tr><td>subtract(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?sub).</td></tr><tr><td>sum([axis,?skipna,?level,?numeric_only])</td><td>Return the sum of the values for the requested axis</td></tr><tr><td>swapaxes(axis1,?axis2[,?copy])</td><td>Interchange axes and swap values axes appropriately</td></tr><tr><td>swaplevel([i,?j,?axis])</td><td>Swap levels i and j in a MultiIndex on a particular axis</td></tr><tr><td>tail([n])</td><td>Returns last n rows</td></tr><tr><td>take(indices[,?axis,?convert,?is_copy])</td><td>Analogous to ndarray.take</td></tr><tr><td>to_clipboard([excel,?sep])</td><td>Attempt to write text representation of object to the system clipboard This can be pasted into Excel, for example.</td></tr><tr><td>to_csv([path_or_buf,?sep,?na_rep,?…])</td><td>Write DataFrame to a comma-separated values (csv) file</td></tr><tr><td>to_dense()</td><td>Return dense representation of NDFrame (as opposed to sparse)</td></tr><tr><td>to_dict([orient])</td><td>Convert DataFrame to dictionary.</td></tr><tr><td>to_excel(excel_writer[,?sheet_name,?na_rep,?…])</td><td>Write DataFrame to a excel sheet</td></tr><tr><td>to_gbq(destination_table,?project_id[,?…])</td><td>Write a DataFrame to a Google BigQuery table.</td></tr><tr><td>to_hdf(path_or_buf,?key,?**kwargs)</td><td>Write the contained data to an HDF5 file using HDFStore.</td></tr><tr><td>to_html([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame as an HTML table.</td></tr><tr><td>to_json([path_or_buf,?orient,?date_format,?…])</td><td>Convert the object to a JSON string.</td></tr><tr><td>to_latex([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame to a tabular environment table.</td></tr><tr><td>to_msgpack([path_or_buf,?encoding])</td><td>msgpack (serialize) object to input file path</td></tr><tr><td>to_panel()</td><td>Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.</td></tr><tr><td>to_period([freq,?axis,?copy])</td><td>Convert DataFrame from DatetimeIndex to PeriodIndex with desired</td></tr><tr><td>to_pickle(path)</td><td>Pickle (serialize) object to input file path.</td></tr><tr><td>to_records([index,?convert_datetime64])</td><td>Convert DataFrame to record array.</td></tr><tr><td>to_sparse([fill_value,?kind])</td><td>Convert to SparseDataFrame</td></tr><tr><td>to_sql(name,?con[,?flavor,?schema,?…])</td><td>Write records stored in a DataFrame to a SQL database.</td></tr><tr><td>to_stata(fname[,?convert_dates,?…])</td><td>A class for writing Stata binary dta files from array-like objects</td></tr><tr><td>to_string([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame to a console-friendly tabular output.</td></tr><tr><td>to_timestamp([freq,?how,?axis,?copy])</td><td>Cast to DatetimeIndex of timestamps, at?beginning?of period</td></tr><tr><td>to_xarray()</td><td>Return an xarray object from the pandas object.</td></tr><tr><td>transpose(*args,?**kwargs)</td><td>Transpose index and columns</td></tr><tr><td>truediv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>truncate([before,?after,?axis,?copy])</td><td>Truncates a sorted NDFrame before and/or after some particular index value.</td></tr><tr><td>tshift([periods,?freq,?axis])</td><td>Shift the time index, using the index’s frequency if available.</td></tr><tr><td>tz_convert(tz[,?axis,?level,?copy])</td><td>Convert tz-aware axis to target time zone.</td></tr><tr><td>tz_localize(*args,?**kwargs)</td><td>Localize tz-naive TimeSeries to target time zone.</td></tr><tr><td>unstack([level,?fill_value])</td><td>Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.</td></tr><tr><td>update(other[,?join,?overwrite,?…])</td><td>Modify DataFrame in place using non-NA values from passed DataFrame.</td></tr><tr><td>var([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return unbiased variance over requested axis.</td></tr><tr><td>where(cond[,?other,?inplace,?axis,?level,?…])</td><td>Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.</td></tr><tr><td>xs(key[,?axis,?level,?drop_level])</td><td>Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.</td></tr></tbody></table>","site":{"data":{}},"excerpt":"<p>整理了pandas中DataFrame的属性和函数，做成了表格。<br>顺便推荐csv转md语法的表格生成网站:<a href=\"http://www.tablesgenerator.com/\" target=\"_blank\" rel=\"noopener\">Tables Generator</a></p>","more":"<h3 id=\"属性\"><a href=\"#属性\" class=\"headerlink\" title=\"属性\"></a>属性</h3><table><thead><tr><th style=\"text-align:center\">属性</th><th style=\"text-align:center\">说明</th></tr></thead><tbody><tr><td style=\"text-align:center\">T</td><td style=\"text-align:center\">Transpose index and columns at Fast label-based scalar accessor</td></tr><tr><td style=\"text-align:center\">axes</td><td style=\"text-align:center\">Return a list with the row axis labels and column axis labels as the only members.</td></tr><tr><td style=\"text-align:center\">blocks</td><td style=\"text-align:center\">Internal property, property synonym for as_blocks()</td></tr><tr><td style=\"text-align:center\">dtypes</td><td style=\"text-align:center\">Return the dtypes in this object.</td></tr><tr><td style=\"text-align:center\">empty</td><td style=\"text-align:center\">True if NDFrame is entirely empty [no items], meaning any of the axes are of length 0.</td></tr><tr><td style=\"text-align:center\">ftypes</td><td style=\"text-align:center\">Return the ftypes (indication of sparse/dense and dtype) in this object.</td></tr><tr><td style=\"text-align:center\">iat</td><td style=\"text-align:center\">Fast integer location scalar accessor.</td></tr><tr><td style=\"text-align:center\">iloc</td><td style=\"text-align:center\">Purely integer-location based indexing for selection by position.</td></tr><tr><td style=\"text-align:center\">is_copy</td><td style=\"text-align:center\"></td></tr><tr><td style=\"text-align:center\">ix</td><td style=\"text-align:center\">A primarily label-location based indexer, with integer position fallback.</td></tr><tr><td style=\"text-align:center\">loc</td><td style=\"text-align:center\">Purely label-location based indexer for selection by label.</td></tr><tr><td style=\"text-align:center\">ndim</td><td style=\"text-align:center\">Number of axes / array dimensions</td></tr><tr><td style=\"text-align:center\">shape</td><td style=\"text-align:center\">Return a tuple representing the dimensionality of the DataFrame.</td></tr><tr><td style=\"text-align:center\">size</td><td style=\"text-align:center\">number of elements in the NDFrame</td></tr><tr><td style=\"text-align:center\">style</td><td style=\"text-align:center\">Property returning a Styler object containing methods for building a styled HTML representation fo the DataFrame.</td></tr><tr><td style=\"text-align:center\">values</td><td style=\"text-align:center\">Numpy representation of NDFrame</td></tr></tbody></table><h3 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h3><table><thead><tr><th>函数名</th><th>作用</th></tr></thead><tbody><tr><td>abs()</td><td>Return an object with absolute value taken�only applicable to objects that are all numeric.</td></tr><tr><td>add(other[,?axis,?level,?fill_value])</td><td>Addition of dataframe and other, element-wise (binary operator?add).</td></tr><tr><td>add_prefix(prefix)</td><td>Concatenate prefix string with panel items names.</td></tr><tr><td>add_suffix(suffix)</td><td>Concatenate suffix string with panel items names.</td></tr><tr><td>align(other[,?join,?axis,?level,?copy,?…])</td><td>Align two object on their axes with the</td></tr><tr><td>all([axis,?bool_only,?skipna,?level])</td><td>Return whether all elements are True over requested axis</td></tr><tr><td>any([axis,?bool_only,?skipna,?level])</td><td>Return whether any element is True over requested axis</td></tr><tr><td>append(other[,?ignore_index,?verify_integrity])</td><td>Append rows of?other?to the end of this frame, returning a new object.</td></tr><tr><td>apply(func[,?axis,?broadcast,?raw,?reduce,?args])</td><td>Applies function along input axis of DataFrame.</td></tr><tr><td>applymap(func)</td><td>Apply a function to a DataFrame that is intended to operate elementwise, i.e.</td></tr><tr><td>as_blocks([copy])</td><td>Convert the frame to a dict of dtype -&gt; Constructor Types that each has a homogeneous dtype.</td></tr><tr><td>as_matrix([columns])</td><td>Convert the frame to its Numpy-array representation.</td></tr><tr><td>asfreq(freq[,?method,?how,?normalize])</td><td>Convert TimeSeries to specified frequency.</td></tr><tr><td>asof(where[,?subset])</td><td>The last row without any NaN is taken (or the last row without</td></tr><tr><td>assign(**kwargs)</td><td>Assign new columns to a DataFrame, returning a new object (a copy) with all the original columns in addition to the new ones.</td></tr><tr><td>astype(dtype[,?copy,?raise_on_error])</td><td>Cast object to input numpy.dtype</td></tr><tr><td>at_time(time[,?asof])</td><td>Select values at particular time of day (e.g.</td></tr><tr><td>between_time(start_time,?end_time[,?…])</td><td>Select values between particular times of the day (e.g., 9:00-9:30 AM).</td></tr><tr><td>bfill([axis,?inplace,?limit,?downcast])</td><td>Synonym for NDFrame.fillna(method=’bfill’)</td></tr><tr><td>bool()</td><td>Return the bool of a single element PandasObject.</td></tr><tr><td>boxplot([column,?by,?ax,?fontsize,?rot,?…])</td><td>Make a box plot from DataFrame column optionally grouped by some columns or</td></tr><tr><td>clip([lower,?upper,?axis])</td><td>Trim values at input threshold(s).</td></tr><tr><td>clip_lower(threshold[,?axis])</td><td>Return copy of the input with values below given value(s) truncated.</td></tr><tr><td>clip_upper(threshold[,?axis])</td><td>Return copy of input with values above given value(s) truncated.</td></tr><tr><td>combine(other,?func[,?fill_value,?overwrite])</td><td>Add two DataFrame objects and do not propagate NaN values, so if for a</td></tr><tr><td>combineAdd(other)</td><td>DEPRECATED.</td></tr><tr><td>combineMult(other)</td><td>DEPRECATED.</td></tr><tr><td>combine_first(other)</td><td>Combine two DataFrame objects and default to non-null values in frame calling the method.</td></tr><tr><td>compound([axis,?skipna,?level])</td><td>Return the compound percentage of the values for the requested axis</td></tr><tr><td>consolidate([inplace])</td><td>Compute NDFrame with “consolidated” internals (data of each dtype grouped together in a single ndarray).</td></tr><tr><td>convert_objects([convert_dates,?…])</td><td>Deprecated.</td></tr><tr><td>copy([deep])</td><td>Make a copy of this objects data.</td></tr><tr><td>corr([method,?min_periods])</td><td>Compute pairwise correlation of columns, excluding NA/null values</td></tr><tr><td>corrwith(other[,?axis,?drop])</td><td>Compute pairwise correlation between rows or columns of two DataFrame objects.</td></tr><tr><td>count([axis,?level,?numeric_only])</td><td>Return Series with number of non-NA/null observations over requested axis.</td></tr><tr><td>cov([min_periods])</td><td>Compute pairwise covariance of columns, excluding NA/null values</td></tr><tr><td>cummax([axis,?skipna])</td><td>Return cumulative max over requested axis.</td></tr><tr><td>cummin([axis,?skipna])</td><td>Return cumulative minimum over requested axis.</td></tr><tr><td>cumprod([axis,?skipna])</td><td>Return cumulative product over requested axis.</td></tr><tr><td>cumsum([axis,?skipna])</td><td>Return cumulative sum over requested axis.</td></tr><tr><td>describe([percentiles,?include,?exclude])</td><td>Generate various summary statistics, excluding NaN values.</td></tr><tr><td>diff([periods,?axis])</td><td>1st discrete difference of object</td></tr><tr><td>div(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>divide(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>dot(other)</td><td>Matrix multiplication with DataFrame or Series objects</td></tr><tr><td>drop(labels[,?axis,?level,?inplace,?errors])</td><td>Return new object with labels in requested axis removed.</td></tr><tr><td>drop_duplicates(*args,?**kwargs)</td><td>Return DataFrame with duplicate rows removed, optionally only</td></tr><tr><td>dropna([axis,?how,?thresh,?subset,?inplace])</td><td>Return object with labels on given axis omitted where alternately any</td></tr><tr><td>duplicated(*args,?**kwargs)</td><td>Return boolean Series denoting duplicate rows, optionally only</td></tr><tr><td>eq(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods eq</td></tr><tr><td>equals(other)</td><td>Determines if two NDFrame objects contain the same elements.</td></tr><tr><td>eval(expr[,?inplace])</td><td>Evaluate an expression in the context of the calling DataFrame instance.</td></tr><tr><td>ewm([com,?span,?halflife,?alpha,?…])</td><td>Provides exponential weighted functions</td></tr><tr><td>expanding([min_periods,?freq,?center,?axis])</td><td>Provides expanding transformations.</td></tr><tr><td>ffill([axis,?inplace,?limit,?downcast])</td><td>Synonym for NDFrame.fillna(method=’ffill’)</td></tr><tr><td>fillna([value,?method,?axis,?inplace,?…])</td><td>Fill NA/NaN values using the specified method</td></tr><tr><td>filter([items,?like,?regex,?axis])</td><td>Subset rows or columns of dataframe according to labels in the specified index.</td></tr><tr><td>first(offset)</td><td>Convenience method for subsetting initial periods of time series data based on a date offset.</td></tr><tr><td>first_valid_index()</td><td>Return label for first non-NA/null value</td></tr><tr><td>floordiv(other[,?axis,?level,?fill_value])</td><td>Integer division of dataframe and other, element-wise (binary operator?floordiv).</td></tr><tr><td>from_csv(path[,?header,?sep,?index_col,?…])</td><td>Read CSV file (DISCOURAGED, please use?pandas.read_csv()?instead).</td></tr><tr><td>from_dict(data[,?orient,?dtype])</td><td>Construct DataFrame from dict of array-like or dicts</td></tr><tr><td>from_items(items[,?columns,?orient])</td><td>Convert (key, value) pairs to DataFrame.</td></tr><tr><td>from_records(data[,?index,?exclude,?…])</td><td>Convert structured or record ndarray to DataFrame</td></tr><tr><td>ge(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods ge</td></tr><tr><td>get(key[,?default])</td><td>Get item from object for given key (DataFrame column, Panel slice, etc.).</td></tr><tr><td>get_dtype_counts()</td><td>Return the counts of dtypes in this object.</td></tr><tr><td>get_ftype_counts()</td><td>Return the counts of ftypes in this object.</td></tr><tr><td>get_value(index,?col[,?takeable])</td><td>Quickly retrieve single value at passed column and index</td></tr><tr><td>get_values()</td><td>same as values (but handles sparseness conversions)</td></tr><tr><td>groupby([by,?axis,?level,?as_index,?sort,?…])</td><td>Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns.</td></tr><tr><td>gt(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods gt</td></tr><tr><td>head([n])</td><td>Returns first n rows</td></tr><tr><td>hist(data[,?column,?by,?grid,?xlabelsize,?…])</td><td>Draw histogram of the DataFrame’s series using matplotlib / pylab.</td></tr><tr><td>icol(i)</td><td>DEPRECATED.</td></tr><tr><td>idxmax([axis,?skipna])</td><td>Return index of first occurrence of maximum over requested axis.</td></tr><tr><td>idxmin([axis,?skipna])</td><td>Return index of first occurrence of minimum over requested axis.</td></tr><tr><td>iget_value(i,?j)</td><td>DEPRECATED.</td></tr><tr><td>info([verbose,?buf,?max_cols,?memory_usage,?…])</td><td>Concise summary of a DataFrame.</td></tr><tr><td>insert(loc,?column,?value[,?allow_duplicates])</td><td>Insert column into DataFrame at specified location.</td></tr><tr><td>interpolate([method,?axis,?limit,?inplace,?…])</td><td>Interpolate values according to different methods.</td></tr><tr><td>irow(i[,?copy])</td><td>DEPRECATED.</td></tr><tr><td>isin(values)</td><td>Return boolean DataFrame showing whether each element in the DataFrame is contained in values.</td></tr><tr><td>isnull()</td><td>Return a boolean same-sized object indicating if the values are null.</td></tr><tr><td>iteritems()</td><td>Iterator over (column name, Series) pairs.</td></tr><tr><td>iterkv(*args,?**kwargs)</td><td>iteritems alias used to get around 2to3. Deprecated</td></tr><tr><td>iterrows()</td><td>Iterate over DataFrame rows as (index, Series) pairs.</td></tr><tr><td>itertuples([index,?name])</td><td>Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.</td></tr><tr><td>join(other[,?on,?how,?lsuffix,?rsuffix,?sort])</td><td>Join columns with other DataFrame either on index or on a key column.</td></tr><tr><td>keys()</td><td>Get the ‘info axis’ (see Indexing for more)</td></tr><tr><td>kurt([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).</td></tr><tr><td>kurtosis([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).</td></tr><tr><td>last(offset)</td><td>Convenience method for subsetting final periods of time series data based on a date offset.</td></tr><tr><td>last_valid_index()</td><td>Return label for last non-NA/null value</td></tr><tr><td>le(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods le</td></tr><tr><td>lookup(row_labels,?col_labels)</td><td>Label-based “fancy indexing” function for DataFrame.</td></tr><tr><td>lt(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods lt</td></tr><tr><td>mad([axis,?skipna,?level])</td><td>Return the mean absolute deviation of the values for the requested axis</td></tr><tr><td>mask(cond[,?other,?inplace,?axis,?level,?…])</td><td>Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other.</td></tr><tr><td>max([axis,?skipna,?level,?numeric_only])</td><td>This method returns the maximum of the values in the object.</td></tr><tr><td>mean([axis,?skipna,?level,?numeric_only])</td><td>Return the mean of the values for the requested axis</td></tr><tr><td>median([axis,?skipna,?level,?numeric_only])</td><td>Return the median of the values for the requested axis</td></tr><tr><td>memory_usage([index,?deep])</td><td>Memory usage of DataFrame columns.</td></tr><tr><td>merge(right[,?how,?on,?left_on,?right_on,?…])</td><td>Merge DataFrame objects by performing a database-style join operation by columns or indexes.</td></tr><tr><td>min([axis,?skipna,?level,?numeric_only])</td><td>This method returns the minimum of the values in the object.</td></tr><tr><td>mod(other[,?axis,?level,?fill_value])</td><td>Modulo of dataframe and other, element-wise (binary operator?mod).</td></tr><tr><td>mode([axis,?numeric_only])</td><td>Gets the mode(s) of each element along the axis selected.</td></tr><tr><td>mul(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?mul).</td></tr><tr><td>multiply(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?mul).</td></tr><tr><td>ne(other[,?axis,?level])</td><td>Wrapper for flexible comparison methods ne</td></tr><tr><td>nlargest(n,?columns[,?keep])</td><td>Get the rows of a DataFrame sorted by the?n?largest values of?columns.</td></tr><tr><td>notnull()</td><td>Return a boolean same-sized object indicating if the values are not null.</td></tr><tr><td>nsmallest(n,?columns[,?keep])</td><td>Get the rows of a DataFrame sorted by the?n?smallest values of?columns.</td></tr><tr><td>pct_change([periods,?fill_method,?limit,?freq])</td><td>Percent change over given number of periods.</td></tr><tr><td>pipe(func,?*args,?**kwargs)</td><td>Apply func(self, <em>args, *</em>kwargs)</td></tr><tr><td>pivot([index,?columns,?values])</td><td>Reshape data (produce a “pivot” table) based on column values.</td></tr><tr><td>pivot_table(data[,?values,?index,?columns,?…])</td><td>Create a spreadsheet-style pivot table as a DataFrame.</td></tr><tr><td>plot</td><td>alias of?FramePlotMethods</td></tr><tr><td>pop(item)</td><td>Return item and drop from frame.</td></tr><tr><td>pow(other[,?axis,?level,?fill_value])</td><td>Exponential power of dataframe and other, element-wise (binary operator?pow).</td></tr><tr><td>prod([axis,?skipna,?level,?numeric_only])</td><td>Return the product of the values for the requested axis</td></tr><tr><td>product([axis,?skipna,?level,?numeric_only])</td><td>Return the product of the values for the requested axis</td></tr><tr><td>quantile([q,?axis,?numeric_only,?interpolation])</td><td>Return values at the given quantile over requested axis, a la numpy.percentile.</td></tr><tr><td>query(expr[,?inplace])</td><td>Query the columns of a frame with a boolean expression.</td></tr><tr><td>radd(other[,?axis,?level,?fill_value])</td><td>Addition of dataframe and other, element-wise (binary operator?radd).</td></tr><tr><td>rank([axis,?method,?numeric_only,?…])</td><td>Compute numerical data ranks (1 through n) along axis.</td></tr><tr><td>rdiv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?rtruediv).</td></tr><tr><td>reindex([index,?columns])</td><td>Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.</td></tr><tr><td>reindex_axis(labels[,?axis,?method,?level,?…])</td><td>Conform input object to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index.</td></tr><tr><td>reindex_like(other[,?method,?copy,?limit,?…])</td><td>Return an object with matching indices to myself.</td></tr><tr><td>rename([index,?columns])</td><td>Alter axes input function or functions.</td></tr><tr><td>rename_axis(mapper[,?axis,?copy,?inplace])</td><td>Alter index and / or columns using input function or functions.</td></tr><tr><td>reorder_levels(order[,?axis])</td><td>Rearrange index levels using input order.</td></tr><tr><td>replace([to_replace,?value,?inplace,?limit,?…])</td><td>Replace values given in ‘to_replace’ with ‘value’.</td></tr><tr><td>resample(rule[,?how,?axis,?fill_method,?…])</td><td>Convenience method for frequency conversion and resampling of time series.</td></tr><tr><td>reset_index([level,?drop,?inplace,?…])</td><td>For DataFrame with multi-level index, return new DataFrame with labeling information in the columns under the index names, defaulting to ‘level_0’, ‘level_1’, etc.</td></tr><tr><td>rfloordiv(other[,?axis,?level,?fill_value])</td><td>Integer division of dataframe and other, element-wise (binary operator?rfloordiv).</td></tr><tr><td>rmod(other[,?axis,?level,?fill_value])</td><td>Modulo of dataframe and other, element-wise (binary operator?rmod).</td></tr><tr><td>rmul(other[,?axis,?level,?fill_value])</td><td>Multiplication of dataframe and other, element-wise (binary operator?rmul).</td></tr><tr><td>rolling(window[,?min_periods,?freq,?center,?…])</td><td>Provides rolling window calculcations.</td></tr><tr><td>round([decimals])</td><td>Round a DataFrame to a variable number of decimal places.</td></tr><tr><td>rpow(other[,?axis,?level,?fill_value])</td><td>Exponential power of dataframe and other, element-wise (binary operator?rpow).</td></tr><tr><td>rsub(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?rsub).</td></tr><tr><td>rtruediv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?rtruediv).</td></tr><tr><td>sample([n,?frac,?replace,?weights,?…])</td><td>Returns a random sample of items from an axis of object.</td></tr><tr><td>select(crit[,?axis])</td><td>Return data corresponding to axis labels matching criteria</td></tr><tr><td>select_dtypes([include,?exclude])</td><td>Return a subset of a DataFrame including/excluding columns based on their?dtype.</td></tr><tr><td>sem([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return unbiased standard error of the mean over requested axis.</td></tr><tr><td>set_axis(axis,?labels)</td><td>public verson of axis assignment</td></tr><tr><td>set_index(keys[,?drop,?append,?inplace,?…])</td><td>Set the DataFrame index (row labels) using one or more existing columns.</td></tr><tr><td>set_value(index,?col,?value[,?takeable])</td><td>Put single value at passed column and index</td></tr><tr><td>shift([periods,?freq,?axis])</td><td>Shift index by desired number of periods with an optional time freq</td></tr><tr><td>skew([axis,?skipna,?level,?numeric_only])</td><td>Return unbiased skew over requested axis</td></tr><tr><td>slice_shift([periods,?axis])</td><td>Equivalent to?shift?without copying data.</td></tr><tr><td>sort([columns,?axis,?ascending,?inplace,?…])</td><td>DEPRECATED: use?DataFrame.sort_values()</td></tr><tr><td>sort_index([axis,?level,?ascending,?…])</td><td>Sort object by labels (along an axis)</td></tr><tr><td>sort_values(by[,?axis,?ascending,?inplace,?…])</td><td>Sort by the values along either axis</td></tr><tr><td>sortlevel([level,?axis,?ascending,?inplace,?…])</td><td>Sort multilevel index by chosen axis and primary level.</td></tr><tr><td>squeeze(**kwargs)</td><td>Squeeze length 1 dimensions.</td></tr><tr><td>stack([level,?dropna])</td><td>Pivot a level of the (possibly hierarchical) column labels, returning a DataFrame (or Series in the case of an object with a single level of column labels) having a hierarchical index with a new inner-most level of row labels.</td></tr><tr><td>std([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return sample standard deviation over requested axis.</td></tr><tr><td>sub(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?sub).</td></tr><tr><td>subtract(other[,?axis,?level,?fill_value])</td><td>Subtraction of dataframe and other, element-wise (binary operator?sub).</td></tr><tr><td>sum([axis,?skipna,?level,?numeric_only])</td><td>Return the sum of the values for the requested axis</td></tr><tr><td>swapaxes(axis1,?axis2[,?copy])</td><td>Interchange axes and swap values axes appropriately</td></tr><tr><td>swaplevel([i,?j,?axis])</td><td>Swap levels i and j in a MultiIndex on a particular axis</td></tr><tr><td>tail([n])</td><td>Returns last n rows</td></tr><tr><td>take(indices[,?axis,?convert,?is_copy])</td><td>Analogous to ndarray.take</td></tr><tr><td>to_clipboard([excel,?sep])</td><td>Attempt to write text representation of object to the system clipboard This can be pasted into Excel, for example.</td></tr><tr><td>to_csv([path_or_buf,?sep,?na_rep,?…])</td><td>Write DataFrame to a comma-separated values (csv) file</td></tr><tr><td>to_dense()</td><td>Return dense representation of NDFrame (as opposed to sparse)</td></tr><tr><td>to_dict([orient])</td><td>Convert DataFrame to dictionary.</td></tr><tr><td>to_excel(excel_writer[,?sheet_name,?na_rep,?…])</td><td>Write DataFrame to a excel sheet</td></tr><tr><td>to_gbq(destination_table,?project_id[,?…])</td><td>Write a DataFrame to a Google BigQuery table.</td></tr><tr><td>to_hdf(path_or_buf,?key,?**kwargs)</td><td>Write the contained data to an HDF5 file using HDFStore.</td></tr><tr><td>to_html([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame as an HTML table.</td></tr><tr><td>to_json([path_or_buf,?orient,?date_format,?…])</td><td>Convert the object to a JSON string.</td></tr><tr><td>to_latex([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame to a tabular environment table.</td></tr><tr><td>to_msgpack([path_or_buf,?encoding])</td><td>msgpack (serialize) object to input file path</td></tr><tr><td>to_panel()</td><td>Transform long (stacked) format (DataFrame) into wide (3D, Panel) format.</td></tr><tr><td>to_period([freq,?axis,?copy])</td><td>Convert DataFrame from DatetimeIndex to PeriodIndex with desired</td></tr><tr><td>to_pickle(path)</td><td>Pickle (serialize) object to input file path.</td></tr><tr><td>to_records([index,?convert_datetime64])</td><td>Convert DataFrame to record array.</td></tr><tr><td>to_sparse([fill_value,?kind])</td><td>Convert to SparseDataFrame</td></tr><tr><td>to_sql(name,?con[,?flavor,?schema,?…])</td><td>Write records stored in a DataFrame to a SQL database.</td></tr><tr><td>to_stata(fname[,?convert_dates,?…])</td><td>A class for writing Stata binary dta files from array-like objects</td></tr><tr><td>to_string([buf,?columns,?col_space,?header,?…])</td><td>Render a DataFrame to a console-friendly tabular output.</td></tr><tr><td>to_timestamp([freq,?how,?axis,?copy])</td><td>Cast to DatetimeIndex of timestamps, at?beginning?of period</td></tr><tr><td>to_xarray()</td><td>Return an xarray object from the pandas object.</td></tr><tr><td>transpose(*args,?**kwargs)</td><td>Transpose index and columns</td></tr><tr><td>truediv(other[,?axis,?level,?fill_value])</td><td>Floating division of dataframe and other, element-wise (binary operator?truediv).</td></tr><tr><td>truncate([before,?after,?axis,?copy])</td><td>Truncates a sorted NDFrame before and/or after some particular index value.</td></tr><tr><td>tshift([periods,?freq,?axis])</td><td>Shift the time index, using the index’s frequency if available.</td></tr><tr><td>tz_convert(tz[,?axis,?level,?copy])</td><td>Convert tz-aware axis to target time zone.</td></tr><tr><td>tz_localize(*args,?**kwargs)</td><td>Localize tz-naive TimeSeries to target time zone.</td></tr><tr><td>unstack([level,?fill_value])</td><td>Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels.</td></tr><tr><td>update(other[,?join,?overwrite,?…])</td><td>Modify DataFrame in place using non-NA values from passed DataFrame.</td></tr><tr><td>var([axis,?skipna,?level,?ddof,?numeric_only])</td><td>Return unbiased variance over requested axis.</td></tr><tr><td>where(cond[,?other,?inplace,?axis,?level,?…])</td><td>Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.</td></tr><tr><td>xs(key[,?axis,?level,?drop_level])</td><td>Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.</td></tr></tbody></table>","popularPost_tmp_postPath":true,"popularPost_tmp_gaData":{"updated":"Mon Jul 23 2018 09:28:38 GMT+0800 (中国标准时间)","title":"DataFrame函数与属性","path":"2017/02/04/pandas-func/","eyeCatchImage":null,"excerpt":"<p>整理了pandas中DataFrame的属性和函数，做成了表格。<br>顺便推荐csv转md语法的表格生成网站:<a href=\"http://www.tablesgenerator.com/\" target=\"_blank\" rel=\"noopener\">Tables Generator</a></p>","date":"2017-02-04T13:02:51.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["math","machinelearning","code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}}],"PostAsset":[],"PostCategory":[{"post_id":"cjmd072c50001qcw6gskbd9a6","category_id":"cjmd072cc0005qcw6zn9stpak","_id":"cjmd072ck000fqcw6k3is1det"},{"post_id":"cjmd072c90003qcw60txh9z51","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072cn000kqcw6d67f3x4i"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","category_id":"cjmd072cc0005qcw6zn9stpak","_id":"cjmd072cq000pqcw6gor8jym9"},{"post_id":"cjmd072ce0007qcw6rbhpipf9","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072cs000tqcw6q229zj8e"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072ct000vqcw61s611gux"},{"post_id":"cjmd072cf0009qcw6pre1bacp","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072cv000yqcw6utzzc8lk"},{"post_id":"cjmd072cp000oqcw65ode1hfz","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072cw0012qcw6dabnbzbt"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072cy0016qcw6o19zljmy"},{"post_id":"cjmd072cg000aqcw6yd4i21pl","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072cz001aqcw6rh5bin21"},{"post_id":"cjmd072cs000uqcw6wnw7fvpf","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072d0001eqcw6iu1trayv"},{"post_id":"cjmd072cu000xqcw6l4m00fyh","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072d2001iqcw6oddmag4o"},{"post_id":"cjmd072ci000dqcw68lvwmb07","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072d3001mqcw6cqszl4ne"},{"post_id":"cjmd072cj000eqcw6fqw2quk3","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072d5001qqcw6r0j20n06"},{"post_id":"cjmd072d0001dqcw6c7l4tbo5","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd072d6001sqcw69g0qougo"},{"post_id":"cjmd072cm000jqcw667rfquno","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072d8001xqcw6fby6qs62"},{"post_id":"cjmd072d1001hqcw6sweam9cm","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072d90020qcw6d08qwsbi"},{"post_id":"cjmd072d4001pqcw6vvk1cc8m","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072db0025qcw6qpvc5kd0"},{"post_id":"cjmd072cv0011qcw6rzw7215r","category_id":"cjmd072d2001jqcw62y9n0ryl","_id":"cjmd072dc0028qcw610g59nct"},{"post_id":"cjmd072d5001rqcw6f90w9tcn","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072dd002cqcw69ddtuuaq"},{"post_id":"cjmd072d7001wqcw6iml78k1e","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072df002fqcw6n4901adb"},{"post_id":"cjmd072cx0015qcw6cz5y0k9y","category_id":"cjmd072d2001jqcw62y9n0ryl","_id":"cjmd072dg002kqcw6xnjlu5p2"},{"post_id":"cjmd072d8001zqcw67w7br38j","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072di002nqcw6t79wivs7"},{"post_id":"cjmd072da0024qcw6k88en422","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072dk002rqcw6oou3chur"},{"post_id":"cjmd072cy0019qcw6dcpt8kzu","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072dk002uqcw6u87jyxse"},{"post_id":"cjmd072db0027qcw6aj6tcfif","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072dl002yqcw6bxehdwiw"},{"post_id":"cjmd072dg002jqcw6br15sstp","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072dm002zqcw67zopv595"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","category_id":"cjmd072cc0005qcw6zn9stpak","_id":"cjmd072dm0031qcw6lb6hw451"},{"post_id":"cjmd072dd002bqcw60d7j3mpd","category_id":"cjmd072df002hqcw64nmrxvpy","_id":"cjmd072dn0033qcw6n2jwzmpa"},{"post_id":"cjmd072dj002pqcw61zo6vmeo","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072dn0036qcw6xy95xirv"},{"post_id":"cjmd072dk002tqcw6gtkb4urj","category_id":"cjmd072ck000gqcw67f4pnp2m","_id":"cjmd072do0037qcw6710k21zy"},{"post_id":"cjmd072de002eqcw6hub4nmxr","category_id":"cjmd072dj002qqcw6o531jf93","_id":"cjmd072do0039qcw6r5ntc89d"},{"post_id":"cjmd072dk002xqcw6fhwjjsvj","category_id":"cjmd072cz001bqcw6g8q5kys5","_id":"cjmd072do003bqcw6c9x8b85h"},{"post_id":"cjmd072d2001lqcw6y1vijoci","category_id":"cjmd072ci000bqcw61krk319l","_id":"cjmd09ll700021kw6rwwjpdg7"}],"PostTag":[{"post_id":"cjmd072c50001qcw6gskbd9a6","tag_id":"cjmd072cd0006qcw6d8weaaxz","_id":"cjmd072cv0010qcw6fyop4i7q"},{"post_id":"cjmd072c50001qcw6gskbd9a6","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072cx0014qcw6iflt6a5b"},{"post_id":"cjmd072c50001qcw6gskbd9a6","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072cy0018qcw6ijoo3gaa"},{"post_id":"cjmd072c50001qcw6gskbd9a6","tag_id":"cjmd072cn000mqcw6wetadc5r","_id":"cjmd072cz001cqcw6pk0hj2h4"},{"post_id":"cjmd072c50001qcw6gskbd9a6","tag_id":"cjmd072cr000rqcw6opovqozs","_id":"cjmd072d1001gqcw6csqrgc24"},{"post_id":"cjmd072c90003qcw60txh9z51","tag_id":"cjmd072cv000zqcw6yzlq9thu","_id":"cjmd072d2001kqcw6dftt1gdk"},{"post_id":"cjmd072c90003qcw60txh9z51","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072d4001oqcw6uq9tqe9h"},{"post_id":"cjmd072d4001pqcw6vvk1cc8m","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072d7001vqcw6orlyva1j"},{"post_id":"cjmd072d4001pqcw6vvk1cc8m","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072d8001yqcw63arikqqr"},{"post_id":"cjmd072ce0007qcw6rbhpipf9","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072da0023qcw6hczjsl9g"},{"post_id":"cjmd072ce0007qcw6rbhpipf9","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072db0026qcw6wdtx5knj"},{"post_id":"cjmd072d5001rqcw6f90w9tcn","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dc002aqcw6apmsbxxb"},{"post_id":"cjmd072d5001rqcw6f90w9tcn","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072de002dqcw6w8ze14fk"},{"post_id":"cjmd072cf0009qcw6pre1bacp","tag_id":"cjmd072cv000zqcw6yzlq9thu","_id":"cjmd072df002iqcw65lv4cfrn"},{"post_id":"cjmd072cf0009qcw6pre1bacp","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dh002lqcw63ede55ja"},{"post_id":"cjmd072cg000aqcw6yd4i21pl","tag_id":"cjmd072cv000zqcw6yzlq9thu","_id":"cjmd072dk002sqcw6u7giz1c3"},{"post_id":"cjmd072cg000aqcw6yd4i21pl","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dk002vqcw6jsuyrodn"},{"post_id":"cjmd072ci000dqcw68lvwmb07","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dm0032qcw6h5hm5bwu"},{"post_id":"cjmd072ci000dqcw68lvwmb07","tag_id":"cjmd072dk002wqcw6qtnhml2f","_id":"cjmd072dn0034qcw6won8jht8"},{"post_id":"cjmd072cj000eqcw6fqw2quk3","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072do003aqcw6cnr338fq"},{"post_id":"cjmd072cj000eqcw6fqw2quk3","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072dp003cqcw60o5t0vq4"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","tag_id":"cjmd072cd0006qcw6d8weaaxz","_id":"cjmd072dq003fqcw6rlsygbvk"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dq003gqcw6ghq50ck6"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072dq003iqcw64tz9o0ue"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","tag_id":"cjmd072cn000mqcw6wetadc5r","_id":"cjmd072dr003jqcw6ak0twlzm"},{"post_id":"cjmd072cl000iqcw6gj4y2i1c","tag_id":"cjmd072cr000rqcw6opovqozs","_id":"cjmd072dr003lqcw6phuqmj3q"},{"post_id":"cjmd072cm000jqcw667rfquno","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072ds003mqcw6lx84lm47"},{"post_id":"cjmd072cm000jqcw667rfquno","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072ds003oqcw6xs1332ar"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","tag_id":"cjmd072cd0006qcw6d8weaaxz","_id":"cjmd072dt003pqcw6wqx2fnfb"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dt003rqcw6veww7h7q"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072dt003sqcw6rqffslow"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","tag_id":"cjmd072cn000mqcw6wetadc5r","_id":"cjmd072du003uqcw6nehf4aml"},{"post_id":"cjmd072co000nqcw6ejn1yfkb","tag_id":"cjmd072cr000rqcw6opovqozs","_id":"cjmd072du003vqcw6tqnmnf9g"},{"post_id":"cjmd072cp000oqcw65ode1hfz","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dv003xqcw6c9p25sum"},{"post_id":"cjmd072cp000oqcw65ode1hfz","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072dv003yqcw6ht4n48sk"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","tag_id":"cjmd072cd0006qcw6d8weaaxz","_id":"cjmd072dv0040qcw6j5pwdww2"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dv0041qcw6phlljtsn"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072dw0043qcw6258ui2g0"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","tag_id":"cjmd072cn000mqcw6wetadc5r","_id":"cjmd072dw0044qcw6z0rxjegn"},{"post_id":"cjmd072cr000sqcw6ptpwtvbc","tag_id":"cjmd072cr000rqcw6opovqozs","_id":"cjmd072dw0046qcw6azkqv50m"},{"post_id":"cjmd072cs000uqcw6wnw7fvpf","tag_id":"cjmd072du003tqcw66w99htxr","_id":"cjmd072dx0047qcw6fnwpg4g2"},{"post_id":"cjmd072cs000uqcw6wnw7fvpf","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dx0049qcw62v3gq9q6"},{"post_id":"cjmd072cu000xqcw6l4m00fyh","tag_id":"cjmd072du003tqcw66w99htxr","_id":"cjmd072dx004aqcw6dgo9pht1"},{"post_id":"cjmd072cu000xqcw6l4m00fyh","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072dx004cqcw6dmlzzgli"},{"post_id":"cjmd072cv0011qcw6rzw7215r","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dy004dqcw6bl3jnbqo"},{"post_id":"cjmd072cv0011qcw6rzw7215r","tag_id":"cjmd072dv0042qcw6kst2qnud","_id":"cjmd072dy004fqcw6mldyrpcv"},{"post_id":"cjmd072cx0015qcw6cz5y0k9y","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dy004gqcw6ihbhstwe"},{"post_id":"cjmd072cx0015qcw6cz5y0k9y","tag_id":"cjmd072dv0042qcw6kst2qnud","_id":"cjmd072dz004hqcw6cmm2qkib"},{"post_id":"cjmd072cy0019qcw6dcpt8kzu","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072dz004kqcw6vtmrful9"},{"post_id":"cjmd072cy0019qcw6dcpt8kzu","tag_id":"cjmd072dy004eqcw6gwdvaowj","_id":"cjmd072dz004lqcw6r1u0xrsv"},{"post_id":"cjmd072cy0019qcw6dcpt8kzu","tag_id":"cjmd072dz004iqcw6f6pnhfcj","_id":"cjmd072e0004nqcw6cj6rmysp"},{"post_id":"cjmd072d0001dqcw6c7l4tbo5","tag_id":"cjmd072dz004jqcw6o4g6rirf","_id":"cjmd072e0004oqcw6wi89u3pd"},{"post_id":"cjmd072d0001dqcw6c7l4tbo5","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e0004qqcw6uvzehxtf"},{"post_id":"cjmd072d1001hqcw6sweam9cm","tag_id":"cjmd072dz004mqcw6b4ol9vid","_id":"cjmd072e0004sqcw6eithjsi8"},{"post_id":"cjmd072d1001hqcw6sweam9cm","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e1004tqcw6t8vag7cl"},{"post_id":"cjmd072d1001hqcw6sweam9cm","tag_id":"cjmd072e0004pqcw60a9frod2","_id":"cjmd072e1004vqcw65f9m365j"},{"post_id":"cjmd072d7001wqcw6iml78k1e","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e1004wqcw6fqfx2r0f"},{"post_id":"cjmd072d7001wqcw6iml78k1e","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072e2004yqcw6n3bkmqnd"},{"post_id":"cjmd072d7001wqcw6iml78k1e","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072e2004zqcw616ay8yr1"},{"post_id":"cjmd072d7001wqcw6iml78k1e","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072e20051qcw63d7x0ds7"},{"post_id":"cjmd072d8001zqcw67w7br38j","tag_id":"cjmd072dz004mqcw6b4ol9vid","_id":"cjmd072e20052qcw6ri27qaed"},{"post_id":"cjmd072d8001zqcw67w7br38j","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e30054qcw66y3mmp4d"},{"post_id":"cjmd072d8001zqcw67w7br38j","tag_id":"cjmd072e0004pqcw60a9frod2","_id":"cjmd072e30055qcw6y5od35xs"},{"post_id":"cjmd072da0024qcw6k88en422","tag_id":"cjmd072e20050qcw69v5sue83","_id":"cjmd072e50059qcw6ko0naune"},{"post_id":"cjmd072da0024qcw6k88en422","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e5005aqcw6689vju7w"},{"post_id":"cjmd072da0024qcw6k88en422","tag_id":"cjmd072e0004pqcw60a9frod2","_id":"cjmd072e5005cqcw62o5y6x0r"},{"post_id":"cjmd072da0024qcw6k88en422","tag_id":"cjmd072e30056qcw6unc9dagp","_id":"cjmd072e6005dqcw6bslcslce"},{"post_id":"cjmd072da0024qcw6k88en422","tag_id":"cjmd072e30057qcw6fmm9hi3f","_id":"cjmd072e6005fqcw6kyc9vca5"},{"post_id":"cjmd072db0027qcw6aj6tcfif","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072e6005gqcw60vu7v7sg"},{"post_id":"cjmd072db0027qcw6aj6tcfif","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072e6005iqcw66b796ok7"},{"post_id":"cjmd072db0027qcw6aj6tcfif","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072e6005jqcw6n6z13gh7"},{"post_id":"cjmd072db0027qcw6aj6tcfif","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072e7005lqcw601gwuxx7"},{"post_id":"cjmd072dd002bqcw60d7j3mpd","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072e7005mqcw6y6lgs0tq"},{"post_id":"cjmd072dd002bqcw60d7j3mpd","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072e8005oqcw6mklxibnb"},{"post_id":"cjmd072dd002bqcw60d7j3mpd","tag_id":"cjmd072e6005eqcw6awg1l4yt","_id":"cjmd072e8005pqcw62bmh0ncp"},{"post_id":"cjmd072dd002bqcw60d7j3mpd","tag_id":"cjmd072e6005hqcw6ua7rfqfe","_id":"cjmd072e8005qqcw6ug6weeu4"},{"post_id":"cjmd072de002eqcw6hub4nmxr","tag_id":"cjmd072e7005kqcw6xs3yiafq","_id":"cjmd072e9005tqcw6uuxk68nw"},{"post_id":"cjmd072de002eqcw6hub4nmxr","tag_id":"cjmd072e8005nqcw6rq3dh4gq","_id":"cjmd072ea005uqcw67bshnzy5"},{"post_id":"cjmd072de002eqcw6hub4nmxr","tag_id":"cjmd072e9005rqcw6u7u2hnyi","_id":"cjmd072ea005wqcw6593sluv8"},{"post_id":"cjmd072dg002jqcw6br15sstp","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072ea005yqcw6ptpvlzka"},{"post_id":"cjmd072dg002jqcw6br15sstp","tag_id":"cjmd072dk002wqcw6qtnhml2f","_id":"cjmd072ea005zqcw641mgj482"},{"post_id":"cjmd072dg002jqcw6br15sstp","tag_id":"cjmd072ea005vqcw6wrl5my0h","_id":"cjmd072eb0061qcw6s0o7ljiu"},{"post_id":"cjmd072dg002jqcw6br15sstp","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072eb0062qcw61m17ukpz"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072cd0006qcw6d8weaaxz","_id":"cjmd072ec0066qcw6w310q5ca"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072ea005xqcw66i7lq2mm","_id":"cjmd072ec0067qcw68trubimq"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072ed0069qcw6jgdx2b2k"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072eb0060qcw65zps3nqu","_id":"cjmd072ed006aqcw6r8pq0clz"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072cr000rqcw6opovqozs","_id":"cjmd072ed006cqcw6hlv6br9p"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072eb0063qcw68oy69ufq","_id":"cjmd072ed006dqcw6s9a90gn1"},{"post_id":"cjmd072dh002mqcw6vp0xmfap","tag_id":"cjmd072eb0064qcw6v93eb8ml","_id":"cjmd072ee006eqcw6600awpg6"},{"post_id":"cjmd072dj002pqcw61zo6vmeo","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072ee006fqcw6tvup4y57"},{"post_id":"cjmd072dj002pqcw61zo6vmeo","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072ee006gqcw63vne1p7g"},{"post_id":"cjmd072dk002tqcw6gtkb4urj","tag_id":"cjmd072ec0068qcw6zvsj1xjx","_id":"cjmd072ee006hqcw69jyufck8"},{"post_id":"cjmd072dk002tqcw6gtkb4urj","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072ee006iqcw6c5n4k43z"},{"post_id":"cjmd072dk002tqcw6gtkb4urj","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072ee006jqcw6azngq0d8"},{"post_id":"cjmd072dk002xqcw6fhwjjsvj","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd072ee006kqcw6ru71b0w7"},{"post_id":"cjmd072dk002xqcw6fhwjjsvj","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd072ee006lqcw6vxwp9y7h"},{"post_id":"cjmd072dk002xqcw6fhwjjsvj","tag_id":"cjmd072dq003hqcw6q3o5wv4h","_id":"cjmd072ee006mqcw69wokd6d7"},{"post_id":"cjmd072dk002xqcw6fhwjjsvj","tag_id":"cjmd072d0001fqcw62arta76c","_id":"cjmd072ee006nqcw6jwr0vc54"},{"post_id":"cjmd072d2001lqcw6y1vijoci","tag_id":"cjmd072ci000cqcw6k6vqnnm8","_id":"cjmd09ll700011kw6mf4w4y48"},{"post_id":"cjmd072d2001lqcw6y1vijoci","tag_id":"cjmd09qao00031kw65049qx42","_id":"cjmd09qao00041kw6c777u97o"},{"post_id":"cjmd072d2001lqcw6y1vijoci","tag_id":"cjmd072ck000hqcw6bgyqs0ck","_id":"cjmd09uzr00051kw64r1jkr38"}],"Tag":[{"name":"abstractive summarization","_id":"cjmd072cd0006qcw6d8weaaxz"},{"name":"math","_id":"cjmd072ci000cqcw6k6vqnnm8"},{"name":"machinelearning","_id":"cjmd072ck000hqcw6bgyqs0ck"},{"name":"theory","_id":"cjmd072cn000mqcw6wetadc5r"},{"name":"nlp","_id":"cjmd072cr000rqcw6opovqozs"},{"name":"linearalgebra","_id":"cjmd072cv000zqcw6yzlq9thu"},{"name":"code","_id":"cjmd072d0001fqcw62arta76c"},{"name":"machine learning","_id":"cjmd072dk002wqcw6qtnhml2f"},{"name":"python","_id":"cjmd072dq003hqcw6q3o5wv4h"},{"name":"convex optimization","_id":"cjmd072du003tqcw66w99htxr"},{"name":"android","_id":"cjmd072dv0042qcw6kst2qnud"},{"name":"server","_id":"cjmd072dy004eqcw6gwdvaowj"},{"name":"linux","_id":"cjmd072dz004iqcw6f6pnhfcj"},{"name":"dpp","_id":"cjmd072dz004jqcw6o4g6rirf"},{"name":"lda","_id":"cjmd072dz004mqcw6b4ol9vid"},{"name":"mcmc","_id":"cjmd072e0004pqcw60a9frod2"},{"name":"inference","_id":"cjmd072e20050qcw69v5sue83"},{"name":"variational inference","_id":"cjmd072e30056qcw6unc9dagp"},{"name":"em","_id":"cjmd072e30057qcw6fmm9hi3f"},{"name":"c++","_id":"cjmd072e6005eqcw6awg1l4yt"},{"name":"algorithm","_id":"cjmd072e6005hqcw6ua7rfqfe"},{"name":"web","_id":"cjmd072e7005kqcw6xs3yiafq"},{"name":"hexo","_id":"cjmd072e8005nqcw6rq3dh4gq"},{"name":"github","_id":"cjmd072e9005rqcw6u7u2hnyi"},{"name":"statistical learning","_id":"cjmd072ea005vqcw6wrl5my0h"},{"name":"seq2seq","_id":"cjmd072ea005xqcw66i7lq2mm"},{"name":"rnn","_id":"cjmd072eb0060qcw65zps3nqu"},{"name":"lstm","_id":"cjmd072eb0063qcw68oy69ufq"},{"name":"gru","_id":"cjmd072eb0064qcw6v93eb8ml"},{"name":"tensorflow","_id":"cjmd072ec0068qcw6zvsj1xjx"},{"name":"bayes","_id":"cjmd09qao00031kw65049qx42"}]}}