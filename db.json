{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"source/novel/index.html","path":"novel/index.html","modified":1,"renderable":0},{"_id":"source/novel/main.js","path":"novel/main.js","modified":1,"renderable":0},{"_id":"source/novel/style.css","path":"novel/style.css","modified":1,"renderable":0},{"_id":"source/novel/tttt.js","path":"novel/tttt.js","modified":1,"renderable":0},{"_id":"source/about/index/divcnn1.png","path":"about/index/divcnn1.png","modified":1,"renderable":0},{"_id":"source/about/index/multi_view.png","path":"about/index/multi_view.png","modified":1,"renderable":0},{"_id":"themes/next-reloaded/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/me.jpg","path":"images/me.jpg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/affix.js","path":"js/affix.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/exturl.js","path":"js/exturl.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/js.cookie.js","path":"js/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/post-details.js","path":"js/post-details.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/scroll-cookie.js","path":"js/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/scrollspy.js","path":"js/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"source/about/index/divcnn2.png","path":"about/index/divcnn2.png","modified":1,"renderable":0},{"_id":"source/novel/ink.js","path":"novel/ink.js","modified":1,"renderable":0},{"_id":"source/about/index/bert_nmt.png","path":"about/index/bert_nmt.png","modified":1,"renderable":0},{"_id":"themes/next-reloaded/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/README.md","path":"lib/algolia-instant-search/README.md","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/LICENSE","path":"lib/algolia-instant-search/LICENSE","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.css.map","path":"lib/algolia-instant-search/instantsearch.min.css.map","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"source/about/index/bert_crf.png","path":"about/index/bert_crf.png","modified":1,"renderable":0},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/images/bp.jpg","path":"images/bp.jpg","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"source/about/index/index.jpeg","path":"about/index/index.jpeg","modified":1,"renderable":0},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.js.map","path":"lib/algolia-instant-search/instantsearch.min.js.map","modified":1,"renderable":1}],"Cache":[{"_id":"source/google785ae9e190b58461.html","hash":"b11b73e5b97fbbffd50fa25ccf00020d4780e6de","modified":1563767123524},{"_id":"themes/next-reloaded/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1561990830000},{"_id":"source/robots.txt","hash":"87926b2573818fe2a55dd4b5538b4984c0c61dad","modified":1563767123775},{"_id":"themes/next-reloaded/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1561990830000},{"_id":"themes/next-reloaded/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1561990830000},{"_id":"themes/next-reloaded/.gitignore","hash":"b80cec1d5e6a73d1cec382aad8046d1352a1e963","modified":1561990830000},{"_id":"themes/next-reloaded/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1561990830000},{"_id":"themes/next-reloaded/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1561990830000},{"_id":"themes/next-reloaded/.all-contributorsrc","hash":"d139a3b623b2e40bbff5c96ad44adf7dbdbc5be1","modified":1561990830000},{"_id":"themes/next-reloaded/bower.json","hash":"e6a80b9ed2d618d1cca5781952c67167a7cfac07","modified":1561990830000},{"_id":"themes/next-reloaded/README.md","hash":"7958c3f70b2133b479ddaf525cc4b6d87a37e04a","modified":1561990830000},{"_id":"themes/next-reloaded/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1561990830000},{"_id":"themes/next-reloaded/_config.yml","hash":"f6d67e85379352bc7b2ec3fe5c5fcfef4287c2b1","modified":1564714872244},{"_id":"themes/next-reloaded/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1561990830000},{"_id":"themes/next-reloaded/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1561990830000},{"_id":"themes/next-reloaded/gulpfile.coffee","hash":"23bd9587807edc4dbecb5c5a29ab96ade24458b5","modified":1561990830000},{"_id":"themes/next-reloaded/package.json","hash":"5870ec2b6d2159a57d84c67bad0a535b76398d5a","modified":1561990830000},{"_id":"source/_posts/CorEx.md","hash":"b99b285e54ef47841ba4379454d47dd59c56e876","modified":1564713744682},{"_id":"source/_posts/AM-Model-for-ASS.md","hash":"47950673449b7fa70c8ed85ecc0c6d969c17710c","modified":1563767122715},{"_id":"source/_posts/Lagrange.md","hash":"12737d24dfdb7fec66ac6eee2bb9fac9a98995c6","modified":1563767122736},{"_id":"source/_posts/LinearAlgebra1.md","hash":"6849e51041a210878e2632e858a3d9e5e39f599f","modified":1563767122758},{"_id":"source/_posts/LinearAlgebra2.md","hash":"d59aaf67b6f15bfe74f356b063b4f42db5c94ac3","modified":1563767122781},{"_id":"source/_posts/LinearAlgebra3.md","hash":"906c687c8907d31571d5636ca6dae41a9e49fd95","modified":1563767122803},{"_id":"source/_posts/MachineLearningNote.md","hash":"8a100e6bc7e54878e30a2ed6cc754a5457a79f9f","modified":1563767122826},{"_id":"source/_posts/PaperReading.md","hash":"36f2753b6cdf6a53f6b1eabfd3e7e6797418d238","modified":1563767122869},{"_id":"source/_posts/NLPBasic.md","hash":"4b8efc8b26c55517b0f403c3ab4f9c9a40de1c60","modified":1563767122847},{"_id":"source/_posts/PaperReading3.md","hash":"4758eb4042d12962db416baeba8c4f6d41932f5b","modified":1563767122909},{"_id":"source/_posts/PaperReading2.md","hash":"5182f5c1582b8bb7125664c76c9790f80523004c","modified":1563767122889},{"_id":"source/_posts/PythonNotes.md","hash":"a3c8ca268bf511ffced0fda0d15746d0e4be5051","modified":1563767122930},{"_id":"source/_posts/acl2019.md","hash":"8493558ce68554a58241217091755cd7acee43ef","modified":1565081549441},{"_id":"source/_posts/TitanicLinearRegression.md","hash":"a2f8704ef01d8535fc47d4216ccca15a8763fd9f","modified":1563767122950},{"_id":"source/_posts/buptroomreview.md","hash":"a52a70aeec208e77e9f5f92c8bd1b6b4b383fc9c","modified":1563767122972},{"_id":"source/_posts/compute-future.md","hash":"64a8fd313fb78bd59bd7f515f7cb8526040c4c63","modified":1563767123026},{"_id":"source/_posts/convex-optimization.md","hash":"288f643c625aed2a45f122f8ebb841fba3550bf4","modified":1563767123043},{"_id":"source/_posts/dachuangserver.md","hash":"67c512fa4a8385be35da3601816fb3512214b6ae","modified":1563767123095},{"_id":"source/_posts/gcn.md","hash":"27c0ea22a2b687a5128a8e5c9dd93a1481ba8851","modified":1564537401469},{"_id":"source/_posts/dachuang.md","hash":"71f17115f4575993da889f0f1bfd17e2176e164a","modified":1563767123063},{"_id":"source/_posts/deepbayes2018.md","hash":"fd137c224c62db787d4ea81d5a1df653c6b3a235","modified":1563767123118},{"_id":"source/_posts/glove.md","hash":"40896bca20cacdb6b6f0773f8194f6598535d976","modified":1563788120866},{"_id":"source/_posts/inference-algorithm.md","hash":"c78565b379ea8d95b6c21652dd3298384b2daa90","modified":1563767123180},{"_id":"source/_posts/lda.md","hash":"04be40380475f8ec6799be709496d54e9b140fc4","modified":1563767123225},{"_id":"source/_posts/kmeans.md","hash":"e94a2f6fd069e3ac4bb6daffb69e90ebf19fcbcf","modified":1563767123201},{"_id":"source/_posts/lr-and-me.md","hash":"26ea8a3bfbf33cb84d75939b9e1209e0bcdd38fc","modified":1564454158455},{"_id":"source/_posts/numpycookbook.md","hash":"2688bf16ab9792ffce43a0f8b1b021af7a525b41","modified":1563767123267},{"_id":"source/_posts/oj.md","hash":"c61a7ab8b66ebb2da72b2164bdd47d36e1eb0da2","modified":1563767123295},{"_id":"source/_posts/pandas-skill.md","hash":"feeac602a8dfa2222f51efac1da8ad009d7964c6","modified":1563767123315},{"_id":"source/_posts/seq2seq-summarization.md","hash":"af1ebb1250fb1c63f0c5b8ee39a7f383ee4c8f5c","modified":1563767123338},{"_id":"source/_posts/statistical-handwriting.md","hash":"492cb6ad4047c25733dd20f9199045388987a588","modified":1563767123377},{"_id":"source/_posts/trie.md","hash":"9906a2df8617fcff56421faf6c121dd2d101e390","modified":1563767123396},{"_id":"source/_posts/vae.md","hash":"cd60021acc056e8b87f85c49d29766a06a3bb6ea","modified":1563767123415},{"_id":"source/about/_config.yml","hash":"1d754ccbbda1320c11b108da5392f33ddfe197b0","modified":1563767123443},{"_id":"source/about/index.md","hash":"0a8e8d83272b32da309b3fe7b4ceaf32e420e2d1","modified":1563867911830},{"_id":"source/categories/_config.yml","hash":"1d754ccbbda1320c11b108da5392f33ddfe197b0","modified":1563767123497},{"_id":"source/categories/index.md","hash":"8874114a645cce1632f79ceb0640102147da9f8b","modified":1563767123509},{"_id":"source/novel/index.html","hash":"0b747f0fa56800dcbfb4e1ee84a3c66436d5630e","modified":1563767123547},{"_id":"source/_posts/setupmywebsite.md","hash":"b202f850c8e740a23359b41115e3ded76d6a3f99","modified":1563849473142},{"_id":"source/novel/main.js","hash":"1fa407d1aa5b00520b3c7844083d5029c4938f72","modified":1563767123708},{"_id":"source/novel/style.css","hash":"24042fee064ded292478f9cca2bb403c333af68e","modified":1563767123733},{"_id":"source/tags/index.md","hash":"fcc3c8b7ee911a1fdf407995bbd43ea7490681b4","modified":1563767123818},{"_id":"source/novel/tttt.js","hash":"6007dbe65bfd5c39249cb498d546986ec963903a","modified":1563767123753},{"_id":"source/tags/_config.yml","hash":"1d754ccbbda1320c11b108da5392f33ddfe197b0","modified":1563767123805},{"_id":"themes/next-reloaded/.github/CODE_OF_CONDUCT.md","hash":"c149f003d03501565e7688915cd8f2e99fbf8f42","modified":1561990830000},{"_id":"themes/next-reloaded/.github/CONTRIBUTING.md","hash":"7ce6cdc8adcbfda68fcbcc54c8b9fd3434a37993","modified":1561990830000},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1561990830000},{"_id":"themes/next-reloaded/.github/PULL_REQUEST_TEMPLATE.md","hash":"3239625bb2573e61f7bcce27a74882a9ff7021e9","modified":1561990830000},{"_id":"themes/next-reloaded/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1563767124034},{"_id":"themes/next-reloaded/.github/auto_assign.yml","hash":"cb68a1dca1c4623448c2ca899614a9f21df1b036","modified":1561990830000},{"_id":"themes/next-reloaded/.github/eslint-disable-bot.yml","hash":"16541fb7b80f5ab90135db96285badb63c4d7d3e","modified":1561990830000},{"_id":"themes/next-reloaded/.github/config.yml","hash":"8a5cbf5aa9529390fe0a782758aca9c3a02f9dcf","modified":1561990830000},{"_id":"themes/next-reloaded/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1561990830000},{"_id":"themes/next-reloaded/.github/stale.yml","hash":"41bf97ee86b8940a0b2e754499ec77fd2b44b717","modified":1561990830000},{"_id":"themes/next-reloaded/.github/lock.yml","hash":"585d2c471047be320aa62f2b74dad797bf09c530","modified":1561990830000},{"_id":"themes/next-reloaded/.github/release-drafter.yml","hash":"c9fdbbdf712327a8ae1ed5972973a75802e245bc","modified":1561990830000},{"_id":"themes/next-reloaded/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1561990830000},{"_id":"themes/next-reloaded/.github/weekly-digest.yml","hash":"404e4ccb7fcd6587bc9b0247a7a7ff256d21f2cb","modified":1561990830000},{"_id":"themes/next-reloaded/.github/topissuebot.yml","hash":"10665bf2b5aba351725715c71e94ad183a0e8f18","modified":1561990830000},{"_id":"themes/next-reloaded/docs/ALGOLIA-SEARCH.md","hash":"0d2f22ea09dd1ef63c66164e048d8239d2ccb2b8","modified":1561990830000},{"_id":"themes/next-reloaded/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1561990830000},{"_id":"themes/next-reloaded/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1561990830000},{"_id":"themes/next-reloaded/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1561990830000},{"_id":"themes/next-reloaded/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1561990830000},{"_id":"themes/next-reloaded/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1561990830000},{"_id":"themes/next-reloaded/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1561990830000},{"_id":"themes/next-reloaded/docs/MATH.md","hash":"026d2cff73c22a30ea39c50783557ff4913aceac","modified":1561990830000},{"_id":"themes/next-reloaded/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1563767124393},{"_id":"themes/next-reloaded/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"212a36d57495990b5f56e46ca8dce1d76c199660","modified":1561990830000},{"_id":"themes/next-reloaded/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1561990830000},{"_id":"themes/next-reloaded/languages/de.yml","hash":"9e524b2bdfb848504b93a51c5650e76bba5fa9e0","modified":1561990830000},{"_id":"themes/next-reloaded/languages/id.yml","hash":"1c4868837f5109f1df863b04fe627352c31d404b","modified":1561990830000},{"_id":"themes/next-reloaded/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1561990830000},{"_id":"themes/next-reloaded/languages/es.yml","hash":"1752429687861b5cedd063c6ebe5dacefbe7e5a7","modified":1561990830000},{"_id":"themes/next-reloaded/languages/it.yml","hash":"b30ff77ad8044e3b021a3b09187cd377dc789fd2","modified":1561990830000},{"_id":"themes/next-reloaded/languages/ja.yml","hash":"1dc35e436da6214cdb3c2ff44bc4a06d0be5b9a0","modified":1561990830000},{"_id":"themes/next-reloaded/languages/fa.yml","hash":"cd41db832af5e399590b70a5227cfe0b0e98e101","modified":1561990830000},{"_id":"themes/next-reloaded/languages/fr.yml","hash":"7005c2b42c2c6e82bd7a1be5cc2f443b5fc79105","modified":1561990830000},{"_id":"themes/next-reloaded/languages/nl.yml","hash":"1c44b3cb2f817808607f3bf6ef47f58ce7599995","modified":1561990830000},{"_id":"themes/next-reloaded/languages/pt.yml","hash":"8ddac820e2c17b484b56c0da8881e142b10e221b","modified":1561990830000},{"_id":"themes/next-reloaded/languages/pt-BR.yml","hash":"08b913a5cf4cc160083069cb4dfb2d66eecd1218","modified":1561990830000},{"_id":"themes/next-reloaded/languages/ko.yml","hash":"20bfaa7600d35235996c18e5c13dcef89c119626","modified":1561990830000},{"_id":"themes/next-reloaded/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1561990830000},{"_id":"themes/next-reloaded/languages/vi.yml","hash":"ba7aff8f88e03f69a0acf7f1b90ee03e077ee88e","modified":1561990830000},{"_id":"themes/next-reloaded/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1561990830000},{"_id":"themes/next-reloaded/languages/uk.yml","hash":"1eb59e581568da9a81d6e20541b4ada5fc1c55c0","modified":1561990830000},{"_id":"themes/next-reloaded/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1561990830000},{"_id":"themes/next-reloaded/languages/zh-CN.yml","hash":"1a4818725409bfffc898824c36756779adb0294e","modified":1563779041640},{"_id":"themes/next-reloaded/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/helpers.js","hash":"a70bfad3efda76738dab12e28e8b75e3989ee3da","modified":1563767128030},{"_id":"themes/next-reloaded/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_layout.swig","hash":"74701fcf2303d59400587436ab4c244e04df7ad9","modified":1561990830000},{"_id":"themes/next-reloaded/layout/index.swig","hash":"9b4733d037c360e8504645b1d6c6dd17817c9d7b","modified":1561990830000},{"_id":"themes/next-reloaded/layout/archive.swig","hash":"7e8f3a41a68e912f2b2aaba905d314306ccaf794","modified":1561990830000},{"_id":"themes/next-reloaded/layout/post.swig","hash":"f74929fd792541916eb25c2addfb35431be071ba","modified":1561990830000},{"_id":"themes/next-reloaded/layout/category.swig","hash":"dda0e6b2139decaf5e865d22ec9d45fdb615a703","modified":1561990830000},{"_id":"themes/next-reloaded/layout/schedule.swig","hash":"3268dd3d90d8b0e142cfa1a2ebb23355baeda148","modified":1561990830000},{"_id":"themes/next-reloaded/layout/page.swig","hash":"29c64c7031aaf276d3d11cdf2e95025996fd6eed","modified":1561990830000},{"_id":"themes/next-reloaded/layout/tag.swig","hash":"a6be69a90924c9d2f4d90fb4867234859bd2c2e9","modified":1561990830000},{"_id":"themes/next-reloaded/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1561990830000},{"_id":"themes/next-reloaded/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1561990830000},{"_id":"themes/next-reloaded/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1561990830000},{"_id":"source/_posts/coling.md","hash":"c334ce3a185f43aab9a70f90f770cc664bd3011a","modified":1563767123004},{"_id":"themes/next-reloaded/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"source/about/index/divcnn1.png","hash":"09e2ce19be4a2a0bcf5095a696b6f36e8da378b5","modified":1563866636105},{"_id":"source/about/index/multi_view.png","hash":"9aae7c4b9ec22c5368ba648b617d56d391842d8e","modified":1563867140190},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c37a60580c901c79ccb22564b228a46e06207445","modified":1561990830000},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE/feature-request.md","hash":"07c423cce4157b8e2dbf60907ccbf3f18c4cf98a","modified":1561990830000},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"57e1e06e845193e80c7df4a4454af28352526f7a","modified":1561990830000},{"_id":"themes/next-reloaded/.github/ISSUE_TEMPLATE/non-english.md","hash":"0b0727ff4d5180ae67f930fb4f8e9488e33eda9f","modified":1561990830000},{"_id":"themes/next-reloaded/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1561990830000},{"_id":"themes/next-reloaded/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1561990830000},{"_id":"themes/next-reloaded/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"caa624092175d44e3d3a8c6ca23922718da2354c","modified":1561990830000},{"_id":"themes/next-reloaded/docs/ru/README.md","hash":"4d7ef717d0b57288e606996ee56c20ffd59d5a99","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/CONTRIBUTING.md","hash":"650fcb9135b6f09d48e866c19e0dbccd831367f1","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"b218e30df4126b6adc87684775ac4c86ea7f7958","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/MATH.md","hash":"83feca62190abcca0332915ffe0eefe582573085","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"5da70d7fa0c988a66a469b9795d33d471a4a4433","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"115ffbde2b3ce01ef1f8c2b3833e6f6794650132","modified":1561990830000},{"_id":"themes/next-reloaded/docs/zh-CN/README.md","hash":"cdd7a8bdcf4a83ff4c74ee6c95c6bcc0b8c1831c","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/filters/exturl.js","hash":"79ad823ca803cb00e0bfc648aa6c9d59711e0519","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/helpers/engine.js","hash":"60eb1554456d9d0e5afc4a2d16f1580a0aa02da8","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/helpers/next-url.js","hash":"799a042bbf497a4c7a2981aa2014ff28fa1bb382","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/button.js","hash":"f3b4f7ae7e58072bbf410d950a99a0b53cbc866d","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/exturl.js","hash":"d605918cf819887e9555212dbe12da97fd887a0b","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/full-image.js","hash":"fcb41c1c81560ed49dc4024654388a28ee7d32b0","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/group-pictures.js","hash":"598220fa92ff3540dcab74f633ba41523daa8364","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/include-raw.js","hash":"5db59d56f4f4082382bf1c16722e6c383892b0c5","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1561990830000},{"_id":"themes/next-reloaded/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_macro/post-copyright.swig","hash":"0790ddbc349508d7ece45a9a4391d0a1cd7263cc","modified":1563767125488},{"_id":"themes/next-reloaded/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1563767125510},{"_id":"themes/next-reloaded/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1563767125624},{"_id":"themes/next-reloaded/layout/_macro/post-collapse.swig","hash":"89b0a0e64637bf5b0cfea0a23642df3d95eedfa4","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1563767125575},{"_id":"themes/next-reloaded/layout/_macro/post.swig","hash":"7920866b88d2c6c2ad0e2e7201e58d37fb0d7cff","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1563767125656},{"_id":"themes/next-reloaded/layout/_macro/sidebar.swig","hash":"480d93619479dcfcbec6906803bb38b2dfbeae53","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/footer.swig","hash":"589f545333e21a8c7823bce89ab45cf1eb7db6e2","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/comments.swig","hash":"784356dd77fe96ea1bc4cb0008e2b40de71bf2f0","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1563767125953},{"_id":"themes/next-reloaded/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/github-banner.swig","hash":"6357537ac0bb114aed4d61bafb39e6690a413697","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/post-edit.swig","hash":"06dac109504812b63766a80ede9ddacbd42d227d","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1563767126270},{"_id":"themes/next-reloaded/layout/_scripts/commons.swig","hash":"50be1762f60222379a8bef5e42ab1a0f3872b7ff","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/exturl.swig","hash":"61ae10d41f67ece004a025077fdb28724af05090","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/next-boot.swig","hash":"012e3ece672cc3b13d5e032139f328d3426d7d65","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/noscript.swig","hash":"edaff4766e0c05fd5c889d9dd32884d376bef9d9","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/scroll-cookie.swig","hash":"ccd13d73429ef91ef5e8b7d9fa43c8188facdf41","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/vendors.swig","hash":"9cd491b8ff2dc9d6976cd9e89c4e56678e3bcefa","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/baidu-push.swig","hash":"4ccf2abbfd070874265b0436a3eff21f7c998dfb","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/chatra.swig","hash":"aa0893cddc803bd3fd34ab78d7d003bd86be86b6","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/bookmark.swig","hash":"10b61a8bac671e375916a4d234c120117098a78f","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1563767127301},{"_id":"themes/next-reloaded/layout/_third-party/copy-code.swig","hash":"a7126355227236f9433615edfd89e86fd51ed676","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1563767127321},{"_id":"themes/next-reloaded/layout/_third-party/mermaid.swig","hash":"d6e6ddda836bd9e2e8d9767a910c7d3280080e81","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/pangu.swig","hash":"c28f9dc96ab735daeb7f599f86470aa5a83c03cf","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/pdf.swig","hash":"810a9b2a6059f46c4a2ddb178f1eaa4c5e23750b","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/needsharebutton.swig","hash":"2c4a66be4677d3e4dec3f169ac8a769098dad1fe","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/rating.swig","hash":"cbe40cb67dad15ade967b0f396c1a95b6871f76a","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/quicklink.swig","hash":"7757bd285732e857996b99af9d917953589fac5e","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/schedule.swig","hash":"2398e5cd0cb466953b6e7a42c2b2caddebf3c348","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1563767127575},{"_id":"themes/next-reloaded/layout/_third-party/tidio.swig","hash":"912368c41de675f458b267a49a99ae3e7e420ebb","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/main.styl","hash":"e010ec8ac73268a0f137204c89e0080ab8d59b3d","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/me.jpg","hash":"18e72cb16043219176b3cefe8b330f27f0cc502d","modified":1563767132486},{"_id":"themes/next-reloaded/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/searchicon.png","hash":"025d64ba0160a3a2257dd2b3032b5f7c9dd9b82b","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/utils.js","hash":"81913c5f75d0949443833cf4269ad63bd7f9be6f","modified":1561990830000},{"_id":"source/about/index/divcnn2.png","hash":"a0c1339149fe399315a44f16267d493aeb58f5f8","modified":1563866648603},{"_id":"source/novel/ink.js","hash":"6ed2ce5b9555de369ef9d3f3fcf312be949bd3ce","modified":1563767123604},{"_id":"themes/next-reloaded/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1561990830000},{"_id":"source/about/index/bert_nmt.png","hash":"f0b5ceaeba59b876de1af6497aa00d1a3976258f","modified":1563867093043},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_macro/menu/menu-item.swig","hash":"9257da95bd032bb3bd1da670e302fd2c7d5610b6","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/head/external-fonts.swig","hash":"fc6bafc8c633afadc538c5afa5620ea2a1cdcb84","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/head/head.swig","hash":"f537846ace6a1afdacf122848dd01a32ceb66006","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/head/head-unique.swig","hash":"02bb5748e8540b024e7f4008a9e640890b45280f","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/header/brand.swig","hash":"648bf7eda66629592cb915c4004534b3913cbc22","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/header/menu.swig","hash":"71af31fea5913fd30c233e555ef13cf2c9768f72","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/header/sub-menu.swig","hash":"5adc60100e129c1d0307bdcaa0c7b8e8375a6ea4","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/page/breadcrumb.swig","hash":"0fa4fadb39467b01cede49f21b22e86b1a2da805","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/page/page-header.swig","hash":"2940df694fff28e8bf71b6546b4162f1e38227db","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/post/reward.swig","hash":"d44f025eb93c99ddf90202d8293ccf80689a00c7","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/post/post-copyright.swig","hash":"3615db591dd910fb9fa96542734c7ec0ef05019c","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/post/wechat-subscriber.swig","hash":"ef11b5be5bfb2f0affe82cf521c002b37fef9819","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/post/post-related.swig","hash":"eea95b785c9c36d28e1839619793f66e89773bee","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/search/algolia-search.swig","hash":"d9fe715fee716f78c7976c4e8838da71439ee0e0","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/search/index.swig","hash":"7d1693416a5dc098f4723a53da2e2d1fc2d6e075","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/search/swiftype.swig","hash":"a5587bd1f60d35e58618576cec45e662aa44ea1f","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1563767126104},{"_id":"themes/next-reloaded/layout/_partials/share/add-this.swig","hash":"15b542f5b06b7532234af367340b9ed9fcebb0ac","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/share/baidushare.swig","hash":"6f181cc188ecbe5e607fd989756e470d4cb9765d","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/share/likely.swig","hash":"b45e934d24d76ec6b6a790e92bdb3d56186b0e2a","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/gemini.swig","hash":"ffc8e8836714ea79abeb77b75859634615652877","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1563767126239},{"_id":"themes/next-reloaded/layout/_scripts/schemes/mist.swig","hash":"108b157fbd1ac3baaf19ae87234fa8728ab79556","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/pisces.swig","hash":"e42604fbb17648484e5f12afe230d826de089388","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/schemes/muse.swig","hash":"0097e45e7b671f8006b8b2d3c4f95cacc76a983c","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_scripts/pages/post-details.swig","hash":"5b05f165547391bf231e52f56f3d925efc09bc44","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/analytics-with-widget.swig","hash":"a5723950c343d220270bfd27bd30050eda6c3fb3","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/baidu-analytics.swig","hash":"591b2ccd9713ccb922b9fcf5e278b6de9c5ec30b","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/application-insights.swig","hash":"798d67e4a736613ab899eabe6529091bbcda7850","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/facebook-sdk.swig","hash":"3d01fa6edc0ad73f81813613f2e8a610777f1852","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/cnzz-analytics.swig","hash":"08cd47ef8572121b7811342d3c9a84a338a18191","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/busuanzi-counter.swig","hash":"8eadb929c9e50e58502ccad2dc2657746f8c592a","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/growingio.swig","hash":"4a966b7ffe2d80ff1b3dd0fd14b355766dc5c70f","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/firestore.swig","hash":"fae69a0e1a1d42f7bb44e594a29857d94594698b","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/google-analytics.swig","hash":"9fa1ca7059243197d8fbbd35108c36629a254570","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/index.swig","hash":"438c6f5e6665d72f4ea7ee206011d669246f6102","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/lean-analytics.swig","hash":"a09d2af2a8470555eeb265b0eb14dc678079e870","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-mta.swig","hash":"92e04a2b9e0c3df594bc22235d1894e5ad458dfc","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/tencent-analytics.swig","hash":"f240a50cd9b627620d9a374a29cf95f0c5e99d7c","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/analytics/vkontakte-api.swig","hash":"0dd5b315d1da55dbfc10f51a1f8952f72eba2720","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/changyan.swig","hash":"3533167c4295637b91d90f3bae7c651cd128bb6e","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/index.swig","hash":"53a59cba82cad49f15a90e1a18007aaac525bddd","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/disqus.swig","hash":"1a00b1b78c429721d6477c2d8f6f68f005285cc8","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/disqusjs.swig","hash":"074a995cd630f56fc4a3135173515c86f2cb34b6","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/livere.swig","hash":"40bab84a4a7a368fa31f0f8ce49af6ec3e5983c9","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1563767127142},{"_id":"themes/next-reloaded/layout/_third-party/comments/gitalk.swig","hash":"e8f91c571ceb4b80aafebc4d36b89fb41b1ae040","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/comments/valine.swig","hash":"15a4d60d3ecc59db2f23629477f8e7b8324981ed","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/math/index.swig","hash":"a7e304b05a44279d3e4f611908d7faef9dc14d7c","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/math/katex.swig","hash":"c2cb2f384bc30d31cdccf9794a729c03e687b45c","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/math/mathjax.swig","hash":"601774d8672577aefbcefac82c94b01f0338da31","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/search/index.swig","hash":"ea94aa85034c6d1b6bb865aecea55c73f8a14501","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1563767127784},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search.swig","hash":"0a13dfd2de52a96901039098c6fc7b515edfc50b","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/search/localsearch.swig","hash":"b3eaab6a269aa3fcbafe24fd06f0c9206dc12716","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1563767127834},{"_id":"themes/next-reloaded/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_mixins/Pisces.styl","hash":"2e8fb29aa92325df39054b5450757858c6cebc41","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_mixins/base.styl","hash":"2036bbb73afd43251982ce824f06c6e88d35a2ef","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_variables/Gemini.styl","hash":"a8aa41625b94cf17a7f473ed10dcbe683b1db705","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_variables/Pisces.styl","hash":"fc15e277d1504532a09b7b1bd31f900ad95ec4b8","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1563767132629},{"_id":"themes/next-reloaded/source/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1563767132679},{"_id":"themes/next-reloaded/source/js/src/bootstrap.js","hash":"1c41508b83cb0c4512e64b4d63afa1be954ce8ef","modified":1563767132720},{"_id":"themes/next-reloaded/source/js/src/motion.js","hash":"b45d2c0d48f2c8e6a0621b8063845f76b89476cc","modified":1563767132837},{"_id":"themes/next-reloaded/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1563767132791},{"_id":"themes/next-reloaded/source/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1563767132967},{"_id":"themes/next-reloaded/source/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1563767132752},{"_id":"themes/next-reloaded/source/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1563767132879},{"_id":"themes/next-reloaded/source/js/src/utils.js","hash":"e437eff1d3781c4a1aec9ff2060565524a37c983","modified":1563767133038},{"_id":"themes/next-reloaded/source/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1563767132998},{"_id":"themes/next-reloaded/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/README.md","hash":"9fa5175cdb7d3d939fe7174b6d68608ca996c174","modified":1563776963325},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1563776963295},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.css","hash":"6e01a39d7f6d58a0895957361b0a942543c18332","modified":1563776963349},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.css.map","hash":"d055d06395af598682873d1b458166dc6f513072","modified":1563776963380},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1561990830000},{"_id":"source/about/index/bert_crf.png","hash":"4ffe02c67803be284ae92fcb1126e01768014126","modified":1563867035960},{"_id":"themes/next-reloaded/source/css/_variables/base.styl","hash":"640f25a63770af5566ccc9cec79c40a4f1c0b29e","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1561990830000},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1563767127670},{"_id":"themes/next-reloaded/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1563767127639},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top-sidebar.styl","hash":"fe5ff961b86004a306778c7d33a85b32e5e00e48","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/buttons.styl","hash":"b98c65006e2546fbf3870c16fbbcbc009dbaab15","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/components.styl","hash":"9d71f34fa13a41b8c8cd2fbdf3fdea608385277c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/back-to-top.styl","hash":"c8b3225396cb444d8baeb94bac78e5216b992a81","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pagination.styl","hash":"ce826aedf42b9eca424a044452f5d193866726a6","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/rainbow.styl","hash":"ce2aae8f3ed8ceac3a2417e0481044cf69c788aa","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1563767130037},{"_id":"themes/next-reloaded/source/css/_common/components/scrollbar.styl","hash":"d7b8bcf2a6031296c84bb4f4ecfb037af01d2d82","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/outline/outline.styl","hash":"7e51ea64611ab5d678c112b4688d4db4fd2737e2","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/base.styl","hash":"6d900b4159eeb869196a619602578bf4d83a117b","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/helpers.styl","hash":"8e0740a9ad349ce5555122325da872923135a698","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/mobile.styl","hash":"9a190ef2f49bdbf69604b48ad1dc7197895ee9b6","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/tables.styl","hash":"33456264a74d1bba38264d14713544d67d003733","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/normalize.styl","hash":"7ffde343bdf10add1f052f3c4308a15180eb4404","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_header.styl","hash":"24230e46fc9fb7b8551f97bb36e9bc1f7423098e","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_menu.styl","hash":"75d2d92af070eb10273558b2436972d3f12b361c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fa33213aceed7bf4bf25437ca9c1a00f7734ae65","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/index.styl","hash":"a96e46a6ae86c423f932bc2bc78b9f7453e4e4e5","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_layout.styl","hash":"6565b4a309325596768d0d32e022c80ef23066cb","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Gemini/index.styl","hash":"9a2d298dbdcbfd758518fd74b63897bc80ce15a5","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_search.styl","hash":"7359880e8d85312861fe0871f58b662e627dae0c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_logo.styl","hash":"fc160583f742c94316a0fee05c18468033173534","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/_menu.styl","hash":"1edf4e69d0ec0dc9cefed6c35d3e803e0da4093d","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_brand.styl","hash":"57044a6d19eb418c1c3d28787e82c69efa9e0ca6","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_layout.styl","hash":"75737591682a2bafa71db4c03fb79e970ac0e7aa","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1563767131704},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_menu.styl","hash":"b6dac5bbf20f090cf4b67d156f030d7170dfb39c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5b936dddb7b4de4720cd1e8428b30a2f06d63fb","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/_sidebar.styl","hash":"6400c98a9fd2b9a8502269f33355bd7ab3ff793b","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Pisces/index.styl","hash":"232aedbd44243b3b80c4503c947060d3269c1afc","modified":1561990830000},{"_id":"themes/next-reloaded/source/js/src/schemes/pisces.js","hash":"ab3932fa3637a5e23ae6287e78fbfeb54f2c85d2","modified":1563767132936},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1563776963271},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1563776962893},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1563776888819},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/config","hash":"6b9d0a36d665af7dc13b7b00969dd505ce623c8b","modified":1563776963131},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/index","hash":"73ade5f81c381369d5a44e09a48f47e1b8b7f976","modified":1563776964124},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/packed-refs","hash":"6521f0ff7bd3127bb97a73505b8a24c3708f2d76","modified":1563776962677},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/footer/footer.styl","hash":"4cfeec9434a72d5efc6ca225d3445d084d4590f7","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/header.styl","hash":"6c4990d375b640ee4551e62c48c1cbe4c3d62212","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/github-banner.styl","hash":"ca97f0b6990eef947039faede80c56d9c4381ee1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-nav.styl","hash":"cc6ee18f47f2e1e06df6fa0eadb37079e580fd11","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/menu.styl","hash":"a410ed529afd46ddf4a96ecf0de6599488716887","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/diff.styl","hash":"71d8d1cc22a2a7627a6db7240f0c4902a14f9bea","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/archive.styl","hash":"6904fd7ea6455e008d9884558b68254608af9a3c","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/breadcrumb.styl","hash":"2d142c6f39853916256ad8fc79eb6b85f4001ae8","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/pages.styl","hash":"f1d52954b9a5d1ca8e224382349f525e598dd923","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/schedule.styl","hash":"80addb9b725e329915c05c27b9fadaf56457a9b3","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/highlight/highlight.styl","hash":"352796ec0a0cbbdb45d2351711d136ae6112b757","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/pages/tag-cloud.styl","hash":"61ca40856e5cacd48e0fa9728fde4605c7dd4c94","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/header/site-meta.styl","hash":"c0d9e18a9210fdcaf33e488518b3b288eb58c0a1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-copyright.styl","hash":"2356226157e8068b0e9bbe2f7d0f74e1ab49199b","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-eof.styl","hash":"c961d37190d9bec58a36306c7e716c4e72c4582f","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-collapse.styl","hash":"6a75bb1f2435f4e895cbbb5abbddf6e8f7257804","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-gallery.styl","hash":"0bf899fab331add63f0c8ead31ca3a3db2ad74d9","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-expand.styl","hash":"df3c19fd447da6d4a807683345007a41338f9a04","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-meta.styl","hash":"67165cd8836e03c289162b96ef06f8b024afe9af","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reward.styl","hash":"5440013a081201ca791582db98159dce93ea9e75","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-reading_progress.styl","hash":"3f33bb862c2aa993f54987fbb345da067b79b112","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-nav.styl","hash":"496f931e3a7e313ba8088fb91bb20789cace72c9","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-type.styl","hash":"d5c8ffed7f2c701052b7a53abaf5ef437374ea72","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-title.styl","hash":"8e058c99dd7d41f0bd34c7c28b6ac9fbb17dcb5e","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post-widgets.styl","hash":"a6c24393dffbdd94dd5c01cdbec5e180b0bfbbbd","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"9224b566cd2632f64c1a964e2c786cee93b93286","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/post/post.styl","hash":"a3170630d8e085889a4bdc20eb7f09c5a0479c47","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-button.styl","hash":"517d541a80d59ad99a3f648be74891e0c7bc72a8","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-author.styl","hash":"707527c9950a7459355c8abcf4751c0964de0bc1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1563767129866},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"cc83816614f21c7e1d8d3f867d547ff7c658cec4","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"c2d9c3b6fbfa65544e6b5a55d3cb2149df04a8a9","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"9a3bfc878ca797946815bed23cd6f92b24a16358","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar.styl","hash":"8e5c884fb950937afa350c608545455c87aa6129","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"c01609176929590f8f347075a9a12b661acd661e","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"8a24b56524a388fbabd408ffc8ba9b56eb9e01ce","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/sidebar/site-state.styl","hash":"967fb3a3c6c851b34ec5df2d945dc266ed63d146","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/exturl.styl","hash":"cf2185a0ea170fd8450f592e859a6c941141e5ee","modified":1563767130116},{"_id":"themes/next-reloaded/source/css/_common/components/tags/blockquote-center.styl","hash":"58ec00eebe68d0eebd2eea435c710063877447df","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/full-image.styl","hash":"6ec8ea7b11a146777b6b8da0f71f0cc1dbd129df","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/label.styl","hash":"d7501ae01fc45fa15b00d1bc5233b9fffa20a3c9","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1563767130231},{"_id":"themes/next-reloaded/source/css/_common/components/tags/pdf.styl","hash":"da8d34729fb6eb0fcb8ee81e67d2be3c02bc1bc4","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/note.styl","hash":"21b32840d8b3a14b10770153114778304ba6d1b0","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tabs.styl","hash":"6e4400d6704dee076434726b7a03ac464eb7bcb4","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/tags/tags.styl","hash":"cbc0be5a3285b469858ec9ead48e2ea90bd47ae1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/algolia-search.styl","hash":"fc58498d4f5081fcf6218e9e18c5bf2328275bef","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1563767130435},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/copy-code.styl","hash":"688ca3eccc26727d050ad098b32b40934719588a","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1563767130406},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/localsearch.styl","hash":"9fac89c8146eb2675721a26f528d7d0f8be7debe","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1563767130464},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/gitalk.styl","hash":"ac7753d536341aa824d7bce0332735e838916995","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/needsharebutton.styl","hash":"61466e3e5459960b5802a267751a0c8018918b0b","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/related-posts.styl","hash":"3ae3f3c276d444862033fd3434c632ad0d2f84e6","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_common/components/third-party/third-party.styl","hash":"dd44d8ca93ad5d366a96d797a0a8b4f3b46f9a77","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1561990830000},{"_id":"themes/next-reloaded/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1563776888977},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1563776888901},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1563776888851},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1563776889068},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1563776889046},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1563776889000},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1563776889093},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1563776888924},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1563776888946},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1563776889023},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1563776889131},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1563776888877},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/logs/HEAD","hash":"0b776d48f6c80e3a26a2230fb5975519309f5c2b","modified":1563776962999},{"_id":"themes/next-reloaded/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1561990830000},{"_id":"themes/next-reloaded/source/images/bp.jpg","hash":"3df9dbed403a89b16af1fe957e0a1c447596a769","modified":1563767132190},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/0d/49463cf444a6411519b6d7b5bb65fa7e96c4dc","hash":"8baa9ed7c87fbf1a9fdeb378c656471cb98c48e5","modified":1563776894079},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/08/777d50ffb9e517f20d2cbf2ea19130862fb501","hash":"110fcf7830673d2a7715f8347834f7393597ecd0","modified":1563776962036},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/2b/a3834eb798ac34ab72ed5fdf7a56d175fee554","hash":"e8edf92b35519a3ec9333c7b54243eca7edc37f2","modified":1563776956996},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/2f/9eba51ec174b1e0c719d12cafa7c3c07140471","hash":"fc994d9d8b3b21ec7c941eea7e3862970e297e9b","modified":1563776893894},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/0c/dada082d621dbfdd00f7020c33dc751129167f","hash":"b490c11cdefde6b331a7d4ddb055e34ad08459d8","modified":1563776893838},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/59/0f6f985f6018c397b2499bb49e599b7109ec06","hash":"650b4b24a81daa3b02646b7ffd5ccdac11ab6be8","modified":1563776957049},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1563776893949},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/92/1eb7be3c529d19b6c92ce67e4099e8f7d6adf3","hash":"023bef5405aee54ae6b00e1e02eb6984b691ccbf","modified":1563776962090},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/74/1e8eedaad6125d0feee4afcc124ba0bffc48c5","hash":"0963b162d9de3c8df0d966c07299efd39389da5d","modified":1563776894131},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/54/7b348ba5c8725ea590087860a352c8ff41cb98","hash":"79281e0d03b3acdbce61914bbfef33ae0c29be6d","modified":1563776893782},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/6d/b4c7bd4b80f34ae7165de7d91291759569d572","hash":"f04a0f8b9dde5d5c36f470634ca8f958a3b6aa15","modified":1563776962130},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/72/77a0daac0e35928cb7e7905e28b7d490f18fea","hash":"fd53bdb026530aa2254de2dbc21f91c6ac4ca66a","modified":1563776894188},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/6d/c836cd5e780134d73d56944a2e7039bacc7cb3","hash":"5a6fc74f4eff0b3bf6429b28df2a5d721c92933a","modified":1563776956934},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/a4/0c9f2635389a60948f0f2235f037277f8acff6","hash":"ec4074d54b477aea533dab38384dd9a52fa58854","modified":1563776894010},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/be/20adccf394f75e31e8f8b5fdf22728a1770602","hash":"665067f8dc3adcb99fa753845e7be4440eed9fc3","modified":1563776893566},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/c7/fbb078f1639009ff35512e07a344065d222948","hash":"123afd34f26169b38c32f4b035562726036fd960","modified":1563776893616},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/c9/af6112eed1dab47eba55651562f768bfbc861d","hash":"a3ce783cca46bdcba0a1eda2d034c77221e07e06","modified":1563776893673},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/da/0b2b5f2ab8f750444499f17e4cbf6074aef7f0","hash":"b343b66897d47a95d9a4363eaab8ff61a1c8a683","modified":1563776893727},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/f5/e09255981809338b668b56510e360595fc12fa","hash":"c385e27cf914367d1fde121a2c920b93832cf2ed","modified":1563776961967},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/refs/heads/master","hash":"00f2f54cafe81cfde55895f7d1f5212b7bce885e","modified":1563776962982},{"_id":"themes/next-reloaded/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1561990830000},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/logs/refs/heads/master","hash":"0b776d48f6c80e3a26a2230fb5975519309f5c2b","modified":1563776963053},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/2b/d5d590d07a161741477ace2339eb37c07cc40c","hash":"10a1f52729a36d3b9d594e93473fef26fed768e1","modified":1563776961896},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.js","hash":"d8f2e1de2142c44500d41debeeb319ba8adb1ccd","modified":1563776963454},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1563776962840},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/a7/8511a30ca600e9db1ecc1a835c0d4b65fbec6b","hash":"c5fbf2c49ade74e34a3ed2cce39a6b54e4686e3e","modified":1563776896229},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/logs/refs/remotes/origin/HEAD","hash":"0b776d48f6c80e3a26a2230fb5975519309f5c2b","modified":1563776962828},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/.git/objects/e4/c7790c85a26069925132383ac286502de41c71","hash":"76b2ebe5b1dc357c9c59c97dbe17c35069257531","modified":1563776956875},{"_id":"source/about/index/index.jpeg","hash":"6ecfd43fafc28a2d85d09e1e96c5ef0bd7497490","modified":1564019538168},{"_id":"themes/next-reloaded/source/lib/algolia-instant-search/instantsearch.min.js.map","hash":"753adf35c67993265fb13b827aeee3f95e3442af","modified":1563776964069},{"_id":"public/novel/index.html","hash":"603e3878adb188e6941dcf71d3b594031ff9ebc1","modified":1565081665092},{"_id":"public/about/index/divcnn1.png","hash":"c50b25502814051d4c2cf452fe61593837c5f56f","modified":1565081665094},{"_id":"public/about/index/multi_view.png","hash":"d549a2cec3a34a58ed3a7e888b109bb47a02bbe6","modified":1565081665094},{"_id":"public/images/algolia_logo.svg","hash":"278ffcea4876b37657f2e192bda48c6bc7dd8784","modified":1565081665095},{"_id":"public/images/apple-touch-icon-next.png","hash":"b972160c147e9bec3a0f7432e6e80dfa92581b0b","modified":1565081665095},{"_id":"public/images/cc-by-nc-nd.svg","hash":"0b9f570195e442174075793d9f8f20d6fbe2bb53","modified":1565081665095},{"_id":"public/images/cc-by-nc-sa.svg","hash":"e7c88caa870024e1aec9f9d68bb1049ecd1224ce","modified":1565081665096},{"_id":"public/images/cc-by-nc.svg","hash":"cfc37914ccd351934f48e5d62f6536be1e08e0c4","modified":1565081665096},{"_id":"public/images/cc-by-nd.svg","hash":"3337dde904ac515c8344823dfb57945219a3ee16","modified":1565081665096},{"_id":"public/images/cc-by-sa.svg","hash":"a55cfa202c7cc2100435fe0b04ec2099eb73c867","modified":1565081665097},{"_id":"public/images/cc-zero.svg","hash":"dc4d120989ebb48b136ecd0fa931245f7a09c274","modified":1565081665097},{"_id":"public/images/favicon-16x16-next.png","hash":"5ee510e58b7b9e062a22da28ce1eb35a2f381021","modified":1565081665097},{"_id":"public/images/cc-by.svg","hash":"da3444198fa9f906c20d10469252ef05802817c6","modified":1565081665097},{"_id":"public/images/favicon-32x32-next.png","hash":"f88e49404e4c2a326e51ae65ea5b2375b5d5fde8","modified":1565081665098},{"_id":"public/images/logo.svg","hash":"d02fedf124aa26d4def2631bbccafa053653abd6","modified":1565081665098},{"_id":"public/images/quote-l.svg","hash":"48136591a60f3dc722d8f00b66e16de2aec3802f","modified":1565081665098},{"_id":"public/images/me.jpg","hash":"80ae6e0386ba8440ffb0a1ac2138622142a17cd6","modified":1565081665098},{"_id":"public/images/searchicon.png","hash":"738c402b78a02397ec8756a4f0aad72989d3e3a0","modified":1565081665099},{"_id":"public/images/quote-r.svg","hash":"dc3f40e3409dd502478a8c6bb41c68d50bd71ca6","modified":1565081665099},{"_id":"public/about/index/divcnn2.png","hash":"3bd499c91c2e321fc16eab95846a79ce130d05ef","modified":1565081665099},{"_id":"public/about/index/bert_nmt.png","hash":"68e35ba8a903b4788165761fa1fb5d6169b2704f","modified":1565081665099},{"_id":"public/lib/algolia-instant-search/README.html","hash":"ff5a5b056d4f3cdac5ec0f252c543de82ea8b78a","modified":1565081665100},{"_id":"public/about/index/bert_crf.png","hash":"fe185c0f336e11e9057d2198a514c8cd424d5e12","modified":1565081665100},{"_id":"public/images/bp.jpg","hash":"a8ced1a0891a2c7dee9774ace1ed201422570f1b","modified":1565081665101},{"_id":"public/baidu_urls.txt","hash":"7a35112f4db00afcea1464b9de6a5794d7fa1967","modified":1565081665101},{"_id":"public/baidusitemap.xml","hash":"cd7b208a50e7c5260f62a8a96de3f611c98bb620","modified":1565081665102},{"_id":"public/search.xml","hash":"bfc093d22a9e03dbf6dab8741c8aaa57b131b753","modified":1565081665102},{"_id":"public/sitemap.xml","hash":"0e16ee466d7140f52c1493253ea426fb82e1e5e0","modified":1565081665102},{"_id":"public/google785ae9e190b58461.html","hash":"24163f86433654c03d63b07f85b0852eb6559696","modified":1565081665104},{"_id":"public/about/index.html","hash":"9b43943527fdb9a629ab15a074ca05c85d1cc7ac","modified":1565081665104},{"_id":"public/categories/index.html","hash":"ce4100cfdbeb9524941ac7a41883f5b2593902f4","modified":1565081665104},{"_id":"public/tags/index.html","hash":"558b85aa32fe993ec24ca9c5b1406344061b6ddd","modified":1565081665104},{"_id":"public/2019/07/29/CorEx/index.html","hash":"c6a1c1c8b626a3eb5facfd730e74f3f7b822bd1f","modified":1565081665105},{"_id":"public/2019/07/28/gcn/index.html","hash":"a038afdfa54cb3576ddf246b997ec66f86c8f7c1","modified":1565081665105},{"_id":"public/2019/07/28/acl2019/index.html","hash":"86dd03007f27f7bd37af5ddad1b1b1a773db4224","modified":1565081665105},{"_id":"public/2019/03/20/vae/index.html","hash":"a54fee9b5091282be62a846985aa3066648ab931","modified":1565081665105},{"_id":"public/2019/01/13/glove/index.html","hash":"f84d69986d8a34efb28047981f2217b5b56b83cf","modified":1565081665106},{"_id":"public/2019/01/03/PaperReading3/index.html","hash":"f3a71d4f19e1150e412c7e3074bc8ce789bd30ec","modified":1565081665106},{"_id":"public/2018/11/16/coling/index.html","hash":"c1afe46e68106a798588ea42958c608562325c02","modified":1565081665106},{"_id":"public/2018/10/14/lr-and-me/index.html","hash":"801121fc3020a0fa61346b9e24c71bb84c5cc957","modified":1565081665107},{"_id":"public/2018/10/13/compute-future/index.html","hash":"88ecb9941c64c59514e0edaf98d953a0d8daf77c","modified":1565081665107},{"_id":"public/2018/09/22/deepbayes2018/index.html","hash":"928dbd366e1bdc8e8aaed04f96deae44fa774b42","modified":1565081665107},{"_id":"public/2018/08/28/inference-algorithm/index.html","hash":"f9ce7638a6c0d717b646c3558a2e92caa5e5cc98","modified":1565081665108},{"_id":"public/2018/08/09/statistical-handwriting/index.html","hash":"620e07e262ab8a8f55980ebb59a67c8cac0bcf3a","modified":1565081665108},{"_id":"public/2018/08/04/convex-optimization/index.html","hash":"0fb7e84fc5e63f1174f44011b442ea9073ec7258","modified":1565081665109},{"_id":"public/2018/07/23/lda/index.html","hash":"3a837d762a311e883ddb82cc418857fd9a7cc14c","modified":1565081665109},{"_id":"public/2018/07/04/seq2seq-summarization/index.html","hash":"0a3fcbbaf016fde0a828382c8114fed435a45252","modified":1565081665109},{"_id":"public/2018/07/03/PaperReading2/index.html","hash":"7e8b94264abec71f586ebde3e64bfb6464259efd","modified":1565081665110},{"_id":"public/2018/03/07/PaperReading/index.html","hash":"761f8ea4bfd63add1e4b9aba8f79f692833cfd6d","modified":1565081665110},{"_id":"public/2018/03/07/NLPBasic/index.html","hash":"5728375cbdaa095f2113e8fc9b6802ef83707401","modified":1565081665110},{"_id":"public/2017/12/27/AM-Model-for-ASS/index.html","hash":"c1354789f08f79e05a6a2109abaa8c3b92fd3c15","modified":1565081665111},{"_id":"public/2017/05/26/dachuangserver/index.html","hash":"f72b99bdc22dbcb200f86312c93af656213c1fe9","modified":1565081665111},{"_id":"public/2017/05/02/trie/index.html","hash":"800233c1c9496349e3177f3bc1d9bc57daa80e4f","modified":1565081665111},{"_id":"public/2017/03/28/PythonNotes/index.html","hash":"b2c4798924d731b7b3bfa21877d4ed9d37aa492a","modified":1565081665112},{"_id":"public/2017/03/27/oj/index.html","hash":"9155c51e531a740255ba5f367e71a0ba10429416","modified":1565081665112},{"_id":"public/2017/03/18/Lagrange/index.html","hash":"2ec8b4d2a17af206dfae14e2c9b23621b25ae8b8","modified":1565081665112},{"_id":"public/2017/03/16/kmeans/index.html","hash":"03309159053766f8673493c8105e8aac05d47955","modified":1565081665112},{"_id":"public/2017/03/09/dachuang/index.html","hash":"b5cfcbadd972b6c753239a870006d0a39bec4c14","modified":1565081665113},{"_id":"public/2017/02/12/MachineLearningNote/index.html","hash":"d78c2be96386c430cc0b3926babbcecde6c1e26e","modified":1565081665113},{"_id":"public/2017/02/07/TitanicLinearRegression/index.html","hash":"a5b3ceb5eac763ecc0ef5d514cbb2a7d8d91658f","modified":1565081665113},{"_id":"public/2017/02/04/pandas-skill/index.html","hash":"1173d80de863c7d6054dabcaa48fedee4d616d7e","modified":1565081665117},{"_id":"public/2017/01/23/numpycookbook/index.html","hash":"6a1f422ec59174b43f01bc6dfc54c49dc0e616f1","modified":1565081665117},{"_id":"public/2017/01/22/LinearAlgebra3/index.html","hash":"2ac362b7d902adbaaf216e7ac08eb6b9f6461c3a","modified":1565081665118},{"_id":"public/2017/01/21/LinearAlgebra2/index.html","hash":"485b816bcbaff7b18623f7579268ddd6ddccb5cc","modified":1565081665118},{"_id":"public/2017/01/21/LinearAlgebra1/index.html","hash":"f21a154efe130e89a909fc3f0d74fbfe43e400bc","modified":1565081665118},{"_id":"public/2017/01/16/setupmywebsite/index.html","hash":"d32730321bee5d7efb9c5ef49464d06ad53c4589","modified":1565081665118},{"_id":"public/2017/01/16/buptroomreview/index.html","hash":"4747d78c28f5fe879ac3336b729a90b83f8304aa","modified":1565081665119},{"_id":"public/archives/index.html","hash":"c472231d3dfb8dbc1811867baec890a1881b1536","modified":1565081665119},{"_id":"public/archives/2017/index.html","hash":"f7e98f9c0bac48ef80d7e0f50db21c23defdb505","modified":1565081665120},{"_id":"public/archives/2017/01/index.html","hash":"c3fd11e25ee54371fa3f45d0f6e429ef6630269e","modified":1565081665120},{"_id":"public/archives/2017/02/index.html","hash":"b9c8998a46b5cabf340413a6144c1c3de904fe04","modified":1565081665120},{"_id":"public/archives/2017/03/index.html","hash":"d07638a31c585d473d416a266a67de9d40780e3c","modified":1565081665120},{"_id":"public/archives/2017/05/index.html","hash":"98f4b720d16fbb3749a85e4981e577c54b7b7c78","modified":1565081665121},{"_id":"public/archives/2017/12/index.html","hash":"b93da9d47f4cc861bf49715110bc382f51ee6837","modified":1565081665121},{"_id":"public/archives/2018/index.html","hash":"e8fddbdbae94c0aca0ab1ae7f45b67ac2c91081e","modified":1565081665121},{"_id":"public/archives/2018/03/index.html","hash":"6298100687684d3310585e19b95ef61559387bdc","modified":1565081665121},{"_id":"public/archives/2018/07/index.html","hash":"1be901c26ac32625ced18d7b0f9573e3af4260d2","modified":1565081665122},{"_id":"public/archives/2018/08/index.html","hash":"3c97d4bfaffa8cf729028d62425254ab3f70cf29","modified":1565081665122},{"_id":"public/archives/2018/09/index.html","hash":"d48f9d0d86e7beaeba847a61423bbfab8eda6999","modified":1565081665122},{"_id":"public/archives/2018/10/index.html","hash":"bf2f3fa2e235afcfbfd562a6a0e609103f83f437","modified":1565081665122},{"_id":"public/archives/2018/11/index.html","hash":"f1591400d05d81677ac8c82663c04317bfa4ba4e","modified":1565081665123},{"_id":"public/archives/2019/index.html","hash":"7b14b443cf08ddeff85105a91104518e66ebf1e8","modified":1565081665123},{"_id":"public/archives/2019/01/index.html","hash":"3b035926769eedfa3b1ce678c62b124be1b6c3a8","modified":1565081665123},{"_id":"public/archives/2019/03/index.html","hash":"c9f8974d3525629c27e1a962f62f8b2701e7955b","modified":1565081665123},{"_id":"public/archives/2019/07/index.html","hash":"cbb477b582c6437c7a631b84651cafc4f7e2d104","modified":1565081665123},{"_id":"public/index.html","hash":"cde9e286e38867c810786e91fc499f78f6d53b4d","modified":1565081665124},{"_id":"public/page/2/index.html","hash":"29488ce5c6c602c923c5ea5fb56b4a72bb4fca5e","modified":1565081665127},{"_id":"public/categories/自然语言处理/index.html","hash":"c9923bea26f8cbd901eddb96f0a092ea35bbedf3","modified":1565081665127},{"_id":"public/categories/机器学习/index.html","hash":"0c8cb0fbfd9f9fa6e0acce886797b5a8fef999cb","modified":1565081665128},{"_id":"public/categories/Python/index.html","hash":"0e699ec953d37bca1568aa1b4cab717ff10a779e","modified":1565081665128},{"_id":"public/categories/数学/index.html","hash":"42754fc58791154ad82e1b3e972d76c715adceba","modified":1565081665128},{"_id":"public/categories/Android/index.html","hash":"f15e042159035372ed5297c41763281ece78dcc9","modified":1565081665129},{"_id":"public/categories/算法/index.html","hash":"2b30de30cd02fb09a7d80d3a1b3c6eab9710ca92","modified":1565081665129},{"_id":"public/categories/瞎折腾/index.html","hash":"e1f767415731e4e58f3972cf156a68a51e4d6922","modified":1565081665129},{"_id":"public/tags/abstractive-summarization/index.html","hash":"3c86d9c360dd3ad053a87f60cd2e4a8b4aaa0a90","modified":1565081665129},{"_id":"public/tags/math/index.html","hash":"3439c708765253d98905356c52bb5b568305998f","modified":1565081665130},{"_id":"public/tags/machinelearning/index.html","hash":"128f0d88d47053d6311fdd92991636edac5afca7","modified":1565081665130},{"_id":"public/tags/theory/index.html","hash":"7746feec91ca84ed203aa76871fd1c97f741d5a8","modified":1565081665131},{"_id":"public/tags/nlp/index.html","hash":"5c6a905da98f7fb562d8ae83aed7ce74bddbed4d","modified":1565081665131},{"_id":"public/tags/code/index.html","hash":"7430d92363fed3924710105b37373dc5c7f44cc3","modified":1565081665131},{"_id":"public/tags/python/index.html","hash":"c645b4b99aa28e449af6eba71ce22d258a99d541","modified":1565081665132},{"_id":"public/tags/comprehension/index.html","hash":"75a84c64de25bee2693fc5dec1afe9b3a33925c4","modified":1565081665132},{"_id":"public/tags/NLI/index.html","hash":"74553ad80063e2b2027c06e057e8af76876c77f9","modified":1565081665132},{"_id":"public/tags/convex-optimization/index.html","hash":"b8aaaafe08e8c9f5fcd7208a52bc350667382e1a","modified":1565081665133},{"_id":"public/tags/server/index.html","hash":"2971a154070dd69626b444b0a9a23384db1eeef0","modified":1565081665133},{"_id":"public/tags/linux/index.html","hash":"e95bf1f8eca0b538cf328ebfa5d3e821f6bb4bbd","modified":1565081665133},{"_id":"public/tags/corex/index.html","hash":"baccb591f5b3b6e3acbd4ed7a44c6075bc8e784d","modified":1565081665133},{"_id":"public/tags/machine-learning/index.html","hash":"79a16ba3d4e5d9c951b7862bccd2a641075d4eec","modified":1565081665134},{"_id":"public/tags/topic-model/index.html","hash":"bb5f0688f2bdb1f4c1dd26406e1f672018f847af","modified":1565081665134},{"_id":"public/tags/acl/index.html","hash":"666239e8db3b10da36d67e536d9f4c1f9b9c8cfa","modified":1565081665135},{"_id":"public/tags/natural-language-processing/index.html","hash":"3d580a905c5f5bff245e7e2f7305de8d9086f4d2","modified":1565081665135},{"_id":"public/tags/android/index.html","hash":"176c54b2812953491f10c09e7a71ce3f35b1086b","modified":1565081665135},{"_id":"public/tags/gcn/index.html","hash":"b04816fc3d93608239375999bf67c3e3bf2beaec","modified":1565081665136},{"_id":"public/tags/glove/index.html","hash":"2505b042dec53a3d643ae6cdf3e00ad0006900d1","modified":1565081665136},{"_id":"public/tags/word-embedding/index.html","hash":"c8685dde739d0a0118a2815ce83c0739ad948672","modified":1565081665136},{"_id":"public/tags/logistic-regression/index.html","hash":"741db9c2ce72cdcce59b556cc5d8c706833c6cf5","modified":1565081665136},{"_id":"public/tags/statistical-learning/index.html","hash":"5a6d9aefa908dfaab0ba6becc14f590591d69a71","modified":1565081665137},{"_id":"public/tags/linearalgebra/index.html","hash":"3e0c4146a3946c706d935dff56fbfebaa900e1b5","modified":1565081665137},{"_id":"public/tags/c/index.html","hash":"02901a168e709efd4990fab73f391889556f9874","modified":1565081665137},{"_id":"public/tags/algorithm/index.html","hash":"112c3a42417ae9ec527a0d4be765c41a4ae5537f","modified":1565081665137},{"_id":"public/tags/seq2seq/index.html","hash":"52645bc63260ef91105f628d2865a9b9a16084b8","modified":1565081665138},{"_id":"public/tags/rnn/index.html","hash":"c2bb5c31118b5f189eeee37c90efda5c7423f580","modified":1565081665138},{"_id":"public/tags/lstm/index.html","hash":"c5636d7f35646176254347f33334fd127a484076","modified":1565081665138},{"_id":"public/tags/gru/index.html","hash":"90fc8ad50dd01403c3223ac75c15851af62aed17","modified":1565081665138},{"_id":"public/tags/vae/index.html","hash":"df3bab55531fd1724209dc12c91d44923db0675e","modified":1565081665139},{"_id":"public/tags/mcmc/index.html","hash":"397c67f286e0b149d4256e3c39ad7d0b1baae294","modified":1565081665139},{"_id":"public/tags/web/index.html","hash":"b3e04efc97fc82744006398ecf3e424a1e08dba3","modified":1565081665140},{"_id":"public/tags/hexo/index.html","hash":"abeb6891299f06cce74690043709f6c3698c223d","modified":1565081665140},{"_id":"public/tags/github/index.html","hash":"63c0aed7de9305c46aa7086ce59fc6b52dfbbe61","modified":1565081665140},{"_id":"public/tags/bayes/index.html","hash":"fe2fa9ab06e617ff0a5c3f1fc04ef909796dcd54","modified":1565081665141},{"_id":"public/tags/inference/index.html","hash":"a9e2ed948cc61b55b6b77e332b4dfe1569b37aa1","modified":1565081665141},{"_id":"public/tags/variational-inference/index.html","hash":"e7499378eda663afa74870f6024e8c0c71b82bb7","modified":1565081665141},{"_id":"public/tags/em/index.html","hash":"78ed59975a70bbc8647d4971173143fccdcc58a4","modified":1565081665141},{"_id":"public/tags/lda/index.html","hash":"6d41efdf5702cc0be2797ffc03fcd85be214ef98","modified":1565081665142},{"_id":"public/lib/blog-encrypt.js","hash":"c9c3cd609c4d93a377849a010497a8eea2a0576e","modified":1565081665169},{"_id":"public/robots.txt","hash":"87926b2573818fe2a55dd4b5538b4984c0c61dad","modified":1565081665229},{"_id":"public/novel/main.js","hash":"1fa407d1aa5b00520b3c7844083d5029c4938f72","modified":1565081665229},{"_id":"public/novel/style.css","hash":"24042fee064ded292478f9cca2bb403c333af68e","modified":1565081665230},{"_id":"public/novel/tttt.js","hash":"6007dbe65bfd5c39249cb498d546986ec963903a","modified":1565081665230},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1565081665230},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1565081665230},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1565081665231},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1565081665231},{"_id":"public/lib/algolia-instant-search/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1565081665231},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css.map","hash":"d055d06395af598682873d1b458166dc6f513072","modified":1565081665231},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1565081665231},{"_id":"public/css/blog-encrypt.css","hash":"262922c3f49cb8cca7b3ea982c49b57de2af53d6","modified":1565081665232},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1565081666842},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1565081666848},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1565081667439},{"_id":"public/lib/crypto-js.js","hash":"3dd73b6f13dc818a3a9c5c7424c1c4a9649b00a2","modified":1565081667443},{"_id":"public/css/main.css","hash":"76dfbc4477641e66bb582371ff358d01e4449609","modified":1565081667547},{"_id":"public/novel/ink.js","hash":"6ed2ce5b9555de369ef9d3f3fcf312be949bd3ce","modified":1565081667547},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"6dc68d14bbfc9c7cc1e7ca636dfd61f27a371ead","modified":1565081667990},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1565081667992},{"_id":"public/js/exturl.js","hash":"05b1007586ebec620fee4bad1f73c1212a768ee4","modified":1565081668543},{"_id":"public/js/algolia-search.js","hash":"8a1dc0ff3dd95709076a67067c155f74bedeeecc","modified":1565081668544},{"_id":"public/js/js.cookie.js","hash":"319c186c7fed01e5993c67140d77280578efe17f","modified":1565081668546},{"_id":"public/js/next-boot.js","hash":"50ad2b37fc9a193d94f8b4ca6b8e961eca149f9f","modified":1565081668546},{"_id":"public/js/scrollspy.js","hash":"d95c0b6805a73912693cdbc434c9664aae7b07ea","modified":1565081668546},{"_id":"public/js/post-details.js","hash":"e995e31a96eb0abd16c4f30a62906d94a17c4346","modified":1565081668547},{"_id":"public/js/schemes/muse.js","hash":"bbeef7ef9ae402e90e0ff5a5e1cd406a78861aa4","modified":1565081668547},{"_id":"public/js/scroll-cookie.js","hash":"eebd37d7ae8a106f0811ae08454698632fdafec1","modified":1565081668547},{"_id":"public/js/src/affix.js","hash":"cfe34a41f9e4828c11df1310d4d18a8e4701a130","modified":1565081668547},{"_id":"public/js/schemes/pisces.js","hash":"d9126cda3cffdfe20e7b2e8d1cfe8563d89e20d0","modified":1565081668547},{"_id":"public/js/src/js.cookie.js","hash":"319c186c7fed01e5993c67140d77280578efe17f","modified":1565081668548},{"_id":"public/js/src/exturl.js","hash":"05b1007586ebec620fee4bad1f73c1212a768ee4","modified":1565081668548},{"_id":"public/js/src/bootstrap.js","hash":"a55edbc1a02fea01504d3eadbbab21ba6ea5d9e6","modified":1565081668548},{"_id":"public/js/src/algolia-search.js","hash":"8a1dc0ff3dd95709076a67067c155f74bedeeecc","modified":1565081668548},{"_id":"public/js/src/post-details.js","hash":"e995e31a96eb0abd16c4f30a62906d94a17c4346","modified":1565081668549},{"_id":"public/js/src/scroll-cookie.js","hash":"eebd37d7ae8a106f0811ae08454698632fdafec1","modified":1565081668549},{"_id":"public/js/src/scrollspy.js","hash":"d95c0b6805a73912693cdbc434c9664aae7b07ea","modified":1565081668549},{"_id":"public/js/src/schemes/pisces.js","hash":"3b1968728bae634ecb796ca789c3814031da45b8","modified":1565081668549},{"_id":"public/js/affix.js","hash":"cfe34a41f9e4828c11df1310d4d18a8e4701a130","modified":1565081668901},{"_id":"public/js/src/motion.js","hash":"cb06e5a4d5c05f3917163bfe55b8370fe55cc7b7","modified":1565081668902},{"_id":"public/js/utils.js","hash":"9001f30e3b910336c260dcc2b36413fed394c5c4","modified":1565081668902},{"_id":"public/js/src/utils.js","hash":"d3507a7386abb3a1ca30664373a9b406cf0edb7c","modified":1565081668902},{"_id":"public/js/motion.js","hash":"b0ff1d6921838ed9934565872fad065f178ef021","modified":1565081668903},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"3029463efd32058d1b4ae1ffa5ad36b88b25a06c","modified":1565081668903},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"f531d8edfb5d3178a7281d5d30d398fb3712d8f9","modified":1565081668903},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"f531d8edfb5d3178a7281d5d30d398fb3712d8f9","modified":1565081668917},{"_id":"public/lib/velocity/velocity.ui.js","hash":"304cb0e0ff1b75aaec344a33147eade1ac1637e1","modified":1565081669315},{"_id":"public/lib/velocity/velocity.min.js","hash":"6563255d04e2a59e9911a69f18ad1bf05c6a0cac","modified":1565081669316},{"_id":"public/lib/jquery/index.js","hash":"ddd838fe9ff6071b106ea819065e1dad41a54d4b","modified":1565081669935},{"_id":"public/about/index/index.jpeg","hash":"6ecfd43fafc28a2d85d09e1e96c5ef0bd7497490","modified":1565081670200},{"_id":"public/lib/velocity/velocity.js","hash":"41e318dc0b33c1c2d415455459d5c6a1392e4e8f","modified":1565081670207},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js.map","hash":"753adf35c67993265fb13b827aeee3f95e3442af","modified":1565081670228},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"cc8811c6ea3dd8dc4f108ce7e03f49b9088e8f14","modified":1565081672711}],"Category":[{"name":"自然语言处理","_id":"cjyzl6uga0003q8t5akntb696"},{"name":"机器学习","_id":"cjyzl6ugx0008q8t5bkzv7txk"},{"name":"Python","_id":"cjyzl6uh1000bq8t59olju5sr"},{"name":"数学","_id":"cjyzl6uh3000fq8t59rsiamjf"},{"name":"Android","_id":"cjyzl6v44001wq8t5uq0xgo8o"},{"name":"算法","_id":"cjyzl6vb2004gq8t5etx39xux"},{"name":"瞎折腾","_id":"cjyzl6vbp004qq8t5247nvv59"}],"Data":[],"Page":[{"_content":"google-site-verification: google785ae9e190b58461.html","source":"google785ae9e190b58461.html","raw":"google-site-verification: google785ae9e190b58461.html","date":"2019-07-22T03:45:23.524Z","updated":"2019-07-22T03:45:23.524Z","path":"google785ae9e190b58461.html","title":"","comments":1,"layout":"page","_id":"cjyzl6ubt0000q8t5uwuufcgl","content":"google-site-verification: google785ae9e190b58461.html","site":{"data":{}},"excerpt":"","more":"google-site-verification: google785ae9e190b58461.html"},{"title":"About Thinkwee","comments":0,"_content":"***\n\n# Resume\n<img src=\"img/index.jpeg\">\n\n# Works\n\n## DivCNN\n<img src=\"img/divcnn1.png\">\n<img src=\"img/divcnn2.png\">\n\n## Multi-view Summarization\n<img src=\"img/multi_view.png\">\n\n## Bert Based Tagging\n<img src=\"img/bert_crf.png\">\n<img src=\"img/bert_nmt.png\">\n\n\n","source":"about/index.md","raw":"title: About Thinkwee\ncomments: false\n---\n***\n\n# Resume\n<img src=\"img/index.jpeg\">\n\n# Works\n\n## DivCNN\n<img src=\"img/divcnn1.png\">\n<img src=\"img/divcnn2.png\">\n\n## Multi-view Summarization\n<img src=\"img/multi_view.png\">\n\n## Bert Based Tagging\n<img src=\"img/bert_crf.png\">\n<img src=\"img/bert_nmt.png\">\n\n\n","date":"2019-07-23T07:45:11.830Z","updated":"2019-07-23T07:45:11.830Z","path":"about/index.html","layout":"page","_id":"cjyzl6v1u0016q8t59fhzx2uz","content":"<hr>\n<h1 id=\"Resume\"><a href=\"#Resume\" class=\"headerlink\" title=\"Resume\"></a>Resume</h1><p><img src=\"/about/index/index.jpeg\"></p>\n<h1 id=\"Works\"><a href=\"#Works\" class=\"headerlink\" title=\"Works\"></a>Works</h1><h2 id=\"DivCNN\"><a href=\"#DivCNN\" class=\"headerlink\" title=\"DivCNN\"></a>DivCNN</h2><p><img src=\"/about/index/divcnn1.png\"><br><img src=\"/about/index/divcnn2.png\"></p>\n<h2 id=\"Multi-view-Summarization\"><a href=\"#Multi-view-Summarization\" class=\"headerlink\" title=\"Multi-view Summarization\"></a>Multi-view Summarization</h2><p><img src=\"/about/index/multi_view.png\"></p>\n<h2 id=\"Bert-Based-Tagging\"><a href=\"#Bert-Based-Tagging\" class=\"headerlink\" title=\"Bert Based Tagging\"></a>Bert Based Tagging</h2><p><img src=\"/about/index/bert_crf.png\"><br><img src=\"/about/index/bert_nmt.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h1 id=\"Resume\"><a href=\"#Resume\" class=\"headerlink\" title=\"Resume\"></a>Resume</h1><p><img src=\"/about/index/index.jpeg\"></p>\n<h1 id=\"Works\"><a href=\"#Works\" class=\"headerlink\" title=\"Works\"></a>Works</h1><h2 id=\"DivCNN\"><a href=\"#DivCNN\" class=\"headerlink\" title=\"DivCNN\"></a>DivCNN</h2><p><img src=\"/about/index/divcnn1.png\"><br><img src=\"/about/index/divcnn2.png\"></p>\n<h2 id=\"Multi-view-Summarization\"><a href=\"#Multi-view-Summarization\" class=\"headerlink\" title=\"Multi-view Summarization\"></a>Multi-view Summarization</h2><p><img src=\"/about/index/multi_view.png\"></p>\n<h2 id=\"Bert-Based-Tagging\"><a href=\"#Bert-Based-Tagging\" class=\"headerlink\" title=\"Bert Based Tagging\"></a>Bert Based Tagging</h2><p><img src=\"/about/index/bert_crf.png\"><br><img src=\"/about/index/bert_nmt.png\"></p>\n"},{"title":"categories","type":"categories","sidebar":false,"comments":0,"_content":"","source":"categories/index.md","raw":"title: categories\ntype: \"categories\"\nsidebar: false\ncomments: false\n---\n","date":"2019-07-22T03:45:23.509Z","updated":"2019-07-22T03:45:23.509Z","path":"categories/index.html","layout":"page","_id":"cjyzl6v280018q8t5o9cat8f0","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"标签","type":"tags","sidebar":false,"comments":0,"_content":"","source":"tags/index.md","raw":"title: 标签\ntype: \"tags\"\nsidebar: false\ncomments: false\n---","date":"2019-07-22T03:45:23.818Z","updated":"2019-07-22T03:45:23.818Z","path":"tags/index.html","layout":"page","_id":"cjyzl6v2o001bq8t53g7gahbt","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"论文翻译：用于生成式自动文摘的一种神经注意力模型","date":"2017-12-27T11:20:43.000Z","author":"Thinkwee","mathjax":true,"_content":"\n论文翻译：\n-\tA Neural Attention Model for Abstractive Sentence Summarization\n\n作者：\n-\tAlexander M. Rush(Facebook AI Research / Harvard SEAS)\n-\tSumit Chopra(Facebook AI Research)\n-\tJason Weston(Facebook AI Research)\n<!--more-->\n\n![i0IyQS.png](https://s1.ax1x.com/2018/10/20/i0IyQS.png)\n\n# 原文地址\n-\t[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)\n\n# 翻译\n\n![i0I0it.jpg](https://s1.ax1x.com/2018/10/20/i0I0it.jpg)\n\n![i0IdII.jpg](https://s1.ax1x.com/2018/10/20/i0IdII.jpg)\n\n![i0IBJP.jpg](https://s1.ax1x.com/2018/10/20/i0IBJP.jpg)\n\n![i0IadA.jpg](https://s1.ax1x.com/2018/10/20/i0IadA.jpg)\n\n![i0Irz8.jpg](https://s1.ax1x.com/2018/10/20/i0Irz8.jpg)\n\n![i0IDRf.jpg](https://s1.ax1x.com/2018/10/20/i0IDRf.jpg)\n\n![i0I2Zj.jpg](https://s1.ax1x.com/2018/10/20/i0I2Zj.jpg)\n\n![i0I6sg.jpg](https://s1.ax1x.com/2018/10/20/i0I6sg.jpg)\n\n![i0IcLQ.jpg](https://s1.ax1x.com/2018/10/20/i0IcLQ.jpg)\n\n![i0IRds.jpg](https://s1.ax1x.com/2018/10/20/i0IRds.jpg)\n\n![i0IWon.jpg](https://s1.ax1x.com/2018/10/20/i0IWon.jpg)","source":"_posts/AM-Model-for-ASS.md","raw":"---\ntitle: 论文翻译：用于生成式自动文摘的一种神经注意力模型\ndate: 2017-12-27 19:20:43\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  - nlp\ncategories:\n  - 自然语言处理\nauthor: Thinkwee\nmathjax: true \n---\n\n论文翻译：\n-\tA Neural Attention Model for Abstractive Sentence Summarization\n\n作者：\n-\tAlexander M. Rush(Facebook AI Research / Harvard SEAS)\n-\tSumit Chopra(Facebook AI Research)\n-\tJason Weston(Facebook AI Research)\n<!--more-->\n\n![i0IyQS.png](https://s1.ax1x.com/2018/10/20/i0IyQS.png)\n\n# 原文地址\n-\t[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)\n\n# 翻译\n\n![i0I0it.jpg](https://s1.ax1x.com/2018/10/20/i0I0it.jpg)\n\n![i0IdII.jpg](https://s1.ax1x.com/2018/10/20/i0IdII.jpg)\n\n![i0IBJP.jpg](https://s1.ax1x.com/2018/10/20/i0IBJP.jpg)\n\n![i0IadA.jpg](https://s1.ax1x.com/2018/10/20/i0IadA.jpg)\n\n![i0Irz8.jpg](https://s1.ax1x.com/2018/10/20/i0Irz8.jpg)\n\n![i0IDRf.jpg](https://s1.ax1x.com/2018/10/20/i0IDRf.jpg)\n\n![i0I2Zj.jpg](https://s1.ax1x.com/2018/10/20/i0I2Zj.jpg)\n\n![i0I6sg.jpg](https://s1.ax1x.com/2018/10/20/i0I6sg.jpg)\n\n![i0IcLQ.jpg](https://s1.ax1x.com/2018/10/20/i0IcLQ.jpg)\n\n![i0IRds.jpg](https://s1.ax1x.com/2018/10/20/i0IRds.jpg)\n\n![i0IWon.jpg](https://s1.ax1x.com/2018/10/20/i0IWon.jpg)","slug":"AM-Model-for-ASS","published":1,"updated":"2019-07-22T03:45:22.715Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6uft0001q8t5nsfblf79","content":"<p>论文翻译：</p>\n<ul>\n<li>A Neural Attention Model for Abstractive Sentence Summarization</li>\n</ul>\n<p>作者：</p>\n<ul>\n<li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li>\n<li>Sumit Chopra(Facebook AI Research)</li>\n<li>Jason Weston(Facebook AI Research)<a id=\"more\"></a>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IyQS.png\" alt=\"i0IyQS.png\"></p>\n<h1 id=\"原文地址\"><a href=\"#原文地址\" class=\"headerlink\" title=\"原文地址\"></a>原文地址</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1509.00685.pdf\" target=\"_blank\" rel=\"noopener\">A Neural Attention Model for Abstractive Sentence Summarization</a></li>\n</ul>\n<h1 id=\"翻译\"><a href=\"#翻译\" class=\"headerlink\" title=\"翻译\"></a>翻译</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0I0it.jpg\" alt=\"i0I0it.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IdII.jpg\" alt=\"i0IdII.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IBJP.jpg\" alt=\"i0IBJP.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IadA.jpg\" alt=\"i0IadA.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0Irz8.jpg\" alt=\"i0Irz8.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IDRf.jpg\" alt=\"i0IDRf.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I2Zj.jpg\" alt=\"i0I2Zj.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I6sg.jpg\" alt=\"i0I6sg.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IcLQ.jpg\" alt=\"i0IcLQ.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IRds.jpg\" alt=\"i0IRds.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IWon.jpg\" alt=\"i0IWon.jpg\"></p>\n","site":{"data":{}},"excerpt":"<p>论文翻译：</p>\n<ul>\n<li>A Neural Attention Model for Abstractive Sentence Summarization</li>\n</ul>\n<p>作者：</p>\n<ul>\n<li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li>\n<li>Sumit Chopra(Facebook AI Research)</li>\n<li>Jason Weston(Facebook AI Research)</li></ul>","more":"\n\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IyQS.png\" alt=\"i0IyQS.png\"></p>\n<h1 id=\"原文地址\"><a href=\"#原文地址\" class=\"headerlink\" title=\"原文地址\"></a>原文地址</h1><ul>\n<li><a href=\"https://arxiv.org/pdf/1509.00685.pdf\" target=\"_blank\" rel=\"noopener\">A Neural Attention Model for Abstractive Sentence Summarization</a></li>\n</ul>\n<h1 id=\"翻译\"><a href=\"#翻译\" class=\"headerlink\" title=\"翻译\"></a>翻译</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0I0it.jpg\" alt=\"i0I0it.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IdII.jpg\" alt=\"i0IdII.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IBJP.jpg\" alt=\"i0IBJP.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IadA.jpg\" alt=\"i0IadA.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0Irz8.jpg\" alt=\"i0Irz8.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IDRf.jpg\" alt=\"i0IDRf.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I2Zj.jpg\" alt=\"i0I2Zj.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I6sg.jpg\" alt=\"i0I6sg.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IcLQ.jpg\" alt=\"i0IcLQ.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IRds.jpg\" alt=\"i0IRds.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IWon.jpg\" alt=\"i0IWon.jpg\"></p>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0IyQS.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"论文翻译：用于生成式自动文摘的一种神经注意力模型","path":"2017/12/27/AM-Model-for-ASS/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0IyQS.png","excerpt":"<p>论文翻译：</p>\n<ul>\n<li>A Neural Attention Model for Abstractive Sentence Summarization</li>\n</ul>\n<p>作者：</p>\n<ul>\n<li>Alexander M. Rush(Facebook AI Research / Harvard SEAS)</li>\n<li>Sumit Chopra(Facebook AI Research)</li>\n<li>Jason Weston(Facebook AI Research)</li></ul>","date":"2017-12-27T11:20:43.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Lagrange,KTT,PCA,SVM","date":"2017-03-18T03:20:35.000Z","mathjax":true,"photos":[],"html":true,"_content":"***\n介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用\n<!--more-->\n\n![i0olwj.png](https://s1.ax1x.com/2018/10/20/i0olwj.png)\n图片来自wikipedia关于拉格朗日乘子法的形象介绍\n\n# 拉格朗日乘子法\n-\t拉格朗日乘子法是一种求约束条件下极值的方法，描述为\n\t$$\n\t在约束条件g(x,y)=c下 \\\\\n\t求函数f(x,y)的极值 \\\\\n\t$$\n\t其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。\n-\t由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。\n-\t显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为\n\t$$\n\t\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))\n\t$$\n\t$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。\n\t拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $\n-\t求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。\n-\t拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$\n\n# 卡罗需-库恩-塔克条件\n-\t如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件\n-\t包含不等约束的极值问题描述为\n\t$$\n\t在约束条件: \\\\\n\th_j(X)=0 j=1,2,...,p \\\\\n\tg_k(X)\\leq 0 k=1,2,...q \\\\\n\t求函数f(X)的极值 \\\\\n\t$$\n-\t拉格朗日函数为\n\t$$\n\tL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)\n\t$$\n-\tKTT条件为:\n\t$$\n\t\\frac{dL}{dX}=0 \\\\\n\t\\lambda _j \\neq 0 \\\\\n\t\\mu _k \\geq 0 \\\\\n\t\\mu _k g_k(X)=0 \\\\\n\th_j(X)=0 \\\\\n\tg_k(X) \\leq 0\\\\\n\t$$\n\n# PCA\n-\tPCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本\n-\t记$x_1,...,x_p$为原始p个维度，新维度是$\\xi _1,....,\\xi _p$\n-\t新维度是原始维度的线性组合，表示为\n\t$$\n\t\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x\n\t$$\n-\t为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即\n\t$$\n\t\\alpha _i^T \\alpha _i=1\n\t$$\n-\t令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类\n-\t此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解\n-\t当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1\\-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度\n-\t依次求出p个新维度\n-\tPCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k<q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间\n-\t如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除\n-\tPCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取\n\n# PCA using Covariance Matrix\n- 上述求解PCA的方法太过于麻烦，在实际中可以通过协方差矩阵来求解（因为有高效的矩阵特征分解算法）\n- PCA的优化目标是，重新选取一组基（特征），使得数据在这组基表示下，不同特征之间的协方差为0，同一特征内的数据方差最大化\n- 可以将问题转述为，对数据张量X，按特征列（共m个特征）零均值化（化简协方差的计算），计算协方差矩阵$C=\\frac 1m X^T X$，希望求得一组基P，使得特征变换后的数据$Y=PX$的协方差矩阵D是对角阵，非对角元素（协方差）为0.若对角元素（方差）按从大到小排列，这时我们取P矩阵的前k行就可以将特征维度从m降到k。易得$D=PCP^T$，且C为实对称阵，那么问题就转变为对实对称C对角化，我们需要的新的一组基就是特征向量组。\n- 算法：\n\t- 有m条n维数据，排成n行m列矩阵X\n\t- 将X的每一行进行零均值化\n\t- 求出协方差矩阵C\n\t- 求出协方差矩阵的特征值和特征向量\n\t- 将特征向量按特征值大小对应从大到小按行排列，组成新的基矩阵P\n\t- 如果需要将为，取P的前k行即可，降维后的数据为$Y=PX$\n```\ndef PCA(X, dims):\n    m = len(X)\n    mean = np.mean(X, axis=0)\n    X = X - mean\n    C = np.dot(X.T, X) / m\n    Eigen_Value, Eigen_Vector = np.linalg.eig(C)\n    index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]\n    PCA_Vector = Eigen_Vector[index]\n    X_PCA = np.dot(PCA_Vector, X.T)\n    return X_PCA.T\n```\n\n# SVM\n-\t在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面\n-\t划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移\n-\t求解一个SVM，即找到满足约束\n\t$$\n\t\\begin{cases}\n\tw^Tx_i+b \\geq +1, y_i=+1 \\\\\n\tw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\t\\end{cases}\n\t$$\n\t的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大\n\t这可以重写为最优化问题\n\t$$\n\tmin_{w,b} \\frac 12 {||w||}^2 \\\\\n\ts.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\\n\t$$\n\t推导见另两篇博文：机器学习笔记和统计学习方法笔记手写版\n-\t对于这个最优化问题，它的拉格朗日方程是\n\t$$\n\tL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))\n\t$$\n\t其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题\n\t$$\n\tmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\n\ts.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\t\\alpha _i \\geq 0,i=1,2,...,m \\\\\n\t$$\n\t上式满足KTT条件，通过SMO算法求解","source":"_posts/Lagrange.md","raw":"---\ntitle: Lagrange,KTT,PCA,SVM\ndate: 2017-03-18 11:20:35\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\nphotos: \nhtml: true\n---\n***\n介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用\n<!--more-->\n\n![i0olwj.png](https://s1.ax1x.com/2018/10/20/i0olwj.png)\n图片来自wikipedia关于拉格朗日乘子法的形象介绍\n\n# 拉格朗日乘子法\n-\t拉格朗日乘子法是一种求约束条件下极值的方法，描述为\n\t$$\n\t在约束条件g(x,y)=c下 \\\\\n\t求函数f(x,y)的极值 \\\\\n\t$$\n\t其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。\n-\t由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。\n-\t显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为\n\t$$\n\t\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))\n\t$$\n\t$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。\n\t拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $\n-\t求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。\n-\t拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$\n\n# 卡罗需-库恩-塔克条件\n-\t如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件\n-\t包含不等约束的极值问题描述为\n\t$$\n\t在约束条件: \\\\\n\th_j(X)=0 j=1,2,...,p \\\\\n\tg_k(X)\\leq 0 k=1,2,...q \\\\\n\t求函数f(X)的极值 \\\\\n\t$$\n-\t拉格朗日函数为\n\t$$\n\tL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)\n\t$$\n-\tKTT条件为:\n\t$$\n\t\\frac{dL}{dX}=0 \\\\\n\t\\lambda _j \\neq 0 \\\\\n\t\\mu _k \\geq 0 \\\\\n\t\\mu _k g_k(X)=0 \\\\\n\th_j(X)=0 \\\\\n\tg_k(X) \\leq 0\\\\\n\t$$\n\n# PCA\n-\tPCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本\n-\t记$x_1,...,x_p$为原始p个维度，新维度是$\\xi _1,....,\\xi _p$\n-\t新维度是原始维度的线性组合，表示为\n\t$$\n\t\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x\n\t$$\n-\t为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即\n\t$$\n\t\\alpha _i^T \\alpha _i=1\n\t$$\n-\t令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类\n-\t此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解\n-\t当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1\\-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度\n-\t依次求出p个新维度\n-\tPCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k<q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间\n-\t如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除\n-\tPCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取\n\n# PCA using Covariance Matrix\n- 上述求解PCA的方法太过于麻烦，在实际中可以通过协方差矩阵来求解（因为有高效的矩阵特征分解算法）\n- PCA的优化目标是，重新选取一组基（特征），使得数据在这组基表示下，不同特征之间的协方差为0，同一特征内的数据方差最大化\n- 可以将问题转述为，对数据张量X，按特征列（共m个特征）零均值化（化简协方差的计算），计算协方差矩阵$C=\\frac 1m X^T X$，希望求得一组基P，使得特征变换后的数据$Y=PX$的协方差矩阵D是对角阵，非对角元素（协方差）为0.若对角元素（方差）按从大到小排列，这时我们取P矩阵的前k行就可以将特征维度从m降到k。易得$D=PCP^T$，且C为实对称阵，那么问题就转变为对实对称C对角化，我们需要的新的一组基就是特征向量组。\n- 算法：\n\t- 有m条n维数据，排成n行m列矩阵X\n\t- 将X的每一行进行零均值化\n\t- 求出协方差矩阵C\n\t- 求出协方差矩阵的特征值和特征向量\n\t- 将特征向量按特征值大小对应从大到小按行排列，组成新的基矩阵P\n\t- 如果需要将为，取P的前k行即可，降维后的数据为$Y=PX$\n```\ndef PCA(X, dims):\n    m = len(X)\n    mean = np.mean(X, axis=0)\n    X = X - mean\n    C = np.dot(X.T, X) / m\n    Eigen_Value, Eigen_Vector = np.linalg.eig(C)\n    index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]\n    PCA_Vector = Eigen_Vector[index]\n    X_PCA = np.dot(PCA_Vector, X.T)\n    return X_PCA.T\n```\n\n# SVM\n-\t在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面\n-\t划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移\n-\t求解一个SVM，即找到满足约束\n\t$$\n\t\\begin{cases}\n\tw^Tx_i+b \\geq +1, y_i=+1 \\\\\n\tw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\t\\end{cases}\n\t$$\n\t的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大\n\t这可以重写为最优化问题\n\t$$\n\tmin_{w,b} \\frac 12 {||w||}^2 \\\\\n\ts.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\\n\t$$\n\t推导见另两篇博文：机器学习笔记和统计学习方法笔记手写版\n-\t对于这个最优化问题，它的拉格朗日方程是\n\t$$\n\tL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))\n\t$$\n\t其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题\n\t$$\n\tmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\n\ts.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\t\\alpha _i \\geq 0,i=1,2,...,m \\\\\n\t$$\n\t上式满足KTT条件，通过SMO算法求解","slug":"Lagrange","published":1,"updated":"2019-07-22T03:45:22.736Z","comments":1,"layout":"post","link":"","_id":"cjyzl6ug50002q8t5ork5a4u8","content":"<hr>\n<p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br><a id=\"more\"></a></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0olwj.png\" alt=\"i0olwj.png\"><br>图片来自wikipedia关于拉格朗日乘子法的形象介绍</p>\n<h1 id=\"拉格朗日乘子法\"><a href=\"#拉格朗日乘子法\" class=\"headerlink\" title=\"拉格朗日乘子法\"></a>拉格朗日乘子法</h1><ul>\n<li>拉格朗日乘子法是一种求约束条件下极值的方法，描述为<script type=\"math/tex; mode=display\">\n在约束条件g(x,y)=c下 \\\\\n求函数f(x,y)的极值 \\\\</script>其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。</li>\n<li>由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。</li>\n<li>显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为<script type=\"math/tex; mode=display\">\n\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))</script>$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。<br>拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $</li>\n<li>求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。</li>\n<li>拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$</li>\n</ul>\n<h1 id=\"卡罗需-库恩-塔克条件\"><a href=\"#卡罗需-库恩-塔克条件\" class=\"headerlink\" title=\"卡罗需-库恩-塔克条件\"></a>卡罗需-库恩-塔克条件</h1><ul>\n<li>如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件</li>\n<li>包含不等约束的极值问题描述为<script type=\"math/tex; mode=display\">\n在约束条件: \\\\\nh_j(X)=0 j=1,2,...,p \\\\\ng_k(X)\\leq 0 k=1,2,...q \\\\\n求函数f(X)的极值 \\\\</script></li>\n<li>拉格朗日函数为<script type=\"math/tex; mode=display\">\nL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)</script></li>\n<li>KTT条件为:<script type=\"math/tex; mode=display\">\n\\frac{dL}{dX}=0 \\\\\n\\lambda _j \\neq 0 \\\\\n\\mu _k \\geq 0 \\\\\n\\mu _k g_k(X)=0 \\\\\nh_j(X)=0 \\\\\ng_k(X) \\leq 0\\\\</script></li>\n</ul>\n<h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><ul>\n<li>PCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本</li>\n<li>记$x_1,…,x_p$为原始p个维度，新维度是$\\xi _1,….,\\xi _p$</li>\n<li>新维度是原始维度的线性组合，表示为<script type=\"math/tex; mode=display\">\n\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x</script></li>\n<li>为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即<script type=\"math/tex; mode=display\">\n\\alpha _i^T \\alpha _i=1</script></li>\n<li>令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类</li>\n<li>此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解</li>\n<li>当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度</li>\n<li>依次求出p个新维度</li>\n<li>PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k&lt;q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间</li>\n<li>如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除</li>\n<li>PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取</li>\n</ul>\n<h1 id=\"PCA-using-Covariance-Matrix\"><a href=\"#PCA-using-Covariance-Matrix\" class=\"headerlink\" title=\"PCA using Covariance Matrix\"></a>PCA using Covariance Matrix</h1><ul>\n<li>上述求解PCA的方法太过于麻烦，在实际中可以通过协方差矩阵来求解（因为有高效的矩阵特征分解算法）</li>\n<li>PCA的优化目标是，重新选取一组基（特征），使得数据在这组基表示下，不同特征之间的协方差为0，同一特征内的数据方差最大化</li>\n<li>可以将问题转述为，对数据张量X，按特征列（共m个特征）零均值化（化简协方差的计算），计算协方差矩阵$C=\\frac 1m X^T X$，希望求得一组基P，使得特征变换后的数据$Y=PX$的协方差矩阵D是对角阵，非对角元素（协方差）为0.若对角元素（方差）按从大到小排列，这时我们取P矩阵的前k行就可以将特征维度从m降到k。易得$D=PCP^T$，且C为实对称阵，那么问题就转变为对实对称C对角化，我们需要的新的一组基就是特征向量组。</li>\n<li>算法：<ul>\n<li>有m条n维数据，排成n行m列矩阵X</li>\n<li>将X的每一行进行零均值化</li>\n<li>求出协方差矩阵C</li>\n<li>求出协方差矩阵的特征值和特征向量</li>\n<li>将特征向量按特征值大小对应从大到小按行排列，组成新的基矩阵P</li>\n<li>如果需要将为，取P的前k行即可，降维后的数据为$Y=PX$<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def PCA(X, dims):</span><br><span class=\"line\">    m = len(X)</span><br><span class=\"line\">    mean = np.mean(X, axis=0)</span><br><span class=\"line\">    X = X - mean</span><br><span class=\"line\">    C = np.dot(X.T, X) / m</span><br><span class=\"line\">    Eigen_Value, Eigen_Vector = np.linalg.eig(C)</span><br><span class=\"line\">    index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]</span><br><span class=\"line\">    PCA_Vector = Eigen_Vector[index]</span><br><span class=\"line\">    X_PCA = np.dot(PCA_Vector, X.T)</span><br><span class=\"line\">    return X_PCA.T</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><ul>\n<li>在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面</li>\n<li>划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移</li>\n<li>求解一个SVM，即找到满足约束<script type=\"math/tex; mode=display\">\n\\begin{cases}\nw^Tx_i+b \\geq +1, y_i=+1 \\\\\nw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\\end{cases}</script>的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大<br>这可以重写为最优化问题<script type=\"math/tex; mode=display\">\nmin_{w,b} \\frac 12 {||w||}^2 \\\\\ns.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\</script>推导见另两篇博文：机器学习笔记和统计学习方法笔记手写版</li>\n<li>对于这个最优化问题，它的拉格朗日方程是<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))</script>其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题<script type=\"math/tex; mode=display\">\nmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\ns.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\\alpha _i \\geq 0,i=1,2,...,m \\\\</script>上式满足KTT条件，通过SMO算法求解</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br></p>","more":"<p></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0olwj.png\" alt=\"i0olwj.png\"><br>图片来自wikipedia关于拉格朗日乘子法的形象介绍</p>\n<h1 id=\"拉格朗日乘子法\"><a href=\"#拉格朗日乘子法\" class=\"headerlink\" title=\"拉格朗日乘子法\"></a>拉格朗日乘子法</h1><ul>\n<li>拉格朗日乘子法是一种求约束条件下极值的方法，描述为<script type=\"math/tex; mode=display\">\n在约束条件g(x,y)=c下 \\\\\n求函数f(x,y)的极值 \\\\</script>其主要思想是将约束条件和原函数合成一个函数，转换为无约束条件，进而求偏导得到极值。</li>\n<li>由图可以看出，f函数值相等的点可以构成类似等高线的蓝色环，约束条件是绿色的路径。问题可以转换为，我们沿着绿色路径走，走到哪一点时这个点所在的蓝色环最靠中心或者最靠外沿(极大极小值)。</li>\n<li>显然，在绿色路径与蓝色环相切的点取得极值，此时它们的梯度(箭头)平行，描述为<script type=\"math/tex; mode=display\">\n\\nabla f (x, y) = \\nabla (\\lambda \\left(g \\left(x, y \\right) - c \\right))</script>$\\lambda$是拉格朗日乘数，在这个式子中代表两个平行梯度的大小倍数，正负代表两个梯度方向相反。拉格朗日乘数不为0。<br>拉格朗日方程即$ F(x,y)=\\nabla \\Big[f \\left(x, y \\right) + \\lambda \\left(g \\left(x, y \\right) - c \\right) \\Big] $</li>\n<li>求解上面的式子，就得到一组$(x,y,\\lambda)$，即极值点和达到极值时的拉格朗日乘数。此时拉格朗日方程$F(x,y)=f(x,y)$，因为取得极值时约束条件部分一定为0(我们是沿着约束条件走找相切点，相切点在约束路径上)。</li>\n<li>拉格朗日系数的含义是最大增长值，$-\\frac{\\partial \\Lambda}{\\partial {c_k}} = \\lambda_k$</li>\n</ul>\n<h1 id=\"卡罗需-库恩-塔克条件\"><a href=\"#卡罗需-库恩-塔克条件\" class=\"headerlink\" title=\"卡罗需-库恩-塔克条件\"></a>卡罗需-库恩-塔克条件</h1><ul>\n<li>如果约束条件不仅仅是等式，还包括不等约束条件，这就需将拉格朗日乘子法推广到KTT条件</li>\n<li>包含不等约束的极值问题描述为<script type=\"math/tex; mode=display\">\n在约束条件: \\\\\nh_j(X)=0 j=1,2,...,p \\\\\ng_k(X)\\leq 0 k=1,2,...q \\\\\n求函数f(X)的极值 \\\\</script></li>\n<li>拉格朗日函数为<script type=\"math/tex; mode=display\">\nL(X,\\lambda ,\\mu)=f(X)+\\sum _{j=1}^p \\lambda _j h_j(X) + \\sum _{k=1}^q \\mu g_k(X)</script></li>\n<li>KTT条件为:<script type=\"math/tex; mode=display\">\n\\frac{dL}{dX}=0 \\\\\n\\lambda _j \\neq 0 \\\\\n\\mu _k \\geq 0 \\\\\n\\mu _k g_k(X)=0 \\\\\nh_j(X)=0 \\\\\ng_k(X) \\leq 0\\\\</script></li>\n</ul>\n<h1 id=\"PCA\"><a href=\"#PCA\" class=\"headerlink\" title=\"PCA\"></a>PCA</h1><ul>\n<li>PCA即Principal Component Analysis，主成分分析，将数据原本的维度进行优化，形成一组新的维度，它们是原有维度的线性组合且互不相关，按重要性大小排序，一个维度可以理解为数据矩阵中一列，一行代表一个样本</li>\n<li>记$x_1,…,x_p$为原始p个维度，新维度是$\\xi _1,….,\\xi _p$</li>\n<li>新维度是原始维度的线性组合，表示为<script type=\"math/tex; mode=display\">\n\\xi _i = \\sum _{j=1}^{p}  \\alpha _{ij} x_j = \\alpha _i^T x</script></li>\n<li>为了各个新维度统一尺度，另每个新维度的线性组合系数的向量长度都为1，即<script type=\"math/tex; mode=display\">\n\\alpha _i^T \\alpha _i=1</script></li>\n<li>令A为特征变换矩阵，由各个新维度的线性组合系数向量构成，则需要求解一个最优的正交变换A，使得新维度的方差达到极值。其中正交变换即保证各个新维度不相关，方差越大则样本在新维度上具有区分度，方便我们进行数据的分类</li>\n<li>此时问题就转化为具有约束条件的极值问题，约束条件为$\\alpha _i^T \\alpha _i=1$，求$var(\\xi _i)$的极值，可以用拉格朗日乘子法求解</li>\n<li>当i=1时，我们求出来第一个也是重要性最大(方差最大)的新维度，再令i=2,并加入约束条件$E[\\xi _2 \\xi _1-E[\\xi _1][\\xi _2]]=0$即两个新维度不相关，求出第二个新维度</li>\n<li>依次求出p个新维度</li>\n<li>PCA能够优化原始数据，找出具有区分度的维度，更重要的是如果原始数据的维度存在相关性，PCA能消除这些相关性，即便原始数据相关性很低，如果我们只取前k(k&lt;q)个新维度，就可以在损失较小精确度的情况下进行降维，大大缩短数据的训练时间</li>\n<li>如果我们取了前k个新维度，再对他们进行PCA的逆运算，就可以实现数据的降噪，因为重要性很低的新维度一般反应了数据中的随机噪声，抛弃它们并恢复原始数据时就实现了噪音的去除</li>\n<li>PCA是非监督的，没有考虑样本本身的类别或者标签，在监督学习中不一定是最优解，可以利用K-L变换实现针对分类的目标进行特征提取</li>\n</ul>\n<h1 id=\"PCA-using-Covariance-Matrix\"><a href=\"#PCA-using-Covariance-Matrix\" class=\"headerlink\" title=\"PCA using Covariance Matrix\"></a>PCA using Covariance Matrix</h1><ul>\n<li>上述求解PCA的方法太过于麻烦，在实际中可以通过协方差矩阵来求解（因为有高效的矩阵特征分解算法）</li>\n<li>PCA的优化目标是，重新选取一组基（特征），使得数据在这组基表示下，不同特征之间的协方差为0，同一特征内的数据方差最大化</li>\n<li>可以将问题转述为，对数据张量X，按特征列（共m个特征）零均值化（化简协方差的计算），计算协方差矩阵$C=\\frac 1m X^T X$，希望求得一组基P，使得特征变换后的数据$Y=PX$的协方差矩阵D是对角阵，非对角元素（协方差）为0.若对角元素（方差）按从大到小排列，这时我们取P矩阵的前k行就可以将特征维度从m降到k。易得$D=PCP^T$，且C为实对称阵，那么问题就转变为对实对称C对角化，我们需要的新的一组基就是特征向量组。</li>\n<li>算法：<ul>\n<li>有m条n维数据，排成n行m列矩阵X</li>\n<li>将X的每一行进行零均值化</li>\n<li>求出协方差矩阵C</li>\n<li>求出协方差矩阵的特征值和特征向量</li>\n<li>将特征向量按特征值大小对应从大到小按行排列，组成新的基矩阵P</li>\n<li>如果需要将为，取P的前k行即可，降维后的数据为$Y=PX$<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def PCA(X, dims):</span><br><span class=\"line\">    m = len(X)</span><br><span class=\"line\">    mean = np.mean(X, axis=0)</span><br><span class=\"line\">    X = X - mean</span><br><span class=\"line\">    C = np.dot(X.T, X) / m</span><br><span class=\"line\">    Eigen_Value, Eigen_Vector = np.linalg.eig(C)</span><br><span class=\"line\">    index = np.argsort(Eigen_Value)[-1:-dims - 1:-1]</span><br><span class=\"line\">    PCA_Vector = Eigen_Vector[index]</span><br><span class=\"line\">    X_PCA = np.dot(PCA_Vector, X.T)</span><br><span class=\"line\">    return X_PCA.T</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"SVM\"><a href=\"#SVM\" class=\"headerlink\" title=\"SVM\"></a>SVM</h1><ul>\n<li>在分类学习中，我们需要找到一个划分超平面，将不同类别的样本分开，而最好的划分超平面显然是离所分各个样本类尽量远，即对训练样本局部扰动容忍性最好的超平面</li>\n<li>划分超平面通过方程$w^Tx+b=0$描述，其中w为法向量，决定了超平面方向，b为超平面到远点的位移</li>\n<li>求解一个SVM，即找到满足约束<script type=\"math/tex; mode=display\">\n\\begin{cases}\nw^Tx_i+b \\geq +1, y_i=+1 \\\\\nw^Tx_i+b \\leq -1, y_i=-1 \\\\\n\\end{cases}</script>的条件下，使得两个异类支持向量到超平面的距离$\\frac{2}{||w||}$最大<br>这可以重写为最优化问题<script type=\"math/tex; mode=display\">\nmin_{w,b} \\frac 12 {||w||}^2 \\\\\ns.t. y_i(w^Tx_i+b) \\geq 1,i=1,2,...,m \\\\</script>推导见另两篇博文：机器学习笔记和统计学习方法笔记手写版</li>\n<li>对于这个最优化问题，它的拉格朗日方程是<script type=\"math/tex; mode=display\">\nL(w,b,\\alpha )=\\frac 12 {||w||}^2+\\sum _{i=1}^{m} \\alpha _i (1-y_i(w^Tx_i+b))</script>其中$\\alpha$是拉格朗日乘子，令方程分别对w,b求偏导，得到对偶问题<script type=\"math/tex; mode=display\">\nmax _{\\alpha } \\sum _{i=1}^m \\alpha _i -\\frac 12 \\sum _{i=1}^m \\sum _{j=1}^m \\alpha _i \\alpha _j y_i y_j x_i^T x_j \\\\\ns.t. \\sum _{i=1}^m \\alpha _i y_i=0, \\\\\n\\alpha _i \\geq 0,i=1,2,...,m \\\\</script>上式满足KTT条件，通过SMO算法求解</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0olwj.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"Lagrange,KTT,PCA,SVM","path":"2017/03/18/Lagrange/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0olwj.png","excerpt":"<hr>\n<p>介绍拉格朗日乘子法及其推广KTT条件，以及它们在PCA和SVM中的应用<br></p>","date":"2017-03-18T03:20:35.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Python特性拾零","date":"2017-03-28T12:02:39.000Z","_content":"***\nPython的一些特性和语法\n总结一些自己跳过的坑\nPython3.5\n\n<!--more-->\n\n\n# 对象皆引用\n-\t不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用\n-\tPython中不需要声明变量类型也是基于此\n\t```Python\n\t\ta=3\n\t\tb=a\n\t```\n\ta和b都只是引用一个整型值3，修改b，a的引用值也会变化\n-\t如果要拷贝，可以用b=a[:]\n\n# string是常量\n-\t字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变\n\n# lambda匿名函数\n-\t简化函数书写，lambda 参量:计算式\n-\t主要用于排序或者reduce\n-\tlambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数\n\t```Python\n\t\tx=10\n\t\ta=lambda y,x=x:x+y\n\t\tx=20\n\t\tprint(a(10))\n\t\t\n\t>>> 20\n\t```\n\t如果不设置默认参数，上面的运行结果就是30\n\t\n# 迭代器与生成器\n-\t通过重写对象的__iter__方法实现自定义迭代器，生成器yield实现迭代器的next方法\n\t```Python\n\t\tclass Countdown:\n\t\t\tdef __init__(self, start):\n\t\t\t\tself.start = start\n\t\t\n\t\t\tdef __iter__(self):\n\t\t\t\tn = self.start\n\t\t\t\twhile n > 0:\n\t\t\t\t\tyield n\n\t\t\t\t\tn -= 1\n    \n\t\t\tdef __reversed__(self):\n\t\t\t\tn = 1\n\t\t\t\twhile n <= self.start:\n\t\t\t\t\tn += 1\n    \n    \n\t\tfor rr in (Countdown(3)):\n\t\t\tprint(rr)\n\t\t\n\t>>> 3\n\t    2\n\t    1\n\t```\n\n# enumerate\n-\t同时输出迭代对象和索引，参数为索引开始号\n\t```Python\n\t\tfor idx,val in enumerate(my_list,1):\n\t\t\tprint(idx,val)\n\t```\n\n# 函数\n-\t接收任意个参数\n\t```Python\n\tdef avg(first,*rest):\n\t\treturn (first+sum(rest))/(1+len(rest))\n\t```\n\t\\*接任意数量的位置参数，也可以用\\*\\*接一个字典，代表任意数量的关键字参数，也可以混用\\*和\\*\\*\n\t顺序(任意个位置参数，\\*，最后一个位置参数，其他参数，\\*\\*)\n-\t函数返回多个值\n\t直接return a,b,c，实际上返回的是一个元祖\n\n# 装饰器\n-\t一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数\n\t```Python\n\t\timport time\n\t\tfrom functools import wraps\n\t\t\n\t\t\n\t\tdef timethis(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tstart = time.time()\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tend = time.time()\n\t\t\t\tprint(func.__name__, end - start)\n\t\t\t\treturn result\n\t\t\n\t\t\treturn wrapper\n\t\t\n\t\t@timethis\n\t\tdef loop(n):\n\t\t\twhile n > 0:\n\t\t\t\tn -= 1\n\t\t\n\t\t\n\t\tloop(100000)\n\t\n\t>>> loop 0.03971695899963379\n\t```\n\t在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行\n\t可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果\n-\t可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的__wrapped__属性直接访问到被装饰的函数，用来解除装饰\n\n# 逗号的特殊作用\n-\t输出时换行变空格\n-\t转换类型为元组\n\n# filter\n-\t接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素\n\t```Python\n\t\tdef is_odd(n):\n\t\t\treturn n % 2 == 1\n\n\t\tfilter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])\n\t```\n\n# any\n-\t原型：\n\t```Python\t\t\t\t\n\t\tdef any(iterable):\n\t\t   for element in iterable:\n\t\t\t   if  element:\n\t\t\t\t   return False\n\t\t   return True\n\t```\n\n# yield\n-\t一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。\n\t```Python\n\t\t>>> def g(n):\n\t\t...     for i in range(n):\n\t\t...             yield i **2\n\t\t...\n\t\t>>> for i in g(5):\n\t\t...     print i,\":\",\n\t\t...\n\t\t0 : 1 : 4 : 9 : 16 :\n\t```","source":"_posts/PythonNotes.md","raw":"title: Python特性拾零\ndate: 2017-03-28 20:02:39\ntags:\n-\tcode\n-\tpython\ncategories:\n-\tPython\n---\n***\nPython的一些特性和语法\n总结一些自己跳过的坑\nPython3.5\n\n<!--more-->\n\n\n# 对象皆引用\n-\t不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用\n-\tPython中不需要声明变量类型也是基于此\n\t```Python\n\t\ta=3\n\t\tb=a\n\t```\n\ta和b都只是引用一个整型值3，修改b，a的引用值也会变化\n-\t如果要拷贝，可以用b=a[:]\n\n# string是常量\n-\t字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变\n\n# lambda匿名函数\n-\t简化函数书写，lambda 参量:计算式\n-\t主要用于排序或者reduce\n-\tlambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数\n\t```Python\n\t\tx=10\n\t\ta=lambda y,x=x:x+y\n\t\tx=20\n\t\tprint(a(10))\n\t\t\n\t>>> 20\n\t```\n\t如果不设置默认参数，上面的运行结果就是30\n\t\n# 迭代器与生成器\n-\t通过重写对象的__iter__方法实现自定义迭代器，生成器yield实现迭代器的next方法\n\t```Python\n\t\tclass Countdown:\n\t\t\tdef __init__(self, start):\n\t\t\t\tself.start = start\n\t\t\n\t\t\tdef __iter__(self):\n\t\t\t\tn = self.start\n\t\t\t\twhile n > 0:\n\t\t\t\t\tyield n\n\t\t\t\t\tn -= 1\n    \n\t\t\tdef __reversed__(self):\n\t\t\t\tn = 1\n\t\t\t\twhile n <= self.start:\n\t\t\t\t\tn += 1\n    \n    \n\t\tfor rr in (Countdown(3)):\n\t\t\tprint(rr)\n\t\t\n\t>>> 3\n\t    2\n\t    1\n\t```\n\n# enumerate\n-\t同时输出迭代对象和索引，参数为索引开始号\n\t```Python\n\t\tfor idx,val in enumerate(my_list,1):\n\t\t\tprint(idx,val)\n\t```\n\n# 函数\n-\t接收任意个参数\n\t```Python\n\tdef avg(first,*rest):\n\t\treturn (first+sum(rest))/(1+len(rest))\n\t```\n\t\\*接任意数量的位置参数，也可以用\\*\\*接一个字典，代表任意数量的关键字参数，也可以混用\\*和\\*\\*\n\t顺序(任意个位置参数，\\*，最后一个位置参数，其他参数，\\*\\*)\n-\t函数返回多个值\n\t直接return a,b,c，实际上返回的是一个元祖\n\n# 装饰器\n-\t一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数\n\t```Python\n\t\timport time\n\t\tfrom functools import wraps\n\t\t\n\t\t\n\t\tdef timethis(func):\n\t\t\t@wraps(func)\n\t\t\tdef wrapper(*args, **kwargs):\n\t\t\t\tstart = time.time()\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tend = time.time()\n\t\t\t\tprint(func.__name__, end - start)\n\t\t\t\treturn result\n\t\t\n\t\t\treturn wrapper\n\t\t\n\t\t@timethis\n\t\tdef loop(n):\n\t\t\twhile n > 0:\n\t\t\t\tn -= 1\n\t\t\n\t\t\n\t\tloop(100000)\n\t\n\t>>> loop 0.03971695899963379\n\t```\n\t在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行\n\t可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果\n-\t可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的__wrapped__属性直接访问到被装饰的函数，用来解除装饰\n\n# 逗号的特殊作用\n-\t输出时换行变空格\n-\t转换类型为元组\n\n# filter\n-\t接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素\n\t```Python\n\t\tdef is_odd(n):\n\t\t\treturn n % 2 == 1\n\n\t\tfilter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])\n\t```\n\n# any\n-\t原型：\n\t```Python\t\t\t\t\n\t\tdef any(iterable):\n\t\t   for element in iterable:\n\t\t\t   if  element:\n\t\t\t\t   return False\n\t\t   return True\n\t```\n\n# yield\n-\t一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。\n\t```Python\n\t\t>>> def g(n):\n\t\t...     for i in range(n):\n\t\t...             yield i **2\n\t\t...\n\t\t>>> for i in g(5):\n\t\t...     print i,\":\",\n\t\t...\n\t\t0 : 1 : 4 : 9 : 16 :\n\t```","slug":"PythonNotes","published":1,"updated":"2019-07-22T03:45:22.930Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6ugf0005q8t5szv2405p","content":"<hr>\n<p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p>\n<a id=\"more\"></a>\n<h1 id=\"对象皆引用\"><a href=\"#对象皆引用\" class=\"headerlink\" title=\"对象皆引用\"></a>对象皆引用</h1><ul>\n<li>不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用</li>\n<li><p>Python中不需要声明变量类型也是基于此</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=<span class=\"number\">3</span></span><br><span class=\"line\">b=a</span><br></pre></td></tr></table></figure>\n<p>a和b都只是引用一个整型值3，修改b，a的引用值也会变化</p>\n</li>\n<li>如果要拷贝，可以用b=a[:]</li>\n</ul>\n<h1 id=\"string是常量\"><a href=\"#string是常量\" class=\"headerlink\" title=\"string是常量\"></a>string是常量</h1><ul>\n<li>字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变</li>\n</ul>\n<h1 id=\"lambda匿名函数\"><a href=\"#lambda匿名函数\" class=\"headerlink\" title=\"lambda匿名函数\"></a>lambda匿名函数</h1><ul>\n<li>简化函数书写，lambda 参量:计算式</li>\n<li>主要用于排序或者reduce</li>\n<li><p>lambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tx=<span class=\"number\">10</span></span><br><span class=\"line\">\ta=<span class=\"keyword\">lambda</span> y,x=x:x+y</span><br><span class=\"line\">\tx=<span class=\"number\">20</span></span><br><span class=\"line\">\tprint(a(<span class=\"number\">10</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">20</span></span><br></pre></td></tr></table></figure>\n<p>如果不设置默认参数，上面的运行结果就是30</p>\n</li>\n</ul>\n<h1 id=\"迭代器与生成器\"><a href=\"#迭代器与生成器\" class=\"headerlink\" title=\"迭代器与生成器\"></a>迭代器与生成器</h1><ul>\n<li>通过重写对象的<strong>iter</strong>方法实现自定义迭代器，生成器yield实现迭代器的next方法<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Countdown</span>:</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, start)</span>:</span></span><br><span class=\"line\">\t\t\tself.start = start</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = self.start</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">yield</span> n</span><br><span class=\"line\">\t\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__reversed__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &lt;= self.start:</span><br><span class=\"line\">\t\t\t\tn += <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">   </span><br><span class=\"line\">\t<span class=\"keyword\">for</span> rr <span class=\"keyword\">in</span> (Countdown(<span class=\"number\">3</span>)):</span><br><span class=\"line\">\t\tprint(rr)</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h1><ul>\n<li>同时输出迭代对象和索引，参数为索引开始号<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx,val <span class=\"keyword\">in</span> enumerate(my_list,<span class=\"number\">1</span>):</span><br><span class=\"line\">\tprint(idx,val)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><ul>\n<li><p>接收任意个参数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">avg</span><span class=\"params\">(first,*rest)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> (first+sum(rest))/(<span class=\"number\">1</span>+len(rest))</span><br></pre></td></tr></table></figure>\n<p>*接任意数量的位置参数，也可以用**接一个字典，代表任意数量的关键字参数，也可以混用*和**<br>顺序(任意个位置参数，*，最后一个位置参数，其他参数，**)</p>\n</li>\n<li>函数返回多个值<br>直接return a,b,c，实际上返回的是一个元祖</li>\n</ul>\n<h1 id=\"装饰器\"><a href=\"#装饰器\" class=\"headerlink\" title=\"装饰器\"></a>装饰器</h1><ul>\n<li><p>一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"keyword\">import</span> time</span><br><span class=\"line\">\t<span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> wraps</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">timethis</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">\t\t@wraps(func)</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">wrapper</span><span class=\"params\">(*args, **kwargs)</span>:</span></span><br><span class=\"line\">\t\t\tstart = time.time()</span><br><span class=\"line\">\t\t\tresult = func(*args, **kwargs)</span><br><span class=\"line\">\t\t\tend = time.time()</span><br><span class=\"line\">\t\t\tprint(func.__name__, end - start)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> result</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> wrapper</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">\t@timethis</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tloop(<span class=\"number\">100000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>loop <span class=\"number\">0.03971695899963379</span></span><br></pre></td></tr></table></figure>\n<p>在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行<br>可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果</p>\n</li>\n<li>可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的<strong>wrapped</strong>属性直接访问到被装饰的函数，用来解除装饰</li>\n</ul>\n<h1 id=\"逗号的特殊作用\"><a href=\"#逗号的特殊作用\" class=\"headerlink\" title=\"逗号的特殊作用\"></a>逗号的特殊作用</h1><ul>\n<li>输出时换行变空格</li>\n<li>转换类型为元组</li>\n</ul>\n<h1 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h1><ul>\n<li>接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">is_odd</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> n % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">filter(is_odd, [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"any\"><a href=\"#any\" class=\"headerlink\" title=\"any\"></a>any</h1><ul>\n<li>原型：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">any</span><span class=\"params\">(iterable)</span>:</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> iterable:</span><br><span class=\"line\">\t   <span class=\"keyword\">if</span>  element:</span><br><span class=\"line\">\t\t   <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"yield\"><a href=\"#yield\" class=\"headerlink\" title=\"yield\"></a>yield</h1><ul>\n<li>一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">g</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\"><span class=\"meta\">... </span>            <span class=\"keyword\">yield</span> i **<span class=\"number\">2</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> g(<span class=\"number\">5</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">print</span> i,<span class=\"string\">\":\"</span>,</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"number\">0</span> : <span class=\"number\">1</span> : <span class=\"number\">4</span> : <span class=\"number\">9</span> : <span class=\"number\">16</span> :</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p>","more":"<h1 id=\"对象皆引用\"><a href=\"#对象皆引用\" class=\"headerlink\" title=\"对象皆引用\"></a>对象皆引用</h1><ul>\n<li>不同于C++，Python只有传引用，不存在传值，在Python中，一切皆对象，变量只是对对象的引用</li>\n<li><p>Python中不需要声明变量类型也是基于此</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=<span class=\"number\">3</span></span><br><span class=\"line\">b=a</span><br></pre></td></tr></table></figure>\n<p>a和b都只是引用一个整型值3，修改b，a的引用值也会变化</p>\n</li>\n<li>如果要拷贝，可以用b=a[:]</li>\n</ul>\n<h1 id=\"string是常量\"><a href=\"#string是常量\" class=\"headerlink\" title=\"string是常量\"></a>string是常量</h1><ul>\n<li>字符串不能更改，只能在原先字符串上更改后赋给一个新字符串，原字符串依然不变</li>\n</ul>\n<h1 id=\"lambda匿名函数\"><a href=\"#lambda匿名函数\" class=\"headerlink\" title=\"lambda匿名函数\"></a>lambda匿名函数</h1><ul>\n<li>简化函数书写，lambda 参量:计算式</li>\n<li>主要用于排序或者reduce</li>\n<li><p>lambda的参数是自由变量，是运行时绑定值，而不是定义时绑定值，如果要实现定义时绑定值，则定义之后在lambda中设置默认参数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tx=<span class=\"number\">10</span></span><br><span class=\"line\">\ta=<span class=\"keyword\">lambda</span> y,x=x:x+y</span><br><span class=\"line\">\tx=<span class=\"number\">20</span></span><br><span class=\"line\">\tprint(a(<span class=\"number\">10</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">20</span></span><br></pre></td></tr></table></figure>\n<p>如果不设置默认参数，上面的运行结果就是30</p>\n</li>\n</ul>\n<h1 id=\"迭代器与生成器\"><a href=\"#迭代器与生成器\" class=\"headerlink\" title=\"迭代器与生成器\"></a>迭代器与生成器</h1><ul>\n<li>通过重写对象的<strong>iter</strong>方法实现自定义迭代器，生成器yield实现迭代器的next方法<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Countdown</span>:</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, start)</span>:</span></span><br><span class=\"line\">\t\t\tself.start = start</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = self.start</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">yield</span> n</span><br><span class=\"line\">\t\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__reversed__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">\t\t\tn = <span class=\"number\">1</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> n &lt;= self.start:</span><br><span class=\"line\">\t\t\t\tn += <span class=\"number\">1</span></span><br><span class=\"line\">   </span><br><span class=\"line\">   </span><br><span class=\"line\">\t<span class=\"keyword\">for</span> rr <span class=\"keyword\">in</span> (Countdown(<span class=\"number\">3</span>)):</span><br><span class=\"line\">\t\tprint(rr)</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"enumerate\"><a href=\"#enumerate\" class=\"headerlink\" title=\"enumerate\"></a>enumerate</h1><ul>\n<li>同时输出迭代对象和索引，参数为索引开始号<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> idx,val <span class=\"keyword\">in</span> enumerate(my_list,<span class=\"number\">1</span>):</span><br><span class=\"line\">\tprint(idx,val)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><ul>\n<li><p>接收任意个参数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">avg</span><span class=\"params\">(first,*rest)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> (first+sum(rest))/(<span class=\"number\">1</span>+len(rest))</span><br></pre></td></tr></table></figure>\n<p>*接任意数量的位置参数，也可以用**接一个字典，代表任意数量的关键字参数，也可以混用*和**<br>顺序(任意个位置参数，*，最后一个位置参数，其他参数，**)</p>\n</li>\n<li>函数返回多个值<br>直接return a,b,c，实际上返回的是一个元祖</li>\n</ul>\n<h1 id=\"装饰器\"><a href=\"#装饰器\" class=\"headerlink\" title=\"装饰器\"></a>装饰器</h1><ul>\n<li><p>一个装饰器就是一个函数，它接收一个函数作为参数并返回一个新的函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\t<span class=\"keyword\">import</span> time</span><br><span class=\"line\">\t<span class=\"keyword\">from</span> functools <span class=\"keyword\">import</span> wraps</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">timethis</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">\t\t@wraps(func)</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">wrapper</span><span class=\"params\">(*args, **kwargs)</span>:</span></span><br><span class=\"line\">\t\t\tstart = time.time()</span><br><span class=\"line\">\t\t\tresult = func(*args, **kwargs)</span><br><span class=\"line\">\t\t\tend = time.time()</span><br><span class=\"line\">\t\t\tprint(func.__name__, end - start)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> result</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> wrapper</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"meta\">\t@timethis</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\tn -= <span class=\"number\">1</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tloop(<span class=\"number\">100000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>loop <span class=\"number\">0.03971695899963379</span></span><br></pre></td></tr></table></figure>\n<p>在上例中，timethis是包装器，其定义中func是被包装的函数，args和kwargs是任意数量的位置参数和关键字参数，来保证被包装的函数能正确接收参数执行<br>可以看到包装器中实现了一个wrapper装饰器函数，它运行了作为参数的func函数并计算打印了运行时间，一般装饰器函数返回原函数的执行结果</p>\n</li>\n<li>可以看到timethis中的@wraps本身也是一个装饰器，它用来注解底层包装函数，这样能够保留原函数的元信息，还能通过装饰器返回函数的<strong>wrapped</strong>属性直接访问到被装饰的函数，用来解除装饰</li>\n</ul>\n<h1 id=\"逗号的特殊作用\"><a href=\"#逗号的特殊作用\" class=\"headerlink\" title=\"逗号的特殊作用\"></a>逗号的特殊作用</h1><ul>\n<li>输出时换行变空格</li>\n<li>转换类型为元组</li>\n</ul>\n<h1 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h1><ul>\n<li>接收一个函数和序列，将函数作用于序列中每一个元素上，根据返回值决定是否删除该元素<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">is_odd</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> n % <span class=\"number\">2</span> == <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">filter(is_odd, [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"any\"><a href=\"#any\" class=\"headerlink\" title=\"any\"></a>any</h1><ul>\n<li>原型：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">any</span><span class=\"params\">(iterable)</span>:</span></span><br><span class=\"line\">   <span class=\"keyword\">for</span> element <span class=\"keyword\">in</span> iterable:</span><br><span class=\"line\">\t   <span class=\"keyword\">if</span>  element:</span><br><span class=\"line\">\t\t   <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">   <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"yield\"><a href=\"#yield\" class=\"headerlink\" title=\"yield\"></a>yield</h1><ul>\n<li>一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过 yield 返回当前的迭代值。<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">g</span><span class=\"params\">(n)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\"><span class=\"meta\">... </span>            <span class=\"keyword\">yield</span> i **<span class=\"number\">2</span></span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> g(<span class=\"number\">5</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"keyword\">print</span> i,<span class=\"string\">\":\"</span>,</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"number\">0</span> : <span class=\"number\">1</span> : <span class=\"number\">4</span> : <span class=\"number\">9</span> : <span class=\"number\">16</span> :</span><br></pre></td></tr></table></figure></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"Python特性拾零","path":"2017/03/28/PythonNotes/","eyeCatchImage":null,"excerpt":"<hr>\n<p>Python的一些特性和语法<br>总结一些自己跳过的坑<br>Python3.5</p>","date":"2017-03-28T12:02:39.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"周六清华游","date":"2018-10-13T09:49:06.000Z","_content":"\n去清华的FIT听了一次轻沙龙，介绍了关于机器阅读理解的一些进展，有趣的是上午九点演讲的博士还说有一个还没公开的工作：BERT，很牛逼，很有钱，八块p100训一年，结果十点半机器之心就发了报道，下午就知乎满天飞了，说NLP新的时代到来了......\n这个沙龙是一个系列，之后可能会有机器翻译、深度贝叶斯、迁移学习和知识图谱啥的，要是有时间的话再听再记录吧\n\n<!--more-->  \n\n# 2018.10.13 机器阅读理解\n- 三场演讲，第一场是概述；第二场是当时在SQuAD2.0上拿到第一名的nlnet作者的presentation，国防科大和微软合作的成果；第三场是一位清华的博士，介绍了他关于开放领域问答中噪声过滤和信息集合的研究。\n\n## 概述\n- 现在的阅读理解和人们所期望的人工智能阅读理解差了太多，研究者把阅读理解的过程分解成了任务，例如选词、选span、生成短文本。深度学习兴起之前都是一些手工设计特征，一些Pipiline的操作，使用深度学习之后就专注于输入到输出的端到端研究，绕过了很多阅读理解过程所需要的东西。\n- 以前的关于阅读理解的研究可以作为一个测试方法，检验模型对于词法、修辞、利用知识的能力。\n- 目前的大规模机器阅读理解数据集处于很低级的推断阶段，提到了一篇论文：Efficient and Robust Question Answering from Minimal Context over Documents。里面讲到如果用深度学习，只用你找出的span来训练，砍掉上下文，其实结果不会差很多，因此端到端的学习并没有“通读全文掌握大意”的过程，而是“你问什么我答什么，别问我为什么这么答，背的”。提到了东京大学一份工作，建立了对模型阅读理解能力的评价指标，30多项，包括消除歧义、指代消解等等，大而简单的数据集无法体现这些特征，而设计巧妙的数据集规模不够大。\n- 还提到了一篇关于衡量模型推断能力的论文，TOWARDS AI-COMPLETE QUESTION ANSWERING:A SET OF PREREQUISITE TOY TASKS。\n- 工业上使用阅读理解所需要解决的问题：简化模型或者加速模型，介绍了诸如SKIM-RNN之类的技巧，虽然训练的时候会变复杂，但推断时能加速。论文：NEURAL SPEED READING VIA SKIM-RNN\n- 现在NLP的迁移学习，预训练词嵌入或者预训练语言模型，用的最多最广泛的，比如Glove和Elmo，然后就提到了BERT，4亿参数，无监督，最重要的是其模型是双向设计，且充分训练了上层参数：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.另外一种迁移学习是直接将某一任务中训练好的模块做其他任务，例如直接将seq2seq中训练好的encoder拿出来计算语义表示（深度学习本来就是表示学习），记得fast Disan就直接一个函数返回句子的向量表示，也是类似的思想。\n- 最新的研究领域：open domain question-answering和learning to ask。前者实际上是加了一个信息检索的过程，阅读理解所需要的相关语料是通过提问检索到的。后者是将回答任务反过来做提问，演讲者提到反向可以辅助阅读理解，且有一个工业上比较有用的设计：做检索时不用query和文档（或者文档关键词）相比较，而是和针对文档生成的提问相比较，相当于两个提问之间计算相似度。\n- 演讲者提到了他关于attention的一个观点：attention是从模型中筛选信息，不代表原模型没有表示出此信息的能力。\n- 介绍了当前比较流行的几个数据集，2015之前用MCTest、ProcessBank，15到17年之间用CNNDM、SQuAD、MS MARCO，17年之后用TriviaQA、SQuAD2.0、CoQA、QuAC、HotpotQA。（然而文摘还在用CNNDM......）\n\n## NLNet\n- 论文：Squad上可以看到，但是好像还没发？\n- NLNet的设计初衷是为了解决阅读理解问题中的鲁棒性和有效性，都是针对集成模型说的，所以NLNet是在集成模型的基础上加了一个蒸馏的过程，使用单模型提升效率，另外还有一个read and verify的过程来提升鲁棒性，所以在加入了对抗样本的SQuAD2.0数据集上表现优异，目前第一。在1.0上落后于四处碾压的BERT，但其实落后也不多。不过1.0版本中nlnet的ensemble版本要好于单模型版本，2.0中没有提交ensemble版本，就很迷......\n- 蒸馏的意思没太听明白，效果是12个模型压缩成一个模型，模型的结构完全相同，但是初始化不同。不是简单的选最优，单一的模型是训练出来的，论文里叫那12个模型为teacher，单一模型为student，student使用teacher训练的结果来指导训练。\n- 设计了一个read and verify机制，在抽取出span回答问题之后还会根据该回答和问题计算一个置信度，置信度太低就认为是没有答案，也就是squad2.0里对抗样本的情况。感觉听下来就是有问题就加loss。\n- 听说一些选取特征的细节没有在论文中表出，而且最后用强化学习优化了一下模型？\n\n## Open Domain QA噪声过滤和信息集合\n- 论文（ACL 2018）：Denoising Distantly Supervised Open-Domain Question Answering\n- 这个噪声是指在检索文档的过程搜到了很多相关但提供的不是正确答案的文档，是对文档的过滤。这一步的过滤本来应该放在检索的过程里，但是作者最后也是用深度学习算概率加loss的方式解决了。\n- 去噪过程是一个document selector，然后阅读理解是一个reader，作者认为对应于人做阅读理解的fast skimming 和careful reading & summarizing。\n- 信息集合没太注意听，就是充分利用多篇文档的信息提取出答案\n","source":"_posts/compute-future.md","raw":"---\ntitle: 周六清华游\ndate: 2018-10-13 17:49:06\ntags: [comprehension,NLI,]\ncategories: 自然语言处理\n---\n\n去清华的FIT听了一次轻沙龙，介绍了关于机器阅读理解的一些进展，有趣的是上午九点演讲的博士还说有一个还没公开的工作：BERT，很牛逼，很有钱，八块p100训一年，结果十点半机器之心就发了报道，下午就知乎满天飞了，说NLP新的时代到来了......\n这个沙龙是一个系列，之后可能会有机器翻译、深度贝叶斯、迁移学习和知识图谱啥的，要是有时间的话再听再记录吧\n\n<!--more-->  \n\n# 2018.10.13 机器阅读理解\n- 三场演讲，第一场是概述；第二场是当时在SQuAD2.0上拿到第一名的nlnet作者的presentation，国防科大和微软合作的成果；第三场是一位清华的博士，介绍了他关于开放领域问答中噪声过滤和信息集合的研究。\n\n## 概述\n- 现在的阅读理解和人们所期望的人工智能阅读理解差了太多，研究者把阅读理解的过程分解成了任务，例如选词、选span、生成短文本。深度学习兴起之前都是一些手工设计特征，一些Pipiline的操作，使用深度学习之后就专注于输入到输出的端到端研究，绕过了很多阅读理解过程所需要的东西。\n- 以前的关于阅读理解的研究可以作为一个测试方法，检验模型对于词法、修辞、利用知识的能力。\n- 目前的大规模机器阅读理解数据集处于很低级的推断阶段，提到了一篇论文：Efficient and Robust Question Answering from Minimal Context over Documents。里面讲到如果用深度学习，只用你找出的span来训练，砍掉上下文，其实结果不会差很多，因此端到端的学习并没有“通读全文掌握大意”的过程，而是“你问什么我答什么，别问我为什么这么答，背的”。提到了东京大学一份工作，建立了对模型阅读理解能力的评价指标，30多项，包括消除歧义、指代消解等等，大而简单的数据集无法体现这些特征，而设计巧妙的数据集规模不够大。\n- 还提到了一篇关于衡量模型推断能力的论文，TOWARDS AI-COMPLETE QUESTION ANSWERING:A SET OF PREREQUISITE TOY TASKS。\n- 工业上使用阅读理解所需要解决的问题：简化模型或者加速模型，介绍了诸如SKIM-RNN之类的技巧，虽然训练的时候会变复杂，但推断时能加速。论文：NEURAL SPEED READING VIA SKIM-RNN\n- 现在NLP的迁移学习，预训练词嵌入或者预训练语言模型，用的最多最广泛的，比如Glove和Elmo，然后就提到了BERT，4亿参数，无监督，最重要的是其模型是双向设计，且充分训练了上层参数：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.另外一种迁移学习是直接将某一任务中训练好的模块做其他任务，例如直接将seq2seq中训练好的encoder拿出来计算语义表示（深度学习本来就是表示学习），记得fast Disan就直接一个函数返回句子的向量表示，也是类似的思想。\n- 最新的研究领域：open domain question-answering和learning to ask。前者实际上是加了一个信息检索的过程，阅读理解所需要的相关语料是通过提问检索到的。后者是将回答任务反过来做提问，演讲者提到反向可以辅助阅读理解，且有一个工业上比较有用的设计：做检索时不用query和文档（或者文档关键词）相比较，而是和针对文档生成的提问相比较，相当于两个提问之间计算相似度。\n- 演讲者提到了他关于attention的一个观点：attention是从模型中筛选信息，不代表原模型没有表示出此信息的能力。\n- 介绍了当前比较流行的几个数据集，2015之前用MCTest、ProcessBank，15到17年之间用CNNDM、SQuAD、MS MARCO，17年之后用TriviaQA、SQuAD2.0、CoQA、QuAC、HotpotQA。（然而文摘还在用CNNDM......）\n\n## NLNet\n- 论文：Squad上可以看到，但是好像还没发？\n- NLNet的设计初衷是为了解决阅读理解问题中的鲁棒性和有效性，都是针对集成模型说的，所以NLNet是在集成模型的基础上加了一个蒸馏的过程，使用单模型提升效率，另外还有一个read and verify的过程来提升鲁棒性，所以在加入了对抗样本的SQuAD2.0数据集上表现优异，目前第一。在1.0上落后于四处碾压的BERT，但其实落后也不多。不过1.0版本中nlnet的ensemble版本要好于单模型版本，2.0中没有提交ensemble版本，就很迷......\n- 蒸馏的意思没太听明白，效果是12个模型压缩成一个模型，模型的结构完全相同，但是初始化不同。不是简单的选最优，单一的模型是训练出来的，论文里叫那12个模型为teacher，单一模型为student，student使用teacher训练的结果来指导训练。\n- 设计了一个read and verify机制，在抽取出span回答问题之后还会根据该回答和问题计算一个置信度，置信度太低就认为是没有答案，也就是squad2.0里对抗样本的情况。感觉听下来就是有问题就加loss。\n- 听说一些选取特征的细节没有在论文中表出，而且最后用强化学习优化了一下模型？\n\n## Open Domain QA噪声过滤和信息集合\n- 论文（ACL 2018）：Denoising Distantly Supervised Open-Domain Question Answering\n- 这个噪声是指在检索文档的过程搜到了很多相关但提供的不是正确答案的文档，是对文档的过滤。这一步的过滤本来应该放在检索的过程里，但是作者最后也是用深度学习算概率加loss的方式解决了。\n- 去噪过程是一个document selector，然后阅读理解是一个reader，作者认为对应于人做阅读理解的fast skimming 和careful reading & summarizing。\n- 信息集合没太注意听，就是充分利用多篇文档的信息提取出答案\n","slug":"compute-future","published":1,"updated":"2019-07-22T03:45:23.026Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6ugq0006q8t5gjjkm0lm","content":"<p>去清华的FIT听了一次轻沙龙，介绍了关于机器阅读理解的一些进展，有趣的是上午九点演讲的博士还说有一个还没公开的工作：BERT，很牛逼，很有钱，八块p100训一年，结果十点半机器之心就发了报道，下午就知乎满天飞了，说NLP新的时代到来了……<br>这个沙龙是一个系列，之后可能会有机器翻译、深度贝叶斯、迁移学习和知识图谱啥的，要是有时间的话再听再记录吧</p>\n<a id=\"more\"></a>  \n<h1 id=\"2018-10-13-机器阅读理解\"><a href=\"#2018-10-13-机器阅读理解\" class=\"headerlink\" title=\"2018.10.13 机器阅读理解\"></a>2018.10.13 机器阅读理解</h1><ul>\n<li>三场演讲，第一场是概述；第二场是当时在SQuAD2.0上拿到第一名的nlnet作者的presentation，国防科大和微软合作的成果；第三场是一位清华的博士，介绍了他关于开放领域问答中噪声过滤和信息集合的研究。</li>\n</ul>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li>现在的阅读理解和人们所期望的人工智能阅读理解差了太多，研究者把阅读理解的过程分解成了任务，例如选词、选span、生成短文本。深度学习兴起之前都是一些手工设计特征，一些Pipiline的操作，使用深度学习之后就专注于输入到输出的端到端研究，绕过了很多阅读理解过程所需要的东西。</li>\n<li>以前的关于阅读理解的研究可以作为一个测试方法，检验模型对于词法、修辞、利用知识的能力。</li>\n<li>目前的大规模机器阅读理解数据集处于很低级的推断阶段，提到了一篇论文：Efficient and Robust Question Answering from Minimal Context over Documents。里面讲到如果用深度学习，只用你找出的span来训练，砍掉上下文，其实结果不会差很多，因此端到端的学习并没有“通读全文掌握大意”的过程，而是“你问什么我答什么，别问我为什么这么答，背的”。提到了东京大学一份工作，建立了对模型阅读理解能力的评价指标，30多项，包括消除歧义、指代消解等等，大而简单的数据集无法体现这些特征，而设计巧妙的数据集规模不够大。</li>\n<li>还提到了一篇关于衡量模型推断能力的论文，TOWARDS AI-COMPLETE QUESTION ANSWERING:A SET OF PREREQUISITE TOY TASKS。</li>\n<li>工业上使用阅读理解所需要解决的问题：简化模型或者加速模型，介绍了诸如SKIM-RNN之类的技巧，虽然训练的时候会变复杂，但推断时能加速。论文：NEURAL SPEED READING VIA SKIM-RNN</li>\n<li>现在NLP的迁移学习，预训练词嵌入或者预训练语言模型，用的最多最广泛的，比如Glove和Elmo，然后就提到了BERT，4亿参数，无监督，最重要的是其模型是双向设计，且充分训练了上层参数：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.另外一种迁移学习是直接将某一任务中训练好的模块做其他任务，例如直接将seq2seq中训练好的encoder拿出来计算语义表示（深度学习本来就是表示学习），记得fast Disan就直接一个函数返回句子的向量表示，也是类似的思想。</li>\n<li>最新的研究领域：open domain question-answering和learning to ask。前者实际上是加了一个信息检索的过程，阅读理解所需要的相关语料是通过提问检索到的。后者是将回答任务反过来做提问，演讲者提到反向可以辅助阅读理解，且有一个工业上比较有用的设计：做检索时不用query和文档（或者文档关键词）相比较，而是和针对文档生成的提问相比较，相当于两个提问之间计算相似度。</li>\n<li>演讲者提到了他关于attention的一个观点：attention是从模型中筛选信息，不代表原模型没有表示出此信息的能力。</li>\n<li>介绍了当前比较流行的几个数据集，2015之前用MCTest、ProcessBank，15到17年之间用CNNDM、SQuAD、MS MARCO，17年之后用TriviaQA、SQuAD2.0、CoQA、QuAC、HotpotQA。（然而文摘还在用CNNDM……）</li>\n</ul>\n<h2 id=\"NLNet\"><a href=\"#NLNet\" class=\"headerlink\" title=\"NLNet\"></a>NLNet</h2><ul>\n<li>论文：Squad上可以看到，但是好像还没发？</li>\n<li>NLNet的设计初衷是为了解决阅读理解问题中的鲁棒性和有效性，都是针对集成模型说的，所以NLNet是在集成模型的基础上加了一个蒸馏的过程，使用单模型提升效率，另外还有一个read and verify的过程来提升鲁棒性，所以在加入了对抗样本的SQuAD2.0数据集上表现优异，目前第一。在1.0上落后于四处碾压的BERT，但其实落后也不多。不过1.0版本中nlnet的ensemble版本要好于单模型版本，2.0中没有提交ensemble版本，就很迷……</li>\n<li>蒸馏的意思没太听明白，效果是12个模型压缩成一个模型，模型的结构完全相同，但是初始化不同。不是简单的选最优，单一的模型是训练出来的，论文里叫那12个模型为teacher，单一模型为student，student使用teacher训练的结果来指导训练。</li>\n<li>设计了一个read and verify机制，在抽取出span回答问题之后还会根据该回答和问题计算一个置信度，置信度太低就认为是没有答案，也就是squad2.0里对抗样本的情况。感觉听下来就是有问题就加loss。</li>\n<li>听说一些选取特征的细节没有在论文中表出，而且最后用强化学习优化了一下模型？</li>\n</ul>\n<h2 id=\"Open-Domain-QA噪声过滤和信息集合\"><a href=\"#Open-Domain-QA噪声过滤和信息集合\" class=\"headerlink\" title=\"Open Domain QA噪声过滤和信息集合\"></a>Open Domain QA噪声过滤和信息集合</h2><ul>\n<li>论文（ACL 2018）：Denoising Distantly Supervised Open-Domain Question Answering</li>\n<li>这个噪声是指在检索文档的过程搜到了很多相关但提供的不是正确答案的文档，是对文档的过滤。这一步的过滤本来应该放在检索的过程里，但是作者最后也是用深度学习算概率加loss的方式解决了。</li>\n<li>去噪过程是一个document selector，然后阅读理解是一个reader，作者认为对应于人做阅读理解的fast skimming 和careful reading &amp; summarizing。</li>\n<li>信息集合没太注意听，就是充分利用多篇文档的信息提取出答案</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>去清华的FIT听了一次轻沙龙，介绍了关于机器阅读理解的一些进展，有趣的是上午九点演讲的博士还说有一个还没公开的工作：BERT，很牛逼，很有钱，八块p100训一年，结果十点半机器之心就发了报道，下午就知乎满天飞了，说NLP新的时代到来了……<br>这个沙龙是一个系列，之后可能会有机器翻译、深度贝叶斯、迁移学习和知识图谱啥的，要是有时间的话再听再记录吧</p>","more":"<h1 id=\"2018-10-13-机器阅读理解\"><a href=\"#2018-10-13-机器阅读理解\" class=\"headerlink\" title=\"2018.10.13 机器阅读理解\"></a>2018.10.13 机器阅读理解</h1><ul>\n<li>三场演讲，第一场是概述；第二场是当时在SQuAD2.0上拿到第一名的nlnet作者的presentation，国防科大和微软合作的成果；第三场是一位清华的博士，介绍了他关于开放领域问答中噪声过滤和信息集合的研究。</li>\n</ul>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li>现在的阅读理解和人们所期望的人工智能阅读理解差了太多，研究者把阅读理解的过程分解成了任务，例如选词、选span、生成短文本。深度学习兴起之前都是一些手工设计特征，一些Pipiline的操作，使用深度学习之后就专注于输入到输出的端到端研究，绕过了很多阅读理解过程所需要的东西。</li>\n<li>以前的关于阅读理解的研究可以作为一个测试方法，检验模型对于词法、修辞、利用知识的能力。</li>\n<li>目前的大规模机器阅读理解数据集处于很低级的推断阶段，提到了一篇论文：Efficient and Robust Question Answering from Minimal Context over Documents。里面讲到如果用深度学习，只用你找出的span来训练，砍掉上下文，其实结果不会差很多，因此端到端的学习并没有“通读全文掌握大意”的过程，而是“你问什么我答什么，别问我为什么这么答，背的”。提到了东京大学一份工作，建立了对模型阅读理解能力的评价指标，30多项，包括消除歧义、指代消解等等，大而简单的数据集无法体现这些特征，而设计巧妙的数据集规模不够大。</li>\n<li>还提到了一篇关于衡量模型推断能力的论文，TOWARDS AI-COMPLETE QUESTION ANSWERING:A SET OF PREREQUISITE TOY TASKS。</li>\n<li>工业上使用阅读理解所需要解决的问题：简化模型或者加速模型，介绍了诸如SKIM-RNN之类的技巧，虽然训练的时候会变复杂，但推断时能加速。论文：NEURAL SPEED READING VIA SKIM-RNN</li>\n<li>现在NLP的迁移学习，预训练词嵌入或者预训练语言模型，用的最多最广泛的，比如Glove和Elmo，然后就提到了BERT，4亿参数，无监督，最重要的是其模型是双向设计，且充分训练了上层参数：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.另外一种迁移学习是直接将某一任务中训练好的模块做其他任务，例如直接将seq2seq中训练好的encoder拿出来计算语义表示（深度学习本来就是表示学习），记得fast Disan就直接一个函数返回句子的向量表示，也是类似的思想。</li>\n<li>最新的研究领域：open domain question-answering和learning to ask。前者实际上是加了一个信息检索的过程，阅读理解所需要的相关语料是通过提问检索到的。后者是将回答任务反过来做提问，演讲者提到反向可以辅助阅读理解，且有一个工业上比较有用的设计：做检索时不用query和文档（或者文档关键词）相比较，而是和针对文档生成的提问相比较，相当于两个提问之间计算相似度。</li>\n<li>演讲者提到了他关于attention的一个观点：attention是从模型中筛选信息，不代表原模型没有表示出此信息的能力。</li>\n<li>介绍了当前比较流行的几个数据集，2015之前用MCTest、ProcessBank，15到17年之间用CNNDM、SQuAD、MS MARCO，17年之后用TriviaQA、SQuAD2.0、CoQA、QuAC、HotpotQA。（然而文摘还在用CNNDM……）</li>\n</ul>\n<h2 id=\"NLNet\"><a href=\"#NLNet\" class=\"headerlink\" title=\"NLNet\"></a>NLNet</h2><ul>\n<li>论文：Squad上可以看到，但是好像还没发？</li>\n<li>NLNet的设计初衷是为了解决阅读理解问题中的鲁棒性和有效性，都是针对集成模型说的，所以NLNet是在集成模型的基础上加了一个蒸馏的过程，使用单模型提升效率，另外还有一个read and verify的过程来提升鲁棒性，所以在加入了对抗样本的SQuAD2.0数据集上表现优异，目前第一。在1.0上落后于四处碾压的BERT，但其实落后也不多。不过1.0版本中nlnet的ensemble版本要好于单模型版本，2.0中没有提交ensemble版本，就很迷……</li>\n<li>蒸馏的意思没太听明白，效果是12个模型压缩成一个模型，模型的结构完全相同，但是初始化不同。不是简单的选最优，单一的模型是训练出来的，论文里叫那12个模型为teacher，单一模型为student，student使用teacher训练的结果来指导训练。</li>\n<li>设计了一个read and verify机制，在抽取出span回答问题之后还会根据该回答和问题计算一个置信度，置信度太低就认为是没有答案，也就是squad2.0里对抗样本的情况。感觉听下来就是有问题就加loss。</li>\n<li>听说一些选取特征的细节没有在论文中表出，而且最后用强化学习优化了一下模型？</li>\n</ul>\n<h2 id=\"Open-Domain-QA噪声过滤和信息集合\"><a href=\"#Open-Domain-QA噪声过滤和信息集合\" class=\"headerlink\" title=\"Open Domain QA噪声过滤和信息集合\"></a>Open Domain QA噪声过滤和信息集合</h2><ul>\n<li>论文（ACL 2018）：Denoising Distantly Supervised Open-Domain Question Answering</li>\n<li>这个噪声是指在检索文档的过程搜到了很多相关但提供的不是正确答案的文档，是对文档的过滤。这一步的过滤本来应该放在检索的过程里，但是作者最后也是用深度学习算概率加loss的方式解决了。</li>\n<li>去噪过程是一个document selector，然后阅读理解是一个reader，作者认为对应于人做阅读理解的fast skimming 和careful reading &amp; summarizing。</li>\n<li>信息集合没太注意听，就是充分利用多篇文档的信息提取出答案</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"周六清华游","path":"2018/10/13/compute-future/","eyeCatchImage":null,"excerpt":"<p>去清华的FIT听了一次轻沙龙，介绍了关于机器阅读理解的一些进展，有趣的是上午九点演讲的博士还说有一个还没公开的工作：BERT，很牛逼，很有钱，八块p100训一年，结果十点半机器之心就发了报道，下午就知乎满天飞了，说NLP新的时代到来了……<br>这个沙龙是一个系列，之后可能会有机器翻译、深度贝叶斯、迁移学习和知识图谱啥的，要是有时间的话再听再记录吧</p>","date":"2018-10-13T09:49:06.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["comprehension","NLI"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"凸优化笔记","date":"2018-08-04T02:07:26.000Z","mathjax":true,"password":"kengbi","html":true,"_content":"***\n今日开一大坑，看毕业之前能不能填完。\nConvex Optimization By Stephen Boyd & Lieven Vandenberghe\n \n<!--more-->\n\n![i07ARP.jpg](https://s1.ax1x.com/2018/10/20/i07ARP.jpg)\n\n# 简介\n\n## 引入\n-\t机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解\n\n## 凸集\n-\t定义\n\t$$\n\t\\theta x + (1-\\theta) y \\in C\n\t$$\n-\t直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合\n-\t常见的二维形式即图中两点连线依然在图中，不会越界\n\t![i07BJ1.png](https://s1.ax1x.com/2018/10/20/i07BJ1.png)\n\n### 例子\n-\t所有的实数n维空间\n-\t非负象限\n-\t范数域\n-\t仿射子空间和多面体\n-\t凸集的交集，注意并集一般不成立\n-\t半正定矩阵\n-\t以上这些例子，他们元素的凸组合依然符合原始集合的性质\n\n## 凸函数\n-\t定义\n\t![i070iR.png](https://s1.ax1x.com/2018/10/20/i070iR.png)-\t直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方\n\t-\t如果严格在直线下方而不是会有相切，则为严格凸性\n\t-\t如果在直线上方则为凹性\n\t-\t严格凹性同理\n\n### 凸性一阶条件\n\n![i07adJ.png](https://s1.ax1x.com/2018/10/20/i07adJ.png)\n-\t前提时函数可微\n-\t即在函数上任意一点做切线，切线在函数的下方\n\n### 凸性二阶条件\n\n![i07ZM8.png](https://s1.ax1x.com/2018/10/20/i07ZM8.png)\n-\t前提函数二阶可微，即Hessian矩阵在所有定义域内存在\n\n### Jensen不等式\n-\t由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式\n\t![i07Exf.png](https://s1.ax1x.com/2018/10/20/i07Exf.png)\n-\t从概率密度的角度改写为\n\t$$\n\tf(E[x]) \\leq E[f(x)]\n\t$$\n-\t即Jensen不等式\n\n### 分段集\n-\t一种特别的凸集称为$\\alpha$分段集，定义如下\n\t$$\n\t\\{ x \\in D(f) : f(x) \\leq \\alpha \\}\n\t$$\n-\t可以证明该集合也是凸集\n\t$$\n\tf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\leq \\theta \\alpha + (1-\\theta) \\alpha = \\alpha\n\t$$\n\n### 凸函数例子\n-\t指数函数\n-\t负对数函数\n-\t线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。\n-\t二次函数\n-\t范数\n-\t权值非负情况下，凸函数的加权和\n\n## 凸优化问题\n-\t变量属于凸集，调整变量使得凸函数值最小。\n-\t变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。\n\t$$\n\tminimize \\ \\ f(x) \\\\\n\tsubject \\ to \\ \\ g_i(x) \\leq 0 , \\ \\ i=1,...,m \\\\\n\th_i(x) = 0 , \\ \\ i=1,...,p \\\\\n\t$$\n-\t凸函数的最小值即最优值，最优值可以取正负无穷\n\n### 凸问题中的全局最优性\n-\t可行点的局部最优条件和全局最优条件，略过\n-\t对于凸优化问题，所有的局部最优点都是全局最优点\n\t证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：\n\t$$\n\tz = \\theta y + (1-\\theta) x \\ with \\ \\theta = \\frac{R}{2{||x-y||}_2}\n\t$$\n\t则可以证明z在x的邻域内\n\t![i07yQK.png](https://s1.ax1x.com/2018/10/20/i07yQK.png)\n\t并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。\n\t![i07JMT.png](https://s1.ax1x.com/2018/10/20/i07JMT.png)\n\n\n### 凸优化问题的特殊情况\n-\t对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题\n-\t线性编程（LP）：f和g都是线性函数\n\t![i07esS.png](https://s1.ax1x.com/2018/10/20/i07esS.png)\n-\t二次编程（QP）：g均为线性函数，f为凸二次函数\n\t![i07uZQ.png](https://s1.ax1x.com/2018/10/20/i07uZQ.png)\n-\t二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数\n\t![i07Kaj.png](https://s1.ax1x.com/2018/10/20/i07Kaj.png)\n-\t半定编程（SDP）\n\t![i07lin.png](https://s1.ax1x.com/2018/10/20/i07lin.png)\n-\t这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例\n\n### 例子\n-\tSVM\n-\t约束最小二乘法\n-\t罗杰斯特回归的最大似然估计\n\n# 理论\n\n## 凸集\n\n## 凸函数\n\n## 凸优化问题\n\n## 对偶性\n\n# 应用\n\n## 近似和拟合\n\n## 统计估计\n\n## 几何问题\n\n# 算法\n\n## 无约束最小化\n\n## 等式约束最小化\n\n## 内点方法 ","source":"_posts/convex-optimization.md","raw":"---\ntitle: 凸优化笔记\ndate: 2018-08-04 10:07:26\ncategories: 数学\ntags:\n  - convex optimization\n  - math\nmathjax: true\npassword: kengbi\nhtml: true\n---\n***\n今日开一大坑，看毕业之前能不能填完。\nConvex Optimization By Stephen Boyd & Lieven Vandenberghe\n \n<!--more-->\n\n![i07ARP.jpg](https://s1.ax1x.com/2018/10/20/i07ARP.jpg)\n\n# 简介\n\n## 引入\n-\t机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解\n\n## 凸集\n-\t定义\n\t$$\n\t\\theta x + (1-\\theta) y \\in C\n\t$$\n-\t直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合\n-\t常见的二维形式即图中两点连线依然在图中，不会越界\n\t![i07BJ1.png](https://s1.ax1x.com/2018/10/20/i07BJ1.png)\n\n### 例子\n-\t所有的实数n维空间\n-\t非负象限\n-\t范数域\n-\t仿射子空间和多面体\n-\t凸集的交集，注意并集一般不成立\n-\t半正定矩阵\n-\t以上这些例子，他们元素的凸组合依然符合原始集合的性质\n\n## 凸函数\n-\t定义\n\t![i070iR.png](https://s1.ax1x.com/2018/10/20/i070iR.png)-\t直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方\n\t-\t如果严格在直线下方而不是会有相切，则为严格凸性\n\t-\t如果在直线上方则为凹性\n\t-\t严格凹性同理\n\n### 凸性一阶条件\n\n![i07adJ.png](https://s1.ax1x.com/2018/10/20/i07adJ.png)\n-\t前提时函数可微\n-\t即在函数上任意一点做切线，切线在函数的下方\n\n### 凸性二阶条件\n\n![i07ZM8.png](https://s1.ax1x.com/2018/10/20/i07ZM8.png)\n-\t前提函数二阶可微，即Hessian矩阵在所有定义域内存在\n\n### Jensen不等式\n-\t由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式\n\t![i07Exf.png](https://s1.ax1x.com/2018/10/20/i07Exf.png)\n-\t从概率密度的角度改写为\n\t$$\n\tf(E[x]) \\leq E[f(x)]\n\t$$\n-\t即Jensen不等式\n\n### 分段集\n-\t一种特别的凸集称为$\\alpha$分段集，定义如下\n\t$$\n\t\\{ x \\in D(f) : f(x) \\leq \\alpha \\}\n\t$$\n-\t可以证明该集合也是凸集\n\t$$\n\tf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\leq \\theta \\alpha + (1-\\theta) \\alpha = \\alpha\n\t$$\n\n### 凸函数例子\n-\t指数函数\n-\t负对数函数\n-\t线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。\n-\t二次函数\n-\t范数\n-\t权值非负情况下，凸函数的加权和\n\n## 凸优化问题\n-\t变量属于凸集，调整变量使得凸函数值最小。\n-\t变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。\n\t$$\n\tminimize \\ \\ f(x) \\\\\n\tsubject \\ to \\ \\ g_i(x) \\leq 0 , \\ \\ i=1,...,m \\\\\n\th_i(x) = 0 , \\ \\ i=1,...,p \\\\\n\t$$\n-\t凸函数的最小值即最优值，最优值可以取正负无穷\n\n### 凸问题中的全局最优性\n-\t可行点的局部最优条件和全局最优条件，略过\n-\t对于凸优化问题，所有的局部最优点都是全局最优点\n\t证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：\n\t$$\n\tz = \\theta y + (1-\\theta) x \\ with \\ \\theta = \\frac{R}{2{||x-y||}_2}\n\t$$\n\t则可以证明z在x的邻域内\n\t![i07yQK.png](https://s1.ax1x.com/2018/10/20/i07yQK.png)\n\t并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。\n\t![i07JMT.png](https://s1.ax1x.com/2018/10/20/i07JMT.png)\n\n\n### 凸优化问题的特殊情况\n-\t对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题\n-\t线性编程（LP）：f和g都是线性函数\n\t![i07esS.png](https://s1.ax1x.com/2018/10/20/i07esS.png)\n-\t二次编程（QP）：g均为线性函数，f为凸二次函数\n\t![i07uZQ.png](https://s1.ax1x.com/2018/10/20/i07uZQ.png)\n-\t二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数\n\t![i07Kaj.png](https://s1.ax1x.com/2018/10/20/i07Kaj.png)\n-\t半定编程（SDP）\n\t![i07lin.png](https://s1.ax1x.com/2018/10/20/i07lin.png)\n-\t这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例\n\n### 例子\n-\tSVM\n-\t约束最小二乘法\n-\t罗杰斯特回归的最大似然估计\n\n# 理论\n\n## 凸集\n\n## 凸函数\n\n## 凸优化问题\n\n## 对偶性\n\n# 应用\n\n## 近似和拟合\n\n## 统计估计\n\n## 几何问题\n\n# 算法\n\n## 无约束最小化\n\n## 等式约束最小化\n\n## 内点方法 ","slug":"convex-optimization","published":1,"updated":"2019-07-22T03:45:23.043Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6ugu0007q8t5782i4jt5","content":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\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\n</div>\n<script src=\"/lib/crypto-js.js\"></script><script src=\"/lib/blog-encrypt.js\"></script><link href=\"/css/blog-encrypt.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"这个坑还没填完，正努力填坑中......</br>","more":"这个坑还没填完，正努力填坑中......</br>","origin":"<hr>\n<p>今日开一大坑，看毕业之前能不能填完。<br>Convex Optimization By Stephen Boyd &amp; Lieven Vandenberghe</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07ARP.jpg\" alt=\"i07ARP.jpg\"></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><h2 id=\"引入\"><a href=\"#引入\" class=\"headerlink\" title=\"引入\"></a>引入</h2><ul>\n<li>机器学习中经常需要解决最优化问题，找到全局最优解很难，但对于凸优化问题，我们通常可以有效找到全局最优解</li>\n</ul>\n<h2 id=\"凸集\"><a href=\"#凸集\" class=\"headerlink\" title=\"凸集\"></a>凸集</h2><ul>\n<li>定义<script type=\"math/tex; mode=display\">\n\\theta x + (1-\\theta) y \\in C</script></li>\n<li>直观上理解及对于集合中两个元素，他们的特殊线性组合（凸组合)$\\theta x+(1-\\theta)y$依然属于该集合</li>\n<li>常见的二维形式即图中两点连线依然在图中，不会越界<br><img src=\"https://s1.ax1x.com/2018/10/20/i07BJ1.png\" alt=\"i07BJ1.png\"></li>\n</ul>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><ul>\n<li>所有的实数n维空间</li>\n<li>非负象限</li>\n<li>范数域</li>\n<li>仿射子空间和多面体</li>\n<li>凸集的交集，注意并集一般不成立</li>\n<li>半正定矩阵</li>\n<li>以上这些例子，他们元素的凸组合依然符合原始集合的性质</li>\n</ul>\n<h2 id=\"凸函数\"><a href=\"#凸函数\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h2><ul>\n<li>定义<br><img src=\"https://s1.ax1x.com/2018/10/20/i070iR.png\" alt=\"i070iR.png\">-    直观上，凸函数即函数上两点连线，两点之间的函数曲线在直线下方<ul>\n<li>如果严格在直线下方而不是会有相切，则为严格凸性</li>\n<li>如果在直线上方则为凹性</li>\n<li>严格凹性同理</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"凸性一阶条件\"><a href=\"#凸性一阶条件\" class=\"headerlink\" title=\"凸性一阶条件\"></a>凸性一阶条件</h3><p><img src=\"https://s1.ax1x.com/2018/10/20/i07adJ.png\" alt=\"i07adJ.png\"></p>\n<ul>\n<li>前提时函数可微</li>\n<li>即在函数上任意一点做切线，切线在函数的下方</li>\n</ul>\n<h3 id=\"凸性二阶条件\"><a href=\"#凸性二阶条件\" class=\"headerlink\" title=\"凸性二阶条件\"></a>凸性二阶条件</h3><p><img src=\"https://s1.ax1x.com/2018/10/20/i07ZM8.png\" alt=\"i07ZM8.png\"></p>\n<ul>\n<li>前提函数二阶可微，即Hessian矩阵在所有定义域内存在</li>\n</ul>\n<h3 id=\"Jensen不等式\"><a href=\"#Jensen不等式\" class=\"headerlink\" title=\"Jensen不等式\"></a>Jensen不等式</h3><ul>\n<li>由凸函数的定义，将凸组合从二维扩展到多维，进而扩展到连续的情况，可以得到3种不等式<br><img src=\"https://s1.ax1x.com/2018/10/20/i07Exf.png\" alt=\"i07Exf.png\"></li>\n<li>从概率密度的角度改写为<script type=\"math/tex; mode=display\">\nf(E[x]) \\leq E[f(x)]</script></li>\n<li>即Jensen不等式</li>\n</ul>\n<h3 id=\"分段集\"><a href=\"#分段集\" class=\"headerlink\" title=\"分段集\"></a>分段集</h3><ul>\n<li>一种特别的凸集称为$\\alpha$分段集，定义如下<script type=\"math/tex; mode=display\">\n\\{ x \\in D(f) : f(x) \\leq \\alpha \\}</script></li>\n<li>可以证明该集合也是凸集<script type=\"math/tex; mode=display\">\nf(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\leq \\theta \\alpha + (1-\\theta) \\alpha = \\alpha</script></li>\n</ul>\n<h3 id=\"凸函数例子\"><a href=\"#凸函数例子\" class=\"headerlink\" title=\"凸函数例子\"></a>凸函数例子</h3><ul>\n<li>指数函数</li>\n<li>负对数函数</li>\n<li>线性函数。特别的时线性函数的Hessian矩阵为0，0矩阵机试正半定也是负半定，因此线性函数既是凸函数也是凹函数。</li>\n<li>二次函数</li>\n<li>范数</li>\n<li>权值非负情况下，凸函数的加权和</li>\n</ul>\n<h2 id=\"凸优化问题\"><a href=\"#凸优化问题\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h2><ul>\n<li>变量属于凸集，调整变量使得凸函数值最小。</li>\n<li>变量术语凸集这一条件可以进一步明确为凸函数的不等式条件和线性函数的等式条件，等式条件可以理解为大于等于和小于等于的交集，即凸函数和凹函数的交集，这一交集只有线性函数满足。<script type=\"math/tex; mode=display\">\nminimize \\ \\ f(x) \\\\\nsubject \\ to \\ \\ g_i(x) \\leq 0 , \\ \\ i=1,...,m \\\\\nh_i(x) = 0 , \\ \\ i=1,...,p \\\\</script></li>\n<li>凸函数的最小值即最优值，最优值可以取正负无穷</li>\n</ul>\n<h3 id=\"凸问题中的全局最优性\"><a href=\"#凸问题中的全局最优性\" class=\"headerlink\" title=\"凸问题中的全局最优性\"></a>凸问题中的全局最优性</h3><ul>\n<li>可行点的局部最优条件和全局最优条件，略过</li>\n<li>对于凸优化问题，所有的局部最优点都是全局最优点<br>证明：反证，假如x是一个局部最优点而不是全局最优点，则存在点y函数值小于点x。根据局部最优条件的定义，x的邻域内不存在点z使得函数值小于点x。假设邻域范围为R，我们取z为x和y的凸组合：<script type=\"math/tex; mode=display\">\nz = \\theta y + (1-\\theta) x \\ with \\ \\theta = \\frac{R}{2{||x-y||}_2}</script>则可以证明z在x的邻域内<br><img src=\"https://s1.ax1x.com/2018/10/20/i07yQK.png\" alt=\"i07yQK.png\"><br>并且z的函数值小于x，推出矛盾。且由于可行域为凸集，x和y为可行点则z一定为可行点。<br><img src=\"https://s1.ax1x.com/2018/10/20/i07JMT.png\" alt=\"i07JMT.png\"></li>\n</ul>\n<h3 id=\"凸优化问题的特殊情况\"><a href=\"#凸优化问题的特殊情况\" class=\"headerlink\" title=\"凸优化问题的特殊情况\"></a>凸优化问题的特殊情况</h3><ul>\n<li>对于一些特殊的凸优化问题，我们定制了特别的算法来解决大规模的问题</li>\n<li>线性编程（LP）：f和g都是线性函数<br><img src=\"https://s1.ax1x.com/2018/10/20/i07esS.png\" alt=\"i07esS.png\"></li>\n<li>二次编程（QP）：g均为线性函数，f为凸二次函数<br><img src=\"https://s1.ax1x.com/2018/10/20/i07uZQ.png\" alt=\"i07uZQ.png\"></li>\n<li>二次约束的二次编程（QCQP）：f和所有的g都是凸二次函数<br><img src=\"https://s1.ax1x.com/2018/10/20/i07Kaj.png\" alt=\"i07Kaj.png\"></li>\n<li>半定编程（SDP）<br><img src=\"https://s1.ax1x.com/2018/10/20/i07lin.png\" alt=\"i07lin.png\"></li>\n<li>这四种类型依次越来越普遍，QCQP是SDP的特例，QP是QCQP的特例，LP是QP的特例</li>\n</ul>\n<h3 id=\"例子-1\"><a href=\"#例子-1\" class=\"headerlink\" title=\"例子\"></a>例子</h3><ul>\n<li>SVM</li>\n<li>约束最小二乘法</li>\n<li>罗杰斯特回归的最大似然估计</li>\n</ul>\n<h1 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h1><h2 id=\"凸集-1\"><a href=\"#凸集-1\" class=\"headerlink\" title=\"凸集\"></a>凸集</h2><h2 id=\"凸函数-1\"><a href=\"#凸函数-1\" class=\"headerlink\" title=\"凸函数\"></a>凸函数</h2><h2 id=\"凸优化问题-1\"><a href=\"#凸优化问题-1\" class=\"headerlink\" title=\"凸优化问题\"></a>凸优化问题</h2><h2 id=\"对偶性\"><a href=\"#对偶性\" class=\"headerlink\" title=\"对偶性\"></a>对偶性</h2><h1 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h1><h2 id=\"近似和拟合\"><a href=\"#近似和拟合\" class=\"headerlink\" title=\"近似和拟合\"></a>近似和拟合</h2><h2 id=\"统计估计\"><a href=\"#统计估计\" class=\"headerlink\" title=\"统计估计\"></a>统计估计</h2><h2 id=\"几何问题\"><a href=\"#几何问题\" class=\"headerlink\" title=\"几何问题\"></a>几何问题</h2><h1 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h1><h2 id=\"无约束最小化\"><a href=\"#无约束最小化\" class=\"headerlink\" title=\"无约束最小化\"></a>无约束最小化</h2><h2 id=\"等式约束最小化\"><a href=\"#等式约束最小化\" class=\"headerlink\" title=\"等式约束最小化\"></a>等式约束最小化</h2><h2 id=\"内点方法\"><a href=\"#内点方法\" class=\"headerlink\" title=\"内点方法\"></a>内点方法</h2>","encrypt":true,"abstract":"这个坑还没填完，正努力填坑中......</br>","template":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\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\n</div>\n","message":"文章还没写完，稍后再读，或者输入kengbi看看草稿","decryptionError":"Incorrect Password!","noContentError":"No content to display!","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"凸优化笔记","path":"2018/08/04/convex-optimization/","eyeCatchImage":null,"excerpt":"这个坑还没填完，正努力填坑中......</br>","date":"2018-08-04T02:07:26.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","convex optimization"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Melodia服务器搭建","date":"2017-05-26T11:18:01.000Z","_content":"***\n-\t大创项目的服务器端，大创以及客户端介绍见[Melodia客户端](http://thinkwee.top/2017/03/09/dachuang/)\n-\t我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端\n\n<!--more-->\n\n\n# 功能\n-\t从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程\n-\t客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端\n\n# 代码\n```Python\n\t# -*- coding:utf-8 -*- \n\t# ! usr/bin/python\n\n\tfrom socket import *\n\timport time\n\timport threading\n\timport os\n\timport md5\n\timport warnings\n\n\tHost = ''\n\tPort = 2017\n\tAddr = (Host, Port)\n\tmidi_dict = {}\n\n\twarnings.filterwarnings(\"ignore\")\n\n\n\tdef md5_encode(src):\n\t\tm1 = md5.new()\n\t\tm1.update(src)\n\t\treturn m1.hexdigest()\n\n\n\tdef tcplink(sock, addr):\n\t\tsessnum = 0\n\t\tmusic_data = ''\n\t\twhile True:\n\t\t\tdata = sock.recv(1480)\n\t\t\tif data[-9:]=='endbidou1':\n\t\t\t\tprint 'wav recv finished'\n\t\t\t\tmusic_data+=data\n\t\t\t\tmusic_data=music_data[:-9]\n\t\t\t\tmidi_data = eval(music_data)\n\t\t\tsessnum = midi_data['request']  \n\t\t\t\tif midi_data['request'] == 1:\n\t\t\t\t\tflag_md5 = md5_encode(str(time.time()))\n\t\t\t\t\tprint 'md5: ', flag_md5\n\t\t\t\t\twav_name = flag_md5 + '.wav'\n\t\t\t\t\twith open(wav_name, 'w+') as f:\n\t\t\t\t\t\tf.write(midi_data['data'].decode('base64'))\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tn = midi_data['config']['n'];\n\t\t\t\t\tm = midi_data['config']['m'];\n\t\t\t\t\tw = midi_data['config']['w'];\n\t\t\t\t\tmidi_name = flag_md5 + '.mid'\n\t\t\t\t\twith open(midi_name, 'w') as f:\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tshellmid = '../mldm/hum2midi.py -n '+str(n)+' -m '+str(m)+' -w '+str(w)+' -o ' + midi_name + ' ' + wav_name\n\t\t\tprint \"running wav2midi shell\"\n\t\t\t\t\tretmid = os.system(shellmid)\n\t\t\t\t\tretmid >= 8\n\t\t\t\t\tif retmid == 0:\n\t\t\t\tprint 'generate midi successful'\n\t\t\t\tshellpng = 'mono ../mlds/sheet '+midi_name+' '+flag_md5\n\t\t\t\tretpng = os.system(shellpng)\n\t\t\t\tif retpng == 0:\n\t\t\t\t\t\t\tsock.send(flag_md5.encode())\n\t\t\t\t\t\t\tprint 'generate png successful'\n\t\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\tprint 'generate png error'\n\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'generate midi error'\n\t\t\t\t\t\tbreak\n\t\t\t\telif midi_data['request'] == 2:\n\t\t\t\t\tflag = midi_data['data']\n\t\t\t\t\tif flag in midi_dict.keys():\n\t\t\t\t\t\tfo = open(flag+'.mid', 'rb')\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tfiledata = fo.read(1024)\n\t\t\t\t\t\t\tif not filedata:\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\tsock.send(filedata)\n\t\t\t\tprint 'midi file sent'\n\t\t\t\t\t\tfo.close()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'can not find midi'\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tprint 'json error'\n\t\t\telse:\n\t\t\t\tmusic_data += data\n\t\tsock.close()\n\t\tprint 'session '+str(sessnum)+' for '+str(addr)+' finished'\n\n\ttcpSerSock = socket(AF_INET, SOCK_STREAM)\n\ttcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n\ttcpSerSock.bind(Addr)\n\ttcpSerSock.listen(5)\n\n\twhile True:\n\t\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()\n\t\tprint 'add ', tcpCliAddr\n\t\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))\n\t\tt.start()\n\ttcpSerSock.close()\n```","source":"_posts/dachuangserver.md","raw":"---\ntitle: Melodia服务器搭建\ndate: 2017-05-26 19:18:01\ntags: [code,server,linux]\ncategories: Python\n---\n***\n-\t大创项目的服务器端，大创以及客户端介绍见[Melodia客户端](http://thinkwee.top/2017/03/09/dachuang/)\n-\t我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端\n\n<!--more-->\n\n\n# 功能\n-\t从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程\n-\t客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端\n\n# 代码\n```Python\n\t# -*- coding:utf-8 -*- \n\t# ! usr/bin/python\n\n\tfrom socket import *\n\timport time\n\timport threading\n\timport os\n\timport md5\n\timport warnings\n\n\tHost = ''\n\tPort = 2017\n\tAddr = (Host, Port)\n\tmidi_dict = {}\n\n\twarnings.filterwarnings(\"ignore\")\n\n\n\tdef md5_encode(src):\n\t\tm1 = md5.new()\n\t\tm1.update(src)\n\t\treturn m1.hexdigest()\n\n\n\tdef tcplink(sock, addr):\n\t\tsessnum = 0\n\t\tmusic_data = ''\n\t\twhile True:\n\t\t\tdata = sock.recv(1480)\n\t\t\tif data[-9:]=='endbidou1':\n\t\t\t\tprint 'wav recv finished'\n\t\t\t\tmusic_data+=data\n\t\t\t\tmusic_data=music_data[:-9]\n\t\t\t\tmidi_data = eval(music_data)\n\t\t\tsessnum = midi_data['request']  \n\t\t\t\tif midi_data['request'] == 1:\n\t\t\t\t\tflag_md5 = md5_encode(str(time.time()))\n\t\t\t\t\tprint 'md5: ', flag_md5\n\t\t\t\t\twav_name = flag_md5 + '.wav'\n\t\t\t\t\twith open(wav_name, 'w+') as f:\n\t\t\t\t\t\tf.write(midi_data['data'].decode('base64'))\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tn = midi_data['config']['n'];\n\t\t\t\t\tm = midi_data['config']['m'];\n\t\t\t\t\tw = midi_data['config']['w'];\n\t\t\t\t\tmidi_name = flag_md5 + '.mid'\n\t\t\t\t\twith open(midi_name, 'w') as f:\n\t\t\t\t\t\tf.close()\n\t\t\t\t\tshellmid = '../mldm/hum2midi.py -n '+str(n)+' -m '+str(m)+' -w '+str(w)+' -o ' + midi_name + ' ' + wav_name\n\t\t\tprint \"running wav2midi shell\"\n\t\t\t\t\tretmid = os.system(shellmid)\n\t\t\t\t\tretmid >= 8\n\t\t\t\t\tif retmid == 0:\n\t\t\t\tprint 'generate midi successful'\n\t\t\t\tshellpng = 'mono ../mlds/sheet '+midi_name+' '+flag_md5\n\t\t\t\tretpng = os.system(shellpng)\n\t\t\t\tif retpng == 0:\n\t\t\t\t\t\t\tsock.send(flag_md5.encode())\n\t\t\t\t\t\t\tprint 'generate png successful'\n\t\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name\n\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\tprint 'generate png error'\n\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'generate midi error'\n\t\t\t\t\t\tbreak\n\t\t\t\telif midi_data['request'] == 2:\n\t\t\t\t\tflag = midi_data['data']\n\t\t\t\t\tif flag in midi_dict.keys():\n\t\t\t\t\t\tfo = open(flag+'.mid', 'rb')\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tfiledata = fo.read(1024)\n\t\t\t\t\t\t\tif not filedata:\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\tsock.send(filedata)\n\t\t\t\tprint 'midi file sent'\n\t\t\t\t\t\tfo.close()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint 'can not find midi'\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tprint 'json error'\n\t\t\telse:\n\t\t\t\tmusic_data += data\n\t\tsock.close()\n\t\tprint 'session '+str(sessnum)+' for '+str(addr)+' finished'\n\n\ttcpSerSock = socket(AF_INET, SOCK_STREAM)\n\ttcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n\ttcpSerSock.bind(Addr)\n\ttcpSerSock.listen(5)\n\n\twhile True:\n\t\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()\n\t\tprint 'add ', tcpCliAddr\n\t\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))\n\t\tt.start()\n\ttcpSerSock.close()\n```","slug":"dachuangserver","published":1,"updated":"2019-07-22T03:45:23.095Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v1m0015q8t5rh4j20af","content":"<hr>\n<ul>\n<li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li>\n<li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h1><ul>\n<li>从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程</li>\n<li>客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端</li>\n</ul>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*- </span></span><br><span class=\"line\"><span class=\"comment\"># ! usr/bin/python</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> socket <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> threading</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> md5</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">Host = <span class=\"string\">''</span></span><br><span class=\"line\">Port = <span class=\"number\">2017</span></span><br><span class=\"line\">Addr = (Host, Port)</span><br><span class=\"line\">midi_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">\"ignore\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">md5_encode</span><span class=\"params\">(src)</span>:</span></span><br><span class=\"line\">\tm1 = md5.new()</span><br><span class=\"line\">\tm1.update(src)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> m1.hexdigest()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tcplink</span><span class=\"params\">(sock, addr)</span>:</span></span><br><span class=\"line\">\tsessnum = <span class=\"number\">0</span></span><br><span class=\"line\">\tmusic_data = <span class=\"string\">''</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\t\tdata = sock.recv(<span class=\"number\">1480</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> data[<span class=\"number\">-9</span>:]==<span class=\"string\">'endbidou1'</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'wav recv finished'</span></span><br><span class=\"line\">\t\t\tmusic_data+=data</span><br><span class=\"line\">\t\t\tmusic_data=music_data[:<span class=\"number\">-9</span>]</span><br><span class=\"line\">\t\t\tmidi_data = eval(music_data)</span><br><span class=\"line\">\t\tsessnum = midi_data[<span class=\"string\">'request'</span>]  </span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\t\t\tflag_md5 = md5_encode(str(time.time()))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'md5: '</span>, flag_md5</span><br><span class=\"line\">\t\t\t\twav_name = flag_md5 + <span class=\"string\">'.wav'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(wav_name, <span class=\"string\">'w+'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.write(midi_data[<span class=\"string\">'data'</span>].decode(<span class=\"string\">'base64'</span>))</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tn = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'n'</span>];</span><br><span class=\"line\">\t\t\t\tm = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'m'</span>];</span><br><span class=\"line\">\t\t\t\tw = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'w'</span>];</span><br><span class=\"line\">\t\t\t\tmidi_name = flag_md5 + <span class=\"string\">'.mid'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(midi_name, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tshellmid = <span class=\"string\">'../mldm/hum2midi.py -n '</span>+str(n)+<span class=\"string\">' -m '</span>+str(m)+<span class=\"string\">' -w '</span>+str(w)+<span class=\"string\">' -o '</span> + midi_name + <span class=\"string\">' '</span> + wav_name</span><br><span class=\"line\">\t\t<span class=\"keyword\">print</span> <span class=\"string\">\"running wav2midi shell\"</span></span><br><span class=\"line\">\t\t\t\tretmid = os.system(shellmid)</span><br><span class=\"line\">\t\t\t\tretmid &gt;= <span class=\"number\">8</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> retmid == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi successful'</span></span><br><span class=\"line\">\t\t\tshellpng = <span class=\"string\">'mono ../mlds/sheet '</span>+midi_name+<span class=\"string\">' '</span>+flag_md5</span><br><span class=\"line\">\t\t\tretpng = os.system(shellpng)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> retpng == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\t\tsock.send(flag_md5.encode())</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png successful'</span></span><br><span class=\"line\">\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png error'</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi error'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">2</span>:</span><br><span class=\"line\">\t\t\t\tflag = midi_data[<span class=\"string\">'data'</span>]</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> flag <span class=\"keyword\">in</span> midi_dict.keys():</span><br><span class=\"line\">\t\t\t\t\tfo = open(flag+<span class=\"string\">'.mid'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\t\t\t\t\t\tfiledata = fo.read(<span class=\"number\">1024</span>)</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> filedata:</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t\t\tsock.send(filedata)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'midi file sent'</span></span><br><span class=\"line\">\t\t\t\t\tfo.close()</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'can not find midi'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'json error'</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\tmusic_data += data</span><br><span class=\"line\">\tsock.close()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'session '</span>+str(sessnum)+<span class=\"string\">' for '</span>+str(addr)+<span class=\"string\">' finished'</span></span><br><span class=\"line\"></span><br><span class=\"line\">tcpSerSock = socket(AF_INET, SOCK_STREAM)</span><br><span class=\"line\">tcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, <span class=\"number\">1</span>)</span><br><span class=\"line\">tcpSerSock.bind(Addr)</span><br><span class=\"line\">tcpSerSock.listen(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'add '</span>, tcpCliAddr</span><br><span class=\"line\">\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))</span><br><span class=\"line\">\tt.start()</span><br><span class=\"line\">tcpSerSock.close()</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<hr>\n<ul>\n<li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li>\n<li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li>\n</ul>","more":"<h1 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h1><ul>\n<li>从android客户端接收的数据都是json格式，base64编解码。以我们自定义的特殊字符串结尾，服务器在建立与一个终端的连接后开一个处理的线程</li>\n<li>客户端完成一次wav到midi转换需要两次通信，第一次json中request值为1，data中是wav文件，服务器负责生成md5编码的时间戳，并用时间戳命名一个wav文件存下，再调用我们的核心程序将wav转换，生成md5.mid和md5.png,即乐曲和曲谱，并回传给客户端md5值。第二次客户端发来的json中request值为2,data值为md5，服务器根据md5索引生成的midi文件回传给客户端</li>\n</ul>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*- </span></span><br><span class=\"line\"><span class=\"comment\"># ! usr/bin/python</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> socket <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> threading</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> md5</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">Host = <span class=\"string\">''</span></span><br><span class=\"line\">Port = <span class=\"number\">2017</span></span><br><span class=\"line\">Addr = (Host, Port)</span><br><span class=\"line\">midi_dict = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">\"ignore\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">md5_encode</span><span class=\"params\">(src)</span>:</span></span><br><span class=\"line\">\tm1 = md5.new()</span><br><span class=\"line\">\tm1.update(src)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> m1.hexdigest()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tcplink</span><span class=\"params\">(sock, addr)</span>:</span></span><br><span class=\"line\">\tsessnum = <span class=\"number\">0</span></span><br><span class=\"line\">\tmusic_data = <span class=\"string\">''</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\t\tdata = sock.recv(<span class=\"number\">1480</span>)</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> data[<span class=\"number\">-9</span>:]==<span class=\"string\">'endbidou1'</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'wav recv finished'</span></span><br><span class=\"line\">\t\t\tmusic_data+=data</span><br><span class=\"line\">\t\t\tmusic_data=music_data[:<span class=\"number\">-9</span>]</span><br><span class=\"line\">\t\t\tmidi_data = eval(music_data)</span><br><span class=\"line\">\t\tsessnum = midi_data[<span class=\"string\">'request'</span>]  </span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">1</span>:</span><br><span class=\"line\">\t\t\t\tflag_md5 = md5_encode(str(time.time()))</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'md5: '</span>, flag_md5</span><br><span class=\"line\">\t\t\t\twav_name = flag_md5 + <span class=\"string\">'.wav'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(wav_name, <span class=\"string\">'w+'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.write(midi_data[<span class=\"string\">'data'</span>].decode(<span class=\"string\">'base64'</span>))</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tn = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'n'</span>];</span><br><span class=\"line\">\t\t\t\tm = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'m'</span>];</span><br><span class=\"line\">\t\t\t\tw = midi_data[<span class=\"string\">'config'</span>][<span class=\"string\">'w'</span>];</span><br><span class=\"line\">\t\t\t\tmidi_name = flag_md5 + <span class=\"string\">'.mid'</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">with</span> open(midi_name, <span class=\"string\">'w'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">\t\t\t\t\tf.close()</span><br><span class=\"line\">\t\t\t\tshellmid = <span class=\"string\">'../mldm/hum2midi.py -n '</span>+str(n)+<span class=\"string\">' -m '</span>+str(m)+<span class=\"string\">' -w '</span>+str(w)+<span class=\"string\">' -o '</span> + midi_name + <span class=\"string\">' '</span> + wav_name</span><br><span class=\"line\">\t\t<span class=\"keyword\">print</span> <span class=\"string\">\"running wav2midi shell\"</span></span><br><span class=\"line\">\t\t\t\tretmid = os.system(shellmid)</span><br><span class=\"line\">\t\t\t\tretmid &gt;= <span class=\"number\">8</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> retmid == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi successful'</span></span><br><span class=\"line\">\t\t\tshellpng = <span class=\"string\">'mono ../mlds/sheet '</span>+midi_name+<span class=\"string\">' '</span>+flag_md5</span><br><span class=\"line\">\t\t\tretpng = os.system(shellpng)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> retpng == <span class=\"number\">0</span>:</span><br><span class=\"line\">\t\t\t\t\t\tsock.send(flag_md5.encode())</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png successful'</span></span><br><span class=\"line\">\t\t\t\t\t\tmidi_dict[flag_md5] = midi_name</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate png error'</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'generate midi error'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">elif</span> midi_data[<span class=\"string\">'request'</span>] == <span class=\"number\">2</span>:</span><br><span class=\"line\">\t\t\t\tflag = midi_data[<span class=\"string\">'data'</span>]</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> flag <span class=\"keyword\">in</span> midi_dict.keys():</span><br><span class=\"line\">\t\t\t\t\tfo = open(flag+<span class=\"string\">'.mid'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\t\t\t\t\t\tfiledata = fo.read(<span class=\"number\">1024</span>)</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> <span class=\"keyword\">not</span> filedata:</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t\t\tsock.send(filedata)</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'midi file sent'</span></span><br><span class=\"line\">\t\t\t\t\tfo.close()</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'can not find midi'</span></span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">print</span> <span class=\"string\">'json error'</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span>:</span><br><span class=\"line\">\t\t\tmusic_data += data</span><br><span class=\"line\">\tsock.close()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'session '</span>+str(sessnum)+<span class=\"string\">' for '</span>+str(addr)+<span class=\"string\">' finished'</span></span><br><span class=\"line\"></span><br><span class=\"line\">tcpSerSock = socket(AF_INET, SOCK_STREAM)</span><br><span class=\"line\">tcpSerSock.setsockopt(SOL_SOCKET, SO_REUSEADDR, <span class=\"number\">1</span>)</span><br><span class=\"line\">tcpSerSock.bind(Addr)</span><br><span class=\"line\">tcpSerSock.listen(<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">\ttcpCliSock, tcpCliAddr = tcpSerSock.accept()</span><br><span class=\"line\">\t<span class=\"keyword\">print</span> <span class=\"string\">'add '</span>, tcpCliAddr</span><br><span class=\"line\">\tt = threading.Thread(target=tcplink, args=(tcpCliSock, tcpCliAddr))</span><br><span class=\"line\">\tt.start()</span><br><span class=\"line\">tcpSerSock.close()</span><br></pre></td></tr></table></figure>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"Melodia服务器搭建","path":"2017/05/26/dachuangserver/","eyeCatchImage":null,"excerpt":"<hr>\n<ul>\n<li>大创项目的服务器端，大创以及客户端介绍见<a href=\"http://thinkwee.top/2017/03/09/dachuang/\">Melodia客户端</a></li>\n<li>我们大创项目的服务器承担的功能比较少，只与android设备收发文件，用Python写了一个简单的服务器端</li>\n</ul>","date":"2017-05-26T11:18:01.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","server","linux"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Correlation Explaination 学习笔记","date":"2019-07-29T03:17:11.000Z","mathjax":true,"<!-- password":"kengbi -->","html":true,"_content":"***\nCorEx(Correlation Explaination)的相关笔记。\n\n<!--more-->\n\n![etsOld.gif](https://s2.ax1x.com/2019/07/31/etsOld.gif)\n\n# 概述\n-\tCorrelation Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total Correlation之差来找出最能explain数据的Correlation的主题。 其中一种快速计算方法就简写为CorEx。\n-\t为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。\n\n# Discovering Structure in High-Dimensional Data Through Correlation Explanation\n## 定义Total Correlation\n-\t定义$X$为一离散随机变量，则其熵为\n$$\nH(X) \\equiv \\mathbb{E}_{X}[-\\log p(x)]\n$$\n-\t两个随机变量之间的互信息定义为\n$$\nI(X_1 : X_2) = H\\left(X_{1}\\right)+H\\left(X_{2}\\right)-H\\left(X_{1}, X_{2}\\right)\n$$\n-\t我们定义Total Correlation（或者叫多元互信息multivariate mutual information）为\n$$\nT C\\left(X_{G}\\right)=\\sum_{i \\in G} H\\left(X_{i}\\right)-H\\left(X_{G}\\right)\n$$\n-\t其中$G$是$X$的一个子集。只管来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。\n-\t为了更方便理解，TC还可以写成KL散度的形式\n$$\nT C\\left(X_{G}\\right)=D_{K L}\\left(p\\left(x_{G}\\right) \\| \\prod_{i \\in G} p\\left(x_{i}\\right)\\right)\n$$\n-\t也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。\n-\t接着我们定义conditional TC\n$$\nT C(X | Y)=\\sum_{i} H\\left(X_{i} | Y\\right)-H(X | Y)\n$$\n-\t那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure the extent to which $Y$ explains the correlations in $X$\n$$\nT C(X ; Y) \\equiv T C(X)-T C(X | Y)=\\sum_{i \\in \\mathbb{N}_{n}} I\\left(X_{i} : Y\\right)-I(X : Y)\n$$\n-\t$T C(X ; Y)$最大时，$T C(X | Y)$为0，也就是已知$Y$时$X$的联合分布可分解，也就说明$Y$ explains all the correlation in $X$。我们认为好的主题应当是文档的一种表示，其解释的文档Total Correlation应该最大。\n-\t现在我们就可以把$Y$看成时解释$X$的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过$p(Y|X)$来定义主题，也就是说只将其定义为一个能够影响$X$的离散随机变量，取值范围有$k$种可能，而不像LDA定义为$|V|$种取值可能。\n-\tLDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率$p(y_j)$，每个词的主题分布$p(y_j|x_i)$，词到主题子集合的分配矩阵$\\alpha$，以及每篇文档的主题分布$p(y_j|x)$\t\n-\t初始化时，我们随机设定$\\alpha$以及文档的主题分布$p(y|x)$\n-\tLDA是生成式模型，而CorEX是判别式模型。\n\n## 迭代\n-\t我们要找到的主题是\n$$\n\\max _{p(y | x)} T C(X ; Y) \\quad \\text { s.t. } \\quad|Y|=k\n$$\n-\t我们可以找m个主题，并将$X$分为m个不相交的子集来建模\n$$\n\\max _{G_{j}, p\\left(y_{j} | x_{C_{j}}\\right)} \\sum_{j=1}^{m} T C\\left(X_{G_{j}} ; Y_{j}\\right) \\quad \\text { s.t. } \\quad\\left|Y_{j}\\right|=k, G_{j} \\cap G_{j^{\\prime} \\neq j}=\\emptyset\n$$\n-\t将上式用互信息改写为\n$$\n\\max _{G, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i \\in G_{j}} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X_{G_{j}}\\right)\n$$\n-\t我们用指示函数进一步简化这个式子，去掉子集$G$的下标，统一用一个$\\alpha$连通矩阵来代表子集的划分结果\n$$\n\\alpha_{i, j}=\\mathbb{I}\\left[X_{i} \\in G_{j}\\right] \\in\\{0,1\\}  \\\\\n\\max _{\\alpha, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\\n$$\n-\t同时我们要加一个限制项保证子集不相交\n$$\n\\sum_{\\overline{j}} \\alpha_{i, \\overline{j}}=1\n$$\n-\t这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出\n$$\n\\begin{aligned} p\\left(y_{j} | x\\right) &=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\ \np\\left(y_{j} | x_{i}\\right) &=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\end{aligned} \\\\\n$$\n\n-\t注意，这是在$\\alpha$矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后$\\alpha$的迭代公式\n$$\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\alpha_{i, j}^{* *}=\\exp \\left(\\gamma\\left(I\\left(X_{i} : Y_{j}\\right)-\\max _{\\overline{j}} I\\left(X_{i} : Y_{\\overline{j}}\\right)\\right)\\right) \\\\\n$$\n## 伪算法\n$$\n\\text { input : A matrix of size } n_{s} \\times n \\text { representing } n_{s} \\text { samples of } n \\text { discrete random variables } \\\\\n\\text { set } : \\text { Set } m, \\text { the number of latent variables, } Y_{j}, \\text { and } k, \\text { so that }\\left|Y_{j}\\right|=k \\\\\n\\text { output: Parameters } \\alpha_{i, j}, p\\left(y_{j} | x_{i}\\right), p\\left(y_{j}\\right), p\\left(y | x^{(l)}\\right) \\\\\n\\text { for } i \\in \\mathbb{N}_{n}, j \\in \\mathbb{N}_{m}, l \\in \\mathbb{N}_{n_{s}}, y \\in \\mathbb{N}_{k}, x_{i} \\in \\mathcal{X}_{i} \\\\\n\\text { Randomly initialize } \\alpha_{i, j}, p\\left(y | x^{(l)}\\right) \\\\\n\\text {repeat} \\\\\n\\text { Estimate marginals, } p\\left(y_{j}\\right), p\\left(y_{j} | x_{i}\\right) \\text { using  } \\\\\np\\left(y_{j} | x_{i}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\\\\n\\text { Calculate } I\\left(X_{i} : Y_{j}\\right) \\text { from marginals; } \\\\\n\\text { Update } \\alpha \\text { using  } \\\\\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\text { Calculate } p\\left(y | x^{(l)}\\right), l=1, \\ldots, n_{s} \\text { using } \\\\\np\\left(y_{j} | x\\right)=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\\n\\text { until convergence; }\n$$\n\n# Maximally Informative Hierarchical Representations of High-Dimensional Data\n-\t本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。\n## 上界和下界\n-\t大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据$X$和表示$Y$，当联合概率可以分解时，我们称$Y$是$X$的一种表示\n$$\np(x, y)=\\prod_{j=1}^{m} p\\left(y_{j} | x\\right) p(x) \\\\\n$$\n-\t这样，一种数据的表示完全由表示变量域和条件概率$p(y_j|x)$决定。\n-\t表示可以层次性堆叠，我们定义层次表示为：\n$$\nY^{1 : r} \\equiv Y^{1}, \\ldots, Y^{r}\n$$\n-\t![eYYvRK.png](https://s2.ax1x.com/2019/07/31/eYYvRK.png)\n-\t其中$Y^k$是$Y^{k-1}$的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。\n-\t定义：\n$$\nT C_{L}(X ; Y) \\equiv \\sum_{i=1}^{n} I\\left(Y : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\\n$$\n-\t则存在以下的边界和分解：\n$$\nT C(X) \\geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)\n$$\n-\t同时得到$Y$关于$X$的TC值的一个下界：\n$$\nT C(X ; Y) \\geq T C_{L}(X ; Y)\n$$\n-\t当$TC(Y)$为0时取到下界，这时$Y$之间相互独立，不包含关于$X$的信息。将上面$TC(X)$的不等式扩展到层次表示，则可以得到\n$$\nT C(X) \\geq \\sum_{k=1}^{r} T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)\n$$\n-\t注意在这里我们定义第0层表示就是$X$，我们还能找到上界\n$$\nT C(X) \\leq \\sum_{k=1}^{r}\\left(T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)+\\sum_{i=1}^{m_{k-1}} H\\left(Y_{i}^{k-1} | Y^{k}\\right)\\right)\n$$\n-\t可以看到上界与下界之间就差了一堆累加的条件熵。\n-\tTC的上下界可以帮助衡量表示对于数据的解释程度，\n## 分析\n-\t先考虑最简单的情况，即第一层表示只有一个变量$Y^{1} \\equiv Y_{1}^{1}$，这时\n$$\nTC(Y)+TC_L(X;Y)=TC(X;Y) \\leq TC(X) \\leq TC_L(X;Y)+\\sum _{i=1}^{m_0} H(X_i|Y)\n$$\n-\t待补充\n## 优化\n-\t我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly explain the correlations in the layer below)，这可以通过优化下界得到，以第一层为例\n$$\n\\max _{\\forall j, p\\left(y_{j}^{1} | x\\right)} T C_{L}\\left(X ; Y^{1}\\right)\n$$\n-\t定义$\\alpha$祖先信息为\n$$\nA I_{\\alpha}(X ; Y) \\equiv \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\\\\n\\alpha_{i} \\in[0,1] \\\\\n$$\n-\t假如给定某个$\\alpha$，其$AI_{\\alpha}$为正，则 it implies the existence of common ancestors for some ($\\alpha$-dependent) set of $X_i$ ’s in any DAG that describes $X$，这里不太懂，但可以看成是上文联通矩阵$\\alpha$的泛化版本，从binarize泛化到01区间。最优化问题用$AI_{\\alpha}$表示可以写成\n$$\n\\max _{p(y | x)} \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X)\n$$\n-\t化成了和上文一样的形式，之后的解法也一样\n$$\np(y | x)=\\frac{1}{Z(x)} p(y) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\n$$\n-\t对归一化分母$Z(x)$取对数期望，可以得到自由能量，这正是我们的优化目标\n$$\n\\begin{aligned} \\mathbb{E}[\\log Z(x)] &=\\mathbb{E}\\left[\\log \\frac{p(y)}{p(y | x)} \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\\right] \\\\ &=\\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\end{aligned}\n$$\n-\t对于多个隐变量，作者重构了下界，同样将$\\alpha$扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的$TC_L(X;Y)$变为优化$p(y_j|x)$和$\\alpha$的下界：\n$$\n\\max _{\\alpha_{i, j}, p\\left(y_{j} | x\\right) \\atop c_{i, j}\\left(\\alpha_{i, j}\\right)=0}^{m} \\sum_{j=1}^m \\left(\\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-I\\left(Y_{j} : X\\right)\\right)\n$$\n-\t$\\alpha$定义了$X_i$和$Y_j$之间的关系，即结构。至于优化结构，理想的情况是\n$$\n\\alpha _{i,j} = \\mathbb{I} [j = argmax _{j} I(X_i : Y_j)]\n$$\n-\t这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于$I(Y_j : X_i | Y_{1:j-1}) \\geq \\alpha _{i,j} I(Y_j : X_i)$,作者提出了一种启发式的算法来估计$\\alpha$。我们检查$X_i$是否正确估计$Y_j$\n$$\nd_{i,j}^l \\equiv \\mathbb{I} [argmax_{y_j} \\log p(Y_j = y_j|x^{(l)}) = argmax_{y_j} \\log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]\n$$\n-\t之后我们在所有样本上累加，统计正确估计数目，并根据比例设置$\\alpha$值。\n\n# Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge\n## 概述\n-\t本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：\n\t-\t不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数\n\t-\t不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型\n-\t模型的迭代依然是这几步：\n$$\np_t(y_j) = \\sum _{\\overline{x}} p_t(y_j | \\overline{x})p(\\overline{x}) \\\\\np_t(x_i | y_j) = \\sum _{\\overline{x}} p_t(y_j|\\overline{x})p(\\overline{x}) \\mathbb{I} [\\overline{x}_i = x_i]/p_t(y_j) \\\\\n\\log p_{t+1} (y_j | x^l) = \\log p_t(y_j) + \\sum _{i=1}^n \\alpha _{i,j}^t \\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \\log \\mathbb{Z} _j (x^l) \\\\\n$$\n-\t由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：\n$$\n\\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \\log \\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \\log (\\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})\n$$\n-\t这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中$\\log P(X_i^l=1|y_j)/p(X_i^l=1)$为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。\n-\t这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。\n## 半监督\n-\t半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的$\\alpha$在01区间之间，而将第i号词anchor在第j号主题可以将$\\alpha_{i,j} = \\beta _{i,j}$，其中$\\beta$是anchor的强度。\n-\t这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。\n-\t在业务上来说，CorEx的优势在于：\n\t-\t在训练超大规模主题数时非常快。\n\t-\t可以方便的anchor词以适应领域。\n\t-\tCorEx的主题之间词是不相交的，不会出现重复主题\n-\t层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。\n","source":"_posts/CorEx.md","raw":"---\ntitle: Correlation Explaination 学习笔记\ndate: 2019-07-29 11:17:11\ncategories: 机器学习\ntags:\n  - corex\n  - machine learning\n  -\ttopic model\nmathjax: true\n<!-- password: kengbi -->\nhtml: true\n---\n***\nCorEx(Correlation Explaination)的相关笔记。\n\n<!--more-->\n\n![etsOld.gif](https://s2.ax1x.com/2019/07/31/etsOld.gif)\n\n# 概述\n-\tCorrelation Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total Correlation之差来找出最能explain数据的Correlation的主题。 其中一种快速计算方法就简写为CorEx。\n-\t为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。\n\n# Discovering Structure in High-Dimensional Data Through Correlation Explanation\n## 定义Total Correlation\n-\t定义$X$为一离散随机变量，则其熵为\n$$\nH(X) \\equiv \\mathbb{E}_{X}[-\\log p(x)]\n$$\n-\t两个随机变量之间的互信息定义为\n$$\nI(X_1 : X_2) = H\\left(X_{1}\\right)+H\\left(X_{2}\\right)-H\\left(X_{1}, X_{2}\\right)\n$$\n-\t我们定义Total Correlation（或者叫多元互信息multivariate mutual information）为\n$$\nT C\\left(X_{G}\\right)=\\sum_{i \\in G} H\\left(X_{i}\\right)-H\\left(X_{G}\\right)\n$$\n-\t其中$G$是$X$的一个子集。只管来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。\n-\t为了更方便理解，TC还可以写成KL散度的形式\n$$\nT C\\left(X_{G}\\right)=D_{K L}\\left(p\\left(x_{G}\\right) \\| \\prod_{i \\in G} p\\left(x_{i}\\right)\\right)\n$$\n-\t也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。\n-\t接着我们定义conditional TC\n$$\nT C(X | Y)=\\sum_{i} H\\left(X_{i} | Y\\right)-H(X | Y)\n$$\n-\t那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure the extent to which $Y$ explains the correlations in $X$\n$$\nT C(X ; Y) \\equiv T C(X)-T C(X | Y)=\\sum_{i \\in \\mathbb{N}_{n}} I\\left(X_{i} : Y\\right)-I(X : Y)\n$$\n-\t$T C(X ; Y)$最大时，$T C(X | Y)$为0，也就是已知$Y$时$X$的联合分布可分解，也就说明$Y$ explains all the correlation in $X$。我们认为好的主题应当是文档的一种表示，其解释的文档Total Correlation应该最大。\n-\t现在我们就可以把$Y$看成时解释$X$的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过$p(Y|X)$来定义主题，也就是说只将其定义为一个能够影响$X$的离散随机变量，取值范围有$k$种可能，而不像LDA定义为$|V|$种取值可能。\n-\tLDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率$p(y_j)$，每个词的主题分布$p(y_j|x_i)$，词到主题子集合的分配矩阵$\\alpha$，以及每篇文档的主题分布$p(y_j|x)$\t\n-\t初始化时，我们随机设定$\\alpha$以及文档的主题分布$p(y|x)$\n-\tLDA是生成式模型，而CorEX是判别式模型。\n\n## 迭代\n-\t我们要找到的主题是\n$$\n\\max _{p(y | x)} T C(X ; Y) \\quad \\text { s.t. } \\quad|Y|=k\n$$\n-\t我们可以找m个主题，并将$X$分为m个不相交的子集来建模\n$$\n\\max _{G_{j}, p\\left(y_{j} | x_{C_{j}}\\right)} \\sum_{j=1}^{m} T C\\left(X_{G_{j}} ; Y_{j}\\right) \\quad \\text { s.t. } \\quad\\left|Y_{j}\\right|=k, G_{j} \\cap G_{j^{\\prime} \\neq j}=\\emptyset\n$$\n-\t将上式用互信息改写为\n$$\n\\max _{G, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i \\in G_{j}} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X_{G_{j}}\\right)\n$$\n-\t我们用指示函数进一步简化这个式子，去掉子集$G$的下标，统一用一个$\\alpha$连通矩阵来代表子集的划分结果\n$$\n\\alpha_{i, j}=\\mathbb{I}\\left[X_{i} \\in G_{j}\\right] \\in\\{0,1\\}  \\\\\n\\max _{\\alpha, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\\n$$\n-\t同时我们要加一个限制项保证子集不相交\n$$\n\\sum_{\\overline{j}} \\alpha_{i, \\overline{j}}=1\n$$\n-\t这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出\n$$\n\\begin{aligned} p\\left(y_{j} | x\\right) &=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\ \np\\left(y_{j} | x_{i}\\right) &=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\end{aligned} \\\\\n$$\n\n-\t注意，这是在$\\alpha$矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后$\\alpha$的迭代公式\n$$\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\alpha_{i, j}^{* *}=\\exp \\left(\\gamma\\left(I\\left(X_{i} : Y_{j}\\right)-\\max _{\\overline{j}} I\\left(X_{i} : Y_{\\overline{j}}\\right)\\right)\\right) \\\\\n$$\n## 伪算法\n$$\n\\text { input : A matrix of size } n_{s} \\times n \\text { representing } n_{s} \\text { samples of } n \\text { discrete random variables } \\\\\n\\text { set } : \\text { Set } m, \\text { the number of latent variables, } Y_{j}, \\text { and } k, \\text { so that }\\left|Y_{j}\\right|=k \\\\\n\\text { output: Parameters } \\alpha_{i, j}, p\\left(y_{j} | x_{i}\\right), p\\left(y_{j}\\right), p\\left(y | x^{(l)}\\right) \\\\\n\\text { for } i \\in \\mathbb{N}_{n}, j \\in \\mathbb{N}_{m}, l \\in \\mathbb{N}_{n_{s}}, y \\in \\mathbb{N}_{k}, x_{i} \\in \\mathcal{X}_{i} \\\\\n\\text { Randomly initialize } \\alpha_{i, j}, p\\left(y | x^{(l)}\\right) \\\\\n\\text {repeat} \\\\\n\\text { Estimate marginals, } p\\left(y_{j}\\right), p\\left(y_{j} | x_{i}\\right) \\text { using  } \\\\\np\\left(y_{j} | x_{i}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\\\\n\\text { Calculate } I\\left(X_{i} : Y_{j}\\right) \\text { from marginals; } \\\\\n\\text { Update } \\alpha \\text { using  } \\\\\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\text { Calculate } p\\left(y | x^{(l)}\\right), l=1, \\ldots, n_{s} \\text { using } \\\\\np\\left(y_{j} | x\\right)=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\\n\\text { until convergence; }\n$$\n\n# Maximally Informative Hierarchical Representations of High-Dimensional Data\n-\t本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。\n## 上界和下界\n-\t大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据$X$和表示$Y$，当联合概率可以分解时，我们称$Y$是$X$的一种表示\n$$\np(x, y)=\\prod_{j=1}^{m} p\\left(y_{j} | x\\right) p(x) \\\\\n$$\n-\t这样，一种数据的表示完全由表示变量域和条件概率$p(y_j|x)$决定。\n-\t表示可以层次性堆叠，我们定义层次表示为：\n$$\nY^{1 : r} \\equiv Y^{1}, \\ldots, Y^{r}\n$$\n-\t![eYYvRK.png](https://s2.ax1x.com/2019/07/31/eYYvRK.png)\n-\t其中$Y^k$是$Y^{k-1}$的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。\n-\t定义：\n$$\nT C_{L}(X ; Y) \\equiv \\sum_{i=1}^{n} I\\left(Y : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\\n$$\n-\t则存在以下的边界和分解：\n$$\nT C(X) \\geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)\n$$\n-\t同时得到$Y$关于$X$的TC值的一个下界：\n$$\nT C(X ; Y) \\geq T C_{L}(X ; Y)\n$$\n-\t当$TC(Y)$为0时取到下界，这时$Y$之间相互独立，不包含关于$X$的信息。将上面$TC(X)$的不等式扩展到层次表示，则可以得到\n$$\nT C(X) \\geq \\sum_{k=1}^{r} T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)\n$$\n-\t注意在这里我们定义第0层表示就是$X$，我们还能找到上界\n$$\nT C(X) \\leq \\sum_{k=1}^{r}\\left(T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)+\\sum_{i=1}^{m_{k-1}} H\\left(Y_{i}^{k-1} | Y^{k}\\right)\\right)\n$$\n-\t可以看到上界与下界之间就差了一堆累加的条件熵。\n-\tTC的上下界可以帮助衡量表示对于数据的解释程度，\n## 分析\n-\t先考虑最简单的情况，即第一层表示只有一个变量$Y^{1} \\equiv Y_{1}^{1}$，这时\n$$\nTC(Y)+TC_L(X;Y)=TC(X;Y) \\leq TC(X) \\leq TC_L(X;Y)+\\sum _{i=1}^{m_0} H(X_i|Y)\n$$\n-\t待补充\n## 优化\n-\t我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly explain the correlations in the layer below)，这可以通过优化下界得到，以第一层为例\n$$\n\\max _{\\forall j, p\\left(y_{j}^{1} | x\\right)} T C_{L}\\left(X ; Y^{1}\\right)\n$$\n-\t定义$\\alpha$祖先信息为\n$$\nA I_{\\alpha}(X ; Y) \\equiv \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\\\\n\\alpha_{i} \\in[0,1] \\\\\n$$\n-\t假如给定某个$\\alpha$，其$AI_{\\alpha}$为正，则 it implies the existence of common ancestors for some ($\\alpha$-dependent) set of $X_i$ ’s in any DAG that describes $X$，这里不太懂，但可以看成是上文联通矩阵$\\alpha$的泛化版本，从binarize泛化到01区间。最优化问题用$AI_{\\alpha}$表示可以写成\n$$\n\\max _{p(y | x)} \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X)\n$$\n-\t化成了和上文一样的形式，之后的解法也一样\n$$\np(y | x)=\\frac{1}{Z(x)} p(y) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\n$$\n-\t对归一化分母$Z(x)$取对数期望，可以得到自由能量，这正是我们的优化目标\n$$\n\\begin{aligned} \\mathbb{E}[\\log Z(x)] &=\\mathbb{E}\\left[\\log \\frac{p(y)}{p(y | x)} \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\\right] \\\\ &=\\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\end{aligned}\n$$\n-\t对于多个隐变量，作者重构了下界，同样将$\\alpha$扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的$TC_L(X;Y)$变为优化$p(y_j|x)$和$\\alpha$的下界：\n$$\n\\max _{\\alpha_{i, j}, p\\left(y_{j} | x\\right) \\atop c_{i, j}\\left(\\alpha_{i, j}\\right)=0}^{m} \\sum_{j=1}^m \\left(\\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-I\\left(Y_{j} : X\\right)\\right)\n$$\n-\t$\\alpha$定义了$X_i$和$Y_j$之间的关系，即结构。至于优化结构，理想的情况是\n$$\n\\alpha _{i,j} = \\mathbb{I} [j = argmax _{j} I(X_i : Y_j)]\n$$\n-\t这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于$I(Y_j : X_i | Y_{1:j-1}) \\geq \\alpha _{i,j} I(Y_j : X_i)$,作者提出了一种启发式的算法来估计$\\alpha$。我们检查$X_i$是否正确估计$Y_j$\n$$\nd_{i,j}^l \\equiv \\mathbb{I} [argmax_{y_j} \\log p(Y_j = y_j|x^{(l)}) = argmax_{y_j} \\log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]\n$$\n-\t之后我们在所有样本上累加，统计正确估计数目，并根据比例设置$\\alpha$值。\n\n# Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge\n## 概述\n-\t本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：\n\t-\t不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数\n\t-\t不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型\n-\t模型的迭代依然是这几步：\n$$\np_t(y_j) = \\sum _{\\overline{x}} p_t(y_j | \\overline{x})p(\\overline{x}) \\\\\np_t(x_i | y_j) = \\sum _{\\overline{x}} p_t(y_j|\\overline{x})p(\\overline{x}) \\mathbb{I} [\\overline{x}_i = x_i]/p_t(y_j) \\\\\n\\log p_{t+1} (y_j | x^l) = \\log p_t(y_j) + \\sum _{i=1}^n \\alpha _{i,j}^t \\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \\log \\mathbb{Z} _j (x^l) \\\\\n$$\n-\t由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：\n$$\n\\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \\log \\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \\log (\\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})\n$$\n-\t这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中$\\log P(X_i^l=1|y_j)/p(X_i^l=1)$为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。\n-\t这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。\n## 半监督\n-\t半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的$\\alpha$在01区间之间，而将第i号词anchor在第j号主题可以将$\\alpha_{i,j} = \\beta _{i,j}$，其中$\\beta$是anchor的强度。\n-\t这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。\n-\t在业务上来说，CorEx的优势在于：\n\t-\t在训练超大规模主题数时非常快。\n\t-\t可以方便的anchor词以适应领域。\n\t-\tCorEx的主题之间词是不相交的，不会出现重复主题\n-\t层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。\n","slug":"CorEx","published":1,"updated":"2019-08-02T02:42:24.682Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v250017q8t5w6ijjtw9","content":"<hr>\n<p>CorEx(Correlation Explaination)的相关笔记。</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s2.ax1x.com/2019/07/31/etsOld.gif\" alt=\"etsOld.gif\"></p>\n<h1 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h1><ul>\n<li>Correlation Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total Correlation之差来找出最能explain数据的Correlation的主题。 其中一种快速计算方法就简写为CorEx。</li>\n<li>为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。</li>\n</ul>\n<h1 id=\"Discovering-Structure-in-High-Dimensional-Data-Through-Correlation-Explanation\"><a href=\"#Discovering-Structure-in-High-Dimensional-Data-Through-Correlation-Explanation\" class=\"headerlink\" title=\"Discovering Structure in High-Dimensional Data Through Correlation Explanation\"></a>Discovering Structure in High-Dimensional Data Through Correlation Explanation</h1><h2 id=\"定义Total-Correlation\"><a href=\"#定义Total-Correlation\" class=\"headerlink\" title=\"定义Total Correlation\"></a>定义Total Correlation</h2><ul>\n<li>定义$X$为一离散随机变量，则其熵为<script type=\"math/tex; mode=display\">\nH(X) \\equiv \\mathbb{E}_{X}[-\\log p(x)]</script></li>\n<li>两个随机变量之间的互信息定义为<script type=\"math/tex; mode=display\">\nI(X_1 : X_2) = H\\left(X_{1}\\right)+H\\left(X_{2}\\right)-H\\left(X_{1}, X_{2}\\right)</script></li>\n<li>我们定义Total Correlation（或者叫多元互信息multivariate mutual information）为<script type=\"math/tex; mode=display\">\nT C\\left(X_{G}\\right)=\\sum_{i \\in G} H\\left(X_{i}\\right)-H\\left(X_{G}\\right)</script></li>\n<li>其中$G$是$X$的一个子集。只管来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。</li>\n<li>为了更方便理解，TC还可以写成KL散度的形式<script type=\"math/tex; mode=display\">\nT C\\left(X_{G}\\right)=D_{K L}\\left(p\\left(x_{G}\\right) \\| \\prod_{i \\in G} p\\left(x_{i}\\right)\\right)</script></li>\n<li>也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。</li>\n<li>接着我们定义conditional TC<script type=\"math/tex; mode=display\">\nT C(X | Y)=\\sum_{i} H\\left(X_{i} | Y\\right)-H(X | Y)</script></li>\n<li>那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure the extent to which $Y$ explains the correlations in $X$<script type=\"math/tex; mode=display\">\nT C(X ; Y) \\equiv T C(X)-T C(X | Y)=\\sum_{i \\in \\mathbb{N}_{n}} I\\left(X_{i} : Y\\right)-I(X : Y)</script></li>\n<li>$T C(X ; Y)$最大时，$T C(X | Y)$为0，也就是已知$Y$时$X$的联合分布可分解，也就说明$Y$ explains all the correlation in $X$。我们认为好的主题应当是文档的一种表示，其解释的文档Total Correlation应该最大。</li>\n<li>现在我们就可以把$Y$看成时解释$X$的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过$p(Y|X)$来定义主题，也就是说只将其定义为一个能够影响$X$的离散随机变量，取值范围有$k$种可能，而不像LDA定义为$|V|$种取值可能。</li>\n<li>LDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率$p(y_j)$，每个词的主题分布$p(y_j|x_i)$，词到主题子集合的分配矩阵$\\alpha$，以及每篇文档的主题分布$p(y_j|x)$    </li>\n<li>初始化时，我们随机设定$\\alpha$以及文档的主题分布$p(y|x)$</li>\n<li>LDA是生成式模型，而CorEX是判别式模型。</li>\n</ul>\n<h2 id=\"迭代\"><a href=\"#迭代\" class=\"headerlink\" title=\"迭代\"></a>迭代</h2><ul>\n<li>我们要找到的主题是<script type=\"math/tex; mode=display\">\n\\max _{p(y | x)} T C(X ; Y) \\quad \\text { s.t. } \\quad|Y|=k</script></li>\n<li>我们可以找m个主题，并将$X$分为m个不相交的子集来建模<script type=\"math/tex; mode=display\">\n\\max _{G_{j}, p\\left(y_{j} | x_{C_{j}}\\right)} \\sum_{j=1}^{m} T C\\left(X_{G_{j}} ; Y_{j}\\right) \\quad \\text { s.t. } \\quad\\left|Y_{j}\\right|=k, G_{j} \\cap G_{j^{\\prime} \\neq j}=\\emptyset</script></li>\n<li>将上式用互信息改写为<script type=\"math/tex; mode=display\">\n\\max _{G, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i \\in G_{j}} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X_{G_{j}}\\right)</script></li>\n<li>我们用指示函数进一步简化这个式子，去掉子集$G$的下标，统一用一个$\\alpha$连通矩阵来代表子集的划分结果<script type=\"math/tex; mode=display\">\n\\alpha_{i, j}=\\mathbb{I}\\left[X_{i} \\in G_{j}\\right] \\in\\{0,1\\}  \\\\\n\\max _{\\alpha, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\</script></li>\n<li>同时我们要加一个限制项保证子集不相交<script type=\"math/tex; mode=display\">\n\\sum_{\\overline{j}} \\alpha_{i, \\overline{j}}=1</script></li>\n<li><p>这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} p\\left(y_{j} | x\\right) &=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\ \np\\left(y_{j} | x_{i}\\right) &=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\end{aligned} \\\\</script></li>\n<li><p>注意，这是在$\\alpha$矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后$\\alpha$的迭代公式</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\alpha_{i, j}^{* *}=\\exp \\left(\\gamma\\left(I\\left(X_{i} : Y_{j}\\right)-\\max _{\\overline{j}} I\\left(X_{i} : Y_{\\overline{j}}\\right)\\right)\\right) \\\\</script><h2 id=\"伪算法\"><a href=\"#伪算法\" class=\"headerlink\" title=\"伪算法\"></a>伪算法</h2><script type=\"math/tex; mode=display\">\n\\text { input : A matrix of size } n_{s} \\times n \\text { representing } n_{s} \\text { samples of } n \\text { discrete random variables } \\\\\n\\text { set } : \\text { Set } m, \\text { the number of latent variables, } Y_{j}, \\text { and } k, \\text { so that }\\left|Y_{j}\\right|=k \\\\\n\\text { output: Parameters } \\alpha_{i, j}, p\\left(y_{j} | x_{i}\\right), p\\left(y_{j}\\right), p\\left(y | x^{(l)}\\right) \\\\\n\\text { for } i \\in \\mathbb{N}_{n}, j \\in \\mathbb{N}_{m}, l \\in \\mathbb{N}_{n_{s}}, y \\in \\mathbb{N}_{k}, x_{i} \\in \\mathcal{X}_{i} \\\\\n\\text { Randomly initialize } \\alpha_{i, j}, p\\left(y | x^{(l)}\\right) \\\\\n\\text {repeat} \\\\\n\\text { Estimate marginals, } p\\left(y_{j}\\right), p\\left(y_{j} | x_{i}\\right) \\text { using  } \\\\\np\\left(y_{j} | x_{i}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\\\\n\\text { Calculate } I\\left(X_{i} : Y_{j}\\right) \\text { from marginals; } \\\\\n\\text { Update } \\alpha \\text { using  } \\\\\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\text { Calculate } p\\left(y | x^{(l)}\\right), l=1, \\ldots, n_{s} \\text { using } \\\\\np\\left(y_{j} | x\\right)=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\\n\\text { until convergence; }</script></li>\n</ul>\n<h1 id=\"Maximally-Informative-Hierarchical-Representations-of-High-Dimensional-Data\"><a href=\"#Maximally-Informative-Hierarchical-Representations-of-High-Dimensional-Data\" class=\"headerlink\" title=\"Maximally Informative Hierarchical Representations of High-Dimensional Data\"></a>Maximally Informative Hierarchical Representations of High-Dimensional Data</h1><ul>\n<li>本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。<h2 id=\"上界和下界\"><a href=\"#上界和下界\" class=\"headerlink\" title=\"上界和下界\"></a>上界和下界</h2></li>\n<li>大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据$X$和表示$Y$，当联合概率可以分解时，我们称$Y$是$X$的一种表示<script type=\"math/tex; mode=display\">\np(x, y)=\\prod_{j=1}^{m} p\\left(y_{j} | x\\right) p(x) \\\\</script></li>\n<li>这样，一种数据的表示完全由表示变量域和条件概率$p(y_j|x)$决定。</li>\n<li>表示可以层次性堆叠，我们定义层次表示为：<script type=\"math/tex; mode=display\">\nY^{1 : r} \\equiv Y^{1}, \\ldots, Y^{r}</script></li>\n<li><img src=\"https://s2.ax1x.com/2019/07/31/eYYvRK.png\" alt=\"eYYvRK.png\"></li>\n<li>其中$Y^k$是$Y^{k-1}$的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。</li>\n<li>定义：<script type=\"math/tex; mode=display\">\nT C_{L}(X ; Y) \\equiv \\sum_{i=1}^{n} I\\left(Y : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\</script></li>\n<li>则存在以下的边界和分解：<script type=\"math/tex; mode=display\">\nT C(X) \\geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)</script></li>\n<li>同时得到$Y$关于$X$的TC值的一个下界：<script type=\"math/tex; mode=display\">\nT C(X ; Y) \\geq T C_{L}(X ; Y)</script></li>\n<li>当$TC(Y)$为0时取到下界，这时$Y$之间相互独立，不包含关于$X$的信息。将上面$TC(X)$的不等式扩展到层次表示，则可以得到<script type=\"math/tex; mode=display\">\nT C(X) \\geq \\sum_{k=1}^{r} T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)</script></li>\n<li>注意在这里我们定义第0层表示就是$X$，我们还能找到上界<script type=\"math/tex; mode=display\">\nT C(X) \\leq \\sum_{k=1}^{r}\\left(T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)+\\sum_{i=1}^{m_{k-1}} H\\left(Y_{i}^{k-1} | Y^{k}\\right)\\right)</script></li>\n<li>可以看到上界与下界之间就差了一堆累加的条件熵。</li>\n<li>TC的上下界可以帮助衡量表示对于数据的解释程度，<h2 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h2></li>\n<li>先考虑最简单的情况，即第一层表示只有一个变量$Y^{1} \\equiv Y_{1}^{1}$，这时<script type=\"math/tex; mode=display\">\nTC(Y)+TC_L(X;Y)=TC(X;Y) \\leq TC(X) \\leq TC_L(X;Y)+\\sum _{i=1}^{m_0} H(X_i|Y)</script></li>\n<li>待补充<h2 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h2></li>\n<li>我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly explain the correlations in the layer below)，这可以通过优化下界得到，以第一层为例<script type=\"math/tex; mode=display\">\n\\max _{\\forall j, p\\left(y_{j}^{1} | x\\right)} T C_{L}\\left(X ; Y^{1}\\right)</script></li>\n<li>定义$\\alpha$祖先信息为<script type=\"math/tex; mode=display\">\nA I_{\\alpha}(X ; Y) \\equiv \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\\\\n\\alpha_{i} \\in[0,1] \\\\</script></li>\n<li>假如给定某个$\\alpha$，其$AI_{\\alpha}$为正，则 it implies the existence of common ancestors for some ($\\alpha$-dependent) set of $X_i$ ’s in any DAG that describes $X$，这里不太懂，但可以看成是上文联通矩阵$\\alpha$的泛化版本，从binarize泛化到01区间。最优化问题用$AI_{\\alpha}$表示可以写成<script type=\"math/tex; mode=display\">\n\\max _{p(y | x)} \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X)</script></li>\n<li>化成了和上文一样的形式，之后的解法也一样<script type=\"math/tex; mode=display\">\np(y | x)=\\frac{1}{Z(x)} p(y) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}</script></li>\n<li>对归一化分母$Z(x)$取对数期望，可以得到自由能量，这正是我们的优化目标<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\mathbb{E}[\\log Z(x)] &=\\mathbb{E}\\left[\\log \\frac{p(y)}{p(y | x)} \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\\right] \\\\ &=\\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\end{aligned}</script></li>\n<li>对于多个隐变量，作者重构了下界，同样将$\\alpha$扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的$TC_L(X;Y)$变为优化$p(y_j|x)$和$\\alpha$的下界：<script type=\"math/tex; mode=display\">\n\\max _{\\alpha_{i, j}, p\\left(y_{j} | x\\right) \\atop c_{i, j}\\left(\\alpha_{i, j}\\right)=0}^{m} \\sum_{j=1}^m \\left(\\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-I\\left(Y_{j} : X\\right)\\right)</script></li>\n<li>$\\alpha$定义了$X_i$和$Y_j$之间的关系，即结构。至于优化结构，理想的情况是<script type=\"math/tex; mode=display\">\n\\alpha _{i,j} = \\mathbb{I} [j = argmax _{j} I(X_i : Y_j)]</script></li>\n<li>这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于$I(Y_j : X_i | Y_{1:j-1}) \\geq \\alpha _{i,j} I(Y_j : X_i)$,作者提出了一种启发式的算法来估计$\\alpha$。我们检查$X_i$是否正确估计$Y_j$<script type=\"math/tex; mode=display\">\nd_{i,j}^l \\equiv \\mathbb{I} [argmax_{y_j} \\log p(Y_j = y_j|x^{(l)}) = argmax_{y_j} \\log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]</script></li>\n<li>之后我们在所有样本上累加，统计正确估计数目，并根据比例设置$\\alpha$值。</li>\n</ul>\n<h1 id=\"Anchored-Correlation-Explanation-Topic-Modeling-with-Minimal-Domain-Knowledge\"><a href=\"#Anchored-Correlation-Explanation-Topic-Modeling-with-Minimal-Domain-Knowledge\" class=\"headerlink\" title=\"Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge\"></a>Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge</h1><h2 id=\"概述-1\"><a href=\"#概述-1\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li>本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：<ul>\n<li>不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数</li>\n<li>不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型</li>\n</ul>\n</li>\n<li>模型的迭代依然是这几步：<script type=\"math/tex; mode=display\">\np_t(y_j) = \\sum _{\\overline{x}} p_t(y_j | \\overline{x})p(\\overline{x}) \\\\\np_t(x_i | y_j) = \\sum _{\\overline{x}} p_t(y_j|\\overline{x})p(\\overline{x}) \\mathbb{I} [\\overline{x}_i = x_i]/p_t(y_j) \\\\\n\\log p_{t+1} (y_j | x^l) = \\log p_t(y_j) + \\sum _{i=1}^n \\alpha _{i,j}^t \\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \\log \\mathbb{Z} _j (x^l) \\\\</script></li>\n<li>由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：<script type=\"math/tex; mode=display\">\n\\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \\log \\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \\log (\\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})</script></li>\n<li>这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中$\\log P(X_i^l=1|y_j)/p(X_i^l=1)$为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。</li>\n<li>这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。<h2 id=\"半监督\"><a href=\"#半监督\" class=\"headerlink\" title=\"半监督\"></a>半监督</h2></li>\n<li>半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的$\\alpha$在01区间之间，而将第i号词anchor在第j号主题可以将$\\alpha_{i,j} = \\beta _{i,j}$，其中$\\beta$是anchor的强度。</li>\n<li>这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。</li>\n<li>在业务上来说，CorEx的优势在于：<ul>\n<li>在训练超大规模主题数时非常快。</li>\n<li>可以方便的anchor词以适应领域。</li>\n<li>CorEx的主题之间词是不相交的，不会出现重复主题</li>\n</ul>\n</li>\n<li>层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>CorEx(Correlation Explaination)的相关笔记。</p>","more":"<p><img src=\"https://s2.ax1x.com/2019/07/31/etsOld.gif\" alt=\"etsOld.gif\"></p>\n<h1 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h1><ul>\n<li>Correlation Explaination是一类表示学习方法，可用于主题模型，与LDA具有相似的结果但其处理过程完全不同。Correlation Explaination不对数据的生成做任何结构上的先验假设，而是类似于信息增益，用Total Correlation之差来找出最能explain数据的Correlation的主题。 其中一种快速计算方法就简写为CorEx。</li>\n<li>为了方便起见，下文都是用LDA中的概念来类比CorEx中的概念，包括背景是文档的主题建模，主题是一个离散随机变量，一篇文档会包含多个主题等等。</li>\n</ul>\n<h1 id=\"Discovering-Structure-in-High-Dimensional-Data-Through-Correlation-Explanation\"><a href=\"#Discovering-Structure-in-High-Dimensional-Data-Through-Correlation-Explanation\" class=\"headerlink\" title=\"Discovering Structure in High-Dimensional Data Through Correlation Explanation\"></a>Discovering Structure in High-Dimensional Data Through Correlation Explanation</h1><h2 id=\"定义Total-Correlation\"><a href=\"#定义Total-Correlation\" class=\"headerlink\" title=\"定义Total Correlation\"></a>定义Total Correlation</h2><ul>\n<li>定义$X$为一离散随机变量，则其熵为<script type=\"math/tex; mode=display\">\nH(X) \\equiv \\mathbb{E}_{X}[-\\log p(x)]</script></li>\n<li>两个随机变量之间的互信息定义为<script type=\"math/tex; mode=display\">\nI(X_1 : X_2) = H\\left(X_{1}\\right)+H\\left(X_{2}\\right)-H\\left(X_{1}, X_{2}\\right)</script></li>\n<li>我们定义Total Correlation（或者叫多元互信息multivariate mutual information）为<script type=\"math/tex; mode=display\">\nT C\\left(X_{G}\\right)=\\sum_{i \\in G} H\\left(X_{i}\\right)-H\\left(X_{G}\\right)</script></li>\n<li>其中$G$是$X$的一个子集。只管来看就是子集中每一个随机变量熵之和减去子集的联合熵。当G中只有两个变量时，TC等价于两个变量的互信息。</li>\n<li>为了更方便理解，TC还可以写成KL散度的形式<script type=\"math/tex; mode=display\">\nT C\\left(X_{G}\\right)=D_{K L}\\left(p\\left(x_{G}\\right) \\| \\prod_{i \\in G} p\\left(x_{i}\\right)\\right)</script></li>\n<li>也就是说TC可以看成联合分布和边缘分布累乘之间的KL散度，那么当TC为0时，KL散度为0，联合分布等于边缘分布累乘，也就意味着数据内部的相关性为0，变量之间相互独立，联合分布可以factorize为边缘分布之积。</li>\n<li>接着我们定义conditional TC<script type=\"math/tex; mode=display\">\nT C(X | Y)=\\sum_{i} H\\left(X_{i} | Y\\right)-H(X | Y)</script></li>\n<li>那么我们就可以用TC与条件TC之差来衡量某一条件（变量）对于数据的correlation的贡献，原文写的是measure the extent to which $Y$ explains the correlations in $X$<script type=\"math/tex; mode=display\">\nT C(X ; Y) \\equiv T C(X)-T C(X | Y)=\\sum_{i \\in \\mathbb{N}_{n}} I\\left(X_{i} : Y\\right)-I(X : Y)</script></li>\n<li>$T C(X ; Y)$最大时，$T C(X | Y)$为0，也就是已知$Y$时$X$的联合分布可分解，也就说明$Y$ explains all the correlation in $X$。我们认为好的主题应当是文档的一种表示，其解释的文档Total Correlation应该最大。</li>\n<li>现在我们就可以把$Y$看成时解释$X$的一个隐变量，也就是主题，接下来我们就要求出主题。在LDA中，主题明确定义为词概率分布，然而在CorEx，我们通过$p(Y|X)$来定义主题，也就是说只将其定义为一个能够影响$X$的离散随机变量，取值范围有$k$种可能，而不像LDA定义为$|V|$种取值可能。</li>\n<li>LDA通过迭代，不断更新为每个词分配的主题，从而间接得到文档的主题分布和主题的词分布。而CorEx则不一样，无论文档还是词都会计算一个主题分布。CorEx根据公式不断更新每个主题的概率$p(y_j)$，每个词的主题分布$p(y_j|x_i)$，词到主题子集合的分配矩阵$\\alpha$，以及每篇文档的主题分布$p(y_j|x)$    </li>\n<li>初始化时，我们随机设定$\\alpha$以及文档的主题分布$p(y|x)$</li>\n<li>LDA是生成式模型，而CorEX是判别式模型。</li>\n</ul>\n<h2 id=\"迭代\"><a href=\"#迭代\" class=\"headerlink\" title=\"迭代\"></a>迭代</h2><ul>\n<li>我们要找到的主题是<script type=\"math/tex; mode=display\">\n\\max _{p(y | x)} T C(X ; Y) \\quad \\text { s.t. } \\quad|Y|=k</script></li>\n<li>我们可以找m个主题，并将$X$分为m个不相交的子集来建模<script type=\"math/tex; mode=display\">\n\\max _{G_{j}, p\\left(y_{j} | x_{C_{j}}\\right)} \\sum_{j=1}^{m} T C\\left(X_{G_{j}} ; Y_{j}\\right) \\quad \\text { s.t. } \\quad\\left|Y_{j}\\right|=k, G_{j} \\cap G_{j^{\\prime} \\neq j}=\\emptyset</script></li>\n<li>将上式用互信息改写为<script type=\"math/tex; mode=display\">\n\\max _{G, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i \\in G_{j}} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X_{G_{j}}\\right)</script></li>\n<li>我们用指示函数进一步简化这个式子，去掉子集$G$的下标，统一用一个$\\alpha$连通矩阵来代表子集的划分结果<script type=\"math/tex; mode=display\">\n\\alpha_{i, j}=\\mathbb{I}\\left[X_{i} \\in G_{j}\\right] \\in\\{0,1\\}  \\\\\n\\max _{\\alpha, p\\left(y_{j} | x\\right)} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\</script></li>\n<li>同时我们要加一个限制项保证子集不相交<script type=\"math/tex; mode=display\">\n\\sum_{\\overline{j}} \\alpha_{i, \\overline{j}}=1</script></li>\n<li><p>这是一个带有限制项的最优化问题，通过拉格朗日乘子法可以解出</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned} p\\left(y_{j} | x\\right) &=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\ \np\\left(y_{j} | x_{i}\\right) &=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\end{aligned} \\\\</script></li>\n<li><p>注意，这是在$\\alpha$矩阵确认的情况下得到的主题最优解，通过放宽最优解条件，我们可以得到在主题更新之后$\\alpha$的迭代公式</p>\n<script type=\"math/tex; mode=display\">\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\alpha_{i, j}^{* *}=\\exp \\left(\\gamma\\left(I\\left(X_{i} : Y_{j}\\right)-\\max _{\\overline{j}} I\\left(X_{i} : Y_{\\overline{j}}\\right)\\right)\\right) \\\\</script><h2 id=\"伪算法\"><a href=\"#伪算法\" class=\"headerlink\" title=\"伪算法\"></a>伪算法</h2><script type=\"math/tex; mode=display\">\n\\text { input : A matrix of size } n_{s} \\times n \\text { representing } n_{s} \\text { samples of } n \\text { discrete random variables } \\\\\n\\text { set } : \\text { Set } m, \\text { the number of latent variables, } Y_{j}, \\text { and } k, \\text { so that }\\left|Y_{j}\\right|=k \\\\\n\\text { output: Parameters } \\alpha_{i, j}, p\\left(y_{j} | x_{i}\\right), p\\left(y_{j}\\right), p\\left(y | x^{(l)}\\right) \\\\\n\\text { for } i \\in \\mathbb{N}_{n}, j \\in \\mathbb{N}_{m}, l \\in \\mathbb{N}_{n_{s}}, y \\in \\mathbb{N}_{k}, x_{i} \\in \\mathcal{X}_{i} \\\\\n\\text { Randomly initialize } \\alpha_{i, j}, p\\left(y | x^{(l)}\\right) \\\\\n\\text {repeat} \\\\\n\\text { Estimate marginals, } p\\left(y_{j}\\right), p\\left(y_{j} | x_{i}\\right) \\text { using  } \\\\\np\\left(y_{j} | x_{i}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\delta_{\\overline{x}_{i}, x_{i}} / p\\left(x_{i}\\right) \\text { and } p\\left(y_{j}\\right)=\\sum_{\\overline{x}} p\\left(y_{j} | \\overline{x}\\right) p(\\overline{x}) \\\\\n\\text { Calculate } I\\left(X_{i} : Y_{j}\\right) \\text { from marginals; } \\\\\n\\text { Update } \\alpha \\text { using  } \\\\\n\\alpha_{i, j}^{t+1}=(1-\\lambda) \\alpha_{i, j}^{t}+\\lambda \\alpha_{i, j}^{* *} \\\\\n\\text { Calculate } p\\left(y | x^{(l)}\\right), l=1, \\ldots, n_{s} \\text { using } \\\\\np\\left(y_{j} | x\\right)=\\frac{1}{Z_{j}(x)} p\\left(y_{j}\\right) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y_{j} | x_{i}\\right)}{p\\left(y_{j}\\right)}\\right)^{\\alpha_{i, j}} \\\\\n\\text { until convergence; }</script></li>\n</ul>\n<h1 id=\"Maximally-Informative-Hierarchical-Representations-of-High-Dimensional-Data\"><a href=\"#Maximally-Informative-Hierarchical-Representations-of-High-Dimensional-Data\" class=\"headerlink\" title=\"Maximally Informative Hierarchical Representations of High-Dimensional Data\"></a>Maximally Informative Hierarchical Representations of High-Dimensional Data</h1><ul>\n<li>本文分析了TC的上下界，有助于进一步理解TC的含义，并提出了一种最大化信息量的层次结构高维数据表示的优化方法，上文提到的CorEx可以看成这种优化方法的一种特例。<h2 id=\"上界和下界\"><a href=\"#上界和下界\" class=\"headerlink\" title=\"上界和下界\"></a>上界和下界</h2></li>\n<li>大部分定义与上文类似，更为一般性，我们将文档和主题扩展为数据$X$和表示$Y$，当联合概率可以分解时，我们称$Y$是$X$的一种表示<script type=\"math/tex; mode=display\">\np(x, y)=\\prod_{j=1}^{m} p\\left(y_{j} | x\\right) p(x) \\\\</script></li>\n<li>这样，一种数据的表示完全由表示变量域和条件概率$p(y_j|x)$决定。</li>\n<li>表示可以层次性堆叠，我们定义层次表示为：<script type=\"math/tex; mode=display\">\nY^{1 : r} \\equiv Y^{1}, \\ldots, Y^{r}</script></li>\n<li><img src=\"https://s2.ax1x.com/2019/07/31/eYYvRK.png\" alt=\"eYYvRK.png\"></li>\n<li>其中$Y^k$是$Y^{k-1}$的表示。我们主要关注量化层次表示对于数据的信息化程度的上下界。这种层次表示是一种一般性表示，包括了RBM和自编码器等等。</li>\n<li>定义：<script type=\"math/tex; mode=display\">\nT C_{L}(X ; Y) \\equiv \\sum_{i=1}^{n} I\\left(Y : X_{i}\\right)-\\sum_{j=1}^{m} I\\left(Y_{j} : X\\right) \\\\</script></li>\n<li>则存在以下的边界和分解：<script type=\"math/tex; mode=display\">\nT C(X) \\geq T C(X ; Y)=T C(Y)+T C_{L}(X ; Y)</script></li>\n<li>同时得到$Y$关于$X$的TC值的一个下界：<script type=\"math/tex; mode=display\">\nT C(X ; Y) \\geq T C_{L}(X ; Y)</script></li>\n<li>当$TC(Y)$为0时取到下界，这时$Y$之间相互独立，不包含关于$X$的信息。将上面$TC(X)$的不等式扩展到层次表示，则可以得到<script type=\"math/tex; mode=display\">\nT C(X) \\geq \\sum_{k=1}^{r} T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)</script></li>\n<li>注意在这里我们定义第0层表示就是$X$，我们还能找到上界<script type=\"math/tex; mode=display\">\nT C(X) \\leq \\sum_{k=1}^{r}\\left(T C_{L}\\left(Y^{k-1} ; Y^{k}\\right)+\\sum_{i=1}^{m_{k-1}} H\\left(Y_{i}^{k-1} | Y^{k}\\right)\\right)</script></li>\n<li>可以看到上界与下界之间就差了一堆累加的条件熵。</li>\n<li>TC的上下界可以帮助衡量表示对于数据的解释程度，<h2 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h2></li>\n<li>先考虑最简单的情况，即第一层表示只有一个变量$Y^{1} \\equiv Y_{1}^{1}$，这时<script type=\"math/tex; mode=display\">\nTC(Y)+TC_L(X;Y)=TC(X;Y) \\leq TC(X) \\leq TC_L(X;Y)+\\sum _{i=1}^{m_0} H(X_i|Y)</script></li>\n<li>待补充<h2 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h2></li>\n<li>我们可以逐层优化，使得每一层最大化解释下一层的相关性（maximallly explain the correlations in the layer below)，这可以通过优化下界得到，以第一层为例<script type=\"math/tex; mode=display\">\n\\max _{\\forall j, p\\left(y_{j}^{1} | x\\right)} T C_{L}\\left(X ; Y^{1}\\right)</script></li>\n<li>定义$\\alpha$祖先信息为<script type=\"math/tex; mode=display\">\nA I_{\\alpha}(X ; Y) \\equiv \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\\\\n\\alpha_{i} \\in[0,1] \\\\</script></li>\n<li>假如给定某个$\\alpha$，其$AI_{\\alpha}$为正，则 it implies the existence of common ancestors for some ($\\alpha$-dependent) set of $X_i$ ’s in any DAG that describes $X$，这里不太懂，但可以看成是上文联通矩阵$\\alpha$的泛化版本，从binarize泛化到01区间。最优化问题用$AI_{\\alpha}$表示可以写成<script type=\"math/tex; mode=display\">\n\\max _{p(y | x)} \\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X)</script></li>\n<li>化成了和上文一样的形式，之后的解法也一样<script type=\"math/tex; mode=display\">\np(y | x)=\\frac{1}{Z(x)} p(y) \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}</script></li>\n<li>对归一化分母$Z(x)$取对数期望，可以得到自由能量，这正是我们的优化目标<script type=\"math/tex; mode=display\">\n\\begin{aligned} \\mathbb{E}[\\log Z(x)] &=\\mathbb{E}\\left[\\log \\frac{p(y)}{p(y | x)} \\prod_{i=1}^{n}\\left(\\frac{p\\left(y | x_{i}\\right)}{p(y)}\\right)^{\\alpha_{i}}\\right] \\\\ &=\\sum_{i=1}^{n} \\alpha_{i} I\\left(Y : X_{i}\\right)-I(Y : X) \\end{aligned}</script></li>\n<li>对于多个隐变量，作者重构了下界，同样将$\\alpha$扩展到01区间的连续值。具体过程比较复杂，最后的优化目标从最大化所有隐单元的$TC_L(X;Y)$变为优化$p(y_j|x)$和$\\alpha$的下界：<script type=\"math/tex; mode=display\">\n\\max _{\\alpha_{i, j}, p\\left(y_{j} | x\\right) \\atop c_{i, j}\\left(\\alpha_{i, j}\\right)=0}^{m} \\sum_{j=1}^m \\left(\\sum_{i=1}^{n} \\alpha_{i, j} I\\left(Y_{j} : X_{i}\\right)-I\\left(Y_{j} : X\\right)\\right)</script></li>\n<li>$\\alpha$定义了$X_i$和$Y_j$之间的关系，即结构。至于优化结构，理想的情况是<script type=\"math/tex; mode=display\">\n\\alpha _{i,j} = \\mathbb{I} [j = argmax _{j} I(X_i : Y_j)]</script></li>\n<li>这样的结构是硬连接的，每一个节点只和下一层的某一个隐藏层节点相连接，基于$I(Y_j : X_i | Y_{1:j-1}) \\geq \\alpha _{i,j} I(Y_j : X_i)$,作者提出了一种启发式的算法来估计$\\alpha$。我们检查$X_i$是否正确估计$Y_j$<script type=\"math/tex; mode=display\">\nd_{i,j}^l \\equiv \\mathbb{I} [argmax_{y_j} \\log p(Y_j = y_j|x^{(l)}) = argmax_{y_j} \\log p(Y_j = y_j | x_i^{(l)}) / p(Y_j = y_j)]</script></li>\n<li>之后我们在所有样本上累加，统计正确估计数目，并根据比例设置$\\alpha$值。</li>\n</ul>\n<h1 id=\"Anchored-Correlation-Explanation-Topic-Modeling-with-Minimal-Domain-Knowledge\"><a href=\"#Anchored-Correlation-Explanation-Topic-Modeling-with-Minimal-Domain-Knowledge\" class=\"headerlink\" title=\"Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge\"></a>Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge</h1><h2 id=\"概述-1\"><a href=\"#概述-1\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li>本文正式将CorEx应用于主题模型中，并强调了优于LDA的几点：<ul>\n<li>不需要对数据做结构假设，相比LDA，CorEX具有更少的超参数</li>\n<li>不同于LDA，无需对模型做结构上的修改，即可泛化到层次模型和半监督没模型</li>\n</ul>\n</li>\n<li>模型的迭代依然是这几步：<script type=\"math/tex; mode=display\">\np_t(y_j) = \\sum _{\\overline{x}} p_t(y_j | \\overline{x})p(\\overline{x}) \\\\\np_t(x_i | y_j) = \\sum _{\\overline{x}} p_t(y_j|\\overline{x})p(\\overline{x}) \\mathbb{I} [\\overline{x}_i = x_i]/p_t(y_j) \\\\\n\\log p_{t+1} (y_j | x^l) = \\log p_t(y_j) + \\sum _{i=1}^n \\alpha _{i,j}^t \\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} - \\log \\mathbb{Z} _j (x^l) \\\\</script></li>\n<li>由于我们利用的是词袋信息，处理的是稀疏矩阵，因此边缘概率和条件概率计算都非常快，迭代中最慢的一步是第三个式子，即计算所有文档的主题分布。我们重写这个式子中累加的对数项：<script type=\"math/tex; mode=display\">\n\\log \\frac{p_t(x_i^l | y_j)}{p(x_i^l)} = \\log \\frac{p_t(X_i=0|y_j)}{p(X_i=0)} + x_i^l \\log (\\frac{p_t(X_i^l=1|y_j)p(X_i=0)}{p_t(X_i=0|y_j)p(X_i^l=1)})</script></li>\n<li>这个累加是对每篇文档，在整个词典上计算似然，然而每篇文档只会出现一小部分词，当词典中的词没有出现在文档中的时候，上式中只有第一项不为0；当词出现时，上式中$\\log P(X_i^l=1|y_j)/p(X_i^l=1)$为0，其余项保留，因此作者优先假设词不在文档中，之后再更新补充那些在文档中的词的概率项。经过这样的优化之后CorEx的计算速度和LDA差不多。</li>\n<li>这个优化最大的好处是计算的复杂度只和文档数和主题数线性相关，因此计算大规模文档上的大规模主题成为可能。<h2 id=\"半监督\"><a href=\"#半监督\" class=\"headerlink\" title=\"半监督\"></a>半监督</h2></li>\n<li>半监督的思路非常简单，我们如何保证某一个主题一定出现某些词？只需要将连通矩阵的某些值固定即可。正常的$\\alpha$在01区间之间，而将第i号词anchor在第j号主题可以将$\\alpha_{i,j} = \\beta _{i,j}$，其中$\\beta$是anchor的强度。</li>\n<li>这么做可以给每个主题anchor词，anchor一个或多个词，非常灵活。</li>\n<li>在业务上来说，CorEx的优势在于：<ul>\n<li>在训练超大规模主题数时非常快。</li>\n<li>可以方便的anchor词以适应领域。</li>\n<li>CorEx的主题之间词是不相交的，不会出现重复主题</li>\n</ul>\n</li>\n<li>层次主题就按照上一篇论文中的层次方法迭代，层次主题可以用于聚合概念，划分子话题。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s2.ax1x.com/2019/07/31/etsOld.gif","popularPost_tmp_gaData":{"updated":"Fri Aug 02 2019 10:42:24 GMT+0800 (GMT+08:00)","title":"Correlation Explaination 学习笔记","path":"2019/07/29/CorEx/","eyeCatchImage":"https://s2.ax1x.com/2019/07/31/etsOld.gif","excerpt":"<hr>\n<p>CorEx(Correlation Explaination)的相关笔记。</p>","date":"2019-07-29T03:17:11.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["corex","machine learning","topic model"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"自然语言处理|深度学习入门","date":"2018-03-07T01:56:23.000Z","author":"Thinkwee","mathjax":true,"_content":"记录入门NLP中seq2seq模型时学习到的一些深度学习基础知识。\n  \n<!--more-->\n![i0I5WV.jpg](https://s1.ax1x.com/2018/10/20/i0I5WV.jpg)\n\n# 前馈神经网络相关\n-\t数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。\n-\t激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。\n-\tSoftmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）\n-\t隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：\n\t$$\n\tg(z,\\alpha)_i = max(0,z_i) + \\alpha _i min(0,z_i)\n\t$$\n\t绝对值整流：右边系数为-1\n\t渗漏整流：右边系数固定为一个较小值\n\t参数化整流：系数放到模型中学习\n\n# 反向传播\n-\t反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。\n-\t神经网络中的反向传播：\n\t![i0IIzT.png](https://s1.ax1x.com/2018/10/20/i0IIzT.png)\n\t初始化梯度表\n\t最后一层输出对输出求梯度，因此初始值为1\n\t从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。\n\t本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。\n\n# RNN循环神经网络\n## RNN\n-\t特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。\n-\t用双曲正切作为隐藏层激活函数\n-\t输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y\n-\t基本结构（展开和非展开）：\n\t![i0I7yF.png](https://s1.ax1x.com/2018/10/20/i0I7yF.png)\n-\t几种变式：\n -\t每一个时间步均有输出，隐藏层之间有循环连接：\n\t![i0ITQU.png](https://s1.ax1x.com/2018/10/20/i0ITQU.png)\n -\t每一个时间步均有输出，输出与隐藏层之间有循环连接：\n\t![i0IqeJ.png](https://s1.ax1x.com/2018/10/20/i0IqeJ.png)\n -\t读取整个序列后产生单个输出：\n\t![i0oCOe.png](https://s1.ax1x.com/2018/10/20/i0oCOe.png)\n-\t普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。\n\t前馈过程：\n\t$$\n\t\\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\\\\n\th^{(t)} = tanh(a^{(t)}), \\\\\n\to^{(t)} = c + Vh^{(t)}, \\\\\n\ty^{(t)} = softmax(o^{(t)}) \\\\\n\t$$\n\t代价函数：\n\t$$\n\tL(\\{ x^{(1)} , ... , x^{(\\tau)}\\},\\{ y^{(1)} , ... , y^{(\\tau)}\\}) \\\\\n\t=\\sum _t L^{(t)} \\\\\n\t= - \\sum _t log p_{model} (y^{(t)}|\\{ x^{(1)} , ... , x^{(\\tau)}\\}) \\\\\n\t$$\n-\t改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）\n\t导师驱动模型：\n\t![i0ILw9.png](https://s1.ax1x.com/2018/10/20/i0ILw9.png)\n\n## 双向RNN\n-\t考虑对未来信息的依赖，相当于两类隐藏层结合在一起\n\n## 序列到序列\n-\t采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。\n\t![i0IOoR.png](https://s1.ax1x.com/2018/10/20/i0IOoR.png)\n\n## 深度RNN\n-\tA.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）\n-\tB.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深\n-\tC.引入跳跃连接来缓解加深网络后导致的路径延长效应\n\t![i0IjF1.png](https://s1.ax1x.com/2018/10/20/i0IjF1.png)\n\n## RNN中的长期依赖问题\n-\t长期依赖问题：模型变深，失去了学习到先前信息的能力\n-\t对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。\n-\t最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）\n\n## 门控RNN\n-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。\n-\t渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。\n\n# LSTM\n-\tLSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）\n-\tLSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：\n\t![i0IvJx.png](https://s1.ax1x.com/2018/10/20/i0IvJx.png)\n-\t可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h\n-\t所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。\n-\t三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。\n-\t内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。\n-\t细胞输出h，内部状态过激活函数，由输出门控制。\n-\t另一张更好理解的图：\n\t![i0IxW6.png](https://s1.ax1x.com/2018/10/20/i0IxW6.png)\n\n# 双向LSTM\n-\t同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。\n-\t因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：\n -\t直接连接(concat)\t\n -\t求和\n\t![i0oSSK.png](https://s1.ax1x.com/2018/10/20/i0oSSK.png)\n\n# 词嵌入、Word2Vec\n-\t使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：\n-\t分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。\n-\t重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。\n-\t没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：[技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720050&idx=2&sn=9fedc937d3128462c478ef7911e77687&chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&scene=21#wechat_redirect)\n\n# 注意力机制\n-\t在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。\n-\t注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。\n","source":"_posts/NLPBasic.md","raw":"---\ntitle: 自然语言处理|深度学习入门\ndate: 2018-03-07 09:56:23\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 自然语言处理\nauthor: Thinkwee\nmathjax: true\n---\n记录入门NLP中seq2seq模型时学习到的一些深度学习基础知识。\n  \n<!--more-->\n![i0I5WV.jpg](https://s1.ax1x.com/2018/10/20/i0I5WV.jpg)\n\n# 前馈神经网络相关\n-\t数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。\n-\t激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。\n-\tSoftmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）\n-\t隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：\n\t$$\n\tg(z,\\alpha)_i = max(0,z_i) + \\alpha _i min(0,z_i)\n\t$$\n\t绝对值整流：右边系数为-1\n\t渗漏整流：右边系数固定为一个较小值\n\t参数化整流：系数放到模型中学习\n\n# 反向传播\n-\t反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。\n-\t神经网络中的反向传播：\n\t![i0IIzT.png](https://s1.ax1x.com/2018/10/20/i0IIzT.png)\n\t初始化梯度表\n\t最后一层输出对输出求梯度，因此初始值为1\n\t从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。\n\t本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。\n\n# RNN循环神经网络\n## RNN\n-\t特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。\n-\t用双曲正切作为隐藏层激活函数\n-\t输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y\n-\t基本结构（展开和非展开）：\n\t![i0I7yF.png](https://s1.ax1x.com/2018/10/20/i0I7yF.png)\n-\t几种变式：\n -\t每一个时间步均有输出，隐藏层之间有循环连接：\n\t![i0ITQU.png](https://s1.ax1x.com/2018/10/20/i0ITQU.png)\n -\t每一个时间步均有输出，输出与隐藏层之间有循环连接：\n\t![i0IqeJ.png](https://s1.ax1x.com/2018/10/20/i0IqeJ.png)\n -\t读取整个序列后产生单个输出：\n\t![i0oCOe.png](https://s1.ax1x.com/2018/10/20/i0oCOe.png)\n-\t普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。\n\t前馈过程：\n\t$$\n\t\\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\\\\n\th^{(t)} = tanh(a^{(t)}), \\\\\n\to^{(t)} = c + Vh^{(t)}, \\\\\n\ty^{(t)} = softmax(o^{(t)}) \\\\\n\t$$\n\t代价函数：\n\t$$\n\tL(\\{ x^{(1)} , ... , x^{(\\tau)}\\},\\{ y^{(1)} , ... , y^{(\\tau)}\\}) \\\\\n\t=\\sum _t L^{(t)} \\\\\n\t= - \\sum _t log p_{model} (y^{(t)}|\\{ x^{(1)} , ... , x^{(\\tau)}\\}) \\\\\n\t$$\n-\t改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）\n\t导师驱动模型：\n\t![i0ILw9.png](https://s1.ax1x.com/2018/10/20/i0ILw9.png)\n\n## 双向RNN\n-\t考虑对未来信息的依赖，相当于两类隐藏层结合在一起\n\n## 序列到序列\n-\t采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。\n\t![i0IOoR.png](https://s1.ax1x.com/2018/10/20/i0IOoR.png)\n\n## 深度RNN\n-\tA.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）\n-\tB.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深\n-\tC.引入跳跃连接来缓解加深网络后导致的路径延长效应\n\t![i0IjF1.png](https://s1.ax1x.com/2018/10/20/i0IjF1.png)\n\n## RNN中的长期依赖问题\n-\t长期依赖问题：模型变深，失去了学习到先前信息的能力\n-\t对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。\n-\t最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）\n\n## 门控RNN\n-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。\n-\t渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。\n\n# LSTM\n-\tLSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）\n-\tLSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：\n\t![i0IvJx.png](https://s1.ax1x.com/2018/10/20/i0IvJx.png)\n-\t可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h\n-\t所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。\n-\t三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。\n-\t内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。\n-\t细胞输出h，内部状态过激活函数，由输出门控制。\n-\t另一张更好理解的图：\n\t![i0IxW6.png](https://s1.ax1x.com/2018/10/20/i0IxW6.png)\n\n# 双向LSTM\n-\t同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。\n-\t因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：\n -\t直接连接(concat)\t\n -\t求和\n\t![i0oSSK.png](https://s1.ax1x.com/2018/10/20/i0oSSK.png)\n\n# 词嵌入、Word2Vec\n-\t使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：\n-\t分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。\n-\t重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。\n-\t没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：[技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720050&idx=2&sn=9fedc937d3128462c478ef7911e77687&chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&scene=21#wechat_redirect)\n\n# 注意力机制\n-\t在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。\n-\t注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。\n","slug":"NLPBasic","published":1,"updated":"2019-07-22T03:45:22.847Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v2k001aq8t50s4cpdg2","content":"<p>记录入门NLP中seq2seq模型时学习到的一些深度学习基础知识。</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I5WV.jpg\" alt=\"i0I5WV.jpg\"></p>\n<h1 id=\"前馈神经网络相关\"><a href=\"#前馈神经网络相关\" class=\"headerlink\" title=\"前馈神经网络相关\"></a>前馈神经网络相关</h1><ul>\n<li>数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。</li>\n<li>激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。</li>\n<li>Softmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）</li>\n<li>隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：<script type=\"math/tex; mode=display\">\ng(z,\\alpha)_i = max(0,z_i) + \\alpha _i min(0,z_i)</script>绝对值整流：右边系数为-1<br>渗漏整流：右边系数固定为一个较小值<br>参数化整流：系数放到模型中学习</li>\n</ul>\n<h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><ul>\n<li>反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。</li>\n<li>神经网络中的反向传播：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IIzT.png\" alt=\"i0IIzT.png\"><br>初始化梯度表<br>最后一层输出对输出求梯度，因此初始值为1<br>从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。<br>本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。</li>\n</ul>\n<h1 id=\"RNN循环神经网络\"><a href=\"#RNN循环神经网络\" class=\"headerlink\" title=\"RNN循环神经网络\"></a>RNN循环神经网络</h1><h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><ul>\n<li>特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。</li>\n<li>用双曲正切作为隐藏层激活函数</li>\n<li>输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y</li>\n<li>基本结构（展开和非展开）：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0I7yF.png\" alt=\"i0I7yF.png\"></li>\n<li>几种变式：<ul>\n<li>每一个时间步均有输出，隐藏层之间有循环连接：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ITQU.png\" alt=\"i0ITQU.png\"></li>\n<li>每一个时间步均有输出，输出与隐藏层之间有循环连接：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IqeJ.png\" alt=\"i0IqeJ.png\"></li>\n<li>读取整个序列后产生单个输出：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oCOe.png\" alt=\"i0oCOe.png\"></li>\n</ul>\n</li>\n<li>普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。<br>前馈过程：<script type=\"math/tex; mode=display\">\n\\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\\\\nh^{(t)} = tanh(a^{(t)}), \\\\\no^{(t)} = c + Vh^{(t)}, \\\\\ny^{(t)} = softmax(o^{(t)}) \\\\</script>代价函数：<script type=\"math/tex; mode=display\">\nL(\\{ x^{(1)} , ... , x^{(\\tau)}\\},\\{ y^{(1)} , ... , y^{(\\tau)}\\}) \\\\\n=\\sum _t L^{(t)} \\\\\n= - \\sum _t log p_{model} (y^{(t)}|\\{ x^{(1)} , ... , x^{(\\tau)}\\}) \\\\</script></li>\n<li>改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）<br>导师驱动模型：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ILw9.png\" alt=\"i0ILw9.png\"></li>\n</ul>\n<h2 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h2><ul>\n<li>考虑对未来信息的依赖，相当于两类隐藏层结合在一起</li>\n</ul>\n<h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul>\n<li>采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IOoR.png\" alt=\"i0IOoR.png\"></li>\n</ul>\n<h2 id=\"深度RNN\"><a href=\"#深度RNN\" class=\"headerlink\" title=\"深度RNN\"></a>深度RNN</h2><ul>\n<li>A.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）</li>\n<li>B.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深</li>\n<li>C.引入跳跃连接来缓解加深网络后导致的路径延长效应<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IjF1.png\" alt=\"i0IjF1.png\"></li>\n</ul>\n<h2 id=\"RNN中的长期依赖问题\"><a href=\"#RNN中的长期依赖问题\" class=\"headerlink\" title=\"RNN中的长期依赖问题\"></a>RNN中的长期依赖问题</h2><ul>\n<li>长期依赖问题：模型变深，失去了学习到先前信息的能力</li>\n<li>对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。</li>\n<li>最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）</li>\n</ul>\n<h2 id=\"门控RNN\"><a href=\"#门控RNN\" class=\"headerlink\" title=\"门控RNN\"></a>门控RNN</h2><p>-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。</p>\n<ul>\n<li>渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。</li>\n</ul>\n<h1 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h1><ul>\n<li>LSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）</li>\n<li>LSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IvJx.png\" alt=\"i0IvJx.png\"></li>\n<li>可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h</li>\n<li>所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。</li>\n<li>三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。</li>\n<li>内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。</li>\n<li>细胞输出h，内部状态过激活函数，由输出门控制。</li>\n<li>另一张更好理解的图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IxW6.png\" alt=\"i0IxW6.png\"></li>\n</ul>\n<h1 id=\"双向LSTM\"><a href=\"#双向LSTM\" class=\"headerlink\" title=\"双向LSTM\"></a>双向LSTM</h1><ul>\n<li>同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。</li>\n<li>因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：<ul>\n<li>直接连接(concat)    </li>\n<li>求和<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oSSK.png\" alt=\"i0oSSK.png\"></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"词嵌入、Word2Vec\"><a href=\"#词嵌入、Word2Vec\" class=\"headerlink\" title=\"词嵌入、Word2Vec\"></a>词嵌入、Word2Vec</h1><ul>\n<li>使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：</li>\n<li>分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。</li>\n<li>重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。</li>\n<li>没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650720050&amp;idx=2&amp;sn=9fedc937d3128462c478ef7911e77687&amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</a></li>\n</ul>\n<h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><ul>\n<li>在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。</li>\n<li>注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>记录入门NLP中seq2seq模型时学习到的一些深度学习基础知识。</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I5WV.jpg\" alt=\"i0I5WV.jpg\"></p>\n<h1 id=\"前馈神经网络相关\"><a href=\"#前馈神经网络相关\" class=\"headerlink\" title=\"前馈神经网络相关\"></a>前馈神经网络相关</h1><ul>\n<li>数据维数很高时，样本空间容量可能远大于训练样本数目，导致维数灾难。</li>\n<li>激活函数.用于表示神经网络中的非线性变换，没有激活函数而只有权重矩阵的话神经网络是线性变换的组合，依然是线性变换。利用非线性变换能提取出方便进行线性变换的特征，避免维数灾难(?)。</li>\n<li>Softmax和sigmoid的意义：具有最大熵，方便求导，万能近似定理（具有挤压性质）</li>\n<li>隐藏层可使用激活函数ReLU，即整流线性，缺陷是不能使用梯度学习使函数激活为0的样本，因此发展了三种扩展：<script type=\"math/tex; mode=display\">\ng(z,\\alpha)_i = max(0,z_i) + \\alpha _i min(0,z_i)</script>绝对值整流：右边系数为-1<br>渗漏整流：右边系数固定为一个较小值<br>参数化整流：系数放到模型中学习</li>\n</ul>\n<h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><ul>\n<li>反向传播将输出的偏差通过梯度计算出参数的更新值，从输出层往输入层一层一层更新参数，传播的是上一层用到的梯度，利用向量的微积分链式法则。 每一层更新量=本层Jacobian矩阵*上一层梯度。</li>\n<li>神经网络中的反向传播：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IIzT.png\" alt=\"i0IIzT.png\"><br>初始化梯度表<br>最后一层输出对输出求梯度，因此初始值为1<br>从后往前循环，本层的梯度表是本层Jacobian矩阵和上一层梯度表相乘（即链式求导）。<br>本层使用上一层的梯度表进行计算，并存储，避免链式法则中的多次重复计算。</li>\n</ul>\n<h1 id=\"RNN循环神经网络\"><a href=\"#RNN循环神经网络\" class=\"headerlink\" title=\"RNN循环神经网络\"></a>RNN循环神经网络</h1><h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><ul>\n<li>特点：所有的隐藏层共享参数，将隐藏层作为状态变量，方便参数化。</li>\n<li>用双曲正切作为隐藏层激活函数</li>\n<li>输入x，x过权重矩阵经隐藏层激活后得到h，h过权重矩阵输出o，代价L，o经输出激活后得到y</li>\n<li>基本结构（展开和非展开）：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0I7yF.png\" alt=\"i0I7yF.png\"></li>\n<li>几种变式：<ul>\n<li>每一个时间步均有输出，隐藏层之间有循环连接：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ITQU.png\" alt=\"i0ITQU.png\"></li>\n<li>每一个时间步均有输出，输出与隐藏层之间有循环连接：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IqeJ.png\" alt=\"i0IqeJ.png\"></li>\n<li>读取整个序列后产生单个输出：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oCOe.png\" alt=\"i0oCOe.png\"></li>\n</ul>\n</li>\n<li>普通的前向传播，softmax处理输出，负对数似然作为损失函数，通过时间反向传播代价过大。<br>前馈过程：<script type=\"math/tex; mode=display\">\n\\alpha ^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}, \\\\\nh^{(t)} = tanh(a^{(t)}), \\\\\no^{(t)} = c + Vh^{(t)}, \\\\\ny^{(t)} = softmax(o^{(t)}) \\\\</script>代价函数：<script type=\"math/tex; mode=display\">\nL(\\{ x^{(1)} , ... , x^{(\\tau)}\\},\\{ y^{(1)} , ... , y^{(\\tau)}\\}) \\\\\n=\\sum _t L^{(t)} \\\\\n= - \\sum _t log p_{model} (y^{(t)}|\\{ x^{(1)} , ... , x^{(\\tau)}\\}) \\\\</script></li>\n<li>改为第二种RNN，使用输出到隐藏层的循环，消除了隐藏层到隐藏层的循环，解耦并行(?)，使用导师驱动模型（用正确输出训练到隐藏层的循环网络W，测试时用贴近正确输出的实际输出经过网络W）<br>导师驱动模型：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ILw9.png\" alt=\"i0ILw9.png\"></li>\n</ul>\n<h2 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h2><ul>\n<li>考虑对未来信息的依赖，相当于两类隐藏层结合在一起</li>\n</ul>\n<h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul>\n<li>采用编码器和解码器，可以让输入输出序列长度不同，生成表示（输入序列到向量），再由表示生成序列（一个向量输入映射到序列）。序列到序列是一类框架，编码解码器使用的具体模型可以自定。例如机器翻译，编码器和解码器都可以用LSTM。端到端的模型利用中间表示c，使得输出仅依赖于表示和之前输出的序列。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IOoR.png\" alt=\"i0IOoR.png\"></li>\n</ul>\n<h2 id=\"深度RNN\"><a href=\"#深度RNN\" class=\"headerlink\" title=\"深度RNN\"></a>深度RNN</h2><ul>\n<li>A.将循环状态加深，分解为多个具有层次的组，即横向加深（一次循环内隐藏层更新经过多次状态）</li>\n<li>B.在输入到隐藏，隐藏到输出，隐藏到隐藏之间引入神经网络，即对隐藏层状态不仅横向（时间步）加深，而且纵向（一次训练）加深</li>\n<li>C.引入跳跃连接来缓解加深网络后导致的路径延长效应<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IjF1.png\" alt=\"i0IjF1.png\"></li>\n</ul>\n<h2 id=\"RNN中的长期依赖问题\"><a href=\"#RNN中的长期依赖问题\" class=\"headerlink\" title=\"RNN中的长期依赖问题\"></a>RNN中的长期依赖问题</h2><ul>\n<li>长期依赖问题：模型变深，失去了学习到先前信息的能力</li>\n<li>对权重矩阵做特征值分解分解，反复做线性变换，相当于矩阵幂运算，特征值也相应做幂运算，特征值量级大于1会爆炸，小于1会消失。梯度值严重偏离会导致梯度悬崖（学习到一个非常大的更新），如果已经爆炸，解决办法是使用梯度截断，使用计算出的梯度的方向但大小限制在一个小步长以内。</li>\n<li>最好是避免梯度爆炸。在循环网络中，隐藏层到隐藏层之间的变换没有引入非线性变换，即相当于对权重矩阵做幂运算，特征值会爆炸或者消失，相对应的长期相互作用的梯度值就会变得指数小。避免的办法包括引入时间维度的跳跃链接（添加长时间跨度的边）、引入渗漏单元（设置权重接近1的线性自连接单元）、删除短时间跨度的边（只保留长时间跨度的边）</li>\n</ul>\n<h2 id=\"门控RNN\"><a href=\"#门控RNN\" class=\"headerlink\" title=\"门控RNN\"></a>门控RNN</h2><p>-用类似渗漏单元的方法解决长期依赖问题，引入了门控RNN，包括LSTM和GRU。</p>\n<ul>\n<li>渗漏单元(?)：我们对某些 v 值应用更新 µ (t) ← αµ(t−1) + (1−α)v (t) 累积一个滑动平均值 µ (t)，其中 α 是一个从 µ (t−1) 到 µ (t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元µ可以模拟滑动平均的行为。这种隐藏单元称为渗漏单元。</li>\n</ul>\n<h1 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h1><ul>\n<li>LSTM：使自循环的权重视上下文而定（通过门控控制此循环的权重）</li>\n<li>LSTM将普通RNN中的隐藏层节点（细胞）改造，内部结构如下图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IvJx.png\" alt=\"i0IvJx.png\"></li>\n<li>可见除了RNN中细胞之间的循环之外，细胞内包含一个遗忘门控制（遗忘多少）的内循环。细胞有一个内部状态s，不同于不同时间步之间隐藏层更新用到的细胞输出h</li>\n<li>所有门控单元具有sigmoid非线性，输入单元是普通神经元，可以用任意非线性激活函数。</li>\n<li>三个门接受相同类型的输入，即当前输入x，前一时间步细胞输出（而不是细胞内部状态s）h，各自有独立的权重矩阵和偏置，输出都过一个sigmoid输出一个（0,1)之间的值，分别代表当前内部状态s对上一时间布内部状态的记忆程度、当前内部状态对当前输入的记忆程度、当前输出对当前细胞内部状态的依赖程度。</li>\n<li>内部状态更新s：根据两部分信息更新：由遗忘门控制的上一步内部状态，由输入门控制的输入和上一时间步细胞输出（未在图中画出？）之和。</li>\n<li>细胞输出h，内部状态过激活函数，由输出门控制。</li>\n<li>另一张更好理解的图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0IxW6.png\" alt=\"i0IxW6.png\"></li>\n</ul>\n<h1 id=\"双向LSTM\"><a href=\"#双向LSTM\" class=\"headerlink\" title=\"双向LSTM\"></a>双向LSTM</h1><ul>\n<li>同双向RNN，每一个隐藏层节点都是lstm节点，且双向的两个隐藏层节点之间没有连接，需要将两个隐藏层全部更新完才能计算输出层，每一时间步的输出依赖w1到w6共6个权重矩阵。</li>\n<li>因为每一个输出层节点接受两个隐藏层节点的输出，需要做一个处理，有多种方式：<ul>\n<li>直接连接(concat)    </li>\n<li>求和<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oSSK.png\" alt=\"i0oSSK.png\"></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"词嵌入、Word2Vec\"><a href=\"#词嵌入、Word2Vec\" class=\"headerlink\" title=\"词嵌入、Word2Vec\"></a>词嵌入、Word2Vec</h1><ul>\n<li>使用词的分布式表示（词嵌入或词向量）对自然语言序列建模，通过上下文-单词对(one-hot向量)的训练，得到神经网络，并将输入层到隐藏层的权重矩阵看成包含了词典中所有单词词向量，即词向量矩阵。此时再将单独的one-hot词向量通过神经网络，就可以借助权重矩阵（词向量矩阵）在隐藏层中得到这个词的低维词嵌入。因为词向量是通过上下文-单词对进行训练得到的副产品，因此这种词向量在空间上的距离具有实际意义，即语义上有联系的单词向量之间的距离较近。这种生成方法的一个问题是高维生成，因为神经网络中输出是词向量过softmax还原成一个one-hot向量，代表各个词的概率，词典容量非常大时会导致最后输出层计算量非常大。W2V是具有实用价值的产生词向量的方案，在使用NLM模型产生词向量的基础上进行优化，解决了高维问题，它利用了两套优化方案：</li>\n<li>分层softmax：输出不再是词典大小的概率向量，而是一棵树，叶子节点是单词，内部节点代表词的组别，用条件概率表示，使用逻辑回归模型。W2V利用这个模型，取消了隐藏层，直接将投影层输出到树中，并将树改进为哈夫曼树。因为取消了隐藏层，W2V避免了从投影层到隐藏层和从隐藏层还原到输出层中与词典大小成线性相关的大规模矩阵计算，但是因为树的叶子节点数依然和词典大小一样，最后归一化计算概率时依然开销很大。</li>\n<li>重要采样(?)：此方法通过减少反向传播时需要计算的梯度来减少计算量。每一次输出概率最高的词（正相项）对梯度应该贡献最大，其余概率低的负相项贡献应该低，因此不对所有的负相项计算梯度，而是采样一部分计算。</li>\n<li>没有完全理解计算softmax层的几种简化方式，待完善。推荐博文：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650720050&amp;idx=2&amp;sn=9fedc937d3128462c478ef7911e77687&amp;chksm=871b034cb06c8a5a8db8a10f708c81025fc62084d871ac5d184bab5098cb64e939c1c23a7369&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">技术 | 词嵌入系列博客Part2：比较语言建模中近似softmax的几种方法</a></li>\n</ul>\n<h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><ul>\n<li>在seq2seq模型中，编码端提供的信息全部压缩成一个中间表示，即编码器最后一个时间步的隐藏层状态输出，解码器只根据这个中间表示和上一次解码的词语进行解码，然而在编码端时间步很多的情况下，中间表示一般信息损失严重，为了解决这个问题引入注意力机制。</li>\n<li>注意力的实际表现是对编码端的各个时间步加权平均生成中间表示，而不是统一在循环的最后一步生成。权重大的编码端时间步即所谓的注意力所在点，给予解码端更多的信息贡献。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0I5WV.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"自然语言处理|深度学习入门","path":"2018/03/07/NLPBasic/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0I5WV.jpg","excerpt":"<p>记录入门NLP中seq2seq模型时学习到的一些深度学习基础知识。</p>","date":"2018-03-07T01:56:23.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"论文阅读笔记2019上半年","date":"2019-01-03T08:21:42.000Z","author":"Thinkwee","mathjax":true,"html":true,"_content":"又是咕咕咕的一年\n<!--more--> \n\n# Convolutional Sequence to Sequence Learning\n-\t非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习\n-\t无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。\n-\ttransformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。\n-\tCNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。\n-\tFacebook在本论文中采用的CNN block采用了普通的一维卷积，但是使用了gated linear unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的......这样做mask然后一步一步生成其实并没有充分利用CNN的加速。\n-\t在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于\n\t-\t采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。\n\t-\t注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。\n-\t作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation convolution设计。\n\n# A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings\n-\t完全无监督的跨语言词嵌入映射\n-\t跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移\n-\t一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系\n-\t这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译\n-\t之前的方法分三种：\n\t-\t有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical correlation analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。\n\t-\t半监督的，通过seed dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；\n\t-\t另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。\n-\t本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。\n-\t![APCWNT.png](https://s2.ax1x.com/2019/03/11/APCWNT.png)\n\n## 模型\n-\t令$X$和$Z$分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵$W_x$和$W_z$，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵\n-\t模型的迭代更新依赖一个对齐矩阵$D$，$D_{ij}=1$当且仅当A语言的第$i$个词对应着B语言的第$j$个词，该矩阵反映的对齐关系是单向的\n-\t模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果\n\n## 预处理\n-\t对词嵌入做长度归一化\n-\t再针对每一维做去均值\n-\t前两个预处理在作者之前的论文Learning principled bilingual mappings of word embeddings while preserving monolingual invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读\n-\t再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离\n\n## 完全无监督的初始化\n-\t初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐\n-\t本文的做法是，先构造两个矩阵$X^{'}$和$Z^{'}$，这两个矩阵的词嵌入每一维是对齐的\n-\t$X^{'}$和$Z^{'}$分别通过计算原词嵌入矩阵的相似矩阵开方得到，即$X^{'} = \\sqrt sorted{XX^T}$，$Z^{'} = \\sqrt sorted{ZZ^T}$。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布\n-\t![Ak0cYd.png](https://s2.ax1x.com/2019/03/13/Ak0cYd.png)\n-\t这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。\n-\t建立了词语的对齐，即建立了初始化的$D$矩阵\n\n## 鲁棒的自学习过程\n-\t计算正交映射以最大化当前$D$矩阵的相似度\n\t$$\n\targmax_{W_x,W_z} \\sum _i \\sum _j D_{ij}((X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z)) \\\\\n\t$$\n\t最优解可以直接计算得到：$W_X=U,W_Z=V$，其中$U,V$来自$USV^T$，是$X^TDZ$的奇异值分解\n-\t将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新$D$矩阵。\n\t$$\n\tD_{ij} = 1 \\ \\ \\ if  \\ \\ j = argmax _k (X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z) \\\\\n\telse \\ \\ D_{ij} = 0 \\\\\n\t$$\n-\t反复迭代，$W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z$\n-\t使用完全无监督的初始化$D$矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新$D$矩阵时提了几个小trick使得学习更为鲁棒\n\t-\t随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能\n\t-\t基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000\n\t-\tCSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新$D_{ij}$为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语\n\t-\t双向词典归纳，不仅针对i找j，也针对j找i\n\t-\t这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000\n\n## 通过对称权重重分配进一步改善结果\n-\t即迭代完成之后计算\n\t$$\n\tW_X = US^{\\frac 12} \\\\\n\tW_Z = UV^{\\frac 12} \\\\\n\t$$\n-\t比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感\n-\t重新分配权重的原因在之前的论文中提到，待阅读","source":"_posts/PaperReading3.md","raw":"---\ntitle: 论文阅读笔记2019上半年\ndate: 2019-01-03 16:21:42\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\nhtml: true\n---\n又是咕咕咕的一年\n<!--more--> \n\n# Convolutional Sequence to Sequence Learning\n-\t非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习\n-\t无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。\n-\ttransformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。\n-\tCNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。\n-\tFacebook在本论文中采用的CNN block采用了普通的一维卷积，但是使用了gated linear unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的......这样做mask然后一步一步生成其实并没有充分利用CNN的加速。\n-\t在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于\n\t-\t采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。\n\t-\t注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。\n-\t作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation convolution设计。\n\n# A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings\n-\t完全无监督的跨语言词嵌入映射\n-\t跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移\n-\t一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系\n-\t这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译\n-\t之前的方法分三种：\n\t-\t有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical correlation analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。\n\t-\t半监督的，通过seed dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；\n\t-\t另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。\n-\t本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。\n-\t![APCWNT.png](https://s2.ax1x.com/2019/03/11/APCWNT.png)\n\n## 模型\n-\t令$X$和$Z$分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵$W_x$和$W_z$，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵\n-\t模型的迭代更新依赖一个对齐矩阵$D$，$D_{ij}=1$当且仅当A语言的第$i$个词对应着B语言的第$j$个词，该矩阵反映的对齐关系是单向的\n-\t模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果\n\n## 预处理\n-\t对词嵌入做长度归一化\n-\t再针对每一维做去均值\n-\t前两个预处理在作者之前的论文Learning principled bilingual mappings of word embeddings while preserving monolingual invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读\n-\t再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离\n\n## 完全无监督的初始化\n-\t初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐\n-\t本文的做法是，先构造两个矩阵$X^{'}$和$Z^{'}$，这两个矩阵的词嵌入每一维是对齐的\n-\t$X^{'}$和$Z^{'}$分别通过计算原词嵌入矩阵的相似矩阵开方得到，即$X^{'} = \\sqrt sorted{XX^T}$，$Z^{'} = \\sqrt sorted{ZZ^T}$。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布\n-\t![Ak0cYd.png](https://s2.ax1x.com/2019/03/13/Ak0cYd.png)\n-\t这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。\n-\t建立了词语的对齐，即建立了初始化的$D$矩阵\n\n## 鲁棒的自学习过程\n-\t计算正交映射以最大化当前$D$矩阵的相似度\n\t$$\n\targmax_{W_x,W_z} \\sum _i \\sum _j D_{ij}((X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z)) \\\\\n\t$$\n\t最优解可以直接计算得到：$W_X=U,W_Z=V$，其中$U,V$来自$USV^T$，是$X^TDZ$的奇异值分解\n-\t将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新$D$矩阵。\n\t$$\n\tD_{ij} = 1 \\ \\ \\ if  \\ \\ j = argmax _k (X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z) \\\\\n\telse \\ \\ D_{ij} = 0 \\\\\n\t$$\n-\t反复迭代，$W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z$\n-\t使用完全无监督的初始化$D$矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新$D$矩阵时提了几个小trick使得学习更为鲁棒\n\t-\t随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能\n\t-\t基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000\n\t-\tCSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新$D_{ij}$为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语\n\t-\t双向词典归纳，不仅针对i找j，也针对j找i\n\t-\t这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000\n\n## 通过对称权重重分配进一步改善结果\n-\t即迭代完成之后计算\n\t$$\n\tW_X = US^{\\frac 12} \\\\\n\tW_Z = UV^{\\frac 12} \\\\\n\t$$\n-\t比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感\n-\t重新分配权重的原因在之前的论文中提到，待阅读","slug":"PaperReading3","published":1,"updated":"2019-07-22T03:45:22.909Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v2s001cq8t5zvi5w6k2","content":"<p>又是咕咕咕的一年<br><a id=\"more\"></a> </p>\n<h1 id=\"Convolutional-Sequence-to-Sequence-Learning\"><a href=\"#Convolutional-Sequence-to-Sequence-Learning\" class=\"headerlink\" title=\"Convolutional Sequence to Sequence Learning\"></a>Convolutional Sequence to Sequence Learning</h1><ul>\n<li>非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习</li>\n<li>无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。</li>\n<li>transformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。</li>\n<li>CNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。</li>\n<li>Facebook在本论文中采用的CNN block采用了普通的一维卷积，但是使用了gated linear unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的……这样做mask然后一步一步生成其实并没有充分利用CNN的加速。</li>\n<li>在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于<ul>\n<li>采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。</li>\n<li>注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。</li>\n</ul>\n</li>\n<li>作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation convolution设计。</li>\n</ul>\n<h1 id=\"A-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings\"><a href=\"#A-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings\" class=\"headerlink\" title=\"A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings\"></a>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</h1><ul>\n<li>完全无监督的跨语言词嵌入映射</li>\n<li>跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移</li>\n<li>一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系</li>\n<li>这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译</li>\n<li>之前的方法分三种：<ul>\n<li>有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical correlation analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。</li>\n<li>半监督的，通过seed dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；</li>\n<li>另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。</li>\n</ul>\n</li>\n<li>本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。</li>\n<li><img src=\"https://s2.ax1x.com/2019/03/11/APCWNT.png\" alt=\"APCWNT.png\"></li>\n</ul>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><ul>\n<li>令$X$和$Z$分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵$W_x$和$W_z$，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵</li>\n<li>模型的迭代更新依赖一个对齐矩阵$D$，$D_{ij}=1$当且仅当A语言的第$i$个词对应着B语言的第$j$个词，该矩阵反映的对齐关系是单向的</li>\n<li>模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果</li>\n</ul>\n<h2 id=\"预处理\"><a href=\"#预处理\" class=\"headerlink\" title=\"预处理\"></a>预处理</h2><ul>\n<li>对词嵌入做长度归一化</li>\n<li>再针对每一维做去均值</li>\n<li>前两个预处理在作者之前的论文Learning principled bilingual mappings of word embeddings while preserving monolingual invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读</li>\n<li>再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离</li>\n</ul>\n<h2 id=\"完全无监督的初始化\"><a href=\"#完全无监督的初始化\" class=\"headerlink\" title=\"完全无监督的初始化\"></a>完全无监督的初始化</h2><ul>\n<li>初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐</li>\n<li>本文的做法是，先构造两个矩阵$X^{‘}$和$Z^{‘}$，这两个矩阵的词嵌入每一维是对齐的</li>\n<li>$X^{‘}$和$Z^{‘}$分别通过计算原词嵌入矩阵的相似矩阵开方得到，即$X^{‘} = \\sqrt sorted{XX^T}$，$Z^{‘} = \\sqrt sorted{ZZ^T}$。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布</li>\n<li><img src=\"https://s2.ax1x.com/2019/03/13/Ak0cYd.png\" alt=\"Ak0cYd.png\"></li>\n<li>这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。</li>\n<li>建立了词语的对齐，即建立了初始化的$D$矩阵</li>\n</ul>\n<h2 id=\"鲁棒的自学习过程\"><a href=\"#鲁棒的自学习过程\" class=\"headerlink\" title=\"鲁棒的自学习过程\"></a>鲁棒的自学习过程</h2><ul>\n<li>计算正交映射以最大化当前$D$矩阵的相似度<script type=\"math/tex; mode=display\">\nargmax_{W_x,W_z} \\sum _i \\sum _j D_{ij}((X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z)) \\\\</script>最优解可以直接计算得到：$W_X=U,W_Z=V$，其中$U,V$来自$USV^T$，是$X^TDZ$的奇异值分解</li>\n<li>将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新$D$矩阵。<script type=\"math/tex; mode=display\">\nD_{ij} = 1 \\ \\ \\ if  \\ \\ j = argmax _k (X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z) \\\\\nelse \\ \\ D_{ij} = 0 \\\\</script></li>\n<li>反复迭代，$W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z$</li>\n<li>使用完全无监督的初始化$D$矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新$D$矩阵时提了几个小trick使得学习更为鲁棒<ul>\n<li>随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能</li>\n<li>基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000</li>\n<li>CSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新$D_{ij}$为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语</li>\n<li>双向词典归纳，不仅针对i找j，也针对j找i</li>\n<li>这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"通过对称权重重分配进一步改善结果\"><a href=\"#通过对称权重重分配进一步改善结果\" class=\"headerlink\" title=\"通过对称权重重分配进一步改善结果\"></a>通过对称权重重分配进一步改善结果</h2><ul>\n<li>即迭代完成之后计算<script type=\"math/tex; mode=display\">\nW_X = US^{\\frac 12} \\\\\nW_Z = UV^{\\frac 12} \\\\</script></li>\n<li>比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感</li>\n<li>重新分配权重的原因在之前的论文中提到，待阅读</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>又是咕咕咕的一年<br></p>","more":"<p></p>\n<h1 id=\"Convolutional-Sequence-to-Sequence-Learning\"><a href=\"#Convolutional-Sequence-to-Sequence-Learning\" class=\"headerlink\" title=\"Convolutional Sequence to Sequence Learning\"></a>Convolutional Sequence to Sequence Learning</h1><ul>\n<li>非常直白，编码器和解码器都使用卷积神经网络的序列到序列学习</li>\n<li>无论是transformer还是CNN作为encoder，都需要捕获整个句子的语义信息。就目前两者大幅领先RNN的现状开来，相比序列结构，树型结构更适合作为自然语言数据的先验结构。</li>\n<li>transformer直接在第一层建模所谓的自注意力，我个人觉得这个自注意力是在建模句子的parse关系，针对所有剖析对（一个词到本句所有其他词）、所有维度（可能是成分剖析、可能是实体关系、可能是共指消解、可能是依存剖析）进行建模，利用注意力的机制对无用的关系筛选，之后再过一层全连接层进行重组。这样剖析+重组的block迭代多层，逐步抽象特征，再加上batch normalization和residual这些深层网络设计常用的结构，就构成了transformer。因此transformer是一次性构建全局剖析关系，再逐步重组、抽象、筛选。</li>\n<li>CNN的结构则更符合常规句法剖析的套路，依然是针对了各个维度建模，但是不是一次性构建全局关系，而是在底层先剖析局部关系（kernal size大小的ngram)，然后通过叠加层对局部关系进行汇总、抽象。</li>\n<li>Facebook在本论文中采用的CNN block采用了普通的一维卷积，但是使用了gated linear unit，即多卷积出一倍的channel来作为门结构输入，利用门结构过滤信息、构建非线性关系，类似LSTM的门控设计，同时也起到了类似自注意力的效果，放弃了pooling的设计。而在decoder端，也使用了CNN（我感觉其实没有很大必要），decoder依然是一个从左往右逐字顺序生成的过程。为了保证这种顺序关系，对decoder的输入做了mask，而我现在还没弄懂具体代码是怎么实现的……这样做mask然后一步一步生成其实并没有充分利用CNN的加速。</li>\n<li>在本文中也引入了注意力，是传统的encoder-decoder间注意力，不同的地方在于<ul>\n<li>采用了多层注意力，虽然key依然是encoder最后一层的输出，但是对于decoder每一层都单独引入了注意力。然而作者自己也说decoder不需要太多层，两层足以，因此这个多层注意力可能也没充分利用。况且多层代表decode出每一个词所需要的上下文更多，看来CNN作为decoder并不需要太多上下文，或者说没有充分利用上比较长的上下文。</li>\n<li>注意力的value不是和key一样，而是encoder最后一层的输出加上encoder输入的embedding，作者认为这样可以综合考虑具体和抽象的表示，实际效果也确实要好一些。</li>\n</ul>\n</li>\n<li>作者提到了bytenet作为参考，但是不知道为啥并没有采用bytenet中的dilation convolution设计。</li>\n</ul>\n<h1 id=\"A-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings\"><a href=\"#A-robust-self-learning-method-for-fully-unsupervised-cross-lingual-mappings-of-word-embeddings\" class=\"headerlink\" title=\"A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings\"></a>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</h1><ul>\n<li>完全无监督的跨语言词嵌入映射</li>\n<li>跨语言词嵌入，即多种语言共用相同的词嵌入矩阵，这样可以将大规模预训练词嵌入和或者语言模型进行跨语言的模型迁移</li>\n<li>一般的做法是利用两个语言的词嵌入矩阵，映射到同一个跨语言的词嵌入空间，并且建立两种语言的词对应关系</li>\n<li>这类研究最近很火，其催生的最知名的一个下游应用应该就是Facebook18年的无监督机器翻译</li>\n<li>之前的方法分三种：<ul>\n<li>有监督的，使用双语词典，构建几千个监督词对，将学习映射看成回归问题：用最小均方目标函数建模，之后催生了各种方法：canonical correlation analysis；正交方法；最大间隔方法。这些方法都可以归为将两类语言的词嵌入矩阵做线性变化映射到同一个空间。</li>\n<li>半监督的，通过seed dictionary和bootstrap来做，这类方法依赖好的seed且容易陷入局部最优；</li>\n<li>另一类是无监督的生成式方法，但是已有的方法太过依赖特定任务，泛化能力不佳，针对语言系统不同的两种语言很难达到很好效果。</li>\n</ul>\n</li>\n<li>本文采用无监督的生成式方法，基于一个观察：在一种语言中，每个词都有一个在这种语言词典上的相似度分布，不同语言中等价的词应该具有相似的相似度分布。基于这种观察，本文建立了初始的seed dictionary，并采用一种更鲁棒的self learning方式来改进学习到的映射。</li>\n<li><img src=\"https://s2.ax1x.com/2019/03/11/APCWNT.png\" alt=\"APCWNT.png\"></li>\n</ul>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><ul>\n<li>令$X$和$Z$分别为两种语言的词嵌入矩阵，目标是学习到线性变换矩阵$W_x$和$W_z$，使得映射后两种语言的矩阵在同一个跨语言空间,形成新的跨语言词嵌入矩阵</li>\n<li>模型的迭代更新依赖一个对齐矩阵$D$，$D_{ij}=1$当且仅当A语言的第$i$个词对应着B语言的第$j$个词，该矩阵反映的对齐关系是单向的</li>\n<li>模型分四步：预处理、完全无监督的初始化、一种鲁棒的自学习过程、通过对称权重重分配进一步改善结果</li>\n</ul>\n<h2 id=\"预处理\"><a href=\"#预处理\" class=\"headerlink\" title=\"预处理\"></a>预处理</h2><ul>\n<li>对词嵌入做长度归一化</li>\n<li>再针对每一维做去均值</li>\n<li>前两个预处理在作者之前的论文Learning principled bilingual mappings of word embeddings while preserving monolingual invariance中提到过，目的分别是简化问题为求余弦相似度和求最大协方差。这篇论文讲的是有监督方法，待阅读</li>\n<li>再做一次长度归一化，保证每一个词嵌入都拥有单位长度，使得两个词嵌入的内积等价于cos距离</li>\n</ul>\n<h2 id=\"完全无监督的初始化\"><a href=\"#完全无监督的初始化\" class=\"headerlink\" title=\"完全无监督的初始化\"></a>完全无监督的初始化</h2><ul>\n<li>初始化很难做，因为两种语言的词嵌入矩阵，两个维度（每个词、嵌入的每一维）都不对齐</li>\n<li>本文的做法是，先构造两个矩阵$X^{‘}$和$Z^{‘}$，这两个矩阵的词嵌入每一维是对齐的</li>\n<li>$X^{‘}$和$Z^{‘}$分别通过计算原词嵌入矩阵的相似矩阵开方得到，即$X^{‘} = \\sqrt sorted{XX^T}$，$Z^{‘} = \\sqrt sorted{ZZ^T}$。自己乘自己的转置即同一语言下的相似度矩阵（因为之前做了预处理）。根据之前的观察，表示同一词义的两种语言下的两个词，应该有相似的单语言相似度分布，那么我们将两种语言的相似度矩阵的每一行单独排序，从大到小排，则假如两个词有相同词义，他们在自己语言中的已排序相似度矩阵中的对应行，应该有相似的分布</li>\n<li><img src=\"https://s2.ax1x.com/2019/03/13/Ak0cYd.png\" alt=\"Ak0cYd.png\"></li>\n<li>这样就跳过了词嵌入每个维度上的直接对齐，转换为词典相似度的对齐，之后只要做词的对齐，即对相似度矩阵每一行单独排序，建立行分布相似的词之间的对应关系即可。</li>\n<li>建立了词语的对齐，即建立了初始化的$D$矩阵</li>\n</ul>\n<h2 id=\"鲁棒的自学习过程\"><a href=\"#鲁棒的自学习过程\" class=\"headerlink\" title=\"鲁棒的自学习过程\"></a>鲁棒的自学习过程</h2><ul>\n<li>计算正交映射以最大化当前$D$矩阵的相似度<script type=\"math/tex; mode=display\">\nargmax_{W_x,W_z} \\sum _i \\sum _j D_{ij}((X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z)) \\\\</script>最优解可以直接计算得到：$W_X=U,W_Z=V$，其中$U,V$来自$USV^T$，是$X^TDZ$的奇异值分解</li>\n<li>将两个语言的词嵌入映射到跨语言词嵌入空间（分别映射，依然是两个词嵌入矩阵，只不过在同一个跨语言空间内）后，对A语言的每一个词，在跨语言词嵌入空间内找其最近的B语言的词，建立映射关系，更新$D$矩阵。<script type=\"math/tex; mode=display\">\nD_{ij} = 1 \\ \\ \\ if  \\ \\ j = argmax _k (X_{i\\*}W_X) \\cdot (Z_{j\\*}W_Z) \\\\\nelse \\ \\ D_{ij} = 0 \\\\</script></li>\n<li>反复迭代，$W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z \\rightarrow D \\rightarrow W_X,W_Z$</li>\n<li>使用完全无监督的初始化$D$矩阵比随机初始化效果要好，但是依然会陷入局部最优，因此作者针对迭代的第二步，即更新$D$矩阵时提了几个小trick使得学习更为鲁棒<ul>\n<li>随机词典归纳：每次迭代以一定概率将D矩阵模型元素设为0，迫使模型探索更多可能</li>\n<li>基于词频的词典截断：每次词典归纳时只更新前k个最频繁的词，避免低频词带来的噪音，截断上限为20000</li>\n<li>CSLS检索：之前的方法是对每一个i,找一个映射到跨语言词嵌入空间后距离最相近的j，更新$D_{ij}$为1，这种最近邻方法受维度灾难影响，效果并不好（具体引起的现象叫hubs，即词语发生聚类，hubs词是许多词的最近邻，差异度不大）。CSLS即跨领域相似度局部放缩，惩罚了这些hubs词语</li>\n<li>双向词典归纳，不仅针对i找j，也针对j找i</li>\n<li>这些trick对初始化构建的矩阵有所差别，不做随机归纳，词典截断上限为4000</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"通过对称权重重分配进一步改善结果\"><a href=\"#通过对称权重重分配进一步改善结果\" class=\"headerlink\" title=\"通过对称权重重分配进一步改善结果\"></a>通过对称权重重分配进一步改善结果</h2><ul>\n<li>即迭代完成之后计算<script type=\"math/tex; mode=display\">\nW_X = US^{\\frac 12} \\\\\nW_Z = UV^{\\frac 12} \\\\</script></li>\n<li>比起之前的论文，在每一次迭代前后做白化和去白化的方法，这种方法鼓励模型探索更多搜索空间，且对方向不敏感</li>\n<li>重新分配权重的原因在之前的论文中提到，待阅读</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s2.ax1x.com/2019/03/11/APCWNT.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"论文阅读笔记2019上半年","path":"2019/01/03/PaperReading3/","eyeCatchImage":"https://s2.ax1x.com/2019/03/11/APCWNT.png","excerpt":"<p>又是咕咕咕的一年<br></p>","date":"2019-01-03T08:21:42.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"论文阅读笔记2018下半年","date":"2018-07-03T07:18:52.000Z","author":"Thinkwee","mathjax":true,"html":true,"_content":"读论文是不可能读完的，这辈子都不可能读完的。\n没错，基本上没读论文，读了也来不及写笔记了。\n\n<!--more--> \n\n![i0o47d.jpg](https://s1.ax1x.com/2018/10/20/i0o47d.jpg)\n\n# Distraction-Based Neural Networks for Document Summarization\n-\t不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。\n![i0oh0H.png](https://s1.ax1x.com/2018/10/20/i0oh0H.png)\n-\t在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：\n\t$$\n\ts_t = GRU _1 (s_t^{temp},c_t) \\\\\n\ts_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\\\\n\t$$\n-\t这个控制层捕捉$s_t^{'}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。\n-\t三种注意力分散模型\n\t-\tM1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制\n\t$$\n\tc_t = tanh (W_c c_t^{temp} - U_c \\sum _{j=1}^{t-1} c_j) \\\\\n\tc_t^{temp} = \\sum _{i=1}^{T_x} \\alpha _{t,i} h_i \\\\\n\t$$\n\t-\tM2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化\n\t$$\n\t\\alpha _{t,i}^{temp} = v_{\\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a \\sum _{j=1}^{t-1}\\alpha _{j,i}) \\\\\n\t\\alpha _{t,i} = \\frac {exp(\\alpha _{t,i}^{temp})}{\\sum _{j=1}^{T_x} exp(\\alpha _{t,j}^{temp})} \\\\\n\t$$\n\t-\tM3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。\n\t$$\n\td_{\\alpha , t} = \\min KL(\\alpha _t , \\alpha _i) \\\\\n\td_{c , t} = \\max cosine(c _t , c _i) \\\\\n\td_{s , t} = \\max cosine(s _t , s _i) \\\\\n\t$$\n\n# Document Modeling with External Attention for Sentence Extraction\n-\t构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。\n在文摘任务中，外部信息是图片配字和文档标题。\n-\t通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。\n\n![i0oLjS.png](https://s1.ax1x.com/2018/10/20/i0oLjS.png)\n\n-\t句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。\n-\t文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。\n-\t句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{'}$：\n\n![i0oIAA.png](https://s1.ax1x.com/2018/10/20/i0oIAA.png)\n\n# Get To The Point: Summarization with Pointer-Generator Networks\n-\t介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题\n-\tPointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率\n\t$$\n\tp_{gen} = \\sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\\\\n\tP(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \\sum _{i:w_i = w} a_i^t \\\\\n\t$$\n-\t指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。\n\n![i0ootI.png](https://s1.ax1x.com/2018/10/20/i0ootI.png)\n\n-\tCoverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力\n-\t普通注意力计算\n\n\t$$\n\te_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\\\\n\ta^t = softmax(e^t) \\\\\n\t$$\n\n-\t维护一个coverage向量，表示每个词在此之前获得了多少注意力:\n\n\t$$\n\tc^t = \\sum _{t^{temp} = 0}^t-1 a^{t^{temp}}\n\t$$\n\n-\t然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积\n\n\t$$\n\te_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})\n\t$$\n\n-\t并在损失函数里加上coverage损失\n\n\t$$\n\tcovloss_t = \\sum _i \\min (a_i^t , c_i^t)\n\t$$\n\n-\t使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小\n\n# SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\n\n![i0oTht.png](https://s1.ax1x.com/2018/10/20/i0oTht.png)\n\n-\t用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法\n-\t将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。\n-\t用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling\n\n\t$$\n\td = tanh(W_d \\frac {1}{N_d} \\sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)\n\t$$\n\n-\t其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码\n-\t之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：\n\n![i0ob1f.png](https://s1.ax1x.com/2018/10/20/i0ob1f.png)\n\n-\t其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：\n\n\t$$\n\ts_j = \\sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)\n\t$$\n\n-\t第一行：参数为当前句子编码，表示当前句子的内容\n-\t第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性\n-\t第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）\n-\t第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）\n-\t最后对整个模型做最大似然估计:\n\n\t$$\n\tl(W,b) = -\\sum _{d=1}^N \\sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 | h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))\n\t$$\n\n-\t作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。\n-\t还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。\n-\t因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：\n\n![i0oqc8.png](https://s1.ax1x.com/2018/10/20/i0oqc8.png)\n\n# Attention Is All You Need\n-\t抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络\n\n![i0oXng.png](https://s1.ax1x.com/2018/10/20/i0oXng.png)\n\n-\t完全使用attention的一个好处就是快。\n-\t为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512\n-\t编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。\n-\t解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。\n-\t编码与解码的6个块都是堆叠的(stack)，\n-\tAttention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。\n\n![i0TSNn.png](https://s1.ax1x.com/2018/10/20/i0TSNn.png)\n\n-\tMulti-head attention由多个scaled dot-product attention并行组成。\n-\tScaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。\n\n\t$$\n\tAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt {d_k}}) V\n\t$$\n\n-\tMulti-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。\n\n\t$$\n\tMultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\\\\n\twhere \\ \\  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\\\\n\t$$\n\n-\t论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64\n-\t这种multi-head attention用在了模型的三个地方：\n\t-\t编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置\n\t-\t编码端块与块之间的自注意力\n\t-\t解码端块与块之间的自注意力\n-\t在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同\n\n\t$$\n\tFFN(x) =\\max (0,xW_1+b_1)W_2 +b_2 \\\\\n\t$$\n\n-\t完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：\n\n\t$$\n\tPE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\\\\n\tPE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\\\\n\t$$\n\n# A Joint Selective Mechanism for Abstractive Sentence Summarization\n\n![ivQCE8.png](https://s1.ax1x.com/2018/11/15/ivQCE8.png)\n\n-\t文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模\n-\t作者针对损失建模，做了两点改进：\n\t-\t在编码完成之后添加了一个门限用于裁剪编码信息\n\t-\t添加了一个选择损失，同时关注输入和输出，辅助门限工作\n-\t选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：\n$$\ng_i = \\sigma (W_g h_i + U_g u_i) \\\\\nh_i^{'} = h_i \\cdot g_i \\\\\n$$\n-\t而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：\n$$\nr_{i,t} = \\sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\\\\nr_i = \\frac 1m \\sum _{t=2}^{m+1} r_{i,t} \\\\\n$$\n-\t作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。\n-\t之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:\n$$\nd(g,r) = \\frac 1n \\sum _{i=1}^n |r_i - g_i | \\\\\nL = -p(y|x,\\theta) + \\lambda d(g,r) \\\\\n$$\n-\t作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective Encoding for Abstractive Sentence Summarization 也没有说出这种设计的动机。","source":"_posts/PaperReading2.md","raw":"---\ntitle: 论文阅读笔记2018下半年\ndate: 2018-07-03 15:18:52\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\nhtml: true\n---\n读论文是不可能读完的，这辈子都不可能读完的。\n没错，基本上没读论文，读了也来不及写笔记了。\n\n<!--more--> \n\n![i0o47d.jpg](https://s1.ax1x.com/2018/10/20/i0o47d.jpg)\n\n# Distraction-Based Neural Networks for Document Summarization\n-\t不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。\n![i0oh0H.png](https://s1.ax1x.com/2018/10/20/i0oh0H.png)\n-\t在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：\n\t$$\n\ts_t = GRU _1 (s_t^{temp},c_t) \\\\\n\ts_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\\\\n\t$$\n-\t这个控制层捕捉$s_t^{'}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。\n-\t三种注意力分散模型\n\t-\tM1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制\n\t$$\n\tc_t = tanh (W_c c_t^{temp} - U_c \\sum _{j=1}^{t-1} c_j) \\\\\n\tc_t^{temp} = \\sum _{i=1}^{T_x} \\alpha _{t,i} h_i \\\\\n\t$$\n\t-\tM2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化\n\t$$\n\t\\alpha _{t,i}^{temp} = v_{\\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a \\sum _{j=1}^{t-1}\\alpha _{j,i}) \\\\\n\t\\alpha _{t,i} = \\frac {exp(\\alpha _{t,i}^{temp})}{\\sum _{j=1}^{T_x} exp(\\alpha _{t,j}^{temp})} \\\\\n\t$$\n\t-\tM3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。\n\t$$\n\td_{\\alpha , t} = \\min KL(\\alpha _t , \\alpha _i) \\\\\n\td_{c , t} = \\max cosine(c _t , c _i) \\\\\n\td_{s , t} = \\max cosine(s _t , s _i) \\\\\n\t$$\n\n# Document Modeling with External Attention for Sentence Extraction\n-\t构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。\n在文摘任务中，外部信息是图片配字和文档标题。\n-\t通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。\n\n![i0oLjS.png](https://s1.ax1x.com/2018/10/20/i0oLjS.png)\n\n-\t句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。\n-\t文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。\n-\t句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{'}$：\n\n![i0oIAA.png](https://s1.ax1x.com/2018/10/20/i0oIAA.png)\n\n# Get To The Point: Summarization with Pointer-Generator Networks\n-\t介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题\n-\tPointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率\n\t$$\n\tp_{gen} = \\sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\\\\n\tP(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \\sum _{i:w_i = w} a_i^t \\\\\n\t$$\n-\t指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。\n\n![i0ootI.png](https://s1.ax1x.com/2018/10/20/i0ootI.png)\n\n-\tCoverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力\n-\t普通注意力计算\n\n\t$$\n\te_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\\\\n\ta^t = softmax(e^t) \\\\\n\t$$\n\n-\t维护一个coverage向量，表示每个词在此之前获得了多少注意力:\n\n\t$$\n\tc^t = \\sum _{t^{temp} = 0}^t-1 a^{t^{temp}}\n\t$$\n\n-\t然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积\n\n\t$$\n\te_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})\n\t$$\n\n-\t并在损失函数里加上coverage损失\n\n\t$$\n\tcovloss_t = \\sum _i \\min (a_i^t , c_i^t)\n\t$$\n\n-\t使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小\n\n# SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\n\n![i0oTht.png](https://s1.ax1x.com/2018/10/20/i0oTht.png)\n\n-\t用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法\n-\t将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。\n-\t用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling\n\n\t$$\n\td = tanh(W_d \\frac {1}{N_d} \\sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)\n\t$$\n\n-\t其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码\n-\t之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：\n\n![i0ob1f.png](https://s1.ax1x.com/2018/10/20/i0ob1f.png)\n\n-\t其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：\n\n\t$$\n\ts_j = \\sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)\n\t$$\n\n-\t第一行：参数为当前句子编码，表示当前句子的内容\n-\t第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性\n-\t第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）\n-\t第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）\n-\t最后对整个模型做最大似然估计:\n\n\t$$\n\tl(W,b) = -\\sum _{d=1}^N \\sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 | h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))\n\t$$\n\n-\t作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。\n-\t还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。\n-\t因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：\n\n![i0oqc8.png](https://s1.ax1x.com/2018/10/20/i0oqc8.png)\n\n# Attention Is All You Need\n-\t抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络\n\n![i0oXng.png](https://s1.ax1x.com/2018/10/20/i0oXng.png)\n\n-\t完全使用attention的一个好处就是快。\n-\t为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512\n-\t编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。\n-\t解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。\n-\t编码与解码的6个块都是堆叠的(stack)，\n-\tAttention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。\n\n![i0TSNn.png](https://s1.ax1x.com/2018/10/20/i0TSNn.png)\n\n-\tMulti-head attention由多个scaled dot-product attention并行组成。\n-\tScaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。\n\n\t$$\n\tAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt {d_k}}) V\n\t$$\n\n-\tMulti-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。\n\n\t$$\n\tMultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\\\\n\twhere \\ \\  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\\\\n\t$$\n\n-\t论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64\n-\t这种multi-head attention用在了模型的三个地方：\n\t-\t编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置\n\t-\t编码端块与块之间的自注意力\n\t-\t解码端块与块之间的自注意力\n-\t在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同\n\n\t$$\n\tFFN(x) =\\max (0,xW_1+b_1)W_2 +b_2 \\\\\n\t$$\n\n-\t完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：\n\n\t$$\n\tPE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\\\\n\tPE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\\\\n\t$$\n\n# A Joint Selective Mechanism for Abstractive Sentence Summarization\n\n![ivQCE8.png](https://s1.ax1x.com/2018/11/15/ivQCE8.png)\n\n-\t文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模\n-\t作者针对损失建模，做了两点改进：\n\t-\t在编码完成之后添加了一个门限用于裁剪编码信息\n\t-\t添加了一个选择损失，同时关注输入和输出，辅助门限工作\n-\t选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：\n$$\ng_i = \\sigma (W_g h_i + U_g u_i) \\\\\nh_i^{'} = h_i \\cdot g_i \\\\\n$$\n-\t而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：\n$$\nr_{i,t} = \\sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\\\\nr_i = \\frac 1m \\sum _{t=2}^{m+1} r_{i,t} \\\\\n$$\n-\t作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。\n-\t之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:\n$$\nd(g,r) = \\frac 1n \\sum _{i=1}^n |r_i - g_i | \\\\\nL = -p(y|x,\\theta) + \\lambda d(g,r) \\\\\n$$\n-\t作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective Encoding for Abstractive Sentence Summarization 也没有说出这种设计的动机。","slug":"PaperReading2","published":1,"updated":"2019-07-22T03:45:22.889Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v31001fq8t57pn8lhho","content":"<p>读论文是不可能读完的，这辈子都不可能读完的。<br>没错，基本上没读论文，读了也来不及写笔记了。</p>\n<a id=\"more\"></a> \n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o47d.jpg\" alt=\"i0o47d.jpg\"></p>\n<h1 id=\"Distraction-Based-Neural-Networks-for-Document-Summarization\"><a href=\"#Distraction-Based-Neural-Networks-for-Document-Summarization\" class=\"headerlink\" title=\"Distraction-Based Neural Networks for Document Summarization\"></a>Distraction-Based Neural Networks for Document Summarization</h1><ul>\n<li>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oh0H.png\" alt=\"i0oh0H.png\"></li>\n<li>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：<script type=\"math/tex; mode=display\">\ns_t = GRU _1 (s_t^{temp},c_t) \\\\\ns_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\\\</script></li>\n<li>这个控制层捕捉$s_t^{‘}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。</li>\n<li>三种注意力分散模型<ul>\n<li>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制<script type=\"math/tex; mode=display\">\nc_t = tanh (W_c c_t^{temp} - U_c \\sum _{j=1}^{t-1} c_j) \\\\\nc_t^{temp} = \\sum _{i=1}^{T_x} \\alpha _{t,i} h_i \\\\</script></li>\n<li>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化<script type=\"math/tex; mode=display\">\n\\alpha _{t,i}^{temp} = v_{\\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a \\sum _{j=1}^{t-1}\\alpha _{j,i}) \\\\\n\\alpha _{t,i} = \\frac {exp(\\alpha _{t,i}^{temp})}{\\sum _{j=1}^{T_x} exp(\\alpha _{t,j}^{temp})} \\\\</script></li>\n<li>M3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。<script type=\"math/tex; mode=display\">\nd_{\\alpha , t} = \\min KL(\\alpha _t , \\alpha _i) \\\\\nd_{c , t} = \\max cosine(c _t , c _i) \\\\\nd_{s , t} = \\max cosine(s _t , s _i) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Document-Modeling-with-External-Attention-for-Sentence-Extraction\"><a href=\"#Document-Modeling-with-External-Attention-for-Sentence-Extraction\" class=\"headerlink\" title=\"Document Modeling with External Attention for Sentence Extraction\"></a>Document Modeling with External Attention for Sentence Extraction</h1><ul>\n<li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。<br>在文摘任务中，外部信息是图片配字和文档标题。</li>\n<li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oLjS.png\" alt=\"i0oLjS.png\"></p>\n<ul>\n<li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li>\n<li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li>\n<li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{‘}$：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oIAA.png\" alt=\"i0oIAA.png\"></p>\n<h1 id=\"Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\"><a href=\"#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\" class=\"headerlink\" title=\"Get To The Point: Summarization with Pointer-Generator Networks\"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><ul>\n<li>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</li>\n<li>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率<script type=\"math/tex; mode=display\">\np_{gen} = \\sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\\\\nP(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \\sum _{i:w_i = w} a_i^t \\\\</script></li>\n<li>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0ootI.png\" alt=\"i0ootI.png\"></p>\n<ul>\n<li>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</li>\n<li><p>普通注意力计算</p>\n<script type=\"math/tex; mode=display\">\ne_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\\\\na^t = softmax(e^t) \\\\</script></li>\n<li><p>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</p>\n<script type=\"math/tex; mode=display\">\nc^t = \\sum _{t^{temp} = 0}^t-1 a^{t^{temp}}</script></li>\n<li><p>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</p>\n<script type=\"math/tex; mode=display\">\ne_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})</script></li>\n<li><p>并在损失函数里加上coverage损失</p>\n<script type=\"math/tex; mode=display\">\ncovloss_t = \\sum _i \\min (a_i^t , c_i^t)</script></li>\n<li><p>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</p>\n</li>\n</ul>\n<h1 id=\"SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\"><a href=\"#SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\" class=\"headerlink\" title=\"SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\"></a>SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0oTht.png\" alt=\"i0oTht.png\"></p>\n<ul>\n<li>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</li>\n<li>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</li>\n<li><p>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</p>\n<script type=\"math/tex; mode=display\">\nd = tanh(W_d \\frac {1}{N_d} \\sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)</script></li>\n<li><p>其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码</p>\n</li>\n<li>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0ob1f.png\" alt=\"i0ob1f.png\"></p>\n<ul>\n<li><p>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</p>\n<script type=\"math/tex; mode=display\">\ns_j = \\sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)</script></li>\n<li><p>第一行：参数为当前句子编码，表示当前句子的内容</p>\n</li>\n<li>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</li>\n<li>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）</li>\n<li>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）</li>\n<li><p>最后对整个模型做最大似然估计:</p>\n<script type=\"math/tex; mode=display\">\nl(W,b) = -\\sum _{d=1}^N \\sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 | h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))</script></li>\n<li><p>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</p>\n</li>\n<li>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</li>\n<li>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oqc8.png\" alt=\"i0oqc8.png\"></p>\n<h1 id=\"Attention-Is-All-You-Need\"><a href=\"#Attention-Is-All-You-Need\" class=\"headerlink\" title=\"Attention Is All You Need\"></a>Attention Is All You Need</h1><ul>\n<li>抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oXng.png\" alt=\"i0oXng.png\"></p>\n<ul>\n<li>完全使用attention的一个好处就是快。</li>\n<li>为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512</li>\n<li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li>\n<li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li>\n<li>编码与解码的6个块都是堆叠的(stack)，</li>\n<li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TSNn.png\" alt=\"i0TSNn.png\"></p>\n<ul>\n<li>Multi-head attention由多个scaled dot-product attention并行组成。</li>\n<li><p>Scaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</p>\n<script type=\"math/tex; mode=display\">\nAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt {d_k}}) V</script></li>\n<li><p>Multi-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</p>\n<script type=\"math/tex; mode=display\">\nMultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\\\\nwhere \\ \\  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\\\</script></li>\n<li><p>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</p>\n</li>\n<li>这种multi-head attention用在了模型的三个地方：<ul>\n<li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置</li>\n<li>编码端块与块之间的自注意力</li>\n<li>解码端块与块之间的自注意力</li>\n</ul>\n</li>\n<li><p>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</p>\n<script type=\"math/tex; mode=display\">\nFFN(x) =\\max (0,xW_1+b_1)W_2 +b_2 \\\\</script></li>\n<li><p>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</p>\n<script type=\"math/tex; mode=display\">\nPE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\\\</script></li>\n</ul>\n<h1 id=\"A-Joint-Selective-Mechanism-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Joint-Selective-Mechanism-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Joint Selective Mechanism for Abstractive Sentence Summarization\"></a>A Joint Selective Mechanism for Abstractive Sentence Summarization</h1><p><img src=\"https://s1.ax1x.com/2018/11/15/ivQCE8.png\" alt=\"ivQCE8.png\"></p>\n<ul>\n<li>文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模</li>\n<li>作者针对损失建模，做了两点改进：<ul>\n<li>在编码完成之后添加了一个门限用于裁剪编码信息</li>\n<li>添加了一个选择损失，同时关注输入和输出，辅助门限工作</li>\n</ul>\n</li>\n<li>选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：<script type=\"math/tex; mode=display\">\ng_i = \\sigma (W_g h_i + U_g u_i) \\\\\nh_i^{'} = h_i \\cdot g_i \\\\</script></li>\n<li>而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：<script type=\"math/tex; mode=display\">\nr_{i,t} = \\sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\\\\nr_i = \\frac 1m \\sum _{t=2}^{m+1} r_{i,t} \\\\</script></li>\n<li>作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。</li>\n<li>之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:<script type=\"math/tex; mode=display\">\nd(g,r) = \\frac 1n \\sum _{i=1}^n |r_i - g_i | \\\\\nL = -p(y|x,\\theta) + \\lambda d(g,r) \\\\</script></li>\n<li>作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective Encoding for Abstractive Sentence Summarization 也没有说出这种设计的动机。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>读论文是不可能读完的，这辈子都不可能读完的。<br>没错，基本上没读论文，读了也来不及写笔记了。</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o47d.jpg\" alt=\"i0o47d.jpg\"></p>\n<h1 id=\"Distraction-Based-Neural-Networks-for-Document-Summarization\"><a href=\"#Distraction-Based-Neural-Networks-for-Document-Summarization\" class=\"headerlink\" title=\"Distraction-Based Neural Networks for Document Summarization\"></a>Distraction-Based Neural Networks for Document Summarization</h1><ul>\n<li>不仅仅使用注意力机制，还使用注意力分散机制，来更好地捕捉文档的整体含义。实验证明这种机制对于输入为长文本时尤其有效。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oh0H.png\" alt=\"i0oh0H.png\"></li>\n<li>在编码器和解码器之间引入控制层，实现注意力集中和注意力分散，用两层GRU构成：<script type=\"math/tex; mode=display\">\ns_t = GRU _1 (s_t^{temp},c_t) \\\\\ns_t^{temp} = GRU _2 (s_{t-1},e(y_{t-1})) \\\\</script></li>\n<li>这个控制层捕捉$s_t^{‘}$和$c_t$之间的联系，前者编码了当前及之前的输出信息，后者编码经过了注意力集中和注意力分散处理的当前输入，而$e(y_{t-1})$是上一次输入的embedding。</li>\n<li>三种注意力分散模型<ul>\n<li>M1：计算c_t用于控制层，在输入上做分散，其中c_t是普通的注意力编码出来的上下文c_t^{temp}，减去了历史上下文得到，类似coverage机制<script type=\"math/tex; mode=display\">\nc_t = tanh (W_c c_t^{temp} - U_c \\sum _{j=1}^{t-1} c_j) \\\\\nc_t^{temp} = \\sum _{i=1}^{T_x} \\alpha _{t,i} h_i \\\\</script></li>\n<li>M2：在注意力权重上做分散，类似的，也是减去历史注意力，再做归一化<script type=\"math/tex; mode=display\">\n\\alpha _{t,i}^{temp} = v_{\\alpha}^T tanh(W_a s_t^{temp} + U_a h_i - b_a \\sum _{j=1}^{t-1}\\alpha _{j,i}) \\\\\n\\alpha _{t,i} = \\frac {exp(\\alpha _{t,i}^{temp})}{\\sum _{j=1}^{T_x} exp(\\alpha _{t,j}^{temp})} \\\\</script></li>\n<li>M3：在解码端做分散，计算当前的$c_t$，$s_t$，$\\alpha _t$和历史的$c_t$，$s_t$，$\\alpha _t$之间的距离，和输出概率一起作为解码时束搜索所依赖的得分。<script type=\"math/tex; mode=display\">\nd_{\\alpha , t} = \\min KL(\\alpha _t , \\alpha _i) \\\\\nd_{c , t} = \\max cosine(c _t , c _i) \\\\\nd_{s , t} = \\max cosine(s _t , s _i) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Document-Modeling-with-External-Attention-for-Sentence-Extraction\"><a href=\"#Document-Modeling-with-External-Attention-for-Sentence-Extraction\" class=\"headerlink\" title=\"Document Modeling with External Attention for Sentence Extraction\"></a>Document Modeling with External Attention for Sentence Extraction</h1><ul>\n<li>构造了一个抽取式文摘模型，由分层文档编码器和基于外部信息注意力的抽取器组成。<br>在文摘任务中，外部信息是图片配字和文档标题。</li>\n<li>通过隐性的估计每个句子与文档的局部和全局关联性，显性的考虑外部信息，来决定每句话是否应该加入文摘。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oLjS.png\" alt=\"i0oLjS.png\"></p>\n<ul>\n<li>句子级编码器：如图所示，使用CNN编码，每个句子用大小为2和4的卷积核各三个，卷积出来的向量做maxpooling最后生成一个值，因此最后生成的向量为6维。</li>\n<li>文档级编码器：将一个文档的句子6维向量依次输入LSTM进行编码。</li>\n<li>句子抽取器：由带注意力机制的LSTM构成，与一般的生成式seq2seq不同，句子的编码不仅作为seq2seq中的编码输入，也作为解码输入，且一个是逆序一个是正序。抽取器依赖编码端输入$s_t$，解码端的上一时间步状态$h_t$，以及进行了注意力加权的外部信息$h_t^{‘}$：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oIAA.png\" alt=\"i0oIAA.png\"></p>\n<h1 id=\"Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\"><a href=\"#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks\" class=\"headerlink\" title=\"Get To The Point: Summarization with Pointer-Generator Networks\"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><ul>\n<li>介绍了两种机制，Pointer-Generator解决OOV问题，coverage解决重复词问题</li>\n<li>Pointer-Generator:通过context，当前timestep的decoder状态及输入学习到指针概率<script type=\"math/tex; mode=display\">\np_{gen} = \\sigma (w_h^T h_t + w_s^T s_t + w_x^T x_t +b_{ptr}) \\\\\nP(w) = p_{gen} P_{vocab}(w) + (1-p_{gen}) \\sum _{i:w_i = w} a_i^t \\\\</script></li>\n<li>指针概率指示是否应该正常生成，还是从输入里按照当前的注意力分布采样一个词汇复制过来，在上式中，如果当前的label是OOV，则左边部分为0，最大化右边使得注意力分布能够指示该复制的词的位置；如果label是生成的新词（原文中没有），则右边部分为0，最大化左边即正常的用decoder生成词。综合起来学习正确的指针概率。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0ootI.png\" alt=\"i0ootI.png\"></p>\n<ul>\n<li>Coverage:使用coverage机制来修改注意力，使得在之前timestep获得了较多注意力的词语之后获得较少注意力</li>\n<li><p>普通注意力计算</p>\n<script type=\"math/tex; mode=display\">\ne_i^t = v^T tanh(W_h h_i + W_s s_t + b_{attn}) \\\\\na^t = softmax(e^t) \\\\</script></li>\n<li><p>维护一个coverage向量，表示每个词在此之前获得了多少注意力:</p>\n<script type=\"math/tex; mode=display\">\nc^t = \\sum _{t^{temp} = 0}^t-1 a^{t^{temp}}</script></li>\n<li><p>然后用其修正注意力的生成，使得注意力生成考虑了之前的注意力累积</p>\n<script type=\"math/tex; mode=display\">\ne_i^t =v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_{attn})</script></li>\n<li><p>并在损失函数里加上coverage损失</p>\n<script type=\"math/tex; mode=display\">\ncovloss_t = \\sum _i \\min (a_i^t , c_i^t)</script></li>\n<li><p>使用min的含义是，我们只惩罚每一个attention和coverage分布重叠的部分，也就是coverage大的，如果attention也大，那covloss就大；coverage小的，不管attention如何，covloss都小</p>\n</li>\n</ul>\n<h1 id=\"SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\"><a href=\"#SummaRuNNer-A-Recurrent-Neural-Network-based-Sequence-Model-for-Extractive-Summarization-of-Documents\" class=\"headerlink\" title=\"SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\"></a>SummaRuNNer A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0oTht.png\" alt=\"i0oTht.png\"></p>\n<ul>\n<li>用RNN做抽取式文摘,可以用可视化展示模型决策过程，并且使用了一种端到端的训练方法</li>\n<li>将抽取视为句子分类任务，对每个句子按原文顺序依次访问，决定是否加入文摘，且这个决策考虑了之前决策的结果。</li>\n<li><p>用一层双向GRU在词级别上编码，再用一层双向GRU在句子级别上编码，两层输出的编码都经过了正反拼接和均值pooling</p>\n<script type=\"math/tex; mode=display\">\nd = tanh(W_d \\frac {1}{N_d} \\sum _{j=1}^{N^d} [h_j^f,h_j^b]+b)</script></li>\n<li><p>其中d是整篇文档的编码，$h_j^f$和$h_j^b$代表句子经过GRU的正反向编码</p>\n</li>\n<li>之后根据整篇文档的编码、句子的编码以及文摘在当前句子位置的动态表示来训练一个神经网络做二分类，决定每个句子是否应该加入文摘：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0ob1f.png\" alt=\"i0ob1f.png\"></p>\n<ul>\n<li><p>其中sj为到j位置为止已经产生的文摘的表示，用每个句子的二分类概率对之前句子的编码加权求和得到：</p>\n<script type=\"math/tex; mode=display\">\ns_j = \\sum _{i=1}^{j-1} h_i P(y_i = 1 | h_i,s_i,d)</script></li>\n<li><p>第一行：参数为当前句子编码，表示当前句子的内容</p>\n</li>\n<li>第二行：参数为文档编码和句子编码，表示当前句子对文档的显著性</li>\n<li>第三行：参数为句子编码和文摘动态编码，表示当前句对已产生文摘的冗余。（We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps.）</li>\n<li>第四行和第五行：考虑了句子在文档中的相对位置和绝对位置。（The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.）</li>\n<li><p>最后对整个模型做最大似然估计:</p>\n<script type=\"math/tex; mode=display\">\nl(W,b) = -\\sum _{d=1}^N \\sum _{j=1}^{N_d} (y_j^d log P(y_j^d = 1 | h_j^d,s_j^d,d_d)+(1-y_j^d)log(1-P(y_j^d=1|h_j^d,s_j^d,d_d)))</script></li>\n<li><p>作者将这种抽取式方法应用在生成式文摘语料上，也就是如何用生成式的文摘为原文中每个句子打上二分类的label。作者认为label为1的句子子集应该和生成式文摘ROUGE值最大，但是找出所有的子集太费时，就用了一种贪心的方法：一句一句将句子加入子集，如果剩下的句子没有一个能使当前子集ROUGE值上升，就不加了。这样将生成式文摘语料转换为抽取式文摘语料。</p>\n</li>\n<li>还有一种方式，直接在生成式文摘语料上做训练，将上面提到的动态文摘表示，取它最后一句也就是包含了整个文档的文摘表示s，输入一个解码器，解码出来生成式文摘。因为文摘表示是解码器的唯一输入，训练解码器的同时也能学习到好的文摘表示，从而完成抽取式文摘的任务。</li>\n<li>因为在生成二分类概率时包含了几个部分，将它们归一化可以得到各个部分做出的贡献，从而可视化决策过程：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oqc8.png\" alt=\"i0oqc8.png\"></p>\n<h1 id=\"Attention-Is-All-You-Need\"><a href=\"#Attention-Is-All-You-Need\" class=\"headerlink\" title=\"Attention Is All You Need\"></a>Attention Is All You Need</h1><ul>\n<li>抛弃了RNN和CNN做seq2seq任务，直接用multi head attention组成网络块叠加，加入BN层和残差连接构造深层网络</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oXng.png\" alt=\"i0oXng.png\"></p>\n<ul>\n<li>完全使用attention的一个好处就是快。</li>\n<li>为了使用残差，所有的子模块（multi-head attention和全连接）都统一输出维度为512</li>\n<li>编码端：6个块，每个块包含attention和全连接两个子模块，均使用了残差和bn。</li>\n<li>解码端：也是6个块，不同的是加了一个attention用于处理编码端的输出，而且与解码端输入相连的attention使用了mask，保证了方向性，即第i个位置的输出只与之前位置的输出有关。</li>\n<li>编码与解码的6个块都是堆叠的(stack)，</li>\n<li>Attention的通用模型是指将一个query和一系列键值对映射到输出的一种机制，其中输出是值的加权和，而每个值的权重将对应的键和query输入一个兼容性函数计算得到，传统的attention键和值相同，都是输入每个位置上的隐藏层状态，query就是当前输出，兼容性函数就是各种attention计算方法。图中指向attention的三个箭头分别代表key,value,query。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TSNn.png\" alt=\"i0TSNn.png\"></p>\n<ul>\n<li>Multi-head attention由多个scaled dot-product attention并行组成。</li>\n<li><p>Scaled dot-product attention如图所示，query和key先做点积，再放缩，如果是解码器输入的attention还要加上mask，之后过softmax函数与value做点积得到attention权重。实际计算时为了加速都是一系列query,key,value一起计算，所以Q,K,V都是矩阵。做放缩是为了防止点积attention在k的维度过大时处于softmax的两端，梯度小。</p>\n<script type=\"math/tex; mode=display\">\nAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt {d_k}}) V</script></li>\n<li><p>Multi-head attention就是有h个scaled dot-product attention作用于V,K,Q的h个线性投影上，学习到不同的特征，最后拼接并进行线性变换。作者认为这种multi-head的设计能使模型学习到不同位置的表示子空间的信息。</p>\n<script type=\"math/tex; mode=display\">\nMultiHead(Q,K,V) = Concat(head_1,...,head_h)W^o \\\\\nwhere \\ \\  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V) \\\\</script></li>\n<li><p>论文中取h=8个head，为了保证维度一致，单个q,k,v的维度取512/8=64</p>\n</li>\n<li>这种multi-head attention用在了模型的三个地方：<ul>\n<li>编码解码之间，其中key,value来自编码输出，query来自解码块中masked multi-head attention的输出。也就是传统的attention位置</li>\n<li>编码端块与块之间的自注意力</li>\n<li>解码端块与块之间的自注意力</li>\n</ul>\n</li>\n<li><p>在每个块里还有一个全连接层，这个层包含两个线性变换，中间插入了ReLU激活，且每个输入位置都有相同的参数，但不同的块的全连接层参数不同</p>\n<script type=\"math/tex; mode=display\">\nFFN(x) =\\max (0,xW_1+b_1)W_2 +b_2 \\\\</script></li>\n<li><p>完全使用注意力的话会抛弃了序列的顺序信息，为了利用这部分信息，加入了三角函数位置编码来利用相对位置信息：</p>\n<script type=\"math/tex; mode=display\">\nPE_{(pos,2i)} = sin(pos/10000 ^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} = cos(pos/10000 ^{2i/d_{model}}) \\\\</script></li>\n</ul>\n<h1 id=\"A-Joint-Selective-Mechanism-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Joint-Selective-Mechanism-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Joint Selective Mechanism for Abstractive Sentence Summarization\"></a>A Joint Selective Mechanism for Abstractive Sentence Summarization</h1><p><img src=\"https://s1.ax1x.com/2018/11/15/ivQCE8.png\" alt=\"ivQCE8.png\"></p>\n<ul>\n<li>文摘不同于翻译，端到端框架应该对损失（信息压缩）建模，而不是和翻译一样单纯对对齐建模</li>\n<li>作者针对损失建模，做了两点改进：<ul>\n<li>在编码完成之后添加了一个门限用于裁剪编码信息</li>\n<li>添加了一个选择损失，同时关注输入和输出，辅助门限工作</li>\n</ul>\n</li>\n<li>选择门限同时考虑了编码之后的隐藏层状态和原始词嵌入，并作用于隐藏层状态之上，对隐层向量做裁剪，之后再经过注意力加权生成上下文。作者认为这个过程相当于让网络观察rnn处理前后的词嵌入，能够知道输入中的哪个单词对于产生文摘很重要：<script type=\"math/tex; mode=display\">\ng_i = \\sigma (W_g h_i + U_g u_i) \\\\\nh_i^{'} = h_i \\cdot g_i \\\\</script></li>\n<li>而选择损失函数则是在解码端构造了一个回顾门限，考虑了编码端的隐层和原始输入，解码端的隐层和原始输入，解码端每一个位置的回顾门限是对编码端所有位置的回顾门限求平均：<script type=\"math/tex; mode=display\">\nr_{i,t} = \\sigma (W_r h_i + U_r u_i + V_r s_{t-1} + Q_r w_{t-1}) \\\\\nr_i = \\frac 1m \\sum _{t=2}^{m+1} r_{i,t} \\\\</script></li>\n<li>作者认为回顾门限的作用相当于让网络阅读产生的文摘，并回顾输入文本，使其知道学会如何挑选文摘。</li>\n<li>之后用选择门限和回顾门限的欧氏距离作为选择损失，加入总损失中:<script type=\"math/tex; mode=display\">\nd(g,r) = \\frac 1n \\sum _{i=1}^n |r_i - g_i | \\\\\nL = -p(y|x,\\theta) + \\lambda d(g,r) \\\\</script></li>\n<li>作者并没有说明为什么将回顾门限和选择门限之间的欧式距离作为损失函数，也没有说明选择门限和注意力的区别，感觉就像是考虑了原始输入embedding的一种注意力机制，且在传统注意力加权之前先对隐层每一时间步做了裁剪。选出来的可视化特例也很精巧，恰恰说明了这个选择机制能识别句子中的转折，因而改变了选择的词，这还是和之前选择门限提出的原论文对比。原论文Selective Encoding for Abstractive Sentence Summarization 也没有说出这种设计的动机。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o47d.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"论文阅读笔记2018下半年","path":"2018/07/03/PaperReading2/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o47d.jpg","excerpt":"<p>读论文是不可能读完的，这辈子都不可能读完的。<br>没错，基本上没读论文，读了也来不及写笔记了。</p>","date":"2018-07-03T07:18:52.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:Coding基础与线性回归","mathjax":true,"date":"2017-02-07T13:57:22.000Z","_content":"\n***\n# 简介\n\n2016年11月的时候决定开始入坑机器学习\n首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。\n\n\n<!--more-->\n\n\n2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression\n\n题目介绍在这：[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n\n下面是数据集表格样式，每个人有12个属性\n\n![i0TjxK.jpg](https://s1.ax1x.com/2018/10/20/i0TjxK.jpg)\n\n\n***\n# 不是算法的算法\n\n官方示例就是按几个属性分类，比如年龄，性别，票价(.....)\n然后对每个属性内所有人的生还数据（0或者1）加一起求平均。\n英文注释都是官方文档的说明\n我就当入门教程学了，也全打了上去\n代码如下：\n```Python\n    # -*- coding: utf-8 -*-\n    \"\"\"\n    Created on Sun Oct 30 15:28:22 2016\n    \n    @author: thinkwee\n    \"\"\"\n    \n    import csv as csv \n    import numpy as np\n    from glue import qglue\n    \n    \n    test_file=(open(r'文件目录略', 'r'))\n    test_file_object = csv.reader(open(r'文件目录略', 'r'))\n    testheader = next(test_file_object)\n    predictions_file = open(r\"文件目录略\", \"w\")\n    predictions_file_object = csv.writer(predictions_file)\n    p = csv.writer(predictions_file)\n    p.writerow([\"PassengerId\", \"Survived\"])\n    csv_file_object = csv.reader(open(r'文件目录略', 'r')) \n    trainheader = next(csv_file_object)  # The next() command just skips the \n\t\t\t\t\t\t\t\t\t\t # first line which is a header\n    data=[]                          \t # Create a variable called 'data'.\n    for row in csv_file_object:      \t # Run through each row in the csv file,\n        data.append(row)             \t # adding each row to the data variable\n    print(type(data))\n    data = np.array(data) \t         \t # Then convert from a list to an array\n\t\t\t\t\t\t\t\t\t\t # Be aware that each item is currently\n\t\t\t\t\t\t\t\t\t\t # a string in this format\n    \n    number_passengers = np.size(data[0::,1].astype(np.float))\n    number_survived = np.sum(data[0::,1].astype(np.float))\n    proportion_survivors = number_survived / number_passengers\n    \n    women_only_stats = data[0::,4] == \"female\" # This finds where all \n                                               # the elements in the gender\n                                               # column that equals “female”\n    men_only_stats = data[0::,4] != \"female\"   # This finds where all the \n                                               # elements do not equal \n                                               # female (i.e. male)\n                                               \n    # Using the index from above we select the females and males separately\n    women_onboard = data[women_only_stats,1].astype(np.float)     \n    men_onboard = data[men_only_stats,1].astype(np.float)\n    \n    # Then we finds the proportions of them that survived\n    proportion_women_survived = \\\n                           np.sum(women_onboard) / np.size(women_onboard)  \n    proportion_men_survived = \\\n                           np.sum(men_onboard) / np.size(men_onboard) \n    \n    # and then print it out\n    print ('Proportion of women who survived is %s' % proportion_women_survived)\n    print ('Proportion of men who survived is %s' % proportion_men_survived)\n    \n    \n    \n    \n    # The script will systematically will loop through each combination \n    # and use the 'where' function in python to search the passengers that fit that combination of variables. \n    # Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. \n    # The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  \n    # For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. \n    # So then you can set the bins\n    \n    # So we add a ceiling\n    fare_ceiling = 40\n    \n    # then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\n    data[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n    \n    fare_bracket_size = 10\n    number_of_price_brackets = fare_ceiling // fare_bracket_size\n    \n    # Take the length of an array of unique values in column index 2\n    number_of_classes = len(np.unique(data[0::,2]))\n    \n    number_of_age_brackets=8 \n    \n    # Initialize the survival table with all zeros\n    survival_table = np.zeros((2, number_of_classes, \n    \t\t\t\t\t\t   number_of_price_brackets,\n    \t\t\t\t\t\t   number_of_age_brackets))\n    \n    \n    \n    #Now that these are set up, \n    #you can loop through each variable \n    #and find all those passengers that agree with the statements\n    \n    for i in range(number_of_classes):       \t\t#loop through each class\n      for j in range(number_of_price_brackets):   \t#loop through each price bin\n        for k in range(number_of_age_brackets):     #loop through each age bin\n            women_only_stats_plus = data[                 #Which element           \n                            (data[0::,4] == \"female\")     #is a female\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                        \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)#the next bin\n                           \n                              , 1]                        #in the 2nd col                           \n     \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t\n    \n            men_only_stats_plus = data[                   #Which element           \n                             (data[0::,4] != \"female\")    #is a male\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                                       \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)#the next bin\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)\n                              , 1]\n                              \n            survival_table[0,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) \n            survival_table[1,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))\n    \t\t\n    \t\t#if nan then the type will change to string from float so this sentence can set nan to 0. \n            survival_table[ survival_table != survival_table ] = 0.\n    \n    #Notice that  data[ where function, 1]  means \n    #it is finding the Survived column for the conditional criteria which is being called. \n    #As the loop starts with i=0 and j=0, \n    #the first loop will return the Survived values for all the 1st-class females (i + 1) \n    #who paid less than 10 ((j+1)*fare_bracket_size) \n    #and similarly all the 1st-class males who paid less than 10.  \n    #Before resetting to the top of the loop, \n    #we can calculate the proportion of survivors for this particular \n    #combination of criteria and record it to our survival table\n    \n        \n    #官方示例中将概率大于0.5的视为生还，这里我们略过\n    #直接打印详细概率\n    #survival_table[ survival_table < 0.5 ] = 0\n    #survival_table[ survival_table >= 0.5 ] = 1 \n        \n        \n    #Then we can make the prediction\n    \n    for row in test_file_object:                  # We are going to loop\n                                                  # through each passenger\n                                                  # in the test set                     \n      for j in range(number_of_price_brackets):   # For each passenger we\n                                                  # loop thro each price bin\n        try:                                      # Some passengers have no\n                                                  # Fare data so try to make\n          row[8] = float(row[8])                  # a float\n        except:                                   # If fails: no data, so \n          bin_fare = 3 - float(row[1])            # bin the fare according Pclass\n          break                                   # Break from the loop\n        if row[8] > fare_ceiling:              \t  # If there is data see if\n                                                  # it is greater than fare\n                                                  # ceiling we set earlier\n          bin_fare = number_of_price_brackets-1   # If so set to highest bin\n          break                                   # And then break loop\n        if row[8] >= j * fare_bracket_size\\\n           and row[8] < \\\n           (j+1) * fare_bracket_size:             # If passed these tests \n                                                  # then loop through each bin \n          bin_fare = j                            # then assign index\n          break\n      \n      for j in range(number_of_age_brackets): \n                                                 \n        try:                                    \n                                                \n          row[4] = float(row[4])              \n        except:                                   \n          bin_age = -1      \n          break                                  \n                                   \n        if row[4] >= j * 10\\\n           and row[4] < \\\n           (j+1) * 10:             # If passed these tests \n                                   # then loop through each bin \n          bin_age = j              # then assign index\n          break\n      \n      if row[3] == 'female':       #If the passenger is female\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[0, int(row[1])-1, bin_fare,bin_age]*100)])\n      else:                        #else if male\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[1, int(row[1])-1, bin_fare,bin_age]*100)])\n         \n    # Close out the files.\n    test_file.close() \n    predictions_file.close()\n```\n\n\n***\n# 多元线性回归\n    \n之后买了西瓜书，我把这个例题改成了线性回归模型：\n假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，\n我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小\n用均方误差作为性能度量，均方误差是线性系数的函数\n对线性系数w求导，可以得到w最优解的闭式\n\n关键公式是\n\t** $$ w^*=(X^TX)^{-1}X^Ty $$ **\n\n-\tX:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，\n\t  假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)\n-\tw:线性系数\n-\ty:生还结果 $$ y=w^T*x $$\n\n写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %\n\n![i0TzrD.jpg](https://s1.ax1x.com/2018/10/20/i0TzrD.jpg)\n\n代码如下\n\n```Python\n        train1=train.dropna(subset=(['Age']),axis=0)\n    vali1=vali.dropna(subset=(['Age']),axis=0)\n    \n    validata=np.array(vali1)\n    data=np.array(train1)\n    \n    fare_ceiling = 40\n    data[data[0::,9].astype(np.float)>=fare_ceiling,9] = fare_ceiling - 1.0\n    \n    train = np.column_stack((data[0::,9],data[0::,2],data[0::,5],data[0::,4]))\n    predict=np.column_stack((validata[0::,9],validata[0::,2],validata[0::,5],validata[0::,4]))\n    survive = np.column_stack((data[0::,1]))\n    \n    \n    for i in range(train.shape[0]):\n        if (train[i][3]=='male'):\n            train[i][3]=0.00\n        else:\n            train[i][3]=1.00\n    for i in range(predict.shape[0]):\n        if (predict[i][3]=='male'):\n            predict[i][3]=0.00\n        else:\n            predict[i][3]=1.00\n    \n    x0=np.ones((train.shape[0],1))\n    train=np.concatenate((train,x0),axis=1)\n    \n    x0=np.ones((predict.shape[0],1))\n    predict=np.concatenate((predict,x0),axis=1)\n    \n    print('raw data finish')\n    \n    survive=survive.T.astype(np.float)\n    traint=train.T.astype(np.float)\n    w0=traint.dot(train.astype(np.float))\n    w1=(np.linalg.inv(w0))  \n    w2=w1.dot(traint)\n    w=w2.dot(survive)  #w=(Xt*X)^-1*Xt*y\n    print('w calc finish')\n    \n    feature=['Fare','Pclass','Age','Sex','b']\n    for i in zip(feature,w):\n        print(i)\n    \n    \n    valipredict_file_object.writerow([\"PassengerName\", \"Actual Survived\",\"Predict Survived\",\"XO\"])\n    count=0.0\n    for i in range(predict.shape[0]):\n        temp=predict[i,0::].T.astype(float)\n        answer=temp.dot(w)\n        answer=answer[0]\n        if ((answer>0.5 and validata[i][1]==1) or (answer<0.5 and validata[i][1]==0)):\n            flag=\"Correct\"\n            count=count+1.0;\n        else:\n            flag=\"Error\"\n        valipredict_file_object.writerow([validata[i][3],validata[i][1],answer,flag])\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/predict.shape[0]*100))  \n```\n***\n# scikit-learn中的多元线性回归\n试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %\n\n![i0TxKO.jpg](https://s1.ax1x.com/2018/10/20/i0TxKO.jpg)\n\n如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览\n\n![i07DRx.jpg](https://s1.ax1x.com/2018/10/20/i07DRx.jpg)\n\n```Python\n    train=train.dropna(subset=['Age','Embarked'],axis=0)\n    vali=vali.dropna(subset=(['Age','Embarked']),axis=0)\n    \n    train.loc[train[\"Sex\"]==\"male\",\"Sex\"]=0\n    train.loc[train[\"Sex\"]==\"female\",\"Sex\"]=1\n    train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    trainx=train.reindex(index=train.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    vali.loc[vali[\"Sex\"]==\"male\",\"Sex\"]=0\n    vali.loc[vali[\"Sex\"]==\"female\",\"Sex\"]=1\n    vali.loc[vali[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    vali.loc[vali[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    vali.loc[vali[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    vali1=vali.reindex(index=vali.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    survive=vali.reindex(index=vali.index[:],columns=['Survived'])\n    survive=np.array(survive)\n    \n    feature=['Age','Sex','Parch','Fare','Embarked','SibSp']\n    \n    trainy=train.reindex(index=train.index[:],columns=['Survived'])  \n    trainy=trainy.Survived\n    \n    X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=1)\n    \n    \n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    print(model)\n    \n    \n    for i in zip(feature,model.coef_):\n        print(i)\n    \n    predict=model.predict(vali1)\n    \n    count=0\n    for i in range(len(predict)):\n        if (predict[i]>1 and survive[i] == 1) or  (predict[i]<1 and survive [i]== 0 ):\n            count=count+1.0\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/len(predict)*100))\n```","source":"_posts/TitanicLinearRegression.md","raw":"title: '机器学习入门:Coding基础与线性回归'\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\ndate: 2017-02-07 21:57:22\n---\n\n***\n# 简介\n\n2016年11月的时候决定开始入坑机器学习\n首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。\n\n\n<!--more-->\n\n\n2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression\n\n题目介绍在这：[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n\n下面是数据集表格样式，每个人有12个属性\n\n![i0TjxK.jpg](https://s1.ax1x.com/2018/10/20/i0TjxK.jpg)\n\n\n***\n# 不是算法的算法\n\n官方示例就是按几个属性分类，比如年龄，性别，票价(.....)\n然后对每个属性内所有人的生还数据（0或者1）加一起求平均。\n英文注释都是官方文档的说明\n我就当入门教程学了，也全打了上去\n代码如下：\n```Python\n    # -*- coding: utf-8 -*-\n    \"\"\"\n    Created on Sun Oct 30 15:28:22 2016\n    \n    @author: thinkwee\n    \"\"\"\n    \n    import csv as csv \n    import numpy as np\n    from glue import qglue\n    \n    \n    test_file=(open(r'文件目录略', 'r'))\n    test_file_object = csv.reader(open(r'文件目录略', 'r'))\n    testheader = next(test_file_object)\n    predictions_file = open(r\"文件目录略\", \"w\")\n    predictions_file_object = csv.writer(predictions_file)\n    p = csv.writer(predictions_file)\n    p.writerow([\"PassengerId\", \"Survived\"])\n    csv_file_object = csv.reader(open(r'文件目录略', 'r')) \n    trainheader = next(csv_file_object)  # The next() command just skips the \n\t\t\t\t\t\t\t\t\t\t # first line which is a header\n    data=[]                          \t # Create a variable called 'data'.\n    for row in csv_file_object:      \t # Run through each row in the csv file,\n        data.append(row)             \t # adding each row to the data variable\n    print(type(data))\n    data = np.array(data) \t         \t # Then convert from a list to an array\n\t\t\t\t\t\t\t\t\t\t # Be aware that each item is currently\n\t\t\t\t\t\t\t\t\t\t # a string in this format\n    \n    number_passengers = np.size(data[0::,1].astype(np.float))\n    number_survived = np.sum(data[0::,1].astype(np.float))\n    proportion_survivors = number_survived / number_passengers\n    \n    women_only_stats = data[0::,4] == \"female\" # This finds where all \n                                               # the elements in the gender\n                                               # column that equals “female”\n    men_only_stats = data[0::,4] != \"female\"   # This finds where all the \n                                               # elements do not equal \n                                               # female (i.e. male)\n                                               \n    # Using the index from above we select the females and males separately\n    women_onboard = data[women_only_stats,1].astype(np.float)     \n    men_onboard = data[men_only_stats,1].astype(np.float)\n    \n    # Then we finds the proportions of them that survived\n    proportion_women_survived = \\\n                           np.sum(women_onboard) / np.size(women_onboard)  \n    proportion_men_survived = \\\n                           np.sum(men_onboard) / np.size(men_onboard) \n    \n    # and then print it out\n    print ('Proportion of women who survived is %s' % proportion_women_survived)\n    print ('Proportion of men who survived is %s' % proportion_men_survived)\n    \n    \n    \n    \n    # The script will systematically will loop through each combination \n    # and use the 'where' function in python to search the passengers that fit that combination of variables. \n    # Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. \n    # The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  \n    # For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. \n    # So then you can set the bins\n    \n    # So we add a ceiling\n    fare_ceiling = 40\n    \n    # then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\n    data[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n    \n    fare_bracket_size = 10\n    number_of_price_brackets = fare_ceiling // fare_bracket_size\n    \n    # Take the length of an array of unique values in column index 2\n    number_of_classes = len(np.unique(data[0::,2]))\n    \n    number_of_age_brackets=8 \n    \n    # Initialize the survival table with all zeros\n    survival_table = np.zeros((2, number_of_classes, \n    \t\t\t\t\t\t   number_of_price_brackets,\n    \t\t\t\t\t\t   number_of_age_brackets))\n    \n    \n    \n    #Now that these are set up, \n    #you can loop through each variable \n    #and find all those passengers that agree with the statements\n    \n    for i in range(number_of_classes):       \t\t#loop through each class\n      for j in range(number_of_price_brackets):   \t#loop through each price bin\n        for k in range(number_of_age_brackets):     #loop through each age bin\n            women_only_stats_plus = data[                 #Which element           \n                            (data[0::,4] == \"female\")     #is a female\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                        \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)#the next bin\n                           \n                              , 1]                        #in the 2nd col                           \n     \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t\n    \n            men_only_stats_plus = data[                   #Which element           \n                             (data[0::,4] != \"female\")    #is a male\n                           &(data[0::,2].astype(np.float) #and was ith class\n                                 == i+1)                                       \n                           &(data[0:,9].astype(np.float)  #was greater \n                                >= j*fare_bracket_size)   #than this bin              \n                           &(data[0:,9].astype(np.float)  #and less than\n                                < (j+1)*fare_bracket_size)#the next bin\n                           &(data[0:,5].astype(np.float)>=k*10)\n                           &(data[0:,5].astype(np.float)<(k+1)*10)\n                              , 1]\n                              \n            survival_table[0,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) \n            survival_table[1,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))\n    \t\t\n    \t\t#if nan then the type will change to string from float so this sentence can set nan to 0. \n            survival_table[ survival_table != survival_table ] = 0.\n    \n    #Notice that  data[ where function, 1]  means \n    #it is finding the Survived column for the conditional criteria which is being called. \n    #As the loop starts with i=0 and j=0, \n    #the first loop will return the Survived values for all the 1st-class females (i + 1) \n    #who paid less than 10 ((j+1)*fare_bracket_size) \n    #and similarly all the 1st-class males who paid less than 10.  \n    #Before resetting to the top of the loop, \n    #we can calculate the proportion of survivors for this particular \n    #combination of criteria and record it to our survival table\n    \n        \n    #官方示例中将概率大于0.5的视为生还，这里我们略过\n    #直接打印详细概率\n    #survival_table[ survival_table < 0.5 ] = 0\n    #survival_table[ survival_table >= 0.5 ] = 1 \n        \n        \n    #Then we can make the prediction\n    \n    for row in test_file_object:                  # We are going to loop\n                                                  # through each passenger\n                                                  # in the test set                     \n      for j in range(number_of_price_brackets):   # For each passenger we\n                                                  # loop thro each price bin\n        try:                                      # Some passengers have no\n                                                  # Fare data so try to make\n          row[8] = float(row[8])                  # a float\n        except:                                   # If fails: no data, so \n          bin_fare = 3 - float(row[1])            # bin the fare according Pclass\n          break                                   # Break from the loop\n        if row[8] > fare_ceiling:              \t  # If there is data see if\n                                                  # it is greater than fare\n                                                  # ceiling we set earlier\n          bin_fare = number_of_price_brackets-1   # If so set to highest bin\n          break                                   # And then break loop\n        if row[8] >= j * fare_bracket_size\\\n           and row[8] < \\\n           (j+1) * fare_bracket_size:             # If passed these tests \n                                                  # then loop through each bin \n          bin_fare = j                            # then assign index\n          break\n      \n      for j in range(number_of_age_brackets): \n                                                 \n        try:                                    \n                                                \n          row[4] = float(row[4])              \n        except:                                   \n          bin_age = -1      \n          break                                  \n                                   \n        if row[4] >= j * 10\\\n           and row[4] < \\\n           (j+1) * 10:             # If passed these tests \n                                   # then loop through each bin \n          bin_age = j              # then assign index\n          break\n      \n      if row[3] == 'female':       #If the passenger is female\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[0, int(row[1])-1, bin_fare,bin_age]*100)])\n      else:                        #else if male\n            p.writerow([row[0], \"%f %%\" % \\\n                       (survival_table[1, int(row[1])-1, bin_fare,bin_age]*100)])\n         \n    # Close out the files.\n    test_file.close() \n    predictions_file.close()\n```\n\n\n***\n# 多元线性回归\n    \n之后买了西瓜书，我把这个例题改成了线性回归模型：\n假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，\n我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小\n用均方误差作为性能度量，均方误差是线性系数的函数\n对线性系数w求导，可以得到w最优解的闭式\n\n关键公式是\n\t** $$ w^*=(X^TX)^{-1}X^Ty $$ **\n\n-\tX:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，\n\t  假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)\n-\tw:线性系数\n-\ty:生还结果 $$ y=w^T*x $$\n\n写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %\n\n![i0TzrD.jpg](https://s1.ax1x.com/2018/10/20/i0TzrD.jpg)\n\n代码如下\n\n```Python\n        train1=train.dropna(subset=(['Age']),axis=0)\n    vali1=vali.dropna(subset=(['Age']),axis=0)\n    \n    validata=np.array(vali1)\n    data=np.array(train1)\n    \n    fare_ceiling = 40\n    data[data[0::,9].astype(np.float)>=fare_ceiling,9] = fare_ceiling - 1.0\n    \n    train = np.column_stack((data[0::,9],data[0::,2],data[0::,5],data[0::,4]))\n    predict=np.column_stack((validata[0::,9],validata[0::,2],validata[0::,5],validata[0::,4]))\n    survive = np.column_stack((data[0::,1]))\n    \n    \n    for i in range(train.shape[0]):\n        if (train[i][3]=='male'):\n            train[i][3]=0.00\n        else:\n            train[i][3]=1.00\n    for i in range(predict.shape[0]):\n        if (predict[i][3]=='male'):\n            predict[i][3]=0.00\n        else:\n            predict[i][3]=1.00\n    \n    x0=np.ones((train.shape[0],1))\n    train=np.concatenate((train,x0),axis=1)\n    \n    x0=np.ones((predict.shape[0],1))\n    predict=np.concatenate((predict,x0),axis=1)\n    \n    print('raw data finish')\n    \n    survive=survive.T.astype(np.float)\n    traint=train.T.astype(np.float)\n    w0=traint.dot(train.astype(np.float))\n    w1=(np.linalg.inv(w0))  \n    w2=w1.dot(traint)\n    w=w2.dot(survive)  #w=(Xt*X)^-1*Xt*y\n    print('w calc finish')\n    \n    feature=['Fare','Pclass','Age','Sex','b']\n    for i in zip(feature,w):\n        print(i)\n    \n    \n    valipredict_file_object.writerow([\"PassengerName\", \"Actual Survived\",\"Predict Survived\",\"XO\"])\n    count=0.0\n    for i in range(predict.shape[0]):\n        temp=predict[i,0::].T.astype(float)\n        answer=temp.dot(w)\n        answer=answer[0]\n        if ((answer>0.5 and validata[i][1]==1) or (answer<0.5 and validata[i][1]==0)):\n            flag=\"Correct\"\n            count=count+1.0;\n        else:\n            flag=\"Error\"\n        valipredict_file_object.writerow([validata[i][3],validata[i][1],answer,flag])\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/predict.shape[0]*100))  \n```\n***\n# scikit-learn中的多元线性回归\n试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %\n\n![i0TxKO.jpg](https://s1.ax1x.com/2018/10/20/i0TxKO.jpg)\n\n如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览\n\n![i07DRx.jpg](https://s1.ax1x.com/2018/10/20/i07DRx.jpg)\n\n```Python\n    train=train.dropna(subset=['Age','Embarked'],axis=0)\n    vali=vali.dropna(subset=(['Age','Embarked']),axis=0)\n    \n    train.loc[train[\"Sex\"]==\"male\",\"Sex\"]=0\n    train.loc[train[\"Sex\"]==\"female\",\"Sex\"]=1\n    train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    trainx=train.reindex(index=train.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    vali.loc[vali[\"Sex\"]==\"male\",\"Sex\"]=0\n    vali.loc[vali[\"Sex\"]==\"female\",\"Sex\"]=1\n    vali.loc[vali[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    vali.loc[vali[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    vali.loc[vali[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n    vali1=vali.reindex(index=vali.index[:],columns=['Age']+['Sex']+['Parch']+['Fare']+['Embarked']+['SibSp'])\n    \n    survive=vali.reindex(index=vali.index[:],columns=['Survived'])\n    survive=np.array(survive)\n    \n    feature=['Age','Sex','Parch','Fare','Embarked','SibSp']\n    \n    trainy=train.reindex(index=train.index[:],columns=['Survived'])  \n    trainy=trainy.Survived\n    \n    X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=1)\n    \n    \n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    print(model)\n    \n    \n    for i in zip(feature,model.coef_):\n        print(i)\n    \n    predict=model.predict(vali1)\n    \n    count=0\n    for i in range(len(predict)):\n        if (predict[i]>1 and survive[i] == 1) or  (predict[i]<1 and survive [i]== 0 ):\n            count=count+1.0\n    \n    print(\"prediction finish\")\n    print(\"prediction ratio:\",\"%f %%\"%(count/len(predict)*100))\n```","slug":"TitanicLinearRegression","published":1,"updated":"2019-07-22T03:45:22.950Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v39001jq8t5uchbmgpj","content":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p>\n<a id=\"more\"></a>\n<p>2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression</p>\n<p>题目介绍在这：<a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener\">Titanic: Machine Learning from Disaster</a></p>\n<p>下面是数据集表格样式，每个人有12个属性</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TjxK.jpg\" alt=\"i0TjxK.jpg\"></p>\n<hr>\n<h1 id=\"不是算法的算法\"><a href=\"#不是算法的算法\" class=\"headerlink\" title=\"不是算法的算法\"></a>不是算法的算法</h1><p>官方示例就是按几个属性分类，比如年龄，性别，票价(…..)<br>然后对每个属性内所有人的生还数据（0或者1）加一起求平均。<br>英文注释都是官方文档的说明<br>我就当入门教程学了，也全打了上去<br>代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">Created on Sun Oct 30 15:28:22 2016</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@author: thinkwee</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> glue <span class=\"keyword\">import</span> qglue</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_file=(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">test_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">testheader = next(test_file_object)</span><br><span class=\"line\">predictions_file = open(<span class=\"string\">r\"文件目录略\"</span>, <span class=\"string\">\"w\"</span>)</span><br><span class=\"line\">predictions_file_object = csv.writer(predictions_file)</span><br><span class=\"line\">p = csv.writer(predictions_file)</span><br><span class=\"line\">p.writerow([<span class=\"string\">\"PassengerId\"</span>, <span class=\"string\">\"Survived\"</span>])</span><br><span class=\"line\">csv_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>)) </span><br><span class=\"line\">trainheader = next(csv_file_object)  <span class=\"comment\"># The next() command just skips the </span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># first line which is a header</span></span><br><span class=\"line\">data=[]                          \t <span class=\"comment\"># Create a variable called 'data'.</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> csv_file_object:      \t <span class=\"comment\"># Run through each row in the csv file,</span></span><br><span class=\"line\">    data.append(row)             \t <span class=\"comment\"># adding each row to the data variable</span></span><br><span class=\"line\">print(type(data))</span><br><span class=\"line\">data = np.array(data) \t         \t <span class=\"comment\"># Then convert from a list to an array</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># Be aware that each item is currently</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># a string in this format</span></span><br><span class=\"line\"></span><br><span class=\"line\">number_passengers = np.size(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">number_survived = np.sum(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">proportion_survivors = number_survived / number_passengers</span><br><span class=\"line\"></span><br><span class=\"line\">women_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span> <span class=\"comment\"># This finds where all </span></span><br><span class=\"line\">                                           <span class=\"comment\"># the elements in the gender</span></span><br><span class=\"line\">                                           <span class=\"comment\"># column that equals “female”</span></span><br><span class=\"line\">men_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>   <span class=\"comment\"># This finds where all the </span></span><br><span class=\"line\">                                           <span class=\"comment\"># elements do not equal </span></span><br><span class=\"line\">                                           <span class=\"comment\"># female (i.e. male)</span></span><br><span class=\"line\">                                           </span><br><span class=\"line\"><span class=\"comment\"># Using the index from above we select the females and males separately</span></span><br><span class=\"line\">women_onboard = data[women_only_stats,<span class=\"number\">1</span>].astype(np.float)     </span><br><span class=\"line\">men_onboard = data[men_only_stats,<span class=\"number\">1</span>].astype(np.float)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Then we finds the proportions of them that survived</span></span><br><span class=\"line\">proportion_women_survived = \\</span><br><span class=\"line\">                       np.sum(women_onboard) / np.size(women_onboard)  </span><br><span class=\"line\">proportion_men_survived = \\</span><br><span class=\"line\">                       np.sum(men_onboard) / np.size(men_onboard) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># and then print it out</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of women who survived is %s'</span> % proportion_women_survived)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of men who survived is %s'</span> % proportion_men_survived)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The script will systematically will loop through each combination </span></span><br><span class=\"line\"><span class=\"comment\"># and use the 'where' function in python to search the passengers that fit that combination of variables. </span></span><br><span class=\"line\"><span class=\"comment\"># Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. </span></span><br><span class=\"line\"><span class=\"comment\"># The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  </span></span><br><span class=\"line\"><span class=\"comment\"># For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. </span></span><br><span class=\"line\"><span class=\"comment\"># So then you can set the bins</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># So we add a ceiling</span></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling</span></span><br><span class=\"line\">data[ data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float) &gt;= fare_ceiling, <span class=\"number\">9</span> ] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">fare_bracket_size = <span class=\"number\">10</span></span><br><span class=\"line\">number_of_price_brackets = fare_ceiling // fare_bracket_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Take the length of an array of unique values in column index 2</span></span><br><span class=\"line\">number_of_classes = len(np.unique(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">number_of_age_brackets=<span class=\"number\">8</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the survival table with all zeros</span></span><br><span class=\"line\">survival_table = np.zeros((<span class=\"number\">2</span>, number_of_classes, </span><br><span class=\"line\">\t\t\t\t\t\t   number_of_price_brackets,</span><br><span class=\"line\">\t\t\t\t\t\t   number_of_age_brackets))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Now that these are set up, </span></span><br><span class=\"line\"><span class=\"comment\">#you can loop through each variable </span></span><br><span class=\"line\"><span class=\"comment\">#and find all those passengers that agree with the statements</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(number_of_classes):       \t\t<span class=\"comment\">#loop through each class</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   \t<span class=\"comment\">#loop through each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(number_of_age_brackets):     <span class=\"comment\">#loop through each age bin</span></span><br><span class=\"line\">        women_only_stats_plus = data[                 <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                        (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span>)     <span class=\"comment\">#is a female</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                        </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       </span><br><span class=\"line\">                          , <span class=\"number\">1</span>]                        <span class=\"comment\">#in the 2nd col                           </span></span><br><span class=\"line\"> \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">        men_only_stats_plus = data[                   <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                         (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>)    <span class=\"comment\">#is a male</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                                       </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)</span><br><span class=\"line\">                          , <span class=\"number\">1</span>]</span><br><span class=\"line\">                          </span><br><span class=\"line\">        survival_table[<span class=\"number\">0</span>,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) </span><br><span class=\"line\">        survival_table[<span class=\"number\">1</span>,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">#if nan then the type will change to string from float so this sentence can set nan to 0. </span></span><br><span class=\"line\">        survival_table[ survival_table != survival_table ] = <span class=\"number\">0.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Notice that  data[ where function, 1]  means </span></span><br><span class=\"line\"><span class=\"comment\">#it is finding the Survived column for the conditional criteria which is being called. </span></span><br><span class=\"line\"><span class=\"comment\">#As the loop starts with i=0 and j=0, </span></span><br><span class=\"line\"><span class=\"comment\">#the first loop will return the Survived values for all the 1st-class females (i + 1) </span></span><br><span class=\"line\"><span class=\"comment\">#who paid less than 10 ((j+1)*fare_bracket_size) </span></span><br><span class=\"line\"><span class=\"comment\">#and similarly all the 1st-class males who paid less than 10.  </span></span><br><span class=\"line\"><span class=\"comment\">#Before resetting to the top of the loop, </span></span><br><span class=\"line\"><span class=\"comment\">#we can calculate the proportion of survivors for this particular </span></span><br><span class=\"line\"><span class=\"comment\">#combination of criteria and record it to our survival table</span></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#官方示例中将概率大于0.5的视为生还，这里我们略过</span></span><br><span class=\"line\"><span class=\"comment\">#直接打印详细概率</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &lt; 0.5 ] = 0</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &gt;= 0.5 ] = 1 </span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#Then we can make the prediction</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> test_file_object:                  <span class=\"comment\"># We are going to loop</span></span><br><span class=\"line\">                                              <span class=\"comment\"># through each passenger</span></span><br><span class=\"line\">                                              <span class=\"comment\"># in the test set                     </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   <span class=\"comment\"># For each passenger we</span></span><br><span class=\"line\">                                              <span class=\"comment\"># loop thro each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                      <span class=\"comment\"># Some passengers have no</span></span><br><span class=\"line\">                                              <span class=\"comment\"># Fare data so try to make</span></span><br><span class=\"line\">      row[<span class=\"number\">8</span>] = float(row[<span class=\"number\">8</span>])                  <span class=\"comment\"># a float</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   <span class=\"comment\"># If fails: no data, so </span></span><br><span class=\"line\">      bin_fare = <span class=\"number\">3</span> - float(row[<span class=\"number\">1</span>])            <span class=\"comment\"># bin the fare according Pclass</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># Break from the loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt; fare_ceiling:              \t  <span class=\"comment\"># If there is data see if</span></span><br><span class=\"line\">                                              <span class=\"comment\"># it is greater than fare</span></span><br><span class=\"line\">                                              <span class=\"comment\"># ceiling we set earlier</span></span><br><span class=\"line\">      bin_fare = number_of_price_brackets<span class=\"number\">-1</span>   <span class=\"comment\"># If so set to highest bin</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># And then break loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt;= j * fare_bracket_size\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">8</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * fare_bracket_size:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                                              <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_fare = j                            <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_age_brackets): </span><br><span class=\"line\">                                             </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                    </span><br><span class=\"line\">                                            </span><br><span class=\"line\">      row[<span class=\"number\">4</span>] = float(row[<span class=\"number\">4</span>])              </span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   </span><br><span class=\"line\">      bin_age = <span class=\"number\">-1</span>      </span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                  </span><br><span class=\"line\">                               </span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">4</span>] &gt;= j * <span class=\"number\">10</span>\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">4</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * <span class=\"number\">10</span>:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                               <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_age = j              <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> row[<span class=\"number\">3</span>] == <span class=\"string\">'female'</span>:       <span class=\"comment\">#If the passenger is female</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">0</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">else</span>:                        <span class=\"comment\">#else if male</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">1</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"comment\"># Close out the files.</span></span><br><span class=\"line\">test_file.close() </span><br><span class=\"line\">predictions_file.close()</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>之后买了西瓜书，我把这个例题改成了线性回归模型：<br>假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，<br>我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小<br>用均方误差作为性能度量，均方误差是线性系数的函数<br>对线性系数w求导，可以得到w最优解的闭式</p>\n<p>关键公式是<br>    <strong> <script type=\"math/tex\">w^*=(X^TX)^{-1}X^Ty</script> </strong></p>\n<ul>\n<li>X:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，<br> 假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)</li>\n<li>w:线性系数</li>\n<li>y:生还结果 <script type=\"math/tex\">y=w^T*x</script></li>\n</ul>\n<p>写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TzrD.jpg\" alt=\"i0TzrD.jpg\"></p>\n<p>代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    train1=train.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali1=vali.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">validata=np.array(vali1)</span><br><span class=\"line\">data=np.array(train1)</span><br><span class=\"line\"></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\">data[data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float)&gt;=fare_ceiling,<span class=\"number\">9</span>] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">train = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">predict=np.column_stack((validata[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">survive = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(train.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (train[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((train.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">train=np.concatenate((train,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((predict.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">predict=np.concatenate((predict,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'raw data finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">survive=survive.T.astype(np.float)</span><br><span class=\"line\">traint=train.T.astype(np.float)</span><br><span class=\"line\">w0=traint.dot(train.astype(np.float))</span><br><span class=\"line\">w1=(np.linalg.inv(w0))  </span><br><span class=\"line\">w2=w1.dot(traint)</span><br><span class=\"line\">w=w2.dot(survive)  <span class=\"comment\">#w=(Xt*X)^-1*Xt*y</span></span><br><span class=\"line\">print(<span class=\"string\">'w calc finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Fare'</span>,<span class=\"string\">'Pclass'</span>,<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'b'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,w):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">valipredict_file_object.writerow([<span class=\"string\">\"PassengerName\"</span>, <span class=\"string\">\"Actual Survived\"</span>,<span class=\"string\">\"Predict Survived\"</span>,<span class=\"string\">\"XO\"</span>])</span><br><span class=\"line\">count=<span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    temp=predict[i,<span class=\"number\">0</span>::].T.astype(float)</span><br><span class=\"line\">    answer=temp.dot(w)</span><br><span class=\"line\">    answer=answer[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((answer&gt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">1</span>) <span class=\"keyword\">or</span> (answer&lt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">0</span>)):</span><br><span class=\"line\">        flag=<span class=\"string\">\"Correct\"</span></span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        flag=<span class=\"string\">\"Error\"</span></span><br><span class=\"line\">    valipredict_file_object.writerow([validata[i][<span class=\"number\">3</span>],validata[i][<span class=\"number\">1</span>],answer,flag])</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/predict.shape[<span class=\"number\">0</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"scikit-learn中的多元线性回归\"><a href=\"#scikit-learn中的多元线性回归\" class=\"headerlink\" title=\"scikit-learn中的多元线性回归\"></a>scikit-learn中的多元线性回归</h1><p>试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TxKO.jpg\" alt=\"i0TxKO.jpg\"></p>\n<p>如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07DRx.jpg\" alt=\"i07DRx.jpg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train=train.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>],axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali=vali.dropna(subset=([<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">trainx=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">vali1=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">survive=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Survived'</span>])</span><br><span class=\"line\">survive=np.array(survive)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'Parch'</span>,<span class=\"string\">'Fare'</span>,<span class=\"string\">'Embarked'</span>,<span class=\"string\">'SibSp'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">trainy=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Survived'</span>])  </span><br><span class=\"line\">trainy=trainy.Survived</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(X_train,y_train)</span><br><span class=\"line\">print(model)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,model.coef_):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\">predict=model.predict(vali1)</span><br><span class=\"line\"></span><br><span class=\"line\">count=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(predict)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i]&gt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive[i] == <span class=\"number\">1</span>) <span class=\"keyword\">or</span>  (predict[i]&lt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive [i]== <span class=\"number\">0</span> ):</span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/len(predict)*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p>","more":"<p>2017年2月更新:用pandas重新整理了数据，计算了详细的正确率，试用了scikit-learn中的LinearRegression</p>\n<p>题目介绍在这：<a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener\">Titanic: Machine Learning from Disaster</a></p>\n<p>下面是数据集表格样式，每个人有12个属性</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TjxK.jpg\" alt=\"i0TjxK.jpg\"></p>\n<hr>\n<h1 id=\"不是算法的算法\"><a href=\"#不是算法的算法\" class=\"headerlink\" title=\"不是算法的算法\"></a>不是算法的算法</h1><p>官方示例就是按几个属性分类，比如年龄，性别，票价(…..)<br>然后对每个属性内所有人的生还数据（0或者1）加一起求平均。<br>英文注释都是官方文档的说明<br>我就当入门教程学了，也全打了上去<br>代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">Created on Sun Oct 30 15:28:22 2016</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">@author: thinkwee</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> glue <span class=\"keyword\">import</span> qglue</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_file=(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">test_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>))</span><br><span class=\"line\">testheader = next(test_file_object)</span><br><span class=\"line\">predictions_file = open(<span class=\"string\">r\"文件目录略\"</span>, <span class=\"string\">\"w\"</span>)</span><br><span class=\"line\">predictions_file_object = csv.writer(predictions_file)</span><br><span class=\"line\">p = csv.writer(predictions_file)</span><br><span class=\"line\">p.writerow([<span class=\"string\">\"PassengerId\"</span>, <span class=\"string\">\"Survived\"</span>])</span><br><span class=\"line\">csv_file_object = csv.reader(open(<span class=\"string\">r'文件目录略'</span>, <span class=\"string\">'r'</span>)) </span><br><span class=\"line\">trainheader = next(csv_file_object)  <span class=\"comment\"># The next() command just skips the </span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># first line which is a header</span></span><br><span class=\"line\">data=[]                          \t <span class=\"comment\"># Create a variable called 'data'.</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> csv_file_object:      \t <span class=\"comment\"># Run through each row in the csv file,</span></span><br><span class=\"line\">    data.append(row)             \t <span class=\"comment\"># adding each row to the data variable</span></span><br><span class=\"line\">print(type(data))</span><br><span class=\"line\">data = np.array(data) \t         \t <span class=\"comment\"># Then convert from a list to an array</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># Be aware that each item is currently</span></span><br><span class=\"line\">\t\t\t\t\t\t <span class=\"comment\"># a string in this format</span></span><br><span class=\"line\"></span><br><span class=\"line\">number_passengers = np.size(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">number_survived = np.sum(data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>].astype(np.float))</span><br><span class=\"line\">proportion_survivors = number_survived / number_passengers</span><br><span class=\"line\"></span><br><span class=\"line\">women_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span> <span class=\"comment\"># This finds where all </span></span><br><span class=\"line\">                                           <span class=\"comment\"># the elements in the gender</span></span><br><span class=\"line\">                                           <span class=\"comment\"># column that equals “female”</span></span><br><span class=\"line\">men_only_stats = data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>   <span class=\"comment\"># This finds where all the </span></span><br><span class=\"line\">                                           <span class=\"comment\"># elements do not equal </span></span><br><span class=\"line\">                                           <span class=\"comment\"># female (i.e. male)</span></span><br><span class=\"line\">                                           </span><br><span class=\"line\"><span class=\"comment\"># Using the index from above we select the females and males separately</span></span><br><span class=\"line\">women_onboard = data[women_only_stats,<span class=\"number\">1</span>].astype(np.float)     </span><br><span class=\"line\">men_onboard = data[men_only_stats,<span class=\"number\">1</span>].astype(np.float)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Then we finds the proportions of them that survived</span></span><br><span class=\"line\">proportion_women_survived = \\</span><br><span class=\"line\">                       np.sum(women_onboard) / np.size(women_onboard)  </span><br><span class=\"line\">proportion_men_survived = \\</span><br><span class=\"line\">                       np.sum(men_onboard) / np.size(men_onboard) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># and then print it out</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of women who survived is %s'</span> % proportion_women_survived)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">'Proportion of men who survived is %s'</span> % proportion_men_survived)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The script will systematically will loop through each combination </span></span><br><span class=\"line\"><span class=\"comment\"># and use the 'where' function in python to search the passengers that fit that combination of variables. </span></span><br><span class=\"line\"><span class=\"comment\"># Just like before, you can ask what indices in your data equals female, 1st class, and paid more than $30. </span></span><br><span class=\"line\"><span class=\"comment\"># The problem is that looping through requires bins of equal sizes, i.e. $0-9,  $10-19,  $20-29,  $30-39.  </span></span><br><span class=\"line\"><span class=\"comment\"># For the sake of binning let's say everything equal to and above 40 \"equals\" 39 so it falls in this bin. </span></span><br><span class=\"line\"><span class=\"comment\"># So then you can set the bins</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># So we add a ceiling</span></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling</span></span><br><span class=\"line\">data[ data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float) &gt;= fare_ceiling, <span class=\"number\">9</span> ] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">fare_bracket_size = <span class=\"number\">10</span></span><br><span class=\"line\">number_of_price_brackets = fare_ceiling // fare_bracket_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Take the length of an array of unique values in column index 2</span></span><br><span class=\"line\">number_of_classes = len(np.unique(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">number_of_age_brackets=<span class=\"number\">8</span> </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Initialize the survival table with all zeros</span></span><br><span class=\"line\">survival_table = np.zeros((<span class=\"number\">2</span>, number_of_classes, </span><br><span class=\"line\">\t\t\t\t\t\t   number_of_price_brackets,</span><br><span class=\"line\">\t\t\t\t\t\t   number_of_age_brackets))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Now that these are set up, </span></span><br><span class=\"line\"><span class=\"comment\">#you can loop through each variable </span></span><br><span class=\"line\"><span class=\"comment\">#and find all those passengers that agree with the statements</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(number_of_classes):       \t\t<span class=\"comment\">#loop through each class</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   \t<span class=\"comment\">#loop through each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(number_of_age_brackets):     <span class=\"comment\">#loop through each age bin</span></span><br><span class=\"line\">        women_only_stats_plus = data[                 <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                        (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] == <span class=\"string\">\"female\"</span>)     <span class=\"comment\">#is a female</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                        </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       </span><br><span class=\"line\">                          , <span class=\"number\">1</span>]                        <span class=\"comment\">#in the 2nd col                           </span></span><br><span class=\"line\"> \t\t\t\t\t\t                                    \t\t\t\t\t\t\t\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">        men_only_stats_plus = data[                   <span class=\"comment\">#Which element           </span></span><br><span class=\"line\">                         (data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>] != <span class=\"string\">\"female\"</span>)    <span class=\"comment\">#is a male</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>].astype(np.float) <span class=\"comment\">#and was ith class</span></span><br><span class=\"line\">                             == i+<span class=\"number\">1</span>)                                       </span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#was greater </span></span><br><span class=\"line\">                            &gt;= j*fare_bracket_size)   <span class=\"comment\">#than this bin              </span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">9</span>].astype(np.float)  <span class=\"comment\">#and less than</span></span><br><span class=\"line\">                            &lt; (j+<span class=\"number\">1</span>)*fare_bracket_size)<span class=\"comment\">#the next bin</span></span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&gt;=k*<span class=\"number\">10</span>)</span><br><span class=\"line\">                       &amp;(data[<span class=\"number\">0</span>:,<span class=\"number\">5</span>].astype(np.float)&lt;(k+<span class=\"number\">1</span>)*<span class=\"number\">10</span>)</span><br><span class=\"line\">                          , <span class=\"number\">1</span>]</span><br><span class=\"line\">                          </span><br><span class=\"line\">        survival_table[<span class=\"number\">0</span>,i,j,k] = np.mean(women_only_stats_plus.astype(np.float)) </span><br><span class=\"line\">        survival_table[<span class=\"number\">1</span>,i,j,k] = np.mean(men_only_stats_plus.astype(np.float))</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">#if nan then the type will change to string from float so this sentence can set nan to 0. </span></span><br><span class=\"line\">        survival_table[ survival_table != survival_table ] = <span class=\"number\">0.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Notice that  data[ where function, 1]  means </span></span><br><span class=\"line\"><span class=\"comment\">#it is finding the Survived column for the conditional criteria which is being called. </span></span><br><span class=\"line\"><span class=\"comment\">#As the loop starts with i=0 and j=0, </span></span><br><span class=\"line\"><span class=\"comment\">#the first loop will return the Survived values for all the 1st-class females (i + 1) </span></span><br><span class=\"line\"><span class=\"comment\">#who paid less than 10 ((j+1)*fare_bracket_size) </span></span><br><span class=\"line\"><span class=\"comment\">#and similarly all the 1st-class males who paid less than 10.  </span></span><br><span class=\"line\"><span class=\"comment\">#Before resetting to the top of the loop, </span></span><br><span class=\"line\"><span class=\"comment\">#we can calculate the proportion of survivors for this particular </span></span><br><span class=\"line\"><span class=\"comment\">#combination of criteria and record it to our survival table</span></span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#官方示例中将概率大于0.5的视为生还，这里我们略过</span></span><br><span class=\"line\"><span class=\"comment\">#直接打印详细概率</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &lt; 0.5 ] = 0</span></span><br><span class=\"line\"><span class=\"comment\">#survival_table[ survival_table &gt;= 0.5 ] = 1 </span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#Then we can make the prediction</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> test_file_object:                  <span class=\"comment\"># We are going to loop</span></span><br><span class=\"line\">                                              <span class=\"comment\"># through each passenger</span></span><br><span class=\"line\">                                              <span class=\"comment\"># in the test set                     </span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_price_brackets):   <span class=\"comment\"># For each passenger we</span></span><br><span class=\"line\">                                              <span class=\"comment\"># loop thro each price bin</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                      <span class=\"comment\"># Some passengers have no</span></span><br><span class=\"line\">                                              <span class=\"comment\"># Fare data so try to make</span></span><br><span class=\"line\">      row[<span class=\"number\">8</span>] = float(row[<span class=\"number\">8</span>])                  <span class=\"comment\"># a float</span></span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   <span class=\"comment\"># If fails: no data, so </span></span><br><span class=\"line\">      bin_fare = <span class=\"number\">3</span> - float(row[<span class=\"number\">1</span>])            <span class=\"comment\"># bin the fare according Pclass</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># Break from the loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt; fare_ceiling:              \t  <span class=\"comment\"># If there is data see if</span></span><br><span class=\"line\">                                              <span class=\"comment\"># it is greater than fare</span></span><br><span class=\"line\">                                              <span class=\"comment\"># ceiling we set earlier</span></span><br><span class=\"line\">      bin_fare = number_of_price_brackets<span class=\"number\">-1</span>   <span class=\"comment\"># If so set to highest bin</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                   <span class=\"comment\"># And then break loop</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">8</span>] &gt;= j * fare_bracket_size\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">8</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * fare_bracket_size:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                                              <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_fare = j                            <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(number_of_age_brackets): </span><br><span class=\"line\">                                             </span><br><span class=\"line\">    <span class=\"keyword\">try</span>:                                    </span><br><span class=\"line\">                                            </span><br><span class=\"line\">      row[<span class=\"number\">4</span>] = float(row[<span class=\"number\">4</span>])              </span><br><span class=\"line\">    <span class=\"keyword\">except</span>:                                   </span><br><span class=\"line\">      bin_age = <span class=\"number\">-1</span>      </span><br><span class=\"line\">      <span class=\"keyword\">break</span>                                  </span><br><span class=\"line\">                               </span><br><span class=\"line\">    <span class=\"keyword\">if</span> row[<span class=\"number\">4</span>] &gt;= j * <span class=\"number\">10</span>\\</span><br><span class=\"line\">       <span class=\"keyword\">and</span> row[<span class=\"number\">4</span>] &lt; \\</span><br><span class=\"line\">       (j+<span class=\"number\">1</span>) * <span class=\"number\">10</span>:             <span class=\"comment\"># If passed these tests </span></span><br><span class=\"line\">                               <span class=\"comment\"># then loop through each bin </span></span><br><span class=\"line\">      bin_age = j              <span class=\"comment\"># then assign index</span></span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">if</span> row[<span class=\"number\">3</span>] == <span class=\"string\">'female'</span>:       <span class=\"comment\">#If the passenger is female</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">0</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">else</span>:                        <span class=\"comment\">#else if male</span></span><br><span class=\"line\">        p.writerow([row[<span class=\"number\">0</span>], <span class=\"string\">\"%f %%\"</span> % \\</span><br><span class=\"line\">                   (survival_table[<span class=\"number\">1</span>, int(row[<span class=\"number\">1</span>])<span class=\"number\">-1</span>, bin_fare,bin_age]*<span class=\"number\">100</span>)])</span><br><span class=\"line\">     </span><br><span class=\"line\"><span class=\"comment\"># Close out the files.</span></span><br><span class=\"line\">test_file.close() </span><br><span class=\"line\">predictions_file.close()</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>之后买了西瓜书，我把这个例题改成了线性回归模型：<br>假设每一个人生还可能与这个人的性别，价位，舱位，年龄四个属性成线性关系，<br>我们就利用最小二乘法找到一组线性系数，是所有样本到这个线性函数直线上的距离最小<br>用均方误差作为性能度量，均方误差是线性系数的函数<br>对线性系数w求导，可以得到w最优解的闭式</p>\n<p>关键公式是<br>    <strong> <script type=\"math/tex\">w^*=(X^TX)^{-1}X^Ty</script> </strong></p>\n<ul>\n<li>X:数据集矩阵，每一行对应一个人的数据，每一行最后添加一个1，<br> 假如训练集有m个人，n个属性，则矩阵大小为m*(n+1)</li>\n<li>w:线性系数</li>\n<li>y:生还结果 <script type=\"math/tex\">y=w^T*x</script></li>\n</ul>\n<p>写的时候把年龄中缺失值全删除了，官方给了891条数据，我分了193条用于验证计算正确率，最后正确率是75.155280 %</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TzrD.jpg\" alt=\"i0TzrD.jpg\"></p>\n<p>代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    train1=train.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali1=vali.dropna(subset=([<span class=\"string\">'Age'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">validata=np.array(vali1)</span><br><span class=\"line\">data=np.array(train1)</span><br><span class=\"line\"></span><br><span class=\"line\">fare_ceiling = <span class=\"number\">40</span></span><br><span class=\"line\">data[data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>].astype(np.float)&gt;=fare_ceiling,<span class=\"number\">9</span>] = fare_ceiling - <span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">train = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],data[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">predict=np.column_stack((validata[<span class=\"number\">0</span>::,<span class=\"number\">9</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">2</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">5</span>],validata[<span class=\"number\">0</span>::,<span class=\"number\">4</span>]))</span><br><span class=\"line\">survive = np.column_stack((data[<span class=\"number\">0</span>::,<span class=\"number\">1</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(train.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (train[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        train[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i][<span class=\"number\">3</span>]==<span class=\"string\">'male'</span>):</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">0.00</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        predict[i][<span class=\"number\">3</span>]=<span class=\"number\">1.00</span></span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((train.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">train=np.concatenate((train,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x0=np.ones((predict.shape[<span class=\"number\">0</span>],<span class=\"number\">1</span>))</span><br><span class=\"line\">predict=np.concatenate((predict,x0),axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'raw data finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">survive=survive.T.astype(np.float)</span><br><span class=\"line\">traint=train.T.astype(np.float)</span><br><span class=\"line\">w0=traint.dot(train.astype(np.float))</span><br><span class=\"line\">w1=(np.linalg.inv(w0))  </span><br><span class=\"line\">w2=w1.dot(traint)</span><br><span class=\"line\">w=w2.dot(survive)  <span class=\"comment\">#w=(Xt*X)^-1*Xt*y</span></span><br><span class=\"line\">print(<span class=\"string\">'w calc finish'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Fare'</span>,<span class=\"string\">'Pclass'</span>,<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'b'</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,w):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">valipredict_file_object.writerow([<span class=\"string\">\"PassengerName\"</span>, <span class=\"string\">\"Actual Survived\"</span>,<span class=\"string\">\"Predict Survived\"</span>,<span class=\"string\">\"XO\"</span>])</span><br><span class=\"line\">count=<span class=\"number\">0.0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(predict.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">    temp=predict[i,<span class=\"number\">0</span>::].T.astype(float)</span><br><span class=\"line\">    answer=temp.dot(w)</span><br><span class=\"line\">    answer=answer[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((answer&gt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">1</span>) <span class=\"keyword\">or</span> (answer&lt;<span class=\"number\">0.5</span> <span class=\"keyword\">and</span> validata[i][<span class=\"number\">1</span>]==<span class=\"number\">0</span>)):</span><br><span class=\"line\">        flag=<span class=\"string\">\"Correct\"</span></span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        flag=<span class=\"string\">\"Error\"</span></span><br><span class=\"line\">    valipredict_file_object.writerow([validata[i][<span class=\"number\">3</span>],validata[i][<span class=\"number\">1</span>],answer,flag])</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/predict.shape[<span class=\"number\">0</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"scikit-learn中的多元线性回归\"><a href=\"#scikit-learn中的多元线性回归\" class=\"headerlink\" title=\"scikit-learn中的多元线性回归\"></a>scikit-learn中的多元线性回归</h1><p>试了一下scikit,增加了几个属性，一样的数据，但是好像有些属性不太好，导致正确率下降至64.375000 %</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TxKO.jpg\" alt=\"i0TxKO.jpg\"></p>\n<p>如果再模型的fit阶段出现错误，请检查你fit的x,y数据集是否出现了空元素，无限大元素，或者各个属性的长度不一致，可以用info()做一个概览</p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07DRx.jpg\" alt=\"i07DRx.jpg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train=train.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>],axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">vali=vali.dropna(subset=([<span class=\"string\">'Age'</span>,<span class=\"string\">'Embarked'</span>]),axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">train.loc[train[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">trainx=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"male\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Sex\"</span>]==<span class=\"string\">\"female\"</span>,<span class=\"string\">\"Sex\"</span>]=<span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"S\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"C\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">vali.loc[vali[<span class=\"string\">\"Embarked\"</span>] == <span class=\"string\">\"Q\"</span>, <span class=\"string\">\"Embarked\"</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">vali1=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>]+[<span class=\"string\">'Parch'</span>]+[<span class=\"string\">'Fare'</span>]+[<span class=\"string\">'Embarked'</span>]+[<span class=\"string\">'SibSp'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">survive=vali.reindex(index=vali.index[:],columns=[<span class=\"string\">'Survived'</span>])</span><br><span class=\"line\">survive=np.array(survive)</span><br><span class=\"line\"></span><br><span class=\"line\">feature=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>,<span class=\"string\">'Parch'</span>,<span class=\"string\">'Fare'</span>,<span class=\"string\">'Embarked'</span>,<span class=\"string\">'SibSp'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">trainy=train.reindex(index=train.index[:],columns=[<span class=\"string\">'Survived'</span>])  </span><br><span class=\"line\">trainy=trainy.Survived</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(trainx, trainy, random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model=LinearRegression()</span><br><span class=\"line\">model.fit(X_train,y_train)</span><br><span class=\"line\">print(model)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> zip(feature,model.coef_):</span><br><span class=\"line\">    print(i)</span><br><span class=\"line\"></span><br><span class=\"line\">predict=model.predict(vali1)</span><br><span class=\"line\"></span><br><span class=\"line\">count=<span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(predict)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (predict[i]&gt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive[i] == <span class=\"number\">1</span>) <span class=\"keyword\">or</span>  (predict[i]&lt;<span class=\"number\">1</span> <span class=\"keyword\">and</span> survive [i]== <span class=\"number\">0</span> ):</span><br><span class=\"line\">        count=count+<span class=\"number\">1.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"prediction finish\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"prediction ratio:\"</span>,<span class=\"string\">\"%f %%\"</span>%(count/len(predict)*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0TjxK.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"机器学习入门:Coding基础与线性回归","path":"2017/02/07/TitanicLinearRegression/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0TjxK.jpg","excerpt":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>2016年11月的时候决定开始入坑机器学习<br>首先照着Kaggle上第一个题目《泰坦尼克号生还者分析》的官方示例敲了一遍。</p>","date":"2017-02-07T13:57:22.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"ACL 2019 若干论文阅读","date":"2019-07-28T01:37:46.000Z","mathjax":true,"html":true,"_content":"\n记录一下ACL 2019一些我感兴趣的论文的阅读笔记，先放出最佳长短论文和杰出论文，之后可能筛一些感兴趣的再读读。\n\n<!--more-->\n\n# Bridging the Gap between Training and Inference for Neural Machine Translation\n## Background\n-\t最佳长论文，这个方向就很吸引人，属于很常见，大家都知道但都选择无视，或者找不出优雅有效解法的问题。\n-\t本文试图解决所有seq2seq都会遇到的问题，训练与推理的不一致，即exposure bias。\n-\texposure bias是解码时产生的偏差。正常来讲，我们生成一句话，是从左往右逐字生成，怎么个逐字？模型生成一个字，然后这个字接着输入解码器解码出下一个字，也就是解码出每一个字时使用的上文是之前解码出的句子片段。但是这样训练收敛很慢，容易导致错误的累积。想想模型一开始本来就难以生成正确的字，现在还要基于这个错误的字生成接下来的字，那就是错上加错了。因此一般训练时，都需要使用teacher forcing的方法：forcing模型在生成每一个字的时候，依靠的是训练数据中正确的上文，也就是不管已经生成的字，只管前提正确的情况下去生成正确的字。但是这种技巧只能用于训练，测试的时候没有ground truth来teacher forcing。\n-\t这个问题说大不大，说小不小，之前做summarization也会遇到这个问题，导致训练的反应很好，但是测试效果差，或者出现难以解释的偏差。如今的seq2seq在编码端已经取得了长足的进步，CNN和Transformer等特征抽取器已经摆脱了单向的抽取方式，但是无论什么模型，在解码端，都得老老实实从左往右生成，都避免不了exposure bias。\n-\t对于翻译，exposure bias还和另一个问题打包影响了翻译的质量：逐字计算的交叉熵损失。模型需要学习到在正确的位置生成正确的词，这个双重正确的标准对于翻译来说太过苛刻，模型难以学到灵活的翻译关系，也就是over correction.\n-\t现有的解决exposure bias以及word-level CrossEntrophy Loss的方法有哪些？\n\t-\t在生成词的时候，有时用ground truth，有时用自己的预测的输出，采样中庸一下，即scheduled sampling\n\t-\t使用预训练模型，做Masked Seq2seq pretraining\n\t-\t使用句子级别的损失函数，目标是整个句子的分数最高，而不是逐字贪心，这里包括了各种各样的优化指标以及强化学习的方法，例如mixed incremental cross-entrophy reinforce\n\t-\t其中预训练是比较新的方法，其余两类方法早在2015年就已经提出，作者也把自己的方法与他们的方法做了对比\n\n## Methods\n-\t本文想要解决以上两个问题，粗看思路还是和以前一样：通过从ground truth 和 predicted results中采样来中和偏差，以及使用句子级别的优化指标来放宽损失的约束。\n-\t具体怎么采样？作者给出的方法如下图（这不就是scheduled sampling的图吗。。。。）：\n![e26fV1.png](https://s2.ax1x.com/2019/08/05/e26fV1.png)\n\t-\t先选出oracle word，即模型预测的词：注意，这里用模型预测的词其实不太准确，因为模型预测的词是确定的，是decoder解码出词典概率分布取最大得到的，然而这里的oracle应该表述为not ground truth，即非真实词。假如我们直接用预测的词，那就会错上加错；假如我们用ground truth，那就会有exposure bias。因此作者取了个折中，不同于之前概率上的折中（可能取预测词可能取ground truth），还做了选词上的优化，不是简单的拿预测出的词作为oracle，具体而言：\n\t\t-\t假如直接取decoder预测概率最大的词作为Oracle,那就是普通的scheduled sampling。\n\t\t-\t然而作者使用Gumbel-Max正则化方法对预测概率分布调整，引入两个参数：一个由01均匀分布变量$u$计算得来的Gumbel noise $\\eta$；以及一个温度变量$\\gamma$。假设原始概率分布为$o$，则调整后的概率分布$P$为\n\t\t$$\n\t\t\t\\eta = - \\log ( - \\log u) \\\\\n\t\t\t\\overline{o} _{j-1} = (o_{j-1} + \\eta) / \\gamma \\\\\n\t\t\t\\overline{P} _{j-1} = softmax(\\overline{o} _{j-1}) \\\\\n\t\t$$\n\t\t-\t这个加入噪音的过程只影响选择 oracle，而不影响模型的损失\n\t-\t这是单词级别的oracle选择，还可以做句子级别的选择，具体做法是\n\t\t-\t先用单词级别的方法，加上beam search，选出几个候选句\n\t\t-\t通过BLEU，ROUGE等指标选出最好的句子，将这个句子的每一个词作为oracle\n\t\t-\t显然这里有一个问题，就是得保证beam search出的oracle句子和ground truth的句子长度一致，作者引入了force decoding，当解码出的句子还不够ground truth长度时，假如解码出了EOS，就排除EOS，取剩下的概率最大前k个单词做beam search；假如长度已经够了，但是还没解码出EOS，就强制设置为EOS并结束解码\n\t-\t再计算概率，决定是用oracle还是ground truth：和scheduled sampling一样，也是要设置动态采样概率，刚开始训练的时候多用ground truth，然后慢慢提高oracle的比例，作者给出的概率设置为：\n\t$$\n\tp = \\frac{\\mu}{\\mu + exp(e / \\mu)} \n\t$$\n-\t结果当然是比naive RNN and Transformer要好，BLEU能有2个点的提升。作者也做了大量实验来测试超参数的影响。方法感觉不是很亮眼，引入Gumbel噪声的动机也没有说，但是很简单很work，尤其是引入句子层级优化的方法简单明了，比一堆目标函数的改动要直观的多。\n\n# Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment\n-\t最佳短论文，研究了一个非常有意思的方向：speaker commitment，叫说话人承诺，或者叫事件事实。这是一项自然语言理解任务，就像幽默、讽刺一样。\n-\t说话人承诺是指，通过说话人的描述，来判断某一事件是否发生，具体而言分三类：事实、非事实、不确定。模型需要从说话人的描述当中挖掘出事件的事实状态。传统的方法关注情态动词、动词短语，但作者引入了CommitmentBank数据集来测试各种已有模型，说明已有的数据集不能捕捉自然语言，尤其是口语当中的词法和句法多样性，且发现引入语言学知识的模型要优于LSTM，为深度学习树立了另一个有待攻克的目标。\n-\t举个例子来形象说明一下说话人承诺问题，“我从没相信我会研究NLP”，“我不相信我可以研究NLP”，两句话都有“相信”作为动词，且都具有否定词“从没”、“不”，那么事件是“我研究NLP”，这个事件究竟有没有发生？显然前者倾向于事件已经发生，而后者倾向于事件还未发生。还有更复杂的情形，例如给定辩论双方的陈述，猜测双方讨论的某一事实是否发生。一般而言每一条样本还会有上下文，说话人承诺任务就是给定上下文、说话人表述和事件，判断事件是否是事实。\n-\t作者在CommitmentBank数据集上测试了两个模型：基于规则的和基于神经网络的\n\t-\t基于规则：Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets。 基于语言学的知识即人为给各种谓语词语/短语打上事实分数，找到谓语的隐藏签名，并根据句法树剖析来联系上形容词和情态动词，进行分数的增强或者反转，最后将各种人类知识库得分和句法结构作为特征输入SVM回归模型，计算出分数\n\t-\t基于神经网络：Neural models of factuality。使用多层双向LSTM和tree-LSTM对句子建模，然后过一个多层MLP计算出回归分数。作者测试了双向、树、集成三种模型。\n-\t文章的主要部分在结果分析，数据展示很丰富，但是作者也没有给出过多的原因分析，只是在陈述哪类事实、哪类状态、哪类语料、哪类情态下哪类模型表现更好。可能是我不做这方面工作，没有感受到从这些结论里能有哪些可以挖掘的研究点。最后得出总的结论，人类知识具有更强的泛化能力，深度模型需要整合人类知识，也只是一个很泛的结论。\n-\t至少这篇论文得了奖，表明学界还是希望NLP研究具有多样性，像这样具有挑战性的任务并不会有太多人做，但做好之后能给下游任务例如信息抽取、对话以极大的提升。\n\n# A Simple Theoretical Model of Importance for Summarization\n\n# Zero-Shot Entity Linking by Reading Entity Descriptions","source":"_posts/acl2019.md","raw":"---\ntitle: ACL 2019 若干论文阅读\ndate: 2019-07-28 09:37:46\ncategories: 自然语言处理\ntags:\n  - acl\n  - machine learning\n  -\tnatural language processing\nmathjax: true\nhtml: true\n---\n\n记录一下ACL 2019一些我感兴趣的论文的阅读笔记，先放出最佳长短论文和杰出论文，之后可能筛一些感兴趣的再读读。\n\n<!--more-->\n\n# Bridging the Gap between Training and Inference for Neural Machine Translation\n## Background\n-\t最佳长论文，这个方向就很吸引人，属于很常见，大家都知道但都选择无视，或者找不出优雅有效解法的问题。\n-\t本文试图解决所有seq2seq都会遇到的问题，训练与推理的不一致，即exposure bias。\n-\texposure bias是解码时产生的偏差。正常来讲，我们生成一句话，是从左往右逐字生成，怎么个逐字？模型生成一个字，然后这个字接着输入解码器解码出下一个字，也就是解码出每一个字时使用的上文是之前解码出的句子片段。但是这样训练收敛很慢，容易导致错误的累积。想想模型一开始本来就难以生成正确的字，现在还要基于这个错误的字生成接下来的字，那就是错上加错了。因此一般训练时，都需要使用teacher forcing的方法：forcing模型在生成每一个字的时候，依靠的是训练数据中正确的上文，也就是不管已经生成的字，只管前提正确的情况下去生成正确的字。但是这种技巧只能用于训练，测试的时候没有ground truth来teacher forcing。\n-\t这个问题说大不大，说小不小，之前做summarization也会遇到这个问题，导致训练的反应很好，但是测试效果差，或者出现难以解释的偏差。如今的seq2seq在编码端已经取得了长足的进步，CNN和Transformer等特征抽取器已经摆脱了单向的抽取方式，但是无论什么模型，在解码端，都得老老实实从左往右生成，都避免不了exposure bias。\n-\t对于翻译，exposure bias还和另一个问题打包影响了翻译的质量：逐字计算的交叉熵损失。模型需要学习到在正确的位置生成正确的词，这个双重正确的标准对于翻译来说太过苛刻，模型难以学到灵活的翻译关系，也就是over correction.\n-\t现有的解决exposure bias以及word-level CrossEntrophy Loss的方法有哪些？\n\t-\t在生成词的时候，有时用ground truth，有时用自己的预测的输出，采样中庸一下，即scheduled sampling\n\t-\t使用预训练模型，做Masked Seq2seq pretraining\n\t-\t使用句子级别的损失函数，目标是整个句子的分数最高，而不是逐字贪心，这里包括了各种各样的优化指标以及强化学习的方法，例如mixed incremental cross-entrophy reinforce\n\t-\t其中预训练是比较新的方法，其余两类方法早在2015年就已经提出，作者也把自己的方法与他们的方法做了对比\n\n## Methods\n-\t本文想要解决以上两个问题，粗看思路还是和以前一样：通过从ground truth 和 predicted results中采样来中和偏差，以及使用句子级别的优化指标来放宽损失的约束。\n-\t具体怎么采样？作者给出的方法如下图（这不就是scheduled sampling的图吗。。。。）：\n![e26fV1.png](https://s2.ax1x.com/2019/08/05/e26fV1.png)\n\t-\t先选出oracle word，即模型预测的词：注意，这里用模型预测的词其实不太准确，因为模型预测的词是确定的，是decoder解码出词典概率分布取最大得到的，然而这里的oracle应该表述为not ground truth，即非真实词。假如我们直接用预测的词，那就会错上加错；假如我们用ground truth，那就会有exposure bias。因此作者取了个折中，不同于之前概率上的折中（可能取预测词可能取ground truth），还做了选词上的优化，不是简单的拿预测出的词作为oracle，具体而言：\n\t\t-\t假如直接取decoder预测概率最大的词作为Oracle,那就是普通的scheduled sampling。\n\t\t-\t然而作者使用Gumbel-Max正则化方法对预测概率分布调整，引入两个参数：一个由01均匀分布变量$u$计算得来的Gumbel noise $\\eta$；以及一个温度变量$\\gamma$。假设原始概率分布为$o$，则调整后的概率分布$P$为\n\t\t$$\n\t\t\t\\eta = - \\log ( - \\log u) \\\\\n\t\t\t\\overline{o} _{j-1} = (o_{j-1} + \\eta) / \\gamma \\\\\n\t\t\t\\overline{P} _{j-1} = softmax(\\overline{o} _{j-1}) \\\\\n\t\t$$\n\t\t-\t这个加入噪音的过程只影响选择 oracle，而不影响模型的损失\n\t-\t这是单词级别的oracle选择，还可以做句子级别的选择，具体做法是\n\t\t-\t先用单词级别的方法，加上beam search，选出几个候选句\n\t\t-\t通过BLEU，ROUGE等指标选出最好的句子，将这个句子的每一个词作为oracle\n\t\t-\t显然这里有一个问题，就是得保证beam search出的oracle句子和ground truth的句子长度一致，作者引入了force decoding，当解码出的句子还不够ground truth长度时，假如解码出了EOS，就排除EOS，取剩下的概率最大前k个单词做beam search；假如长度已经够了，但是还没解码出EOS，就强制设置为EOS并结束解码\n\t-\t再计算概率，决定是用oracle还是ground truth：和scheduled sampling一样，也是要设置动态采样概率，刚开始训练的时候多用ground truth，然后慢慢提高oracle的比例，作者给出的概率设置为：\n\t$$\n\tp = \\frac{\\mu}{\\mu + exp(e / \\mu)} \n\t$$\n-\t结果当然是比naive RNN and Transformer要好，BLEU能有2个点的提升。作者也做了大量实验来测试超参数的影响。方法感觉不是很亮眼，引入Gumbel噪声的动机也没有说，但是很简单很work，尤其是引入句子层级优化的方法简单明了，比一堆目标函数的改动要直观的多。\n\n# Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment\n-\t最佳短论文，研究了一个非常有意思的方向：speaker commitment，叫说话人承诺，或者叫事件事实。这是一项自然语言理解任务，就像幽默、讽刺一样。\n-\t说话人承诺是指，通过说话人的描述，来判断某一事件是否发生，具体而言分三类：事实、非事实、不确定。模型需要从说话人的描述当中挖掘出事件的事实状态。传统的方法关注情态动词、动词短语，但作者引入了CommitmentBank数据集来测试各种已有模型，说明已有的数据集不能捕捉自然语言，尤其是口语当中的词法和句法多样性，且发现引入语言学知识的模型要优于LSTM，为深度学习树立了另一个有待攻克的目标。\n-\t举个例子来形象说明一下说话人承诺问题，“我从没相信我会研究NLP”，“我不相信我可以研究NLP”，两句话都有“相信”作为动词，且都具有否定词“从没”、“不”，那么事件是“我研究NLP”，这个事件究竟有没有发生？显然前者倾向于事件已经发生，而后者倾向于事件还未发生。还有更复杂的情形，例如给定辩论双方的陈述，猜测双方讨论的某一事实是否发生。一般而言每一条样本还会有上下文，说话人承诺任务就是给定上下文、说话人表述和事件，判断事件是否是事实。\n-\t作者在CommitmentBank数据集上测试了两个模型：基于规则的和基于神经网络的\n\t-\t基于规则：Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets。 基于语言学的知识即人为给各种谓语词语/短语打上事实分数，找到谓语的隐藏签名，并根据句法树剖析来联系上形容词和情态动词，进行分数的增强或者反转，最后将各种人类知识库得分和句法结构作为特征输入SVM回归模型，计算出分数\n\t-\t基于神经网络：Neural models of factuality。使用多层双向LSTM和tree-LSTM对句子建模，然后过一个多层MLP计算出回归分数。作者测试了双向、树、集成三种模型。\n-\t文章的主要部分在结果分析，数据展示很丰富，但是作者也没有给出过多的原因分析，只是在陈述哪类事实、哪类状态、哪类语料、哪类情态下哪类模型表现更好。可能是我不做这方面工作，没有感受到从这些结论里能有哪些可以挖掘的研究点。最后得出总的结论，人类知识具有更强的泛化能力，深度模型需要整合人类知识，也只是一个很泛的结论。\n-\t至少这篇论文得了奖，表明学界还是希望NLP研究具有多样性，像这样具有挑战性的任务并不会有太多人做，但做好之后能给下游任务例如信息抽取、对话以极大的提升。\n\n# A Simple Theoretical Model of Importance for Summarization\n\n# Zero-Shot Entity Linking by Reading Entity Descriptions","slug":"acl2019","published":1,"updated":"2019-08-06T08:52:29.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v3j001mq8t5c22youiv","content":"<p>记录一下ACL 2019一些我感兴趣的论文的阅读笔记，先放出最佳长短论文和杰出论文，之后可能筛一些感兴趣的再读读。</p>\n<a id=\"more\"></a>\n<h1 id=\"Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation\"><a href=\"#Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation\" class=\"headerlink\" title=\"Bridging the Gap between Training and Inference for Neural Machine Translation\"></a>Bridging the Gap between Training and Inference for Neural Machine Translation</h1><h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>最佳长论文，这个方向就很吸引人，属于很常见，大家都知道但都选择无视，或者找不出优雅有效解法的问题。</li>\n<li>本文试图解决所有seq2seq都会遇到的问题，训练与推理的不一致，即exposure bias。</li>\n<li>exposure bias是解码时产生的偏差。正常来讲，我们生成一句话，是从左往右逐字生成，怎么个逐字？模型生成一个字，然后这个字接着输入解码器解码出下一个字，也就是解码出每一个字时使用的上文是之前解码出的句子片段。但是这样训练收敛很慢，容易导致错误的累积。想想模型一开始本来就难以生成正确的字，现在还要基于这个错误的字生成接下来的字，那就是错上加错了。因此一般训练时，都需要使用teacher forcing的方法：forcing模型在生成每一个字的时候，依靠的是训练数据中正确的上文，也就是不管已经生成的字，只管前提正确的情况下去生成正确的字。但是这种技巧只能用于训练，测试的时候没有ground truth来teacher forcing。</li>\n<li>这个问题说大不大，说小不小，之前做summarization也会遇到这个问题，导致训练的反应很好，但是测试效果差，或者出现难以解释的偏差。如今的seq2seq在编码端已经取得了长足的进步，CNN和Transformer等特征抽取器已经摆脱了单向的抽取方式，但是无论什么模型，在解码端，都得老老实实从左往右生成，都避免不了exposure bias。</li>\n<li>对于翻译，exposure bias还和另一个问题打包影响了翻译的质量：逐字计算的交叉熵损失。模型需要学习到在正确的位置生成正确的词，这个双重正确的标准对于翻译来说太过苛刻，模型难以学到灵活的翻译关系，也就是over correction.</li>\n<li>现有的解决exposure bias以及word-level CrossEntrophy Loss的方法有哪些？<ul>\n<li>在生成词的时候，有时用ground truth，有时用自己的预测的输出，采样中庸一下，即scheduled sampling</li>\n<li>使用预训练模型，做Masked Seq2seq pretraining</li>\n<li>使用句子级别的损失函数，目标是整个句子的分数最高，而不是逐字贪心，这里包括了各种各样的优化指标以及强化学习的方法，例如mixed incremental cross-entrophy reinforce</li>\n<li>其中预训练是比较新的方法，其余两类方法早在2015年就已经提出，作者也把自己的方法与他们的方法做了对比</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li>本文想要解决以上两个问题，粗看思路还是和以前一样：通过从ground truth 和 predicted results中采样来中和偏差，以及使用句子级别的优化指标来放宽损失的约束。</li>\n<li>具体怎么采样？作者给出的方法如下图（这不就是scheduled sampling的图吗。。。。）：<br><img src=\"https://s2.ax1x.com/2019/08/05/e26fV1.png\" alt=\"e26fV1.png\"><ul>\n<li>先选出oracle word，即模型预测的词：注意，这里用模型预测的词其实不太准确，因为模型预测的词是确定的，是decoder解码出词典概率分布取最大得到的，然而这里的oracle应该表述为not ground truth，即非真实词。假如我们直接用预测的词，那就会错上加错；假如我们用ground truth，那就会有exposure bias。因此作者取了个折中，不同于之前概率上的折中（可能取预测词可能取ground truth），还做了选词上的优化，不是简单的拿预测出的词作为oracle，具体而言：<ul>\n<li>假如直接取decoder预测概率最大的词作为Oracle,那就是普通的scheduled sampling。</li>\n<li>然而作者使用Gumbel-Max正则化方法对预测概率分布调整，引入两个参数：一个由01均匀分布变量$u$计算得来的Gumbel noise $\\eta$；以及一个温度变量$\\gamma$。假设原始概率分布为$o$，则调整后的概率分布$P$为<script type=\"math/tex; mode=display\">\n\\eta = - \\log ( - \\log u) \\\\\n\\overline{o} _{j-1} = (o_{j-1} + \\eta) / \\gamma \\\\\n\\overline{P} _{j-1} = softmax(\\overline{o} _{j-1}) \\\\</script></li>\n<li>这个加入噪音的过程只影响选择 oracle，而不影响模型的损失</li>\n</ul>\n</li>\n<li>这是单词级别的oracle选择，还可以做句子级别的选择，具体做法是<ul>\n<li>先用单词级别的方法，加上beam search，选出几个候选句</li>\n<li>通过BLEU，ROUGE等指标选出最好的句子，将这个句子的每一个词作为oracle</li>\n<li>显然这里有一个问题，就是得保证beam search出的oracle句子和ground truth的句子长度一致，作者引入了force decoding，当解码出的句子还不够ground truth长度时，假如解码出了EOS，就排除EOS，取剩下的概率最大前k个单词做beam search；假如长度已经够了，但是还没解码出EOS，就强制设置为EOS并结束解码</li>\n</ul>\n</li>\n<li>再计算概率，决定是用oracle还是ground truth：和scheduled sampling一样，也是要设置动态采样概率，刚开始训练的时候多用ground truth，然后慢慢提高oracle的比例，作者给出的概率设置为：<script type=\"math/tex; mode=display\">\np = \\frac{\\mu}{\\mu + exp(e / \\mu)}</script></li>\n</ul>\n</li>\n<li>结果当然是比naive RNN and Transformer要好，BLEU能有2个点的提升。作者也做了大量实验来测试超参数的影响。方法感觉不是很亮眼，引入Gumbel噪声的动机也没有说，但是很简单很work，尤其是引入句子层级优化的方法简单明了，比一堆目标函数的改动要直观的多。</li>\n</ul>\n<h1 id=\"Do-you-know-that-Florence-is-packed-with-visitors-Evaluating-state-of-the-art-models-of-speaker-commitment\"><a href=\"#Do-you-know-that-Florence-is-packed-with-visitors-Evaluating-state-of-the-art-models-of-speaker-commitment\" class=\"headerlink\" title=\"Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment\"></a>Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment</h1><ul>\n<li>最佳短论文，研究了一个非常有意思的方向：speaker commitment，叫说话人承诺，或者叫事件事实。这是一项自然语言理解任务，就像幽默、讽刺一样。</li>\n<li>说话人承诺是指，通过说话人的描述，来判断某一事件是否发生，具体而言分三类：事实、非事实、不确定。模型需要从说话人的描述当中挖掘出事件的事实状态。传统的方法关注情态动词、动词短语，但作者引入了CommitmentBank数据集来测试各种已有模型，说明已有的数据集不能捕捉自然语言，尤其是口语当中的词法和句法多样性，且发现引入语言学知识的模型要优于LSTM，为深度学习树立了另一个有待攻克的目标。</li>\n<li>举个例子来形象说明一下说话人承诺问题，“我从没相信我会研究NLP”，“我不相信我可以研究NLP”，两句话都有“相信”作为动词，且都具有否定词“从没”、“不”，那么事件是“我研究NLP”，这个事件究竟有没有发生？显然前者倾向于事件已经发生，而后者倾向于事件还未发生。还有更复杂的情形，例如给定辩论双方的陈述，猜测双方讨论的某一事实是否发生。一般而言每一条样本还会有上下文，说话人承诺任务就是给定上下文、说话人表述和事件，判断事件是否是事实。</li>\n<li>作者在CommitmentBank数据集上测试了两个模型：基于规则的和基于神经网络的<ul>\n<li>基于规则：Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets。 基于语言学的知识即人为给各种谓语词语/短语打上事实分数，找到谓语的隐藏签名，并根据句法树剖析来联系上形容词和情态动词，进行分数的增强或者反转，最后将各种人类知识库得分和句法结构作为特征输入SVM回归模型，计算出分数</li>\n<li>基于神经网络：Neural models of factuality。使用多层双向LSTM和tree-LSTM对句子建模，然后过一个多层MLP计算出回归分数。作者测试了双向、树、集成三种模型。</li>\n</ul>\n</li>\n<li>文章的主要部分在结果分析，数据展示很丰富，但是作者也没有给出过多的原因分析，只是在陈述哪类事实、哪类状态、哪类语料、哪类情态下哪类模型表现更好。可能是我不做这方面工作，没有感受到从这些结论里能有哪些可以挖掘的研究点。最后得出总的结论，人类知识具有更强的泛化能力，深度模型需要整合人类知识，也只是一个很泛的结论。</li>\n<li>至少这篇论文得了奖，表明学界还是希望NLP研究具有多样性，像这样具有挑战性的任务并不会有太多人做，但做好之后能给下游任务例如信息抽取、对话以极大的提升。</li>\n</ul>\n<h1 id=\"A-Simple-Theoretical-Model-of-Importance-for-Summarization\"><a href=\"#A-Simple-Theoretical-Model-of-Importance-for-Summarization\" class=\"headerlink\" title=\"A Simple Theoretical Model of Importance for Summarization\"></a>A Simple Theoretical Model of Importance for Summarization</h1><h1 id=\"Zero-Shot-Entity-Linking-by-Reading-Entity-Descriptions\"><a href=\"#Zero-Shot-Entity-Linking-by-Reading-Entity-Descriptions\" class=\"headerlink\" title=\"Zero-Shot Entity Linking by Reading Entity Descriptions\"></a>Zero-Shot Entity Linking by Reading Entity Descriptions</h1>","site":{"data":{}},"excerpt":"<p>记录一下ACL 2019一些我感兴趣的论文的阅读笔记，先放出最佳长短论文和杰出论文，之后可能筛一些感兴趣的再读读。</p>","more":"<h1 id=\"Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation\"><a href=\"#Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation\" class=\"headerlink\" title=\"Bridging the Gap between Training and Inference for Neural Machine Translation\"></a>Bridging the Gap between Training and Inference for Neural Machine Translation</h1><h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><ul>\n<li>最佳长论文，这个方向就很吸引人，属于很常见，大家都知道但都选择无视，或者找不出优雅有效解法的问题。</li>\n<li>本文试图解决所有seq2seq都会遇到的问题，训练与推理的不一致，即exposure bias。</li>\n<li>exposure bias是解码时产生的偏差。正常来讲，我们生成一句话，是从左往右逐字生成，怎么个逐字？模型生成一个字，然后这个字接着输入解码器解码出下一个字，也就是解码出每一个字时使用的上文是之前解码出的句子片段。但是这样训练收敛很慢，容易导致错误的累积。想想模型一开始本来就难以生成正确的字，现在还要基于这个错误的字生成接下来的字，那就是错上加错了。因此一般训练时，都需要使用teacher forcing的方法：forcing模型在生成每一个字的时候，依靠的是训练数据中正确的上文，也就是不管已经生成的字，只管前提正确的情况下去生成正确的字。但是这种技巧只能用于训练，测试的时候没有ground truth来teacher forcing。</li>\n<li>这个问题说大不大，说小不小，之前做summarization也会遇到这个问题，导致训练的反应很好，但是测试效果差，或者出现难以解释的偏差。如今的seq2seq在编码端已经取得了长足的进步，CNN和Transformer等特征抽取器已经摆脱了单向的抽取方式，但是无论什么模型，在解码端，都得老老实实从左往右生成，都避免不了exposure bias。</li>\n<li>对于翻译，exposure bias还和另一个问题打包影响了翻译的质量：逐字计算的交叉熵损失。模型需要学习到在正确的位置生成正确的词，这个双重正确的标准对于翻译来说太过苛刻，模型难以学到灵活的翻译关系，也就是over correction.</li>\n<li>现有的解决exposure bias以及word-level CrossEntrophy Loss的方法有哪些？<ul>\n<li>在生成词的时候，有时用ground truth，有时用自己的预测的输出，采样中庸一下，即scheduled sampling</li>\n<li>使用预训练模型，做Masked Seq2seq pretraining</li>\n<li>使用句子级别的损失函数，目标是整个句子的分数最高，而不是逐字贪心，这里包括了各种各样的优化指标以及强化学习的方法，例如mixed incremental cross-entrophy reinforce</li>\n<li>其中预训练是比较新的方法，其余两类方法早在2015年就已经提出，作者也把自己的方法与他们的方法做了对比</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h2><ul>\n<li>本文想要解决以上两个问题，粗看思路还是和以前一样：通过从ground truth 和 predicted results中采样来中和偏差，以及使用句子级别的优化指标来放宽损失的约束。</li>\n<li>具体怎么采样？作者给出的方法如下图（这不就是scheduled sampling的图吗。。。。）：<br><img src=\"https://s2.ax1x.com/2019/08/05/e26fV1.png\" alt=\"e26fV1.png\"><ul>\n<li>先选出oracle word，即模型预测的词：注意，这里用模型预测的词其实不太准确，因为模型预测的词是确定的，是decoder解码出词典概率分布取最大得到的，然而这里的oracle应该表述为not ground truth，即非真实词。假如我们直接用预测的词，那就会错上加错；假如我们用ground truth，那就会有exposure bias。因此作者取了个折中，不同于之前概率上的折中（可能取预测词可能取ground truth），还做了选词上的优化，不是简单的拿预测出的词作为oracle，具体而言：<ul>\n<li>假如直接取decoder预测概率最大的词作为Oracle,那就是普通的scheduled sampling。</li>\n<li>然而作者使用Gumbel-Max正则化方法对预测概率分布调整，引入两个参数：一个由01均匀分布变量$u$计算得来的Gumbel noise $\\eta$；以及一个温度变量$\\gamma$。假设原始概率分布为$o$，则调整后的概率分布$P$为<script type=\"math/tex; mode=display\">\n\\eta = - \\log ( - \\log u) \\\\\n\\overline{o} _{j-1} = (o_{j-1} + \\eta) / \\gamma \\\\\n\\overline{P} _{j-1} = softmax(\\overline{o} _{j-1}) \\\\</script></li>\n<li>这个加入噪音的过程只影响选择 oracle，而不影响模型的损失</li>\n</ul>\n</li>\n<li>这是单词级别的oracle选择，还可以做句子级别的选择，具体做法是<ul>\n<li>先用单词级别的方法，加上beam search，选出几个候选句</li>\n<li>通过BLEU，ROUGE等指标选出最好的句子，将这个句子的每一个词作为oracle</li>\n<li>显然这里有一个问题，就是得保证beam search出的oracle句子和ground truth的句子长度一致，作者引入了force decoding，当解码出的句子还不够ground truth长度时，假如解码出了EOS，就排除EOS，取剩下的概率最大前k个单词做beam search；假如长度已经够了，但是还没解码出EOS，就强制设置为EOS并结束解码</li>\n</ul>\n</li>\n<li>再计算概率，决定是用oracle还是ground truth：和scheduled sampling一样，也是要设置动态采样概率，刚开始训练的时候多用ground truth，然后慢慢提高oracle的比例，作者给出的概率设置为：<script type=\"math/tex; mode=display\">\np = \\frac{\\mu}{\\mu + exp(e / \\mu)}</script></li>\n</ul>\n</li>\n<li>结果当然是比naive RNN and Transformer要好，BLEU能有2个点的提升。作者也做了大量实验来测试超参数的影响。方法感觉不是很亮眼，引入Gumbel噪声的动机也没有说，但是很简单很work，尤其是引入句子层级优化的方法简单明了，比一堆目标函数的改动要直观的多。</li>\n</ul>\n<h1 id=\"Do-you-know-that-Florence-is-packed-with-visitors-Evaluating-state-of-the-art-models-of-speaker-commitment\"><a href=\"#Do-you-know-that-Florence-is-packed-with-visitors-Evaluating-state-of-the-art-models-of-speaker-commitment\" class=\"headerlink\" title=\"Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment\"></a>Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment</h1><ul>\n<li>最佳短论文，研究了一个非常有意思的方向：speaker commitment，叫说话人承诺，或者叫事件事实。这是一项自然语言理解任务，就像幽默、讽刺一样。</li>\n<li>说话人承诺是指，通过说话人的描述，来判断某一事件是否发生，具体而言分三类：事实、非事实、不确定。模型需要从说话人的描述当中挖掘出事件的事实状态。传统的方法关注情态动词、动词短语，但作者引入了CommitmentBank数据集来测试各种已有模型，说明已有的数据集不能捕捉自然语言，尤其是口语当中的词法和句法多样性，且发现引入语言学知识的模型要优于LSTM，为深度学习树立了另一个有待攻克的目标。</li>\n<li>举个例子来形象说明一下说话人承诺问题，“我从没相信我会研究NLP”，“我不相信我可以研究NLP”，两句话都有“相信”作为动词，且都具有否定词“从没”、“不”，那么事件是“我研究NLP”，这个事件究竟有没有发生？显然前者倾向于事件已经发生，而后者倾向于事件还未发生。还有更复杂的情形，例如给定辩论双方的陈述，猜测双方讨论的某一事实是否发生。一般而言每一条样本还会有上下文，说话人承诺任务就是给定上下文、说话人表述和事件，判断事件是否是事实。</li>\n<li>作者在CommitmentBank数据集上测试了两个模型：基于规则的和基于神经网络的<ul>\n<li>基于规则：Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets。 基于语言学的知识即人为给各种谓语词语/短语打上事实分数，找到谓语的隐藏签名，并根据句法树剖析来联系上形容词和情态动词，进行分数的增强或者反转，最后将各种人类知识库得分和句法结构作为特征输入SVM回归模型，计算出分数</li>\n<li>基于神经网络：Neural models of factuality。使用多层双向LSTM和tree-LSTM对句子建模，然后过一个多层MLP计算出回归分数。作者测试了双向、树、集成三种模型。</li>\n</ul>\n</li>\n<li>文章的主要部分在结果分析，数据展示很丰富，但是作者也没有给出过多的原因分析，只是在陈述哪类事实、哪类状态、哪类语料、哪类情态下哪类模型表现更好。可能是我不做这方面工作，没有感受到从这些结论里能有哪些可以挖掘的研究点。最后得出总的结论，人类知识具有更强的泛化能力，深度模型需要整合人类知识，也只是一个很泛的结论。</li>\n<li>至少这篇论文得了奖，表明学界还是希望NLP研究具有多样性，像这样具有挑战性的任务并不会有太多人做，但做好之后能给下游任务例如信息抽取、对话以极大的提升。</li>\n</ul>\n<h1 id=\"A-Simple-Theoretical-Model-of-Importance-for-Summarization\"><a href=\"#A-Simple-Theoretical-Model-of-Importance-for-Summarization\" class=\"headerlink\" title=\"A Simple Theoretical Model of Importance for Summarization\"></a>A Simple Theoretical Model of Importance for Summarization</h1><h1 id=\"Zero-Shot-Entity-Linking-by-Reading-Entity-Descriptions\"><a href=\"#Zero-Shot-Entity-Linking-by-Reading-Entity-Descriptions\" class=\"headerlink\" title=\"Zero-Shot Entity Linking by Reading Entity Descriptions\"></a>Zero-Shot Entity Linking by Reading Entity Descriptions</h1>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s2.ax1x.com/2019/08/05/e26fV1.png","popularPost_tmp_gaData":{"updated":"Tue Aug 06 2019 16:52:29 GMT+0800 (GMT+08:00)","title":"ACL 2019 若干论文阅读","path":"2019/07/28/acl2019/","eyeCatchImage":"https://s2.ax1x.com/2019/08/05/e26fV1.png","excerpt":"<p>记录一下ACL 2019一些我感兴趣的论文的阅读笔记，先放出最佳长短论文和杰出论文，之后可能筛一些感兴趣的再读读。</p>","date":"2019-07-28T01:37:46.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["machine learning","acl","natural language processing"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Android:BuptRoom总结","date":"2017-01-16T03:56:39.000Z","_content":"***\n\n# 简介\n写了一个查询学校空闲教室的APP\n拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的\n毕竟第一次写android，什么都想尝试一下\n点这下载：[BuptRoom](https://fir.im/buptroom)\nrepository地址:[一个简单的北邮自习室查询系统](https://github.com/thinkwee/BuptRoom)\n完成第一个版本大概是3个周末\n之后花了1个月陆陆续续更新了杂七杂八的\n很多东西写的不规范，也是临时查到了就用上\n总结一下这次写App的经过:\n\n<!--more-->\n\n![i0IHL4.png](https://s1.ax1x.com/2018/10/20/i0IHL4.png)\n\n# 学习的内容\n-\tAndroid基本架构，组件，生命周期\n-\tFragment的使用\n-\tJava库与库之间的调用\n-\tGithub的使用\n-\t部署app\n-\t图像处理的一些方法\n-\t一个愚蠢的拉取网页内容的方式\n-\tGitHub第三方库的利用\n-\t颜色方面的知识\n-\tAndroid Material Design\n-\t简单的优化\n-\t多线程与Handler\n\n# 解决的问题\n主要解决了这么几个问题\n\n-\tAndroid6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:\n\n```Java\n    <uses-permission android:name=\"android.permission.INTERNET\"></uses-permission>\n    <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"></uses-permission>\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"></uses-permission>\n```\n\n-\t网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中\n\n```Java\n    final class MyWebViewClient extends WebViewClient {\n        public boolean shouldOverrideUrlLoading(WebView view, String url) {\n            view.loadUrl(url);\n            return true;\n        }\n        public void onPageStarted(WebView view, String url, Bitmap favicon) {\n            Log.d(\"WebView\",\"onPageStarted\");\n            super.onPageStarted(view, url, favicon);\n        }\n        public void onPageFinished(WebView view, String url) {\n            Log.d(\"WebView\",\"onPageFinished \");\n            view.loadUrl(\"javascript:window.handler.getContent(document.body.innerHTML);\");\n            super.onPageFinished(view, url);\n        }\n    }\n```\n\n-\t写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容\n\n```Java\n    final  class JavascriptHandler{\n        @JavascriptInterface\n        public void getContent(String htmlContent){\n            Log.i(Tag,\"html content: \"+htmlContent);\n            document= Jsoup.parse(htmlContent);\n            htmlstring=htmlContent;\n            content=document.getElementsByTag(\"body\").text();\n            Toast.makeText(MainActivity.this,\"加载完成\",Toast.LENGTH_SHORT).show();\n        }\n    }\n```\n\n\n\n-\t之后是字符串处理，根据教务处给的格式精简分类\n\n```Java\n    去逗号\n    String contenttemp=content;\n    content=\"\";\n    String[] contentstemp=contenttemp.split(\",\");\n    for (String temp:contentstemp){\n        content=content+temp;\n    }\n    \n    分组\n    contents=content.split(\" |:\");\n    String showcontent=\"\";\n    count=0;\n    int tsgflag=0;\n    int cishu=0;\n    j12.clear();\n    j34.clear();\n    j56.clear();\n    j78.clear();\n    j9.clear();\n    j1011.clear();\n    if (keyword.contains(\"图书馆\")) tsgflag=1;\n    for (String temp:contents){\n        if (temp.contains(keyword)){\n            cishu++;\n            SaveBuidlingInfo(count,cishu,tsgflag);\n        }\n        count++;\n    }\n    \n    SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....\n    while (1 == 1) {\n        if (contents[k].contains(\"楼\") || contents[k].contains(\"节\") || contents[k].contains(\"图\"))\n            break;\n        ;\n        switch (c) {\n            case 1:\n                j12.add(contents[k]);\n                break;\n            case 2:\n                j34.add(contents[k]);\n                break;\n            case 3:\n                j56.add(contents[k]);\n                break;\n            case 4:\n                j78.add(contents[k]);\n                break;\n            case 5:\n                j9.add(contents[k]);\n                break;\n            case 6:\n                j1011.add(contents[k]);\n                break;\n            default:\n                break;\n        }\n        k++;\n    }\n```\n\n-\t界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面\n\n\n-\t尝试了MaterialDesign组件，加入一点系统时间方面的东西\n\n```Java\n    final Calendar c = Calendar.getInstance();\n     c.setTimeZone(TimeZone.getTimeZone(\"GMT+8:00\"));\n     mYear = String.valueOf(c.get(Calendar.YEAR)); // 获取当前年份\n     mMonth = String.valueOf(c.get(Calendar.MONTH) + 1);// 获取当前月份\n     mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));// 获取当前月份的日期号码\n     mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));\n     mHour= c.get(Calendar.HOUR_OF_DAY);\n     mMinute= c.get(Calendar.MINUTE);\n    \n     if (mHour>=8&&mHour<10){\n         nowtime=\"现在是一二节课\";\n     }else\n     if (mHour>=10&&mHour<12){\n         nowtime=\"现在是三四节课\";\n     }else\n     if ((mHour==13&&mMinute>=30)||(mHour==14)||(mHour==15&&mMinute<30)){\n         nowtime=\"现在是五六节课\";\n     }else\n     if ((mHour==15&&mMinute>=30)||(mHour==16)||(mHour==17&&mMinute<30)){\n         nowtime=\"现在是七八节课\";\n     }else\n     if ((mHour==17&&mMinute>=30)||(mHour==18&&mMinute<30)){\n         nowtime=\"现在是第九节课\";\n     }else\n     if ((mHour==18&&mMinute>=30)||(mHour==19)||(mHour==20&&mMinute<30)){\n         nowtime=\"现在是十、十一节课\";\n     }else\n    nowtime=\"现在是休息时间\";\n    \n     if(\"1\".equals(mWay)){\n         mWay =\"天\";\n         daycount=6;\n     }else if(\"2\".equals(mWay)){\n         mWay =\"一\";\n         daycount=0;\n     }else if(\"3\".equals(mWay)){\n         mWay =\"二\";\n         daycount=1;\n     }else if(\"4\".equals(mWay)){\n         mWay =\"三\";\n         daycount=2;\n     }else if(\"5\".equals(mWay)){\n         mWay =\"四\";\n         daycount=3;\n     }else if(\"6\".equals(mWay)){\n         mWay =\"五\";\n         daycount=4;\n     }else if(\"7\".equals(mWay)){\n         mWay =\"六\";\n         daycount=5;\n     }\n     Timestring=mYear + \"年\" + mMonth + \"月\" + mDay+\"日\"+\"星期\"+mWay;\n    \n     FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);\n     fab.setOnClickListener(new View.OnClickListener() {\n         @Override\n         public void onClick(View view) {\n             Snackbar.make(view, \"今天是\"+Timestring+\"\\n\"+nowtime+\"  \"+interesting[daycount], Snackbar.LENGTH_SHORT)\n                     .setAction(\"Action\", null).show();\n         }\n     });\n```\n\n# 在GitHub上学到的\n此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等\n部分GitHub repository链接在这里\n-\t滑动卡片界面：[Android-SwipeToDismiss](https://github.com/romannurik/Android-SwipeToDismiss)\n-\tfir更新模块:[UpdateDemo](https://github.com/hugeterry/UpdateDemo)\n\n还有一些直接写在代码里了，忘记原地址了....\n-\t摇一摇的传感器调用\n```Java\npublic class ShakeService extends Service {\n    public static final String TAG = \"ShakeService\";\n    private SensorManager mSensorManager;\n    public boolean flag=false;\n    private ShakeBinder shakebinder= new ShakeBinder();\n    private String htmlbody=\"\";\n\n    @Override\n    public void onCreate() {\n        // TODO Auto-generated method stub\n        super.onCreate();\n        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);\n        Log.i(TAG,\"Shake Service Create\");\n    }\n\n    @Override\n    public void onDestroy() {\n        // TODO Auto-generated method stub\n        flag=false;\n        super.onDestroy();\n        mSensorManager.unregisterListener(mShakeListener);\n    }\n\n    @Override\n    public void onStart(Intent intent, int startId) {\n        // TODO Auto-generated method stub\n        super.onStart(intent, startId);\n        Log.i(TAG,\"Shake Service Start\");\n    }\n\n    @Override\n    public int onStartCommand(Intent intent, int flags, int startId) {\n        // TODO Auto-generated method stub\n        mSensorManager.registerListener(mShakeListener,\n                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),\n                //SensorManager.SENSOR_DELAY_GAME,\n                50 * 1000); //batch every 50 milliseconds\n        htmlbody=intent.getStringExtra(\"htmlbody\");\n\n        return super.onStartCommand(intent, flags, startId);\n    }\n\n    private final SensorEventListener mShakeListener = new SensorEventListener() {\n        private static final float SENSITIVITY = 10;\n        private static final int BUFFER = 5;\n        private float[] gravity = new float[3];\n        private float average = 0;\n        private int fill = 0;\n\n        @Override\n        public void onAccuracyChanged(Sensor sensor, int acc) {\n        }\n\n        public void onSensorChanged(SensorEvent event) {\n            final float alpha = 0.8F;\n\n            for (int i = 0; i < 3; i++) {\n                gravity[i] = alpha * gravity[i] + (1 - alpha) * event.values[i];\n            }\n\n            float x = event.values[0] - gravity[0];\n            float y = event.values[1] - gravity[1];\n            float z = event.values[2] - gravity[2];\n\n            if (fill <= BUFFER) {\n                average += Math.abs(x) + Math.abs(y) + Math.abs(z);\n                fill++;\n            } else {\n                Log.i(TAG, \"average:\"+average);\n                Log.i(TAG, \"average / BUFFER:\"+(average / BUFFER));\n                if (average / BUFFER >= SENSITIVITY) {\n                    handleShakeAction();//如果达到阈值则处理摇一摇响应\n                }\n                average = 0;\n                fill = 0;\n            }\n        }\n    };\n\n    protected void handleShakeAction() {\n        // TODO Auto-generated method stub\n        flag=true;\n        Toast.makeText(getApplicationContext(), \"摇一摇成功\", Toast.LENGTH_SHORT).show();\n        Intent intent= new Intent();\n        intent.putExtra(\"htmlbody\",htmlbody);\n        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);\n        intent.setClassName(this,\"thinkwee.buptroom.ShakeTestActivity\");\n        startActivity(intent);\n    }\n\n    @Override\n    public IBinder onBind(Intent intent) {\n        // TODO Auto-generated method stub\n        return shakebinder;\n    }\n    class ShakeBinder extends Binder{\n\n    }\n}\n\n```\n\n# 独立网络拉取，并使用多线程\n-\t在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用\n-\t然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新\n```Java\n        webget = new Webget();\n        webget.init(webView);\n        HaveNetFlag = webget.WebInit();\n\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                ImageView img = (ImageView) findViewById(R.id.welcomeimg);\n                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);\n                animation.setFillAfter(true);\n                img.startAnimation(animation);\n            }\n        }, 50);\n        \n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                WrongNet = webget.getWrongnet();\n                HaveNetFlag = webget.getHaveNetFlag();\n                htmlbody = webget.getHtmlbody();\n                Log.i(\"welcome\", \"2HaveNetFlag: \" + HaveNetFlag);\n                Log.i(\"welcome\", \"2Wrongnet: \" + WrongNet);\n                Log.i(\"welcome\", \"2html: \" + htmlbody);\n            }\n        }, 2000);\n\n        new Handler().postDelayed(new Runnable() {\n\n            @Override\n            public void run() {\n                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);\n                intent.putExtra(\"WrongNet\", WrongNet);\n                intent.putExtra(\"HtmlBody\", htmlbody);\n                startActivity(intent);\n                WelcomeActivity.this.finish();\n\n            }\n\n        }, 2500);\n    }\n```\n\n\n\n\n\n\n\n\n","source":"_posts/buptroomreview.md","raw":"---\ntitle: Android:BuptRoom总结\ndate: 2017-01-16 11:56:39\ntags: [code,android]\ncategories: Android\n---\n***\n\n# 简介\n写了一个查询学校空闲教室的APP\n拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的\n毕竟第一次写android，什么都想尝试一下\n点这下载：[BuptRoom](https://fir.im/buptroom)\nrepository地址:[一个简单的北邮自习室查询系统](https://github.com/thinkwee/BuptRoom)\n完成第一个版本大概是3个周末\n之后花了1个月陆陆续续更新了杂七杂八的\n很多东西写的不规范，也是临时查到了就用上\n总结一下这次写App的经过:\n\n<!--more-->\n\n![i0IHL4.png](https://s1.ax1x.com/2018/10/20/i0IHL4.png)\n\n# 学习的内容\n-\tAndroid基本架构，组件，生命周期\n-\tFragment的使用\n-\tJava库与库之间的调用\n-\tGithub的使用\n-\t部署app\n-\t图像处理的一些方法\n-\t一个愚蠢的拉取网页内容的方式\n-\tGitHub第三方库的利用\n-\t颜色方面的知识\n-\tAndroid Material Design\n-\t简单的优化\n-\t多线程与Handler\n\n# 解决的问题\n主要解决了这么几个问题\n\n-\tAndroid6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:\n\n```Java\n    <uses-permission android:name=\"android.permission.INTERNET\"></uses-permission>\n    <uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"></uses-permission>\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"></uses-permission>\n    <uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"></uses-permission>\n```\n\n-\t网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中\n\n```Java\n    final class MyWebViewClient extends WebViewClient {\n        public boolean shouldOverrideUrlLoading(WebView view, String url) {\n            view.loadUrl(url);\n            return true;\n        }\n        public void onPageStarted(WebView view, String url, Bitmap favicon) {\n            Log.d(\"WebView\",\"onPageStarted\");\n            super.onPageStarted(view, url, favicon);\n        }\n        public void onPageFinished(WebView view, String url) {\n            Log.d(\"WebView\",\"onPageFinished \");\n            view.loadUrl(\"javascript:window.handler.getContent(document.body.innerHTML);\");\n            super.onPageFinished(view, url);\n        }\n    }\n```\n\n-\t写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容\n\n```Java\n    final  class JavascriptHandler{\n        @JavascriptInterface\n        public void getContent(String htmlContent){\n            Log.i(Tag,\"html content: \"+htmlContent);\n            document= Jsoup.parse(htmlContent);\n            htmlstring=htmlContent;\n            content=document.getElementsByTag(\"body\").text();\n            Toast.makeText(MainActivity.this,\"加载完成\",Toast.LENGTH_SHORT).show();\n        }\n    }\n```\n\n\n\n-\t之后是字符串处理，根据教务处给的格式精简分类\n\n```Java\n    去逗号\n    String contenttemp=content;\n    content=\"\";\n    String[] contentstemp=contenttemp.split(\",\");\n    for (String temp:contentstemp){\n        content=content+temp;\n    }\n    \n    分组\n    contents=content.split(\" |:\");\n    String showcontent=\"\";\n    count=0;\n    int tsgflag=0;\n    int cishu=0;\n    j12.clear();\n    j34.clear();\n    j56.clear();\n    j78.clear();\n    j9.clear();\n    j1011.clear();\n    if (keyword.contains(\"图书馆\")) tsgflag=1;\n    for (String temp:contents){\n        if (temp.contains(keyword)){\n            cishu++;\n            SaveBuidlingInfo(count,cishu,tsgflag);\n        }\n        count++;\n    }\n    \n    SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....\n    while (1 == 1) {\n        if (contents[k].contains(\"楼\") || contents[k].contains(\"节\") || contents[k].contains(\"图\"))\n            break;\n        ;\n        switch (c) {\n            case 1:\n                j12.add(contents[k]);\n                break;\n            case 2:\n                j34.add(contents[k]);\n                break;\n            case 3:\n                j56.add(contents[k]);\n                break;\n            case 4:\n                j78.add(contents[k]);\n                break;\n            case 5:\n                j9.add(contents[k]);\n                break;\n            case 6:\n                j1011.add(contents[k]);\n                break;\n            default:\n                break;\n        }\n        k++;\n    }\n```\n\n-\t界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面\n\n\n-\t尝试了MaterialDesign组件，加入一点系统时间方面的东西\n\n```Java\n    final Calendar c = Calendar.getInstance();\n     c.setTimeZone(TimeZone.getTimeZone(\"GMT+8:00\"));\n     mYear = String.valueOf(c.get(Calendar.YEAR)); // 获取当前年份\n     mMonth = String.valueOf(c.get(Calendar.MONTH) + 1);// 获取当前月份\n     mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));// 获取当前月份的日期号码\n     mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));\n     mHour= c.get(Calendar.HOUR_OF_DAY);\n     mMinute= c.get(Calendar.MINUTE);\n    \n     if (mHour>=8&&mHour<10){\n         nowtime=\"现在是一二节课\";\n     }else\n     if (mHour>=10&&mHour<12){\n         nowtime=\"现在是三四节课\";\n     }else\n     if ((mHour==13&&mMinute>=30)||(mHour==14)||(mHour==15&&mMinute<30)){\n         nowtime=\"现在是五六节课\";\n     }else\n     if ((mHour==15&&mMinute>=30)||(mHour==16)||(mHour==17&&mMinute<30)){\n         nowtime=\"现在是七八节课\";\n     }else\n     if ((mHour==17&&mMinute>=30)||(mHour==18&&mMinute<30)){\n         nowtime=\"现在是第九节课\";\n     }else\n     if ((mHour==18&&mMinute>=30)||(mHour==19)||(mHour==20&&mMinute<30)){\n         nowtime=\"现在是十、十一节课\";\n     }else\n    nowtime=\"现在是休息时间\";\n    \n     if(\"1\".equals(mWay)){\n         mWay =\"天\";\n         daycount=6;\n     }else if(\"2\".equals(mWay)){\n         mWay =\"一\";\n         daycount=0;\n     }else if(\"3\".equals(mWay)){\n         mWay =\"二\";\n         daycount=1;\n     }else if(\"4\".equals(mWay)){\n         mWay =\"三\";\n         daycount=2;\n     }else if(\"5\".equals(mWay)){\n         mWay =\"四\";\n         daycount=3;\n     }else if(\"6\".equals(mWay)){\n         mWay =\"五\";\n         daycount=4;\n     }else if(\"7\".equals(mWay)){\n         mWay =\"六\";\n         daycount=5;\n     }\n     Timestring=mYear + \"年\" + mMonth + \"月\" + mDay+\"日\"+\"星期\"+mWay;\n    \n     FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);\n     fab.setOnClickListener(new View.OnClickListener() {\n         @Override\n         public void onClick(View view) {\n             Snackbar.make(view, \"今天是\"+Timestring+\"\\n\"+nowtime+\"  \"+interesting[daycount], Snackbar.LENGTH_SHORT)\n                     .setAction(\"Action\", null).show();\n         }\n     });\n```\n\n# 在GitHub上学到的\n此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等\n部分GitHub repository链接在这里\n-\t滑动卡片界面：[Android-SwipeToDismiss](https://github.com/romannurik/Android-SwipeToDismiss)\n-\tfir更新模块:[UpdateDemo](https://github.com/hugeterry/UpdateDemo)\n\n还有一些直接写在代码里了，忘记原地址了....\n-\t摇一摇的传感器调用\n```Java\npublic class ShakeService extends Service {\n    public static final String TAG = \"ShakeService\";\n    private SensorManager mSensorManager;\n    public boolean flag=false;\n    private ShakeBinder shakebinder= new ShakeBinder();\n    private String htmlbody=\"\";\n\n    @Override\n    public void onCreate() {\n        // TODO Auto-generated method stub\n        super.onCreate();\n        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);\n        Log.i(TAG,\"Shake Service Create\");\n    }\n\n    @Override\n    public void onDestroy() {\n        // TODO Auto-generated method stub\n        flag=false;\n        super.onDestroy();\n        mSensorManager.unregisterListener(mShakeListener);\n    }\n\n    @Override\n    public void onStart(Intent intent, int startId) {\n        // TODO Auto-generated method stub\n        super.onStart(intent, startId);\n        Log.i(TAG,\"Shake Service Start\");\n    }\n\n    @Override\n    public int onStartCommand(Intent intent, int flags, int startId) {\n        // TODO Auto-generated method stub\n        mSensorManager.registerListener(mShakeListener,\n                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),\n                //SensorManager.SENSOR_DELAY_GAME,\n                50 * 1000); //batch every 50 milliseconds\n        htmlbody=intent.getStringExtra(\"htmlbody\");\n\n        return super.onStartCommand(intent, flags, startId);\n    }\n\n    private final SensorEventListener mShakeListener = new SensorEventListener() {\n        private static final float SENSITIVITY = 10;\n        private static final int BUFFER = 5;\n        private float[] gravity = new float[3];\n        private float average = 0;\n        private int fill = 0;\n\n        @Override\n        public void onAccuracyChanged(Sensor sensor, int acc) {\n        }\n\n        public void onSensorChanged(SensorEvent event) {\n            final float alpha = 0.8F;\n\n            for (int i = 0; i < 3; i++) {\n                gravity[i] = alpha * gravity[i] + (1 - alpha) * event.values[i];\n            }\n\n            float x = event.values[0] - gravity[0];\n            float y = event.values[1] - gravity[1];\n            float z = event.values[2] - gravity[2];\n\n            if (fill <= BUFFER) {\n                average += Math.abs(x) + Math.abs(y) + Math.abs(z);\n                fill++;\n            } else {\n                Log.i(TAG, \"average:\"+average);\n                Log.i(TAG, \"average / BUFFER:\"+(average / BUFFER));\n                if (average / BUFFER >= SENSITIVITY) {\n                    handleShakeAction();//如果达到阈值则处理摇一摇响应\n                }\n                average = 0;\n                fill = 0;\n            }\n        }\n    };\n\n    protected void handleShakeAction() {\n        // TODO Auto-generated method stub\n        flag=true;\n        Toast.makeText(getApplicationContext(), \"摇一摇成功\", Toast.LENGTH_SHORT).show();\n        Intent intent= new Intent();\n        intent.putExtra(\"htmlbody\",htmlbody);\n        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);\n        intent.setClassName(this,\"thinkwee.buptroom.ShakeTestActivity\");\n        startActivity(intent);\n    }\n\n    @Override\n    public IBinder onBind(Intent intent) {\n        // TODO Auto-generated method stub\n        return shakebinder;\n    }\n    class ShakeBinder extends Binder{\n\n    }\n}\n\n```\n\n# 独立网络拉取，并使用多线程\n-\t在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用\n-\t然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新\n```Java\n        webget = new Webget();\n        webget.init(webView);\n        HaveNetFlag = webget.WebInit();\n\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                ImageView img = (ImageView) findViewById(R.id.welcomeimg);\n                Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.this, R.anim.enlarge);\n                animation.setFillAfter(true);\n                img.startAnimation(animation);\n            }\n        }, 50);\n        \n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                WrongNet = webget.getWrongnet();\n                HaveNetFlag = webget.getHaveNetFlag();\n                htmlbody = webget.getHtmlbody();\n                Log.i(\"welcome\", \"2HaveNetFlag: \" + HaveNetFlag);\n                Log.i(\"welcome\", \"2Wrongnet: \" + WrongNet);\n                Log.i(\"welcome\", \"2html: \" + htmlbody);\n            }\n        }, 2000);\n\n        new Handler().postDelayed(new Runnable() {\n\n            @Override\n            public void run() {\n                Intent intent = new Intent(WelcomeActivity.this, MainActivity.class);\n                intent.putExtra(\"WrongNet\", WrongNet);\n                intent.putExtra(\"HtmlBody\", htmlbody);\n                startActivity(intent);\n                WelcomeActivity.this.finish();\n\n            }\n\n        }, 2500);\n    }\n```\n\n\n\n\n\n\n\n\n","slug":"buptroomreview","published":1,"updated":"2019-07-22T03:45:22.972Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v3s001qq8t53esarwaf","content":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IHL4.png\" alt=\"i0IHL4.png\"></p>\n<h1 id=\"学习的内容\"><a href=\"#学习的内容\" class=\"headerlink\" title=\"学习的内容\"></a>学习的内容</h1><ul>\n<li>Android基本架构，组件，生命周期</li>\n<li>Fragment的使用</li>\n<li>Java库与库之间的调用</li>\n<li>Github的使用</li>\n<li>部署app</li>\n<li>图像处理的一些方法</li>\n<li>一个愚蠢的拉取网页内容的方式</li>\n<li>GitHub第三方库的利用</li>\n<li>颜色方面的知识</li>\n<li>Android Material Design</li>\n<li>简单的优化</li>\n<li>多线程与Handler</li>\n</ul>\n<h1 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h1><p>主要解决了这么几个问题</p>\n<ul>\n<li>Android6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:</li>\n</ul>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;uses-permission android:name=\"android.permission.INTERNET\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWebViewClient</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebViewClient</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">shouldOverrideUrlLoading</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        view.loadUrl(url);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageStarted</span><span class=\"params\">(WebView view, String url, Bitmap favicon)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageStarted\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageStarted(view, url, favicon);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageFinished</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageFinished \"</span>);</span><br><span class=\"line\">        view.loadUrl(<span class=\"string\">\"javascript:window.handler.getContent(document.body.innerHTML);\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageFinished(view, url);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span>  <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavascriptHandler</span></span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@JavascriptInterface</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getContent</span><span class=\"params\">(String htmlContent)</span></span>&#123;</span><br><span class=\"line\">        Log.i(Tag,<span class=\"string\">\"html content: \"</span>+htmlContent);</span><br><span class=\"line\">        document= Jsoup.parse(htmlContent);</span><br><span class=\"line\">        htmlstring=htmlContent;</span><br><span class=\"line\">        content=document.getElementsByTag(<span class=\"string\">\"body\"</span>).text();</span><br><span class=\"line\">        Toast.makeText(MainActivity.<span class=\"keyword\">this</span>,<span class=\"string\">\"加载完成\"</span>,Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>之后是字符串处理，根据教务处给的格式精简分类</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">去逗号</span><br><span class=\"line\">String contenttemp=content;</span><br><span class=\"line\">content=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">String[] contentstemp=contenttemp.split(<span class=\"string\">\",\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contentstemp)&#123;</span><br><span class=\"line\">    content=content+temp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">分组</span><br><span class=\"line\">contents=content.split(<span class=\"string\">\" |:\"</span>);</span><br><span class=\"line\">String showcontent=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">count=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> tsgflag=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> cishu=<span class=\"number\">0</span>;</span><br><span class=\"line\">j12.clear();</span><br><span class=\"line\">j34.clear();</span><br><span class=\"line\">j56.clear();</span><br><span class=\"line\">j78.clear();</span><br><span class=\"line\">j9.clear();</span><br><span class=\"line\">j1011.clear();</span><br><span class=\"line\"><span class=\"keyword\">if</span> (keyword.contains(<span class=\"string\">\"图书馆\"</span>)) tsgflag=<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contents)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (temp.contains(keyword))&#123;</span><br><span class=\"line\">        cishu++;</span><br><span class=\"line\">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    count++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....</span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"number\">1</span> == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (contents[k].contains(<span class=\"string\">\"楼\"</span>) || contents[k].contains(<span class=\"string\">\"节\"</span>) || contents[k].contains(<span class=\"string\">\"图\"</span>))</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ;</span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (c) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">1</span>:</span><br><span class=\"line\">            j12.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">2</span>:</span><br><span class=\"line\">            j34.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">3</span>:</span><br><span class=\"line\">            j56.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">4</span>:</span><br><span class=\"line\">            j78.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">5</span>:</span><br><span class=\"line\">            j9.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">6</span>:</span><br><span class=\"line\">            j1011.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    k++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面</li>\n</ul>\n<ul>\n<li>尝试了MaterialDesign组件，加入一点系统时间方面的东西</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Calendar c = Calendar.getInstance();</span><br><span class=\"line\"> c.setTimeZone(TimeZone.getTimeZone(<span class=\"string\">\"GMT+8:00\"</span>));</span><br><span class=\"line\"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class=\"comment\">// 获取当前年份</span></span><br><span class=\"line\"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class=\"number\">1</span>);<span class=\"comment\">// 获取当前月份</span></span><br><span class=\"line\"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class=\"comment\">// 获取当前月份的日期号码</span></span><br><span class=\"line\"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class=\"line\"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class=\"line\"> mMinute= c.get(Calendar.MINUTE);</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">8</span>&amp;&amp;mHour&lt;<span class=\"number\">10</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是一二节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">10</span>&amp;&amp;mHour&lt;<span class=\"number\">12</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是三四节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">13</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">14</span>)||(mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是五六节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">16</span>)||(mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是七八节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是第九节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">19</span>)||(mHour==<span class=\"number\">20</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是十、十一节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">nowtime=<span class=\"string\">\"现在是休息时间\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span>(<span class=\"string\">\"1\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"天\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">6</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"2\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"一\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">0</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"3\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"二\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">1</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"4\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"三\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">2</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"5\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"四\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">3</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"6\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"五\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">4</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"7\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"六\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">5</span>;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> Timestring=mYear + <span class=\"string\">\"年\"</span> + mMonth + <span class=\"string\">\"月\"</span> + mDay+<span class=\"string\">\"日\"</span>+<span class=\"string\">\"星期\"</span>+mWay;</span><br><span class=\"line\"></span><br><span class=\"line\"> FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);</span><br><span class=\"line\"> fab.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View view)</span> </span>&#123;</span><br><span class=\"line\">         Snackbar.make(view, <span class=\"string\">\"今天是\"</span>+Timestring+<span class=\"string\">\"\\n\"</span>+nowtime+<span class=\"string\">\"  \"</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">                 .setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure>\n<h1 id=\"在GitHub上学到的\"><a href=\"#在GitHub上学到的\" class=\"headerlink\" title=\"在GitHub上学到的\"></a>在GitHub上学到的</h1><p>此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等<br>部分GitHub repository链接在这里</p>\n<ul>\n<li>滑动卡片界面：<a href=\"https://github.com/romannurik/Android-SwipeToDismiss\" target=\"_blank\" rel=\"noopener\">Android-SwipeToDismiss</a></li>\n<li>fir更新模块:<a href=\"https://github.com/hugeterry/UpdateDemo\" target=\"_blank\" rel=\"noopener\">UpdateDemo</a></li>\n</ul>\n<p>还有一些直接写在代码里了，忘记原地址了….</p>\n<ul>\n<li>摇一摇的传感器调用<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeService</span> <span class=\"keyword\">extends</span> <span class=\"title\">Service</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> String TAG = <span class=\"string\">\"ShakeService\"</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SensorManager mSensorManager;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> ShakeBinder shakebinder= <span class=\"keyword\">new</span> ShakeBinder();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String htmlbody=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate();</span><br><span class=\"line\">        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Create\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onDestroy</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onDestroy();</span><br><span class=\"line\">        mSensorManager.unregisterListener(mShakeListener);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onStart</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onStart(intent, startId);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Start\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">onStartCommand</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> flags, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        mSensorManager.registerListener(mShakeListener,</span><br><span class=\"line\">                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class=\"line\">                <span class=\"comment\">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class=\"line\">                <span class=\"number\">50</span> * <span class=\"number\">1000</span>); <span class=\"comment\">//batch every 50 milliseconds</span></span><br><span class=\"line\">        htmlbody=intent.getStringExtra(<span class=\"string\">\"htmlbody\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.onStartCommand(intent, flags, startId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SensorEventListener mShakeListener = <span class=\"keyword\">new</span> SensorEventListener() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> SENSITIVITY = <span class=\"number\">10</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BUFFER = <span class=\"number\">5</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span>[] gravity = <span class=\"keyword\">new</span> <span class=\"keyword\">float</span>[<span class=\"number\">3</span>];</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span> average = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> fill = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onAccuracyChanged</span><span class=\"params\">(Sensor sensor, <span class=\"keyword\">int</span> acc)</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onSensorChanged</span><span class=\"params\">(SensorEvent event)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> alpha = <span class=\"number\">0.8F</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; i++) &#123;</span><br><span class=\"line\">                gravity[i] = alpha * gravity[i] + (<span class=\"number\">1</span> - alpha) * event.values[i];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">float</span> x = event.values[<span class=\"number\">0</span>] - gravity[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> y = event.values[<span class=\"number\">1</span>] - gravity[<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> z = event.values[<span class=\"number\">2</span>] - gravity[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class=\"line\">                average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class=\"line\">                fill++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average:\"</span>+average);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average / BUFFER:\"</span>+(average / BUFFER));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class=\"line\">                    handleShakeAction();<span class=\"comment\">//如果达到阈值则处理摇一摇响应</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                average = <span class=\"number\">0</span>;</span><br><span class=\"line\">                fill = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">handleShakeAction</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">true</span>;</span><br><span class=\"line\">        Toast.makeText(getApplicationContext(), <span class=\"string\">\"摇一摇成功\"</span>, Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">        Intent intent= <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">        intent.putExtra(<span class=\"string\">\"htmlbody\"</span>,htmlbody);</span><br><span class=\"line\">        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class=\"line\">        intent.setClassName(<span class=\"keyword\">this</span>,<span class=\"string\">\"thinkwee.buptroom.ShakeTestActivity\"</span>);</span><br><span class=\"line\">        startActivity(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> IBinder <span class=\"title\">onBind</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> shakebinder;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeBinder</span> <span class=\"keyword\">extends</span> <span class=\"title\">Binder</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"独立网络拉取，并使用多线程\"><a href=\"#独立网络拉取，并使用多线程\" class=\"headerlink\" title=\"独立网络拉取，并使用多线程\"></a>独立网络拉取，并使用多线程</h1><ul>\n<li>在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用</li>\n<li>然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    webget = <span class=\"keyword\">new</span> Webget();</span><br><span class=\"line\">    webget.init(webView);</span><br><span class=\"line\">    HaveNetFlag = webget.WebInit();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class=\"line\">            Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.<span class=\"keyword\">this</span>, R.anim.enlarge);</span><br><span class=\"line\">            animation.setFillAfter(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">            img.startAnimation(animation);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">50</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            WrongNet = webget.getWrongnet();</span><br><span class=\"line\">            HaveNetFlag = webget.getHaveNetFlag();</span><br><span class=\"line\">            htmlbody = webget.getHtmlbody();</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2HaveNetFlag: \"</span> + HaveNetFlag);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2Wrongnet: \"</span> + WrongNet);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2html: \"</span> + htmlbody);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            Intent intent = <span class=\"keyword\">new</span> Intent(WelcomeActivity.<span class=\"keyword\">this</span>, MainActivity.class);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"WrongNet\"</span>, WrongNet);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"HtmlBody\"</span>, htmlbody);</span><br><span class=\"line\">            startActivity(intent);</span><br><span class=\"line\">            WelcomeActivity.<span class=\"keyword\">this</span>.finish();</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;, <span class=\"number\">2500</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0IHL4.png\" alt=\"i0IHL4.png\"></p>\n<h1 id=\"学习的内容\"><a href=\"#学习的内容\" class=\"headerlink\" title=\"学习的内容\"></a>学习的内容</h1><ul>\n<li>Android基本架构，组件，生命周期</li>\n<li>Fragment的使用</li>\n<li>Java库与库之间的调用</li>\n<li>Github的使用</li>\n<li>部署app</li>\n<li>图像处理的一些方法</li>\n<li>一个愚蠢的拉取网页内容的方式</li>\n<li>GitHub第三方库的利用</li>\n<li>颜色方面的知识</li>\n<li>Android Material Design</li>\n<li>简单的优化</li>\n<li>多线程与Handler</li>\n</ul>\n<h1 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h1><p>主要解决了这么几个问题</p>\n<ul>\n<li>Android6.0以上的版本貌似权限需要动态验证，现在写的只支持5.0及以下版本，用到的permisson:</li>\n</ul>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;uses-permission android:name=\"android.permission.INTERNET\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.SYSTEM_ALERT_WINDOW\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"&gt;&lt;/uses-permission&gt;</span><br><span class=\"line\">&lt;uses-permission android:name=\"android.permission.MOUNT_UNMOUNT_FILESYSTEMS\"&gt;&lt;/uses-permission&gt;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>网页是jsp动态网页，不能简单地parse，最后采用在webview中loadurl，执行javascript命令，需下载jsoup-1.9.2.jar这个包添加到库文件中</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyWebViewClient</span> <span class=\"keyword\">extends</span> <span class=\"title\">WebViewClient</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">shouldOverrideUrlLoading</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        view.loadUrl(url);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageStarted</span><span class=\"params\">(WebView view, String url, Bitmap favicon)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageStarted\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageStarted(view, url, favicon);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onPageFinished</span><span class=\"params\">(WebView view, String url)</span> </span>&#123;</span><br><span class=\"line\">        Log.d(<span class=\"string\">\"WebView\"</span>,<span class=\"string\">\"onPageFinished \"</span>);</span><br><span class=\"line\">        view.loadUrl(<span class=\"string\">\"javascript:window.handler.getContent(document.body.innerHTML);\"</span>);</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onPageFinished(view, url);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>写一个handler响应javascript命令,这样在content中就拿到String形式的html文件中body内容</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span>  <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavascriptHandler</span></span>&#123;</span><br><span class=\"line\">    <span class=\"meta\">@JavascriptInterface</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">getContent</span><span class=\"params\">(String htmlContent)</span></span>&#123;</span><br><span class=\"line\">        Log.i(Tag,<span class=\"string\">\"html content: \"</span>+htmlContent);</span><br><span class=\"line\">        document= Jsoup.parse(htmlContent);</span><br><span class=\"line\">        htmlstring=htmlContent;</span><br><span class=\"line\">        content=document.getElementsByTag(<span class=\"string\">\"body\"</span>).text();</span><br><span class=\"line\">        Toast.makeText(MainActivity.<span class=\"keyword\">this</span>,<span class=\"string\">\"加载完成\"</span>,Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>之后是字符串处理，根据教务处给的格式精简分类</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">去逗号</span><br><span class=\"line\">String contenttemp=content;</span><br><span class=\"line\">content=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">String[] contentstemp=contenttemp.split(<span class=\"string\">\",\"</span>);</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contentstemp)&#123;</span><br><span class=\"line\">    content=content+temp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">分组</span><br><span class=\"line\">contents=content.split(<span class=\"string\">\" |:\"</span>);</span><br><span class=\"line\">String showcontent=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\">count=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> tsgflag=<span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">int</span> cishu=<span class=\"number\">0</span>;</span><br><span class=\"line\">j12.clear();</span><br><span class=\"line\">j34.clear();</span><br><span class=\"line\">j56.clear();</span><br><span class=\"line\">j78.clear();</span><br><span class=\"line\">j9.clear();</span><br><span class=\"line\">j1011.clear();</span><br><span class=\"line\"><span class=\"keyword\">if</span> (keyword.contains(<span class=\"string\">\"图书馆\"</span>)) tsgflag=<span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (String temp:contents)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (temp.contains(keyword))&#123;</span><br><span class=\"line\">        cishu++;</span><br><span class=\"line\">        SaveBuidlingInfo(count,cishu,tsgflag);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    count++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">SaveBuildingInfo是按教学楼分类存取一天教室，其中再按时间段分类存到j12,j34.....</span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"number\">1</span> == <span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (contents[k].contains(<span class=\"string\">\"楼\"</span>) || contents[k].contains(<span class=\"string\">\"节\"</span>) || contents[k].contains(<span class=\"string\">\"图\"</span>))</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    ;</span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (c) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">1</span>:</span><br><span class=\"line\">            j12.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">2</span>:</span><br><span class=\"line\">            j34.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">3</span>:</span><br><span class=\"line\">            j56.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">4</span>:</span><br><span class=\"line\">            j78.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">5</span>:</span><br><span class=\"line\">            j9.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">case</span> <span class=\"number\">6</span>:</span><br><span class=\"line\">            j1011.add(contents[k]);</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        <span class=\"keyword\">default</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    k++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>界面上套了一个NavigationView，没有什么特别设计的，因为没有设置多界面，就靠刷新TextView来伪装多个界面</li>\n</ul>\n<ul>\n<li>尝试了MaterialDesign组件，加入一点系统时间方面的东西</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Calendar c = Calendar.getInstance();</span><br><span class=\"line\"> c.setTimeZone(TimeZone.getTimeZone(<span class=\"string\">\"GMT+8:00\"</span>));</span><br><span class=\"line\"> mYear = String.valueOf(c.get(Calendar.YEAR)); <span class=\"comment\">// 获取当前年份</span></span><br><span class=\"line\"> mMonth = String.valueOf(c.get(Calendar.MONTH) + <span class=\"number\">1</span>);<span class=\"comment\">// 获取当前月份</span></span><br><span class=\"line\"> mDay = String.valueOf(c.get(Calendar.DAY_OF_MONTH));<span class=\"comment\">// 获取当前月份的日期号码</span></span><br><span class=\"line\"> mWay = String.valueOf(c.get(Calendar.DAY_OF_WEEK));</span><br><span class=\"line\"> mHour= c.get(Calendar.HOUR_OF_DAY);</span><br><span class=\"line\"> mMinute= c.get(Calendar.MINUTE);</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">8</span>&amp;&amp;mHour&lt;<span class=\"number\">10</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是一二节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> (mHour&gt;=<span class=\"number\">10</span>&amp;&amp;mHour&lt;<span class=\"number\">12</span>)&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是三四节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">13</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">14</span>)||(mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是五六节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">15</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">16</span>)||(mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是七八节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">17</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是第九节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> ((mHour==<span class=\"number\">18</span>&amp;&amp;mMinute&gt;=<span class=\"number\">30</span>)||(mHour==<span class=\"number\">19</span>)||(mHour==<span class=\"number\">20</span>&amp;&amp;mMinute&lt;<span class=\"number\">30</span>))&#123;</span><br><span class=\"line\">     nowtime=<span class=\"string\">\"现在是十、十一节课\"</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">nowtime=<span class=\"string\">\"现在是休息时间\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">if</span>(<span class=\"string\">\"1\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"天\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">6</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"2\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"一\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">0</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"3\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"二\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">1</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"4\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"三\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">2</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"5\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"四\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">3</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"6\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"五\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">4</span>;</span><br><span class=\"line\"> &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(<span class=\"string\">\"7\"</span>.equals(mWay))&#123;</span><br><span class=\"line\">     mWay =<span class=\"string\">\"六\"</span>;</span><br><span class=\"line\">     daycount=<span class=\"number\">5</span>;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> Timestring=mYear + <span class=\"string\">\"年\"</span> + mMonth + <span class=\"string\">\"月\"</span> + mDay+<span class=\"string\">\"日\"</span>+<span class=\"string\">\"星期\"</span>+mWay;</span><br><span class=\"line\"></span><br><span class=\"line\"> FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);</span><br><span class=\"line\"> fab.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">     <span class=\"meta\">@Override</span></span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View view)</span> </span>&#123;</span><br><span class=\"line\">         Snackbar.make(view, <span class=\"string\">\"今天是\"</span>+Timestring+<span class=\"string\">\"\\n\"</span>+nowtime+<span class=\"string\">\"  \"</span>+interesting[daycount], Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">                 .setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure>\n<h1 id=\"在GitHub上学到的\"><a href=\"#在GitHub上学到的\" class=\"headerlink\" title=\"在GitHub上学到的\"></a>在GitHub上学到的</h1><p>此外还尝试引用了其他的一些GitHub库，学习了许多，包括调色盘，摇一摇模块，fir更新模块，滑动卡片界面等等<br>部分GitHub repository链接在这里</p>\n<ul>\n<li>滑动卡片界面：<a href=\"https://github.com/romannurik/Android-SwipeToDismiss\" target=\"_blank\" rel=\"noopener\">Android-SwipeToDismiss</a></li>\n<li>fir更新模块:<a href=\"https://github.com/hugeterry/UpdateDemo\" target=\"_blank\" rel=\"noopener\">UpdateDemo</a></li>\n</ul>\n<p>还有一些直接写在代码里了，忘记原地址了….</p>\n<ul>\n<li>摇一摇的传感器调用<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeService</span> <span class=\"keyword\">extends</span> <span class=\"title\">Service</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> String TAG = <span class=\"string\">\"ShakeService\"</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> SensorManager mSensorManager;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> ShakeBinder shakebinder= <span class=\"keyword\">new</span> ShakeBinder();</span><br><span class=\"line\">    <span class=\"keyword\">private</span> String htmlbody=<span class=\"string\">\"\"</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate();</span><br><span class=\"line\">        mSensorManager = (SensorManager) getSystemService(Context.SENSOR_SERVICE);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Create\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onDestroy</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">false</span>;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onDestroy();</span><br><span class=\"line\">        mSensorManager.unregisterListener(mShakeListener);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onStart</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onStart(intent, startId);</span><br><span class=\"line\">        Log.i(TAG,<span class=\"string\">\"Shake Service Start\"</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">onStartCommand</span><span class=\"params\">(Intent intent, <span class=\"keyword\">int</span> flags, <span class=\"keyword\">int</span> startId)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        mSensorManager.registerListener(mShakeListener,</span><br><span class=\"line\">                mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),</span><br><span class=\"line\">                <span class=\"comment\">//SensorManager.SENSOR_DELAY_GAME,</span></span><br><span class=\"line\">                <span class=\"number\">50</span> * <span class=\"number\">1000</span>); <span class=\"comment\">//batch every 50 milliseconds</span></span><br><span class=\"line\">        htmlbody=intent.getStringExtra(<span class=\"string\">\"htmlbody\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">super</span>.onStartCommand(intent, flags, startId);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> SensorEventListener mShakeListener = <span class=\"keyword\">new</span> SensorEventListener() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> SENSITIVITY = <span class=\"number\">10</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">int</span> BUFFER = <span class=\"number\">5</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span>[] gravity = <span class=\"keyword\">new</span> <span class=\"keyword\">float</span>[<span class=\"number\">3</span>];</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">float</span> average = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> fill = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onAccuracyChanged</span><span class=\"params\">(Sensor sensor, <span class=\"keyword\">int</span> acc)</span> </span>&#123;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onSensorChanged</span><span class=\"params\">(SensorEvent event)</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">final</span> <span class=\"keyword\">float</span> alpha = <span class=\"number\">0.8F</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; i++) &#123;</span><br><span class=\"line\">                gravity[i] = alpha * gravity[i] + (<span class=\"number\">1</span> - alpha) * event.values[i];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">float</span> x = event.values[<span class=\"number\">0</span>] - gravity[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> y = event.values[<span class=\"number\">1</span>] - gravity[<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">float</span> z = event.values[<span class=\"number\">2</span>] - gravity[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (fill &lt;= BUFFER) &#123;</span><br><span class=\"line\">                average += Math.abs(x) + Math.abs(y) + Math.abs(z);</span><br><span class=\"line\">                fill++;</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average:\"</span>+average);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"average / BUFFER:\"</span>+(average / BUFFER));</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (average / BUFFER &gt;= SENSITIVITY) &#123;</span><br><span class=\"line\">                    handleShakeAction();<span class=\"comment\">//如果达到阈值则处理摇一摇响应</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                average = <span class=\"number\">0</span>;</span><br><span class=\"line\">                fill = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">handleShakeAction</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        flag=<span class=\"keyword\">true</span>;</span><br><span class=\"line\">        Toast.makeText(getApplicationContext(), <span class=\"string\">\"摇一摇成功\"</span>, Toast.LENGTH_SHORT).show();</span><br><span class=\"line\">        Intent intent= <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">        intent.putExtra(<span class=\"string\">\"htmlbody\"</span>,htmlbody);</span><br><span class=\"line\">        intent.addFlags(FLAG_ACTIVITY_NEW_TASK);</span><br><span class=\"line\">        intent.setClassName(<span class=\"keyword\">this</span>,<span class=\"string\">\"thinkwee.buptroom.ShakeTestActivity\"</span>);</span><br><span class=\"line\">        startActivity(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> IBinder <span class=\"title\">onBind</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> shakebinder;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ShakeBinder</span> <span class=\"keyword\">extends</span> <span class=\"title\">Binder</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"独立网络拉取，并使用多线程\"><a href=\"#独立网络拉取，并使用多线程\" class=\"headerlink\" title=\"独立网络拉取，并使用多线程\"></a>独立网络拉取，并使用多线程</h1><ul>\n<li>在之前的结构中网络拉取整合在欢迎界面的activity中，为了在主界面中添加刷新功能，随时调用网络拉取，我把网络拉取单独写成了一个类，需要的时候调用</li>\n<li>然而在欢迎界面中显示欢迎动画和网络拉取在两个独立的线程中（为了使得动画不卡顿），这样就出现了可能欢迎动画做完了进入主界面时网络拉取还没有完成，传不了拉取的内容到主界面，最后的解决方案是设置网络拉取2s超时，若没拉取到则传一个错误的参数到启动主界面的activity中，提示刷新<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    webget = <span class=\"keyword\">new</span> Webget();</span><br><span class=\"line\">    webget.init(webView);</span><br><span class=\"line\">    HaveNetFlag = webget.WebInit();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            ImageView img = (ImageView) findViewById(R.id.welcomeimg);</span><br><span class=\"line\">            Animation animation = AnimationUtils.loadAnimation(WelcomeActivity.<span class=\"keyword\">this</span>, R.anim.enlarge);</span><br><span class=\"line\">            animation.setFillAfter(<span class=\"keyword\">true</span>);</span><br><span class=\"line\">            img.startAnimation(animation);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">50</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            WrongNet = webget.getWrongnet();</span><br><span class=\"line\">            HaveNetFlag = webget.getHaveNetFlag();</span><br><span class=\"line\">            htmlbody = webget.getHtmlbody();</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2HaveNetFlag: \"</span> + HaveNetFlag);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2Wrongnet: \"</span> + WrongNet);</span><br><span class=\"line\">            Log.i(<span class=\"string\">\"welcome\"</span>, <span class=\"string\">\"2html: \"</span> + htmlbody);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            Intent intent = <span class=\"keyword\">new</span> Intent(WelcomeActivity.<span class=\"keyword\">this</span>, MainActivity.class);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"WrongNet\"</span>, WrongNet);</span><br><span class=\"line\">            intent.putExtra(<span class=\"string\">\"HtmlBody\"</span>, htmlbody);</span><br><span class=\"line\">            startActivity(intent);</span><br><span class=\"line\">            WelcomeActivity.<span class=\"keyword\">this</span>.finish();</span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;, <span class=\"number\">2500</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0IHL4.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"Android:BuptRoom总结","path":"2017/01/16/buptroomreview/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0IHL4.png","excerpt":"<hr>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>写了一个查询学校空闲教室的APP<br>拉取学校教务处网站的信息，分类显示,还加了一些杂七杂八的<br>毕竟第一次写android，什么都想尝试一下<br>点这下载：<a href=\"https://fir.im/buptroom\" target=\"_blank\" rel=\"noopener\">BuptRoom</a><br>repository地址:<a href=\"https://github.com/thinkwee/BuptRoom\" target=\"_blank\" rel=\"noopener\">一个简单的北邮自习室查询系统</a><br>完成第一个版本大概是3个周末<br>之后花了1个月陆陆续续更新了杂七杂八的<br>很多东西写的不规范，也是临时查到了就用上<br>总结一下这次写App的经过:</p>","date":"2017-01-16T03:56:39.000Z","pv":0,"totalPV":0,"categories":"Android","tags":["code","android"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"图卷积网络学习笔记","date":"2019-07-28T01:37:46.000Z","mathjax":true,"password":"kengbi","html":true,"_content":"***\nGCN(Graph Convolutional Network)的相关笔记，以及其在自然语言处理上的一些应用论文笔记。\n\n<!--more--> \n\n# GCN的形式\n## 动机\n\n## 先验假设\n\n## 与CNN异同\n\n## spectral domain\n\n## spatial domain\n\n## Spectral Networks and Deep Locally Connected Networks on Graphs\n\n## Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n## SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\n\n# 图拉普拉斯\n## 形式\n\n## 物理意义\n\n## 谱图理论\n\n## 图傅里叶变换\n\n# GCN用于自然语言处理\n\n\n\n","source":"_posts/gcn.md","raw":"---\ntitle: 图卷积网络学习笔记\ndate: 2019-07-28 09:37:46\ncategories: 机器学习\ntags:\n  - gcn\n  - machine learning\n  -\tmath\nmathjax: true\npassword: kengbi\nhtml: true\n---\n***\nGCN(Graph Convolutional Network)的相关笔记，以及其在自然语言处理上的一些应用论文笔记。\n\n<!--more--> \n\n# GCN的形式\n## 动机\n\n## 先验假设\n\n## 与CNN异同\n\n## spectral domain\n\n## spatial domain\n\n## Spectral Networks and Deep Locally Connected Networks on Graphs\n\n## Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n## SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\n\n# 图拉普拉斯\n## 形式\n\n## 物理意义\n\n## 谱图理论\n\n## 图傅里叶变换\n\n# GCN用于自然语言处理\n\n\n\n","slug":"gcn","published":1,"updated":"2019-07-31T01:43:21.469Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v3y001tq8t5arlkkzb8","content":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\nU2FsdGVkX19HqunRw3qJWkCmx8UZk/WWHnquwvPmhmUqSl7O7fJynjC8MuYtEbcDhWleRraXTPqxaVwLi7R9uXfKnK0e+b7fYv9ps+zHZLk3AFwReawWmj1K+T/6DthYJNZcYpaJggaDIcNDI66EfjLSwmvep6FchrUjbS9zu0cM4rh0+bq7tmo2YBx3+V3Lko+lR6KriASVQODGshdkuCEh7RDx/j3ZXTicfYY3fU4kM5j9ZRVLRt2jqfs65cLIMhIKUcnq3EIQQzsvAf1XxNg4jYHeBoQVH/9CN55FXUVycO79GhGUlOL90Xznn5AXqJd8IoaylHlIr3BqzZRvYuFmgEhmuBbOo0DOZ801XC0396ZZGMBq3piLgI5h+i4kEQOIESQUwErJRMcCJNu9BphVIQgfVsUKjbzsUdgs2vVB621Xdy0CY6CfhcglzXT5vbI+sw5rz9rI6/dRruZsyjVuLGApaOctSsqDV2U+OaYMRCmTQyJ1SvxQvrEh/fn+A7+/iGvMXHouqWx0yyWget+wi9kgJNYKNr3VnMtjlZWs+0nl9zHaP+2st/xDuHkVByRRJXuVYcD6ZkFGyG17cMTMFyzqSB89O3XOAzw6s1SuxYxIVCBRO2CRpEi9Orl+IaU+0q09+8FLOa7tVTyTHKcWDVdJVhhvOosHE0lplPgUF5Ak0LYf/ByGvaGjJYnZmy5ycoQRuWnUaUAhboAyZrklIePukBeny7SEKuNpJAjneU+W5RAir51OP0bMNMe7wXFVXuzNLrBCLKBicA2sg3w5mVB6aGMP9DAMAiAmKus3uPsJgbiK/uQCuWZMfAxHrV5RutL6jRzj920MamzXydqv4Xt3gdUP5yueOLQ7dr/+3hgcQGcV4w7JSOe18VrFABfVrTXAjzkHgfI1H7jT4nocM2ccWo1JnNX+fA4n9JkRdaKMlixcHv90mR9gaqQZzhIUBxepHzF9ShYmqwwC7T87fJWKr5bwqpj4mWrbioTfPchu7K4HYRWnuW3nBkUpuEMZ2SjBL81lQIJMh0JC0+HGGAqdrGqv2d9fxHFtLK5yAFJd/wAV4+bhKHMfTSooqzgF8l9M/2hYHVii8d+JsT8YX6e1xquCFze4wn4vhRVYQHyaUHoPZ5papRH9vcYhbRm9d3qeFWO3RjYsc6pDQSjLW8q19G7/AXxc9f47uExamSDOrLDqXr9arINf6GgA9kNZT0JtmY7xUuKvPGp91a0k63ivh/09XRLmygrAxycMk0hlsBn36WiFPwRv+gfV/nMIKY2iIGfx88Qbh7UeyJt+kl4cNidDopsgt4V7cd0OZ/5zxEpWz//fyVSWroYalkbDXO0Ontp6V2UtiXjChgJC3U5XeH+4wNchaM9TThgGvMBwT9hva3ekKazHcPHNw+AATg/OBlpz65KyvRelBeJ0gvSbURWKyn01gbMV8lwoy0oMQ+sL7s27uCyt0EvMVFD8HG1qhAIgL72xu4RQlPOkLPiXDC0zbAlujS6Lq1CoXYzuTYZ6zyAs2fnsrySHOz8zBRLmAjGnqqxMp33ctQchHwNw3Gei/4xVHuQ1Yp0WD5WbxF/c0FcQIZk/IkYeGKPXrkBwD/SurJN5S1TOkn+jwwa7XWP8uaoAgahhRBCFdUn2gMngREIlvQQ+k83VqlM2nxCJif19kkth9MOhhAGoDijvy8H0o1ElfNjo/rPd+xekrbsohWIm6ErRS9Z/RXvljzTV7O2tEwxs9Bn5w6bG8UGbxeBmAPytFW1DV1I4qXdi/f+oziYOv484VAnFk3Zy1IuB/NOfFXhd+G6bONT2yjHyo/OyXYkV7HJ85U5nv892VVJl6U+bURiXUOL6bgPLVNkQD+wOoZ8DztAUwKn5vTso40xLLId0XkxwKepz95oOOv7jvsS5psQNIgUrXJvFVoBS3kisCQRGY58NVG31UotJGPA/jd2YLgV933F8C2DEWKnfdFwPP0TFwBvu8JTA+OdARmln1hmyIwP94x3TRRGlGBP095n4yzt1iSnZcG6KVZwihHy1wxv6eaVb1ciPek6Hko52ep4LsweBNq0zPm5BkMYcxHEHimWEH8zaIUQqyeS0Lu/qEsaMqWTs0HIc1ecs/IoHXzucE1bu7MlExmH3RzGUErkkhYC09O2JcI3MK5wRqRQLfoy3QAfEgVCqEFcEU3GwGNGjlfZ9fLaorCn0Q8jnYXD4fmBUdiIYdKICQS6rQd3aR0QKI3FeL/J54nvlOcnRXhQsyPYrmq2b444vZEf9egI4eMRS264lxsR+mF9LPO7L29fj4msMsEsOsOP68GHlfLn1yA0qb7cQppfsRIQLeeINfXw+HVRx0pVy07maKRz/dTtOrMo2lj4+t+MXSY1cUWOkpdXv1LtTmV9qEx/noeL5cN6L3yD93Uh6+EW1pWlJ8+QdbVYvxLVeXhjEBGiR8ntE6053skOfZKlCXsRVyhhW07sw294NjElXbANZvmtaaldTWfHoBog28KJM+eAJtC/uKXcLxgxylRrToNpogOiqZcVXfS3zjS6XOE2revJ09/UjwbRur4+00oP5ZIQZJ0FOdmTqIyIv0VnmqahpG8sLeIe5Q+IKkBJDMfm+k3rJHOGKzF589GMEBjGEGP/WoDsIQnfMKqtZ7m35nG8Eu0TBzkK9Z4vnpDvYjVyvhAASGPN4PIAMxbOLnQaRa5eCJkNqPllcLJUvfofxWtWxLBCDzjIjdwS0H+GCnZRcxBVVR8naMjLw6sqfho7sCIOUrDnkUFy7G+Tv56vEqwV35fj+2OJSoTDCWAFo6qWItZhMGZSJOwdU6wgu2Pc8If/TOBKt8nYeI7i5fgmjfUUr0VUjzn2yZ25Px2p+hoeiPMLVztTf05840mHLo6/HzlcnKwhstS7TGC/87aGT92HVVxCA8OK7IfeqScUjOLTYkxXHn9e9mMCdpl5zc3J8mu4Bo/bUYSzBZYdC2y1dwOEMtFnSw1HeYRpJuOZZ3r1Hi9yEYOnzs8LV5nZc3z9bcJbn4KXIKrwIOt1cj9eBvNKquiQEjhBhyy9/EZ7AsEbl4SydZ6FSAhS8Q/b1LQwioCv8C53rK0A313IHzknyg1LH6lsYN5iSsZYPRPi7YhvON9s38K13yZJLaYohCMbCmO36P9A2DS+TLJc1JMSSEfxGEc6oWZzRuhiXDfHc8RMyzpaoL5g8m17vMaMZFuiEWn+Cs5uxlGLDx9Gv+Gp6HFe+ChXOhtr/4jq8B1vyeW0aTr5ixl1wDUQDnta65njbrMMd19jDsL7xN9WxZF0BSJxmcD3F+UmyLYvcwaatmfH6/Mk/hiXGcxeZTGNR/cFccqGcdJ4oqWLGId8NOwH+ZQuTGtzBSuEh2O3GIsjKpR4oiOfOGeV7HD3GL+tdQUuimX86cJKylRGIRXsh+7yxDZzjylMsDMSCs4Q+CxrAAhTTlJYZ3oM6e6D2J8OlKLo7h2A9nwWUfjPa0nevFzAXnXIMPHtX/tVDnzwXwoz1ceXoVo3CWpQw/8Osu5z1wYusQAoDff36oI4npRvOisNzRW42/VUcY+isp8f7c7H4KRdURYa8NSrv3Eeli1hIXgLeqesAvUhyZMdsMZTewkBiSwJsPX2KUD0kUPdTtctPC1Z8ISO+SazAGoMCAvSCOnkEmQee2Phz/vRPZkeXNJtvyDsxI2ze1aWBj1hOtfb+e3kdcFbZGQcvKSw4wqKNx25n3qUGIaVtz7NhsnfUP3/AVuuDvT0qz1P/m+DVhovKDBfiPxwe3LGqfUSaGgkvMpj/QgSsNkM1WU7wLP6yOU86fPTL/yR3mHEQNwfXzh/9z5JOmY9daAtYOn0uAptldrMzHnnh0UuFoimMlExWQCxtmA8sGfoVPhr8x5ZvoLPo0nyY3twf+47ik5Lx0JYFLx2ho4MK/GKnUnTsEugoJOw4heFWYwcpcivAO282MyK9lw1+YQvdD2f7kzML95V/cVBUKaaD9V+tBQ/5SzhexNNHZtzdr9ymhuNGF1lbAzD+eK17lq+XJM70qZCMZ26G9nhvw1WymO5Yg7ukq4NPzILDHoN4Y1zseJ6p22BSar2SYMPU73KlAKb/iPe6/RlXztPiC6dGzcrckuhHwmZjSqahp+Qdrzt9mZx8gSddAwYePOdFZPtvs04w3omWHNql1U6pS4OmDYWE+t+vIsEdHHUBl7xPaMvbcXxCXigXCJ0Jj8RJJFw6NnocVl8tUQz19zTtOAfMFZRs41uJqzLVINdgsosBxx2ehs1ApvFLILYVfKXGT4EtZNwehaRg5lv8TpTSpcgKr7qNQC1NbTxnZBSpZltslcpfb8PrTJ8MoQI3gPbvBTT6lcmIclYlAk/hLTM4w+urUdqqlqNH7aPahO/Rmfsu9oJHGDG8Xsiyl9v3lR2drsjWxbYEIg/QcNvTBFC8CWD5Nr+Bhjqvxz1Axjd127HjVyQnZaBCz0orH6yXL17zeGFDwi1ExGm/3BqFb51S5YEMKzVb86aRMYI2frn8Z0Sh8HnUC8Coz3g9Kf8HqdiaY++NcSWu0e6UXZ+SKywhCZtDcgN+qe1P8NhMOO0Cjyp0Rmnb3ETmooI9Qi9l0MomNOEDYXTehqNyptBEBXdD3HbmUtkLrBJORhvGfcSVLSkVjQGYSPtbD//jfu674dNuF3mYHG156R5D3mx/CJmE+oqRczj3C7jzaMceNKLIzY84HMdunfQ56s3wn4O5q3+qwkLyqu2pcR7eZtALoug0IjToFL2yktxm+0uvjNZcYqLzD3QSnjvnQPGmmqW/APm3kYJ/L38L8yXvk4E7l/SKuaHC1nfvewAqmhFd0pj6XVK+xSnuNU2g5KbT65Ng+2gv1dn22WqiKKw7GevNSU2twDy99AMwurnw74hpPp90L8P6lBzJE+IT3MlJaBwzElXUJu59MllqiscjfGiXsWVDrSVpeqXb+WhdPzr4M3AC4osly0BWF8hl1/Cu1f3wV0I7Fzs5K9gSVlkSkA6HQwlmoceDB28fSGjad72ipixkH68sUZDfeRmeOug3M86nagTU2KDhTTxQnJfGqvzqDgiO+3bpeyzXTFIu5kTUe32RaIinf63dfFdISg2ek13jKjkZQXZCuj3T3hfO2JIqb5YvMatdvr0Ei1mbiYA/pge44fjKgwkAtmq5ySnn7QkxzMO5L1rrAz3UL/qdhwljWOVEXsQhVrmANreitzVOR0u5sxYhOpwNkkqzPTQ5HbBAipnnvun5/u/zMYIpgWD++wG/qs7R4FRRwPi7FfggTyZHZ7g9463a1C4zJXTb3i1prlOk2VwefoT9qqsySeVlVp0ybOZbzk83I4Z0zNvAsfQ37fYrOUYY5Jr/jpy4aWAIaBYfg94vmdb93sQLUjSrgI3MzPpn813jjXPF02datxYVSiajIfS0JVAy/1soNkl++0WPoA2nwJlSfUXg7RQznuZx1MKA2b1BszoUUHdOhJL+wO7/d5jCcmeN4aPfIoRfNzr8GatKHwXY6Bzg/uSanYcf3iqpaRvuvEeL7kDUZpZpEX1aDOqFWGtl9PYYHpDrArD8jSG3rrjdWXh1B9yLIBXrJnI9+rHyu2ewMlXKl1aXkvqMTm24PA9rpY9hy4xqCkQRuHNvzpMAWJjwa04El77oMnTtvWIBxB+u7vUo0vVdagPJiaF9eewTOfplgufAMfAVodIX8vtTioJK5Lu6mE8Z8V7fNGIzx7kCFkeoQgRWebcCFc8RQ2riPCnExeGiow7VfURj0nDBfrxj6w1OQDlRwrFW0Tg3BkcJMQ4HgXqfrsiJ9uLhjlvj4JqVeMoRI1cfegNxyc0bVi9/eqmkYrCXFR5qmIyoa1sTg5WB9sEM0dGxEgffGmdMgWTd5j91ZjJESh1miXlWI2NMQhpALkxi7jAzIWevTdN08AHpZozYDqQeqQJDmCjjyxEPanEMeS6VuqOJZGuT+iyG95h9PRCuC5gYMV5iwFiJYlSMIum8Pxas2ziY7cIU1nl8aeK4zTIQJHVnbGRx1DCubTo/AsjnYIgT1bsHceecRvSXzxJlLRR6PhZHPXiIsuS5ZJ/kMs7mrJrmV6v//rQY7YlcnNem+5JHGuTZ9QvCkhnvJn6lxm3LzeyiLhgHiiqf1p+mxYRmFWkNO7xMetN5AtMnr4pp3ohJafXvUwPIZnYDMIR345QeV1xUL6Q89gXC139+Gjn4OhPzfDGRwpYdr3vepNQ+jTVPGrbsMnzJeYBRS5utJS11+tWPu6yrm7OLQZR05FAVv9svSxAYNKKEgzpOyv9CtX6Mrx+bY9srnMLWKsC/9x42I+0TfN9yaapZdITuBnWvs3vjluBl12kpV0/fWSAr9Pyn+sb7ErACM+lJt3fSsMWv9+73Oxx82FUUnc3wvUXJE8NSKM56/bnk/El+tzBIGxtAfV199U1y43xEEi1qHDZjUb+RBxyzrtiOpVsJIv8CxSD8a+ngfzlBH1tgPrQql3eFG8q1lYkIQgGwsxiWE8AzU7NlZ4Goe2fVpBPA3SSpG7iSHdMx9VZz49r1h9T7vcTRQRGrssnkuM+b2LewmTQmXe/d4vyanuVFxsk4OQlNjFtQ8rnk3KS7kO7JHp0DUn4hhjOdgDyz0QWck117F8D+SQ+tNxGdQL053DNwC00fs8AD1NMfrhCodeO9vTZB1ozJL8qL3snaKxvcbnP5uum1qHGv9Lkoc8sUooNjbnKJD65NDf3G67W0OR2bSc/GE7X4KAkuo4Upac/qSQ8z+n4PeoW335ayw4q7ffGwmMq6/qdXzFtN1+FwtLbumAIpHdh0un8mGLjymJHmntUB45ADUP0Db6P7hY13nqFOO3mZDR2WOzktGjl8AN5DIhLrIQyj5XOU8i1wDZNMYAS0BpWzZuqN0N6wITKc9uax25ATlyFpjOG6bHCnuJm6p68AEtIN2IcaPk+nrShZ7kPFudbnR2dIxSC7CMOq/+usn/9n0qLibMzr0x8LaCQopLfSNUG7v9ohVCWWi55deYZRZ+t481MXzidBd4m+NaTta5eLpTsYlORnEJk1nmUtZ/f32xOvgoJOLZZxbw==\n</div>\n<script src=\"/lib/crypto-js.js\"></script><script src=\"/lib/blog-encrypt.js\"></script><link href=\"/css/blog-encrypt.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"这个坑还没填完，正努力填坑中......</br>","more":"这个坑还没填完，正努力填坑中......</br>","origin":"<hr>\n<p>GCN(Graph Convolutional Network)的相关笔记，以及其在自然语言处理上的一些应用论文笔记。</p>\n<a id=\"more\"></a> \n<h1 id=\"GCN的形式\"><a href=\"#GCN的形式\" class=\"headerlink\" title=\"GCN的形式\"></a>GCN的形式</h1><h2 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h2><h2 id=\"先验假设\"><a href=\"#先验假设\" class=\"headerlink\" title=\"先验假设\"></a>先验假设</h2><h2 id=\"与CNN异同\"><a href=\"#与CNN异同\" class=\"headerlink\" title=\"与CNN异同\"></a>与CNN异同</h2><h2 id=\"spectral-domain\"><a href=\"#spectral-domain\" class=\"headerlink\" title=\"spectral domain\"></a>spectral domain</h2><h2 id=\"spatial-domain\"><a href=\"#spatial-domain\" class=\"headerlink\" title=\"spatial domain\"></a>spatial domain</h2><h2 id=\"Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs\"><a href=\"#Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs\" class=\"headerlink\" title=\"Spectral Networks and Deep Locally Connected Networks on Graphs\"></a>Spectral Networks and Deep Locally Connected Networks on Graphs</h2><h2 id=\"Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering\"><a href=\"#Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering\" class=\"headerlink\" title=\"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\"></a>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</h2><h2 id=\"SEMI-SUPERVISED-CLASSIFICATION-WITH-GRAPH-CONVOLUTIONAL-NETWORKS\"><a href=\"#SEMI-SUPERVISED-CLASSIFICATION-WITH-GRAPH-CONVOLUTIONAL-NETWORKS\" class=\"headerlink\" title=\"SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\"></a>SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</h2><h1 id=\"图拉普拉斯\"><a href=\"#图拉普拉斯\" class=\"headerlink\" title=\"图拉普拉斯\"></a>图拉普拉斯</h1><h2 id=\"形式\"><a href=\"#形式\" class=\"headerlink\" title=\"形式\"></a>形式</h2><h2 id=\"物理意义\"><a href=\"#物理意义\" class=\"headerlink\" title=\"物理意义\"></a>物理意义</h2><h2 id=\"谱图理论\"><a href=\"#谱图理论\" class=\"headerlink\" title=\"谱图理论\"></a>谱图理论</h2><h2 id=\"图傅里叶变换\"><a href=\"#图傅里叶变换\" class=\"headerlink\" title=\"图傅里叶变换\"></a>图傅里叶变换</h2><h1 id=\"GCN用于自然语言处理\"><a href=\"#GCN用于自然语言处理\" class=\"headerlink\" title=\"GCN用于自然语言处理\"></a>GCN用于自然语言处理</h1>","encrypt":true,"abstract":"这个坑还没填完，正努力填坑中......</br>","template":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\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\n</div>\n","message":"文章还没写完，稍后再读，或者输入kengbi看看草稿","decryptionError":"Incorrect Password!","noContentError":"No content to display!","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Wed Jul 31 2019 09:43:21 GMT+0800 (GMT+08:00)","title":"图卷积网络学习笔记","path":"2019/07/28/gcn/","eyeCatchImage":null,"excerpt":"这个坑还没填完，正努力填坑中......</br>","date":"2019-07-28T01:37:46.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","machine learning","gcn"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Glove数学推导","date":"2019-01-13T01:42:37.000Z","mathjax":true,"html":true,"_content":"***\n-\t记录一下Glove词向量的数学推导，因为原论文不是画模型得出的，而是纯数学操作计算得到的目标函数，这种设计方式非常有意思，而且还将word2vec的数学本质写出来进行了对比。\n-\t原论文：*GloVe: Global Vectors for Word Representation*\n\n<!--more-->\n\n# 词向量\n-\t无论是基于全局矩阵分解的还是基于局部窗口的词向量，其提取semantic的方式都是从词与词的共现统计信息中挖掘意义。\n-\t显然，全局的方式没有利用到局部的优点：全局例如LSA等技术对于局部上下文信息不敏感，难以根据上下文挖掘近义词；局部的方式没有利用到全局的优点，它只依赖于独立的局部上下文，窗口太小的话不能有效利用整个文档乃至语料的信息。\n-\tGlove的思路是利用全局的词与词共现矩阵，同时利用局部上下文关系计算相关性。\n-\t词向量的结果是能产生有意义的语义关系到距离关系的映射，针对这个目标，Glove设计了一个log-bilinear回归模型，并具体采用一个加权最小均方回归模型来训练词向量。\n\n# 发现\n-\t定义：\n\t-\t$x$：为单个词。\n\t-\t$X_{ij}$：$x_j$ 出现在$x_i$的上下文中的次数。\n\t-\t$X_i = \\sum _k x_{ik}$：所有词出现在$x_i$的上下文中的次数。\n\t-\t$P_{ij} = P(j|i) = \\frac {x_{ij}} {X_i}$：$x_j$出现在$x_i$的上下文中的概率，即上下文出现频次计数概率化，论文中称之为\"co-occurrence probabilities\"。\n\t-\t$r = \\frac {P_{ik}}{P_{jk}}$：引入中间词$x_k$，论文中叫\"probe word\"，通过引入这个$x_k$可以间接的衡量$x_i$和$x_j$的关系，通过$r$即ratio表示。\n-\t$r$引入的作用体现在两个方面：\n\t-\t对于要比较的$x_i$和$x_j$，筛除对于没有区分度的$x_k$，也就是噪音。当$r \\approx 1$时，$x_k$即为噪音。\n\t-\t给定$x_k$，使得$r >> 1$的那些$x_i$具有相近的词义，使得$r << 1$的那些$x_j$具有相近的词义。\n-\t因此，我们可以过滤噪音，仅仅在$r$很大或很小的词共现数据中挖掘词义关系。\n\n# 设计\n-\t接下来，作者直接根据目标设计函数。\n-\t目标是：设计出来的词向量之间的距离计算结果应该能够反映之前我们从词共现矩阵中发现的ratio。\n-\t那么直接设计,定义$w_i$为$x_i$对应的词向量，则假设$F$为计算距离的函数：\n\t$$\n\tF(w_i,w_j,w^{\\*}_k) = r \\\\\n\t= \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t上面$w_k$的词向量加了星号区别于$w_i$和$w_j$的词向量，因为$w_k$是独立的上下文词向量，与我们需要的词向量是平行的两套，类似于word2vec里面的前后词嵌入矩阵。\n-\t接下来，一个自然的想法是，减少参数，即只需要词向量和上下文词向量，因为是距离计算函数且向量空间是线性空间，我们使用$w_i$和$w_j$的向量差作为参数：\n\t$$\n\tF(w_i - w_j,w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t现在函数的参数是向量，输出是张量，最简单的一个结构就是做点乘：\n\t$$\n\tF((w_i-w_j)^T w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t注意到虽然区分了上下文和非上下文词向量，但是$x_i,x_j,x_k$可以是语料中任意词，因此$F$函数的两个参数应该是可以交换位置（$w$和$w^{\\*}$，$X$和$X^T$），即函数应该是对称的。这里运用了一点数学技巧将函数对称化：\n\t-\t设计：\n\t\t$$\n\t\tF((w_i-w_j)^T w^{\\*}_k) = \\frac {F(w_i w^{\\*}_k)} {F(w_j w^{\\*}_k)} \\\\\n\t\t$$\n\t\t其中\n\t\t$$\n\t\tF(w_i w^{\\*}_k) = P_{ik} = \\frac {X_{ik}} {X_i} \\\\\n\t\t$$\n\t-\t要满足上面$F$可以拆分为两个子$F$的比，则$F$可以为$exp$函数，即\n\t\t$$\n\t\tw_i^T w_k^{\\*} = log(X_{ik}) - log {X_i} \\\\\n\t\t$$\n\t-\t这样k,i,j下标可互换位置且表达意思一致。但是注意到上面两个log式子相减并不是对称的，因此我们补上一个$log{x_k}$使之对称，并将其简化为偏置$b$：\n\t\t$$\n\t\tw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(X_{ik}) \\\\\n\t\t$$\n\t-\t最后加上平滑，防止log的参数取0：\n\t\t$$\n\t\tw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(1 + X_{ik}) \\\\\n\t\t$$\n-\t到这里我们已经初步完成了$F$函数的设计，但这个还存在的一个问题是，它是平均加权每一个共现的，而一般语料中大部分共现都频次很低\n-\tGlove的解决办法是使用加权函数。加权之后将词向量的训练看成是F函数的最小均方误差回归，设计损失函数：\n\t$$\n\tJ = \\sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{\\*} + b_i + b_j^{\\*} - log (1 + X_{ij}))^2 \\\\\n\t$$\n-\t其中f为加权函数，其参数是共现频次，作者指出该函数必须满足三条性质：\n\t-\t$f(0)=0$：显然，没有出现共现则权重为0。\n\t-\tNon-decreasing：共现频次越大则权重越大。\n\t-\trelatively small for large X：防止对于某些频次很高的常见共现加权过大，影响结果。\n-\t基于以上三种性质，作者设计了截尾的加权函数，在阈值$X_{max}$以内：\n\t$$\n\tf(x) = (\\frac {x}{X_{max}}) ^ {\\alpha} \\\\\n\t$$\n\t超过阈值则函数值为1.\n\n# 与Word2vec比较\n-\t对于Word2vec中的skip-gram模型，其目标是最大化给定上下文之后预测正确中心词的概率，一般通过softmax函数将其概率化，即：\n\t$$\n\tQ_{ij} = \\frac {exp (w_i^T w_j^{\\*})} { \\sum _{k=1}^V exp(w_i^T w_k^{\\*})} \\\\\n\t$$\n-\t通过梯度下降求解，则整体损失函数可以写成：\n\t$$\n\tJ = - \\sum _{i \\in corpus , j \\in context(i)} log Q_{ij} \\\\\n\t$$\n-\t将相同的$Q_{ij}$先分组再累加，得到：\n\t$$\n\tJ = - \\sum _{i=1}^V \\sum _{j=1}^V X_{ij} log Q_{ij} \\\\\n\t$$\n-\t接下来用之前定义的符号进一步变换：\n\t$$\n\tJ = - \\sum _{i=1^V} X_i \\sum _{j=1}^V P_{ij} log Q_{ij} \\\\\n\t= \\sum _{i=1}^V X_i H(P_i,Q_i) \\\\\n\t$$\n-\t也就是说，Word2vec的损失函数实际上是加权的交叉熵，然而交叉熵只是一种可能的度量，且具有很多缺点：\n\t-\t需要归一化的概率作为参数\n\t-\tsoftmax计算量大，称为模型的计算瓶颈\n\t-\t对于长尾分布，交叉熵常常分配给不太可能的项太多权重\n-\t解决以上问题的方法：干脆不归一化，直接用共现计数，不用交叉熵和softmax，直接用均方误差，令$Q_{ij} = exp(w_i^T w_j^{\\*})$，$P_{ij} = X_{ij}$，则：\n\t$$\n\tJ = \\sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\\\\n\t$$\n-\t但是不归一化会造成数值上溢，那就再取个对数：\n\t$$\n\tJ = \\sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\\\\n\t=  \\sum _{i,j} X_i (w_i^T w_j^{\\*} - log X_{ij})^2 \\\\\n\t$$\n-\t这样就得到了Glove最朴素的目标函数。\n-\tWord2vec的作者发现筛除一些常见词能够提高词向量效果，而Word2vec中的加权函数即$f(X_i)=X_i$，因此筛除常见词等价于设计一个非降的加权函数。Glove则设计了更为精巧的加权函数。\n-\t因此从数学公式推导上看，Glove简化了Word2vec的目标函数，用均方误差替换交叉熵，并重新设计了加权函数。\n\n# 思路\n-\t该文提供了一个很好的设计模型的思路，即根据评测指标设计目标函数，反过来训练模型，得到函数的参数（副产品）作为所需的结果。\n","source":"_posts/glove.md","raw":"---\ntitle: Glove数学推导\ndate: 2019-01-13 09:42:37\ncategories: 机器学习\ntags:\n  - glove\n  - math\n  -\tword embedding\nmathjax: true\nhtml: true\n---\n***\n-\t记录一下Glove词向量的数学推导，因为原论文不是画模型得出的，而是纯数学操作计算得到的目标函数，这种设计方式非常有意思，而且还将word2vec的数学本质写出来进行了对比。\n-\t原论文：*GloVe: Global Vectors for Word Representation*\n\n<!--more-->\n\n# 词向量\n-\t无论是基于全局矩阵分解的还是基于局部窗口的词向量，其提取semantic的方式都是从词与词的共现统计信息中挖掘意义。\n-\t显然，全局的方式没有利用到局部的优点：全局例如LSA等技术对于局部上下文信息不敏感，难以根据上下文挖掘近义词；局部的方式没有利用到全局的优点，它只依赖于独立的局部上下文，窗口太小的话不能有效利用整个文档乃至语料的信息。\n-\tGlove的思路是利用全局的词与词共现矩阵，同时利用局部上下文关系计算相关性。\n-\t词向量的结果是能产生有意义的语义关系到距离关系的映射，针对这个目标，Glove设计了一个log-bilinear回归模型，并具体采用一个加权最小均方回归模型来训练词向量。\n\n# 发现\n-\t定义：\n\t-\t$x$：为单个词。\n\t-\t$X_{ij}$：$x_j$ 出现在$x_i$的上下文中的次数。\n\t-\t$X_i = \\sum _k x_{ik}$：所有词出现在$x_i$的上下文中的次数。\n\t-\t$P_{ij} = P(j|i) = \\frac {x_{ij}} {X_i}$：$x_j$出现在$x_i$的上下文中的概率，即上下文出现频次计数概率化，论文中称之为\"co-occurrence probabilities\"。\n\t-\t$r = \\frac {P_{ik}}{P_{jk}}$：引入中间词$x_k$，论文中叫\"probe word\"，通过引入这个$x_k$可以间接的衡量$x_i$和$x_j$的关系，通过$r$即ratio表示。\n-\t$r$引入的作用体现在两个方面：\n\t-\t对于要比较的$x_i$和$x_j$，筛除对于没有区分度的$x_k$，也就是噪音。当$r \\approx 1$时，$x_k$即为噪音。\n\t-\t给定$x_k$，使得$r >> 1$的那些$x_i$具有相近的词义，使得$r << 1$的那些$x_j$具有相近的词义。\n-\t因此，我们可以过滤噪音，仅仅在$r$很大或很小的词共现数据中挖掘词义关系。\n\n# 设计\n-\t接下来，作者直接根据目标设计函数。\n-\t目标是：设计出来的词向量之间的距离计算结果应该能够反映之前我们从词共现矩阵中发现的ratio。\n-\t那么直接设计,定义$w_i$为$x_i$对应的词向量，则假设$F$为计算距离的函数：\n\t$$\n\tF(w_i,w_j,w^{\\*}_k) = r \\\\\n\t= \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t上面$w_k$的词向量加了星号区别于$w_i$和$w_j$的词向量，因为$w_k$是独立的上下文词向量，与我们需要的词向量是平行的两套，类似于word2vec里面的前后词嵌入矩阵。\n-\t接下来，一个自然的想法是，减少参数，即只需要词向量和上下文词向量，因为是距离计算函数且向量空间是线性空间，我们使用$w_i$和$w_j$的向量差作为参数：\n\t$$\n\tF(w_i - w_j,w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t现在函数的参数是向量，输出是张量，最简单的一个结构就是做点乘：\n\t$$\n\tF((w_i-w_j)^T w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\\n\t$$\n-\t注意到虽然区分了上下文和非上下文词向量，但是$x_i,x_j,x_k$可以是语料中任意词，因此$F$函数的两个参数应该是可以交换位置（$w$和$w^{\\*}$，$X$和$X^T$），即函数应该是对称的。这里运用了一点数学技巧将函数对称化：\n\t-\t设计：\n\t\t$$\n\t\tF((w_i-w_j)^T w^{\\*}_k) = \\frac {F(w_i w^{\\*}_k)} {F(w_j w^{\\*}_k)} \\\\\n\t\t$$\n\t\t其中\n\t\t$$\n\t\tF(w_i w^{\\*}_k) = P_{ik} = \\frac {X_{ik}} {X_i} \\\\\n\t\t$$\n\t-\t要满足上面$F$可以拆分为两个子$F$的比，则$F$可以为$exp$函数，即\n\t\t$$\n\t\tw_i^T w_k^{\\*} = log(X_{ik}) - log {X_i} \\\\\n\t\t$$\n\t-\t这样k,i,j下标可互换位置且表达意思一致。但是注意到上面两个log式子相减并不是对称的，因此我们补上一个$log{x_k}$使之对称，并将其简化为偏置$b$：\n\t\t$$\n\t\tw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(X_{ik}) \\\\\n\t\t$$\n\t-\t最后加上平滑，防止log的参数取0：\n\t\t$$\n\t\tw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(1 + X_{ik}) \\\\\n\t\t$$\n-\t到这里我们已经初步完成了$F$函数的设计，但这个还存在的一个问题是，它是平均加权每一个共现的，而一般语料中大部分共现都频次很低\n-\tGlove的解决办法是使用加权函数。加权之后将词向量的训练看成是F函数的最小均方误差回归，设计损失函数：\n\t$$\n\tJ = \\sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{\\*} + b_i + b_j^{\\*} - log (1 + X_{ij}))^2 \\\\\n\t$$\n-\t其中f为加权函数，其参数是共现频次，作者指出该函数必须满足三条性质：\n\t-\t$f(0)=0$：显然，没有出现共现则权重为0。\n\t-\tNon-decreasing：共现频次越大则权重越大。\n\t-\trelatively small for large X：防止对于某些频次很高的常见共现加权过大，影响结果。\n-\t基于以上三种性质，作者设计了截尾的加权函数，在阈值$X_{max}$以内：\n\t$$\n\tf(x) = (\\frac {x}{X_{max}}) ^ {\\alpha} \\\\\n\t$$\n\t超过阈值则函数值为1.\n\n# 与Word2vec比较\n-\t对于Word2vec中的skip-gram模型，其目标是最大化给定上下文之后预测正确中心词的概率，一般通过softmax函数将其概率化，即：\n\t$$\n\tQ_{ij} = \\frac {exp (w_i^T w_j^{\\*})} { \\sum _{k=1}^V exp(w_i^T w_k^{\\*})} \\\\\n\t$$\n-\t通过梯度下降求解，则整体损失函数可以写成：\n\t$$\n\tJ = - \\sum _{i \\in corpus , j \\in context(i)} log Q_{ij} \\\\\n\t$$\n-\t将相同的$Q_{ij}$先分组再累加，得到：\n\t$$\n\tJ = - \\sum _{i=1}^V \\sum _{j=1}^V X_{ij} log Q_{ij} \\\\\n\t$$\n-\t接下来用之前定义的符号进一步变换：\n\t$$\n\tJ = - \\sum _{i=1^V} X_i \\sum _{j=1}^V P_{ij} log Q_{ij} \\\\\n\t= \\sum _{i=1}^V X_i H(P_i,Q_i) \\\\\n\t$$\n-\t也就是说，Word2vec的损失函数实际上是加权的交叉熵，然而交叉熵只是一种可能的度量，且具有很多缺点：\n\t-\t需要归一化的概率作为参数\n\t-\tsoftmax计算量大，称为模型的计算瓶颈\n\t-\t对于长尾分布，交叉熵常常分配给不太可能的项太多权重\n-\t解决以上问题的方法：干脆不归一化，直接用共现计数，不用交叉熵和softmax，直接用均方误差，令$Q_{ij} = exp(w_i^T w_j^{\\*})$，$P_{ij} = X_{ij}$，则：\n\t$$\n\tJ = \\sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\\\\n\t$$\n-\t但是不归一化会造成数值上溢，那就再取个对数：\n\t$$\n\tJ = \\sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\\\\n\t=  \\sum _{i,j} X_i (w_i^T w_j^{\\*} - log X_{ij})^2 \\\\\n\t$$\n-\t这样就得到了Glove最朴素的目标函数。\n-\tWord2vec的作者发现筛除一些常见词能够提高词向量效果，而Word2vec中的加权函数即$f(X_i)=X_i$，因此筛除常见词等价于设计一个非降的加权函数。Glove则设计了更为精巧的加权函数。\n-\t因此从数学公式推导上看，Glove简化了Word2vec的目标函数，用均方误差替换交叉熵，并重新设计了加权函数。\n\n# 思路\n-\t该文提供了一个很好的设计模型的思路，即根据评测指标设计目标函数，反过来训练模型，得到函数的参数（副产品）作为所需的结果。\n","slug":"glove","published":1,"updated":"2019-07-22T09:35:20.866Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v46001yq8t5evnnqfb6","content":"<hr>\n<ul>\n<li>记录一下Glove词向量的数学推导，因为原论文不是画模型得出的，而是纯数学操作计算得到的目标函数，这种设计方式非常有意思，而且还将word2vec的数学本质写出来进行了对比。</li>\n<li>原论文：<em>GloVe: Global Vectors for Word Representation</em></li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"词向量\"><a href=\"#词向量\" class=\"headerlink\" title=\"词向量\"></a>词向量</h1><ul>\n<li>无论是基于全局矩阵分解的还是基于局部窗口的词向量，其提取semantic的方式都是从词与词的共现统计信息中挖掘意义。</li>\n<li>显然，全局的方式没有利用到局部的优点：全局例如LSA等技术对于局部上下文信息不敏感，难以根据上下文挖掘近义词；局部的方式没有利用到全局的优点，它只依赖于独立的局部上下文，窗口太小的话不能有效利用整个文档乃至语料的信息。</li>\n<li>Glove的思路是利用全局的词与词共现矩阵，同时利用局部上下文关系计算相关性。</li>\n<li>词向量的结果是能产生有意义的语义关系到距离关系的映射，针对这个目标，Glove设计了一个log-bilinear回归模型，并具体采用一个加权最小均方回归模型来训练词向量。</li>\n</ul>\n<h1 id=\"发现\"><a href=\"#发现\" class=\"headerlink\" title=\"发现\"></a>发现</h1><ul>\n<li>定义：<ul>\n<li>$x$：为单个词。</li>\n<li>$X_{ij}$：$x_j$ 出现在$x_i$的上下文中的次数。</li>\n<li>$X_i = \\sum _k x_{ik}$：所有词出现在$x_i$的上下文中的次数。</li>\n<li>$P_{ij} = P(j|i) = \\frac {x_{ij}} {X_i}$：$x_j$出现在$x_i$的上下文中的概率，即上下文出现频次计数概率化，论文中称之为”co-occurrence probabilities”。</li>\n<li>$r = \\frac {P_{ik}}{P_{jk}}$：引入中间词$x_k$，论文中叫”probe word”，通过引入这个$x_k$可以间接的衡量$x_i$和$x_j$的关系，通过$r$即ratio表示。</li>\n</ul>\n</li>\n<li>$r$引入的作用体现在两个方面：<ul>\n<li>对于要比较的$x_i$和$x_j$，筛除对于没有区分度的$x_k$，也就是噪音。当$r \\approx 1$时，$x_k$即为噪音。</li>\n<li>给定$x_k$，使得$r &gt;&gt; 1$的那些$x_i$具有相近的词义，使得$r &lt;&lt; 1$的那些$x_j$具有相近的词义。</li>\n</ul>\n</li>\n<li>因此，我们可以过滤噪音，仅仅在$r$很大或很小的词共现数据中挖掘词义关系。</li>\n</ul>\n<h1 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h1><ul>\n<li>接下来，作者直接根据目标设计函数。</li>\n<li>目标是：设计出来的词向量之间的距离计算结果应该能够反映之前我们从词共现矩阵中发现的ratio。</li>\n<li>那么直接设计,定义$w_i$为$x_i$对应的词向量，则假设$F$为计算距离的函数：<script type=\"math/tex; mode=display\">\nF(w_i,w_j,w^{\\*}_k) = r \\\\\n= \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>上面$w_k$的词向量加了星号区别于$w_i$和$w_j$的词向量，因为$w_k$是独立的上下文词向量，与我们需要的词向量是平行的两套，类似于word2vec里面的前后词嵌入矩阵。</li>\n<li>接下来，一个自然的想法是，减少参数，即只需要词向量和上下文词向量，因为是距离计算函数且向量空间是线性空间，我们使用$w_i$和$w_j$的向量差作为参数：<script type=\"math/tex; mode=display\">\nF(w_i - w_j,w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>现在函数的参数是向量，输出是张量，最简单的一个结构就是做点乘：<script type=\"math/tex; mode=display\">\nF((w_i-w_j)^T w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>注意到虽然区分了上下文和非上下文词向量，但是$x_i,x_j,x_k$可以是语料中任意词，因此$F$函数的两个参数应该是可以交换位置（$w$和$w^{*}$，$X$和$X^T$），即函数应该是对称的。这里运用了一点数学技巧将函数对称化：<ul>\n<li>设计：<script type=\"math/tex; mode=display\">\nF((w_i-w_j)^T w^{\\*}_k) = \\frac {F(w_i w^{\\*}_k)} {F(w_j w^{\\*}_k)} \\\\</script>其中<script type=\"math/tex; mode=display\">\nF(w_i w^{\\*}_k) = P_{ik} = \\frac {X_{ik}} {X_i} \\\\</script></li>\n<li>要满足上面$F$可以拆分为两个子$F$的比，则$F$可以为$exp$函数，即<script type=\"math/tex; mode=display\">\nw_i^T w_k^{\\*} = log(X_{ik}) - log {X_i} \\\\</script></li>\n<li>这样k,i,j下标可互换位置且表达意思一致。但是注意到上面两个log式子相减并不是对称的，因此我们补上一个$log{x_k}$使之对称，并将其简化为偏置$b$：<script type=\"math/tex; mode=display\">\nw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(X_{ik}) \\\\</script></li>\n<li>最后加上平滑，防止log的参数取0：<script type=\"math/tex; mode=display\">\nw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(1 + X_{ik}) \\\\</script></li>\n</ul>\n</li>\n<li>到这里我们已经初步完成了$F$函数的设计，但这个还存在的一个问题是，它是平均加权每一个共现的，而一般语料中大部分共现都频次很低</li>\n<li>Glove的解决办法是使用加权函数。加权之后将词向量的训练看成是F函数的最小均方误差回归，设计损失函数：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{\\*} + b_i + b_j^{\\*} - log (1 + X_{ij}))^2 \\\\</script></li>\n<li>其中f为加权函数，其参数是共现频次，作者指出该函数必须满足三条性质：<ul>\n<li>$f(0)=0$：显然，没有出现共现则权重为0。</li>\n<li>Non-decreasing：共现频次越大则权重越大。</li>\n<li>relatively small for large X：防止对于某些频次很高的常见共现加权过大，影响结果。</li>\n</ul>\n</li>\n<li>基于以上三种性质，作者设计了截尾的加权函数，在阈值$X_{max}$以内：<script type=\"math/tex; mode=display\">\nf(x) = (\\frac {x}{X_{max}}) ^ {\\alpha} \\\\</script>超过阈值则函数值为1.</li>\n</ul>\n<h1 id=\"与Word2vec比较\"><a href=\"#与Word2vec比较\" class=\"headerlink\" title=\"与Word2vec比较\"></a>与Word2vec比较</h1><ul>\n<li>对于Word2vec中的skip-gram模型，其目标是最大化给定上下文之后预测正确中心词的概率，一般通过softmax函数将其概率化，即：<script type=\"math/tex; mode=display\">\nQ_{ij} = \\frac {exp (w_i^T w_j^{\\*})} { \\sum _{k=1}^V exp(w_i^T w_k^{\\*})} \\\\</script></li>\n<li>通过梯度下降求解，则整体损失函数可以写成：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i \\in corpus , j \\in context(i)} log Q_{ij} \\\\</script></li>\n<li>将相同的$Q_{ij}$先分组再累加，得到：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i=1}^V \\sum _{j=1}^V X_{ij} log Q_{ij} \\\\</script></li>\n<li>接下来用之前定义的符号进一步变换：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i=1^V} X_i \\sum _{j=1}^V P_{ij} log Q_{ij} \\\\\n= \\sum _{i=1}^V X_i H(P_i,Q_i) \\\\</script></li>\n<li>也就是说，Word2vec的损失函数实际上是加权的交叉熵，然而交叉熵只是一种可能的度量，且具有很多缺点：<ul>\n<li>需要归一化的概率作为参数</li>\n<li>softmax计算量大，称为模型的计算瓶颈</li>\n<li>对于长尾分布，交叉熵常常分配给不太可能的项太多权重</li>\n</ul>\n</li>\n<li>解决以上问题的方法：干脆不归一化，直接用共现计数，不用交叉熵和softmax，直接用均方误差，令$Q_{ij} = exp(w_i^T w_j^{*})$，$P_{ij} = X_{ij}$，则：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\\\</script></li>\n<li>但是不归一化会造成数值上溢，那就再取个对数：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\\\\n=  \\sum _{i,j} X_i (w_i^T w_j^{\\*} - log X_{ij})^2 \\\\</script></li>\n<li>这样就得到了Glove最朴素的目标函数。</li>\n<li>Word2vec的作者发现筛除一些常见词能够提高词向量效果，而Word2vec中的加权函数即$f(X_i)=X_i$，因此筛除常见词等价于设计一个非降的加权函数。Glove则设计了更为精巧的加权函数。</li>\n<li>因此从数学公式推导上看，Glove简化了Word2vec的目标函数，用均方误差替换交叉熵，并重新设计了加权函数。</li>\n</ul>\n<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><ul>\n<li>该文提供了一个很好的设计模型的思路，即根据评测指标设计目标函数，反过来训练模型，得到函数的参数（副产品）作为所需的结果。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<ul>\n<li>记录一下Glove词向量的数学推导，因为原论文不是画模型得出的，而是纯数学操作计算得到的目标函数，这种设计方式非常有意思，而且还将word2vec的数学本质写出来进行了对比。</li>\n<li>原论文：<em>GloVe: Global Vectors for Word Representation</em></li>\n</ul>","more":"<h1 id=\"词向量\"><a href=\"#词向量\" class=\"headerlink\" title=\"词向量\"></a>词向量</h1><ul>\n<li>无论是基于全局矩阵分解的还是基于局部窗口的词向量，其提取semantic的方式都是从词与词的共现统计信息中挖掘意义。</li>\n<li>显然，全局的方式没有利用到局部的优点：全局例如LSA等技术对于局部上下文信息不敏感，难以根据上下文挖掘近义词；局部的方式没有利用到全局的优点，它只依赖于独立的局部上下文，窗口太小的话不能有效利用整个文档乃至语料的信息。</li>\n<li>Glove的思路是利用全局的词与词共现矩阵，同时利用局部上下文关系计算相关性。</li>\n<li>词向量的结果是能产生有意义的语义关系到距离关系的映射，针对这个目标，Glove设计了一个log-bilinear回归模型，并具体采用一个加权最小均方回归模型来训练词向量。</li>\n</ul>\n<h1 id=\"发现\"><a href=\"#发现\" class=\"headerlink\" title=\"发现\"></a>发现</h1><ul>\n<li>定义：<ul>\n<li>$x$：为单个词。</li>\n<li>$X_{ij}$：$x_j$ 出现在$x_i$的上下文中的次数。</li>\n<li>$X_i = \\sum _k x_{ik}$：所有词出现在$x_i$的上下文中的次数。</li>\n<li>$P_{ij} = P(j|i) = \\frac {x_{ij}} {X_i}$：$x_j$出现在$x_i$的上下文中的概率，即上下文出现频次计数概率化，论文中称之为”co-occurrence probabilities”。</li>\n<li>$r = \\frac {P_{ik}}{P_{jk}}$：引入中间词$x_k$，论文中叫”probe word”，通过引入这个$x_k$可以间接的衡量$x_i$和$x_j$的关系，通过$r$即ratio表示。</li>\n</ul>\n</li>\n<li>$r$引入的作用体现在两个方面：<ul>\n<li>对于要比较的$x_i$和$x_j$，筛除对于没有区分度的$x_k$，也就是噪音。当$r \\approx 1$时，$x_k$即为噪音。</li>\n<li>给定$x_k$，使得$r &gt;&gt; 1$的那些$x_i$具有相近的词义，使得$r &lt;&lt; 1$的那些$x_j$具有相近的词义。</li>\n</ul>\n</li>\n<li>因此，我们可以过滤噪音，仅仅在$r$很大或很小的词共现数据中挖掘词义关系。</li>\n</ul>\n<h1 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h1><ul>\n<li>接下来，作者直接根据目标设计函数。</li>\n<li>目标是：设计出来的词向量之间的距离计算结果应该能够反映之前我们从词共现矩阵中发现的ratio。</li>\n<li>那么直接设计,定义$w_i$为$x_i$对应的词向量，则假设$F$为计算距离的函数：<script type=\"math/tex; mode=display\">\nF(w_i,w_j,w^{\\*}_k) = r \\\\\n= \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>上面$w_k$的词向量加了星号区别于$w_i$和$w_j$的词向量，因为$w_k$是独立的上下文词向量，与我们需要的词向量是平行的两套，类似于word2vec里面的前后词嵌入矩阵。</li>\n<li>接下来，一个自然的想法是，减少参数，即只需要词向量和上下文词向量，因为是距离计算函数且向量空间是线性空间，我们使用$w_i$和$w_j$的向量差作为参数：<script type=\"math/tex; mode=display\">\nF(w_i - w_j,w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>现在函数的参数是向量，输出是张量，最简单的一个结构就是做点乘：<script type=\"math/tex; mode=display\">\nF((w_i-w_j)^T w^{\\*}_k) = \\frac {P_{ik}}{P_{jk}} \\\\</script></li>\n<li>注意到虽然区分了上下文和非上下文词向量，但是$x_i,x_j,x_k$可以是语料中任意词，因此$F$函数的两个参数应该是可以交换位置（$w$和$w^{*}$，$X$和$X^T$），即函数应该是对称的。这里运用了一点数学技巧将函数对称化：<ul>\n<li>设计：<script type=\"math/tex; mode=display\">\nF((w_i-w_j)^T w^{\\*}_k) = \\frac {F(w_i w^{\\*}_k)} {F(w_j w^{\\*}_k)} \\\\</script>其中<script type=\"math/tex; mode=display\">\nF(w_i w^{\\*}_k) = P_{ik} = \\frac {X_{ik}} {X_i} \\\\</script></li>\n<li>要满足上面$F$可以拆分为两个子$F$的比，则$F$可以为$exp$函数，即<script type=\"math/tex; mode=display\">\nw_i^T w_k^{\\*} = log(X_{ik}) - log {X_i} \\\\</script></li>\n<li>这样k,i,j下标可互换位置且表达意思一致。但是注意到上面两个log式子相减并不是对称的，因此我们补上一个$log{x_k}$使之对称，并将其简化为偏置$b$：<script type=\"math/tex; mode=display\">\nw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(X_{ik}) \\\\</script></li>\n<li>最后加上平滑，防止log的参数取0：<script type=\"math/tex; mode=display\">\nw_i^Tw_k^{\\*} + b_i + b_k^{\\*} = log(1 + X_{ik}) \\\\</script></li>\n</ul>\n</li>\n<li>到这里我们已经初步完成了$F$函数的设计，但这个还存在的一个问题是，它是平均加权每一个共现的，而一般语料中大部分共现都频次很低</li>\n<li>Glove的解决办法是使用加权函数。加权之后将词向量的训练看成是F函数的最小均方误差回归，设计损失函数：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j}^V f(X_{ij}) (w_i^T w_j^{\\*} + b_i + b_j^{\\*} - log (1 + X_{ij}))^2 \\\\</script></li>\n<li>其中f为加权函数，其参数是共现频次，作者指出该函数必须满足三条性质：<ul>\n<li>$f(0)=0$：显然，没有出现共现则权重为0。</li>\n<li>Non-decreasing：共现频次越大则权重越大。</li>\n<li>relatively small for large X：防止对于某些频次很高的常见共现加权过大，影响结果。</li>\n</ul>\n</li>\n<li>基于以上三种性质，作者设计了截尾的加权函数，在阈值$X_{max}$以内：<script type=\"math/tex; mode=display\">\nf(x) = (\\frac {x}{X_{max}}) ^ {\\alpha} \\\\</script>超过阈值则函数值为1.</li>\n</ul>\n<h1 id=\"与Word2vec比较\"><a href=\"#与Word2vec比较\" class=\"headerlink\" title=\"与Word2vec比较\"></a>与Word2vec比较</h1><ul>\n<li>对于Word2vec中的skip-gram模型，其目标是最大化给定上下文之后预测正确中心词的概率，一般通过softmax函数将其概率化，即：<script type=\"math/tex; mode=display\">\nQ_{ij} = \\frac {exp (w_i^T w_j^{\\*})} { \\sum _{k=1}^V exp(w_i^T w_k^{\\*})} \\\\</script></li>\n<li>通过梯度下降求解，则整体损失函数可以写成：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i \\in corpus , j \\in context(i)} log Q_{ij} \\\\</script></li>\n<li>将相同的$Q_{ij}$先分组再累加，得到：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i=1}^V \\sum _{j=1}^V X_{ij} log Q_{ij} \\\\</script></li>\n<li>接下来用之前定义的符号进一步变换：<script type=\"math/tex; mode=display\">\nJ = - \\sum _{i=1^V} X_i \\sum _{j=1}^V P_{ij} log Q_{ij} \\\\\n= \\sum _{i=1}^V X_i H(P_i,Q_i) \\\\</script></li>\n<li>也就是说，Word2vec的损失函数实际上是加权的交叉熵，然而交叉熵只是一种可能的度量，且具有很多缺点：<ul>\n<li>需要归一化的概率作为参数</li>\n<li>softmax计算量大，称为模型的计算瓶颈</li>\n<li>对于长尾分布，交叉熵常常分配给不太可能的项太多权重</li>\n</ul>\n</li>\n<li>解决以上问题的方法：干脆不归一化，直接用共现计数，不用交叉熵和softmax，直接用均方误差，令$Q_{ij} = exp(w_i^T w_j^{*})$，$P_{ij} = X_{ij}$，则：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j} X_i (P_{ij} - Q_{ij})^2 \\\\</script></li>\n<li>但是不归一化会造成数值上溢，那就再取个对数：<script type=\"math/tex; mode=display\">\nJ = \\sum _{i,j} X_i (log P_{ij} - log Q_{ij})^2 \\\\\n=  \\sum _{i,j} X_i (w_i^T w_j^{\\*} - log X_{ij})^2 \\\\</script></li>\n<li>这样就得到了Glove最朴素的目标函数。</li>\n<li>Word2vec的作者发现筛除一些常见词能够提高词向量效果，而Word2vec中的加权函数即$f(X_i)=X_i$，因此筛除常见词等价于设计一个非降的加权函数。Glove则设计了更为精巧的加权函数。</li>\n<li>因此从数学公式推导上看，Glove简化了Word2vec的目标函数，用均方误差替换交叉熵，并重新设计了加权函数。</li>\n</ul>\n<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><ul>\n<li>该文提供了一个很好的设计模型的思路，即根据评测指标设计目标函数，反过来训练模型，得到函数的参数（副产品）作为所需的结果。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 17:35:20 GMT+0800 (GMT+08:00)","title":"Glove数学推导","path":"2019/01/13/glove/","eyeCatchImage":null,"excerpt":"<hr>\n<ul>\n<li>记录一下Glove词向量的数学推导，因为原论文不是画模型得出的，而是纯数学操作计算得到的目标函数，这种设计方式非常有意思，而且还将word2vec的数学本质写出来进行了对比。</li>\n<li>原论文：<em>GloVe: Global Vectors for Word Representation</em></li>\n</ul>","date":"2019-01-13T01:42:37.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","glove","word embedding"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Logistic回归与最大熵","date":"2018-10-14T12:38:59.000Z","mathjax":true,"html":true,"_content":"\n翻译John Mount的*The equivalence of logistic regression and maximum entropy models* 一文，并说明了这种证明是在统计学习方法中介绍最大熵模型的通用导出证明的一个特例\n\n结论\n- 最大熵模型就是softmax分类\n- 在满足广义线性模型的平衡条件下，满足最大熵条件的模型映射函数就是softmax函数\n- 在统计机器学习方法一书中，给出了在特征函数定义下的最大熵模型，其与softmax回归都属于对数线性模型\n- 当特征函数从二值函数扩展为特征值本身时，最大熵模型就化为softmax回归模型\n- 最大熵最大化的是条件熵，不是条件概率的熵，也不是联合概率的熵。\n\n\n<!--more-->  \n\n# 明确符号\n- n维特征，m个样本，$x(i)_j$表示第i个样本第j维特征，讨论多分类情况，输出分类$y(i)$有k类，映射概率函数$\\pi$从$R^n$映射到$R^k$，我们希望$\\pi(x(i))_{y(i)}$尽可能大。\n- 指示函数$A(u,v)$，当$u==v$时为1，否则为0\n\n# Logistic回归\n$$\n\\pi(x)_1 = \\frac{e^{\\lambda x}}{1+e^{\\lambda x}} \\\\\n\\pi(x)_2 = 1 - \\pi(x)_1\\\\\n$$\n- 其中要学习到的参数$\\lambda$为$R^n$\n\n# Softmax回归\n$$\n\\pi(x)_v = \\frac{e^{\\lambda _v x}} {\\sum _{u=1}^k e^{\\lambda _u x}}\n$$\n- $\\lambda$为$R^{k * n}$\n\n# 求解softmax\n- 当使用softmax或者logistic作为非线性函数时，它们存在一个很好的求导的性质，即导函数可以用原函数表示\n$$\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{v,j}} = x_j  \\pi (x)_v (1-\\pi (x)_v) \\\\\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{u,j}} = -x_j \\pi (x)_v \\pi (x)_u \\ where \\  u \\neq v \\\\\n$$ \n- 现在我们可以定义目标函数，即希望$\\pi$函数输出的正确类别概率最大（最大似然），并定义最优化得到的$\\lambda$：\n$$\n\\lambda = argmax \\sum _{i=1}^m log (\\pi (x(i))_{y(i)}) \\\\\n= argmax f(\\lambda) \\\\\n$$\n\n# 平衡等式\n- 对上面的目标函数求导并令导函数为0：\n$$\n\\frac {\\partial f(\\lambda)}{\\partial \\lambda _{u,j}} = \\sum _{i=1，y(i)=u}^m x(i)_j - \\sum _{i=1}^m x(i)_j \\pi (x(i))_u =0 \\\\\n$$\n- 这样我们就得到一个重要的平衡等式(Balance Equation)：\n$$\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n$$\n- 分析这个等式：\n\t- 大白话：我们希望得到这么一个映射函数$\\pi$，对某一维(j)特征，用所有样本被映射函数归为第u类的概率加权所有样本的特征值之和，等于第u类内所有样本的特征值之和。显然，最好的情况就是左右两个累加式内的元素完全一样，只有第u类的样本被累加，且第u类样本被映射函数归为第u类的概率为1，其他类样本被归为第u类样本的概率为0.\n\t- 但是，这个等式非常的宽松，它只要求两个和式相同，并不要求每一个元素相同，而且这个式子没有显示的写出映射函数的表达式，任何满足该式的非线性映射都有可能称为映射函数。\n\t- 用公式表达，就是\n\t$$\n\t\\sum _{i=1}^m A(u,y(i)) x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\t\\pi (x(i))_u \\approx A(u,y(i)) \\\\\n\t$$\n\n# 由最大熵推出softmax\n- 上面说到了平衡等式并没有要求映射函数的格式，那么为什么我们选择了softmax？换句话，什么条件下能从平衡等式的约束推出非线性映射为softmax？\n- 答案是最大熵。我们现在回顾一下$\\pi$需要满足的条件：\n\t- 平衡等式（即这个$\\pi$能拟合数据）：\n\t$$\n\t\\ \\  for \\ all \\ u,j \\\\\n\t\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\t$$\n\t- $\\pi$的输出得是一个概率：\n\t$$\n\t\\pi (x)_v \\geq 0 \\\\\n\t\\sum _{v=1}^k \\pi (x)_v = 1 \\\\\n\t$$\n- 根据最大熵原理，我们希望满足上述约束条件的$\\pi$能够具有最大的熵:\n$$\n\\pi = argmax \\ Ent(\\pi) \\\\\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\\n$$\n- 最大熵可以从两个角度理解：\n\t- 最大熵也就是最小困惑度，在无监督模型中我们经常用困惑度衡量概率模型的效果，根据奥卡姆剃刀原则，在多个具有相同效果的模型中复杂程度小的模型具有更好的泛化能力，困惑度是一种衡量复杂程度的指标\n\t- 约束条件是我们的模型已知的需要满足、需要拟合的部分，剩下的部分是未知的部分，没有规则或者数据指导我们分配概率，那该怎么办？在未知的情况下就应该均匀分配概率给所有可能，这正是对应了最大熵的情况\n- 现在问题已经形式化带约束条件的最优化问题，利用拉格朗日乘子法求解即可。这里有一个trick，原文中说如果直接考虑概率的不等条件就有点复杂，需要使用KTT条件，这里先不考虑，之后如果求出的$\\pi$满足不等式条件的话就可以跳过了（事实也正是如此）。\n\n$$\nL = \\sum _{j=1}^n \\sum _{v=1}^k \\lambda _{v,j} (\\sum _{i=1}^m \\pi (x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\\\\n+ \\sum _{v=1}^k \\sum _{i=1}^m \\beta _i (\\pi (x(i))_v -1) \\\\\n- \\sum _{v=1}^k \\sum _{i=1}^m \\pi(x(i))_v log(\\pi (x(i))_v) \\\\\n$$\n- 这里又有一个trick，本来应该对所有参数求导，这里我们先对$\\pi (x(i))_u$求导令其为0可得：\n$$\n\\pi (x(i))_u = e^{\\lambda _u x(i) + \\beta _i -1}\n$$\n- 再考虑等式约束条件（概率之和为1），这样就不用再对$\\beta$求导：\n$$\n\\sum _{v=1}^k e^{\\lambda _v x(i) + \\beta _i -1} = 1 \\\\\ne^{\\beta} = \\frac {1}{\\sum _{v=1}^k e^{\\lambda _v x(i) - 1}} \\\\\n$$\n- 回代可得：\n$$\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}\n$$\n\n# 求解参数\n- 从推出平衡等式的时候可以看到，我们需要解$n \\* k$个方程来得到$n \\* k$个参数$\\lambda$，或者在最大熵的拉格朗日方程里对$n \\* k$个$\\lambda$求偏导，因为$\\pi$是$\\lambda$的非线性函数，这两种求解方法比较困难，但是我们可以求导计算这些等式的雅各比方程（或者说是目标函数的Hessian矩阵），之后我们就可以用某种牛顿法、Fisher Scoring或者迭代的方法求解$\\lambda$\n\n# 与特征函数定义的最大熵模型的联系\n- 在本文中，约束为（省略了$\\pi$必须为概率的约束）：\n$$\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n$$\n- 最大化的熵为：\n$$\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\\n$$\n- 得到的结果为：\n$$\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}\n$$\n- 而在统计学习方法中，约束为（同样省略了概率约束），其中$P^{*}$代表经验分布：\n$$\n\\sum _{x,y} P^{*} (x,y)f(x,y) = \\sum _{x,y} P^{*} (x)P(y|x)f(x,y)\n$$\n- 最大化的熵为：\n$$\nEnt(P) = - \\sum _{x,y} P^{*}(x) P(y|x) log P(y|x)\n$$\n- 得到的结果为：\n$$\nP(y|x) = \\frac{e^{\\sum _i w_i f_i(x,y)}}{\\sum _y e^{\\sum _i w_i f_i(x,y)}}\n$$\n- 可以看到两者的表示有区别，前者直接得到了softmax函数的形式，但是最大化的不是条件熵，后者则相反\n- 实际上两者是统一的。首先，模型的参数都是拉格朗日乘子，前者是$\\lambda$，后者是$w$，两者的关系：\n$$\n\\lambda = \\{w_0,...,w_i,...\\}\n$$\n- 当特征函数扩展到特征值时，两者得到的模型就是一样的（softmax函数）：\n$$\nf_i(x_j,y) = x(j)_i\n$$\n- 两者的平衡条件也是一致的，注意到$P^{*}$是经验分布，是在训练集上通过古典概型统计出来的，一般情况下不考虑重复数据（样本总数为N，类别数为K），则有：\n$$\nP^{*} (x) = \\frac 1N \\\\\n\\sum _{x,y} P^{*} (x,y) = 1 \\\\\nP^{*} (x,y) \\in \\{0,\\frac 1N \\} \\\\\n$$\n- 代入之后会发现两者的平衡条件一致，而论文中计算的貌似是熵，实际上是条件熵，只不过把$P^{*} (x) = \\frac 1N $这一常量条件从argmax表达式中忽略了，写成了熵的形式。\n","source":"_posts/lr-and-me.md","raw":"---\ntitle: Logistic回归与最大熵\ndate: 2018-10-14 20:38:59\ntags: [logistic regression,math,machinelearning]\ncategories: 机器学习\nmathjax: true\nhtml: true\n---\n\n翻译John Mount的*The equivalence of logistic regression and maximum entropy models* 一文，并说明了这种证明是在统计学习方法中介绍最大熵模型的通用导出证明的一个特例\n\n结论\n- 最大熵模型就是softmax分类\n- 在满足广义线性模型的平衡条件下，满足最大熵条件的模型映射函数就是softmax函数\n- 在统计机器学习方法一书中，给出了在特征函数定义下的最大熵模型，其与softmax回归都属于对数线性模型\n- 当特征函数从二值函数扩展为特征值本身时，最大熵模型就化为softmax回归模型\n- 最大熵最大化的是条件熵，不是条件概率的熵，也不是联合概率的熵。\n\n\n<!--more-->  \n\n# 明确符号\n- n维特征，m个样本，$x(i)_j$表示第i个样本第j维特征，讨论多分类情况，输出分类$y(i)$有k类，映射概率函数$\\pi$从$R^n$映射到$R^k$，我们希望$\\pi(x(i))_{y(i)}$尽可能大。\n- 指示函数$A(u,v)$，当$u==v$时为1，否则为0\n\n# Logistic回归\n$$\n\\pi(x)_1 = \\frac{e^{\\lambda x}}{1+e^{\\lambda x}} \\\\\n\\pi(x)_2 = 1 - \\pi(x)_1\\\\\n$$\n- 其中要学习到的参数$\\lambda$为$R^n$\n\n# Softmax回归\n$$\n\\pi(x)_v = \\frac{e^{\\lambda _v x}} {\\sum _{u=1}^k e^{\\lambda _u x}}\n$$\n- $\\lambda$为$R^{k * n}$\n\n# 求解softmax\n- 当使用softmax或者logistic作为非线性函数时，它们存在一个很好的求导的性质，即导函数可以用原函数表示\n$$\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{v,j}} = x_j  \\pi (x)_v (1-\\pi (x)_v) \\\\\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{u,j}} = -x_j \\pi (x)_v \\pi (x)_u \\ where \\  u \\neq v \\\\\n$$ \n- 现在我们可以定义目标函数，即希望$\\pi$函数输出的正确类别概率最大（最大似然），并定义最优化得到的$\\lambda$：\n$$\n\\lambda = argmax \\sum _{i=1}^m log (\\pi (x(i))_{y(i)}) \\\\\n= argmax f(\\lambda) \\\\\n$$\n\n# 平衡等式\n- 对上面的目标函数求导并令导函数为0：\n$$\n\\frac {\\partial f(\\lambda)}{\\partial \\lambda _{u,j}} = \\sum _{i=1，y(i)=u}^m x(i)_j - \\sum _{i=1}^m x(i)_j \\pi (x(i))_u =0 \\\\\n$$\n- 这样我们就得到一个重要的平衡等式(Balance Equation)：\n$$\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n$$\n- 分析这个等式：\n\t- 大白话：我们希望得到这么一个映射函数$\\pi$，对某一维(j)特征，用所有样本被映射函数归为第u类的概率加权所有样本的特征值之和，等于第u类内所有样本的特征值之和。显然，最好的情况就是左右两个累加式内的元素完全一样，只有第u类的样本被累加，且第u类样本被映射函数归为第u类的概率为1，其他类样本被归为第u类样本的概率为0.\n\t- 但是，这个等式非常的宽松，它只要求两个和式相同，并不要求每一个元素相同，而且这个式子没有显示的写出映射函数的表达式，任何满足该式的非线性映射都有可能称为映射函数。\n\t- 用公式表达，就是\n\t$$\n\t\\sum _{i=1}^m A(u,y(i)) x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\t\\pi (x(i))_u \\approx A(u,y(i)) \\\\\n\t$$\n\n# 由最大熵推出softmax\n- 上面说到了平衡等式并没有要求映射函数的格式，那么为什么我们选择了softmax？换句话，什么条件下能从平衡等式的约束推出非线性映射为softmax？\n- 答案是最大熵。我们现在回顾一下$\\pi$需要满足的条件：\n\t- 平衡等式（即这个$\\pi$能拟合数据）：\n\t$$\n\t\\ \\  for \\ all \\ u,j \\\\\n\t\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\t$$\n\t- $\\pi$的输出得是一个概率：\n\t$$\n\t\\pi (x)_v \\geq 0 \\\\\n\t\\sum _{v=1}^k \\pi (x)_v = 1 \\\\\n\t$$\n- 根据最大熵原理，我们希望满足上述约束条件的$\\pi$能够具有最大的熵:\n$$\n\\pi = argmax \\ Ent(\\pi) \\\\\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\\n$$\n- 最大熵可以从两个角度理解：\n\t- 最大熵也就是最小困惑度，在无监督模型中我们经常用困惑度衡量概率模型的效果，根据奥卡姆剃刀原则，在多个具有相同效果的模型中复杂程度小的模型具有更好的泛化能力，困惑度是一种衡量复杂程度的指标\n\t- 约束条件是我们的模型已知的需要满足、需要拟合的部分，剩下的部分是未知的部分，没有规则或者数据指导我们分配概率，那该怎么办？在未知的情况下就应该均匀分配概率给所有可能，这正是对应了最大熵的情况\n- 现在问题已经形式化带约束条件的最优化问题，利用拉格朗日乘子法求解即可。这里有一个trick，原文中说如果直接考虑概率的不等条件就有点复杂，需要使用KTT条件，这里先不考虑，之后如果求出的$\\pi$满足不等式条件的话就可以跳过了（事实也正是如此）。\n\n$$\nL = \\sum _{j=1}^n \\sum _{v=1}^k \\lambda _{v,j} (\\sum _{i=1}^m \\pi (x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\\\\n+ \\sum _{v=1}^k \\sum _{i=1}^m \\beta _i (\\pi (x(i))_v -1) \\\\\n- \\sum _{v=1}^k \\sum _{i=1}^m \\pi(x(i))_v log(\\pi (x(i))_v) \\\\\n$$\n- 这里又有一个trick，本来应该对所有参数求导，这里我们先对$\\pi (x(i))_u$求导令其为0可得：\n$$\n\\pi (x(i))_u = e^{\\lambda _u x(i) + \\beta _i -1}\n$$\n- 再考虑等式约束条件（概率之和为1），这样就不用再对$\\beta$求导：\n$$\n\\sum _{v=1}^k e^{\\lambda _v x(i) + \\beta _i -1} = 1 \\\\\ne^{\\beta} = \\frac {1}{\\sum _{v=1}^k e^{\\lambda _v x(i) - 1}} \\\\\n$$\n- 回代可得：\n$$\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}\n$$\n\n# 求解参数\n- 从推出平衡等式的时候可以看到，我们需要解$n \\* k$个方程来得到$n \\* k$个参数$\\lambda$，或者在最大熵的拉格朗日方程里对$n \\* k$个$\\lambda$求偏导，因为$\\pi$是$\\lambda$的非线性函数，这两种求解方法比较困难，但是我们可以求导计算这些等式的雅各比方程（或者说是目标函数的Hessian矩阵），之后我们就可以用某种牛顿法、Fisher Scoring或者迭代的方法求解$\\lambda$\n\n# 与特征函数定义的最大熵模型的联系\n- 在本文中，约束为（省略了$\\pi$必须为概率的约束）：\n$$\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n$$\n- 最大化的熵为：\n$$\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\\n$$\n- 得到的结果为：\n$$\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}\n$$\n- 而在统计学习方法中，约束为（同样省略了概率约束），其中$P^{*}$代表经验分布：\n$$\n\\sum _{x,y} P^{*} (x,y)f(x,y) = \\sum _{x,y} P^{*} (x)P(y|x)f(x,y)\n$$\n- 最大化的熵为：\n$$\nEnt(P) = - \\sum _{x,y} P^{*}(x) P(y|x) log P(y|x)\n$$\n- 得到的结果为：\n$$\nP(y|x) = \\frac{e^{\\sum _i w_i f_i(x,y)}}{\\sum _y e^{\\sum _i w_i f_i(x,y)}}\n$$\n- 可以看到两者的表示有区别，前者直接得到了softmax函数的形式，但是最大化的不是条件熵，后者则相反\n- 实际上两者是统一的。首先，模型的参数都是拉格朗日乘子，前者是$\\lambda$，后者是$w$，两者的关系：\n$$\n\\lambda = \\{w_0,...,w_i,...\\}\n$$\n- 当特征函数扩展到特征值时，两者得到的模型就是一样的（softmax函数）：\n$$\nf_i(x_j,y) = x(j)_i\n$$\n- 两者的平衡条件也是一致的，注意到$P^{*}$是经验分布，是在训练集上通过古典概型统计出来的，一般情况下不考虑重复数据（样本总数为N，类别数为K），则有：\n$$\nP^{*} (x) = \\frac 1N \\\\\n\\sum _{x,y} P^{*} (x,y) = 1 \\\\\nP^{*} (x,y) \\in \\{0,\\frac 1N \\} \\\\\n$$\n- 代入之后会发现两者的平衡条件一致，而论文中计算的貌似是熵，实际上是条件熵，只不过把$P^{*} (x) = \\frac 1N $这一常量条件从argmax表达式中忽略了，写成了熵的形式。\n","slug":"lr-and-me","published":1,"updated":"2019-07-30T02:35:58.455Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v4g0021q8t5ctgyarzn","content":"<p>翻译John Mount的<em>The equivalence of logistic regression and maximum entropy models</em> 一文，并说明了这种证明是在统计学习方法中介绍最大熵模型的通用导出证明的一个特例</p>\n<p>结论</p>\n<ul>\n<li>最大熵模型就是softmax分类</li>\n<li>在满足广义线性模型的平衡条件下，满足最大熵条件的模型映射函数就是softmax函数</li>\n<li>在统计机器学习方法一书中，给出了在特征函数定义下的最大熵模型，其与softmax回归都属于对数线性模型</li>\n<li>当特征函数从二值函数扩展为特征值本身时，最大熵模型就化为softmax回归模型</li>\n<li>最大熵最大化的是条件熵，不是条件概率的熵，也不是联合概率的熵。</li>\n</ul>\n<a id=\"more\"></a>  \n<h1 id=\"明确符号\"><a href=\"#明确符号\" class=\"headerlink\" title=\"明确符号\"></a>明确符号</h1><ul>\n<li>n维特征，m个样本，$x(i)_j$表示第i个样本第j维特征，讨论多分类情况，输出分类$y(i)$有k类，映射概率函数$\\pi$从$R^n$映射到$R^k$，我们希望$\\pi(x(i))_{y(i)}$尽可能大。</li>\n<li>指示函数$A(u,v)$，当$u==v$时为1，否则为0</li>\n</ul>\n<h1 id=\"Logistic回归\"><a href=\"#Logistic回归\" class=\"headerlink\" title=\"Logistic回归\"></a>Logistic回归</h1><script type=\"math/tex; mode=display\">\n\\pi(x)_1 = \\frac{e^{\\lambda x}}{1+e^{\\lambda x}} \\\\\n\\pi(x)_2 = 1 - \\pi(x)_1\\\\</script><ul>\n<li>其中要学习到的参数$\\lambda$为$R^n$</li>\n</ul>\n<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><script type=\"math/tex; mode=display\">\n\\pi(x)_v = \\frac{e^{\\lambda _v x}} {\\sum _{u=1}^k e^{\\lambda _u x}}</script><ul>\n<li>$\\lambda$为$R^{k * n}$</li>\n</ul>\n<h1 id=\"求解softmax\"><a href=\"#求解softmax\" class=\"headerlink\" title=\"求解softmax\"></a>求解softmax</h1><ul>\n<li>当使用softmax或者logistic作为非线性函数时，它们存在一个很好的求导的性质，即导函数可以用原函数表示<script type=\"math/tex; mode=display\">\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{v,j}} = x_j  \\pi (x)_v (1-\\pi (x)_v) \\\\\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{u,j}} = -x_j \\pi (x)_v \\pi (x)_u \\ where \\  u \\neq v \\\\</script></li>\n<li>现在我们可以定义目标函数，即希望$\\pi$函数输出的正确类别概率最大（最大似然），并定义最优化得到的$\\lambda$：<script type=\"math/tex; mode=display\">\n\\lambda = argmax \\sum _{i=1}^m log (\\pi (x(i))_{y(i)}) \\\\\n= argmax f(\\lambda) \\\\</script></li>\n</ul>\n<h1 id=\"平衡等式\"><a href=\"#平衡等式\" class=\"headerlink\" title=\"平衡等式\"></a>平衡等式</h1><ul>\n<li>对上面的目标函数求导并令导函数为0：<script type=\"math/tex; mode=display\">\n\\frac {\\partial f(\\lambda)}{\\partial \\lambda _{u,j}} = \\sum _{i=1，y(i)=u}^m x(i)_j - \\sum _{i=1}^m x(i)_j \\pi (x(i))_u =0 \\\\</script></li>\n<li>这样我们就得到一个重要的平衡等式(Balance Equation)：<script type=\"math/tex; mode=display\">\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>分析这个等式：<ul>\n<li>大白话：我们希望得到这么一个映射函数$\\pi$，对某一维(j)特征，用所有样本被映射函数归为第u类的概率加权所有样本的特征值之和，等于第u类内所有样本的特征值之和。显然，最好的情况就是左右两个累加式内的元素完全一样，只有第u类的样本被累加，且第u类样本被映射函数归为第u类的概率为1，其他类样本被归为第u类样本的概率为0.</li>\n<li>但是，这个等式非常的宽松，它只要求两个和式相同，并不要求每一个元素相同，而且这个式子没有显示的写出映射函数的表达式，任何满足该式的非线性映射都有可能称为映射函数。</li>\n<li>用公式表达，就是<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^m A(u,y(i)) x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\\pi (x(i))_u \\approx A(u,y(i)) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"由最大熵推出softmax\"><a href=\"#由最大熵推出softmax\" class=\"headerlink\" title=\"由最大熵推出softmax\"></a>由最大熵推出softmax</h1><ul>\n<li>上面说到了平衡等式并没有要求映射函数的格式，那么为什么我们选择了softmax？换句话，什么条件下能从平衡等式的约束推出非线性映射为softmax？</li>\n<li>答案是最大熵。我们现在回顾一下$\\pi$需要满足的条件：<ul>\n<li>平衡等式（即这个$\\pi$能拟合数据）：<script type=\"math/tex; mode=display\">\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>$\\pi$的输出得是一个概率：<script type=\"math/tex; mode=display\">\n\\pi (x)_v \\geq 0 \\\\\n\\sum _{v=1}^k \\pi (x)_v = 1 \\\\</script></li>\n</ul>\n</li>\n<li>根据最大熵原理，我们希望满足上述约束条件的$\\pi$能够具有最大的熵:<script type=\"math/tex; mode=display\">\n\\pi = argmax \\ Ent(\\pi) \\\\\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\</script></li>\n<li>最大熵可以从两个角度理解：<ul>\n<li>最大熵也就是最小困惑度，在无监督模型中我们经常用困惑度衡量概率模型的效果，根据奥卡姆剃刀原则，在多个具有相同效果的模型中复杂程度小的模型具有更好的泛化能力，困惑度是一种衡量复杂程度的指标</li>\n<li>约束条件是我们的模型已知的需要满足、需要拟合的部分，剩下的部分是未知的部分，没有规则或者数据指导我们分配概率，那该怎么办？在未知的情况下就应该均匀分配概率给所有可能，这正是对应了最大熵的情况</li>\n</ul>\n</li>\n<li>现在问题已经形式化带约束条件的最优化问题，利用拉格朗日乘子法求解即可。这里有一个trick，原文中说如果直接考虑概率的不等条件就有点复杂，需要使用KTT条件，这里先不考虑，之后如果求出的$\\pi$满足不等式条件的话就可以跳过了（事实也正是如此）。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL = \\sum _{j=1}^n \\sum _{v=1}^k \\lambda _{v,j} (\\sum _{i=1}^m \\pi (x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\\\\n+ \\sum _{v=1}^k \\sum _{i=1}^m \\beta _i (\\pi (x(i))_v -1) \\\\\n- \\sum _{v=1}^k \\sum _{i=1}^m \\pi(x(i))_v log(\\pi (x(i))_v) \\\\</script><ul>\n<li>这里又有一个trick，本来应该对所有参数求导，这里我们先对$\\pi (x(i))_u$求导令其为0可得：<script type=\"math/tex; mode=display\">\n\\pi (x(i))_u = e^{\\lambda _u x(i) + \\beta _i -1}</script></li>\n<li>再考虑等式约束条件（概率之和为1），这样就不用再对$\\beta$求导：<script type=\"math/tex; mode=display\">\n\\sum _{v=1}^k e^{\\lambda _v x(i) + \\beta _i -1} = 1 \\\\\ne^{\\beta} = \\frac {1}{\\sum _{v=1}^k e^{\\lambda _v x(i) - 1}} \\\\</script></li>\n<li>回代可得：<script type=\"math/tex; mode=display\">\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}</script></li>\n</ul>\n<h1 id=\"求解参数\"><a href=\"#求解参数\" class=\"headerlink\" title=\"求解参数\"></a>求解参数</h1><ul>\n<li>从推出平衡等式的时候可以看到，我们需要解$n * k$个方程来得到$n * k$个参数$\\lambda$，或者在最大熵的拉格朗日方程里对$n * k$个$\\lambda$求偏导，因为$\\pi$是$\\lambda$的非线性函数，这两种求解方法比较困难，但是我们可以求导计算这些等式的雅各比方程（或者说是目标函数的Hessian矩阵），之后我们就可以用某种牛顿法、Fisher Scoring或者迭代的方法求解$\\lambda$</li>\n</ul>\n<h1 id=\"与特征函数定义的最大熵模型的联系\"><a href=\"#与特征函数定义的最大熵模型的联系\" class=\"headerlink\" title=\"与特征函数定义的最大熵模型的联系\"></a>与特征函数定义的最大熵模型的联系</h1><ul>\n<li>在本文中，约束为（省略了$\\pi$必须为概率的约束）：<script type=\"math/tex; mode=display\">\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>最大化的熵为：<script type=\"math/tex; mode=display\">\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\</script></li>\n<li>得到的结果为：<script type=\"math/tex; mode=display\">\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}</script></li>\n<li>而在统计学习方法中，约束为（同样省略了概率约束），其中$P^{*}$代表经验分布：<script type=\"math/tex; mode=display\">\n\\sum _{x,y} P^{*} (x,y)f(x,y) = \\sum _{x,y} P^{*} (x)P(y|x)f(x,y)</script></li>\n<li>最大化的熵为：<script type=\"math/tex; mode=display\">\nEnt(P) = - \\sum _{x,y} P^{*}(x) P(y|x) log P(y|x)</script></li>\n<li>得到的结果为：<script type=\"math/tex; mode=display\">\nP(y|x) = \\frac{e^{\\sum _i w_i f_i(x,y)}}{\\sum _y e^{\\sum _i w_i f_i(x,y)}}</script></li>\n<li>可以看到两者的表示有区别，前者直接得到了softmax函数的形式，但是最大化的不是条件熵，后者则相反</li>\n<li>实际上两者是统一的。首先，模型的参数都是拉格朗日乘子，前者是$\\lambda$，后者是$w$，两者的关系：<script type=\"math/tex; mode=display\">\n\\lambda = \\{w_0,...,w_i,...\\}</script></li>\n<li>当特征函数扩展到特征值时，两者得到的模型就是一样的（softmax函数）：<script type=\"math/tex; mode=display\">\nf_i(x_j,y) = x(j)_i</script></li>\n<li>两者的平衡条件也是一致的，注意到$P^{*}$是经验分布，是在训练集上通过古典概型统计出来的，一般情况下不考虑重复数据（样本总数为N，类别数为K），则有：<script type=\"math/tex; mode=display\">\nP^{*} (x) = \\frac 1N \\\\\n\\sum _{x,y} P^{*} (x,y) = 1 \\\\\nP^{*} (x,y) \\in \\{0,\\frac 1N \\} \\\\</script></li>\n<li>代入之后会发现两者的平衡条件一致，而论文中计算的貌似是熵，实际上是条件熵，只不过把$P^{*} (x) = \\frac 1N $这一常量条件从argmax表达式中忽略了，写成了熵的形式。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>翻译John Mount的<em>The equivalence of logistic regression and maximum entropy models</em> 一文，并说明了这种证明是在统计学习方法中介绍最大熵模型的通用导出证明的一个特例</p>\n<p>结论</p>\n<ul>\n<li>最大熵模型就是softmax分类</li>\n<li>在满足广义线性模型的平衡条件下，满足最大熵条件的模型映射函数就是softmax函数</li>\n<li>在统计机器学习方法一书中，给出了在特征函数定义下的最大熵模型，其与softmax回归都属于对数线性模型</li>\n<li>当特征函数从二值函数扩展为特征值本身时，最大熵模型就化为softmax回归模型</li>\n<li>最大熵最大化的是条件熵，不是条件概率的熵，也不是联合概率的熵。</li>\n</ul>","more":"<h1 id=\"明确符号\"><a href=\"#明确符号\" class=\"headerlink\" title=\"明确符号\"></a>明确符号</h1><ul>\n<li>n维特征，m个样本，$x(i)_j$表示第i个样本第j维特征，讨论多分类情况，输出分类$y(i)$有k类，映射概率函数$\\pi$从$R^n$映射到$R^k$，我们希望$\\pi(x(i))_{y(i)}$尽可能大。</li>\n<li>指示函数$A(u,v)$，当$u==v$时为1，否则为0</li>\n</ul>\n<h1 id=\"Logistic回归\"><a href=\"#Logistic回归\" class=\"headerlink\" title=\"Logistic回归\"></a>Logistic回归</h1><script type=\"math/tex; mode=display\">\n\\pi(x)_1 = \\frac{e^{\\lambda x}}{1+e^{\\lambda x}} \\\\\n\\pi(x)_2 = 1 - \\pi(x)_1\\\\</script><ul>\n<li>其中要学习到的参数$\\lambda$为$R^n$</li>\n</ul>\n<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><script type=\"math/tex; mode=display\">\n\\pi(x)_v = \\frac{e^{\\lambda _v x}} {\\sum _{u=1}^k e^{\\lambda _u x}}</script><ul>\n<li>$\\lambda$为$R^{k * n}$</li>\n</ul>\n<h1 id=\"求解softmax\"><a href=\"#求解softmax\" class=\"headerlink\" title=\"求解softmax\"></a>求解softmax</h1><ul>\n<li>当使用softmax或者logistic作为非线性函数时，它们存在一个很好的求导的性质，即导函数可以用原函数表示<script type=\"math/tex; mode=display\">\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{v,j}} = x_j  \\pi (x)_v (1-\\pi (x)_v) \\\\\n\\frac {\\partial \\pi (x)_v}{\\partial \\lambda _{u,j}} = -x_j \\pi (x)_v \\pi (x)_u \\ where \\  u \\neq v \\\\</script></li>\n<li>现在我们可以定义目标函数，即希望$\\pi$函数输出的正确类别概率最大（最大似然），并定义最优化得到的$\\lambda$：<script type=\"math/tex; mode=display\">\n\\lambda = argmax \\sum _{i=1}^m log (\\pi (x(i))_{y(i)}) \\\\\n= argmax f(\\lambda) \\\\</script></li>\n</ul>\n<h1 id=\"平衡等式\"><a href=\"#平衡等式\" class=\"headerlink\" title=\"平衡等式\"></a>平衡等式</h1><ul>\n<li>对上面的目标函数求导并令导函数为0：<script type=\"math/tex; mode=display\">\n\\frac {\\partial f(\\lambda)}{\\partial \\lambda _{u,j}} = \\sum _{i=1，y(i)=u}^m x(i)_j - \\sum _{i=1}^m x(i)_j \\pi (x(i))_u =0 \\\\</script></li>\n<li>这样我们就得到一个重要的平衡等式(Balance Equation)：<script type=\"math/tex; mode=display\">\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>分析这个等式：<ul>\n<li>大白话：我们希望得到这么一个映射函数$\\pi$，对某一维(j)特征，用所有样本被映射函数归为第u类的概率加权所有样本的特征值之和，等于第u类内所有样本的特征值之和。显然，最好的情况就是左右两个累加式内的元素完全一样，只有第u类的样本被累加，且第u类样本被映射函数归为第u类的概率为1，其他类样本被归为第u类样本的概率为0.</li>\n<li>但是，这个等式非常的宽松，它只要求两个和式相同，并不要求每一个元素相同，而且这个式子没有显示的写出映射函数的表达式，任何满足该式的非线性映射都有可能称为映射函数。</li>\n<li>用公式表达，就是<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^m A(u,y(i)) x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\\n\\pi (x(i))_u \\approx A(u,y(i)) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"由最大熵推出softmax\"><a href=\"#由最大熵推出softmax\" class=\"headerlink\" title=\"由最大熵推出softmax\"></a>由最大熵推出softmax</h1><ul>\n<li>上面说到了平衡等式并没有要求映射函数的格式，那么为什么我们选择了softmax？换句话，什么条件下能从平衡等式的约束推出非线性映射为softmax？</li>\n<li>答案是最大熵。我们现在回顾一下$\\pi$需要满足的条件：<ul>\n<li>平衡等式（即这个$\\pi$能拟合数据）：<script type=\"math/tex; mode=display\">\n\\ \\  for \\ all \\ u,j \\\\\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>$\\pi$的输出得是一个概率：<script type=\"math/tex; mode=display\">\n\\pi (x)_v \\geq 0 \\\\\n\\sum _{v=1}^k \\pi (x)_v = 1 \\\\</script></li>\n</ul>\n</li>\n<li>根据最大熵原理，我们希望满足上述约束条件的$\\pi$能够具有最大的熵:<script type=\"math/tex; mode=display\">\n\\pi = argmax \\ Ent(\\pi) \\\\\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\</script></li>\n<li>最大熵可以从两个角度理解：<ul>\n<li>最大熵也就是最小困惑度，在无监督模型中我们经常用困惑度衡量概率模型的效果，根据奥卡姆剃刀原则，在多个具有相同效果的模型中复杂程度小的模型具有更好的泛化能力，困惑度是一种衡量复杂程度的指标</li>\n<li>约束条件是我们的模型已知的需要满足、需要拟合的部分，剩下的部分是未知的部分，没有规则或者数据指导我们分配概率，那该怎么办？在未知的情况下就应该均匀分配概率给所有可能，这正是对应了最大熵的情况</li>\n</ul>\n</li>\n<li>现在问题已经形式化带约束条件的最优化问题，利用拉格朗日乘子法求解即可。这里有一个trick，原文中说如果直接考虑概率的不等条件就有点复杂，需要使用KTT条件，这里先不考虑，之后如果求出的$\\pi$满足不等式条件的话就可以跳过了（事实也正是如此）。</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nL = \\sum _{j=1}^n \\sum _{v=1}^k \\lambda _{v,j} (\\sum _{i=1}^m \\pi (x(i))_v x(i)_j - A(v,y(i)) x(i)_j) \\\\\n+ \\sum _{v=1}^k \\sum _{i=1}^m \\beta _i (\\pi (x(i))_v -1) \\\\\n- \\sum _{v=1}^k \\sum _{i=1}^m \\pi(x(i))_v log(\\pi (x(i))_v) \\\\</script><ul>\n<li>这里又有一个trick，本来应该对所有参数求导，这里我们先对$\\pi (x(i))_u$求导令其为0可得：<script type=\"math/tex; mode=display\">\n\\pi (x(i))_u = e^{\\lambda _u x(i) + \\beta _i -1}</script></li>\n<li>再考虑等式约束条件（概率之和为1），这样就不用再对$\\beta$求导：<script type=\"math/tex; mode=display\">\n\\sum _{v=1}^k e^{\\lambda _v x(i) + \\beta _i -1} = 1 \\\\\ne^{\\beta} = \\frac {1}{\\sum _{v=1}^k e^{\\lambda _v x(i) - 1}} \\\\</script></li>\n<li>回代可得：<script type=\"math/tex; mode=display\">\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}</script></li>\n</ul>\n<h1 id=\"求解参数\"><a href=\"#求解参数\" class=\"headerlink\" title=\"求解参数\"></a>求解参数</h1><ul>\n<li>从推出平衡等式的时候可以看到，我们需要解$n * k$个方程来得到$n * k$个参数$\\lambda$，或者在最大熵的拉格朗日方程里对$n * k$个$\\lambda$求偏导，因为$\\pi$是$\\lambda$的非线性函数，这两种求解方法比较困难，但是我们可以求导计算这些等式的雅各比方程（或者说是目标函数的Hessian矩阵），之后我们就可以用某种牛顿法、Fisher Scoring或者迭代的方法求解$\\lambda$</li>\n</ul>\n<h1 id=\"与特征函数定义的最大熵模型的联系\"><a href=\"#与特征函数定义的最大熵模型的联系\" class=\"headerlink\" title=\"与特征函数定义的最大熵模型的联系\"></a>与特征函数定义的最大熵模型的联系</h1><ul>\n<li>在本文中，约束为（省略了$\\pi$必须为概率的约束）：<script type=\"math/tex; mode=display\">\n\\sum _{i=1，y(i)=u}^m x(i)_j = \\sum _{i=1}^m x(i)_j \\pi (x(i))_u \\\\</script></li>\n<li>最大化的熵为：<script type=\"math/tex; mode=display\">\nEnt(\\pi) = - \\sum_{v=1}^k \\sum _{i=1}^m \\pi (x(i))_v log (\\pi (x(i))_v) \\\\</script></li>\n<li>得到的结果为：<script type=\"math/tex; mode=display\">\n\\pi (x)_u = \\frac {e^{\\lambda _u}x}{\\sum _{v=1}^k e^{\\lambda _v}x}</script></li>\n<li>而在统计学习方法中，约束为（同样省略了概率约束），其中$P^{*}$代表经验分布：<script type=\"math/tex; mode=display\">\n\\sum _{x,y} P^{*} (x,y)f(x,y) = \\sum _{x,y} P^{*} (x)P(y|x)f(x,y)</script></li>\n<li>最大化的熵为：<script type=\"math/tex; mode=display\">\nEnt(P) = - \\sum _{x,y} P^{*}(x) P(y|x) log P(y|x)</script></li>\n<li>得到的结果为：<script type=\"math/tex; mode=display\">\nP(y|x) = \\frac{e^{\\sum _i w_i f_i(x,y)}}{\\sum _y e^{\\sum _i w_i f_i(x,y)}}</script></li>\n<li>可以看到两者的表示有区别，前者直接得到了softmax函数的形式，但是最大化的不是条件熵，后者则相反</li>\n<li>实际上两者是统一的。首先，模型的参数都是拉格朗日乘子，前者是$\\lambda$，后者是$w$，两者的关系：<script type=\"math/tex; mode=display\">\n\\lambda = \\{w_0,...,w_i,...\\}</script></li>\n<li>当特征函数扩展到特征值时，两者得到的模型就是一样的（softmax函数）：<script type=\"math/tex; mode=display\">\nf_i(x_j,y) = x(j)_i</script></li>\n<li>两者的平衡条件也是一致的，注意到$P^{*}$是经验分布，是在训练集上通过古典概型统计出来的，一般情况下不考虑重复数据（样本总数为N，类别数为K），则有：<script type=\"math/tex; mode=display\">\nP^{*} (x) = \\frac 1N \\\\\n\\sum _{x,y} P^{*} (x,y) = 1 \\\\\nP^{*} (x,y) \\in \\{0,\\frac 1N \\} \\\\</script></li>\n<li>代入之后会发现两者的平衡条件一致，而论文中计算的貌似是熵，实际上是条件熵，只不过把$P^{*} (x) = \\frac 1N $这一常量条件从argmax表达式中忽略了，写成了熵的形式。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Tue Jul 30 2019 10:35:58 GMT+0800 (GMT+08:00)","title":"Logistic回归与最大熵","path":"2018/10/14/lr-and-me/","eyeCatchImage":null,"excerpt":"<p>翻译John Mount的<em>The equivalence of logistic regression and maximum entropy models</em> 一文，并说明了这种证明是在统计学习方法中介绍最大熵模型的通用导出证明的一个特例</p>\n<p>结论</p>\n<ul>\n<li>最大熵模型就是softmax分类</li>\n<li>在满足广义线性模型的平衡条件下，满足最大熵条件的模型映射函数就是softmax函数</li>\n<li>在统计机器学习方法一书中，给出了在特征函数定义下的最大熵模型，其与softmax回归都属于对数线性模型</li>\n<li>当特征函数从二值函数扩展为特征值本身时，最大熵模型就化为softmax回归模型</li>\n<li>最大熵最大化的是条件熵，不是条件概率的熵，也不是联合概率的熵。</li>\n</ul>","date":"2018-10-14T12:38:59.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","machinelearning","logistic regression"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Numpy Cookbook","date":"2017-01-23T12:12:40.000Z","_content":"Cookbook网址：[Numpy Cookbook](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\nNumpy的一些语法查询和总结\n持续更新\n\n<!--more-->\n\n- shape()\nshape是numpy函数库中的方法，用于查看矩阵或者数组的维素\nshape(array) 若矩阵有m行n列，则返回(m,n)\narray.shape[0] 返回矩阵的行数m，参数为1的话返回列数n\n\n- tile()\ntile是numpy函数库中的方法，用法如下:\ntile(A,(m,n))  将数组A作为元素构造出m行n列的数组\n\n- sum()\nsum()是numpy函数库中的方法\narray.sum(axis=1)按行累加，axis=0为按列累加\n\n- argsort()\nargsort()是numpy中的方法，得到矩阵中每个元素的排序序号 \nA=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标\n\n- dict.get(key,x)\nPython中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0\n\n- sorted()\n\n- numpy中有min()、max()方法，用法如下\narray.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值\narray.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值\n\n-\tlistdir('str')\nstrlist=listdir('str')  读取目录str下的所有文件名，返回一个字符串列表\n\n-\tsplit()\npython中的方法，切片函数\nstring.split('str')以字符str为分隔符切片，返回list\n\n-\tzeros\na=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵","source":"_posts/numpycookbook.md","raw":"---\ntitle: Numpy Cookbook\ndate: 2017-01-23 20:12:40\ntags: [math,machinelearning,python,code]\ncategories: Python\n---\nCookbook网址：[Numpy Cookbook](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\nNumpy的一些语法查询和总结\n持续更新\n\n<!--more-->\n\n- shape()\nshape是numpy函数库中的方法，用于查看矩阵或者数组的维素\nshape(array) 若矩阵有m行n列，则返回(m,n)\narray.shape[0] 返回矩阵的行数m，参数为1的话返回列数n\n\n- tile()\ntile是numpy函数库中的方法，用法如下:\ntile(A,(m,n))  将数组A作为元素构造出m行n列的数组\n\n- sum()\nsum()是numpy函数库中的方法\narray.sum(axis=1)按行累加，axis=0为按列累加\n\n- argsort()\nargsort()是numpy中的方法，得到矩阵中每个元素的排序序号 \nA=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标\n\n- dict.get(key,x)\nPython中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0\n\n- sorted()\n\n- numpy中有min()、max()方法，用法如下\narray.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值\narray.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值\n\n-\tlistdir('str')\nstrlist=listdir('str')  读取目录str下的所有文件名，返回一个字符串列表\n\n-\tsplit()\npython中的方法，切片函数\nstring.split('str')以字符str为分隔符切片，返回list\n\n-\tzeros\na=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵","slug":"numpycookbook","published":1,"updated":"2019-07-22T03:45:23.267Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v4m0024q8t5lfl52et2","content":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p>\n<a id=\"more\"></a>\n<ul>\n<li><p>shape()<br>shape是numpy函数库中的方法，用于查看矩阵或者数组的维素<br>shape(array) 若矩阵有m行n列，则返回(m,n)<br>array.shape[0] 返回矩阵的行数m，参数为1的话返回列数n</p>\n</li>\n<li><p>tile()<br>tile是numpy函数库中的方法，用法如下:<br>tile(A,(m,n))  将数组A作为元素构造出m行n列的数组</p>\n</li>\n<li><p>sum()<br>sum()是numpy函数库中的方法<br>array.sum(axis=1)按行累加，axis=0为按列累加</p>\n</li>\n<li><p>argsort()<br>argsort()是numpy中的方法，得到矩阵中每个元素的排序序号<br>A=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标</p>\n</li>\n<li><p>dict.get(key,x)<br>Python中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0</p>\n</li>\n<li><p>sorted()</p>\n</li>\n<li><p>numpy中有min()、max()方法，用法如下<br>array.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值<br>array.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值</p>\n</li>\n<li><p>listdir(‘str’)<br>strlist=listdir(‘str’)  读取目录str下的所有文件名，返回一个字符串列表</p>\n</li>\n<li><p>split()<br>python中的方法，切片函数<br>string.split(‘str’)以字符str为分隔符切片，返回list</p>\n</li>\n<li><p>zeros<br>a=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p>","more":"<ul>\n<li><p>shape()<br>shape是numpy函数库中的方法，用于查看矩阵或者数组的维素<br>shape(array) 若矩阵有m行n列，则返回(m,n)<br>array.shape[0] 返回矩阵的行数m，参数为1的话返回列数n</p>\n</li>\n<li><p>tile()<br>tile是numpy函数库中的方法，用法如下:<br>tile(A,(m,n))  将数组A作为元素构造出m行n列的数组</p>\n</li>\n<li><p>sum()<br>sum()是numpy函数库中的方法<br>array.sum(axis=1)按行累加，axis=0为按列累加</p>\n</li>\n<li><p>argsort()<br>argsort()是numpy中的方法，得到矩阵中每个元素的排序序号<br>A=array.argsort()  A[0]表示排序后 排在第一个的那个数在原来数组中的下标</p>\n</li>\n<li><p>dict.get(key,x)<br>Python中字典的方法，get(key,x)从字典中获取key对应的value，字典中没有key的话返回0</p>\n</li>\n<li><p>sorted()</p>\n</li>\n<li><p>numpy中有min()、max()方法，用法如下<br>array.min(0)  返回一个数组，数组中每个数都是它所在列的所有数的最小值<br>array.min(1)  返回一个数组，数组中每个数都是它所在行的所有数的最小值</p>\n</li>\n<li><p>listdir(‘str’)<br>strlist=listdir(‘str’)  读取目录str下的所有文件名，返回一个字符串列表</p>\n</li>\n<li><p>split()<br>python中的方法，切片函数<br>string.split(‘str’)以字符str为分隔符切片，返回list</p>\n</li>\n<li><p>zeros<br>a=np.zeros((m,n), dtype=np.int) #创建数据类型为int型的大小为m*n的零矩阵</p>\n</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"Numpy Cookbook","path":"2017/01/23/numpycookbook/","eyeCatchImage":null,"excerpt":"<p>Cookbook网址：<a href=\"https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\" target=\"_blank\" rel=\"noopener\">Numpy Cookbook</a><br>Numpy的一些语法查询和总结<br>持续更新</p>","date":"2017-01-23T12:12:40.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["math","machinelearning","code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"pandas 数据处理基础","date":"2017-02-04T13:02:39.000Z","_content":"***\n以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。\n\n<!--more-->\n\n# 引入库\n```python\nimport csv as csv \nimport pandas as pd\nimport numpy as np\n```\n# 读取文件\n```python\ntrain = pd.read_csv(r\"文件目录\") \n```\n此时数据的样式是：\n![i0Tn41.jpg](https://s1.ax1x.com/2018/10/20/i0Tn41.jpg)\n\n# 数据概览\n-\tdescribe 显示整体数据常见属性\n```python\nprint(train.describe())\n```\n![i0TP3V.jpg](https://s1.ax1x.com/2018/10/20/i0TP3V.jpg)\n-\thead tail 显示首尾一些数据\n```python\nprint(train.head(5))\nprint(train.tail(3))\n```\n![i0TFjU.jpg](https://s1.ax1x.com/2018/10/20/i0TFjU.jpg)\n-\tindex：索引，默认自建整型索引；columns：列；values：数据数值\n```python\nprint(train.index)\nprint(train.columns)\nprint(train.values)\n```\n# 数据操作\n-\tT：数据的转置\n```python\nprint(train.T)\n```\n![i0TAuF.jpg](https://s1.ax1x.com/2018/10/20/i0TAuF.jpg)\n-\tsort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序\n```python\nprint(train.sort_index(axis=0,ascending=True))\nprint(train.sort_values(by=\"Age\",ascending=False))\n```\n![i0TEB4.jpg](https://s1.ax1x.com/2018/10/20/i0TEB4.jpg)\n![i0TmNR.jpg](https://s1.ax1x.com/2018/10/20/i0TmNR.jpg)\n\n# 数据选择\n-\t按照标签选择，选择列，行切片\n```python\nprint(train['Age'])\nprint(train[0:9])\n```\n![i0TVHJ.jpg](https://s1.ax1x.com/2018/10/20/i0TVHJ.jpg)\n![i0TeE9.jpg](https://s1.ax1x.com/2018/10/20/i0TeE9.jpg)\n-\t利用loc自由选择某些行某些列，可以用at替代\n```python\nprint(train.loc[train.index[4:6]])\nprint(train.loc[:,['Age','Fare']])\nprint(train.loc[3:5,['Age','Fare']])\nprint(train.loc[4,'Age'])\nprint(train.at[4,'Age'])\n```\n-\t利用iloc按照位置进行选择\n```python\nprint(train.iloc[5])\nprint(train.iloc[3:5,2:4])\nprint(train.iloc[[1,2,4],[2,5]])\nprint(train.iloc[3,3])\n```\n-\t布尔选择\n```python\nprint( train[ (train['Age']>40) & (train['Age']<50) ] )\nprint(train[train['Parch'].isin([1,2])])\nprint(train[pd.isnull(train['Age'])==True])\n```\n![i0TK9x.jpg](https://s1.ax1x.com/2018/10/20/i0TK9x.jpg)\n![i0TM36.jpg](https://s1.ax1x.com/2018/10/20/i0TM36.jpg)\n![i0TljO.jpg](https://s1.ax1x.com/2018/10/20/i0TljO.jpg)\n\n# 缺失值处理\n-\t利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()\n```python\ntrain1=train.reindex(index=train.index[0:5],columns=['PassengerId']+['Age']+['Sex'])#选择前5行，只取选定的三列\nprint(train1)\nprint(train1.dropna(axis=0)) #删除存在nan值的行\nprint(train1.dropna(subset=['Age','Sex'])) #删除年龄性别列中存在nan值的行\nprint(pd.isnull(train1)) #nan值改为true，其余值改为false\nprint(train1.fillna(value=2333)) #缺失值替换为2333\n```\n![i0TcUs.jpg](https://s1.ax1x.com/2018/10/20/i0TcUs.jpg)\n# 应用函数\n-\t可以自己写函数并应用到数据的行或者列，通过axis参数选择行列\n```python\n#写函数统计包含nan值的行数\ndef null_count(column):\n    column_null=pd.isnull(column)\n    null=column[column_null == True]\n    return len(null)\nprint(train.apply(null_count))\n```\n![i0TQgK.jpg](https://s1.ax1x.com/2018/10/20/i0TQgK.jpg)\n```python\n#写函数对年龄列进行分类\ndef judge(row):\n    if pd.isnull(row['Age']) ==True:\n        return 'unknown'\n    return 'youngth' if row['Age']<18 else 'adult'\nprint(train.apply(judge,axis=1))\n```\n![i07jFs.jpg](https://s1.ax1x.com/2018/10/20/i07jFs.jpg)\n# 数据透视表\n-\t自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值\n```python\nprint(train.pivot_table(index=[\"Pclass\",\"Sex\"], values=[\"Age\", \"Fare\"], aggfunc=np.mean))\n```\n![i0T3uD.jpg](https://s1.ax1x.com/2018/10/20/i0T3uD.jpg)\n\n\n\n# 数据合并\n\n- 数据合并的一些操作，待补全\n\n  ```Python\n  import pandas as pd\n  data1 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber':[1,3,5,7]})\n   \n  data2=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber':[2,3,6,10]})\n\n  print(\"merge:\\n\",pd.merge(data1,data2),\"\\n\")\n\n  data3 = pd.DataFrame({'level1':['a','b','c','d'],\n                   'numeber1':[1,3,5,7]})\n  data4 = pd.DataFrame({'level2':['a','b','c','e'],\n                   'numeber2':[2,3,6,10]})\n  print(\"merge with left_on,right_on: \\n\",pd.merge(data3,data4,left_on='level1',right_on='level2'),\"\\n\")\n\n  print(\"concat: \\n\",pd.concat([data1,data2]),\"\\n\")\n\n  data3 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber1':[1,3,5,np.nan]})\n  data4=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber2':[2,np.nan,6,10]})\n  print(\"combine: \\n\",data3.combine_first(data4),\"\\n\")\n  ```\n![i0T8De.jpg](https://s1.ax1x.com/2018/10/20/i0T8De.jpg)\n","source":"_posts/pandas-skill.md","raw":"---\ntitle: pandas 数据处理基础\ndate: 2017-02-04 21:02:39\ntags: [math,machinelearning,python,code]\ncategories: Python\n---\n***\n以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。\n\n<!--more-->\n\n# 引入库\n```python\nimport csv as csv \nimport pandas as pd\nimport numpy as np\n```\n# 读取文件\n```python\ntrain = pd.read_csv(r\"文件目录\") \n```\n此时数据的样式是：\n![i0Tn41.jpg](https://s1.ax1x.com/2018/10/20/i0Tn41.jpg)\n\n# 数据概览\n-\tdescribe 显示整体数据常见属性\n```python\nprint(train.describe())\n```\n![i0TP3V.jpg](https://s1.ax1x.com/2018/10/20/i0TP3V.jpg)\n-\thead tail 显示首尾一些数据\n```python\nprint(train.head(5))\nprint(train.tail(3))\n```\n![i0TFjU.jpg](https://s1.ax1x.com/2018/10/20/i0TFjU.jpg)\n-\tindex：索引，默认自建整型索引；columns：列；values：数据数值\n```python\nprint(train.index)\nprint(train.columns)\nprint(train.values)\n```\n# 数据操作\n-\tT：数据的转置\n```python\nprint(train.T)\n```\n![i0TAuF.jpg](https://s1.ax1x.com/2018/10/20/i0TAuF.jpg)\n-\tsort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序\n```python\nprint(train.sort_index(axis=0,ascending=True))\nprint(train.sort_values(by=\"Age\",ascending=False))\n```\n![i0TEB4.jpg](https://s1.ax1x.com/2018/10/20/i0TEB4.jpg)\n![i0TmNR.jpg](https://s1.ax1x.com/2018/10/20/i0TmNR.jpg)\n\n# 数据选择\n-\t按照标签选择，选择列，行切片\n```python\nprint(train['Age'])\nprint(train[0:9])\n```\n![i0TVHJ.jpg](https://s1.ax1x.com/2018/10/20/i0TVHJ.jpg)\n![i0TeE9.jpg](https://s1.ax1x.com/2018/10/20/i0TeE9.jpg)\n-\t利用loc自由选择某些行某些列，可以用at替代\n```python\nprint(train.loc[train.index[4:6]])\nprint(train.loc[:,['Age','Fare']])\nprint(train.loc[3:5,['Age','Fare']])\nprint(train.loc[4,'Age'])\nprint(train.at[4,'Age'])\n```\n-\t利用iloc按照位置进行选择\n```python\nprint(train.iloc[5])\nprint(train.iloc[3:5,2:4])\nprint(train.iloc[[1,2,4],[2,5]])\nprint(train.iloc[3,3])\n```\n-\t布尔选择\n```python\nprint( train[ (train['Age']>40) & (train['Age']<50) ] )\nprint(train[train['Parch'].isin([1,2])])\nprint(train[pd.isnull(train['Age'])==True])\n```\n![i0TK9x.jpg](https://s1.ax1x.com/2018/10/20/i0TK9x.jpg)\n![i0TM36.jpg](https://s1.ax1x.com/2018/10/20/i0TM36.jpg)\n![i0TljO.jpg](https://s1.ax1x.com/2018/10/20/i0TljO.jpg)\n\n# 缺失值处理\n-\t利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()\n```python\ntrain1=train.reindex(index=train.index[0:5],columns=['PassengerId']+['Age']+['Sex'])#选择前5行，只取选定的三列\nprint(train1)\nprint(train1.dropna(axis=0)) #删除存在nan值的行\nprint(train1.dropna(subset=['Age','Sex'])) #删除年龄性别列中存在nan值的行\nprint(pd.isnull(train1)) #nan值改为true，其余值改为false\nprint(train1.fillna(value=2333)) #缺失值替换为2333\n```\n![i0TcUs.jpg](https://s1.ax1x.com/2018/10/20/i0TcUs.jpg)\n# 应用函数\n-\t可以自己写函数并应用到数据的行或者列，通过axis参数选择行列\n```python\n#写函数统计包含nan值的行数\ndef null_count(column):\n    column_null=pd.isnull(column)\n    null=column[column_null == True]\n    return len(null)\nprint(train.apply(null_count))\n```\n![i0TQgK.jpg](https://s1.ax1x.com/2018/10/20/i0TQgK.jpg)\n```python\n#写函数对年龄列进行分类\ndef judge(row):\n    if pd.isnull(row['Age']) ==True:\n        return 'unknown'\n    return 'youngth' if row['Age']<18 else 'adult'\nprint(train.apply(judge,axis=1))\n```\n![i07jFs.jpg](https://s1.ax1x.com/2018/10/20/i07jFs.jpg)\n# 数据透视表\n-\t自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值\n```python\nprint(train.pivot_table(index=[\"Pclass\",\"Sex\"], values=[\"Age\", \"Fare\"], aggfunc=np.mean))\n```\n![i0T3uD.jpg](https://s1.ax1x.com/2018/10/20/i0T3uD.jpg)\n\n\n\n# 数据合并\n\n- 数据合并的一些操作，待补全\n\n  ```Python\n  import pandas as pd\n  data1 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber':[1,3,5,7]})\n   \n  data2=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber':[2,3,6,10]})\n\n  print(\"merge:\\n\",pd.merge(data1,data2),\"\\n\")\n\n  data3 = pd.DataFrame({'level1':['a','b','c','d'],\n                   'numeber1':[1,3,5,7]})\n  data4 = pd.DataFrame({'level2':['a','b','c','e'],\n                   'numeber2':[2,3,6,10]})\n  print(\"merge with left_on,right_on: \\n\",pd.merge(data3,data4,left_on='level1',right_on='level2'),\"\\n\")\n\n  print(\"concat: \\n\",pd.concat([data1,data2]),\"\\n\")\n\n  data3 = pd.DataFrame({'level':['a','b','c','d'],\n                   'numeber1':[1,3,5,np.nan]})\n  data4=pd.DataFrame({'level':['a','b','c','e'],\n                   'numeber2':[2,np.nan,6,10]})\n  print(\"combine: \\n\",data3.combine_first(data4),\"\\n\")\n  ```\n![i0T8De.jpg](https://s1.ax1x.com/2018/10/20/i0T8De.jpg)\n","slug":"pandas-skill","published":1,"updated":"2019-07-22T03:45:23.315Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v4t0027q8t55vh92gj8","content":"<hr>\n<p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。</p>\n<a id=\"more\"></a>\n<h1 id=\"引入库\"><a href=\"#引入库\" class=\"headerlink\" title=\"引入库\"></a>引入库</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure>\n<h1 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = pd.read_csv(<span class=\"string\">r\"文件目录\"</span>)</span><br></pre></td></tr></table></figure>\n<p>此时数据的样式是：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tn41.jpg\" alt=\"i0Tn41.jpg\"></p>\n<h1 id=\"数据概览\"><a href=\"#数据概览\" class=\"headerlink\" title=\"数据概览\"></a>数据概览</h1><ul>\n<li>describe 显示整体数据常见属性<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.describe())</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TP3V.jpg\" alt=\"i0TP3V.jpg\"></p>\n<ul>\n<li>head tail 显示首尾一些数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.head(<span class=\"number\">5</span>))</span><br><span class=\"line\">print(train.tail(<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TFjU.jpg\" alt=\"i0TFjU.jpg\"></p>\n<ul>\n<li>index：索引，默认自建整型索引；columns：列；values：数据数值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.index)</span><br><span class=\"line\">print(train.columns)</span><br><span class=\"line\">print(train.values)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h1><ul>\n<li>T：数据的转置<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.T)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TAuF.jpg\" alt=\"i0TAuF.jpg\"></p>\n<ul>\n<li>sort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.sort_index(axis=<span class=\"number\">0</span>,ascending=<span class=\"literal\">True</span>))</span><br><span class=\"line\">print(train.sort_values(by=<span class=\"string\">\"Age\"</span>,ascending=<span class=\"literal\">False</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TEB4.jpg\" alt=\"i0TEB4.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TmNR.jpg\" alt=\"i0TmNR.jpg\"></p>\n<h1 id=\"数据选择\"><a href=\"#数据选择\" class=\"headerlink\" title=\"数据选择\"></a>数据选择</h1><ul>\n<li>按照标签选择，选择列，行切片<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train[<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train[<span class=\"number\">0</span>:<span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TVHJ.jpg\" alt=\"i0TVHJ.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TeE9.jpg\" alt=\"i0TeE9.jpg\"></p>\n<ul>\n<li><p>利用loc自由选择某些行某些列，可以用at替代</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.loc[train.index[<span class=\"number\">4</span>:<span class=\"number\">6</span>]])</span><br><span class=\"line\">print(train.loc[:,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train.at[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>利用iloc按照位置进行选择</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.iloc[<span class=\"number\">5</span>])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">2</span>:<span class=\"number\">4</span>])</span><br><span class=\"line\">print(train.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">5</span>]])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>布尔选择</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print( train[ (train[<span class=\"string\">'Age'</span>]&gt;<span class=\"number\">40</span>) &amp; (train[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">50</span>) ] )</span><br><span class=\"line\">print(train[train[<span class=\"string\">'Parch'</span>].isin([<span class=\"number\">1</span>,<span class=\"number\">2</span>])])</span><br><span class=\"line\">print(train[pd.isnull(train[<span class=\"string\">'Age'</span>])==<span class=\"literal\">True</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TK9x.jpg\" alt=\"i0TK9x.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TM36.jpg\" alt=\"i0TM36.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TljO.jpg\" alt=\"i0TljO.jpg\"></p>\n<h1 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h1><ul>\n<li>利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train1=train.reindex(index=train.index[<span class=\"number\">0</span>:<span class=\"number\">5</span>],columns=[<span class=\"string\">'PassengerId'</span>]+[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>])<span class=\"comment\">#选择前5行，只取选定的三列</span></span><br><span class=\"line\">print(train1)</span><br><span class=\"line\">print(train1.dropna(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#删除存在nan值的行</span></span><br><span class=\"line\">print(train1.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>])) <span class=\"comment\">#删除年龄性别列中存在nan值的行</span></span><br><span class=\"line\">print(pd.isnull(train1)) <span class=\"comment\">#nan值改为true，其余值改为false</span></span><br><span class=\"line\">print(train1.fillna(value=<span class=\"number\">2333</span>)) <span class=\"comment\">#缺失值替换为2333</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TcUs.jpg\" alt=\"i0TcUs.jpg\"></p>\n<h1 id=\"应用函数\"><a href=\"#应用函数\" class=\"headerlink\" title=\"应用函数\"></a>应用函数</h1><ul>\n<li>可以自己写函数并应用到数据的行或者列，通过axis参数选择行列<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数统计包含nan值的行数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">null_count</span><span class=\"params\">(column)</span>:</span></span><br><span class=\"line\">    column_null=pd.isnull(column)</span><br><span class=\"line\">    null=column[column_null == <span class=\"literal\">True</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> len(null)</span><br><span class=\"line\">print(train.apply(null_count))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TQgK.jpg\" alt=\"i0TQgK.jpg\"><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数对年龄列进行分类</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">judge</span><span class=\"params\">(row)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pd.isnull(row[<span class=\"string\">'Age'</span>]) ==<span class=\"literal\">True</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'unknown'</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">'youngth'</span> <span class=\"keyword\">if</span> row[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">18</span> <span class=\"keyword\">else</span> <span class=\"string\">'adult'</span></span><br><span class=\"line\">print(train.apply(judge,axis=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07jFs.jpg\" alt=\"i07jFs.jpg\"></p>\n<h1 id=\"数据透视表\"><a href=\"#数据透视表\" class=\"headerlink\" title=\"数据透视表\"></a>数据透视表</h1><ul>\n<li>自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.pivot_table(index=[<span class=\"string\">\"Pclass\"</span>,<span class=\"string\">\"Sex\"</span>], values=[<span class=\"string\">\"Age\"</span>, <span class=\"string\">\"Fare\"</span>], aggfunc=np.mean))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0T3uD.jpg\" alt=\"i0T3uD.jpg\"></p>\n<h1 id=\"数据合并\"><a href=\"#数据合并\" class=\"headerlink\" title=\"数据合并\"></a>数据合并</h1><ul>\n<li><p>数据合并的一些操作，待补全</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data1 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\"> </span><br><span class=\"line\">data2=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"merge:\\n\"</span>,pd.merge(data1,data2),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level1'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\">data4 = pd.DataFrame(&#123;<span class=\"string\">'level2'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"merge with left_on,right_on: \\n\"</span>,pd.merge(data3,data4,left_on=<span class=\"string\">'level1'</span>,right_on=<span class=\"string\">'level2'</span>),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"concat: \\n\"</span>,pd.concat([data1,data2]),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,np.nan]&#125;)</span><br><span class=\"line\">data4=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,np.nan,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"combine: \\n\"</span>,data3.combine_first(data4),<span class=\"string\">\"\\n\"</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0T8De.jpg\" alt=\"i0T8De.jpg\"></p>\n","site":{"data":{}},"excerpt":"<hr>\n<p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。</p>","more":"<h1 id=\"引入库\"><a href=\"#引入库\" class=\"headerlink\" title=\"引入库\"></a>引入库</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv <span class=\"keyword\">as</span> csv </span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure>\n<h1 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = pd.read_csv(<span class=\"string\">r\"文件目录\"</span>)</span><br></pre></td></tr></table></figure>\n<p>此时数据的样式是：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tn41.jpg\" alt=\"i0Tn41.jpg\"></p>\n<h1 id=\"数据概览\"><a href=\"#数据概览\" class=\"headerlink\" title=\"数据概览\"></a>数据概览</h1><ul>\n<li>describe 显示整体数据常见属性<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.describe())</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TP3V.jpg\" alt=\"i0TP3V.jpg\"></p>\n<ul>\n<li>head tail 显示首尾一些数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.head(<span class=\"number\">5</span>))</span><br><span class=\"line\">print(train.tail(<span class=\"number\">3</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TFjU.jpg\" alt=\"i0TFjU.jpg\"></p>\n<ul>\n<li>index：索引，默认自建整型索引；columns：列；values：数据数值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.index)</span><br><span class=\"line\">print(train.columns)</span><br><span class=\"line\">print(train.values)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"数据操作\"><a href=\"#数据操作\" class=\"headerlink\" title=\"数据操作\"></a>数据操作</h1><ul>\n<li>T：数据的转置<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.T)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TAuF.jpg\" alt=\"i0TAuF.jpg\"></p>\n<ul>\n<li>sort：可以按索引或者值进行排序，axis选择维度(行还是列),ascending选择升序或者降序,Nan永远排在最后，无论升序降序<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.sort_index(axis=<span class=\"number\">0</span>,ascending=<span class=\"literal\">True</span>))</span><br><span class=\"line\">print(train.sort_values(by=<span class=\"string\">\"Age\"</span>,ascending=<span class=\"literal\">False</span>))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TEB4.jpg\" alt=\"i0TEB4.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TmNR.jpg\" alt=\"i0TmNR.jpg\"></p>\n<h1 id=\"数据选择\"><a href=\"#数据选择\" class=\"headerlink\" title=\"数据选择\"></a>数据选择</h1><ul>\n<li>按照标签选择，选择列，行切片<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train[<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train[<span class=\"number\">0</span>:<span class=\"number\">9</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TVHJ.jpg\" alt=\"i0TVHJ.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TeE9.jpg\" alt=\"i0TeE9.jpg\"></p>\n<ul>\n<li><p>利用loc自由选择某些行某些列，可以用at替代</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.loc[train.index[<span class=\"number\">4</span>:<span class=\"number\">6</span>]])</span><br><span class=\"line\">print(train.loc[:,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,[<span class=\"string\">'Age'</span>,<span class=\"string\">'Fare'</span>]])</span><br><span class=\"line\">print(train.loc[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br><span class=\"line\">print(train.at[<span class=\"number\">4</span>,<span class=\"string\">'Age'</span>])</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>利用iloc按照位置进行选择</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.iloc[<span class=\"number\">5</span>])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>:<span class=\"number\">5</span>,<span class=\"number\">2</span>:<span class=\"number\">4</span>])</span><br><span class=\"line\">print(train.iloc[[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">4</span>],[<span class=\"number\">2</span>,<span class=\"number\">5</span>]])</span><br><span class=\"line\">print(train.iloc[<span class=\"number\">3</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>布尔选择</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print( train[ (train[<span class=\"string\">'Age'</span>]&gt;<span class=\"number\">40</span>) &amp; (train[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">50</span>) ] )</span><br><span class=\"line\">print(train[train[<span class=\"string\">'Parch'</span>].isin([<span class=\"number\">1</span>,<span class=\"number\">2</span>])])</span><br><span class=\"line\">print(train[pd.isnull(train[<span class=\"string\">'Age'</span>])==<span class=\"literal\">True</span>])</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TK9x.jpg\" alt=\"i0TK9x.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TM36.jpg\" alt=\"i0TM36.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TljO.jpg\" alt=\"i0TljO.jpg\"></p>\n<h1 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h1><ul>\n<li>利用reindex选择部分数据进行拷贝，并进行缺失值处理。一些函数会自动过滤掉缺失值，比如mean()<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train1=train.reindex(index=train.index[<span class=\"number\">0</span>:<span class=\"number\">5</span>],columns=[<span class=\"string\">'PassengerId'</span>]+[<span class=\"string\">'Age'</span>]+[<span class=\"string\">'Sex'</span>])<span class=\"comment\">#选择前5行，只取选定的三列</span></span><br><span class=\"line\">print(train1)</span><br><span class=\"line\">print(train1.dropna(axis=<span class=\"number\">0</span>)) <span class=\"comment\">#删除存在nan值的行</span></span><br><span class=\"line\">print(train1.dropna(subset=[<span class=\"string\">'Age'</span>,<span class=\"string\">'Sex'</span>])) <span class=\"comment\">#删除年龄性别列中存在nan值的行</span></span><br><span class=\"line\">print(pd.isnull(train1)) <span class=\"comment\">#nan值改为true，其余值改为false</span></span><br><span class=\"line\">print(train1.fillna(value=<span class=\"number\">2333</span>)) <span class=\"comment\">#缺失值替换为2333</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TcUs.jpg\" alt=\"i0TcUs.jpg\"></p>\n<h1 id=\"应用函数\"><a href=\"#应用函数\" class=\"headerlink\" title=\"应用函数\"></a>应用函数</h1><ul>\n<li>可以自己写函数并应用到数据的行或者列，通过axis参数选择行列<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数统计包含nan值的行数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">null_count</span><span class=\"params\">(column)</span>:</span></span><br><span class=\"line\">    column_null=pd.isnull(column)</span><br><span class=\"line\">    null=column[column_null == <span class=\"literal\">True</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> len(null)</span><br><span class=\"line\">print(train.apply(null_count))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TQgK.jpg\" alt=\"i0TQgK.jpg\"><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#写函数对年龄列进行分类</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">judge</span><span class=\"params\">(row)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> pd.isnull(row[<span class=\"string\">'Age'</span>]) ==<span class=\"literal\">True</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">'unknown'</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">'youngth'</span> <span class=\"keyword\">if</span> row[<span class=\"string\">'Age'</span>]&lt;<span class=\"number\">18</span> <span class=\"keyword\">else</span> <span class=\"string\">'adult'</span></span><br><span class=\"line\">print(train.apply(judge,axis=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07jFs.jpg\" alt=\"i07jFs.jpg\"></p>\n<h1 id=\"数据透视表\"><a href=\"#数据透视表\" class=\"headerlink\" title=\"数据透视表\"></a>数据透视表</h1><ul>\n<li>自选分类和值进行数据透视，比如按照pclass和sex分类，统计age和fare的平均值<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(train.pivot_table(index=[<span class=\"string\">\"Pclass\"</span>,<span class=\"string\">\"Sex\"</span>], values=[<span class=\"string\">\"Age\"</span>, <span class=\"string\">\"Fare\"</span>], aggfunc=np.mean))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0T3uD.jpg\" alt=\"i0T3uD.jpg\"></p>\n<h1 id=\"数据合并\"><a href=\"#数据合并\" class=\"headerlink\" title=\"数据合并\"></a>数据合并</h1><ul>\n<li><p>数据合并的一些操作，待补全</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data1 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\"> </span><br><span class=\"line\">data2=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"merge:\\n\"</span>,pd.merge(data1,data2),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level1'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">7</span>]&#125;)</span><br><span class=\"line\">data4 = pd.DataFrame(&#123;<span class=\"string\">'level2'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"merge with left_on,right_on: \\n\"</span>,pd.merge(data3,data4,left_on=<span class=\"string\">'level1'</span>,right_on=<span class=\"string\">'level2'</span>),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"concat: \\n\"</span>,pd.concat([data1,data2]),<span class=\"string\">\"\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">data3 = pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber1'</span>:[<span class=\"number\">1</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,np.nan]&#125;)</span><br><span class=\"line\">data4=pd.DataFrame(&#123;<span class=\"string\">'level'</span>:[<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>,<span class=\"string\">'c'</span>,<span class=\"string\">'e'</span>],</span><br><span class=\"line\">                 <span class=\"string\">'numeber2'</span>:[<span class=\"number\">2</span>,np.nan,<span class=\"number\">6</span>,<span class=\"number\">10</span>]&#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"combine: \\n\"</span>,data3.combine_first(data4),<span class=\"string\">\"\\n\"</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0T8De.jpg\" alt=\"i0T8De.jpg\"></p>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0Tn41.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"pandas 数据处理基础","path":"2017/02/04/pandas-skill/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0Tn41.jpg","excerpt":"<hr>\n<p>以泰坦尼克号的数据为例介绍一下前期对数据的基础操作。</p>","date":"2017-02-04T13:02:39.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["math","machinelearning","code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"统计学习方法手写版笔记","date":"2018-08-09T02:03:46.000Z","mathjax":true,"html":true,"_content":"***\n把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）\n现在只有算法本身的流程，以后如果有什么新的理解再补充\n字太丑，自己都看不下去，发上来纯粹做个备份\n\n<!--more--> \n# 概论\n![i0HLjK.jpg](https://s1.ax1x.com/2018/10/20/i0HLjK.jpg)\n\n![i0Hb1x.jpg](https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg)\n\n![i0HoN9.jpg](https://s1.ax1x.com/2018/10/20/i0HoN9.jpg)\n\n# 感知机\n![i0HH91.jpg](https://s1.ax1x.com/2018/10/20/i0HH91.jpg)\n\n![i0HThR.jpg](https://s1.ax1x.com/2018/10/20/i0HThR.jpg)\n\n# k近邻\n![i0Hqc6.jpg](https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg)\n\n![i0HXnO.jpg](https://s1.ax1x.com/2018/10/20/i0HXnO.jpg)\n\n# 朴素贝叶斯\n![i0HjBD.jpg](https://s1.ax1x.com/2018/10/20/i0HjBD.jpg)\n\n# 决策树\n-\tGBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。\n![i0HzAH.jpg](https://s1.ax1x.com/2018/10/20/i0HzAH.jpg)\n\n![i0HvHe.jpg](https://s1.ax1x.com/2018/10/20/i0HvHe.jpg)\n\n![i0bP3t.jpg](https://s1.ax1x.com/2018/10/20/i0bP3t.jpg)\n\n![i0bSNd.jpg](https://s1.ax1x.com/2018/10/20/i0bSNd.jpg)\n\n# 逻辑斯蒂回归、最大熵\n-\t待补充最大熵和逻辑斯蒂回归之间的相互推导\n![i0bigP.jpg](https://s1.ax1x.com/2018/10/20/i0bigP.jpg)\n\n![i0bp4A.jpg](https://s1.ax1x.com/2018/10/20/i0bp4A.jpg)\n\n# 支持向量机\n![i0bFjf.jpg](https://s1.ax1x.com/2018/10/20/i0bFjf.jpg)\n\n![i0bAu8.jpg](https://s1.ax1x.com/2018/10/20/i0bAu8.jpg)\n\n![i0bEDS.jpg](https://s1.ax1x.com/2018/10/20/i0bEDS.jpg)\n\n![i0beEQ.jpg](https://s1.ax1x.com/2018/10/20/i0beEQ.jpg)\n\n![i0bVHg.jpg](https://s1.ax1x.com/2018/10/20/i0bVHg.jpg)\n\n![i0bmNj.jpg](https://s1.ax1x.com/2018/10/20/i0bmNj.jpg)\n\n# 提升方法\n-\t待补充XGBoost\n![i0bn4s.jpg](https://s1.ax1x.com/2018/10/20/i0bn4s.jpg)\n\n![i0bKCn.jpg](https://s1.ax1x.com/2018/10/20/i0bKCn.jpg)\n\n# EM算法\n![i0bM3q.jpg](https://s1.ax1x.com/2018/10/20/i0bM3q.jpg)\n\n![i0bQg0.jpg](https://s1.ax1x.com/2018/10/20/i0bQg0.jpg)\n\n-\t用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。\n-\t在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。\n\n# 隐马尔可夫\n![i0b3uT.jpg](https://s1.ax1x.com/2018/10/20/i0b3uT.jpg)\n\n![i0blvV.jpg](https://s1.ax1x.com/2018/10/20/i0blvV.jpg)\n\n![i0b8DU.jpg](https://s1.ax1x.com/2018/10/20/i0b8DU.jpg)\n\n![i0bGbF.jpg](https://s1.ax1x.com/2018/10/20/i0bGbF.jpg)\n\n# 条件随机场\n-\t待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似\n![i0bYE4.jpg](https://s1.ax1x.com/2018/10/20/i0bYE4.jpg)\n","source":"_posts/statistical-handwriting.md","raw":"---\ntitle: 统计学习方法手写版笔记\ndate: 2018-08-09 10:03:46\ncategories: 机器学习\ntags:\n- code\n- machine learning\n- statistical learning\n- math\n\nmathjax: true\nhtml: true \n---\n***\n把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）\n现在只有算法本身的流程，以后如果有什么新的理解再补充\n字太丑，自己都看不下去，发上来纯粹做个备份\n\n<!--more--> \n# 概论\n![i0HLjK.jpg](https://s1.ax1x.com/2018/10/20/i0HLjK.jpg)\n\n![i0Hb1x.jpg](https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg)\n\n![i0HoN9.jpg](https://s1.ax1x.com/2018/10/20/i0HoN9.jpg)\n\n# 感知机\n![i0HH91.jpg](https://s1.ax1x.com/2018/10/20/i0HH91.jpg)\n\n![i0HThR.jpg](https://s1.ax1x.com/2018/10/20/i0HThR.jpg)\n\n# k近邻\n![i0Hqc6.jpg](https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg)\n\n![i0HXnO.jpg](https://s1.ax1x.com/2018/10/20/i0HXnO.jpg)\n\n# 朴素贝叶斯\n![i0HjBD.jpg](https://s1.ax1x.com/2018/10/20/i0HjBD.jpg)\n\n# 决策树\n-\tGBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。\n![i0HzAH.jpg](https://s1.ax1x.com/2018/10/20/i0HzAH.jpg)\n\n![i0HvHe.jpg](https://s1.ax1x.com/2018/10/20/i0HvHe.jpg)\n\n![i0bP3t.jpg](https://s1.ax1x.com/2018/10/20/i0bP3t.jpg)\n\n![i0bSNd.jpg](https://s1.ax1x.com/2018/10/20/i0bSNd.jpg)\n\n# 逻辑斯蒂回归、最大熵\n-\t待补充最大熵和逻辑斯蒂回归之间的相互推导\n![i0bigP.jpg](https://s1.ax1x.com/2018/10/20/i0bigP.jpg)\n\n![i0bp4A.jpg](https://s1.ax1x.com/2018/10/20/i0bp4A.jpg)\n\n# 支持向量机\n![i0bFjf.jpg](https://s1.ax1x.com/2018/10/20/i0bFjf.jpg)\n\n![i0bAu8.jpg](https://s1.ax1x.com/2018/10/20/i0bAu8.jpg)\n\n![i0bEDS.jpg](https://s1.ax1x.com/2018/10/20/i0bEDS.jpg)\n\n![i0beEQ.jpg](https://s1.ax1x.com/2018/10/20/i0beEQ.jpg)\n\n![i0bVHg.jpg](https://s1.ax1x.com/2018/10/20/i0bVHg.jpg)\n\n![i0bmNj.jpg](https://s1.ax1x.com/2018/10/20/i0bmNj.jpg)\n\n# 提升方法\n-\t待补充XGBoost\n![i0bn4s.jpg](https://s1.ax1x.com/2018/10/20/i0bn4s.jpg)\n\n![i0bKCn.jpg](https://s1.ax1x.com/2018/10/20/i0bKCn.jpg)\n\n# EM算法\n![i0bM3q.jpg](https://s1.ax1x.com/2018/10/20/i0bM3q.jpg)\n\n![i0bQg0.jpg](https://s1.ax1x.com/2018/10/20/i0bQg0.jpg)\n\n-\t用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。\n-\t在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。\n\n# 隐马尔可夫\n![i0b3uT.jpg](https://s1.ax1x.com/2018/10/20/i0b3uT.jpg)\n\n![i0blvV.jpg](https://s1.ax1x.com/2018/10/20/i0blvV.jpg)\n\n![i0b8DU.jpg](https://s1.ax1x.com/2018/10/20/i0b8DU.jpg)\n\n![i0bGbF.jpg](https://s1.ax1x.com/2018/10/20/i0bGbF.jpg)\n\n# 条件随机场\n-\t待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似\n![i0bYE4.jpg](https://s1.ax1x.com/2018/10/20/i0bYE4.jpg)\n","slug":"statistical-handwriting","published":1,"updated":"2019-07-22T03:45:23.377Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v4z002aq8t582f7wtuo","content":"<hr>\n<p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p>\n<a id=\"more\"></a> \n<h1 id=\"概论\"><a href=\"#概论\" class=\"headerlink\" title=\"概论\"></a>概论</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HLjK.jpg\" alt=\"i0HLjK.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg\" alt=\"i0Hb1x.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HoN9.jpg\" alt=\"i0HoN9.jpg\"></p>\n<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HH91.jpg\" alt=\"i0HH91.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HThR.jpg\" alt=\"i0HThR.jpg\"></p>\n<h1 id=\"k近邻\"><a href=\"#k近邻\" class=\"headerlink\" title=\"k近邻\"></a>k近邻</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg\" alt=\"i0Hqc6.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HXnO.jpg\" alt=\"i0HXnO.jpg\"></p>\n<h1 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HjBD.jpg\" alt=\"i0HjBD.jpg\"></p>\n<h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><ul>\n<li>GBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0HzAH.jpg\" alt=\"i0HzAH.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HvHe.jpg\" alt=\"i0HvHe.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bP3t.jpg\" alt=\"i0bP3t.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bSNd.jpg\" alt=\"i0bSNd.jpg\"></p>\n<h1 id=\"逻辑斯蒂回归、最大熵\"><a href=\"#逻辑斯蒂回归、最大熵\" class=\"headerlink\" title=\"逻辑斯蒂回归、最大熵\"></a>逻辑斯蒂回归、最大熵</h1><ul>\n<li>待补充最大熵和逻辑斯蒂回归之间的相互推导<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bigP.jpg\" alt=\"i0bigP.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bp4A.jpg\" alt=\"i0bp4A.jpg\"></p>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0bFjf.jpg\" alt=\"i0bFjf.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bAu8.jpg\" alt=\"i0bAu8.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bEDS.jpg\" alt=\"i0bEDS.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0beEQ.jpg\" alt=\"i0beEQ.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bVHg.jpg\" alt=\"i0bVHg.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bmNj.jpg\" alt=\"i0bmNj.jpg\"></p>\n<h1 id=\"提升方法\"><a href=\"#提升方法\" class=\"headerlink\" title=\"提升方法\"></a>提升方法</h1><ul>\n<li>待补充XGBoost<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bn4s.jpg\" alt=\"i0bn4s.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bKCn.jpg\" alt=\"i0bKCn.jpg\"></p>\n<h1 id=\"EM算法\"><a href=\"#EM算法\" class=\"headerlink\" title=\"EM算法\"></a>EM算法</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0bM3q.jpg\" alt=\"i0bM3q.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bQg0.jpg\" alt=\"i0bQg0.jpg\"></p>\n<ul>\n<li>用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。</li>\n<li>在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。</li>\n</ul>\n<h1 id=\"隐马尔可夫\"><a href=\"#隐马尔可夫\" class=\"headerlink\" title=\"隐马尔可夫\"></a>隐马尔可夫</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0b3uT.jpg\" alt=\"i0b3uT.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0blvV.jpg\" alt=\"i0blvV.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0b8DU.jpg\" alt=\"i0b8DU.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bGbF.jpg\" alt=\"i0bGbF.jpg\"></p>\n<h1 id=\"条件随机场\"><a href=\"#条件随机场\" class=\"headerlink\" title=\"条件随机场\"></a>条件随机场</h1><ul>\n<li>待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bYE4.jpg\" alt=\"i0bYE4.jpg\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p>","more":"<h1 id=\"概论\"><a href=\"#概论\" class=\"headerlink\" title=\"概论\"></a>概论</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HLjK.jpg\" alt=\"i0HLjK.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0Hb1x.jpg\" alt=\"i0Hb1x.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HoN9.jpg\" alt=\"i0HoN9.jpg\"></p>\n<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HH91.jpg\" alt=\"i0HH91.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HThR.jpg\" alt=\"i0HThR.jpg\"></p>\n<h1 id=\"k近邻\"><a href=\"#k近邻\" class=\"headerlink\" title=\"k近邻\"></a>k近邻</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0Hqc6.jpg\" alt=\"i0Hqc6.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HXnO.jpg\" alt=\"i0HXnO.jpg\"></p>\n<h1 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0HjBD.jpg\" alt=\"i0HjBD.jpg\"></p>\n<h1 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h1><ul>\n<li>GBDT写在了提升方法里，另外可以扩展看看随机森林，是一个自举的方法，利用了决策树。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0HzAH.jpg\" alt=\"i0HzAH.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0HvHe.jpg\" alt=\"i0HvHe.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bP3t.jpg\" alt=\"i0bP3t.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bSNd.jpg\" alt=\"i0bSNd.jpg\"></p>\n<h1 id=\"逻辑斯蒂回归、最大熵\"><a href=\"#逻辑斯蒂回归、最大熵\" class=\"headerlink\" title=\"逻辑斯蒂回归、最大熵\"></a>逻辑斯蒂回归、最大熵</h1><ul>\n<li>待补充最大熵和逻辑斯蒂回归之间的相互推导<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bigP.jpg\" alt=\"i0bigP.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bp4A.jpg\" alt=\"i0bp4A.jpg\"></p>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0bFjf.jpg\" alt=\"i0bFjf.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bAu8.jpg\" alt=\"i0bAu8.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bEDS.jpg\" alt=\"i0bEDS.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0beEQ.jpg\" alt=\"i0beEQ.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bVHg.jpg\" alt=\"i0bVHg.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bmNj.jpg\" alt=\"i0bmNj.jpg\"></p>\n<h1 id=\"提升方法\"><a href=\"#提升方法\" class=\"headerlink\" title=\"提升方法\"></a>提升方法</h1><ul>\n<li>待补充XGBoost<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bn4s.jpg\" alt=\"i0bn4s.jpg\"></li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bKCn.jpg\" alt=\"i0bKCn.jpg\"></p>\n<h1 id=\"EM算法\"><a href=\"#EM算法\" class=\"headerlink\" title=\"EM算法\"></a>EM算法</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0bM3q.jpg\" alt=\"i0bM3q.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bQg0.jpg\" alt=\"i0bQg0.jpg\"></p>\n<ul>\n<li>用EM算法做高斯混合模型的推断时，需要推断的参数包括k个高斯模型的均值、方差、比例系数，隐变量代表第j个观测样本来自第k个高斯模型的可能，叫做responsibility，而$n_k$则是对第k个高斯模型在所有样本上的responsibility的总和，除以$N$即以其均值来更新GMM比例系数，用responsibility加权样本来更新均值，方差同理。</li>\n<li>在更新完参数之后，再用这些参数重新计算responsibility，重新计算E步骤，再继续做M步骤，从而完成迭代。</li>\n</ul>\n<h1 id=\"隐马尔可夫\"><a href=\"#隐马尔可夫\" class=\"headerlink\" title=\"隐马尔可夫\"></a>隐马尔可夫</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0b3uT.jpg\" alt=\"i0b3uT.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0blvV.jpg\" alt=\"i0blvV.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0b8DU.jpg\" alt=\"i0b8DU.jpg\"></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0bGbF.jpg\" alt=\"i0bGbF.jpg\"></p>\n<h1 id=\"条件随机场\"><a href=\"#条件随机场\" class=\"headerlink\" title=\"条件随机场\"></a>条件随机场</h1><ul>\n<li>待补充三种问题的解法，因为条件随机场是隐马尔可夫模型的条件化扩展，算法也类似<br><img src=\"https://s1.ax1x.com/2018/10/20/i0bYE4.jpg\" alt=\"i0bYE4.jpg\"></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0HLjK.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"统计学习方法手写版笔记","path":"2018/08/09/statistical-handwriting/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0HLjK.jpg","excerpt":"<hr>\n<p>把统计学习方法十大算法精简了一些手写了出来（虽然我觉得书本身已经很精简了）<br>现在只有算法本身的流程，以后如果有什么新的理解再补充<br>字太丑，自己都看不下去，发上来纯粹做个备份</p>","date":"2018-08-09T02:03:46.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","code","machine learning","statistical learning"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"用Python实现字典树","date":"2017-05-02T02:09:19.000Z","_content":"***\n在Python中有字典这一数据结构，因此用Python实现字典树很方便\n\n<!--more-->\n\n![i07rz6.jpg](https://s1.ax1x.com/2018/10/20/i07rz6.jpg)\n\n# 字典树(trie)\n-\t字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。\n-\t字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找\n-\t一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身\n\n# 例子\n-\t以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成\n-\t初始化trie\n\t```Python\n\t\ttrie = {}\n\t```\n-\ttrie中插入单词\n\t```Python\n\t\tdef insert(trie, w):\n\t\t\tfor c in w:\n\t\t\t\ttrie = trie.setdefault(c, {})  # if c not in trie then set trie[c]={}\n\t\t\ttrie[None] = w\n\t```\n\t对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点\n\t遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数\n\t这样一棵树最终完成时就如标题图所示，其中前四行是加入[\"cat\", \"cats\",\"dog\", \"rat\"]之后树的内容\n-\ttrie中查找某一个单词\n\t```Python\n\t\tdef prefixs(trie, w, lo):\n\t\t\tfor i in range(lo, len(w)):\n\t\t\t\ttrie = trie.get(w[i])\n\t\t\t\tif trie is None:\n\t\t\t\t\tbreak\n\t\t\t\tprefix = trie.get(None)\n\t\t\tif prefix:\n\t\t\t\tyield i + 1, prefix\n\t```\n\t因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同\n\t特别说明的是，trie的一大优势便是支持插入与查找同时进行\n\t\n# 后缀树\n-\t如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。\n-\t简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化\n-\t待续\n\t\t\t\t","source":"_posts/trie.md","raw":"---\ntitle: 用Python实现字典树\ndate: 2017-05-02 10:09:19\ntags:\n-\tcode\n-\tpython\ncategories:\n-\tPython\n---\n***\n在Python中有字典这一数据结构，因此用Python实现字典树很方便\n\n<!--more-->\n\n![i07rz6.jpg](https://s1.ax1x.com/2018/10/20/i07rz6.jpg)\n\n# 字典树(trie)\n-\t字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。\n-\t字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找\n-\t一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身\n\n# 例子\n-\t以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成\n-\t初始化trie\n\t```Python\n\t\ttrie = {}\n\t```\n-\ttrie中插入单词\n\t```Python\n\t\tdef insert(trie, w):\n\t\t\tfor c in w:\n\t\t\t\ttrie = trie.setdefault(c, {})  # if c not in trie then set trie[c]={}\n\t\t\ttrie[None] = w\n\t```\n\t对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点\n\t遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数\n\t这样一棵树最终完成时就如标题图所示，其中前四行是加入[\"cat\", \"cats\",\"dog\", \"rat\"]之后树的内容\n-\ttrie中查找某一个单词\n\t```Python\n\t\tdef prefixs(trie, w, lo):\n\t\t\tfor i in range(lo, len(w)):\n\t\t\t\ttrie = trie.get(w[i])\n\t\t\t\tif trie is None:\n\t\t\t\t\tbreak\n\t\t\t\tprefix = trie.get(None)\n\t\t\tif prefix:\n\t\t\t\tyield i + 1, prefix\n\t```\n\t因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同\n\t特别说明的是，trie的一大优势便是支持插入与查找同时进行\n\t\n# 后缀树\n-\t如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。\n-\t简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化\n-\t待续\n\t\t\t\t","slug":"trie","published":1,"updated":"2019-07-22T03:45:23.396Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v54002eq8t5cgiqlg7x","content":"<hr>\n<p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07rz6.jpg\" alt=\"i07rz6.jpg\"></p>\n<h1 id=\"字典树-trie\"><a href=\"#字典树-trie\" class=\"headerlink\" title=\"字典树(trie)\"></a>字典树(trie)</h1><ul>\n<li>字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。</li>\n<li>字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找</li>\n<li>一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身</li>\n</ul>\n<h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><ul>\n<li>以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成</li>\n<li><p>初始化trie</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trie = &#123;&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>trie中插入单词</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert</span><span class=\"params\">(trie, w)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> w:</span><br><span class=\"line\">\t\ttrie = trie.setdefault(c, &#123;&#125;)  <span class=\"comment\"># if c not in trie then set trie[c]=&#123;&#125;</span></span><br><span class=\"line\">\ttrie[<span class=\"literal\">None</span>] = w</span><br></pre></td></tr></table></figure>\n<p>对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点<br>遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数<br>这样一棵树最终完成时就如标题图所示，其中前四行是加入[“cat”, “cats”,”dog”, “rat”]之后树的内容</p>\n</li>\n<li><p>trie中查找某一个单词</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">prefixs</span><span class=\"params\">(trie, w, lo)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(lo, len(w)):</span><br><span class=\"line\">\t\ttrie = trie.get(w[i])</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> trie <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\tprefix = trie.get(<span class=\"literal\">None</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> prefix:</span><br><span class=\"line\">\t\t<span class=\"keyword\">yield</span> i + <span class=\"number\">1</span>, prefix</span><br></pre></td></tr></table></figure>\n<p>因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同<br>特别说明的是，trie的一大优势便是支持插入与查找同时进行</p>\n</li>\n</ul>\n<h1 id=\"后缀树\"><a href=\"#后缀树\" class=\"headerlink\" title=\"后缀树\"></a>后缀树</h1><ul>\n<li>如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。</li>\n<li>简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化</li>\n<li>待续</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i07rz6.jpg\" alt=\"i07rz6.jpg\"></p>\n<h1 id=\"字典树-trie\"><a href=\"#字典树-trie\" class=\"headerlink\" title=\"字典树(trie)\"></a>字典树(trie)</h1><ul>\n<li>字典树主要用于词频统计，前缀后缀相关的算法，树的根节点不存任何字符，每一条边代表一个字符，其他每一个节点代表从根节点到此节点的所有边上字符构成的单词，存的内容根据需求而定。</li>\n<li>字典树快的原因就是充分利用的单词的共同前缀，如果前缀都不一样，就不需要继续查找</li>\n<li>一个单词不一定在叶子节点，因为它可能构成其他更长单词的前缀，因此如果用于词频统计，则可以插入完一个单词后在此单词最后一个节点中count++。如果仅仅用于判断某个词是否在字典树构成的字典中，则可以在插入完一个单词后，在最后一个节点中添加一个None节点，内容为单词本身</li>\n</ul>\n<h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><ul>\n<li>以leetcode472为例，当中需要判断某个序列中某个单词是否能由它前面的单词构成</li>\n<li><p>初始化trie</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trie = &#123;&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>trie中插入单词</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert</span><span class=\"params\">(trie, w)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> w:</span><br><span class=\"line\">\t\ttrie = trie.setdefault(c, &#123;&#125;)  <span class=\"comment\"># if c not in trie then set trie[c]=&#123;&#125;</span></span><br><span class=\"line\">\ttrie[<span class=\"literal\">None</span>] = w</span><br></pre></td></tr></table></figure>\n<p>对每一个字符串，依次按字母索引，当索引不到时就对当前字母建立新节点<br>遍历完单词后建立None节点存放单词(因为题目需要返回所有单词，因此此处存放单词,也可以存放出现次数<br>这样一棵树最终完成时就如标题图所示，其中前四行是加入[“cat”, “cats”,”dog”, “rat”]之后树的内容</p>\n</li>\n<li><p>trie中查找某一个单词</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">prefixs</span><span class=\"params\">(trie, w, lo)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(lo, len(w)):</span><br><span class=\"line\">\t\ttrie = trie.get(w[i])</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> trie <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">break</span></span><br><span class=\"line\">\t\tprefix = trie.get(<span class=\"literal\">None</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> prefix:</span><br><span class=\"line\">\t\t<span class=\"keyword\">yield</span> i + <span class=\"number\">1</span>, prefix</span><br></pre></td></tr></table></figure>\n<p>因为题目需要，利用了生成器，这段函数是查找单词w中i从lo位置开始，i到单词尾这一段构成的字符串，是否在trie的字典集合中，返回所有符合结果的i+1。查找的方式与插入相同<br>特别说明的是，trie的一大优势便是支持插入与查找同时进行</p>\n</li>\n</ul>\n<h1 id=\"后缀树\"><a href=\"#后缀树\" class=\"headerlink\" title=\"后缀树\"></a>后缀树</h1><ul>\n<li>如果将一个单词，长度为l，拆分成l个子单词插入到trie中，每个子单词都是这个单词的[i:l]构成的后缀，则这样的字典树称之为后缀树。</li>\n<li>简单按上述方法建立后缀树会存在许多冗余信息，在空间上还可以进行优化</li>\n<li>待续</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i07rz6.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"用Python实现字典树","path":"2017/05/02/trie/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i07rz6.jpg","excerpt":"<hr>\n<p>在Python中有字典这一数据结构，因此用Python实现字典树很方便</p>","date":"2017-05-02T02:09:19.000Z","pv":0,"totalPV":0,"categories":"Python","tags":["code","python"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记1","date":"2017-01-21T03:45:28.000Z","mathjax":true,"html":true,"_content":"\n***\n# <font size=5 >第一讲：方程组的几何解释\n\n-\t从3个角度看待方程组：行图形，列图像，矩阵\n\n-\t例如对方程组：\n\n$$\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}\n$$\n\n<!--more-->\n\n## 行图像\n\n-\t行图像为：\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t也可以写成\n\n$$\nAx=b\n$$\n\n-\t即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png)\n\n## 列图像\n-\t列图像为：\n$$\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png)\n\n## 矩阵\n\n-\t现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？\n-\t如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆\n\n# <font size=5 >第二讲：消元、回代和置换 \n\n## 消元\n\n-\t考虑方程组\n$$\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}\n$$\n-\t他的A矩阵为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t经过行变换后为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再变换为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}\n$$\n-\t这样一系列变换即消元\n-\t变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）\n-\t矩阵\n$$\n\\left[\\begin{array}{c|c}\n A & X \\\\\n\\end{array}\\right]\n$$\n-\t称为增广矩阵(Augmented matrix)。b做同样变换可以得到c\n\n## 回代\n-\t解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例\n-\t因为U为上三角矩阵，z很容易求得\n-\t将z代入第二行求得y\n-\t将z,y代入第一行求得x\n-\t这个过程即回代\n\n## 置换\n\n$$\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A\n$$\n-\t这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3\n\n同理\n$$\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}\n$$\n-\t这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3\n-\t可以推出，交换A两行的矩阵为\n\n$$\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A\n$$\n-\t交换A两列的矩阵为\n\n$$\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵\n-\t在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$\n-\t消元可写成\n$$\nE_{32}E_{31}E_{21}A=U\n$$\n\n# <font size=5 >第三讲：乘法和逆矩阵\n\n## 矩阵乘法\n\n-\t考虑矩阵乘法\n$$\nA*B=C\n$$\n-\t第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$\n-\t第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列\n- \t第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行\n-\t第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C\n-\t第五种算法：矩阵分块算\n\n## 逆矩阵\n-\t对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵\n-\t对方阵，左逆矩阵与右逆矩阵相同\n-\t若存在非零矩阵X,使得$AX=0$,则A不可逆\n-\t求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵\n -\t证明：\n$$\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\\n$$\n\n# <font size=5 >第四讲：A的LU分解\n\n## LU分解\n-\t$(AB)^{-1}=B^{-1}A^{-1}$\n-\t对A的转置矩阵$A^T$,易得\n$$\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\\n$$\n-\t对单个矩阵而言，转置和求逆可以互换\n-\t矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例\n$$\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\\n$$\n-\t为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数\n\n## 消元消耗\n-\t记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为\n$$\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3\n$$\n\n## 群\n-\t以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)\n-\t对这些矩阵，$P^{-1}=P^T$\n-\t这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群\n-\tn*n矩阵共有n!个行置换矩阵\n\n# 第五讲：转置、置换、向量空间R\n\n## 置换\n-\t置换矩阵是用来完成行交换的矩阵\n-\tA=LU,L对角线上都是1，下方为消元乘数，U下三角为0\n-\tPA=LU用于描述包含行交换的lu分解\n-\tP(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价\n\n## 转置\n-\t行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$\n-\t$(AB)^T=B^TA^T$\n-\t对称矩阵(symmetric),$A^T=A$\n-\t对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$\n\n\n## 向量空间\n-\t向量可以相加减，点乘\n-\t**空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件**\n-\t例如$R^2$，代表所有实数的二维向量空间\n-\t向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)\n-\t不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的\n-\t在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)\n-\t$R^2$的子空间都有哪些？\n -\t$R^2$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t(0,0),简写为Z\n-\t$R^3$的子空间都有哪些？\n -\t$R^3$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t过零点的无限大平面\n -\t(0,0,0)\n \n\n\n## 通过矩阵构造向量子空间\n$$\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png)\n\n# <font size=5 >第六讲：列空间和零空间 \n\n## 列空间\n-\t上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间\n-\t对任意子空间S、T,$S \\bigcap T$是一个子空间\n\n-\t举个栗子\n$$\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n$$\n-\tC(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间\n-\t下面将子空间与线性方程组联系起来\n-\t现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？\n\t-\t前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，**即列空间无法填充整个四维空间**\n\t-\t后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，**等同于只有b在A的列空间内，x有解**\n-\t如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间\n\n## 零空间\n-\t零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x\n-\t列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n$$\n-\t显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间\n-\t为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：......矩阵乘法可以展开......分配率......\n\n\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}\n$$\n-\t我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？\n-\t显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0\n-\t列空间和零空间是两种构造子空间的方法\n\t-\t从几个向量通过线性组合来得到子空间\n\t-\t从一个方程组，通过让x满足特定条件来得到子空间\n\n# <font size=5 >第七讲：主变量、特解\n\n## 主变量\n-\t如何用算法解Ax=0\n-\t举个栗子:\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}\n$$\n-\t第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来\n-\t消元不改变方程的组，因为消元改动列空间,不改动解空间\n-\t第一次消元之后,第一列只有第一行的主元不为零\n\n\n\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}\n$$\n-\t此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U\n$$\n-\t如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)\n-\t现在我们可以解Ux=0,并进行回代\n-\t自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的**主变量**x1,x3可以通过回代解出\n\n## 特解\n-\t在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)\n-\t所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。\n-\t两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为**特解**，根据特解我们可以得到解空间：两组特解的线性组合,a\\*(-2,1,0,0)+b\\*(2,0,-2,1)\n-\t 秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量\n\n## 简化行阶梯形式\n-\tU还能进一步简化 \n$$\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t而且需将主元化为1,因为b=0,所以第二行可以直接除以2\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R\n$$\n-\t简化行阶梯形式以最简形式包含了矩阵的所有信息\n-\t单位矩阵位于主行与主列交汇处\n-\t最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列\n$$\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n$$\n其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列\n\n## 零空间矩阵\n\n-\t零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解\n$$\nR*N=0\n$$\n$$\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}\n$$\n-\t整个方程可以写成\n$$\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0\n$$\n$$\nx_{pivot}=-F\n$$\n\n## 最后举个栗子过一遍算法\n-\t原矩阵\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}\n$$\n-\t第一遍消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}\n$$\n-\t第二遍消元(进行一次行交换使得第二个主元在第二行)\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U\n$$\n-\t显然r=2,1个自由变量,令自由变量为1，得到特解x\n$$\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n-\t零空间就是cx,一条直线，这个x为零空间的基\n-\t接下来继续化简U\n$$\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}\n$$\n\n$$\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U\n$$\n\n$$\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N\n$$\n\n# <font size=5 >第八讲：可解性与解的结构\n\n## 可解性\n$$\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}\n$$\n-\t写成增广矩阵形式：\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]\n$$\n-\t消元得到:\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]\n$$\n-\t第一列和第三列为主列，第二列和第四列是自由列\n-\t可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里\n-\t**如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零**\n-\t如何求Ax=b的所有解？\n -\t第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5\n -\t第二步：完整的解为一个特解加上零空间中任意向量\n -\t$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$\n -\t在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)\n -\t完整解为：\n$$\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n -\t其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点\n \n## 解的结构\n-\t现在考虑秩为r的m*n矩阵，r<=m，r<=n ，r取满秩时的情况,r=min(m,n)\n-\t列满秩：r=n<m，此时没有自由变量 ，**N(A)={0}**,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n$$\n-\t行满秩：r=m<n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI & F \\\\\n\t\\end{bmatrix}\n$$\n-\tr=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一\n\n## 一个网友从向量空间角度的解释\n\n>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。\n>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）\n>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）\n>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。\n\n\n","source":"_posts/LinearAlgebra1.md","raw":"---\ntitle: MIT线性代数笔记1\ndate: 2017-01-21 11:45:28\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# <font size=5 >第一讲：方程组的几何解释\n\n-\t从3个角度看待方程组：行图形，列图像，矩阵\n\n-\t例如对方程组：\n\n$$\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}\n$$\n\n<!--more-->\n\n## 行图像\n\n-\t行图像为：\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t也可以写成\n\n$$\nAx=b\n$$\n\n-\t即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png)\n\n## 列图像\n-\t列图像为：\n$$\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n-\t方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png)\n\n## 矩阵\n\n-\t现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？\n-\t如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆\n\n# <font size=5 >第二讲：消元、回代和置换 \n\n## 消元\n\n-\t考虑方程组\n$$\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}\n$$\n-\t他的A矩阵为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t经过行变换后为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再变换为\n$$\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}\n$$\n-\t这样一系列变换即消元\n-\t变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）\n-\t矩阵\n$$\n\\left[\\begin{array}{c|c}\n A & X \\\\\n\\end{array}\\right]\n$$\n-\t称为增广矩阵(Augmented matrix)。b做同样变换可以得到c\n\n## 回代\n-\t解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例\n-\t因为U为上三角矩阵，z很容易求得\n-\t将z代入第二行求得y\n-\t将z,y代入第一行求得x\n-\t这个过程即回代\n\n## 置换\n\n$$\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A\n$$\n-\t这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3\n\n同理\n$$\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}\n$$\n-\t这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3\n-\t可以推出，交换A两行的矩阵为\n\n$$\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A\n$$\n-\t交换A两列的矩阵为\n\n$$\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵\n-\t在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$\n-\t消元可写成\n$$\nE_{32}E_{31}E_{21}A=U\n$$\n\n# <font size=5 >第三讲：乘法和逆矩阵\n\n## 矩阵乘法\n\n-\t考虑矩阵乘法\n$$\nA*B=C\n$$\n-\t第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$\n-\t第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列\n- \t第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行\n-\t第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C\n-\t第五种算法：矩阵分块算\n\n## 逆矩阵\n-\t对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵\n-\t对方阵，左逆矩阵与右逆矩阵相同\n-\t若存在非零矩阵X,使得$AX=0$,则A不可逆\n-\t求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵\n -\t证明：\n$$\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\\n$$\n\n# <font size=5 >第四讲：A的LU分解\n\n## LU分解\n-\t$(AB)^{-1}=B^{-1}A^{-1}$\n-\t对A的转置矩阵$A^T$,易得\n$$\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\\n$$\n-\t对单个矩阵而言，转置和求逆可以互换\n-\t矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例\n$$\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\\n$$\n-\t为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数\n\n## 消元消耗\n-\t记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为\n$$\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3\n$$\n\n## 群\n-\t以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)\n-\t对这些矩阵，$P^{-1}=P^T$\n-\t这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群\n-\tn*n矩阵共有n!个行置换矩阵\n\n# 第五讲：转置、置换、向量空间R\n\n## 置换\n-\t置换矩阵是用来完成行交换的矩阵\n-\tA=LU,L对角线上都是1，下方为消元乘数，U下三角为0\n-\tPA=LU用于描述包含行交换的lu分解\n-\tP(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价\n\n## 转置\n-\t行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$\n-\t$(AB)^T=B^TA^T$\n-\t对称矩阵(symmetric),$A^T=A$\n-\t对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$\n\n\n## 向量空间\n-\t向量可以相加减，点乘\n-\t**空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件**\n-\t例如$R^2$，代表所有实数的二维向量空间\n-\t向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)\n-\t不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的\n-\t在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)\n-\t$R^2$的子空间都有哪些？\n -\t$R^2$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t(0,0),简写为Z\n-\t$R^3$的子空间都有哪些？\n -\t$R^3$本身\n -\t过零点两端无限延伸的直线(注意这和$R^1$不同)\n -\t过零点的无限大平面\n -\t(0,0,0)\n \n\n\n## 通过矩阵构造向量子空间\n$$\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png)\n\n# <font size=5 >第六讲：列空间和零空间 \n\n## 列空间\n-\t上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间\n-\t对任意子空间S、T,$S \\bigcap T$是一个子空间\n\n-\t举个栗子\n$$\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n$$\n-\tC(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间\n-\t下面将子空间与线性方程组联系起来\n-\t现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？\n\t-\t前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，**即列空间无法填充整个四维空间**\n\t-\t后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，**等同于只有b在A的列空间内，x有解**\n-\t如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间\n\n## 零空间\n-\t零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x\n-\t列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n$$\n-\t显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间\n-\t为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：......矩阵乘法可以展开......分配率......\n\n\n$$\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}\n$$\n-\t我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？\n-\t显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0\n-\t列空间和零空间是两种构造子空间的方法\n\t-\t从几个向量通过线性组合来得到子空间\n\t-\t从一个方程组，通过让x满足特定条件来得到子空间\n\n# <font size=5 >第七讲：主变量、特解\n\n## 主变量\n-\t如何用算法解Ax=0\n-\t举个栗子:\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}\n$$\n-\t第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来\n-\t消元不改变方程的组，因为消元改动列空间,不改动解空间\n-\t第一次消元之后,第一列只有第一行的主元不为零\n\n\n\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}\n$$\n-\t此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U\n$$\n-\t如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)\n-\t现在我们可以解Ux=0,并进行回代\n-\t自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的**主变量**x1,x3可以通过回代解出\n\n## 特解\n-\t在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)\n-\t所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。\n-\t两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为**特解**，根据特解我们可以得到解空间：两组特解的线性组合,a\\*(-2,1,0,0)+b\\*(2,0,-2,1)\n-\t 秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量\n\n## 简化行阶梯形式\n-\tU还能进一步简化 \n$$\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t而且需将主元化为1,因为b=0,所以第二行可以直接除以2\n$$\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R\n$$\n-\t简化行阶梯形式以最简形式包含了矩阵的所有信息\n-\t单位矩阵位于主行与主列交汇处\n-\t最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列\n$$\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n$$\n其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列\n\n## 零空间矩阵\n\n-\t零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解\n$$\nR*N=0\n$$\n$$\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}\n$$\n-\t整个方程可以写成\n$$\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0\n$$\n$$\nx_{pivot}=-F\n$$\n\n## 最后举个栗子过一遍算法\n-\t原矩阵\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}\n$$\n-\t第一遍消元\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}\n$$\n-\t第二遍消元(进行一次行交换使得第二个主元在第二行)\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U\n$$\n-\t显然r=2,1个自由变量,令自由变量为1，得到特解x\n$$\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n-\t零空间就是cx,一条直线，这个x为零空间的基\n-\t接下来继续化简U\n$$\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}\n$$\n\n$$\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U\n$$\n\n$$\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N\n$$\n\n# <font size=5 >第八讲：可解性与解的结构\n\n## 可解性\n$$\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}\n$$\n-\t写成增广矩阵形式：\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]\n$$\n-\t消元得到:\n$$\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]\n$$\n-\t第一列和第三列为主列，第二列和第四列是自由列\n-\t可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里\n-\t**如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零**\n-\t如何求Ax=b的所有解？\n -\t第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5\n -\t第二步：完整的解为一个特解加上零空间中任意向量\n -\t$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$\n -\t在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)\n -\t完整解为：\n$$\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}\n$$\n -\t其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点\n \n## 解的结构\n-\t现在考虑秩为r的m*n矩阵，r<=m，r<=n ，r取满秩时的情况,r=min(m,n)\n-\t列满秩：r=n<m，此时没有自由变量 ，**N(A)={0}**,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n$$\n-\t行满秩：r=m<n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为\n$$\n\tR=\\begin{bmatrix}\n\tI & F \\\\\n\t\\end{bmatrix}\n$$\n-\tr=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一\n\n## 一个网友从向量空间角度的解释\n\n>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。\n>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）\n>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）\n>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。\n\n\n","slug":"LinearAlgebra1","published":1,"updated":"2019-07-22T03:45:22.758Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v98003xq8t5d07qr5ez","content":"<hr>\n<h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul>\n<li><p>从3个角度看待方程组：行图形，列图像，矩阵</p>\n</li>\n<li><p>例如对方程组：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}</script><a id=\"more\"></a>\n<h2 id=\"行图像\"><a href=\"#行图像\" class=\"headerlink\" title=\"行图像\"></a>行图像</h2><ul>\n<li>行图像为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}</script><ul>\n<li>也可以写成</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nAx=b</script><ul>\n<li>即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点</li>\n</ul>\n<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png\" alt=\"mark\"></p>\n<h2 id=\"列图像\"><a href=\"#列图像\" class=\"headerlink\" title=\"列图像\"></a>列图像</h2><ul>\n<li><p>列图像为：</p>\n<script type=\"math/tex; mode=display\">\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}</script></li>\n<li><p>方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png\" alt=\"mark\"></p>\n</li>\n</ul>\n<h2 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h2><ul>\n<li>现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？</li>\n<li>如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆</li>\n</ul>\n<h1 id=\"第二讲：消元、回代和置换\"><a href=\"#第二讲：消元、回代和置换\" class=\"headerlink\" title=\"第二讲：消元、回代和置换\"></a><font size=\"5\">第二讲：消元、回代和置换</font></h1><h2 id=\"消元\"><a href=\"#消元\" class=\"headerlink\" title=\"消元\"></a>消元</h2><ul>\n<li>考虑方程组<script type=\"math/tex; mode=display\">\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}</script></li>\n<li>他的A矩阵为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>经过行变换后为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>再变换为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}</script></li>\n<li>这样一系列变换即消元</li>\n<li>变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）</li>\n<li>矩阵<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c|c}\nA & X \\\\\n\\end{array}\\right]</script></li>\n<li>称为增广矩阵(Augmented matrix)。b做同样变换可以得到c</li>\n</ul>\n<h2 id=\"回代\"><a href=\"#回代\" class=\"headerlink\" title=\"回代\"></a>回代</h2><ul>\n<li>解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例</li>\n<li>因为U为上三角矩阵，z很容易求得</li>\n<li>将z代入第二行求得y</li>\n<li>将z,y代入第一行求得x</li>\n<li>这个过程即回代</li>\n</ul>\n<h2 id=\"置换\"><a href=\"#置换\" class=\"headerlink\" title=\"置换\"></a>置换</h2><script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A</script><ul>\n<li>这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3</li>\n</ul>\n<p>同理</p>\n<script type=\"math/tex; mode=display\">\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}</script><ul>\n<li>这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3</li>\n<li>可以推出，交换A两行的矩阵为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A</script><ul>\n<li>交换A两列的矩阵为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}</script><ul>\n<li>与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵</li>\n<li>在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$</li>\n<li>消元可写成<script type=\"math/tex; mode=display\">\nE_{32}E_{31}E_{21}A=U</script></li>\n</ul>\n<h1 id=\"第三讲：乘法和逆矩阵\"><a href=\"#第三讲：乘法和逆矩阵\" class=\"headerlink\" title=\"第三讲：乘法和逆矩阵\"></a><font size=\"5\">第三讲：乘法和逆矩阵</font></h1><h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><ul>\n<li>考虑矩阵乘法<script type=\"math/tex; mode=display\">\nA*B=C</script></li>\n<li>第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$</li>\n<li>第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列</li>\n<li>第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行</li>\n<li>第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C</li>\n<li>第五种算法：矩阵分块算</li>\n</ul>\n<h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul>\n<li>对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵</li>\n<li>对方阵，左逆矩阵与右逆矩阵相同</li>\n<li>若存在非零矩阵X,使得$AX=0$,则A不可逆</li>\n<li>求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵<ul>\n<li>证明：<script type=\"math/tex; mode=display\">\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第四讲：A的LU分解\"><a href=\"#第四讲：A的LU分解\" class=\"headerlink\" title=\"第四讲：A的LU分解\"></a><font size=\"5\">第四讲：A的LU分解</font></h1><h2 id=\"LU分解\"><a href=\"#LU分解\" class=\"headerlink\" title=\"LU分解\"></a>LU分解</h2><ul>\n<li>$(AB)^{-1}=B^{-1}A^{-1}$</li>\n<li>对A的转置矩阵$A^T$,易得<script type=\"math/tex; mode=display\">\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\</script></li>\n<li>对单个矩阵而言，转置和求逆可以互换</li>\n<li>矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例<script type=\"math/tex; mode=display\">\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\</script></li>\n<li>为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数</li>\n</ul>\n<h2 id=\"消元消耗\"><a href=\"#消元消耗\" class=\"headerlink\" title=\"消元消耗\"></a>消元消耗</h2><ul>\n<li>记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3</script></li>\n</ul>\n<h2 id=\"群\"><a href=\"#群\" class=\"headerlink\" title=\"群\"></a>群</h2><ul>\n<li>以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)</li>\n<li>对这些矩阵，$P^{-1}=P^T$</li>\n<li>这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群</li>\n<li>n*n矩阵共有n!个行置换矩阵</li>\n</ul>\n<h1 id=\"第五讲：转置、置换、向量空间R\"><a href=\"#第五讲：转置、置换、向量空间R\" class=\"headerlink\" title=\"第五讲：转置、置换、向量空间R\"></a>第五讲：转置、置换、向量空间R</h1><h2 id=\"置换-1\"><a href=\"#置换-1\" class=\"headerlink\" title=\"置换\"></a>置换</h2><ul>\n<li>置换矩阵是用来完成行交换的矩阵</li>\n<li>A=LU,L对角线上都是1，下方为消元乘数，U下三角为0</li>\n<li>PA=LU用于描述包含行交换的lu分解</li>\n<li>P(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价</li>\n</ul>\n<h2 id=\"转置\"><a href=\"#转置\" class=\"headerlink\" title=\"转置\"></a>转置</h2><ul>\n<li>行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$</li>\n<li>$(AB)^T=B^TA^T$</li>\n<li>对称矩阵(symmetric),$A^T=A$</li>\n<li>对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$</li>\n</ul>\n<h2 id=\"向量空间\"><a href=\"#向量空间\" class=\"headerlink\" title=\"向量空间\"></a>向量空间</h2><ul>\n<li>向量可以相加减，点乘</li>\n<li><strong>空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件</strong></li>\n<li>例如$R^2$，代表所有实数的二维向量空间</li>\n<li>向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)</li>\n<li>不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的</li>\n<li>在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)</li>\n<li>$R^2$的子空间都有哪些？<ul>\n<li>$R^2$本身</li>\n<li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li>\n<li>(0,0),简写为Z</li>\n</ul>\n</li>\n<li>$R^3$的子空间都有哪些？<ul>\n<li>$R^3$本身</li>\n<li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li>\n<li>过零点的无限大平面</li>\n<li>(0,0,0)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"通过矩阵构造向量子空间\"><a href=\"#通过矩阵构造向量子空间\" class=\"headerlink\" title=\"通过矩阵构造向量子空间\"></a>通过矩阵构造向量子空间</h2><script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}</script><ul>\n<li>各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png\" alt=\"mark\"></li>\n</ul>\n<h1 id=\"第六讲：列空间和零空间\"><a href=\"#第六讲：列空间和零空间\" class=\"headerlink\" title=\"第六讲：列空间和零空间\"></a><font size=\"5\">第六讲：列空间和零空间</font></h1><h2 id=\"列空间\"><a href=\"#列空间\" class=\"headerlink\" title=\"列空间\"></a>列空间</h2><ul>\n<li>上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间</li>\n<li><p>对任意子空间S、T,$S \\bigcap T$是一个子空间</p>\n</li>\n<li><p>举个栗子</p>\n<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}</script></li>\n<li>C(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间</li>\n<li>下面将子空间与线性方程组联系起来</li>\n<li>现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？<ul>\n<li>前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，<strong>即列空间无法填充整个四维空间</strong></li>\n<li>后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，<strong>等同于只有b在A的列空间内，x有解</strong></li>\n</ul>\n</li>\n<li>如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间</li>\n</ul>\n<h2 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h2><ul>\n<li>零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x</li>\n<li>列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n<li>显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间</li>\n<li>为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：……矩阵乘法可以展开……分配率……</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}</script><ul>\n<li>我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？</li>\n<li>显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0</li>\n<li>列空间和零空间是两种构造子空间的方法<ul>\n<li>从几个向量通过线性组合来得到子空间</li>\n<li>从一个方程组，通过让x满足特定条件来得到子空间</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第七讲：主变量、特解\"><a href=\"#第七讲：主变量、特解\" class=\"headerlink\" title=\"第七讲：主变量、特解\"></a><font size=\"5\">第七讲：主变量、特解</font></h1><h2 id=\"主变量\"><a href=\"#主变量\" class=\"headerlink\" title=\"主变量\"></a>主变量</h2><ul>\n<li>如何用算法解Ax=0</li>\n<li>举个栗子:<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}</script></li>\n<li>第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来</li>\n<li>消元不改变方程的组，因为消元改动列空间,不改动解空间</li>\n<li>第一次消元之后,第一列只有第一行的主元不为零</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}</script><ul>\n<li>此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U</script></li>\n<li>如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)</li>\n<li>现在我们可以解Ux=0,并进行回代</li>\n<li>自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的<strong>主变量</strong>x1,x3可以通过回代解出</li>\n</ul>\n<h2 id=\"特解\"><a href=\"#特解\" class=\"headerlink\" title=\"特解\"></a>特解</h2><ul>\n<li>在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)</li>\n<li>所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。</li>\n<li>两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为<strong>特解</strong>，根据特解我们可以得到解空间：两组特解的线性组合,a*(-2,1,0,0)+b*(2,0,-2,1)</li>\n<li>秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量</li>\n</ul>\n<h2 id=\"简化行阶梯形式\"><a href=\"#简化行阶梯形式\" class=\"headerlink\" title=\"简化行阶梯形式\"></a>简化行阶梯形式</h2><ul>\n<li>U还能进一步简化 <script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>而且需将主元化为1,因为b=0,所以第二行可以直接除以2<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R</script></li>\n<li>简化行阶梯形式以最简形式包含了矩阵的所有信息</li>\n<li>单位矩阵位于主行与主列交汇处</li>\n<li>最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}</script>其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列</li>\n</ul>\n<h2 id=\"零空间矩阵\"><a href=\"#零空间矩阵\" class=\"headerlink\" title=\"零空间矩阵\"></a>零空间矩阵</h2><ul>\n<li>零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解<script type=\"math/tex; mode=display\">\nR*N=0</script><script type=\"math/tex; mode=display\">\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}</script></li>\n<li>整个方程可以写成<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0</script><script type=\"math/tex; mode=display\">\nx_{pivot}=-F</script></li>\n</ul>\n<h2 id=\"最后举个栗子过一遍算法\"><a href=\"#最后举个栗子过一遍算法\" class=\"headerlink\" title=\"最后举个栗子过一遍算法\"></a>最后举个栗子过一遍算法</h2><ul>\n<li>原矩阵<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}</script></li>\n<li>第一遍消元<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}</script></li>\n<li>第二遍消元(进行一次行交换使得第二个主元在第二行)<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U</script></li>\n<li>显然r=2,1个自由变量,令自由变量为1，得到特解x<script type=\"math/tex; mode=display\">\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>零空间就是cx,一条直线，这个x为零空间的基</li>\n<li>接下来继续化简U<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}</script></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U</script><script type=\"math/tex; mode=display\">\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N</script><h1 id=\"第八讲：可解性与解的结构\"><a href=\"#第八讲：可解性与解的结构\" class=\"headerlink\" title=\"第八讲：可解性与解的结构\"></a><font size=\"5\">第八讲：可解性与解的结构</font></h1><h2 id=\"可解性\"><a href=\"#可解性\" class=\"headerlink\" title=\"可解性\"></a>可解性</h2><script type=\"math/tex; mode=display\">\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}</script><ul>\n<li>写成增广矩阵形式：<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]</script></li>\n<li>消元得到:<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]</script></li>\n<li>第一列和第三列为主列，第二列和第四列是自由列</li>\n<li>可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里</li>\n<li><strong>如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零</strong></li>\n<li>如何求Ax=b的所有解？<ul>\n<li>第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5</li>\n<li>第二步：完整的解为一个特解加上零空间中任意向量</li>\n<li>$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$</li>\n<li>在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)</li>\n<li>完整解为：<script type=\"math/tex; mode=display\">\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"解的结构\"><a href=\"#解的结构\" class=\"headerlink\" title=\"解的结构\"></a>解的结构</h2><ul>\n<li>现在考虑秩为r的m*n矩阵，r&lt;=m，r&lt;=n ，r取满秩时的情况,r=min(m,n)</li>\n<li>列满秩：r=n&lt;m，此时没有自由变量 ，<strong>N(A)={0}</strong>,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n<li>行满秩：r=m&lt;n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}</script></li>\n<li>r=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一</li>\n</ul>\n<h2 id=\"一个网友从向量空间角度的解释\"><a href=\"#一个网友从向量空间角度的解释\" class=\"headerlink\" title=\"一个网友从向量空间角度的解释\"></a>一个网友从向量空间角度的解释</h2><blockquote>\n<p>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。<br>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）<br>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）<br>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<hr>\n<h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul>\n<li><p>从3个角度看待方程组：行图形，列图像，矩阵</p>\n</li>\n<li><p>例如对方程组：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}</script>","more":"<h2 id=\"行图像\"><a href=\"#行图像\" class=\"headerlink\" title=\"行图像\"></a>行图像</h2><ul>\n<li>行图像为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}</script><ul>\n<li>也可以写成</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nAx=b</script><ul>\n<li>即二维平面上两条直线的交点为方程的解，推广到n维就是n维平面上n条直线的交点</li>\n</ul>\n<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png\" alt=\"mark\"></p>\n<h2 id=\"列图像\"><a href=\"#列图像\" class=\"headerlink\" title=\"列图像\"></a>列图像</h2><ul>\n<li><p>列图像为：</p>\n<script type=\"math/tex; mode=display\">\nx\n\\begin{bmatrix}\n2  \\\\\n-1 \\\\\n\\end{bmatrix}\n+y\n\\begin{bmatrix}\n-1 \\\\\n2 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n3 \\\\\n\\end{bmatrix}</script></li>\n<li><p>方程的解即向量组的线性组合系数，在这个组合系数下向量组合成目标向量<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123823434.png\" alt=\"mark\"></p>\n</li>\n</ul>\n<h2 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h2><ul>\n<li>现在考虑列图像，不同的x,y可以导致不同的线性组合，是否对任意b，x都有解？或者说这两个向量的线性组合能否覆盖整个空间？或者说这两(或n)个向量是否线性独立？</li>\n<li>如果是，那么这两个(或n个)向量组成的矩阵我们称之为非奇异矩阵(nonsingular matrix)，且可逆(invertible)；反之称之为奇异矩阵，不可逆</li>\n</ul>\n<h1 id=\"第二讲：消元、回代和置换\"><a href=\"#第二讲：消元、回代和置换\" class=\"headerlink\" title=\"第二讲：消元、回代和置换\"></a><font size=\"5\">第二讲：消元、回代和置换</font></h1><h2 id=\"消元\"><a href=\"#消元\" class=\"headerlink\" title=\"消元\"></a>消元</h2><ul>\n<li>考虑方程组<script type=\"math/tex; mode=display\">\\begin{cases}\nx+2y+z=2\\\\\n3x+8y+z=12\\\\\n4y+z=2\\\\\n\\end{cases}</script></li>\n<li>他的A矩阵为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n3 & 8 & 1  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>经过行变换后为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 4 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>再变换为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 1  \\\\\n0 & 2 & -2  \\\\\n0 & 0 & 5  \\\\\n\\end{bmatrix}</script></li>\n<li>这样一系列变换即消元</li>\n<li>变换的规律是，第i行的第i个元素设为主元(pivot)，通过行变换依次消除每一行主元前面的元素，这样矩阵A就变成了矩阵U(Upper Triangle上三角）</li>\n<li>矩阵<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c|c}\nA & X \\\\\n\\end{array}\\right]</script></li>\n<li>称为增广矩阵(Augmented matrix)。b做同样变换可以得到c</li>\n</ul>\n<h2 id=\"回代\"><a href=\"#回代\" class=\"headerlink\" title=\"回代\"></a>回代</h2><ul>\n<li>解方程Ax=b等同于解方程Ux=c，Ux=c非常容易求得解，以三元方程为例</li>\n<li>因为U为上三角矩阵，z很容易求得</li>\n<li>将z代入第二行求得y</li>\n<li>将z,y代入第一行求得x</li>\n<li>这个过程即回代</li>\n</ul>\n<h2 id=\"置换\"><a href=\"#置换\" class=\"headerlink\" title=\"置换\"></a>置换</h2><script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na & b & c  \\\\\n\\end{bmatrix}*A</script><ul>\n<li>这个式子的含义是求得一个行矩阵，其值为a倍A行1+b倍A行2+c倍A行3</li>\n</ul>\n<p>同理</p>\n<script type=\"math/tex; mode=display\">\nA*\\begin{bmatrix}\na \\\\\nb \\\\\nc \\\\\n\\end{bmatrix}</script><ul>\n<li>这个式子的含义是求得一个列矩阵，其值为a倍A列1+b倍A列2+c倍A列3</li>\n<li>可以推出，交换A两行的矩阵为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}*A</script><ul>\n<li>交换A两列的矩阵为</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA*\\begin{bmatrix}\n0 & 1  \\\\\n1 & 0  \\\\\n\\end{bmatrix}</script><ul>\n<li>与矩阵相乘完成了行列变换，这样的矩阵就是置换矩阵</li>\n<li>在消元中，把第i行第j列处元素消掉所需要的行列变换表示为置换矩阵，记作$E_{ij}$</li>\n<li>消元可写成<script type=\"math/tex; mode=display\">\nE_{32}E_{31}E_{21}A=U</script></li>\n</ul>\n<h1 id=\"第三讲：乘法和逆矩阵\"><a href=\"#第三讲：乘法和逆矩阵\" class=\"headerlink\" title=\"第三讲：乘法和逆矩阵\"></a><font size=\"5\">第三讲：乘法和逆矩阵</font></h1><h2 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h2><ul>\n<li>考虑矩阵乘法<script type=\"math/tex; mode=display\">\nA*B=C</script></li>\n<li>第一种算法：点乘 $C_{ij}=\\sum_iA_{ik}B_{kj}$</li>\n<li>第二种算法：看成矩阵乘向量，C列为A列的线性组合,组合系数在B矩阵中，例如：B的第一列中每行元素就是A中各个列的线性组合系数，线性组合之后得到C的第一列</li>\n<li>第三种算法：看成向量乘矩阵，C行为B行的线性组合,组合系数在A矩阵中，例如：A的第一行中每列元素就是B中各个行的线性组合系数，线性组合之后得到C的第一行</li>\n<li>第四种算法：A的某一列乘B的某一行得到一个子矩阵，所有子矩阵相加即为C</li>\n<li>第五种算法：矩阵分块算</li>\n</ul>\n<h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul>\n<li>对逆矩阵$A^{-1}$,有$AA^{-1}=I$,I为单位矩阵</li>\n<li>对方阵，左逆矩阵与右逆矩阵相同</li>\n<li>若存在非零矩阵X,使得$AX=0$,则A不可逆</li>\n<li>求逆矩阵的高斯若尔当思想:将A|I作为增广矩阵，将A变换到I时，I相应变换到A的逆矩阵<ul>\n<li>证明：<script type=\"math/tex; mode=display\">\nEA=I \\\\\nE=A^{-1} \\\\\nEI=A^{-1} \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第四讲：A的LU分解\"><a href=\"#第四讲：A的LU分解\" class=\"headerlink\" title=\"第四讲：A的LU分解\"></a><font size=\"5\">第四讲：A的LU分解</font></h1><h2 id=\"LU分解\"><a href=\"#LU分解\" class=\"headerlink\" title=\"LU分解\"></a>LU分解</h2><ul>\n<li>$(AB)^{-1}=B^{-1}A^{-1}$</li>\n<li>对A的转置矩阵$A^T$,易得<script type=\"math/tex; mode=display\">\nAA^{-1}=I \\\\\n(A^{-1})^TA^T=I \\\\\n所以(A^T)^{-1}=(A^{-1})^T \\\\</script></li>\n<li>对单个矩阵而言，转置和求逆可以互换</li>\n<li>矩阵分解：A=LU,即U通过一系列置换矩阵变回为A，L就是置换矩阵的累积.以3*3矩阵为例<script type=\"math/tex; mode=display\">\nE_{32}E_{31}E_{21}A=U \\\\\n所以可得L: \\\\\nL=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} \\\\</script></li>\n<li>为什么研究A=LU而不是EA=U：因为如果不存在行变换，消元系数可以直接写进L中，反之，如果研究E,第n行的运算与前面已经消元过的第n-1行运算相关，不能直观的写出消元系数</li>\n</ul>\n<h2 id=\"消元消耗\"><a href=\"#消元消耗\" class=\"headerlink\" title=\"消元消耗\"></a>消元消耗</h2><ul>\n<li>记消元中一次乘法加一次减法即消掉某一元素为一次消耗(是数的乘和减为单位而不是行的乘和减)，总消耗为<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^{n}i*(i-1) \\approx \\sum_{i=1}^{n}i^2 \\approx \\frac 13 n^3</script></li>\n</ul>\n<h2 id=\"群\"><a href=\"#群\" class=\"headerlink\" title=\"群\"></a>群</h2><ul>\n<li>以3*3单位置换矩阵为例，总共有6个(即行互换矩阵)</li>\n<li>对这些矩阵，$P^{-1}=P^T$</li>\n<li>这6个矩阵的置换和逆依然在这6个矩阵之中，称之为群</li>\n<li>n*n矩阵共有n!个行置换矩阵</li>\n</ul>\n<h1 id=\"第五讲：转置、置换、向量空间R\"><a href=\"#第五讲：转置、置换、向量空间R\" class=\"headerlink\" title=\"第五讲：转置、置换、向量空间R\"></a>第五讲：转置、置换、向量空间R</h1><h2 id=\"置换-1\"><a href=\"#置换-1\" class=\"headerlink\" title=\"置换\"></a>置换</h2><ul>\n<li>置换矩阵是用来完成行交换的矩阵</li>\n<li>A=LU,L对角线上都是1，下方为消元乘数，U下三角为0</li>\n<li>PA=LU用于描述包含行交换的lu分解</li>\n<li>P(Permutation置换矩阵)是行重新排列了的单位矩阵，n*n置换矩阵共n!种，即各行重新排列后的数目，他们均可逆，且求逆与求转置等价</li>\n</ul>\n<h2 id=\"转置\"><a href=\"#转置\" class=\"headerlink\" title=\"转置\"></a>转置</h2><ul>\n<li>行列交换即转置，记作$A^T$，$A_{ij}=A_{ji}^T$</li>\n<li>$(AB)^T=B^TA^T$</li>\n<li>对称矩阵(symmetric),$A^T=A$</li>\n<li>对任意矩阵A，$AA^T$总是对称的,因为$(A^TA)^T=(A^TA^{TT})=(A^TA)$</li>\n</ul>\n<h2 id=\"向量空间\"><a href=\"#向量空间\" class=\"headerlink\" title=\"向量空间\"></a>向量空间</h2><ul>\n<li>向量可以相加减，点乘</li>\n<li><strong>空间代表一些向量的集合，不代表所有向量，向量空间是有约束条件的，需要满足对线性组合自封闭的条件</strong></li>\n<li>例如$R^2$，代表所有实数的二维向量空间</li>\n<li>向量空间内的任何向量进行线性组合后依然在向量空间内，所以$R^2$向量空间内必须存在(0,0)</li>\n<li>不是向量空间的一个例子：只取$R^2$的第一象限，任意空间内的向量相加依然在空间内，但数乘就不一定(可以乘以一个负数),向量空间是封闭的</li>\n<li>在$R^2$内取一条过零点直线可以称为$R^2$的向量子空间，这个子空间依然满足自封闭性(加减和数乘)</li>\n<li>$R^2$的子空间都有哪些？<ul>\n<li>$R^2$本身</li>\n<li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li>\n<li>(0,0),简写为Z</li>\n</ul>\n</li>\n<li>$R^3$的子空间都有哪些？<ul>\n<li>$R^3$本身</li>\n<li>过零点两端无限延伸的直线(注意这和$R^1$不同)</li>\n<li>过零点的无限大平面</li>\n<li>(0,0,0)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"通过矩阵构造向量子空间\"><a href=\"#通过矩阵构造向量子空间\" class=\"headerlink\" title=\"通过矩阵构造向量子空间\"></a>通过矩阵构造向量子空间</h2><script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 3  \\\\\n2 & 3  \\\\\n4 & 1  \\\\\n\\end{bmatrix}</script><ul>\n<li>各列属于$R^3$，这两列的任何线性组合(数乘和加法)应该在子空间中,称这个子空间为列空间,记作C(A)，在三维空间中这个列空间就是一个平面，过这两个列向量及(0,0,0)<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170122/203313042.png\" alt=\"mark\"></li>\n</ul>\n<h1 id=\"第六讲：列空间和零空间\"><a href=\"#第六讲：列空间和零空间\" class=\"headerlink\" title=\"第六讲：列空间和零空间\"></a><font size=\"5\">第六讲：列空间和零空间</font></h1><h2 id=\"列空间\"><a href=\"#列空间\" class=\"headerlink\" title=\"列空间\"></a>列空间</h2><ul>\n<li>上一讲提到两种子空间，平面P和直线L。$P \\bigcup L$不是一个子空间，$P \\bigcap L$是一个子空间</li>\n<li><p>对任意子空间S、T,$S \\bigcap T$是一个子空间</p>\n</li>\n<li><p>举个栗子</p>\n<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}</script></li>\n<li>C(A)是$R^4$子空间，将这三个列向量做线性组合可以得到子空间</li>\n<li>下面将子空间与线性方程组联系起来</li>\n<li>现提出两个问题：Ax=b对任意b是否都有解?b怎样才能使x有解？<ul>\n<li>前者回答是，否，因为四个方程，三个未知数，等同于3个列向量的线性组合无法填充整个$R^4$空间，<strong>即列空间无法填充整个四维空间</strong></li>\n<li>后者回答，显然b=(0,0,0,0)是一个答案，b=(1,2,3,4)显然也是一个答案,即先写出任意解(x1,x2,x3)，计算出的b就是使x有解的b，<strong>等同于只有b在A的列空间内，x有解</strong></li>\n</ul>\n</li>\n<li>如果我们去掉第三列，我们依然可以得到相同的列空间，因为这三列并不是线性无关，第三列是前两列之和，此时我们称前两列为主列,所以此栗中的列空间是一个二维子空间</li>\n</ul>\n<h2 id=\"零空间\"><a href=\"#零空间\" class=\"headerlink\" title=\"零空间\"></a>零空间</h2><ul>\n<li>零空间(null space)与列空间完全不同，A的零空间包含Ax=0的所有解x</li>\n<li>列空间关心A,零空间关心x(在b=0的情况下)，在上面那个栗子中，列空间是四维空间的子空间,零空间是三维空间的子空间<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n<li>显然零空间包含(0,0,0)，(1,1,-1),这两个向量确定一条直线(c,c,-c)，所以这条直线就是零空间</li>\n<li>为什么零空间可以称为空间(满足向量空间的自封闭性?):即证明对Ax=0的任意两个解，他们的线性组合依然是解。因为：……矩阵乘法可以展开……分配率……</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1 & 2  \\\\\n2 & 1 & 3  \\\\\n3 & 1 & 5  \\\\\n4 & 1 & 5  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n4 \\\\\n\\end{bmatrix}</script><ul>\n<li>我们更换了b，解为(1,0,0),有其他解吗，如果存在，这些解能构成子空间吗？</li>\n<li>显然不构成，因为解中不包含(0,0,0),不满足向量空间的基本条件，如本例，两个解(1,0,0),(0,-1,1),但这两个向量的线性组合不通过原点，无法组成向量空间。所以讨论解空间或者说零空间，前提是b=0</li>\n<li>列空间和零空间是两种构造子空间的方法<ul>\n<li>从几个向量通过线性组合来得到子空间</li>\n<li>从一个方程组，通过让x满足特定条件来得到子空间</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第七讲：主变量、特解\"><a href=\"#第七讲：主变量、特解\" class=\"headerlink\" title=\"第七讲：主变量、特解\"></a><font size=\"5\">第七讲：主变量、特解</font></h1><h2 id=\"主变量\"><a href=\"#主变量\" class=\"headerlink\" title=\"主变量\"></a>主变量</h2><ul>\n<li>如何用算法解Ax=0</li>\n<li>举个栗子:<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n2 & 4 & 6 & 8  \\\\\n3 & 6 & 8 & 10  \\\\\n\\end{bmatrix}</script></li>\n<li>第三行是第一行加第二行，他们线性相关，这将在之后的消元中体现出来</li>\n<li>消元不改变方程的组，因为消元改动列空间,不改动解空间</li>\n<li>第一次消元之后,第一列只有第一行的主元不为零</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 2 & 4  \\\\\n\\end{bmatrix}</script><ul>\n<li>此时因为第二列第三列线性相关，第二行的主元到了第三列,继续消元<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=U</script></li>\n<li>如果我们将非0元素和0分开，会得到一个阶梯线，阶梯数是主元(非0)数，在本例中是2，我们称之为矩阵的秩(消元后剩下几个方程)，主元所在的列叫主列(1,3)，其余的列是自由列(2,4)</li>\n<li>现在我们可以解Ux=0,并进行回代</li>\n<li>自由列所对应的解为自由变量x2,x4，可以任意选择，选定之后主列对应的<strong>主变量</strong>x1,x3可以通过回代解出</li>\n</ul>\n<h2 id=\"特解\"><a href=\"#特解\" class=\"headerlink\" title=\"特解\"></a>特解</h2><ul>\n<li>在本例中假如取x2=1,x4=0,可以得到x=(-2,1,0,0),而(-2,1,0,0)乘任意实数依然是解，这样就确定了一条直线，但这条直线是解(零)空间吗?不是。因为我们有两个自由变量，可以确定不止一条直线，例如取x2=0,x4=1,可以得到x=(2,0,-2,1)</li>\n<li>所以算法是先消元，得到主列和自由列，然后对自由变量分配数值(1,0)，完成整个解(-2,1,0,0),再对自由变量取另外一组值(0,1)，再得到一组完全解(2,0,-2,1)。</li>\n<li>两次对自由变量取特殊值(其中一个为1，剩下的都是0，不能全为0，那样得到的完整解也全为0)得到的两组解称为<strong>特解</strong>，根据特解我们可以得到解空间：两组特解的线性组合,a*(-2,1,0,0)+b*(2,0,-2,1)</li>\n<li>秩r代表主变量即主元的个数，只有r个方程起作用，m*n的A矩阵有n-r个自由变量</li>\n</ul>\n<h2 id=\"简化行阶梯形式\"><a href=\"#简化行阶梯形式\" class=\"headerlink\" title=\"简化行阶梯形式\"></a>简化行阶梯形式</h2><ul>\n<li>U还能进一步简化 <script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 2 & 2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>在简化行阶梯形式(reduced row echelon form RREF)中，主元上方也全是0<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 2 & 4  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>而且需将主元化为1,因为b=0,所以第二行可以直接除以2<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 2 & 0 & -2  \\\\\n0 & 0 & 1 & 2  \\\\\n0 & 0 & 0 & 0  \\\\\n\\end{bmatrix}=R</script></li>\n<li>简化行阶梯形式以最简形式包含了矩阵的所有信息</li>\n<li>单位矩阵位于主行与主列交汇处</li>\n<li>最终得到一个极简的方程组:Rx=0(列可以随便交换位置)，F代表自由列<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI & F \\\\\n0 & 0 \\\\\n\\end{bmatrix}</script>其中I为单位矩阵(主列)，F(自由列对应的矩阵),R有r行，I有r列，F有n-r列</li>\n</ul>\n<h2 id=\"零空间矩阵\"><a href=\"#零空间矩阵\" class=\"headerlink\" title=\"零空间矩阵\"></a>零空间矩阵</h2><ul>\n<li>零空间矩阵，它的各列由特解组成，记作N，可以看出若有a个自由变量，则N有a列，若无自由变量，则N不存在，x只有唯一解或无解<script type=\"math/tex; mode=display\">\nR*N=0</script><script type=\"math/tex; mode=display\">\nN=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}</script></li>\n<li>整个方程可以写成<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{pivot} \\\\\nx_{free}  \\\\\n\\end{bmatrix}=0</script><script type=\"math/tex; mode=display\">\nx_{pivot}=-F</script></li>\n</ul>\n<h2 id=\"最后举个栗子过一遍算法\"><a href=\"#最后举个栗子过一遍算法\" class=\"headerlink\" title=\"最后举个栗子过一遍算法\"></a>最后举个栗子过一遍算法</h2><ul>\n<li>原矩阵<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n2 & 6 & 8 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}</script></li>\n<li>第一遍消元<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 0 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 4 & 4 \\\\\n\\end{bmatrix}</script></li>\n<li>第二遍消元(进行一次行交换使得第二个主元在第二行)<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=U</script></li>\n<li>显然r=2,1个自由变量,令自由变量为1，得到特解x<script type=\"math/tex; mode=display\">\nx=\\begin{bmatrix}\n-1 \\\\\n-1 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>零空间就是cx,一条直线，这个x为零空间的基</li>\n<li>接下来继续化简U<script type=\"math/tex; mode=display\">\nU=\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}=R=\n\\begin{bmatrix}\nI & F  \\\\\n0 & 0  \\\\\n0 & 0  \\\\\n\\end{bmatrix}</script></li>\n</ul>\n<script type=\"math/tex; mode=display\">\nF=\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}=U</script><script type=\"math/tex; mode=display\">\nx=\\begin{bmatrix}\n-F \\\\\nI  \\\\\n\\end{bmatrix}=N</script><h1 id=\"第八讲：可解性与解的结构\"><a href=\"#第八讲：可解性与解的结构\" class=\"headerlink\" title=\"第八讲：可解性与解的结构\"></a><font size=\"5\">第八讲：可解性与解的结构</font></h1><h2 id=\"可解性\"><a href=\"#可解性\" class=\"headerlink\" title=\"可解性\"></a>可解性</h2><script type=\"math/tex; mode=display\">\\begin{cases}\nx_1+2x_2+2x_3+2x_4=b_1\\\\\n2x_1+4x_2+6x_3+8x_4=b_2\\\\\n3x_1+6x_2+8x_3+10x_4=b_3\\\\\n\\end{cases}</script><ul>\n<li>写成增广矩阵形式：<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n2 & 4 & 6 & 8 & b_2 \\\\\n3 & 6 & 8 & 10 & b_3 \\\\\n\\end{array}\\right]</script></li>\n<li>消元得到:<script type=\"math/tex; mode=display\">\n\\left[\\begin{array}{c c c c|c}\n1 & 2 & 2 & 2 & b_1 \\\\\n0 & 0 & 2 & 4 & b_2-2b_1 \\\\\n0 & 0 & 0 & 0 & b_3-b_2-b_1 \\\\\n\\end{array}\\right]</script></li>\n<li>第一列和第三列为主列，第二列和第四列是自由列</li>\n<li>可解性：有解时b需要满足的条件？易得条件为b必须在A的列空间里</li>\n<li><strong>如果A各行的线性组合得到0，b需要满足什么条件？那么b中元素的同样组合必然也是零</strong></li>\n<li>如何求Ax=b的所有解？<ul>\n<li>第一步：求一个特解，将所有自由变量设为0，求所有主变量，在例子中，$x_2和x_4$设为0，可以解得$x_1和x_3$分别为-2、1.5</li>\n<li>第二步：完整的解为一个特解加上零空间中任意向量</li>\n<li>$Ax_{particular}=b   \\\\   Ax_{nullspace}=0   \\\\   A(x_{particular}+x_{nullspace})=b$</li>\n<li>在此例中，特解为(-2,0,1.5,0),零空间中的解为(-2,1,0,0)和(2,0,-2,1)</li>\n<li>完整解为：<script type=\"math/tex; mode=display\">\nx_{complete}=\n\\begin{bmatrix}\n-2 \\\\\n0 \\\\\n1.5 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_1\\begin{bmatrix}\n-2 \\\\\n1\\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}+\nc_2\\begin{bmatrix}\n2 \\\\\n0 \\\\\n-2 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>其图像如果在以4个解为轴的四维空间中画出，是一个平面，类似于子空间从零点平移过特解点</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"解的结构\"><a href=\"#解的结构\" class=\"headerlink\" title=\"解的结构\"></a>解的结构</h2><ul>\n<li>现在考虑秩为r的m*n矩阵，r&lt;=m，r&lt;=n ，r取满秩时的情况,r=min(m,n)</li>\n<li>列满秩：r=n&lt;m，此时没有自由变量 ，<strong>N(A)={0}</strong>,Ax=b的解只有特解一个(b在列空间内)，或者无解。此时R的形式为<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n<li>行满秩：r=m&lt;n，此时消元时不会出现零行，对任意b，Ax=b有解，共有n-r即n-m个自由变量,此时r的形式为<script type=\"math/tex; mode=display\">\nR=\\begin{bmatrix}\nI & F \\\\\n\\end{bmatrix}</script></li>\n<li>r=m=n时，A为可逆矩阵，R=I，N(A)={0},Ax=b对任意b有解，解唯一</li>\n</ul>\n<h2 id=\"一个网友从向量空间角度的解释\"><a href=\"#一个网友从向量空间角度的解释\" class=\"headerlink\" title=\"一个网友从向量空间角度的解释\"></a>一个网友从向量空间角度的解释</h2><blockquote>\n<p>当向量所占的维数r等于向量的个数n又等于母空间m的维数的时候。这些向量就可以组合成母空间内任意的向量了，即无论b为何值一定有解，但由于必须要所有的向量共同组合才能到达整个母空间任意坐标点，所以每个向量的伸缩必须时特定的量，即x只有一组解。<br>当向量所占的维数r等于母空间m的维数的时候小于向量的个数n时，即A中的部分向量伸缩组合就可以到达母空间的任意坐标点。那么这里就存在着自由向量了，无论b取空间里的什么位置，你可以先随意伸缩你的自由向量得到一个新向量，然后通过那部分可以完全到达母空间的向量与这个新向量一起进过特定的收缩得到向量b。只要自由向量的伸缩量改变那么其它向量的收缩量也要跟着改变，那么X就有无穷多组解。（用x的表达公式来描述就是你可以用A中部分向量（m个主元向量）伸缩组合得到b(此为特解）并且再通过m个主元向量与另外n-m个自由向量随意组成0向量，就可以得到无穷多个x组了）<br>当向量所占维数等于向量的个数小于母空间的维数时，即A中的向量无论怎么伸缩组合只达到母空间中的一个子空间。那么当b在这个子空间时那么A通过特定的伸缩可以到达这一坐标点即X有一组解（这里由于没有自由向量所以没有多解的情况，不要存在b只占子空间部分维数留另外的给自由向量的想法，b在r的每个方向都有值，0也是值。就拿子空间为3维空间举例，如果b只在xy平面内，Z仍然需要进行收缩，缩为0，不是自由的）。如果b没在这一子空间内，那么无论A中向量如何收缩都不能得到即无解（同样拿三维举例，如果A中的向量只在xy平面那么如果b为（1 2 3）你如何收缩取得？）<br>当向量所占的维数小于向量的个数小于母空间的个数时，即A中的向量只能覆盖母空间的一个子空间但在这子空间有自由向量，那么如果b在这个子空间内那么情况和第二点相同，X有无穷多组解；如果b在子空间之外，X无论如何收缩都不能达到，无解。</p>\n</blockquote>","popularPost_tmp_postPath":true,"eyeCatchImage":"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"MIT线性代数笔记1","path":"2017/01/21/LinearAlgebra1/","eyeCatchImage":"http://ojtdnrpmt.bkt.clouddn.com/blog/20170121/123759373.png","excerpt":"<hr>\n<h1 id=\"第一讲：方程组的几何解释\"><a href=\"#第一讲：方程组的几何解释\" class=\"headerlink\" title=\"第一讲：方程组的几何解释\"></a><font size=\"5\">第一讲：方程组的几何解释</font></h1><ul>\n<li><p>从3个角度看待方程组：行图形，列图像，矩阵</p>\n</li>\n<li><p>例如对方程组：</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">\\begin{cases}\n2x-y=0\\\\\n-x+2y=3\\\\\n\\end{cases}</script>","date":"2017-01-21T03:45:28.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记3","date":"2017-01-22T11:21:02.000Z","mathjax":true,"html":true,"_content":"\n***\n# 第十七讲：行列式及其性质\n\n## 行列式\n-\t矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$\n-\t行列式的性质\n -\t$detI=1$\n -\t交换行，行列式的值的符号会相反\n -\t一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶\n -\t两行相等使得行列式为0(由性质二可以直接推出)\n -\t矩阵消元不改变其行列式(证明见下)\n -\t某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)\n -\t$detA=0$当且仅当A是奇异矩阵\n -\t$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$\n -\t$detA^{-1}detA=1$\n -\t$detA^2=(detA)^2$\n -\t$det2A=2^n detA$\n -\t$detA^T=detA$(证明见下)\n<!--more-->\n-\t行列式按行是线性的，但行列式本身不是线性的\n\t$$\n\t\\begin{vmatrix}\n\t1 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{vmatrix}=1 \\\\\n\t\\begin{vmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{vmatrix}=-1 \\\\\n\t\\begin{vmatrix}\n\tta & tb \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\tt\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix} \\\\\n\t\\begin{vmatrix}\n\tt+a & t+b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\tt & t \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明消元不改变行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc-la & d-lb \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}-l\n\t\\begin{vmatrix}\n\ta & b \\\\\n\ta & b \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明转置不改变行列式\n\t$$\n\tA=LU \\\\\n\t$$\n-\t即证 $|U^TL^T|=|LU|$\n$$\n|U^T||L^T|=|L||U|\n$$ \n-\t以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 \n\t\n\t\n## 三角阵行列式\n-\t对三角阵U的行列式,值为对角线上元素乘积(主元乘积)\n-\t为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式\n-\t为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3...d_nI$，其中单位矩阵的行列式为1\n-\t奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积\n\n## A little more\n-\t进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的\n\t\n# 第十八讲：行列式公式和代数余子式\n\n## 行列式公式\n-\t推导2*2行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\t0 & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\t0 & d \\\\\n\t\\end{vmatrix} \\\\\n\t=0+ad-bc+0\n\t$$\n\t我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案\n-\t如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。\n-\t例如\n\t$$\n\t\\begin{vmatrix}\n\ta & 0 & 0\\\\\n\t0 & 0 & b\\\\\n\t0 & c & 0\\\\\n\t\\end{vmatrix}\n\t$$\n\t先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$\n-\tn*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分\n-\t行列式公式就是这$n!$个部分加起来\n\n\n## 代数余子式\n-\t$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n-\t提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式\n-\t从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式\n-\t$a_{ij}$的代数余子式记作$c_{ij}$\n-\t注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号\n-\t$detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}$\t\n\n# 第十九讲：克拉默法则，逆矩阵，体积\n\n## 逆矩阵\n-\t只有行列式不为0时，矩阵才是可逆的\n-\t逆矩阵公式\n\t$$\n\tA^{-1}=\\frac{1}{detA}C^T\n\t$$\n\t其中$C_{ij}$是$A_{ij}$的代数余子式\n-\t证明：即证$AC^T=(detA)I$\n\t$$\n\t\\begin{bmatrix}\n\ta_{11} & ... & a_{1n} \\\\\n\ta_{n1} & ... & a_{nn} \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tc_{11} & ... & c_{n1} \\\\\n\tc_{1n} & ... & c_{nn} \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\tdetA & 0 & 0 \\\\\n\t0 & detA & 0 \\\\\n\t0 & 0 & detA \\\\\n\t\\end{bmatrix}\n\t$$\n\t对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n\t其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0\n\t\n## 克拉默法则\n-\t解Ax=b\n\t$$\n\tAx=b \\\\\n\tx=A^{-1}b \\\\\n\tx=\\frac{1}{detA}C^Tb \\\\\n\t \\\\\n\tx_1=\\frac{detB_1}{detA} \\\\\n\tx_3=\\frac{detB_2}{detA} \\\\\n\t... \\\\\n\t$$\n-\t克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变\n\n## 体积\n-\tA的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积\n-\t矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。\n-\t(1)单位矩阵对应单位立方体，体积为1\n-\t对正交矩阵Q,\n\t$$\n\tQQ^T=I \\\\\n\t|QQ^T|=|I| \\\\\n\t|Q||Q^T|=1 \\\\\n\t{|Q|}^2=1 \\\\\n\t|Q|=1 \\\\\n\t$$\n\tQ对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度\n-\t(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍\n-\t(2)交换矩阵两行，盒子的体积不变\n-\t(3b)矩阵某一行拆分，盒子也相应切分为两部分\n-\t以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证\n\t\n# 第二十讲：特征值和特征向量\n\n## 特征向量\n-\t给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax\n-\t当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值\n-\t如果A是奇异矩阵，$\\lambda = 0$是一个特征值\n\n## 几个例子\n-\t如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.\n-\t再举一例\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =1, x=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =-1, x=\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix} \\\\\t\n\t$$\n-\tn*n矩阵有n个特征值\n-\t特征值的和等于对角线元素和，这个和称为迹(trace)，\n-\t如何求解$Ax=\\lambda x$\n\t$$\n\t(A-\\lambda I)x=0 \\\\\n\t$$\n-\t可见方程有非零解，$(A-\\lambda I)$必须是奇异的 \n\t即: \n\t$$\n\tdet(A-\\lambda I)=0 \\\\\n\t$$\n-\t$$\n\tIf \\qquad Ax=\\lambda x \\\\\n\tThen \\qquad (A+3I)x=(\\lambda +3)x \\\\\n\t$$\n-\t因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$\n-\tA+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积\n-\t再举一例，对旋转矩阵Q\n\t$$\n\tQ=\n\t\\begin{bmatrix}\n\t0 & -1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\ttrace=0=\\lambda _1 +\\lambda _2 \\\\\n\tdet=1=\\lambda _1 \\lambda _2 \\\\\n\t$$\n-\t但是可以看出 $\\lambda _1，\\lambda _2$无实数解 \n-\t再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t3 & 1 \\\\\n\t0 & 3 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t3-\\lambda & 1 \\\\\n\t0 & 3-\\lambda \\\\\n\t\\end{vmatrix}\n\t==(3-\\lambda )^2=0 \\\\\n\t\\lambda _1=\\lambda _2=3 \\\\\n\tx_1=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t$$\n\t\n# 第二十一讲：对角化和A的幂\n\n## 对角化\n-\t假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵\n-\t以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下\n-\t$$\n\tAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n\t=[x_1,x_2,...x_n]\n\t\\begin{bmatrix}\n\t\\lambda _1 & 0 & ... & 0 \\\\\n\t0 & \\lambda _2 & ... & 0 \\\\\n\t... & ... & ... & ... \\\\\n\t0 & 0  & 0 & \\lambda _n \\\\\n\t\\end{bmatrix} \\\\\n\t=S \\Lambda \\\\\n\t$$\n\n\n-\t假设S可逆，即n个特征向量无关，此时可以得到\n\t$$\n\tS^{-1}AS=\\Lambda \\\\\n\tA=S\\Lambda S^{-1} \\\\\n\t$$\n-\t$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解\n-\t$$\n\tif \\qquad Ax=\\lambda x \\\\\n\tA^2 x=\\lambda AX=\\lambda ^2 x \\\\\n\tA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\\n\t$$\n-\t上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理\n-\t特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式\n-\t什么样的矩阵的幂趋向于0(稳定)\n\t$$\n\tA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\n\tif \\quad all |\\lambda _i|<1 \\\\ \n\t$$\n-\t哪些矩阵可以对角化？\n\t如果所有特征值不同，则A可以对角化\n-\t如果矩阵A已经是对角阵，则$\\Lambda$与A相同\n-\t特征值重复的次数称为代数重度，对三角阵，如\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t2 & 1 \\\\\n\t0 & 2 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t2-\\lambda & 1 \\\\\n\t0 & 2-\\lambda \\\\\n\t\\end{vmatrix}=0 \\\\\n\t\\lambda =2 \\\\\n\tA-\\lambda I=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t0 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2\n-\t特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。\n\n## A的幂\n-\t多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂\n-\t$$\n\tgive \\quad u_0 \\\\\n\tu_{k+1}=Au_k \\\\\n\tu_k=A^ku_0 \\\\\n\thow \\quad to \\quad solve \\quad u_k \\\\\n\tu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\n\tAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\n\tA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n\t=S\\Lambda ^{100} C \\\\\n\t=u_{100} \\\\\n\t$$\n-\t因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例\n\t$$\n\tF_0=0 \\\\\n\tF_1=1 \\\\\n\tF_2=1 \\\\\n\tF_3=2 \\\\\n\tF_4=3 \\\\\n\tF_5=5 \\\\\n\t..... \\\\\n\tF_{100}=? \\\\\n\t$$\n-\t斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系\n\t$$\n\tF_{k+2}=F_{k+1}+F_k \\\\\n\tF_{k+1}=F_{k+1} \\\\\n\t$$\n-\t定义向量\n\t$$\n\tu_k=\n\t\\begin{bmatrix}\n\tF_{k+1} \\\\\n\tF_k \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t利用这个向量可以将前两个等式写成矩阵形式 \n\t$$\n\tu_{k+1}=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix}\n\tu_k \\\\\n\tA=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\\n\t$$\n-\t得到两个特征值，我们很容易得到特征向量\n-\t回到斐波那契数列，斐波那契数列的增长速率由我们构造的\"数列更新矩阵\"的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +...+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式\n\t$$\n\tF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\\n\t$$\n-\t再有初始值有\n\t$$\n\tu_0=\n\t\\begin{bmatrix}\n\tF_1 \\\\\n\tF_0 \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t=c_1x_1+c_2x_2\n\t$$\n-\t其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值\n\n## 总结\n-\t我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式\n-\t这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂\n-\t我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式\n-\t求出矩阵的特征值，特征向量\n-\t由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式\n-\t由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值\n-\t以上是差分方程的一个例子，下一节将讨论微分方程\n\n# 第二十二讲：微分方程和exp(At)\n\n## 微分方程\n-\t常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解\n-\t举个例子\n\t$$\n\t\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\t\\frac{du_2}{dt}=u_1-2u_2 \\\\\n\tu(0)=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t首先我们列出系数矩阵，并找出矩阵的特征值和特征向量\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t-1 & 2 \\\\\n\t1 & -2 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量\n\t$$\n\tx_1=\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\tx_2=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t微分方程解的通解形式将是\n\t$$\n\tu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2\n\t$$\n-\t为什么？\n\t$$\n\t\\frac{du}{dt} \\\\\n\t=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n\t=A c_1 e^{\\lambda _1 t}x_1 \\\\\n\tbecause \\quad A x_1=\\lambda _1 x_1 \\\\\n\t$$\n-\t在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$\n-\t在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$\n-\t$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值\n-\t可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$\n-\t什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0\n-\t如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0\n-\t什么时候存在稳态？特征值中只存在0和负数，就如上面的例子\n-\t什么时候解无法收敛？任何特征值的实数部分大于0\n-\t改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散\n-\t如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？\n-\t矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如\n\t$$\n\t\\begin{bmatrix}\n\t-2 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0\n\t\n## exp(At)\t\n-\t是否可以把解表示成$S,\\Lambda$的形式\n-\t矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦\n-\t$$\n\t\\frac{du}{dt} = Au \\\\\n\tset \\quad u=Sv \\\\\n\tS \\frac{dv}{dt} = ASv \\\\\n\t\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\n\tv(t)=e^{\\Lambda t}v(0) \\\\\n\tu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\\n\t$$\n\t\n# 第二十一讲：马尔科夫矩阵;傅立叶级数\n\n## 马尔科夫矩阵\n-\t一个典型的马尔科夫矩阵\n\t$$\n\t\\begin{bmatrix}\n\t0.1 & 0.01 & 0.3 \\\\\n\t0.2 & 0.99 & 0.3 \\\\\n\t0.7 & 0 & 0.4 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵\n-\t$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1\n\n\n-\t在上一讲中我们谈到矩阵的幂可以分解为\n\t$$\n\tu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....\n\t$$\n-\t当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0\n-\t当每一列和为1时，必然存在一个特征值$\\lambda =1$\n-\t证明：\n\t$$\n\tA-I=\n\t\\begin{bmatrix}\n\t-0.9 & 0.01 & 0.3 \\\\\n\t0.2 & -0.01 & 0.3 \\\\\n\t0.7 & 0 & -0.6 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。\n-\t对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$\n-\t一个例子，u是麻省和加州的人数，A是人口流动矩阵\n\t$$\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k+1}\n\t=\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k}\n\t$$\n-\t可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省\n-\t对马尔科夫矩阵A\n\t$$\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda _1 =1 \\\\\n\t\\lambda _2 =0.7 \\\\\n\t$$\n-\t对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)\n-\t得到我们要研究的公式\n\t$$\n\tu_k=c_1\\*1^k\\*\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t+c_2\\*(0.7)^k\\*\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。\n-\t行向量为和为1是另外一种定义马尔科夫矩阵的方式\n\n## 傅里叶级数\n-\t先讨论带有标准正交基的投影问题\n-\t假设$q_1....q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合\n-\t现在我们要求出线性组合系数$x_1....x_n$\n\t$v=x_1q_1+x_2q_2+...x_nq_n$\n\t一种方法是将$v$与$q_i$做内积，逐一求出系数\n\t$$\n\tq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\\n\t$$\n-\t写成矩阵形式\n\t$$\n\t\\begin{bmatrix}\n\tq_1 & q_2 & ... & q_n \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\t... \\\\\n\tx_n \\\\\n\t\\end{bmatrix}=\n\tv \\\\\n\tQx=v \\\\\n\tx=Q^{-1}v=Q^Tv \\\\\n\t$$\n-\t现在讨论傅里叶级数\n-\t我们希望将函数分解\n\t$$\n\tf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......\n\t$$\n-\t关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。\n-\t如何求出傅里叶系数？\n-\t利用之前的向量例子来求\n-\t将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如\n\t$$\n\t\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\\n\t$$\n\n# 第二十二讲：对称矩阵及其正定性\n\n## 对称矩阵\n-\t对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交\n-\t对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵\n-\t对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵\n-\t为什么特征值都是实数？\n-\t$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{\\*}=\\lambda ^{\\*} x^{\\*}$\n-\t即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{\\* T}A^T=x^{\\* T} \\lambda ^{\\* T} $\n-\t上式中$A=A^T$，且两边同乘以$x$，与$x^{\\* T}A\\lambda x^{\\* T}x$对比可得$\\lambda ^{\\*}=\\lambda$，即特征值是实数\n-\t可见，对于复数矩阵，需要$A=A^{\\* T}$才满足对称\n-\t对于对称矩阵\n\t$$\n\tA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n\t=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\\n\t$$\n-\t所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合\n-\t对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式\n\n## 正定性\n-\t正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数\n-\t特征值的符号与稳定性有关\n-\t主元、行列式、特征值三位一体，线性代数将其统一\n\n\n\n\n\n","source":"_posts/LinearAlgebra3.md","raw":"---\ntitle: MIT线性代数笔记3\ndate: 2017-01-22 19:21:02\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# 第十七讲：行列式及其性质\n\n## 行列式\n-\t矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$\n-\t行列式的性质\n -\t$detI=1$\n -\t交换行，行列式的值的符号会相反\n -\t一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶\n -\t两行相等使得行列式为0(由性质二可以直接推出)\n -\t矩阵消元不改变其行列式(证明见下)\n -\t某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)\n -\t$detA=0$当且仅当A是奇异矩阵\n -\t$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$\n -\t$detA^{-1}detA=1$\n -\t$detA^2=(detA)^2$\n -\t$det2A=2^n detA$\n -\t$detA^T=detA$(证明见下)\n<!--more-->\n-\t行列式按行是线性的，但行列式本身不是线性的\n\t$$\n\t\\begin{vmatrix}\n\t1 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{vmatrix}=1 \\\\\n\t\\begin{vmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{vmatrix}=-1 \\\\\n\t\\begin{vmatrix}\n\tta & tb \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\tt\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix} \\\\\n\t\\begin{vmatrix}\n\tt+a & t+b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\tt & t \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明消元不改变行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc-la & d-lb \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}-l\n\t\\begin{vmatrix}\n\ta & b \\\\\n\ta & b \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}\n\t$$\n-\t证明转置不改变行列式\n\t$$\n\tA=LU \\\\\n\t$$\n-\t即证 $|U^TL^T|=|LU|$\n$$\n|U^T||L^T|=|L||U|\n$$ \n-\t以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 \n\t\n\t\n## 三角阵行列式\n-\t对三角阵U的行列式,值为对角线上元素乘积(主元乘积)\n-\t为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式\n-\t为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3...d_nI$，其中单位矩阵的行列式为1\n-\t奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积\n\n## A little more\n-\t进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的\n\t\n# 第十八讲：行列式公式和代数余子式\n\n## 行列式公式\n-\t推导2*2行列式\n\t$$\n\t\\begin{vmatrix}\n\ta & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & d \\\\\n\t\\end{vmatrix}=\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\ta & 0 \\\\\n\t0 & d \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\tc & 0 \\\\\n\t\\end{vmatrix}+\n\t\\begin{vmatrix}\n\t0 & b \\\\\n\t0 & d \\\\\n\t\\end{vmatrix} \\\\\n\t=0+ad-bc+0\n\t$$\n\t我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案\n-\t如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。\n-\t例如\n\t$$\n\t\\begin{vmatrix}\n\ta & 0 & 0\\\\\n\t0 & 0 & b\\\\\n\t0 & c & 0\\\\\n\t\\end{vmatrix}\n\t$$\n\t先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$\n-\tn*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分\n-\t行列式公式就是这$n!$个部分加起来\n\n\n## 代数余子式\n-\t$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n-\t提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式\n-\t从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式\n-\t$a_{ij}$的代数余子式记作$c_{ij}$\n-\t注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号\n-\t$detA=a_{11}C_{11}+a_{12}C_{12}+....+a_{1n}C_{1n}$\t\n\n# 第十九讲：克拉默法则，逆矩阵，体积\n\n## 逆矩阵\n-\t只有行列式不为0时，矩阵才是可逆的\n-\t逆矩阵公式\n\t$$\n\tA^{-1}=\\frac{1}{detA}C^T\n\t$$\n\t其中$C_{ij}$是$A_{ij}$的代数余子式\n-\t证明：即证$AC^T=(detA)I$\n\t$$\n\t\\begin{bmatrix}\n\ta_{11} & ... & a_{1n} \\\\\n\ta_{n1} & ... & a_{nn} \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tc_{11} & ... & c_{n1} \\\\\n\tc_{1n} & ... & c_{nn} \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\tdetA & 0 & 0 \\\\\n\t0 & detA & 0 \\\\\n\t0 & 0 & detA \\\\\n\t\\end{bmatrix}\n\t$$\n\t对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(....)+a_{13}(....)$\n\t其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0\n\t\n## 克拉默法则\n-\t解Ax=b\n\t$$\n\tAx=b \\\\\n\tx=A^{-1}b \\\\\n\tx=\\frac{1}{detA}C^Tb \\\\\n\t \\\\\n\tx_1=\\frac{detB_1}{detA} \\\\\n\tx_3=\\frac{detB_2}{detA} \\\\\n\t... \\\\\n\t$$\n-\t克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变\n\n## 体积\n-\tA的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积\n-\t矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。\n-\t(1)单位矩阵对应单位立方体，体积为1\n-\t对正交矩阵Q,\n\t$$\n\tQQ^T=I \\\\\n\t|QQ^T|=|I| \\\\\n\t|Q||Q^T|=1 \\\\\n\t{|Q|}^2=1 \\\\\n\t|Q|=1 \\\\\n\t$$\n\tQ对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度\n-\t(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍\n-\t(2)交换矩阵两行，盒子的体积不变\n-\t(3b)矩阵某一行拆分，盒子也相应切分为两部分\n-\t以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证\n\t\n# 第二十讲：特征值和特征向量\n\n## 特征向量\n-\t给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax\n-\t当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值\n-\t如果A是奇异矩阵，$\\lambda = 0$是一个特征值\n\n## 几个例子\n-\t如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.\n-\t再举一例\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =1, x=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =-1, x=\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\tAx=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix} \\\\\t\n\t$$\n-\tn*n矩阵有n个特征值\n-\t特征值的和等于对角线元素和，这个和称为迹(trace)，\n-\t如何求解$Ax=\\lambda x$\n\t$$\n\t(A-\\lambda I)x=0 \\\\\n\t$$\n-\t可见方程有非零解，$(A-\\lambda I)$必须是奇异的 \n\t即: \n\t$$\n\tdet(A-\\lambda I)=0 \\\\\n\t$$\n-\t$$\n\tIf \\qquad Ax=\\lambda x \\\\\n\tThen \\qquad (A+3I)x=(\\lambda +3)x \\\\\n\t$$\n-\t因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$\n-\tA+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积\n-\t再举一例，对旋转矩阵Q\n\t$$\n\tQ=\n\t\\begin{bmatrix}\n\t0 & -1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\ttrace=0=\\lambda _1 +\\lambda _2 \\\\\n\tdet=1=\\lambda _1 \\lambda _2 \\\\\n\t$$\n-\t但是可以看出 $\\lambda _1，\\lambda _2$无实数解 \n-\t再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t3 & 1 \\\\\n\t0 & 3 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t3-\\lambda & 1 \\\\\n\t0 & 3-\\lambda \\\\\n\t\\end{vmatrix}\n\t==(3-\\lambda )^2=0 \\\\\n\t\\lambda _1=\\lambda _2=3 \\\\\n\tx_1=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t$$\n\t\n# 第二十一讲：对角化和A的幂\n\n## 对角化\n-\t假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵\n-\t以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下\n-\t$$\n\tAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n\t=[x_1,x_2,...x_n]\n\t\\begin{bmatrix}\n\t\\lambda _1 & 0 & ... & 0 \\\\\n\t0 & \\lambda _2 & ... & 0 \\\\\n\t... & ... & ... & ... \\\\\n\t0 & 0  & 0 & \\lambda _n \\\\\n\t\\end{bmatrix} \\\\\n\t=S \\Lambda \\\\\n\t$$\n\n\n-\t假设S可逆，即n个特征向量无关，此时可以得到\n\t$$\n\tS^{-1}AS=\\Lambda \\\\\n\tA=S\\Lambda S^{-1} \\\\\n\t$$\n-\t$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解\n-\t$$\n\tif \\qquad Ax=\\lambda x \\\\\n\tA^2 x=\\lambda AX=\\lambda ^2 x \\\\\n\tA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\\n\t$$\n-\t上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理\n-\t特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式\n-\t什么样的矩阵的幂趋向于0(稳定)\n\t$$\n\tA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\n\tif \\quad all |\\lambda _i|<1 \\\\ \n\t$$\n-\t哪些矩阵可以对角化？\n\t如果所有特征值不同，则A可以对角化\n-\t如果矩阵A已经是对角阵，则$\\Lambda$与A相同\n-\t特征值重复的次数称为代数重度，对三角阵，如\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t2 & 1 \\\\\n\t0 & 2 \\\\\n\t\\end{bmatrix} \\\\\n\tdet(A-\\lambda I)=\n\t\\begin{vmatrix}\n\t2-\\lambda & 1 \\\\\n\t0 & 2-\\lambda \\\\\n\t\\end{vmatrix}=0 \\\\\n\t\\lambda =2 \\\\\n\tA-\\lambda I=\n\t\\begin{bmatrix}\n\t0 & 1 \\\\\n\t0 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2\n-\t特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。\n\n## A的幂\n-\t多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂\n-\t$$\n\tgive \\quad u_0 \\\\\n\tu_{k+1}=Au_k \\\\\n\tu_k=A^ku_0 \\\\\n\thow \\quad to \\quad solve \\quad u_k \\\\\n\tu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\n\tAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\n\tA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n\t=S\\Lambda ^{100} C \\\\\n\t=u_{100} \\\\\n\t$$\n-\t因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例\n\t$$\n\tF_0=0 \\\\\n\tF_1=1 \\\\\n\tF_2=1 \\\\\n\tF_3=2 \\\\\n\tF_4=3 \\\\\n\tF_5=5 \\\\\n\t..... \\\\\n\tF_{100}=? \\\\\n\t$$\n-\t斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系\n\t$$\n\tF_{k+2}=F_{k+1}+F_k \\\\\n\tF_{k+1}=F_{k+1} \\\\\n\t$$\n-\t定义向量\n\t$$\n\tu_k=\n\t\\begin{bmatrix}\n\tF_{k+1} \\\\\n\tF_k \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t利用这个向量可以将前两个等式写成矩阵形式 \n\t$$\n\tu_{k+1}=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix}\n\tu_k \\\\\n\tA=\n\t\\begin{bmatrix}\n\t1 & 1 \\\\\n\t1 & 0 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\\n\t$$\n-\t得到两个特征值，我们很容易得到特征向量\n-\t回到斐波那契数列，斐波那契数列的增长速率由我们构造的\"数列更新矩阵\"的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +...+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式\n\t$$\n\tF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\\n\t$$\n-\t再有初始值有\n\t$$\n\tu_0=\n\t\\begin{bmatrix}\n\tF_1 \\\\\n\tF_0 \\\\\n\t\\end{bmatrix}=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix}\n\t=c_1x_1+c_2x_2\n\t$$\n-\t其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值\n\n## 总结\n-\t我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式\n-\t这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂\n-\t我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式\n-\t求出矩阵的特征值，特征向量\n-\t由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式\n-\t由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值\n-\t以上是差分方程的一个例子，下一节将讨论微分方程\n\n# 第二十二讲：微分方程和exp(At)\n\n## 微分方程\n-\t常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解\n-\t举个例子\n\t$$\n\t\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\t\\frac{du_2}{dt}=u_1-2u_2 \\\\\n\tu(0)=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t0 \\\\\n\t\\end{bmatrix} \\\\\n\t$$\n-\t首先我们列出系数矩阵，并找出矩阵的特征值和特征向量\n\t$$\n\tA=\n\t\\begin{bmatrix}\n\t-1 & 2 \\\\\n\t1 & -2 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量\n\t$$\n\tx_1=\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix} \\\\\n\tx_2=\n\t\\begin{bmatrix}\n\t1 \\\\\n\t-1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t微分方程解的通解形式将是\n\t$$\n\tu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2\n\t$$\n-\t为什么？\n\t$$\n\t\\frac{du}{dt} \\\\\n\t=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n\t=A c_1 e^{\\lambda _1 t}x_1 \\\\\n\tbecause \\quad A x_1=\\lambda _1 x_1 \\\\\n\t$$\n-\t在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$\n-\t在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$\n-\t$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值\n-\t可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$\n-\t什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0\n-\t如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0\n-\t什么时候存在稳态？特征值中只存在0和负数，就如上面的例子\n-\t什么时候解无法收敛？任何特征值的实数部分大于0\n-\t改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散\n-\t如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？\n-\t矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如\n\t$$\n\t\\begin{bmatrix}\n\t-2 & 0 \\\\\n\t0 & 1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0\n\t\n## exp(At)\t\n-\t是否可以把解表示成$S,\\Lambda$的形式\n-\t矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦\n-\t$$\n\t\\frac{du}{dt} = Au \\\\\n\tset \\quad u=Sv \\\\\n\tS \\frac{dv}{dt} = ASv \\\\\n\t\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\n\tv(t)=e^{\\Lambda t}v(0) \\\\\n\tu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\\n\t$$\n\t\n# 第二十一讲：马尔科夫矩阵;傅立叶级数\n\n## 马尔科夫矩阵\n-\t一个典型的马尔科夫矩阵\n\t$$\n\t\\begin{bmatrix}\n\t0.1 & 0.01 & 0.3 \\\\\n\t0.2 & 0.99 & 0.3 \\\\\n\t0.7 & 0 & 0.4 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵\n-\t$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1\n\n\n-\t在上一讲中我们谈到矩阵的幂可以分解为\n\t$$\n\tu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....\n\t$$\n-\t当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0\n-\t当每一列和为1时，必然存在一个特征值$\\lambda =1$\n-\t证明：\n\t$$\n\tA-I=\n\t\\begin{bmatrix}\n\t-0.9 & 0.01 & 0.3 \\\\\n\t0.2 & -0.01 & 0.3 \\\\\n\t0.7 & 0 & -0.6 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。\n-\t对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$\n-\t一个例子，u是麻省和加州的人数，A是人口流动矩阵\n\t$$\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k+1}\n\t=\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tu_{cal} \\\\\n\tu_{mass} \\\\\n\t\\end{bmatrix}_{t=k}\n\t$$\n-\t可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省\n-\t对马尔科夫矩阵A\n\t$$\n\t\\begin{bmatrix}\n\t0.9 & 0.2 \\\\\n\t0.1 & 0.8 \\\\\n\t\\end{bmatrix} \\\\\n\t\\lambda _1 =1 \\\\\n\t\\lambda _2 =0.7 \\\\\n\t$$\n-\t对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)\n-\t得到我们要研究的公式\n\t$$\n\tu_k=c_1\\*1^k\\*\n\t\\begin{bmatrix}\n\t2 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t+c_2\\*(0.7)^k\\*\n\t\\begin{bmatrix}\n\t-1 \\\\\n\t1 \\\\\n\t\\end{bmatrix}\n\t$$\n-\t假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。\n-\t行向量为和为1是另外一种定义马尔科夫矩阵的方式\n\n## 傅里叶级数\n-\t先讨论带有标准正交基的投影问题\n-\t假设$q_1....q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合\n-\t现在我们要求出线性组合系数$x_1....x_n$\n\t$v=x_1q_1+x_2q_2+...x_nq_n$\n\t一种方法是将$v$与$q_i$做内积，逐一求出系数\n\t$$\n\tq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\\n\t$$\n-\t写成矩阵形式\n\t$$\n\t\\begin{bmatrix}\n\tq_1 & q_2 & ... & q_n \\\\\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\t... \\\\\n\tx_n \\\\\n\t\\end{bmatrix}=\n\tv \\\\\n\tQx=v \\\\\n\tx=Q^{-1}v=Q^Tv \\\\\n\t$$\n-\t现在讨论傅里叶级数\n-\t我们希望将函数分解\n\t$$\n\tf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......\n\t$$\n-\t关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。\n-\t如何求出傅里叶系数？\n-\t利用之前的向量例子来求\n-\t将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如\n\t$$\n\t\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\\n\t$$\n\n# 第二十二讲：对称矩阵及其正定性\n\n## 对称矩阵\n-\t对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交\n-\t对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵\n-\t对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵\n-\t为什么特征值都是实数？\n-\t$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{\\*}=\\lambda ^{\\*} x^{\\*}$\n-\t即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{\\* T}A^T=x^{\\* T} \\lambda ^{\\* T} $\n-\t上式中$A=A^T$，且两边同乘以$x$，与$x^{\\* T}A\\lambda x^{\\* T}x$对比可得$\\lambda ^{\\*}=\\lambda$，即特征值是实数\n-\t可见，对于复数矩阵，需要$A=A^{\\* T}$才满足对称\n-\t对于对称矩阵\n\t$$\n\tA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n\t=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\\n\t$$\n-\t所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合\n-\t对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式\n\n## 正定性\n-\t正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数\n-\t特征值的符号与稳定性有关\n-\t主元、行列式、特征值三位一体，线性代数将其统一\n\n\n\n\n\n","slug":"LinearAlgebra3","published":1,"updated":"2019-07-22T03:45:22.803Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v9g003yq8t5kffn38zm","content":"<hr>\n<h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul>\n<li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li>\n<li>行列式的性质<ul>\n<li>$detI=1$</li>\n<li>交换行，行列式的值的符号会相反</li>\n<li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li>\n<li>两行相等使得行列式为0(由性质二可以直接推出)</li>\n<li>矩阵消元不改变其行列式(证明见下)</li>\n<li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li>\n<li>$detA=0$当且仅当A是奇异矩阵</li>\n<li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li>\n<li>$detA^{-1}detA=1$</li>\n<li>$detA^2=(detA)^2$</li>\n<li>$det2A=2^n detA$</li>\n<li>$detA^T=detA$(证明见下)<a id=\"more\"></a></li>\n</ul>\n</li>\n<li>行列式按行是线性的，但行列式本身不是线性的<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{vmatrix}=1 \\\\\n\\begin{vmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{vmatrix}=-1 \\\\\n\\begin{vmatrix}\nta & tb \\\\\nc & d \\\\\n\\end{vmatrix}=\nt\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix} \\\\\n\\begin{vmatrix}\nt+a & t+b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\nt & t \\\\\nc & d \\\\\n\\end{vmatrix}</script></li>\n<li>证明消元不改变行列式<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & b \\\\\nc-la & d-lb \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}-l\n\\begin{vmatrix}\na & b \\\\\na & b \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}</script></li>\n<li>证明转置不改变行列式<script type=\"math/tex; mode=display\">\nA=LU \\\\</script></li>\n<li>即证 $|U^TL^T|=|LU|$<script type=\"math/tex; mode=display\">\n|U^T||L^T|=|L||U|</script></li>\n<li>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 </li>\n</ul>\n<h2 id=\"三角阵行列式\"><a href=\"#三角阵行列式\" class=\"headerlink\" title=\"三角阵行列式\"></a>三角阵行列式</h2><ul>\n<li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li>\n<li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li>\n<li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3…d_nI$，其中单位矩阵的行列式为1</li>\n<li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li>\n</ul>\n<h2 id=\"A-little-more\"><a href=\"#A-little-more\" class=\"headerlink\" title=\"A little more\"></a>A little more</h2><ul>\n<li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li>\n</ul>\n<h1 id=\"第十八讲：行列式公式和代数余子式\"><a href=\"#第十八讲：行列式公式和代数余子式\" class=\"headerlink\" title=\"第十八讲：行列式公式和代数余子式\"></a>第十八讲：行列式公式和代数余子式</h1><h2 id=\"行列式公式\"><a href=\"#行列式公式\" class=\"headerlink\" title=\"行列式公式\"></a>行列式公式</h2><ul>\n<li>推导2*2行列式<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & 0 \\\\\nc & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & 0 \\\\\nc & 0 \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\na & 0 \\\\\n0 & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\nc & 0 \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\n0 & d \\\\\n\\end{vmatrix} \\\\\n=0+ad-bc+0</script>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</li>\n<li>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</li>\n<li>例如<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & 0 & 0\\\\\n0 & 0 & b\\\\\n0 & c & 0\\\\\n\\end{vmatrix}</script>先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$</li>\n<li>n*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分</li>\n<li>行列式公式就是这$n!$个部分加起来</li>\n</ul>\n<h2 id=\"代数余子式\"><a href=\"#代数余子式\" class=\"headerlink\" title=\"代数余子式\"></a>代数余子式</h2><ul>\n<li>$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$</li>\n<li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li>\n<li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li>\n<li>$a_{ij}$的代数余子式记作$c_{ij}$</li>\n<li>注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li>\n<li>$detA=a_{11}C_{11}+a_{12}C_{12}+….+a_{1n}C_{1n}$    </li>\n</ul>\n<h1 id=\"第十九讲：克拉默法则，逆矩阵，体积\"><a href=\"#第十九讲：克拉默法则，逆矩阵，体积\" class=\"headerlink\" title=\"第十九讲：克拉默法则，逆矩阵，体积\"></a>第十九讲：克拉默法则，逆矩阵，体积</h1><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul>\n<li>只有行列式不为0时，矩阵才是可逆的</li>\n<li>逆矩阵公式<script type=\"math/tex; mode=display\">\nA^{-1}=\\frac{1}{detA}C^T</script>其中$C_{ij}$是$A_{ij}$的代数余子式</li>\n<li>证明：即证$AC^T=(detA)I$<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na_{11} & ... & a_{1n} \\\\\na_{n1} & ... & a_{nn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{11} & ... & c_{n1} \\\\\nc_{1n} & ... & c_{nn} \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\ndetA & 0 & 0 \\\\\n0 & detA & 0 \\\\\n0 & 0 & detA \\\\\n\\end{bmatrix}</script>对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$<br>其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</li>\n</ul>\n<h2 id=\"克拉默法则\"><a href=\"#克拉默法则\" class=\"headerlink\" title=\"克拉默法则\"></a>克拉默法则</h2><ul>\n<li>解Ax=b<script type=\"math/tex; mode=display\">\nAx=b \\\\\nx=A^{-1}b \\\\\nx=\\frac{1}{detA}C^Tb \\\\\n\\\\\nx_1=\\frac{detB_1}{detA} \\\\\nx_3=\\frac{detB_2}{detA} \\\\\n... \\\\</script></li>\n<li>克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变</li>\n</ul>\n<h2 id=\"体积\"><a href=\"#体积\" class=\"headerlink\" title=\"体积\"></a>体积</h2><ul>\n<li>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</li>\n<li>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</li>\n<li>(1)单位矩阵对应单位立方体，体积为1</li>\n<li>对正交矩阵Q,<script type=\"math/tex; mode=display\">\nQQ^T=I \\\\\n|QQ^T|=|I| \\\\\n|Q||Q^T|=1 \\\\\n{|Q|}^2=1 \\\\\n|Q|=1 \\\\</script>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</li>\n<li>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</li>\n<li>(2)交换矩阵两行，盒子的体积不变</li>\n<li>(3b)矩阵某一行拆分，盒子也相应切分为两部分</li>\n<li>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</li>\n</ul>\n<h1 id=\"第二十讲：特征值和特征向量\"><a href=\"#第二十讲：特征值和特征向量\" class=\"headerlink\" title=\"第二十讲：特征值和特征向量\"></a>第二十讲：特征值和特征向量</h1><h2 id=\"特征向量\"><a href=\"#特征向量\" class=\"headerlink\" title=\"特征向量\"></a>特征向量</h2><ul>\n<li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li>\n<li>当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值</li>\n<li>如果A是奇异矩阵，$\\lambda = 0$是一个特征值</li>\n</ul>\n<h2 id=\"几个例子\"><a href=\"#几个例子\" class=\"headerlink\" title=\"几个例子\"></a>几个例子</h2><ul>\n<li>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</li>\n<li>再举一例<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\n\\lambda =1, x=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}\nAx=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix} \\\\\n\\lambda =-1, x=\n\\begin{bmatrix}\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\nAx=\n\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>n*n矩阵有n个特征值</li>\n<li>特征值的和等于对角线元素和，这个和称为迹(trace)，</li>\n<li>如何求解$Ax=\\lambda x$<script type=\"math/tex; mode=display\">\n(A-\\lambda I)x=0 \\\\</script></li>\n<li>可见方程有非零解，$(A-\\lambda I)$必须是奇异的<br>即: <script type=\"math/tex; mode=display\">\ndet(A-\\lambda I)=0 \\\\</script></li>\n<li><script type=\"math/tex; mode=display\">\nIf \\qquad Ax=\\lambda x \\\\\nThen \\qquad (A+3I)x=(\\lambda +3)x \\\\</script></li>\n<li>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$</li>\n<li>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</li>\n<li>再举一例，对旋转矩阵Q<script type=\"math/tex; mode=display\">\nQ=\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\ntrace=0=\\lambda _1 +\\lambda _2 \\\\\ndet=1=\\lambda _1 \\lambda _2 \\\\</script></li>\n<li>但是可以看出 $\\lambda _1，\\lambda _2$无实数解 </li>\n<li>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n3 & 1 \\\\\n0 & 3 \\\\\n\\end{bmatrix} \\\\\ndet(A-\\lambda I)=\n\\begin{vmatrix}\n3-\\lambda & 1 \\\\\n0 & 3-\\lambda \\\\\n\\end{vmatrix}\n==(3-\\lambda )^2=0 \\\\\n\\lambda _1=\\lambda _2=3 \\\\\nx_1=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n</ul>\n<h1 id=\"第二十一讲：对角化和A的幂\"><a href=\"#第二十一讲：对角化和A的幂\" class=\"headerlink\" title=\"第二十一讲：对角化和A的幂\"></a>第二十一讲：对角化和A的幂</h1><h2 id=\"对角化\"><a href=\"#对角化\" class=\"headerlink\" title=\"对角化\"></a>对角化</h2><ul>\n<li>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</li>\n<li>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</li>\n<li><script type=\"math/tex; mode=display\">\nAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n=[x_1,x_2,...x_n]\n\\begin{bmatrix}\n\\lambda _1 & 0 & ... & 0 \\\\\n0 & \\lambda _2 & ... & 0 \\\\\n... & ... & ... & ... \\\\\n0 & 0  & 0 & \\lambda _n \\\\\n\\end{bmatrix} \\\\\n=S \\Lambda \\\\</script></li>\n</ul>\n<ul>\n<li>假设S可逆，即n个特征向量无关，此时可以得到<script type=\"math/tex; mode=display\">\nS^{-1}AS=\\Lambda \\\\\nA=S\\Lambda S^{-1} \\\\</script></li>\n<li>$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解</li>\n<li><script type=\"math/tex; mode=display\">\nif \\qquad Ax=\\lambda x \\\\\nA^2 x=\\lambda AX=\\lambda ^2 x \\\\\nA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\</script></li>\n<li>上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理</li>\n<li>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</li>\n<li>什么样的矩阵的幂趋向于0(稳定)<script type=\"math/tex; mode=display\">\nA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\nif \\quad all |\\lambda _i|<1 \\\\</script></li>\n<li>哪些矩阵可以对角化？<br>如果所有特征值不同，则A可以对角化</li>\n<li>如果矩阵A已经是对角阵，则$\\Lambda$与A相同</li>\n<li>特征值重复的次数称为代数重度，对三角阵，如<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n2 & 1 \\\\\n0 & 2 \\\\\n\\end{bmatrix} \\\\\ndet(A-\\lambda I)=\n\\begin{vmatrix}\n2-\\lambda & 1 \\\\\n0 & 2-\\lambda \\\\\n\\end{vmatrix}=0 \\\\\n\\lambda =2 \\\\\nA-\\lambda I=\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2</li>\n<li>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</li>\n</ul>\n<h2 id=\"A的幂\"><a href=\"#A的幂\" class=\"headerlink\" title=\"A的幂\"></a>A的幂</h2><ul>\n<li>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</li>\n<li><script type=\"math/tex; mode=display\">\ngive \\quad u_0 \\\\\nu_{k+1}=Au_k \\\\\nu_k=A^ku_0 \\\\\nhow \\quad to \\quad solve \\quad u_k \\\\\nu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\nAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\nA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n=S\\Lambda ^{100} C \\\\\n=u_{100} \\\\</script></li>\n<li>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例<script type=\"math/tex; mode=display\">\nF_0=0 \\\\\nF_1=1 \\\\\nF_2=1 \\\\\nF_3=2 \\\\\nF_4=3 \\\\\nF_5=5 \\\\\n..... \\\\\nF_{100}=? \\\\</script></li>\n<li>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系<script type=\"math/tex; mode=display\">\nF_{k+2}=F_{k+1}+F_k \\\\\nF_{k+1}=F_{k+1} \\\\</script></li>\n<li>定义向量<script type=\"math/tex; mode=display\">\nu_k=\n\\begin{bmatrix}\nF_{k+1} \\\\\nF_k \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>利用这个向量可以将前两个等式写成矩阵形式 <script type=\"math/tex; mode=display\">\nu_{k+1}=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix}\nu_k \\\\\nA=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\n\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\</script></li>\n<li>得到两个特征值，我们很容易得到特征向量</li>\n<li>回到斐波那契数列，斐波那契数列的增长速率由我们构造的”数列更新矩阵”的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +…+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式<script type=\"math/tex; mode=display\">\nF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\</script></li>\n<li>再有初始值有<script type=\"math/tex; mode=display\">\nu_0=\n\\begin{bmatrix}\nF_1 \\\\\nF_0 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}\n=c_1x_1+c_2x_2</script></li>\n<li>其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式</li>\n<li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li>\n<li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li>\n<li>求出矩阵的特征值，特征向量</li>\n<li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式</li>\n<li>由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值</li>\n<li>以上是差分方程的一个例子，下一节将讨论微分方程</li>\n</ul>\n<h1 id=\"第二十二讲：微分方程和exp-At\"><a href=\"#第二十二讲：微分方程和exp-At\" class=\"headerlink\" title=\"第二十二讲：微分方程和exp(At)\"></a>第二十二讲：微分方程和exp(At)</h1><h2 id=\"微分方程\"><a href=\"#微分方程\" class=\"headerlink\" title=\"微分方程\"></a>微分方程</h2><ul>\n<li>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</li>\n<li>举个例子<script type=\"math/tex; mode=display\">\n\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\\frac{du_2}{dt}=u_1-2u_2 \\\\\nu(0)=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n-1 & 2 \\\\\n1 & -2 \\\\\n\\end{bmatrix}</script></li>\n<li>易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量<script type=\"math/tex; mode=display\">\nx_1=\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n\\end{bmatrix} \\\\\nx_2=\n\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n\\end{bmatrix}</script></li>\n<li>微分方程解的通解形式将是<script type=\"math/tex; mode=display\">\nu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2</script></li>\n<li>为什么？<script type=\"math/tex; mode=display\">\n\\frac{du}{dt} \\\\\n=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n=A c_1 e^{\\lambda _1 t}x_1 \\\\\nbecause \\quad A x_1=\\lambda _1 x_1 \\\\</script></li>\n<li>在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$</li>\n<li>在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$</li>\n<li>$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</li>\n<li>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$</li>\n<li>什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0</li>\n<li>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</li>\n<li>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</li>\n<li>什么时候解无法收敛？任何特征值的实数部分大于0</li>\n<li>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</li>\n<li>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</li>\n<li>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-2 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}</script></li>\n<li>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</li>\n</ul>\n<h2 id=\"exp-At\"><a href=\"#exp-At\" class=\"headerlink\" title=\"exp(At)\"></a>exp(At)</h2><ul>\n<li>是否可以把解表示成$S,\\Lambda$的形式</li>\n<li>矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦</li>\n<li><script type=\"math/tex; mode=display\">\n\\frac{du}{dt} = Au \\\\\nset \\quad u=Sv \\\\\nS \\frac{dv}{dt} = ASv \\\\\n\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\nv(t)=e^{\\Lambda t}v(0) \\\\\nu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\</script></li>\n</ul>\n<h1 id=\"第二十一讲：马尔科夫矩阵-傅立叶级数\"><a href=\"#第二十一讲：马尔科夫矩阵-傅立叶级数\" class=\"headerlink\" title=\"第二十一讲：马尔科夫矩阵;傅立叶级数\"></a>第二十一讲：马尔科夫矩阵;傅立叶级数</h1><h2 id=\"马尔科夫矩阵\"><a href=\"#马尔科夫矩阵\" class=\"headerlink\" title=\"马尔科夫矩阵\"></a>马尔科夫矩阵</h2><ul>\n<li>一个典型的马尔科夫矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0.1 & 0.01 & 0.3 \\\\\n0.2 & 0.99 & 0.3 \\\\\n0.7 & 0 & 0.4 \\\\\n\\end{bmatrix}</script></li>\n<li>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</li>\n<li>$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1</li>\n</ul>\n<ul>\n<li>在上一讲中我们谈到矩阵的幂可以分解为<script type=\"math/tex; mode=display\">\nu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....</script></li>\n<li>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</li>\n<li>当每一列和为1时，必然存在一个特征值$\\lambda =1$</li>\n<li>证明：<script type=\"math/tex; mode=display\">\nA-I=\n\\begin{bmatrix}\n-0.9 & 0.01 & 0.3 \\\\\n0.2 & -0.01 & 0.3 \\\\\n0.7 & 0 & -0.6 \\\\\n\\end{bmatrix}</script></li>\n<li>若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</li>\n<li>对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$</li>\n<li>一个例子，u是麻省和加州的人数，A是人口流动矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_{cal} \\\\\nu_{mass} \\\\\n\\end{bmatrix}_{t=k+1}\n=\n\\begin{bmatrix}\n0.9 & 0.2 \\\\\n0.1 & 0.8 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{cal} \\\\\nu_{mass} \\\\\n\\end{bmatrix}_{t=k}</script></li>\n<li>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</li>\n<li>对马尔科夫矩阵A<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0.9 & 0.2 \\\\\n0.1 & 0.8 \\\\\n\\end{bmatrix} \\\\\n\\lambda _1 =1 \\\\\n\\lambda _2 =0.7 \\\\</script></li>\n<li>对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)</li>\n<li>得到我们要研究的公式<script type=\"math/tex; mode=display\">\nu_k=c_1\\*1^k\\*\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n\\end{bmatrix}\n+c_2\\*(0.7)^k\\*\n\\begin{bmatrix}\n-1 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</li>\n<li>行向量为和为1是另外一种定义马尔科夫矩阵的方式</li>\n</ul>\n<h2 id=\"傅里叶级数\"><a href=\"#傅里叶级数\" class=\"headerlink\" title=\"傅里叶级数\"></a>傅里叶级数</h2><ul>\n<li>先讨论带有标准正交基的投影问题</li>\n<li>假设$q_1….q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合</li>\n<li>现在我们要求出线性组合系数$x_1….x_n$<br>$v=x_1q_1+x_2q_2+…x_nq_n$<br>一种方法是将$v$与$q_i$做内积，逐一求出系数<script type=\"math/tex; mode=display\">\nq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\</script></li>\n<li>写成矩阵形式<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n... \\\\\nx_n \\\\\n\\end{bmatrix}=\nv \\\\\nQx=v \\\\\nx=Q^{-1}v=Q^Tv \\\\</script></li>\n<li>现在讨论傅里叶级数</li>\n<li>我们希望将函数分解<script type=\"math/tex; mode=display\">\nf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......</script></li>\n<li>关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</li>\n<li>如何求出傅里叶系数？</li>\n<li>利用之前的向量例子来求</li>\n<li>将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如<script type=\"math/tex; mode=display\">\n\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\</script></li>\n</ul>\n<h1 id=\"第二十二讲：对称矩阵及其正定性\"><a href=\"#第二十二讲：对称矩阵及其正定性\" class=\"headerlink\" title=\"第二十二讲：对称矩阵及其正定性\"></a>第二十二讲：对称矩阵及其正定性</h1><h2 id=\"对称矩阵\"><a href=\"#对称矩阵\" class=\"headerlink\" title=\"对称矩阵\"></a>对称矩阵</h2><ul>\n<li>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</li>\n<li>对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵</li>\n<li>对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵</li>\n<li>为什么特征值都是实数？</li>\n<li>$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{*}=\\lambda ^{*} x^{*}$</li>\n<li>即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{* T}A^T=x^{* T} \\lambda ^{* T} $</li>\n<li>上式中$A=A^T$，且两边同乘以$x$，与$x^{* T}A\\lambda x^{* T}x$对比可得$\\lambda ^{*}=\\lambda$，即特征值是实数</li>\n<li>可见，对于复数矩阵，需要$A=A^{* T}$才满足对称</li>\n<li>对于对称矩阵<script type=\"math/tex; mode=display\">\nA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\</script></li>\n<li>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</li>\n<li>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</li>\n</ul>\n<h2 id=\"正定性\"><a href=\"#正定性\" class=\"headerlink\" title=\"正定性\"></a>正定性</h2><ul>\n<li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li>\n<li>特征值的符号与稳定性有关</li>\n<li>主元、行列式、特征值三位一体，线性代数将其统一</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul>\n<li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li>\n<li>行列式的性质<ul>\n<li>$detI=1$</li>\n<li>交换行，行列式的值的符号会相反</li>\n<li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li>\n<li>两行相等使得行列式为0(由性质二可以直接推出)</li>\n<li>矩阵消元不改变其行列式(证明见下)</li>\n<li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li>\n<li>$detA=0$当且仅当A是奇异矩阵</li>\n<li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li>\n<li>$detA^{-1}detA=1$</li>\n<li>$detA^2=(detA)^2$</li>\n<li>$det2A=2^n detA$</li>\n<li>$detA^T=detA$(证明见下)</li></ul></li></ul>","more":"\n\n\n<li>行列式按行是线性的，但行列式本身不是线性的<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{vmatrix}=1 \\\\\n\\begin{vmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{vmatrix}=-1 \\\\\n\\begin{vmatrix}\nta & tb \\\\\nc & d \\\\\n\\end{vmatrix}=\nt\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix} \\\\\n\\begin{vmatrix}\nt+a & t+b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\nt & t \\\\\nc & d \\\\\n\\end{vmatrix}</script></li>\n<li>证明消元不改变行列式<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & b \\\\\nc-la & d-lb \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}-l\n\\begin{vmatrix}\na & b \\\\\na & b \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}</script></li>\n<li>证明转置不改变行列式<script type=\"math/tex; mode=display\">\nA=LU \\\\</script></li>\n<li>即证 $|U^TL^T|=|LU|$<script type=\"math/tex; mode=display\">\n|U^T||L^T|=|L||U|</script></li>\n<li>以上四个矩阵都是三角矩阵，行列式等于对角线乘积，转置没有影响，所以相等 </li>\n\n<h2 id=\"三角阵行列式\"><a href=\"#三角阵行列式\" class=\"headerlink\" title=\"三角阵行列式\"></a>三角阵行列式</h2><ul>\n<li>对三角阵U的行列式,值为对角线上元素乘积(主元乘积)</li>\n<li>为什么三角阵其他元素不起作用？因为通过消元我们可以得到只有对角元素的矩阵，而消元不改变行列式</li>\n<li>为什么是对角线元素的乘积？因为可以消元后可以依次把对角元素提出来，即得到$d_1d_2d_3…d_nI$，其中单位矩阵的行列式为1</li>\n<li>奇异矩阵行列式为0，存在全0行；可逆矩阵行列式不为0，能化成三角阵，行列式是三角矩阵对角元素乘积</li>\n</ul>\n<h2 id=\"A-little-more\"><a href=\"#A-little-more\" class=\"headerlink\" title=\"A little more\"></a>A little more</h2><ul>\n<li>进行奇数次置换和偶数次置换得到的行列式肯定不一样(符号不同)，这意味着进行奇数次置换和偶数次置换后的矩阵不会一样，即置换是严格区分奇偶的</li>\n</ul>\n<h1 id=\"第十八讲：行列式公式和代数余子式\"><a href=\"#第十八讲：行列式公式和代数余子式\" class=\"headerlink\" title=\"第十八讲：行列式公式和代数余子式\"></a>第十八讲：行列式公式和代数余子式</h1><h2 id=\"行列式公式\"><a href=\"#行列式公式\" class=\"headerlink\" title=\"行列式公式\"></a>行列式公式</h2><ul>\n<li>推导2*2行列式<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & 0 \\\\\nc & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\nc & d \\\\\n\\end{vmatrix}=\n\\begin{vmatrix}\na & 0 \\\\\nc & 0 \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\na & 0 \\\\\n0 & d \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\nc & 0 \\\\\n\\end{vmatrix}+\n\\begin{vmatrix}\n0 & b \\\\\n0 & d \\\\\n\\end{vmatrix} \\\\\n=0+ad-bc+0</script>我们可以发现这种方法是一次取一行，将这一行拆解(行列式按行是线性的)，再提取出因子，通过行交换得到单位矩阵，通过性质一和性质二得到答案</li>\n<li>如果扩展到3*3矩阵，则第一行分解成三部分，每部分针对第二行又分解成三部分，所以最后得到27部分，其中不为0的部分是那些各行各列均有元素的矩阵。</li>\n<li>例如<script type=\"math/tex; mode=display\">\n\\begin{vmatrix}\na & 0 & 0\\\\\n0 & 0 & b\\\\\n0 & c & 0\\\\\n\\end{vmatrix}</script>先提取出因子，得到$abc$，交换第二行第三行得到单位矩阵，于是答案就是$abc*detI=abc$，又因为进行了一次行交换，所以答案是负的，$-abc$</li>\n<li>n*n的矩阵可以分成$n!$个部分，因为第一行分成n个部分，第二行不能重复，选择n-1行，一次重复，所以得到$n!$部分</li>\n<li>行列式公式就是这$n!$个部分加起来</li>\n</ul>\n<h2 id=\"代数余子式\"><a href=\"#代数余子式\" class=\"headerlink\" title=\"代数余子式\"></a>代数余子式</h2><ul>\n<li>$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$</li>\n<li>提取出一个因子，由剩余的因子即括号内的内容组成的就是余子式</li>\n<li>从矩阵上看，选择一个元素，它的代数余子式就是排除这个元素所在行和列剩下的矩阵的行列式</li>\n<li>$a_{ij}$的代数余子式记作$c_{ij}$</li>\n<li>注意代数余子式的正负，与$i+j$的奇偶性有关，偶数取正，奇数取负，这里的符号是指代数余子式对应的子矩阵正常计算出行列式后前面的符号</li>\n<li>$detA=a_{11}C_{11}+a_{12}C_{12}+….+a_{1n}C_{1n}$    </li>\n</ul>\n<h1 id=\"第十九讲：克拉默法则，逆矩阵，体积\"><a href=\"#第十九讲：克拉默法则，逆矩阵，体积\" class=\"headerlink\" title=\"第十九讲：克拉默法则，逆矩阵，体积\"></a>第十九讲：克拉默法则，逆矩阵，体积</h1><h2 id=\"逆矩阵\"><a href=\"#逆矩阵\" class=\"headerlink\" title=\"逆矩阵\"></a>逆矩阵</h2><ul>\n<li>只有行列式不为0时，矩阵才是可逆的</li>\n<li>逆矩阵公式<script type=\"math/tex; mode=display\">\nA^{-1}=\\frac{1}{detA}C^T</script>其中$C_{ij}$是$A_{ij}$的代数余子式</li>\n<li>证明：即证$AC^T=(detA)I$<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na_{11} & ... & a_{1n} \\\\\na_{n1} & ... & a_{nn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{11} & ... & c_{n1} \\\\\nc_{1n} & ... & c_{nn} \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\ndetA & 0 & 0 \\\\\n0 & detA & 0 \\\\\n0 & 0 & detA \\\\\n\\end{bmatrix}</script>对角线上都是行列式，因为$det=a_{11}(a_{22}a_{33}-a_{23}a_{32})+a_{12}(….)+a_{13}(….)$<br>其他位置都是0，因为行a乘以行b的代数余子式相当于求一个矩阵的行列式，这个矩阵行a与行b相等，行列式为0</li>\n</ul>\n<h2 id=\"克拉默法则\"><a href=\"#克拉默法则\" class=\"headerlink\" title=\"克拉默法则\"></a>克拉默法则</h2><ul>\n<li>解Ax=b<script type=\"math/tex; mode=display\">\nAx=b \\\\\nx=A^{-1}b \\\\\nx=\\frac{1}{detA}C^Tb \\\\\n\\\\\nx_1=\\frac{detB_1}{detA} \\\\\nx_3=\\frac{detB_2}{detA} \\\\\n... \\\\</script></li>\n<li>克拉默法则即发现矩阵$B_i$就是矩阵$A$的第i列换成b，其余不变</li>\n</ul>\n<h2 id=\"体积\"><a href=\"#体积\" class=\"headerlink\" title=\"体积\"></a>体积</h2><ul>\n<li>A的行列式可以代表一个体积，例如3*3矩阵的行列式代表一个三维空间内的体积</li>\n<li>矩阵的每一行代表一个盒子的一条边(从同一顶点连出的)，行列式就是这个盒子的体积，行列式的正负代表左手或者右手系。</li>\n<li>(1)单位矩阵对应单位立方体，体积为1</li>\n<li>对正交矩阵Q,<script type=\"math/tex; mode=display\">\nQQ^T=I \\\\\n|QQ^T|=|I| \\\\\n|Q||Q^T|=1 \\\\\n{|Q|}^2=1 \\\\\n|Q|=1 \\\\</script>Q对应的盒子是单位矩阵对应的单位立方体在空间中旋转过一个角度</li>\n<li>(3a)如果矩阵的某一行翻倍，即盒子一组边翻倍，体积也翻倍，从行列式角度可以把倍数提出来，因此行列式也是翻倍</li>\n<li>(2)交换矩阵两行，盒子的体积不变</li>\n<li>(3b)矩阵某一行拆分，盒子也相应切分为两部分</li>\n<li>以上，行列式的三条性质(1,2,3a,3b)均可以在体积上验证</li>\n</ul>\n<h1 id=\"第二十讲：特征值和特征向量\"><a href=\"#第二十讲：特征值和特征向量\" class=\"headerlink\" title=\"第二十讲：特征值和特征向量\"></a>第二十讲：特征值和特征向量</h1><h2 id=\"特征向量\"><a href=\"#特征向量\" class=\"headerlink\" title=\"特征向量\"></a>特征向量</h2><ul>\n<li>给定矩阵A，矩阵A可以看成一个函数，作用在一个向量x上，得到向量Ax</li>\n<li>当Ax平行于x时，即$Ax=\\lambda x$，我们称$x$为特征向量，$\\lambda$为特征值</li>\n<li>如果A是奇异矩阵，$\\lambda = 0$是一个特征值</li>\n</ul>\n<h2 id=\"几个例子\"><a href=\"#几个例子\" class=\"headerlink\" title=\"几个例子\"></a>几个例子</h2><ul>\n<li>如果A是投影矩阵，可以发现它的特征向量就是投影平面上的任意向量，因为$Ax$即投影到平面上，平面上的所有向量投影后不变，自然平行，同时特征值就是1。如果向量垂直于平面，$Ax=0$，特征值为0.因此投影矩阵A的特征向量就分以上两种情况，特征值为1或0.</li>\n<li>再举一例<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\n\\lambda =1, x=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}\nAx=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix} \\\\\n\\lambda =-1, x=\n\\begin{bmatrix}\n-1 \\\\\n1 \\\\\n\\end{bmatrix}\nAx=\n\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>n*n矩阵有n个特征值</li>\n<li>特征值的和等于对角线元素和，这个和称为迹(trace)，</li>\n<li>如何求解$Ax=\\lambda x$<script type=\"math/tex; mode=display\">\n(A-\\lambda I)x=0 \\\\</script></li>\n<li>可见方程有非零解，$(A-\\lambda I)$必须是奇异的<br>即: <script type=\"math/tex; mode=display\">\ndet(A-\\lambda I)=0 \\\\</script></li>\n<li><script type=\"math/tex; mode=display\">\nIf \\qquad Ax=\\lambda x \\\\\nThen \\qquad (A+3I)x=(\\lambda +3)x \\\\</script></li>\n<li>因为加上单位矩阵，特征向量不变依然为x，特征值加上单位矩阵的系数即$(\\lambda +3)$</li>\n<li>A+B的特征值不一定是A的特征值加上B的特征值，因为他们的特征向量不一定相同。同理AB的特征值也不一定是他们的特征值的乘积</li>\n<li>再举一例，对旋转矩阵Q<script type=\"math/tex; mode=display\">\nQ=\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\ntrace=0=\\lambda _1 +\\lambda _2 \\\\\ndet=1=\\lambda _1 \\lambda _2 \\\\</script></li>\n<li>但是可以看出 $\\lambda _1，\\lambda _2$无实数解 </li>\n<li>再看看更加糟糕的情况(矩阵更加不对称，更难得到实数解的特征值)<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n3 & 1 \\\\\n0 & 3 \\\\\n\\end{bmatrix} \\\\\ndet(A-\\lambda I)=\n\\begin{vmatrix}\n3-\\lambda & 1 \\\\\n0 & 3-\\lambda \\\\\n\\end{vmatrix}\n==(3-\\lambda )^2=0 \\\\\n\\lambda _1=\\lambda _2=3 \\\\\nx_1=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}</script></li>\n</ul>\n<h1 id=\"第二十一讲：对角化和A的幂\"><a href=\"#第二十一讲：对角化和A的幂\" class=\"headerlink\" title=\"第二十一讲：对角化和A的幂\"></a>第二十一讲：对角化和A的幂</h1><h2 id=\"对角化\"><a href=\"#对角化\" class=\"headerlink\" title=\"对角化\"></a>对角化</h2><ul>\n<li>假设A有n个线性无关特征向量，按列组成矩阵S，即特征向量矩阵</li>\n<li>以下所有关于矩阵对角化的讨论都在S可逆，即n个特征向量线性无关的前提下</li>\n<li><script type=\"math/tex; mode=display\">\nAS=A[x_1,x_2...x_n]=[\\lambda _1 x_1,....\\lambda _n x_n] \\\\\n=[x_1,x_2,...x_n]\n\\begin{bmatrix}\n\\lambda _1 & 0 & ... & 0 \\\\\n0 & \\lambda _2 & ... & 0 \\\\\n... & ... & ... & ... \\\\\n0 & 0  & 0 & \\lambda _n \\\\\n\\end{bmatrix} \\\\\n=S \\Lambda \\\\</script></li>\n</ul>\n<ul>\n<li>假设S可逆，即n个特征向量无关，此时可以得到<script type=\"math/tex; mode=display\">\nS^{-1}AS=\\Lambda \\\\\nA=S\\Lambda S^{-1} \\\\</script></li>\n<li>$\\Lambda$是对角矩阵，这里我们得到了除了$A=LU$和$A=QR$之外的一种矩阵分解</li>\n<li><script type=\"math/tex; mode=display\">\nif \\qquad Ax=\\lambda x \\\\\nA^2 x=\\lambda AX=\\lambda ^2 x \\\\\nA^2=S\\Lambda S^{-1} S \\Lambda S^{-1}=S \\Lambda ^2 S^{-1} \\\\</script></li>\n<li>上面关于$A^2$的两式说明平方后特征向量不变，特征值平方，K次方同理</li>\n<li>特征值和特征向量帮助我们理解矩阵幂，当计算矩阵幂时，我们可以把矩阵分解成特征向量矩阵和对角阵相乘的形式，K个相乘两两可以抵消，如上式</li>\n<li>什么样的矩阵的幂趋向于0(稳定)<script type=\"math/tex; mode=display\">\nA^K \\rightarrow 0 \\quad as \\quad K \\rightarrow \\infty \\\\\nif \\quad all |\\lambda _i|<1 \\\\</script></li>\n<li>哪些矩阵可以对角化？<br>如果所有特征值不同，则A可以对角化</li>\n<li>如果矩阵A已经是对角阵，则$\\Lambda$与A相同</li>\n<li>特征值重复的次数称为代数重度，对三角阵，如<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n2 & 1 \\\\\n0 & 2 \\\\\n\\end{bmatrix} \\\\\ndet(A-\\lambda I)=\n\\begin{vmatrix}\n2-\\lambda & 1 \\\\\n0 & 2-\\lambda \\\\\n\\end{vmatrix}=0 \\\\\n\\lambda =2 \\\\\nA-\\lambda I=\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 0 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>对$A-\\lambda I$，几何重数是1，而特征值的代数重度是2</li>\n<li>特征向量只有(1,0)，因此对于三角阵，它不可以对角化，不存在两个线性无关的特征向量。</li>\n</ul>\n<h2 id=\"A的幂\"><a href=\"#A的幂\" class=\"headerlink\" title=\"A的幂\"></a>A的幂</h2><ul>\n<li>多数矩阵拥有互相线性无关的一组特征值，可以对角化。假如可以对角化，我们需要关注如何求解A的幂</li>\n<li><script type=\"math/tex; mode=display\">\ngive \\quad u_0 \\\\\nu_{k+1}=Au_k \\\\\nu_k=A^ku_0 \\\\\nhow \\quad to \\quad solve \\quad u_k \\\\\nu_0=c_1x_1+c_2x_2+...+c_nx_n=SC \\\\\nAu_0=c_1 \\lambda _1 x_1 + c_2 \\lambda _2 x_2 +...+c_n \\lambda _n x_n \\\\\nA^{100}u_0=c_1 \\lambda _1^{100} x_1 + c_2 \\lambda _2^{100} x_2 +...+c_n \\lambda _n^{100} x_n \\\\\n=S\\Lambda ^{100} C \\\\\n=u_{100} \\\\</script></li>\n<li>因为n个特征向量互相不线性相关，因此它们可以作为一组基覆盖整个n维空间，自然$u_0$可以用特征向量的线性组合表示，C是线性系数向量。上式得出了矩阵幂的解法，接下来以斐波那契数列为例<script type=\"math/tex; mode=display\">\nF_0=0 \\\\\nF_1=1 \\\\\nF_2=1 \\\\\nF_3=2 \\\\\nF_4=3 \\\\\nF_5=5 \\\\\n..... \\\\\nF_{100}=? \\\\</script></li>\n<li>斐波那契数列的增长速度有多快?由特征值决定，我们尝试构造向量，来找到斐波那契数列迭代的矩阵关系<script type=\"math/tex; mode=display\">\nF_{k+2}=F_{k+1}+F_k \\\\\nF_{k+1}=F_{k+1} \\\\</script></li>\n<li>定义向量<script type=\"math/tex; mode=display\">\nu_k=\n\\begin{bmatrix}\nF_{k+1} \\\\\nF_k \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>利用这个向量可以将前两个等式写成矩阵形式 <script type=\"math/tex; mode=display\">\nu_{k+1}=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix}\nu_k \\\\\nA=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n\\end{bmatrix} \\\\\n\\lambda =\\frac {1 \\pm \\sqrt 5}2 \\\\</script></li>\n<li>得到两个特征值，我们很容易得到特征向量</li>\n<li>回到斐波那契数列，斐波那契数列的增长速率由我们构造的”数列更新矩阵”的特征值决定，而且由$A^{100}u_0=c_1 \\lambda _1^100 x_1 + c_2 \\lambda _2^100 x_2 +…+c_n \\lambda _n^100 x_n$可以看出增长率主要由由较大的特征值决定，因此$F_{100}$可以写成如下形式<script type=\"math/tex; mode=display\">\nF_{100} \\approx c_1 {\\frac {1 + \\sqrt 5}2}^{100} \\\\</script></li>\n<li>再有初始值有<script type=\"math/tex; mode=display\">\nu_0=\n\\begin{bmatrix}\nF_1 \\\\\nF_0 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}\n=c_1x_1+c_2x_2</script></li>\n<li>其中$x_1,x_2$是两个特征向量，线性系数可求，代入公式可求$F_{100}$的近似值</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>我们发现在A可逆的情况下，A可以分解成$S\\Lambda S^{-1}$的形式</li>\n<li>这种形式有一个特点，方便求A的幂，即分解后可以看出A的幂的特征值单位矩阵是A的特征值单位矩阵的幂</li>\n<li>我们在求解斐波那契数列中尝试运用此特点，首先将数列的更新转换为矩阵形式</li>\n<li>求出矩阵的特征值，特征向量</li>\n<li>由A的幂的展开式可以看出A的幂主要由较大的特征值决定，因此$F_{100}$可以写成$F_{100} \\approx c_1 {(\\frac {1 + \\sqrt 5}2)}^{100}$的形式</li>\n<li>由初始值$F_0$求出线性系数，代入上式，得到$F_{100}$的近似值</li>\n<li>以上是差分方程的一个例子，下一节将讨论微分方程</li>\n</ul>\n<h1 id=\"第二十二讲：微分方程和exp-At\"><a href=\"#第二十二讲：微分方程和exp-At\" class=\"headerlink\" title=\"第二十二讲：微分方程和exp(At)\"></a>第二十二讲：微分方程和exp(At)</h1><h2 id=\"微分方程\"><a href=\"#微分方程\" class=\"headerlink\" title=\"微分方程\"></a>微分方程</h2><ul>\n<li>常系数线性方程的解是指数形式的，如果微分方程的解是指数形式，只需利用线代求出指数，系数，就可以求出解</li>\n<li>举个例子<script type=\"math/tex; mode=display\">\n\\frac{du_1}{dt}=-u_1+2u_2 \\\\\n\\frac{du_2}{dt}=u_1-2u_2 \\\\\nu(0)=\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>首先我们列出系数矩阵，并找出矩阵的特征值和特征向量<script type=\"math/tex; mode=display\">\nA=\n\\begin{bmatrix}\n-1 & 2 \\\\\n1 & -2 \\\\\n\\end{bmatrix}</script></li>\n<li>易得$\\lambda=0$是这个奇异矩阵的一个解，由迹可以看出第二个特征值是$\\lambda=-3$，并得到两个特征向量<script type=\"math/tex; mode=display\">\nx_1=\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n\\end{bmatrix} \\\\\nx_2=\n\\begin{bmatrix}\n1 \\\\\n-1 \\\\\n\\end{bmatrix}</script></li>\n<li>微分方程解的通解形式将是<script type=\"math/tex; mode=display\">\nu(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2</script></li>\n<li>为什么？<script type=\"math/tex; mode=display\">\n\\frac{du}{dt} \\\\\n=c_1 \\lambda _1 e^{\\lambda _1 t}x_1 \\\\\n=A c_1 e^{\\lambda _1 t}x_1 \\\\\nbecause \\quad A x_1=\\lambda _1 x_1 \\\\</script></li>\n<li>在差分方程$u_{k+1}=Au_k$当中，解的形式是$c_1\\lambda _1 ^k x_1+c_2 \\lambda _2 ^k x_2$</li>\n<li>在微分方程$\\frac {du}{dt}=Au$当中，解的形式是$u(t)=c_1e^{\\lambda _1 t}x_1+c_1e^{\\lambda _2 t}x_2$</li>\n<li>$c_1,c_2$由初始值解出，即系数矩阵C乘特征向量矩阵S得到初始值</li>\n<li>可以看出t趋于无穷时，例子方程的解只剩下稳态部分，即$(\\frac 23,\\frac 13)$</li>\n<li>什么时候解趋向于0？存在负数特征值，因为$e^{\\lambda t}$需要趋向于0</li>\n<li>如果特征值是复数呢？虚数部分的模值是1，所以如果复数的实数部分是负数，解依然趋向于0</li>\n<li>什么时候存在稳态？特征值中只存在0和负数，就如上面的例子</li>\n<li>什么时候解无法收敛？任何特征值的实数部分大于0</li>\n<li>改变系数矩阵的符号，特征值也改变符号，稳态的解依然稳态，收敛的解就会变成发散</li>\n<li>如何从矩阵直接判断解是否收敛？即特征值的实数部分都小于0？</li>\n<li>矩阵的迹应该小于0，但对角线之和为0依然不一定收敛，如<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-2 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}</script></li>\n<li>因此还需要另一个条件：行列式的值是特征值乘积，因此行列式的值应该大于0</li>\n</ul>\n<h2 id=\"exp-At\"><a href=\"#exp-At\" class=\"headerlink\" title=\"exp(At)\"></a>exp(At)</h2><ul>\n<li>是否可以把解表示成$S,\\Lambda$的形式</li>\n<li>矩阵A表示$u_1,u_2$耦合，首先我们需要将u对角化，解耦</li>\n<li><script type=\"math/tex; mode=display\">\n\\frac{du}{dt} = Au \\\\\nset \\quad u=Sv \\\\\nS \\frac{dv}{dt} = ASv \\\\\n\\frac{dv}{dt}=S^{-1}ASv=\\Lambda v \\\\\nv(t)=e^{\\Lambda t}v(0) \\\\\nu(t)=Se^{\\Lambda t}S^{-1}u(0) \\\\</script></li>\n</ul>\n<h1 id=\"第二十一讲：马尔科夫矩阵-傅立叶级数\"><a href=\"#第二十一讲：马尔科夫矩阵-傅立叶级数\" class=\"headerlink\" title=\"第二十一讲：马尔科夫矩阵;傅立叶级数\"></a>第二十一讲：马尔科夫矩阵;傅立叶级数</h1><h2 id=\"马尔科夫矩阵\"><a href=\"#马尔科夫矩阵\" class=\"headerlink\" title=\"马尔科夫矩阵\"></a>马尔科夫矩阵</h2><ul>\n<li>一个典型的马尔科夫矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0.1 & 0.01 & 0.3 \\\\\n0.2 & 0.99 & 0.3 \\\\\n0.7 & 0 & 0.4 \\\\\n\\end{bmatrix}</script></li>\n<li>每一个元素大于等于0，每一列之和为1，马尔科夫矩阵的幂都是马尔科夫矩阵</li>\n<li>$\\lambda=1$是一个特征值，其余的特征值的绝对值都小于1</li>\n</ul>\n<ul>\n<li>在上一讲中我们谈到矩阵的幂可以分解为<script type=\"math/tex; mode=display\">\nu_k=A^ku_0=c_1\\lambda _1 ^kx_1+c_2\\lambda _2 ^kx_2+.....</script></li>\n<li>当A是马尔科夫矩阵时，只有一个特征值为1，其余特征值小于1，随着k的变大，小于1的特征值所在项趋向于0，只保留特征值为1的那一项，同时对应的特征向量的元素都大于0</li>\n<li>当每一列和为1时，必然存在一个特征值$\\lambda =1$</li>\n<li>证明：<script type=\"math/tex; mode=display\">\nA-I=\n\\begin{bmatrix}\n-0.9 & 0.01 & 0.3 \\\\\n0.2 & -0.01 & 0.3 \\\\\n0.7 & 0 & -0.6 \\\\\n\\end{bmatrix}</script></li>\n<li>若1是一个特征值，则$A-I$应该是奇异的，可以看到$A-I$每一列和为0，即说明行向量线性相关，即矩阵奇异,同时全1向量在左零空间。</li>\n<li>对于马尔科夫矩阵A，我们研究$u_{k+1}=Au_k$</li>\n<li>一个例子，u是麻省和加州的人数，A是人口流动矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nu_{cal} \\\\\nu_{mass} \\\\\n\\end{bmatrix}_{t=k+1}\n=\n\\begin{bmatrix}\n0.9 & 0.2 \\\\\n0.1 & 0.8 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{cal} \\\\\nu_{mass} \\\\\n\\end{bmatrix}_{t=k}</script></li>\n<li>可以看到每一年(k)80%的人留在麻省，20%的人前往加州，加州那边也有10%移居麻省</li>\n<li>对马尔科夫矩阵A<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0.9 & 0.2 \\\\\n0.1 & 0.8 \\\\\n\\end{bmatrix} \\\\\n\\lambda _1 =1 \\\\\n\\lambda _2 =0.7 \\\\</script></li>\n<li>对特征值为1的项，容易求出特征向量为$(2,1)$，对特征值为0.7的项，特征向量为(-1,1)</li>\n<li>得到我们要研究的公式<script type=\"math/tex; mode=display\">\nu_k=c_1\\*1^k\\*\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n\\end{bmatrix}\n+c_2\\*(0.7)^k\\*\n\\begin{bmatrix}\n-1 \\\\\n1 \\\\\n\\end{bmatrix}</script></li>\n<li>假设一开始加州有0人，麻省有1000人，即$u_0$，代入公式可以得到$c_1,c_2$，可以看到很多年之后，加州和麻省的人数将稳定，各占1000人中的三分之一和三分之二。</li>\n<li>行向量为和为1是另外一种定义马尔科夫矩阵的方式</li>\n</ul>\n<h2 id=\"傅里叶级数\"><a href=\"#傅里叶级数\" class=\"headerlink\" title=\"傅里叶级数\"></a>傅里叶级数</h2><ul>\n<li>先讨论带有标准正交基的投影问题</li>\n<li>假设$q_1….q_n$是一组标准正交基，任何向量$v$都是这组基的线性组合</li>\n<li>现在我们要求出线性组合系数$x_1….x_n$<br>$v=x_1q_1+x_2q_2+…x_nq_n$<br>一种方法是将$v$与$q_i$做内积，逐一求出系数<script type=\"math/tex; mode=display\">\nq_1^Tv=x_1q_1^Tq_1+0+0+0....+0=x_1 \\\\</script></li>\n<li>写成矩阵形式<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n... \\\\\nx_n \\\\\n\\end{bmatrix}=\nv \\\\\nQx=v \\\\\nx=Q^{-1}v=Q^Tv \\\\</script></li>\n<li>现在讨论傅里叶级数</li>\n<li>我们希望将函数分解<script type=\"math/tex; mode=display\">\nf(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2cos2x+.......</script></li>\n<li>关键是，在这种分解中，$coskx,sinkx$构成一组函数空间的无穷正交基，即这些函数内积为0(向量的内积是离散的值累加，函数的内积是连续的值积分)。</li>\n<li>如何求出傅里叶系数？</li>\n<li>利用之前的向量例子来求</li>\n<li>将$f(x)$逐一与正交基元素内积，得到这个正交基元素对应的系数乘$\\pi$，例如<script type=\"math/tex; mode=display\">\n\\int _0 ^{2\\pi} f(x)cosx dx=0+ a_1 \\int _0^{2\\pi}(cosx)^2dx+0+0...+0=\\pi a_1 \\\\</script></li>\n</ul>\n<h1 id=\"第二十二讲：对称矩阵及其正定性\"><a href=\"#第二十二讲：对称矩阵及其正定性\" class=\"headerlink\" title=\"第二十二讲：对称矩阵及其正定性\"></a>第二十二讲：对称矩阵及其正定性</h1><h2 id=\"对称矩阵\"><a href=\"#对称矩阵\" class=\"headerlink\" title=\"对称矩阵\"></a>对称矩阵</h2><ul>\n<li>对称矩阵的特征值是实数，不重复的特征值对应的特征向量互相正交</li>\n<li>对一般矩阵$A=S\\Lambda S^{-1}$，S为特征向量矩阵</li>\n<li>对对称矩阵$A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T$，Q为标准正交的特征向量矩阵</li>\n<li>为什么特征值都是实数？</li>\n<li>$Ax=\\lambda x$对左右同时取共轭，因为我们现在只考虑实数矩阵A，$Ax^{*}=\\lambda ^{*} x^{*}$</li>\n<li>即$\\lambda$和它的共轭都是特征值，现在再对等式两边取转置，$x^{* T}A^T=x^{* T} \\lambda ^{* T} $</li>\n<li>上式中$A=A^T$，且两边同乘以$x$，与$x^{* T}A\\lambda x^{* T}x$对比可得$\\lambda ^{*}=\\lambda$，即特征值是实数</li>\n<li>可见，对于复数矩阵，需要$A=A^{* T}$才满足对称</li>\n<li>对于对称矩阵<script type=\"math/tex; mode=display\">\nA=Q\\Lambda Q^{-1}=Q\\Lambda Q^T \\\\\n=\\lambda _1 q_1 q_1^T+\\lambda _2 q_2 q_2^T+.... \\\\</script></li>\n<li>所以每一个对称矩阵都是一些互相垂直的投影矩阵的组合</li>\n<li>对于对称矩阵，正主元的个数等于正特征值的个数，且主元的乘积等于特征值的乘积等于矩阵的行列式</li>\n</ul>\n<h2 id=\"正定性\"><a href=\"#正定性\" class=\"headerlink\" title=\"正定性\"></a>正定性</h2><ul>\n<li>正定矩阵都是对称矩阵，是对称矩阵的一个子类，其所有特征值为正数，所有主元为正数，所有的子行列式都是正数</li>\n<li>特征值的符号与稳定性有关</li>\n<li>主元、行列式、特征值三位一体，线性代数将其统一</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"MIT线性代数笔记3","path":"2017/01/22/LinearAlgebra3/","eyeCatchImage":null,"excerpt":"<hr>\n<h1 id=\"第十七讲：行列式及其性质\"><a href=\"#第十七讲：行列式及其性质\" class=\"headerlink\" title=\"第十七讲：行列式及其性质\"></a>第十七讲：行列式及其性质</h1><h2 id=\"行列式\"><a href=\"#行列式\" class=\"headerlink\" title=\"行列式\"></a>行列式</h2><ul>\n<li>矩阵A的行列式是与矩阵相关的一个数，记作$detA或者|A|$</li>\n<li>行列式的性质<ul>\n<li>$detI=1$</li>\n<li>交换行，行列式的值的符号会相反</li>\n<li>一个置换矩阵的行列式是1或-1，取决于交换行次数的奇偶</li>\n<li>两行相等使得行列式为0(由性质二可以直接推出)</li>\n<li>矩阵消元不改变其行列式(证明见下)</li>\n<li>某一行为0，行列式为0(与0相乘等价于某一行为0，结果为0)</li>\n<li>$detA=0$当且仅当A是奇异矩阵</li>\n<li>$det(A+B) \\neq detA+detB \\\\ detAB=(detA)(detB)$</li>\n<li>$detA^{-1}detA=1$</li>\n<li>$detA^2=(detA)^2$</li>\n<li>$det2A=2^n detA$</li>\n<li>$detA^T=detA$(证明见下)</li></ul></li></ul>","date":"2017-01-22T11:21:02.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"论文阅读笔记2018上半年","date":"2018-03-07T02:20:14.000Z","author":"Thinkwee","mathjax":true,"_content":"\n论文阅读笔记\n主要关注自动文摘方向\n<!--more--> \n\n![i0o00J.jpg](https://s1.ax1x.com/2018/10/20/i0o00J.jpg)\n\n# Neural Machine Translation By Jointly Learning To Align And Translate\n-\t发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。\n-\t编码器解码器模型，翻译任务。\n-\t其中双向GRU做编码器。编码隐藏层向量由双向连接而成。\n-\t生成每一个单词时有不同的表示。\n-\t权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。\n-\t对所有步编码隐藏层向量加权生成表示。\n\t![i0oB79.png](https://s1.ax1x.com/2018/10/20/i0oB79.png)\n\n# Effective Approaches to Attention-based Neural Machine Translation\n-\t发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。\n-\t其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：\n\t$$\n\th_t = tanh(W_c[c_t;h_t])\n\t$$\n\t之后注意力向量过softmax生成概率分布。\n\t\n## 全局注意力\n-\t文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。\n\t![i0oH9P.png](https://s1.ax1x.com/2018/10/20/i0oH9P.png)\n-\t之后引入了两种Effective Approaches，即局部注意力和input-feeding。\n\n## 局部注意力\n-\t局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：\n -\t单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。\n -\t预测对齐：训练对齐位置。\n\t$$\n\tp_t = S \\cdots sigmoid(v_p^T tanh(W_ph_t)) \\\\\n\t$$\n\t其中$h_t$是第t个生成单词的隐藏层向量\n\t$W_p$和$v_p$都是需要训练的权重\n\tS是输入单词长度,与sigmoid相乘就得到输入句中任意位置\n-\t另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。\n\t$$\n\ta_t(s) = align(h_t,h_s)exp(-\\frac{(s-p_t)^2}{2\\sigma ^2})\n\t$$\n\t![i0ost1.png](https://s1.ax1x.com/2018/10/20/i0ost1.png)\n\n\n## Input-feeding\n-\tInput-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。\n-\t实验结果表明使用预测对齐的局部注意力模型表现最好。\n\t![i0oyfx.png](https://s1.ax1x.com/2018/10/20/i0oyfx.png)\n\n# A Neural Attention Model for Abstractive Sentence Summarization\n-\t发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。\n-\t提出了一种注意力编码器，使用普通的NNLM解码器。\n-\t未使用RNN，直接用词向量。\n-\t使用全部输入信息,局部输出信息(yc)构建注意力权重。\n-\t直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。\n-\t模型如下图:\n\t![i0ocp6.png](https://s1.ax1x.com/2018/10/20/i0ocp6.png)\n\t\n# Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\n-\t发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。\n-\t基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。\n-\t改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。\n\n## LVT\n-\t减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。\n\n## Feature-rich Encoder\n-\t不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量\n\t![i0og1K.png](https://s1.ax1x.com/2018/10/20/i0og1K.png)\n\t\n## Switching Generator-pointer\n-\t解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。\n\t![i0oRXD.png](https://s1.ax1x.com/2018/10/20/i0oRXD.png))\n\tSwitching generator/pointer model\n\t开关为G时就用传统方法生成文摘\n\t开关为P时就从输入中拷贝单词到文摘中\n\t\n## 分层注意力\n-\t传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。\n\t![i0ofne.png](https://s1.ax1x.com/2018/10/20/i0ofne.png)\n# Recurrent Neural Network Regularization\n-\t本文介绍了如何在循环神经网络中使用dropout来防止过拟合\n-\tDropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。\n-\t对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。\n-\t在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。\n-\t作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息\n-\t效果如图:\n\t![i0ov7j.png](https://s1.ax1x.com/2018/10/20/i0ov7j.png)\n\n# Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n-\t本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。\n-\t训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。\n-\t分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。\n\t![i0TC90.png](https://s1.ax1x.com/2018/10/20/i0TC90.png)\n\t三段式对话包含在两个词水平RNN端到端系统中\n\t中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量\n-\t文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。\n-\t系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度\n-\t论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。\n\n# News Event Summarization Complemented by Micropoints\n-\t这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。\n-\t文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。\n-\t这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。\n-\tMicropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。\n-\t筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。\n-\t将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), ..., p(topic n |t))，最后使用DBSCAN完成主题聚类。\n-\t使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。\n-\t现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。\n-\t得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。\n\n# DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\n-\t更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充\n-\t作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。\n-\t作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。\n-\t作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。\n-\t传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。\n\t![i0ojBQ.png](https://s1.ax1x.com/2018/10/20/i0ojBQ.png)\n\t右边是多维度注意力\n\t可以看到注意力权重变成了向量，与输入词向量维度数相同\n-\t一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。\n-\t有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：\n\t![i0ozAs.png](https://s1.ax1x.com/2018/10/20/i0ozAs.png)\n-\t最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。\n\n# Neural Summarization by Extracting Sentences and Words\n-\t本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。\n-\t与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。\n-\t因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。\n-\t编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。\n-\t句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。\n-\t与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。\n-\t与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。\n-\t抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。\n-\t抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。\n\n# A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\n-\t使用强化学习来优化当前的端到端生成式文摘模型\n\t![i0TicT.png](https://s1.ax1x.com/2018/10/20/i0TicT.png)\n-\t解决长文摘生成和重复短语问题\n-\t强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘\n-\t模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励\n-\t编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。\n-\t编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。\n-\t解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。\n-\t在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。\n-\t在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。\n-\t之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。\n-\t因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：\n\t$$\n\tL_{rl} = (r(y)-r(y^s))\\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)\n\t$$\n\tr是人工评价奖励函数\n\t两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子\n\t目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）\n-\t之后作者将监督学习和强化学习的两种目标函数结合起来：\n\n# Distributed Representations of Words and Phrases and their Compositionality\n-\t介绍了w2v的负采样版本。\n-\t以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。\n-\t用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：\n\t$$\n\tp(w_O | w_I) = \\frac {exp(v_{w_O}^T v_{w_I})}{\\sum _{w=1}^W exp(v_{w_O}^T v_{w_I})}\n\t$$\n\t被替换成\n\t$$\n\tlog \\sigma (v_{w_O}^T v_{w_I}) + \\sum _{i=1}^k E_{w_i \\sim P_n(w)} [log \\sigma (v_{w_O}^T v_{w_I})]\n-\t每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。\n-\t对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以一定概率跳过：\n-\t这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。","source":"_posts/PaperReading.md","raw":"---\ntitle: 论文阅读笔记2018上半年\ndate: 2018-03-07 10:20:14\ntags:\n  - abstractive summarization\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\n---\n\n论文阅读笔记\n主要关注自动文摘方向\n<!--more--> \n\n![i0o00J.jpg](https://s1.ax1x.com/2018/10/20/i0o00J.jpg)\n\n# Neural Machine Translation By Jointly Learning To Align And Translate\n-\t发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。\n-\t编码器解码器模型，翻译任务。\n-\t其中双向GRU做编码器。编码隐藏层向量由双向连接而成。\n-\t生成每一个单词时有不同的表示。\n-\t权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。\n-\t对所有步编码隐藏层向量加权生成表示。\n\t![i0oB79.png](https://s1.ax1x.com/2018/10/20/i0oB79.png)\n\n# Effective Approaches to Attention-based Neural Machine Translation\n-\t发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。\n-\t其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：\n\t$$\n\th_t = tanh(W_c[c_t;h_t])\n\t$$\n\t之后注意力向量过softmax生成概率分布。\n\t\n## 全局注意力\n-\t文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。\n\t![i0oH9P.png](https://s1.ax1x.com/2018/10/20/i0oH9P.png)\n-\t之后引入了两种Effective Approaches，即局部注意力和input-feeding。\n\n## 局部注意力\n-\t局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：\n -\t单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。\n -\t预测对齐：训练对齐位置。\n\t$$\n\tp_t = S \\cdots sigmoid(v_p^T tanh(W_ph_t)) \\\\\n\t$$\n\t其中$h_t$是第t个生成单词的隐藏层向量\n\t$W_p$和$v_p$都是需要训练的权重\n\tS是输入单词长度,与sigmoid相乘就得到输入句中任意位置\n-\t另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。\n\t$$\n\ta_t(s) = align(h_t,h_s)exp(-\\frac{(s-p_t)^2}{2\\sigma ^2})\n\t$$\n\t![i0ost1.png](https://s1.ax1x.com/2018/10/20/i0ost1.png)\n\n\n## Input-feeding\n-\tInput-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。\n-\t实验结果表明使用预测对齐的局部注意力模型表现最好。\n\t![i0oyfx.png](https://s1.ax1x.com/2018/10/20/i0oyfx.png)\n\n# A Neural Attention Model for Abstractive Sentence Summarization\n-\t发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。\n-\t提出了一种注意力编码器，使用普通的NNLM解码器。\n-\t未使用RNN，直接用词向量。\n-\t使用全部输入信息,局部输出信息(yc)构建注意力权重。\n-\t直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。\n-\t模型如下图:\n\t![i0ocp6.png](https://s1.ax1x.com/2018/10/20/i0ocp6.png)\n\t\n# Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\n-\t发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。\n-\t基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。\n-\t改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。\n\n## LVT\n-\t减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。\n\n## Feature-rich Encoder\n-\t不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量\n\t![i0og1K.png](https://s1.ax1x.com/2018/10/20/i0og1K.png)\n\t\n## Switching Generator-pointer\n-\t解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。\n\t![i0oRXD.png](https://s1.ax1x.com/2018/10/20/i0oRXD.png))\n\tSwitching generator/pointer model\n\t开关为G时就用传统方法生成文摘\n\t开关为P时就从输入中拷贝单词到文摘中\n\t\n## 分层注意力\n-\t传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。\n\t![i0ofne.png](https://s1.ax1x.com/2018/10/20/i0ofne.png)\n# Recurrent Neural Network Regularization\n-\t本文介绍了如何在循环神经网络中使用dropout来防止过拟合\n-\tDropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。\n-\t对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。\n-\t在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。\n-\t作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息\n-\t效果如图:\n\t![i0ov7j.png](https://s1.ax1x.com/2018/10/20/i0ov7j.png)\n\n# Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n-\t本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。\n-\t训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。\n-\t分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。\n\t![i0TC90.png](https://s1.ax1x.com/2018/10/20/i0TC90.png)\n\t三段式对话包含在两个词水平RNN端到端系统中\n\t中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量\n-\t文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。\n-\t系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度\n-\t论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。\n\n# News Event Summarization Complemented by Micropoints\n-\t这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。\n-\t文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。\n-\t这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。\n-\tMicropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。\n-\t筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。\n-\t将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), ..., p(topic n |t))，最后使用DBSCAN完成主题聚类。\n-\t使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。\n-\t现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。\n-\t得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。\n\n# DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\n-\t更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充\n-\t作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。\n-\t作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。\n-\t作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。\n-\t传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。\n\t![i0ojBQ.png](https://s1.ax1x.com/2018/10/20/i0ojBQ.png)\n\t右边是多维度注意力\n\t可以看到注意力权重变成了向量，与输入词向量维度数相同\n-\t一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。\n-\t有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：\n\t![i0ozAs.png](https://s1.ax1x.com/2018/10/20/i0ozAs.png)\n-\t最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。\n\n# Neural Summarization by Extracting Sentences and Words\n-\t本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。\n-\t与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。\n-\t因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。\n-\t编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。\n-\t句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。\n-\t与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。\n-\t与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。\n-\t抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。\n-\t抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。\n\n# A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\n-\t使用强化学习来优化当前的端到端生成式文摘模型\n\t![i0TicT.png](https://s1.ax1x.com/2018/10/20/i0TicT.png)\n-\t解决长文摘生成和重复短语问题\n-\t强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘\n-\t模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励\n-\t编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。\n-\t编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。\n-\t解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。\n-\t在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。\n-\t在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。\n-\t之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。\n-\t因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：\n\t$$\n\tL_{rl} = (r(y)-r(y^s))\\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)\n\t$$\n\tr是人工评价奖励函数\n\t两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子\n\t目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）\n-\t之后作者将监督学习和强化学习的两种目标函数结合起来：\n\n# Distributed Representations of Words and Phrases and their Compositionality\n-\t介绍了w2v的负采样版本。\n-\t以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。\n-\t用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：\n\t$$\n\tp(w_O | w_I) = \\frac {exp(v_{w_O}^T v_{w_I})}{\\sum _{w=1}^W exp(v_{w_O}^T v_{w_I})}\n\t$$\n\t被替换成\n\t$$\n\tlog \\sigma (v_{w_O}^T v_{w_I}) + \\sum _{i=1}^k E_{w_i \\sim P_n(w)} [log \\sigma (v_{w_O}^T v_{w_I})]\n-\t每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。\n-\t对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以一定概率跳过：\n-\t这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。","slug":"PaperReading","published":1,"updated":"2019-07-22T03:45:22.869Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v9o0040q8t5bgxnearj","content":"<p>论文阅读笔记<br>主要关注自动文摘方向<br><a id=\"more\"></a> </p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o00J.jpg\" alt=\"i0o00J.jpg\"></p>\n<h1 id=\"Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\"><a href=\"#Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\" class=\"headerlink\" title=\"Neural Machine Translation By Jointly Learning To Align And Translate\"></a>Neural Machine Translation By Jointly Learning To Align And Translate</h1><ul>\n<li>发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。</li>\n<li>编码器解码器模型，翻译任务。</li>\n<li>其中双向GRU做编码器。编码隐藏层向量由双向连接而成。</li>\n<li>生成每一个单词时有不同的表示。</li>\n<li>权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。</li>\n<li>对所有步编码隐藏层向量加权生成表示。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oB79.png\" alt=\"i0oB79.png\"></li>\n</ul>\n<h1 id=\"Effective-Approaches-to-Attention-based-Neural-Machine-Translation\"><a href=\"#Effective-Approaches-to-Attention-based-Neural-Machine-Translation\" class=\"headerlink\" title=\"Effective Approaches to Attention-based Neural Machine Translation\"></a>Effective Approaches to Attention-based Neural Machine Translation</h1><ul>\n<li>发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。</li>\n<li>其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：<script type=\"math/tex; mode=display\">\nh_t = tanh(W_c[c_t;h_t])</script>之后注意力向量过softmax生成概率分布。</li>\n</ul>\n<h2 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h2><ul>\n<li>文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oH9P.png\" alt=\"i0oH9P.png\"></li>\n<li>之后引入了两种Effective Approaches，即局部注意力和input-feeding。</li>\n</ul>\n<h2 id=\"局部注意力\"><a href=\"#局部注意力\" class=\"headerlink\" title=\"局部注意力\"></a>局部注意力</h2><ul>\n<li>局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：<ul>\n<li>单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。</li>\n<li>预测对齐：训练对齐位置。<script type=\"math/tex; mode=display\">\np_t = S \\cdots sigmoid(v_p^T tanh(W_ph_t)) \\\\</script>其中$h_t$是第t个生成单词的隐藏层向量<br>$W_p$和$v_p$都是需要训练的权重<br>S是输入单词长度,与sigmoid相乘就得到输入句中任意位置</li>\n</ul>\n</li>\n<li>另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。<script type=\"math/tex; mode=display\">\na_t(s) = align(h_t,h_s)exp(-\\frac{(s-p_t)^2}{2\\sigma ^2})</script><img src=\"https://s1.ax1x.com/2018/10/20/i0ost1.png\" alt=\"i0ost1.png\"></li>\n</ul>\n<h2 id=\"Input-feeding\"><a href=\"#Input-feeding\" class=\"headerlink\" title=\"Input-feeding\"></a>Input-feeding</h2><ul>\n<li>Input-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。</li>\n<li>实验结果表明使用预测对齐的局部注意力模型表现最好。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oyfx.png\" alt=\"i0oyfx.png\"></li>\n</ul>\n<h1 id=\"A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Neural Attention Model for Abstractive Sentence Summarization\"></a>A Neural Attention Model for Abstractive Sentence Summarization</h1><ul>\n<li>发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。</li>\n<li>提出了一种注意力编码器，使用普通的NNLM解码器。</li>\n<li>未使用RNN，直接用词向量。</li>\n<li>使用全部输入信息,局部输出信息(yc)构建注意力权重。</li>\n<li>直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。</li>\n<li>模型如下图:<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ocp6.png\" alt=\"i0ocp6.png\"></li>\n</ul>\n<h1 id=\"Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\"><a href=\"#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\" class=\"headerlink\" title=\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\"></a>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</h1><ul>\n<li>发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。</li>\n<li>基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。</li>\n<li>改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。</li>\n</ul>\n<h2 id=\"LVT\"><a href=\"#LVT\" class=\"headerlink\" title=\"LVT\"></a>LVT</h2><ul>\n<li>减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。</li>\n</ul>\n<h2 id=\"Feature-rich-Encoder\"><a href=\"#Feature-rich-Encoder\" class=\"headerlink\" title=\"Feature-rich Encoder\"></a>Feature-rich Encoder</h2><ul>\n<li>不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量<br><img src=\"https://s1.ax1x.com/2018/10/20/i0og1K.png\" alt=\"i0og1K.png\"></li>\n</ul>\n<h2 id=\"Switching-Generator-pointer\"><a href=\"#Switching-Generator-pointer\" class=\"headerlink\" title=\"Switching Generator-pointer\"></a>Switching Generator-pointer</h2><ul>\n<li>解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oRXD.png\" alt=\"i0oRXD.png\">)<br>Switching generator/pointer model<br>开关为G时就用传统方法生成文摘<br>开关为P时就从输入中拷贝单词到文摘中</li>\n</ul>\n<h2 id=\"分层注意力\"><a href=\"#分层注意力\" class=\"headerlink\" title=\"分层注意力\"></a>分层注意力</h2><ul>\n<li>传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ofne.png\" alt=\"i0ofne.png\"><h1 id=\"Recurrent-Neural-Network-Regularization\"><a href=\"#Recurrent-Neural-Network-Regularization\" class=\"headerlink\" title=\"Recurrent Neural Network Regularization\"></a>Recurrent Neural Network Regularization</h1></li>\n<li>本文介绍了如何在循环神经网络中使用dropout来防止过拟合</li>\n<li>Dropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。</li>\n<li>对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。</li>\n<li>在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。</li>\n<li>作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息</li>\n<li>效果如图:<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ov7j.png\" alt=\"i0ov7j.png\"></li>\n</ul>\n<h1 id=\"Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\"><a href=\"#Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\" class=\"headerlink\" title=\"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\"></a>Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</h1><ul>\n<li>本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。</li>\n<li>训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。</li>\n<li>分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TC90.png\" alt=\"i0TC90.png\"><br>三段式对话包含在两个词水平RNN端到端系统中<br>中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量</li>\n<li>文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。</li>\n<li>系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度</li>\n<li>论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。</li>\n</ul>\n<h1 id=\"News-Event-Summarization-Complemented-by-Micropoints\"><a href=\"#News-Event-Summarization-Complemented-by-Micropoints\" class=\"headerlink\" title=\"News Event Summarization Complemented by Micropoints\"></a>News Event Summarization Complemented by Micropoints</h1><ul>\n<li>这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。</li>\n<li>文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。</li>\n<li>这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。</li>\n<li>Micropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。</li>\n<li>筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。</li>\n<li>将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), …, p(topic n |t))，最后使用DBSCAN完成主题聚类。</li>\n<li>使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。</li>\n<li>现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。</li>\n<li>得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。</li>\n</ul>\n<h1 id=\"DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\"><a href=\"#DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\" class=\"headerlink\" title=\"DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\"></a>DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</h1><ul>\n<li>更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充</li>\n<li>作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。</li>\n<li>作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。</li>\n<li>作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。</li>\n<li>传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ojBQ.png\" alt=\"i0ojBQ.png\"><br>右边是多维度注意力<br>可以看到注意力权重变成了向量，与输入词向量维度数相同</li>\n<li>一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。</li>\n<li>有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ozAs.png\" alt=\"i0ozAs.png\"></li>\n<li>最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。</li>\n</ul>\n<h1 id=\"Neural-Summarization-by-Extracting-Sentences-and-Words\"><a href=\"#Neural-Summarization-by-Extracting-Sentences-and-Words\" class=\"headerlink\" title=\"Neural Summarization by Extracting Sentences and Words\"></a>Neural Summarization by Extracting Sentences and Words</h1><ul>\n<li>本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。</li>\n<li>与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。</li>\n<li>因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。</li>\n<li>编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。</li>\n<li>句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。</li>\n<li>与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。</li>\n<li>与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。</li>\n<li>抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。</li>\n<li>抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。</li>\n</ul>\n<h1 id=\"A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\"><a href=\"#A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\" class=\"headerlink\" title=\"A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\"></a>A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1><ul>\n<li>使用强化学习来优化当前的端到端生成式文摘模型<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TicT.png\" alt=\"i0TicT.png\"></li>\n<li>解决长文摘生成和重复短语问题</li>\n<li>强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘</li>\n<li>模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励</li>\n<li>编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。</li>\n<li>编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。</li>\n<li>解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。</li>\n<li>在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。</li>\n<li>在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。</li>\n<li>之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。</li>\n<li>因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：<script type=\"math/tex; mode=display\">\nL_{rl} = (r(y)-r(y^s))\\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)</script>r是人工评价奖励函数<br>两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子<br>目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）</li>\n<li>之后作者将监督学习和强化学习的两种目标函数结合起来：</li>\n</ul>\n<h1 id=\"Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\"><a href=\"#Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\" class=\"headerlink\" title=\"Distributed Representations of Words and Phrases and their Compositionality\"></a>Distributed Representations of Words and Phrases and their Compositionality</h1><ul>\n<li>介绍了w2v的负采样版本。</li>\n<li>以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。</li>\n<li>用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：<script type=\"math/tex; mode=display\">\np(w_O | w_I) = \\frac {exp(v_{w_O}^T v_{w_I})}{\\sum _{w=1}^W exp(v_{w_O}^T v_{w_I})}</script>被替换成<br>$$<br>log \\sigma (v_{w_O}^T v_{w_I}) + \\sum _{i=1}^k E_{w_i \\sim P_n(w)} [log \\sigma (v_{w_O}^T v_{w_I})]</li>\n<li>每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。</li>\n<li>对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以一定概率跳过：</li>\n<li>这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>论文阅读笔记<br>主要关注自动文摘方向<br></p>","more":"<p></p>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o00J.jpg\" alt=\"i0o00J.jpg\"></p>\n<h1 id=\"Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\"><a href=\"#Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate\" class=\"headerlink\" title=\"Neural Machine Translation By Jointly Learning To Align And Translate\"></a>Neural Machine Translation By Jointly Learning To Align And Translate</h1><ul>\n<li>发布于2015.5(ICLR2015)，作者Dzmitry Bahdanau。</li>\n<li>编码器解码器模型，翻译任务。</li>\n<li>其中双向GRU做编码器。编码隐藏层向量由双向连接而成。</li>\n<li>生成每一个单词时有不同的表示。</li>\n<li>权重由所有步编码隐藏层向量和前一步的解码隐藏层向量决定。</li>\n<li>对所有步编码隐藏层向量加权生成表示。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oB79.png\" alt=\"i0oB79.png\"></li>\n</ul>\n<h1 id=\"Effective-Approaches-to-Attention-based-Neural-Machine-Translation\"><a href=\"#Effective-Approaches-to-Attention-based-Neural-Machine-Translation\" class=\"headerlink\" title=\"Effective Approaches to Attention-based Neural Machine Translation\"></a>Effective Approaches to Attention-based Neural Machine Translation</h1><ul>\n<li>发布于2015.8，作者（Minh-Thang Luong）使用RNN编码器解码器模型，翻译任务。</li>\n<li>其中解码时是将包含注意力的表示和目标单词对应的解码隐藏层向量连接再经激活函数生成注意力向量：<script type=\"math/tex; mode=display\">\nh_t = tanh(W_c[c_t;h_t])</script>之后注意力向量过softmax生成概率分布。</li>\n</ul>\n<h2 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h2><ul>\n<li>文章先介绍全局注意力模型,即对全部编码隐藏层信息进行注意力加权生成表示,这样会导致对齐向量长度不定(对齐向量对输入信息加权,长度和输入句子的单词数相同).上文中Dzmitry Bahdanau提出的模型即全局注意力模型。本文中的全局注意力模型更为一般化：未使用双向RNN拼接输入向量而是普通的RNN；直接用当前步解码隐藏层向量计算权重，而不是前一步，避免了复杂计算。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oH9P.png\" alt=\"i0oH9P.png\"></li>\n<li>之后引入了两种Effective Approaches，即局部注意力和input-feeding。</li>\n</ul>\n<h2 id=\"局部注意力\"><a href=\"#局部注意力\" class=\"headerlink\" title=\"局部注意力\"></a>局部注意力</h2><ul>\n<li>局部注意力：不使用全部输入信息，而是对每一个输出的单词先生成一个对齐位置，然后只对对齐位置附近窗内的输入信息注意力加权生成表示。文章给出了两种种生成对齐位置的方式：<ul>\n<li>单调对齐：简单的将第i输出单词的对齐位置设为i,显然在文摘中这种方式不可取。</li>\n<li>预测对齐：训练对齐位置。<script type=\"math/tex; mode=display\">\np_t = S \\cdots sigmoid(v_p^T tanh(W_ph_t)) \\\\</script>其中$h_t$是第t个生成单词的隐藏层向量<br>$W_p$和$v_p$都是需要训练的权重<br>S是输入单词长度,与sigmoid相乘就得到输入句中任意位置</li>\n</ul>\n</li>\n<li>另外为了使得对齐位置的权重最大，先以对齐位置为期望、半窗长为标准差生成高斯分布，再以此为基础生成权重。<script type=\"math/tex; mode=display\">\na_t(s) = align(h_t,h_s)exp(-\\frac{(s-p_t)^2}{2\\sigma ^2})</script><img src=\"https://s1.ax1x.com/2018/10/20/i0ost1.png\" alt=\"i0ost1.png\"></li>\n</ul>\n<h2 id=\"Input-feeding\"><a href=\"#Input-feeding\" class=\"headerlink\" title=\"Input-feeding\"></a>Input-feeding</h2><ul>\n<li>Input-feeding：生成对齐时还需要依赖过去的对齐，实际实现是将上一步的注意力向量作为下一步解码隐藏层的feed，好处在于模型可以完全了解之前的对齐决策，而且在水平层次和垂直层次上创造了一个非常深的网络。</li>\n<li>实验结果表明使用预测对齐的局部注意力模型表现最好。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oyfx.png\" alt=\"i0oyfx.png\"></li>\n</ul>\n<h1 id=\"A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\"><a href=\"#A-Neural-Attention-Model-for-Abstractive-Sentence-Summarization\" class=\"headerlink\" title=\"A Neural Attention Model for Abstractive Sentence Summarization\"></a>A Neural Attention Model for Abstractive Sentence Summarization</h1><ul>\n<li>发布于2015.9，作者Alexander M. Rush，解码器编码器模型，文摘任务。</li>\n<li>提出了一种注意力编码器，使用普通的NNLM解码器。</li>\n<li>未使用RNN，直接用词向量。</li>\n<li>使用全部输入信息,局部输出信息(yc)构建注意力权重。</li>\n<li>直接对平滑化的输入句子的词向量矩阵加权而不是RNN隐藏层向量。</li>\n<li>模型如下图:<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ocp6.png\" alt=\"i0ocp6.png\"></li>\n</ul>\n<h1 id=\"Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\"><a href=\"#Abstractive-Text-Summarization-using-Sequence-to-sequence-RNNs-and-Beyond\" class=\"headerlink\" title=\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\"></a>Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</h1><ul>\n<li>发布于2016.8，作者Ramesh Nallapati。编码器解码器模型，使用RNN，注意力，文摘任务。</li>\n<li>基于Dzmitry Bahdanau的机器翻译模型（双向GRU编码，单向GRU解码）进行改进。</li>\n<li>改进包括LVT（large vocabulary trick)、Feature-rich encoder、switching generator-pointer、分层注意力。</li>\n</ul>\n<h2 id=\"LVT\"><a href=\"#LVT\" class=\"headerlink\" title=\"LVT\"></a>LVT</h2><ul>\n<li>减少解码器的softmax层大小，加速计算，加速收敛。实际实现是在每一个mini-batch中解码器的词典只限于本batch内的输入文本，而且把之前batch内解码词典中最频繁的单词加入之后batch的解码词典（直至达到一个上限）。</li>\n</ul>\n<h2 id=\"Feature-rich-Encoder\"><a href=\"#Feature-rich-Encoder\" class=\"headerlink\" title=\"Feature-rich Encoder\"></a>Feature-rich Encoder</h2><ul>\n<li>不使用简单的只表示语义距离的词向量，而是构建包含了实体信息等多种语义特征，分别构成向量并拼接起来形成新的词向量<br><img src=\"https://s1.ax1x.com/2018/10/20/i0og1K.png\" alt=\"i0og1K.png\"></li>\n</ul>\n<h2 id=\"Switching-Generator-pointer\"><a href=\"#Switching-Generator-pointer\" class=\"headerlink\" title=\"Switching Generator-pointer\"></a>Switching Generator-pointer</h2><ul>\n<li>解决罕见词，额外词问题。解码器的词典是固定的，如果测试文本中包含词典外的单词该如何解决。其提供的解决方案是给解码器加上一个开关，开关打开时就普通的使用自己词典生成，开关关上时，就产生一个指针指向输入文本中的一个单词，并将其复制入文摘。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oRXD.png\" alt=\"i0oRXD.png\">)<br>Switching generator/pointer model<br>开关为G时就用传统方法生成文摘<br>开关为P时就从输入中拷贝单词到文摘中</li>\n</ul>\n<h2 id=\"分层注意力\"><a href=\"#分层注意力\" class=\"headerlink\" title=\"分层注意力\"></a>分层注意力</h2><ul>\n<li>传统的注意力是指关注句子中的关键词位置，分层还包括上一层，即文本中的关键句位置。使用两层双向RNN分别在词层次和句层次捕捉注意力。注意力机制同时运行在两个层次之上，词层次的注意力权重会被句层次的注意力权重重新加权调整。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ofne.png\" alt=\"i0ofne.png\"><h1 id=\"Recurrent-Neural-Network-Regularization\"><a href=\"#Recurrent-Neural-Network-Regularization\" class=\"headerlink\" title=\"Recurrent Neural Network Regularization\"></a>Recurrent Neural Network Regularization</h1></li>\n<li>本文介绍了如何在循环神经网络中使用dropout来防止过拟合</li>\n<li>Dropout是指在深度神经网络当中，在每次训练时随机丢掉某些隐藏层的某些节点，测试时不丢弃节点但是将节点输出乘以丢弃概率。这种方法可以有效解决深度学习费时且容易过拟合的问题。</li>\n<li>对于dropout的理解有两种，1：它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。2：它相当于创造了一些噪声数据，增加了训练数据的稀疏性，增加了特征的区分度。</li>\n<li>在RNN中不能直接按时间步丢弃隐藏层节点，因为这样会丢失RNN所需要的长期依赖信息，引入很大的噪声破坏了学习过程。</li>\n<li>作者提出的是按层丢弃节点，即使用多层RNN结构，即便丢失了一层中的某一时间步的节点，其他层的相同时间步的节点也能传递过来，不破坏序列的长期依赖信息</li>\n<li>效果如图:<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ov7j.png\" alt=\"i0ov7j.png\"></li>\n</ul>\n<h1 id=\"Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\"><a href=\"#Building-End-To-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models\" class=\"headerlink\" title=\"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\"></a>Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</h1><ul>\n<li>本文介绍了使用完全数据驱动的分层RNN端到端模型来构建non-goal-driven的自然语言对话系统。</li>\n<li>训练数据是从电影字幕中提取的triples，即两个对话人按A-B-A的顺序完成三次表达。</li>\n<li>分层的结构不是简单的增加RNN隐藏层层数，而是分别在词水平和句水平构建两个RNN，如下图。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TC90.png\" alt=\"i0TC90.png\"><br>三段式对话包含在两个词水平RNN端到端系统中<br>中间生成的句表示又作为更高层次即句水平RNN的隐藏层向量</li>\n<li>文章中使用了bootstrapping的技巧，即用一些预先训练好的数据作为模型的初始值。例如用w2v在大语料库中训练好词嵌入矩阵的初始值。本文中甚至将整个模型都预训练好了，原理是使用QA系统的二段式对话大语料库预训练整个模型，第三段设为空。在实际训练中，先预训练词嵌入完成词嵌入的初始化，然后预训练整个模型4个epoch，最后固定词嵌入不变接着预训练整个模型到最佳值。</li>\n<li>系统是给定三段式对话中的前两段推测出第三段。Evaluation使用两种标准：word perplexity衡量语法准确度，word classification error衡量语义准确度</li>\n<li>论文最后总结了一种不好的现象出现的原因。在最大后验概率输出时经常出现一些通常的回答，例如”i don’t know”或者”i’m sorry”。作者认为原因有三种：1：数据太少。因为对话系统是有内在歧义性和多模态性的，大规模数据才能训练出较好的结果。2：标点符号和代词在对话中占有很大比例，但是系统很难区分不同上下文环境中标点符号和代词的意义。3：对话一般很简短，因此infer时一个triple可以提供的信息太少。区分度不够。因此设计自然语言神经模型时应尽量区分语义和语法结构。作者还发现，如果不用最大后验概率而是随机输出时不会出现此问题，且infer出来的句子一般能保持话题，出现一个与话题相关的特殊词。</li>\n</ul>\n<h1 id=\"News-Event-Summarization-Complemented-by-Micropoints\"><a href=\"#News-Event-Summarization-Complemented-by-Micropoints\" class=\"headerlink\" title=\"News Event Summarization Complemented by Micropoints\"></a>News Event Summarization Complemented by Micropoints</h1><ul>\n<li>这是来自北大的一篇论文，使用微博上的数据，其工作是给定从传统新闻中提取出的文摘，在相同主题的微博博文中构造出一些micropoints来补充文摘，实验证明这种补充的文摘能在ROUGE中取得更好分数。</li>\n<li>文章的阐述重点在于提取micropoints而不是如何将micropoints整合进原始文摘。</li>\n<li>这个团队之前提出了两种工具：用于提取新闻关键词的一种文本聚类模型CDW；用于将一段博文分割成各自拥有相对完整含义的片段（几句话）的Snippet Growth Model。</li>\n<li>Micropoints生成的主要步骤：筛选博文、将博文按主题分类、将博文分割成片段、从同一主题的博文片段中挑选一些组成micropoints、对micropoints进行筛选。</li>\n<li>筛选博文考虑两个指标：相关性和差异性，既要与原新闻文摘主题相关，又不能太重复而导致冗余。作者使用CDW提取原新闻多个角度的关键词，并将博文与这些关键词计算cos相似度，以筛选多个具有相关性的博文。另外作者利用Joint Topic Modeling(Joint topic modeling for event summarization across news and social media streams)计算博文和文摘的差异性。将计算出的两个指标取调和平均作为总体筛选的指标。</li>\n<li>将博文按主题分类：受限使用LDA得到p(topic|tweet)，再利用此条件概率为每一个博文构造向量v(t) = (p(topic 1 |t), p(topic 2 |t), …, p(topic n |t))，最后使用DBSCAN完成主题聚类。</li>\n<li>使用团队之前提出的Snippet Growth Model将博文分成片段，大致方法是先取一个句子，然后计算其他句子与这个句子之间的文本相似度、距离度量、影响度量来决定是否将其他句子加入这个句子所在的片段当中。</li>\n<li>现在已经得到了按主题分类的一堆片段，之后需要在一个主题的片段中挑一些最能代表本主题的片段出来。其方法是挑选到同主题其他片段平均距离最小的前几个片段。因为句子不太长，作者将一个片段用词袋表示，词袋中装了组成本片段中所有句子的所有词，用词向量表示。距离用KL散度衡量。如果新挑出来的片段与已挑片段距离太近则放弃，以保证挑选出来的片段依然具有差异性。</li>\n<li>得到这些片段将组成micropoints，但是在补充进文摘之前还要进行筛选。作者提出了三个指标：信息量，流行度，简洁性。信息量即补充进文摘之后文摘获得信息熵增益，信息量越大越好；流行度用原博文评论数来衡量，越流行的博文越不容易极端，用流行度来防止极端、三观不正的博文被补充进文摘。流行度越大越好；简洁性用补充的部分和原文摘的长度比来描述，越小表示补充的越简洁，不会喧宾夺主。此时问题就化成在给定简洁性要求下，每一个片段能带来信息量和流行度的收益，同时耗费一定的简洁性，选择片段使收益最大，即离散的有限制条件的最优化问题，可以抽象为0-1背包问题，用动态规划解决。作者还设置了阈值，使用分段函数：当流行度大于阈值时，信息增益对于收益的贡献会更大。这样设置是为了保证不会因为出现流行度和信息增益一方很大而另一方几乎没有的片段被加入文摘。</li>\n</ul>\n<h1 id=\"DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\"><a href=\"#DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding\" class=\"headerlink\" title=\"DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\"></a>DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</h1><ul>\n<li>更新，作者后来又推出了fast disan，貌似是改了注意力的计算，细节待补充</li>\n<li>作者提出了一种有向自注意网络，不依赖RNN或者CNN结构也能很好的完成NLP问题中的编码任务。</li>\n<li>作者认为现有的编码器中，使用RNN能很好的捕捉序列的顺序信息但是慢；使用纯注意力机制（就如同A Neural Attention Model for Abstractive Sentence Summarization中不使用RNN而是直接对词向量序列进行注意力加权）虽然能利用现有的分布式或者并行式计算框架加速运算，却丢失了序列的顺序信息。因此作者提出了一种能捕捉序列顺序信息的纯注意力编码器结构，结合了两者的优点。</li>\n<li>作者首先提出了三种注意力概念：多维度注意力，自注意力，有向注意力。</li>\n<li>传统的注意力是对一个句子中各个单词加权，权值是标量。而多维度注意力中权值是向量，维度和词向量维度相同。使用多维度的理由在于这样是对每一个词的每一个特征进行注意力加权，词向量本身具有一词多义性，传统的对整个词向量进行加权的注意力机制对同一个词在不同上下文环境中的情况不能很好区分，多维度是对词向量的每一个分量加权，它可以给能表示当前上下文环境的特征更多的注意力权重。我的理解是对词向量的分量进行注意力加权，这样相当于同一个词在不同的上下文环境中有略微不同的表示，可以用来区分。下图是传统注意力与多维度注意力的区别。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ojBQ.png\" alt=\"i0ojBQ.png\"><br>右边是多维度注意力<br>可以看到注意力权重变成了向量，与输入词向量维度数相同</li>\n<li>一般的注意力权重是编码输入和一个解码输出作为参数生成，权重与当前输出有关。自注意力与解码端无关，要么用本句子中的每一个词替代解码输出，要么用整个输入句子替代解码输出。前者与多维度结合形成token2token注意力，后者与多维度结合形成source2token。</li>\n<li>有向注意力即在生成token2token注意力时根据需要添加一个掩码矩阵，矩阵元素为0或者负无穷。矩阵可以为上三角或者下三角，代表两个方向的掩码，例如从i到j是0，从j到i是负无穷，则在token2token中词之间的注意力就有了方向，不正确方向的注意力过softmax之后会降到0，而正确方向的注意力不受影响。掩码矩阵还有第三种，无对角线矩阵，即对角线上的值为负无穷，这样token2token中一个单词对自己不会产生注意力。有向注意力如下图：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ozAs.png\" alt=\"i0ozAs.png\"></li>\n<li>最后有向自注意网络的构成利用了以上三种注意力，首先上下三角两种掩码搭配多维度token2token可以产生前向后向两个自注意力向量，类似blstm，然后将向量连接，过一个多维度source2token产生最终的编码输出。作者测试了这种编码能在自然语言推测和情感分析任务中达到最佳水平，也可以作为其他模型的一部分在其他任务中使用。</li>\n</ul>\n<h1 id=\"Neural-Summarization-by-Extracting-Sentences-and-Words\"><a href=\"#Neural-Summarization-by-Extracting-Sentences-and-Words\" class=\"headerlink\" title=\"Neural Summarization by Extracting Sentences and Words\"></a>Neural Summarization by Extracting Sentences and Words</h1><ul>\n<li>本文是使用了完全数据驱动的模型来完成抽取式文摘。其模型结构由一个分层文本编码器和一个注意力机制抽取器构成。</li>\n<li>与生成式注意力机制文摘不同的地方在于：使用CNN而不是w2v构建词嵌入；注意力用来直接抽取词而不是加权生成中间表示。</li>\n<li>因为本文是用数据驱动的抽取式文摘，所以需要大量的抽取式文摘训练数据，这样的训练数据很少，作者提出了制造词水平和句水平的抽取式训练数据的方法：对于句抽取，作者的思路是将生成式文摘转换成抽取式文摘，首先获得生成式文摘，然后将原文中每一句与生成式文摘对比以决定是否应该抽取出来，对比的指标包括句子在文档中的位置，一元语法和二元语法重叠性，出现的命名实体数量等等；对于词抽取，同样也是对比生成式文摘和原文中词的词义重叠程度来决定该词是否应该抽取出来。对于那些生成式文摘中出现而原文中没有的词，作者的解决方案是用原文中词嵌入距离相近的词替代形成训练数据。</li>\n<li>编码时，用CNN形成词嵌入，将句子表示为词嵌入序列，再用RNN形成文档层次上的编码（一个句子为一个时间步输入）。</li>\n<li>句抽取时，与生成式不同，抽取的RNN输出依赖是上一个抽取生成的句子乘以一个置信系数，这个置信系数代表上一个句子有多大可能被抽取出来。</li>\n<li>与生成式一样，train和infer存在差异，前期抽取infer出现的问题会在后期越滚越大。为了解决这个问题，作者使用了一种“课程学习策略”：训练刚开始置信水平不能准确预测，就设为1，之后随着训练进行逐渐将置信水平恢复成训练出来的值。</li>\n<li>与句抽取相比，词抽取更加贴近生成式算法，可以看成是词典受限的词水平上的生成式文摘。</li>\n<li>抽取式文摘在处理稀疏词汇和命名实体上有优势，可以让模型检查这些词汇或实体的上下文、句中相对位置等来降低注意力权重，减少这类词影响。</li>\n<li>抽取式要解决的一个问题是决定抽取数量。作者取抽取置信水平最高的三句作为文摘。另一个问题是每一个batch的词典一般是不同的。作者采用了一种负采样的解决方案。</li>\n</ul>\n<h1 id=\"A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\"><a href=\"#A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION\" class=\"headerlink\" title=\"A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION\"></a>A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION</h1><ul>\n<li>使用强化学习来优化当前的端到端生成式文摘模型<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TicT.png\" alt=\"i0TicT.png\"></li>\n<li>解决长文摘生成和重复短语问题</li>\n<li>强化学习需要外界给予模型反馈，这里作者使用人工对生成的文摘进行评价并反馈给模型，使得模型可以生成可读性更好的文摘</li>\n<li>模型的改进主要在两点：在编码端和解码端分别加入了内注意力，其中编码端是之前提出过的，本文主要引入解码端的内注意力机制；提出了一种新的目标函数，结合了交叉熵损失和来自强化学习的奖励</li>\n<li>编码解码两端的内注意力是从两个方面解决重复短语问题，因为重复问题在长文本生成文摘中相比短文本更加严重。</li>\n<li>编码端加入内注意力是认为重复来自于在解码各个时间步对输入长文本的注意力分布不均匀，没用充分利用长文本，在解码各个时间步可能注意力的分布都相似，导致生成重复语段。因此作者在模型中对过去已经获得高注意力权重的输入位置给予惩罚，保证输入文本的各个部分充分利用。引入惩罚的方式是在新的某一解码时间步，某一编码输入位置的注意力权重是本次产生的注意力权重除以之前所有时间步注意力权重之和，这样如果过去的产生了大的注意力权重，则新产生的注意力权重会变小。</li>\n<li>解码端加入内注意力是认为重复还来源于解码端本身的隐藏状态重复。作者认为解码时依赖的解码端信息应该不止包含上一时间步的解码端隐藏层状态，而是过去所有时间步的隐藏层状态并给予注意力加权，因此在解码端引入了类似的内注意力机制和惩罚机制。</li>\n<li>在这个端到端模型中注意力并不是沟通编码端和解码端的方式，而是独立在两端，仅依赖于编码/解码端之前的状态和当前状态产生，因此是内注意力（自注意力）。</li>\n<li>在搭建端到端模型时作者还采用了一些其他前人提出过的技巧，例如使用复制指针和开关解决稀疏词问题，编码解码共享词嵌入矩阵，另外还特别提出了一个小技巧：基于观察，一般文摘中不会出现重复的三词短语，因此在解码端束搜索中若出现了重复的三词短语就剪枝。</li>\n<li>之后作者分析了静态的监督学习在文摘评价标准中常常取不到理想结果的两个原因：一是exposure bias，即模型在训练时是接触到了正确的输出(ground truth)，但是在infer时是没有正确输出做矫正的，因此如果infer时错了一个词，之后错误会越积越大；二是文摘生成本身不是静态的，没有标准答案，而好的文摘有许多种可能（文摘评价标准中一般考虑了这些可能），但使用最大似然目标函数的静态的学习方法扼杀了这些可能。</li>\n<li>因此作者在监督学习之外为文摘任务引入了policy learning，一种策略搜索强化学习方式。在强化学习中模型不是以生成与标签最相似的输出为目标，而是以最大化某一种指标为目标。在这里作者借鉴了图像标注任务中的一种强化学习算法：self-critical policy gradient training algorithm：<script type=\"math/tex; mode=display\">\nL_{rl} = (r(y)-r(y^s))\\sum _{t=1}^n log p(y_t^s | y_1^s,...,y_{t-1}^s,x)</script>r是人工评价奖励函数<br>两个r函数的参数：前者是最大化输出概率得到的基准句子，后者是根据每一步输出条件概率分布采样得到的句子<br>目标是最小化这个L目标函数，假如采样的句子得到的人工奖励比基准句子多，则这个最小化目标函数等价于最大化采样句子的条件概率（前面的两个r函数计算之后为负号）</li>\n<li>之后作者将监督学习和强化学习的两种目标函数结合起来：</li>\n</ul>\n<h1 id=\"Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\"><a href=\"#Distributed-Representations-of-Words-and-Phrases-and-their-Compositionality\" class=\"headerlink\" title=\"Distributed Representations of Words and Phrases and their Compositionality\"></a>Distributed Representations of Words and Phrases and their Compositionality</h1><ul>\n<li>介绍了w2v的负采样版本。</li>\n<li>以短语为基本单位训练而不是单词，这样能够更好的表示一些idiomatic phrase。</li>\n<li>用nce（噪声对比估计）替代分层softmax，nce近似最大化softmax的对数概率，在w2v中只关心学到好的表示，因此用简化版的nce，负采样，用下式替代输出的条件概率：<script type=\"math/tex; mode=display\">\np(w_O | w_I) = \\frac {exp(v_{w_O}^T v_{w_I})}{\\sum _{w=1}^W exp(v_{w_O}^T v_{w_I})}</script>被替换成<br>$$<br>log \\sigma (v_{w_O}^T v_{w_I}) + \\sum _{i=1}^k E_{w_i \\sim P_n(w)} [log \\sigma (v_{w_O}^T v_{w_I})]</li>\n<li>每次在softmax输出层只激活目标label和k个噪声label（即非目标label），即对每一个单词，有k+1个样本，1个正样本，k个采样得到的负样本，进行logistic回归分类，上式即logistics回归的似然函数，其中Pn是噪声的概率分布。</li>\n<li>对常见词进行降采样，因为常见词的向量表示容易稳定，再训练几百万次也不会发生大的改变，因此每一次对词的训练以一定概率跳过：</li>\n<li>这样训练出来的skip-gram模型具有很好的加法语义合成性（两个向量的逐分量相加），即俄国+河与伏尔加河相近，因为向量与输出层的概率对数相关，两个向量相加与两个上下文的乘积相关，这种乘积相当于逻辑与：高概率乘高概率为高概率，其余为低概率。因此具有这种简单的算术语义合成性。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o00J.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"论文阅读笔记2018上半年","path":"2018/03/07/PaperReading/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o00J.jpg","excerpt":"<p>论文阅读笔记<br>主要关注自动文摘方向<br></p>","date":"2018-03-07T02:20:14.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["abstractive summarization","math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习笔记","mathjax":true,"html":true,"date":"2017-02-12T14:40:38.000Z","_content":" \n***\n记录机器学习中关于一些概念和算法的笔记，来源于:\n-\t选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)\n-\t西瓜书\n-\t《统计学习方法》\n-\t《深度学习》（感谢中文翻译：[exacity/deeplearningbook-chinese](https://github.com/exacity/deeplearningbook-chinese)）\n\n更新：\n-\t2017-02-12 更新概论\n-\t2017-03-01 更新k近邻\n-\t2017-03-08 更新SVM\n-\t2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识\n-\t2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容\n<!--more-->\n\n![i0H2cV.png](https://s1.ax1x.com/2018/10/20/i0H2cV.png)\n\n# <font size=5 >统计学习方法概论</font>\n## <font size=4 >统计学习，监督学习，三要素</font>\n-\t如果一个系统能够通过执行某个过程改进它的性能，这就是学习\n-\t统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析\n-\t得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析\n-\t监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测\n-\t每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征\n-\t监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)\n-\t监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数\n-\t统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法\n-\t损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$\n-\t理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化\n\n## <font size=4 >模型评估，模型选择</font>\n-\t模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的\n-\t过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量\n-\t模型选择的两种方法：正则化和交叉验证\n\n## <font size=4 >正则化，交叉验证</font>\n-\t即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高\n-\t一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择\n-\t交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证\n\n## <font size=4 >泛化能力</font>\n-\t泛化误差:对未知数据预测的误差\n-\t泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大\n-\t对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$\n\n$$\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\\n$$\n\n## <font size=4 >生成模型，判别模型</font>\n-\t生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型\n-\t判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等\n-\t生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况\n-\t判别方法准确率高，可以抽象数据，简化学习问题\n\n## <font size=4 >分类，标注，回归</font>\n-\t分类，即输出取离散有限值，分类决策函数也叫分类器\n-\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n$$\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n$$\n-\t标注:输入一个观测序列，输出一个标记序列\n-\t回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合\n\n# <font size=5 >k近邻法</font>\n## <font size=4 >k近邻法</font>\n\n- k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。\n- k值选择，距离度量以及分类决策规则是k近邻法三要素。\n-\tk近邻法是一种懒惰学习，他不对样本进行训练。\n\n## <font size=4 >k近邻算法</font>\n\n- 对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:\n  $$\n  y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K\n  $$\n  其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。\n\n- k=1时称为最近邻算法\n\n- k近邻算法没有显式的学习过程\n\n## <font size=4 >k近邻模型</font>\n\n- k近邻模型即对特征空间的划分。\n\n- 特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。\n\n-\t距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。\n\n-\t欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：\n  $$\n  L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}\n  $$\n\n-\tk值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单\n\n-\t一般采用交叉验证法确定k值\n\n-\t多数表决规则等价于经验风险最小化\n\n## <font size=4 >kd树</font>\n-\tkd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树\n-\tkd树的每一个节点对应于一个k维超矩形区域\n-\t构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。\n-\t构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：\n\t-\t从根节点出发，递归向下搜索目标点所在区域，直到叶节点\n\t-\t以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离\n\t-\t递归向上回退，对每个节点做如下操作\n\t\t-\t如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离\n\t\t-\t该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。\n\t\t-\t直到搜索到根节点，此时的当前最近点即目标点的最近邻点。\n-\tkd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索\n\n\n# <font size=5 >支持向量机</font>\n## <font size=4 >线性可分支持向量机与硬间隔最大化</font>\n-\t学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示\n-\t这里的x是特征向量$(x_1,x_2,...)$，而y是特征向量的标签\n-\t给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机\n-\t在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为\n$$\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}\n$$\n-\t而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔\n$$\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\\n$$\n-\t同时定义相对距离为函数间隔\n$$\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\\n$$\n-\t硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言\n-\t求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题\n$$\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\\n$$\n-\t我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为\n$$\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\\n$$\n-\t上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法\n-\t最大间隔分离超平面存在且唯一，证明略\n-\t在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量\n-\t对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$\n-\t由此可见支持向量机由很少的重要的训练样本(支持向量)决定\n-\t为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题\n-\t待补充\n\n# <font size=5 >线代基础</font>\n## <font size=4 >Moore-penrose</font>\n-\t对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆\n$$\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T\n$$ \n-\t计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：\n$$\nA^+=VD^+U^T\n$$\n\t其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。\n-\t当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。\n-\t当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。\n-\t待补充\n\n## <font size=4 >迹</font>\n-\t迹运算返回的是矩阵对角元素的和.\n-\t使用迹运算可以描述矩阵Frobenius范数的方式：\n$$\n||A_F||=\\sqrt{Tr(AA^T)}\n$$\n-\t迹具有转置不变性和轮换不变性\n-\t标量的迹是其本身\n\n## <font size=4 >PCA解释</font>\n-\t待补充\n\n# <font size=5 >概率论信息论</font>\n## <font size=4 >Logistic Sigmoid</font>\n-\tLogistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：\n\t$$\n\t\\sigma (x) = \\frac{1}{1+exp(-x)}\n\t$$\n-\tSoftmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：\n\t$$\n\t\\sigma (z)_j = \\frac{e^z j}{\\sum _{k=1}^K e^z k}\n\t$$\n-\t两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。\n-\tSoftmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。\n-\t利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：\n\t![i0HRXT.png](https://s1.ax1x.com/2018/10/20/i0HRXT.png)\n\t\n## <font size=4 >KL散度和交叉熵</font>\n-\tKL散度：用以衡量PQ两个分布之间的差异，非负且不对称：\n\t$$\n\tD_{KL}(P||Q) = E_{x \\sim P} [log \\frac{P(x)}{Q(x)}] = E_{x \\sim P} [log P(x) - log Q(x)]\n\t$$\n-\t交叉熵：\n\t$$\n\tH(P,Q) = -E_{x \\sim P} log Q(x)\n\t$$\n-\t交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数\n-\t在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)\n-\t对q按照p求自信息期望即二元交叉熵(Logistic代价函数):\n\t$$\n\tJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m y^{(i)} log h_{\\theta} (x^{(i)}) + (1-y^{(i)}) log (1-h_{\\theta}(x^{(i)}))]\n\t$$\n-\t同理可得多元交叉熵(Softmaxs代价函数):\n\t$$\n\tJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m \\sum _{j=1}^k 1\\{ y^{(i)}=j \\} log \\frac {e^{\\theta _j ^T x^{(i)}}} {\\sum _{l=1}^k e^{\\theta _j ^T x^{(i)}}}]\n\t$$\n\n## <font size=4 >交叉熵与最大对数似然关系</font>\t\n-\t已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。\n在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} p_{model} (X;\\theta)\n\t$$\n-\t因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} E_{x \\sim p_{data}} log p_{model}(x;\\theta)\n\t$$\n-\t可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:\n-\t最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数\n-\t最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} \\sum_{i=1}^m log P(y^{(i)} | x^{(i)} ; \\theta)\n\t$$\n-\t最大似然估计具有一致性。\n\n# <font size=5 >计算方法</font>\n## <font size=4 >梯度下降</font>\n-\t问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？\n-\t原理：将输入向导数的反方向移动一小步可以减小函数输出。\n-\t将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。\n-\t一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：\n\t![i0opQO.png](https://s1.ax1x.com/2018/10/20/i0opQO.png)\n\n## <font size=4 >牛顿法</font>\n-\t二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：\n\t![i0o9yD.png](https://s1.ax1x.com/2018/10/20/i0o9yD.png)\n## <font size=4 >约束优化</font>\n-\t只包含等式约束条件：Lagrange \n-\t包含不等式约束条件：KTT\n\n# <font size=5 >修改算法</font>\n## <font size=4 >修改假设空间</font>\n-\t机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。\n-\t调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：\n\t$$\n\ty = b + wx\n\t$$\n-\t如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：\n\t$$\n\ty= b + w_1 x + w_2 x^2\n\t$$\n\t此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\n\n## <font size=4 >正则化</font>\n-\t没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。\n-\t一个例子是权重衰减，加入权重衰减正则化项的代价函数是：\n\t$$\n\tJ(w) = MSE_{train} + \\lambda w^T w\n\t$$\n\tλ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。\n\n\n\n\n\n","source":"_posts/MachineLearningNote.md","raw":"---\ntitle: 机器学习笔记\ncategories: 机器学习\ntags:\n- code\n- machine learning\n\nmathjax: true\nhtml: true\ndate: 2017-02-12 22:40:38\n---\n \n***\n记录机器学习中关于一些概念和算法的笔记，来源于:\n-\t选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)\n-\t西瓜书\n-\t《统计学习方法》\n-\t《深度学习》（感谢中文翻译：[exacity/deeplearningbook-chinese](https://github.com/exacity/deeplearningbook-chinese)）\n\n更新：\n-\t2017-02-12 更新概论\n-\t2017-03-01 更新k近邻\n-\t2017-03-08 更新SVM\n-\t2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识\n-\t2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容\n<!--more-->\n\n![i0H2cV.png](https://s1.ax1x.com/2018/10/20/i0H2cV.png)\n\n# <font size=5 >统计学习方法概论</font>\n## <font size=4 >统计学习，监督学习，三要素</font>\n-\t如果一个系统能够通过执行某个过程改进它的性能，这就是学习\n-\t统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析\n-\t得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析\n-\t监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测\n-\t每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征\n-\t监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)\n-\t监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数\n-\t统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法\n-\t损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：$$R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$经验风险(经验损失)是模型关于训练集的平均损失:$$R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))$$\n-\t理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：$$R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)$$,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化\n\n## <font size=4 >模型评估，模型选择</font>\n-\t模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的\n-\t过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量\n-\t模型选择的两种方法：正则化和交叉验证\n\n## <font size=4 >正则化，交叉验证</font>\n-\t即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高\n-\t一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择\n-\t交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证\n\n## <font size=4 >泛化能力</font>\n-\t泛化误差:对未知数据预测的误差\n-\t泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大\n-\t对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$\n\n$$\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\\n$$\n\n## <font size=4 >生成模型，判别模型</font>\n-\t生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型\n-\t判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等\n-\t生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况\n-\t判别方法准确率高，可以抽象数据，简化学习问题\n\n## <font size=4 >分类，标注，回归</font>\n-\t分类，即输出取离散有限值，分类决策函数也叫分类器\n-\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n$$\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n$$\n-\t标注:输入一个观测序列，输出一个标记序列\n-\t回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合\n\n# <font size=5 >k近邻法</font>\n## <font size=4 >k近邻法</font>\n\n- k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。\n- k值选择，距离度量以及分类决策规则是k近邻法三要素。\n-\tk近邻法是一种懒惰学习，他不对样本进行训练。\n\n## <font size=4 >k近邻算法</font>\n\n- 对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:\n  $$\n  y=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K\n  $$\n  其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。\n\n- k=1时称为最近邻算法\n\n- k近邻算法没有显式的学习过程\n\n## <font size=4 >k近邻模型</font>\n\n- k近邻模型即对特征空间的划分。\n\n- 特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。\n\n-\t距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。\n\n-\t欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：\n  $$\n  L_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}\n  $$\n\n-\tk值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单\n\n-\t一般采用交叉验证法确定k值\n\n-\t多数表决规则等价于经验风险最小化\n\n## <font size=4 >kd树</font>\n-\tkd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树\n-\tkd树的每一个节点对应于一个k维超矩形区域\n-\t构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。\n-\t构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：\n\t-\t从根节点出发，递归向下搜索目标点所在区域，直到叶节点\n\t-\t以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离\n\t-\t递归向上回退，对每个节点做如下操作\n\t\t-\t如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离\n\t\t-\t该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。\n\t\t-\t直到搜索到根节点，此时的当前最近点即目标点的最近邻点。\n-\tkd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索\n\n\n# <font size=5 >支持向量机</font>\n## <font size=4 >线性可分支持向量机与硬间隔最大化</font>\n-\t学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示\n-\t这里的x是特征向量$(x_1,x_2,...)$，而y是特征向量的标签\n-\t给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机\n-\t在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为\n$$\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}\n$$\n-\t而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔\n$$\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\\n$$\n-\t同时定义相对距离为函数间隔\n$$\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\\n$$\n-\t硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言\n-\t求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题\n$$\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\\n$$\n-\t我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为\n$$\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\\n$$\n-\t上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法\n-\t最大间隔分离超平面存在且唯一，证明略\n-\t在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量\n-\t对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$\n-\t由此可见支持向量机由很少的重要的训练样本(支持向量)决定\n-\t为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题\n-\t待补充\n\n# <font size=5 >线代基础</font>\n## <font size=4 >Moore-penrose</font>\n-\t对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆\n$$\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T\n$$ \n-\t计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：\n$$\nA^+=VD^+U^T\n$$\n\t其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。\n-\t当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。\n-\t当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。\n-\t待补充\n\n## <font size=4 >迹</font>\n-\t迹运算返回的是矩阵对角元素的和.\n-\t使用迹运算可以描述矩阵Frobenius范数的方式：\n$$\n||A_F||=\\sqrt{Tr(AA^T)}\n$$\n-\t迹具有转置不变性和轮换不变性\n-\t标量的迹是其本身\n\n## <font size=4 >PCA解释</font>\n-\t待补充\n\n# <font size=5 >概率论信息论</font>\n## <font size=4 >Logistic Sigmoid</font>\n-\tLogistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：\n\t$$\n\t\\sigma (x) = \\frac{1}{1+exp(-x)}\n\t$$\n-\tSoftmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：\n\t$$\n\t\\sigma (z)_j = \\frac{e^z j}{\\sum _{k=1}^K e^z k}\n\t$$\n-\t两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。\n-\tSoftmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。\n-\t利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：\n\t![i0HRXT.png](https://s1.ax1x.com/2018/10/20/i0HRXT.png)\n\t\n## <font size=4 >KL散度和交叉熵</font>\n-\tKL散度：用以衡量PQ两个分布之间的差异，非负且不对称：\n\t$$\n\tD_{KL}(P||Q) = E_{x \\sim P} [log \\frac{P(x)}{Q(x)}] = E_{x \\sim P} [log P(x) - log Q(x)]\n\t$$\n-\t交叉熵：\n\t$$\n\tH(P,Q) = -E_{x \\sim P} log Q(x)\n\t$$\n-\t交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数\n-\t在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)\n-\t对q按照p求自信息期望即二元交叉熵(Logistic代价函数):\n\t$$\n\tJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m y^{(i)} log h_{\\theta} (x^{(i)}) + (1-y^{(i)}) log (1-h_{\\theta}(x^{(i)}))]\n\t$$\n-\t同理可得多元交叉熵(Softmaxs代价函数):\n\t$$\n\tJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m \\sum _{j=1}^k 1\\{ y^{(i)}=j \\} log \\frac {e^{\\theta _j ^T x^{(i)}}} {\\sum _{l=1}^k e^{\\theta _j ^T x^{(i)}}}]\n\t$$\n\n## <font size=4 >交叉熵与最大对数似然关系</font>\t\n-\t已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。\n在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} p_{model} (X;\\theta)\n\t$$\n-\t因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} E_{x \\sim p_{data}} log p_{model}(x;\\theta)\n\t$$\n-\t可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:\n-\t最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数\n-\t最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：\n\t$$\n\t\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} \\sum_{i=1}^m log P(y^{(i)} | x^{(i)} ; \\theta)\n\t$$\n-\t最大似然估计具有一致性。\n\n# <font size=5 >计算方法</font>\n## <font size=4 >梯度下降</font>\n-\t问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？\n-\t原理：将输入向导数的反方向移动一小步可以减小函数输出。\n-\t将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。\n-\t一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：\n\t![i0opQO.png](https://s1.ax1x.com/2018/10/20/i0opQO.png)\n\n## <font size=4 >牛顿法</font>\n-\t二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：\n\t![i0o9yD.png](https://s1.ax1x.com/2018/10/20/i0o9yD.png)\n## <font size=4 >约束优化</font>\n-\t只包含等式约束条件：Lagrange \n-\t包含不等式约束条件：KTT\n\n# <font size=5 >修改算法</font>\n## <font size=4 >修改假设空间</font>\n-\t机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。\n-\t调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：\n\t$$\n\ty = b + wx\n\t$$\n-\t如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：\n\t$$\n\ty= b + w_1 x + w_2 x^2\n\t$$\n\t此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。\n\n## <font size=4 >正则化</font>\n-\t没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。\n-\t一个例子是权重衰减，加入权重衰减正则化项的代价函数是：\n\t$$\n\tJ(w) = MSE_{train} + \\lambda w^T w\n\t$$\n\tλ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。\n\n\n\n\n\n","slug":"MachineLearningNote","published":1,"updated":"2019-07-22T03:45:22.826Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6v9y0041q8t52qdwmwqx","content":"<hr>\n<p>记录机器学习中关于一些概念和算法的笔记，来源于:</p>\n<ul>\n<li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li>\n<li>西瓜书</li>\n<li>《统计学习方法》</li>\n<li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li>\n</ul>\n<p>更新：</p>\n<ul>\n<li>2017-02-12 更新概论</li>\n<li>2017-03-01 更新k近邻</li>\n<li>2017-03-08 更新SVM</li>\n<li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li>\n<li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容<a id=\"more\"></a>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0H2cV.png\" alt=\"i0H2cV.png\"></p>\n<h1 id=\"统计学习方法概论\"><a href=\"#统计学习方法概论\" class=\"headerlink\" title=\"统计学习方法概论\"></a><font size=\"5\">统计学习方法概论</font></h1><h2 id=\"统计学习，监督学习，三要素\"><a href=\"#统计学习，监督学习，三要素\" class=\"headerlink\" title=\"统计学习，监督学习，三要素\"></a><font size=\"4\">统计学习，监督学习，三要素</font></h2><ul>\n<li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li>\n<li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li>\n<li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li>\n<li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li>\n<li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li>\n<li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li>\n<li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li>\n<li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li>\n<li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：<script type=\"math/tex\">R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy</script>经验风险(经验损失)是模型关于训练集的平均损失:<script type=\"math/tex\">R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))</script></li>\n<li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：<script type=\"math/tex\">R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)</script>,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li>\n</ul>\n<h2 id=\"模型评估，模型选择\"><a href=\"#模型评估，模型选择\" class=\"headerlink\" title=\"模型评估，模型选择\"></a><font size=\"4\">模型评估，模型选择</font></h2><ul>\n<li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li>\n<li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li>\n<li>模型选择的两种方法：正则化和交叉验证</li>\n</ul>\n<h2 id=\"正则化，交叉验证\"><a href=\"#正则化，交叉验证\" class=\"headerlink\" title=\"正则化，交叉验证\"></a><font size=\"4\">正则化，交叉验证</font></h2><ul>\n<li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li>\n<li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li>\n<li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li>\n</ul>\n<h2 id=\"泛化能力\"><a href=\"#泛化能力\" class=\"headerlink\" title=\"泛化能力\"></a><font size=\"4\">泛化能力</font></h2><ul>\n<li>泛化误差:对未知数据预测的误差</li>\n<li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li>\n<li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\</script><h2 id=\"生成模型，判别模型\"><a href=\"#生成模型，判别模型\" class=\"headerlink\" title=\"生成模型，判别模型\"></a><font size=\"4\">生成模型，判别模型</font></h2><ul>\n<li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li>\n<li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li>\n<li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li>\n<li>判别方法准确率高，可以抽象数据，简化学习问题</li>\n</ul>\n<h2 id=\"分类，标注，回归\"><a href=\"#分类，标注，回归\" class=\"headerlink\" title=\"分类，标注，回归\"></a><font size=\"4\">分类，标注，回归</font></h2><ul>\n<li>分类，即输出取离散有限值，分类决策函数也叫分类器</li>\n<li>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<script type=\"math/tex; mode=display\">\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\</script></li>\n<li>标注:输入一个观测序列，输出一个标记序列</li>\n<li>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</li>\n</ul>\n<h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"5\">k近邻法</font></h1><h2 id=\"k近邻法-1\"><a href=\"#k近邻法-1\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"4\">k近邻法</font></h2><ul>\n<li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li>\n<li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li>\n<li>k近邻法是一种懒惰学习，他不对样本进行训练。</li>\n</ul>\n<h2 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a><font size=\"4\">k近邻算法</font></h2><ul>\n<li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:</p>\n<script type=\"math/tex; mode=display\">\ny=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K</script><p>其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。</p>\n</li>\n<li><p>k=1时称为最近邻算法</p>\n</li>\n<li><p>k近邻算法没有显式的学习过程</p>\n</li>\n</ul>\n<h2 id=\"k近邻模型\"><a href=\"#k近邻模型\" class=\"headerlink\" title=\"k近邻模型\"></a><font size=\"4\">k近邻模型</font></h2><ul>\n<li><p>k近邻模型即对特征空间的划分。</p>\n</li>\n<li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p>\n</li>\n<li><p>距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。</p>\n</li>\n<li><p>欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：</p>\n<script type=\"math/tex; mode=display\">\nL_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}</script></li>\n<li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p>\n</li>\n<li><p>一般采用交叉验证法确定k值</p>\n</li>\n<li><p>多数表决规则等价于经验风险最小化</p>\n</li>\n</ul>\n<h2 id=\"kd树\"><a href=\"#kd树\" class=\"headerlink\" title=\"kd树\"></a><font size=\"4\">kd树</font></h2><ul>\n<li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li>\n<li>kd树的每一个节点对应于一个k维超矩形区域</li>\n<li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li>\n<li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：<ul>\n<li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li>\n<li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li>\n<li>递归向上回退，对每个节点做如下操作<ul>\n<li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li>\n<li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li>\n<li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>kd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索</li>\n</ul>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a><font size=\"5\">支持向量机</font></h1><h2 id=\"线性可分支持向量机与硬间隔最大化\"><a href=\"#线性可分支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可分支持向量机与硬间隔最大化\"></a><font size=\"4\">线性可分支持向量机与硬间隔最大化</font></h2><ul>\n<li>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示</li>\n<li>这里的x是特征向量$(x_1,x_2,…)$，而y是特征向量的标签</li>\n<li>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机</li>\n<li>在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为<script type=\"math/tex; mode=display\">\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}</script></li>\n<li>而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔<script type=\"math/tex; mode=display\">\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\</script></li>\n<li>同时定义相对距离为函数间隔<script type=\"math/tex; mode=display\">\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\</script></li>\n<li>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</li>\n<li>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题<script type=\"math/tex; mode=display\">\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\</script></li>\n<li>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为<script type=\"math/tex; mode=display\">\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\</script></li>\n<li>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</li>\n<li>最大间隔分离超平面存在且唯一，证明略</li>\n<li>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</li>\n<li>对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$</li>\n<li>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</li>\n<li>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</li>\n<li>待补充</li>\n</ul>\n<h1 id=\"线代基础\"><a href=\"#线代基础\" class=\"headerlink\" title=\"线代基础\"></a><font size=\"5\">线代基础</font></h1><h2 id=\"Moore-penrose\"><a href=\"#Moore-penrose\" class=\"headerlink\" title=\"Moore-penrose\"></a><font size=\"4\">Moore-penrose</font></h2><ul>\n<li>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆<script type=\"math/tex; mode=display\">\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T</script></li>\n<li>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：<script type=\"math/tex; mode=display\">\nA^+=VD^+U^T</script>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。</li>\n<li>当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。</li>\n<li>当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。</li>\n<li>待补充</li>\n</ul>\n<h2 id=\"迹\"><a href=\"#迹\" class=\"headerlink\" title=\"迹\"></a><font size=\"4\">迹</font></h2><ul>\n<li>迹运算返回的是矩阵对角元素的和.</li>\n<li>使用迹运算可以描述矩阵Frobenius范数的方式：<script type=\"math/tex; mode=display\">\n||A_F||=\\sqrt{Tr(AA^T)}</script></li>\n<li>迹具有转置不变性和轮换不变性</li>\n<li>标量的迹是其本身</li>\n</ul>\n<h2 id=\"PCA解释\"><a href=\"#PCA解释\" class=\"headerlink\" title=\"PCA解释\"></a><font size=\"4\">PCA解释</font></h2><ul>\n<li>待补充</li>\n</ul>\n<h1 id=\"概率论信息论\"><a href=\"#概率论信息论\" class=\"headerlink\" title=\"概率论信息论\"></a><font size=\"5\">概率论信息论</font></h1><h2 id=\"Logistic-Sigmoid\"><a href=\"#Logistic-Sigmoid\" class=\"headerlink\" title=\"Logistic Sigmoid\"></a><font size=\"4\">Logistic Sigmoid</font></h2><ul>\n<li>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：<script type=\"math/tex; mode=display\">\n\\sigma (x) = \\frac{1}{1+exp(-x)}</script></li>\n<li>Softmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：<script type=\"math/tex; mode=display\">\n\\sigma (z)_j = \\frac{e^z j}{\\sum _{k=1}^K e^z k}</script></li>\n<li>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。</li>\n<li>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</li>\n<li>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0HRXT.png\" alt=\"i0HRXT.png\"></li>\n</ul>\n<h2 id=\"KL散度和交叉熵\"><a href=\"#KL散度和交叉熵\" class=\"headerlink\" title=\"KL散度和交叉熵\"></a><font size=\"4\">KL散度和交叉熵</font></h2><ul>\n<li>KL散度：用以衡量PQ两个分布之间的差异，非负且不对称：<script type=\"math/tex; mode=display\">\nD_{KL}(P||Q) = E_{x \\sim P} [log \\frac{P(x)}{Q(x)}] = E_{x \\sim P} [log P(x) - log Q(x)]</script></li>\n<li>交叉熵：<script type=\"math/tex; mode=display\">\nH(P,Q) = -E_{x \\sim P} log Q(x)</script></li>\n<li>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</li>\n<li>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</li>\n<li>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):<script type=\"math/tex; mode=display\">\nJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m y^{(i)} log h_{\\theta} (x^{(i)}) + (1-y^{(i)}) log (1-h_{\\theta}(x^{(i)}))]</script></li>\n<li>同理可得多元交叉熵(Softmaxs代价函数):<script type=\"math/tex; mode=display\">\nJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m \\sum _{j=1}^k 1\\{ y^{(i)}=j \\} log \\frac {e^{\\theta _j ^T x^{(i)}}} {\\sum _{l=1}^k e^{\\theta _j ^T x^{(i)}}}]</script></li>\n</ul>\n<h2 id=\"交叉熵与最大对数似然关系\"><a href=\"#交叉熵与最大对数似然关系\" class=\"headerlink\" title=\"交叉熵与最大对数似然关系\"></a><font size=\"4\">交叉熵与最大对数似然关系</font></h2><ul>\n<li>已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。<br>在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} p_{model} (X;\\theta)</script></li>\n<li>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} E_{x \\sim p_{data}} log p_{model}(x;\\theta)</script></li>\n<li>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</li>\n<li>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</li>\n<li>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} \\sum_{i=1}^m log P(y^{(i)} | x^{(i)} ; \\theta)</script></li>\n<li>最大似然估计具有一致性。</li>\n</ul>\n<h1 id=\"计算方法\"><a href=\"#计算方法\" class=\"headerlink\" title=\"计算方法\"></a><font size=\"5\">计算方法</font></h1><h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a><font size=\"4\">梯度下降</font></h2><ul>\n<li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li>\n<li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li>\n<li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li>\n<li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0opQO.png\" alt=\"i0opQO.png\"></li>\n</ul>\n<h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a><font size=\"4\">牛顿法</font></h2><ul>\n<li>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o9yD.png\" alt=\"i0o9yD.png\"><h2 id=\"约束优化\"><a href=\"#约束优化\" class=\"headerlink\" title=\"约束优化\"></a><font size=\"4\">约束优化</font></h2></li>\n<li>只包含等式约束条件：Lagrange </li>\n<li>包含不等式约束条件：KTT</li>\n</ul>\n<h1 id=\"修改算法\"><a href=\"#修改算法\" class=\"headerlink\" title=\"修改算法\"></a><font size=\"5\">修改算法</font></h1><h2 id=\"修改假设空间\"><a href=\"#修改假设空间\" class=\"headerlink\" title=\"修改假设空间\"></a><font size=\"4\">修改假设空间</font></h2><ul>\n<li>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</li>\n<li>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：<script type=\"math/tex; mode=display\">\ny = b + wx</script></li>\n<li>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：<script type=\"math/tex; mode=display\">\ny= b + w_1 x + w_2 x^2</script>此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><ul>\n<li>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</li>\n<li>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：<script type=\"math/tex; mode=display\">\nJ(w) = MSE_{train} + \\lambda w^T w</script>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>记录机器学习中关于一些概念和算法的笔记，来源于:</p>\n<ul>\n<li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li>\n<li>西瓜书</li>\n<li>《统计学习方法》</li>\n<li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li>\n</ul>\n<p>更新：</p>\n<ul>\n<li>2017-02-12 更新概论</li>\n<li>2017-03-01 更新k近邻</li>\n<li>2017-03-08 更新SVM</li>\n<li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li>\n<li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容</li></ul>","more":"\n\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0H2cV.png\" alt=\"i0H2cV.png\"></p>\n<h1 id=\"统计学习方法概论\"><a href=\"#统计学习方法概论\" class=\"headerlink\" title=\"统计学习方法概论\"></a><font size=\"5\">统计学习方法概论</font></h1><h2 id=\"统计学习，监督学习，三要素\"><a href=\"#统计学习，监督学习，三要素\" class=\"headerlink\" title=\"统计学习，监督学习，三要素\"></a><font size=\"4\">统计学习，监督学习，三要素</font></h2><ul>\n<li>如果一个系统能够通过执行某个过程改进它的性能，这就是学习</li>\n<li>统计学习的方法是基于数据构建统计模型从而对数据进行预测和分析</li>\n<li>得到训练数据集合；确定包含所有可能模型的假设空间；确定模型选择的准则；实现求解最优模型的算法；通过学习方法选择最优模型；利用最优模型对新数据进行预测或分析</li>\n<li>监督学习的任务是学习一个模型，是模型对任意给定的输入，对其相应的输出做出一个好的预测</li>\n<li>每个具体的输入是一个实例，通常由特征向量表示。构成特征空间，每一维对应一个特征</li>\n<li>监督学习从训练数据集合中学习模型，训练数据由输入与输出对构成(样本)</li>\n<li>监督学习从训练集中学习到一个模型，表示为条件概率分布或者决策函数</li>\n<li>统计学习三要素:方法=模型+策略+算法。模型包括概率模型(条件概率)和非概率模型(决策函数)；策略即选择最优模型的方法，引入损失函数(代价函数)，风险函数的概念，实现经验风险或者结构风险最优化；算法是指学习模型的具体计算方法</li>\n<li>损失函数用来度量预测值相比真实值的错误程度，常见的有:0-1损失函数，平方损失函数，绝对损失函数对数损失函数，记为$L(Y,P(Y|X))$,风险函数(期望损失)是模型在联合分布的平均以下的损失：<script type=\"math/tex\">R_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy</script>经验风险(经验损失)是模型关于训练集的平均损失:<script type=\"math/tex\">R_{emp}(f)=\\frac 1N \\sum_{i=1}^NL(y_i,f(x_i))</script></li>\n<li>理想情况下，可以用经验风险估计期望风险，然而样本容量很小时，经验风险最小化易导致过拟合，从而提出了结构风险(正则化)：<script type=\"math/tex\">R_{srm}(f)=\\frac1N \\sum_{i=1}^NL(y_i,f(x_i))+ \\lambda J(f)</script>,其中J(f)为模型的复杂性，系数$\\lambda$用来权衡经验风险和模型复杂度。ML属于经验风险最小化，MAP属于结构风险最小化</li>\n</ul>\n<h2 id=\"模型评估，模型选择\"><a href=\"#模型评估，模型选择\" class=\"headerlink\" title=\"模型评估，模型选择\"></a><font size=\"4\">模型评估，模型选择</font></h2><ul>\n<li>模型在训练集和测试集上的误差分别称为训练误差和测试误差，所用的损失函数不一定一致，让两者一致是比较理想的</li>\n<li>过拟合:学过头了，模型的复杂度比真实模型要高，只对学过的数据性能良好，对未知数据的预测能力差。避免过拟合需要正确的特征个数和正确的特征向量</li>\n<li>模型选择的两种方法：正则化和交叉验证</li>\n</ul>\n<h2 id=\"正则化，交叉验证\"><a href=\"#正则化，交叉验证\" class=\"headerlink\" title=\"正则化，交叉验证\"></a><font size=\"4\">正则化，交叉验证</font></h2><ul>\n<li>即在经验风险上加一个正则化项(罚项)，模型越复杂惩罚越高</li>\n<li>一般方法是将数据集随机的分成三部分:训练集、验证集、测试集，分别用来训练数据，选择模型，最终对学习方法的评估。交叉验证是将数据反复随机切分为训练集和测试集，学习多次，进行测试和模型选择</li>\n<li>交叉验证类型:简单交叉验证;S折交叉验证;留一交叉验证</li>\n</ul>\n<h2 id=\"泛化能力\"><a href=\"#泛化能力\" class=\"headerlink\" title=\"泛化能力\"></a><font size=\"4\">泛化能力</font></h2><ul>\n<li>泛化误差:对未知数据预测的误差</li>\n<li>泛化误差上界，一般是样本容量的函数，当样本容量增加时，泛化上界趋于0，是假设空间容量越大，模型就越难学，泛化误差上界就越大</li>\n<li>对于二分类问题，泛化误差上界：其中d是函数集合容量，对任意一个函数，至少以概率$1-\\delta$</li>\n</ul>\n<script type=\"math/tex; mode=display\">\nR_{exp}(f)\\leq R_{emp}(f)+\\varepsilon (d,N,\\delta ) \\\\\n\\varepsilon (d,N,\\delta )=\\sqrt{\\frac 1{2N}(log d+log \\frac 1 \\delta)} \\\\</script><h2 id=\"生成模型，判别模型\"><a href=\"#生成模型，判别模型\" class=\"headerlink\" title=\"生成模型，判别模型\"></a><font size=\"4\">生成模型，判别模型</font></h2><ul>\n<li>生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测的模型，即生成模型，比如朴素贝叶斯法和隐马尔科夫模型</li>\n<li>判别方法由数据直接学习决策函数或者条件概率分布作为预测的模型，即判别模型，比如k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</li>\n<li>生成方法可以还原联合概率分布，学习收敛速度快，适用于存在隐变量的情况</li>\n<li>判别方法准确率高，可以抽象数据，简化学习问题</li>\n</ul>\n<h2 id=\"分类，标注，回归\"><a href=\"#分类，标注，回归\" class=\"headerlink\" title=\"分类，标注，回归\"></a><font size=\"4\">分类，标注，回归</font></h2><ul>\n<li>分类，即输出取离散有限值，分类决策函数也叫分类器</li>\n<li>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<script type=\"math/tex; mode=display\">\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\</script></li>\n<li>标注:输入一个观测序列，输出一个标记序列</li>\n<li>回归：函数拟合，常用的损失函数是平方损失函数，利用最小二乘法拟合</li>\n</ul>\n<h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"5\">k近邻法</font></h1><h2 id=\"k近邻法-1\"><a href=\"#k近邻法-1\" class=\"headerlink\" title=\"k近邻法\"></a><font size=\"4\">k近邻法</font></h2><ul>\n<li>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。</li>\n<li>k值选择，距离度量以及分类决策规则是k近邻法三要素。</li>\n<li>k近邻法是一种懒惰学习，他不对样本进行训练。</li>\n</ul>\n<h2 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a><font size=\"4\">k近邻算法</font></h2><ul>\n<li><p>对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分到这个类。即:</p>\n<script type=\"math/tex; mode=display\">\ny=arg \\max_{c_j} \\sum_{x_i \\in N_k(x)} I(y_i=c_j), \\  i=1,2,...,N; \\ j=1,2,...,K</script><p>其中$y_i=c_i$时$I=1$，$N_k(x)$是覆盖x的k个近邻点的邻域。</p>\n</li>\n<li><p>k=1时称为最近邻算法</p>\n</li>\n<li><p>k近邻算法没有显式的学习过程</p>\n</li>\n</ul>\n<h2 id=\"k近邻模型\"><a href=\"#k近邻模型\" class=\"headerlink\" title=\"k近邻模型\"></a><font size=\"4\">k近邻模型</font></h2><ul>\n<li><p>k近邻模型即对特征空间的划分。</p>\n</li>\n<li><p>特征空间中，对于每个训练实例点，距离该点比其他点更近的所有点组成一个区域，叫做单元。每一个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分，每个实例的类是其单元中所有点的类标记。</p>\n</li>\n<li><p>距离度量包括：欧氏距离，$L_p$距离，Minkowski距离。</p>\n</li>\n<li><p>欧氏距离是$L_p$距离的一种特殊情况(p=2)，p=1时即曼哈顿距离，定义为：</p>\n<script type=\"math/tex; mode=display\">\nL_p(x_i,x_j)=(\\sum _{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{\\frac1p}</script></li>\n<li><p>k值较小，近似误差小，估计误差大，整体模型复杂，容易发生过拟合。k值较大，估计误差小，近似误差大，模型简单</p>\n</li>\n<li><p>一般采用交叉验证法确定k值</p>\n</li>\n<li><p>多数表决规则等价于经验风险最小化</p>\n</li>\n</ul>\n<h2 id=\"kd树\"><a href=\"#kd树\" class=\"headerlink\" title=\"kd树\"></a><font size=\"4\">kd树</font></h2><ul>\n<li>kd树是用来存储训练数据的特殊结构，提高了k近邻搜索的效率，其实就是一种二叉查找树</li>\n<li>kd树的每一个节点对应于一个k维超矩形区域</li>\n<li>构造平衡kd树的方法:依次对各个维度上取中位数进行划分，例如三维，先以x轴上中位数划线，分为两部分，每一个部分在以y轴中位数划线，然后再z轴，然后再x,y,z循环。</li>\n<li>构造完成kd树后就可以利用kd树进行k近邻搜索，以下用最近邻搜索为例，即k=1：<ul>\n<li>从根节点出发，递归向下搜索目标点所在区域，直到叶节点</li>\n<li>以此叶节点为当前最近点，当前最近点到目标点的距离是当前最近距离，我们的目标就是在树中搜索，找到合适的节点更新当前最近点和当前最近距离</li>\n<li>递归向上回退，对每个节点做如下操作<ul>\n<li>如果该节点保存的实例点比当前最近点距离目标点更近，则更新该实例点为当前最近点，更新该实例点到目标点的距离为当前最近距离</li>\n<li>该节点的子节点分为两个，其中一个包含了我们的目标点，这一部分我们是从目标点一路向上递归过来的，已经更新完了，因此要找该节点的另外一个子节点，看看可不可以更新：检查另一子节点对应区域是否与以目标点为球心，当前最近距离为半径的超球体相交，相交的话就移动到这个子节点，然后接着向上搜索，不相交则不移动，依然接着向上搜索。</li>\n<li>直到搜索到根节点，此时的当前最近点即目标点的最近邻点。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>kd树搜索的计算复杂度是$O(logN)$，适用于训练实例数远大于空间维数的k近邻搜索</li>\n</ul>\n<h1 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a><font size=\"5\">支持向量机</font></h1><h2 id=\"线性可分支持向量机与硬间隔最大化\"><a href=\"#线性可分支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可分支持向量机与硬间隔最大化\"></a><font size=\"4\">线性可分支持向量机与硬间隔最大化</font></h2><ul>\n<li>学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类(二分类)，分离超平面对应于方程$wx+b=0$，由法向量w和截距b决定，可由(w,b)表示</li>\n<li>这里的x是特征向量$(x_1,x_2,…)$，而y是特征向量的标签</li>\n<li>给定线性可分训练数据集，通过间隔最大化或者等价的求解相应的凸二次规划问题学习得到的分离超平面为$wx+b=0$，以及相应的分类决策函数$f(x)=sign(wx+b)$称为线性可分支持向量机</li>\n<li>在超平面$wx+b=0$确定的情况下，点(x,y)到超平面的距离可以为<script type=\"math/tex; mode=display\">\n\\gamma _i=\\frac{w}{||w||}x_i+\\frac{b}{||w||}</script></li>\n<li>而$wx+b$的符号与类标记y的符号是否一致可以表示分类是否正确，$y=\\pm 1$，这样就可以得到几何间隔<script type=\"math/tex; mode=display\">\n\\gamma _i=y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\\\\n定义超平面(w,b)关于训练数据集T的几何间隔为关于所有样本点的几何间隔的最小值 \\\\\n\\gamma=min\\gamma _i \\\\</script></li>\n<li>同时定义相对距离为函数间隔<script type=\"math/tex; mode=display\">\n\\gamma _i=y_i(wx_i+b) \\\\\n\\gamma =min\\gamma _i \\\\</script></li>\n<li>硬间隔最大化针对线性可分超平面而言，软间隔最大化针对近似线性可分而言</li>\n<li>求一个几何间隔最大的分离超平面，可以表示为下面的约束最优化问题<script type=\"math/tex; mode=display\">\nmax_{(w,b)} \\gamma \\\\\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||})\\geq \\gamma ,i=1,2,...,N \\\\</script></li>\n<li>我们可以将几何间隔转化为函数间隔，对最优化没有影响，同时，如果我们固定相对间隔为常量(1)，则对几何间隔的最大化可以转化为对$||w||$的最小化，因此约束最优化问题可以改写为<script type=\"math/tex; mode=display\">\nmin_{(w,b)} \\frac12 {||w||}^2 \\\\\ns.t. \\quad y_i(wx_i+b)-1 \\geq 0 \\\\</script></li>\n<li>上式就是SVM的基本型,当解出以上最优化问题时，我们就得到了一个拥有最大间隔的分离超平面，这就是最大间隔法</li>\n<li>最大间隔分离超平面存在且唯一，证明略</li>\n<li>在线性可分情况下，训练数据集的样本点中分离超平面最近的样本点的实例称为支持向量</li>\n<li>对$y_i=1$的正例点，支持向量在平面$wx+b=1$上，同理负例点在平面$wx+b=-1$上，这两个平面平行，之间没有任何训练数据点，两个平面的间隔距离为间隔(margin)，间隔依赖于分离超平面的法向量w，为$\\frac 2 {||w||}$</li>\n<li>由此可见支持向量机由很少的重要的训练样本(支持向量)决定</li>\n<li>为了解最优化问题，我们引入对偶算法，同时引入核函数以便推广到非线性分类问题</li>\n<li>待补充</li>\n</ul>\n<h1 id=\"线代基础\"><a href=\"#线代基础\" class=\"headerlink\" title=\"线代基础\"></a><font size=\"5\">线代基础</font></h1><h2 id=\"Moore-penrose\"><a href=\"#Moore-penrose\" class=\"headerlink\" title=\"Moore-penrose\"></a><font size=\"4\">Moore-penrose</font></h2><ul>\n<li>对于非方矩阵，其逆矩阵没有定义，因此我们特别定义非方矩阵的伪逆：Moore-Penrose 伪逆<script type=\"math/tex; mode=display\">\nA^+=lim_{\\alpha \\rightarrow 0}(A^TA+\\alpha I)^{-1}A^T</script></li>\n<li>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：<script type=\"math/tex; mode=display\">\nA^+=VD^+U^T</script>其中U、D、V是矩阵A奇异值分解后得到的矩阵，对角矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。</li>\n<li>当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+y$是方程所有可行解中欧几里得范数最小的一个。</li>\n<li>当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离最小。</li>\n<li>待补充</li>\n</ul>\n<h2 id=\"迹\"><a href=\"#迹\" class=\"headerlink\" title=\"迹\"></a><font size=\"4\">迹</font></h2><ul>\n<li>迹运算返回的是矩阵对角元素的和.</li>\n<li>使用迹运算可以描述矩阵Frobenius范数的方式：<script type=\"math/tex; mode=display\">\n||A_F||=\\sqrt{Tr(AA^T)}</script></li>\n<li>迹具有转置不变性和轮换不变性</li>\n<li>标量的迹是其本身</li>\n</ul>\n<h2 id=\"PCA解释\"><a href=\"#PCA解释\" class=\"headerlink\" title=\"PCA解释\"></a><font size=\"4\">PCA解释</font></h2><ul>\n<li>待补充</li>\n</ul>\n<h1 id=\"概率论信息论\"><a href=\"#概率论信息论\" class=\"headerlink\" title=\"概率论信息论\"></a><font size=\"5\">概率论信息论</font></h1><h2 id=\"Logistic-Sigmoid\"><a href=\"#Logistic-Sigmoid\" class=\"headerlink\" title=\"Logistic Sigmoid\"></a><font size=\"4\">Logistic Sigmoid</font></h2><ul>\n<li>Logistic和sigmoid两种称呼经常混用，这个函数用于将实数压缩到(0,1)之间，代表二分类概率：<script type=\"math/tex; mode=display\">\n\\sigma (x) = \\frac{1}{1+exp(-x)}</script></li>\n<li>Softmax 是sigmoid的扩展版，是argmax函数的软化版本（argmax返回一个one hot 向量而softmax返回的是各种可能的概率），将二分类扩展到多分类（互斥）情况：<script type=\"math/tex; mode=display\">\n\\sigma (z)_j = \\frac{e^z j}{\\sum _{k=1}^K e^z k}</script></li>\n<li>两者在输入过大过小时都存在饱和现象，但将两个函数作为非线性激活单元引入神经网络时，因为代价函数是取负对数，可以消除这种饱和现象。</li>\n<li>Softmax函数因为包含指数函数，还存在上下溢出问题。当输入均匀分布且输入样本数量很大时，分母指数值接近于0，累加也可能接近于0，导致分母下溢。当指数函数参数很大时也会导致上溢。解决办法是将输入x处理为z=x-max(xi)，即向量的每个分量都减去最大分量，输入向量加减标量不会导致softmax函数值改变(softmax函数的冗余性），但此时输入经处理后最大值为0，排除上溢，经过指数函数后分母的累加项中至少存在一个1，排除下溢。</li>\n<li>利用softmax函数的冗余性也可以推出sigmoid是softmax的一种特例：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0HRXT.png\" alt=\"i0HRXT.png\"></li>\n</ul>\n<h2 id=\"KL散度和交叉熵\"><a href=\"#KL散度和交叉熵\" class=\"headerlink\" title=\"KL散度和交叉熵\"></a><font size=\"4\">KL散度和交叉熵</font></h2><ul>\n<li>KL散度：用以衡量PQ两个分布之间的差异，非负且不对称：<script type=\"math/tex; mode=display\">\nD_{KL}(P||Q) = E_{x \\sim P} [log \\frac{P(x)}{Q(x)}] = E_{x \\sim P} [log P(x) - log Q(x)]</script></li>\n<li>交叉熵：<script type=\"math/tex; mode=display\">\nH(P,Q) = -E_{x \\sim P} log Q(x)</script></li>\n<li>交叉熵形式简单，而且针对Q（实际输出）最小化KL散度与散度公式中前一项无关系，因此最小化KL散度实际上可以看成最小化交叉熵，又因为KL散度代表PQ（实际输出和正确输出）之间的差异，即可以看作是损失函数</li>\n<li>在用logistic处理二分类问题中，q(x)即logistic函数,p(x)即实际数据的正确分布(0或者1)</li>\n<li>对q按照p求自信息期望即二元交叉熵(Logistic代价函数):<script type=\"math/tex; mode=display\">\nJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m y^{(i)} log h_{\\theta} (x^{(i)}) + (1-y^{(i)}) log (1-h_{\\theta}(x^{(i)}))]</script></li>\n<li>同理可得多元交叉熵(Softmaxs代价函数):<script type=\"math/tex; mode=display\">\nJ(\\theta) = - \\frac 1m [\\sum _{i=1}^m \\sum _{j=1}^k 1\\{ y^{(i)}=j \\} log \\frac {e^{\\theta _j ^T x^{(i)}}} {\\sum _{l=1}^k e^{\\theta _j ^T x^{(i)}}}]</script></li>\n</ul>\n<h2 id=\"交叉熵与最大对数似然关系\"><a href=\"#交叉熵与最大对数似然关系\" class=\"headerlink\" title=\"交叉熵与最大对数似然关系\"></a><font size=\"4\">交叉熵与最大对数似然关系</font></h2><ul>\n<li>已知一个样本数据集X，分布为$P_{data}(x)$，我们希望得到一个模型$P_{model}(x,\\theta)$，其分布尽可能接近$P_{data}(x)$。$P_model(x,\\theta)$将任意x映射为实数来估计真实概率$P_{data}(x)$。<br>在$P_{model}(x,\\theta)$中，对$\\theta$的最大似然估计为使样本数据通过模型得到概率之积最大的$\\theta$：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} p_{model} (X;\\theta)</script></li>\n<li>因为取对数和尺度变换不会改变argmax，取对数变累加并除以样本数量平均后得到：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} E_{x \\sim p_{data}} log p_{model}(x;\\theta)</script></li>\n<li>可以发现上式即交叉熵的相反数，当Pdata(x)=Pmodel(x,θ)时上式值最大，所以:</li>\n<li>最大似然=最小负对数似然=最小化交叉熵=最小化KL散度=最小化数据与模型之间的差距∈最小化代价函数</li>\n<li>最大似然估计可扩展到最大条件似然估计，构成了大多数监督学习基础：公式：<script type=\"math/tex; mode=display\">\n\\theta _{ML} = \\mathop{argmax}\\limits_{\\theta} \\sum_{i=1}^m log P(y^{(i)} | x^{(i)} ; \\theta)</script></li>\n<li>最大似然估计具有一致性。</li>\n</ul>\n<h1 id=\"计算方法\"><a href=\"#计算方法\" class=\"headerlink\" title=\"计算方法\"></a><font size=\"5\">计算方法</font></h1><h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a><font size=\"4\">梯度下降</font></h2><ul>\n<li>问题：为了使函数变小（最小化代价函数），怎样变换参数（输入）？</li>\n<li>原理：将输入向导数的反方向移动一小步可以减小函数输出。</li>\n<li>将输入扩展到向量形式的参数，将函数看成代价函数，即得到基于梯度的优化算法。</li>\n<li>一阶优化算法：包括梯度下降，使用Jacobian矩阵（包含向量之间偏导数关系），通过梯度下降对参数的建议更新为：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0opQO.png\" alt=\"i0opQO.png\"></li>\n</ul>\n<h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a><font size=\"4\">牛顿法</font></h2><ul>\n<li>二阶优化算法：（求最优补偿，定性临界点）:一阶优化需要调整合适的学习率（步长），否则无法达到最优点或者会产生抖动，且在临界点（梯度为0）无法更新参数，这反映我们需要代价函数的二阶导数信息，例如函数向上凸出或向下凸出时基于梯度的预测值和真实的代价函数值之间有偏差。Hessian矩阵包含了二阶信息。牛顿法使用了Hessian矩阵的信息，利用泰勒二阶展开得到函数信息，利用下式更新参数：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o9yD.png\" alt=\"i0o9yD.png\"><h2 id=\"约束优化\"><a href=\"#约束优化\" class=\"headerlink\" title=\"约束优化\"></a><font size=\"4\">约束优化</font></h2></li>\n<li>只包含等式约束条件：Lagrange </li>\n<li>包含不等式约束条件：KTT</li>\n</ul>\n<h1 id=\"修改算法\"><a href=\"#修改算法\" class=\"headerlink\" title=\"修改算法\"></a><font size=\"5\">修改算法</font></h1><h2 id=\"修改假设空间\"><a href=\"#修改假设空间\" class=\"headerlink\" title=\"修改假设空间\"></a><font size=\"4\">修改假设空间</font></h2><ul>\n<li>机器学习算法应避免过拟合和欠拟合，可以通过调整模型容量（拟合各种函数的能力）来解决。</li>\n<li>调整模型容量的方案是选择合适的假设空间（假设输入而不是参数），例如之前只拟合多项式线性函数：<script type=\"math/tex; mode=display\">\ny = b + wx</script></li>\n<li>如果引入非线性单元，例如高次项，输出相对于参数仍然是线性分布：<script type=\"math/tex; mode=display\">\ny= b + w_1 x + w_2 x^2</script>此时就增加了模型的容量，但简化了生成的参数，适用于解决复杂的问题，然而容量太高也有可能过拟合。</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a><font size=\"4\">正则化</font></h2><ul>\n<li>没有免费午餐定理（在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率）说明应在特定任务上设计机器学习算法，算法应该有偏好。将代价函数中加入正则化即引入偏好，使得学习到的参数偏向于使正则化项变小。</li>\n<li>一个例子是权重衰减，加入权重衰减正则化项的代价函数是：<script type=\"math/tex; mode=display\">\nJ(w) = MSE_{train} + \\lambda w^T w</script>λ控制偏好程度，生成的模型倾向于小参数，可以避免过拟合。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0H2cV.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"机器学习笔记","path":"2017/02/12/MachineLearningNote/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0H2cV.png","excerpt":"<hr>\n<p>记录机器学习中关于一些概念和算法的笔记，来源于:</p>\n<ul>\n<li>选修课模式识别(大三北邮选修课，模式识别，教材是张学工编著的《模式识别》，清华大学出版社)</li>\n<li>西瓜书</li>\n<li>《统计学习方法》</li>\n<li>《深度学习》（感谢中文翻译：<a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"noopener\">exacity/deeplearningbook-chinese</a>）</li>\n</ul>\n<p>更新：</p>\n<ul>\n<li>2017-02-12 更新概论</li>\n<li>2017-03-01 更新k近邻</li>\n<li>2017-03-08 更新SVM</li>\n<li>2018-01-04 更新《深度学习》一书中的机器学习基础知识和数学知识</li>\n<li>2018-08-09 统计学习方法的内容已经贴在另一篇《统计学习方法手写版笔记》里了，估计不会更新了，之后可能更新《深度学习》里一些剩下的内容</li></ul>","date":"2017-02-12T14:40:38.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["code","machine learning"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"MIT线性代数笔记2","date":"2017-01-21T11:28:03.000Z","mathjax":true,"html":true,"_content":"\n***\n# 第九讲：线性相关性、基、维数\n\n## 线性相关性\n-\t背景知识:假设一个矩阵A，m<n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。\n-\t什么条件下，$x_1,x_2,x_3...x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。\n-\t如果向量组里存在一个零向量，则这个向量组不可能线性无关。\n-\t假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。\n-\t对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。\n-\t换一种方式解释：当$v_1,v_2...v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。\n-\t列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。\n\n<!--more-->\n\n\n## 生成空间、基\n-\t$v_1...,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。\n-\t向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。\n-\t举个栗子：求$R^3$的一组基，最容易想到的是\n$$\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}\n$$\n-\t这是一组标准基，另一个栗子:\n$$\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}\n$$\n\n-\t显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。\n-\t如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。\n-\t若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。\n-\t基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。\n\n## 维数\n-\t上面提到的所有基向量的个数相同，这个个数就是空间的维数。**不是基向量的维数，而是基向量的个数**\n\n## 最后举个栗子\n对矩阵A\n$$\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t四列并不线性无关，可取第一列第二列为主列\n-\t2=A的秩=主列数=列空间维数\n-\t第一列和第二列构成列空间的一组基。\n-\t如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。\n-\t零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：\n$$\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1\t\\\\\n\\end{bmatrix}\n$$\n-\t这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。\n\n# 第十讲：四个基本子空间\n-\t列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。\n-\t分别处于$R^m、R^n、R^n、R^m$空间中\n-\t列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r\n-\t列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行\n-\t行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化\n-\t为什么叫做左零空间？\n$$\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\\n$$\n-\t易得rref=E，即EA=R \n-\t通过E可以计算左零空\n-\t求左零空间即找一个产生零行向量的行组合 \n-\t左零空间的基就是R非0行对应的E行,共m-r行 \n\n\n# 第十一讲：矩阵空间、秩1矩阵和小世界图\n\n## 矩阵空间\n-\t可以看成向量空间，可以数乘，可以相加\n-\t以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n$$\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基\n-\t接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3\n-\t如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9\n-\t整理一下可得\n$$\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\\n$$\n-\t再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间\n$$\n\\frac{d^2y}{dx^2}+y=0 \n$$\n-\t他的几个解为 \n$$\ny=cos(x),y=sin(x) \n$$\n-\t完整解为  \n$$\ny=c_1cos(x)+c_2sin(x) \n$$\n-\t即得到一个向量空间，基为2\n\n\n## 秩1矩阵\n-\t先写一个简单的秩1矩阵\n$$\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}\n$$\n-\t所有的秩1矩阵都可以表示为一列乘一行\n-\t秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成\n-\t再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为\n$$\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为\n$$\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n$$\n-\t矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$\n-\t整理一下\n$$\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\\n$$\n\n## 小世界图\n-\t仅仅引入了图的概念，为下一讲准备\n\n# 第十二讲：图和网络\n## 图\n-  图的一些基础概念，略过\n\n## 网络\n\n-  图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0\n\n-  构成回路的几行线性相关，回路意味着相关\n\n-  关联矩阵A描述了图的拓扑结构\n\n-  $dimN(A^T)=m-r​$\n\n-  假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路\n\n-  树就是没有回路的图\n\n-  再来看看$dimN(A^T)=m-r$\n\n-\t$dimN(A^T)$=无关回路数  \n-\t$m$=边数 \n-\t$r=n-1$=节点数-1 (因为零空间是一维的) \n-\t即:节点数-边数+回路数=1(欧拉公式) \n\n\n\n## <font size=4>总结\n\n- 将电势记为e,$e=Ax$\n\n- 电势差导致电流产生，$y=Ce$\n\n- 电流满足基尔霍夫电流方程,$A^Ty=0$\n\n- 将三个方程联立：\n  $$\n  A^TCAx=f\n  $$\n  这就是应用数学中最基本的平衡方程\n\n# 第十三讲：正交向量与子空间\n\n## 正交向量\n-\t正交即垂直，意味着在n维空间内，这些向量的夹角是90度\n-\t当$x^Ty=0$,x与y正交，证明：\n-\t若x与y正交，易得:\n$$\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\\n$$\n-\t即：\n$$\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\\n$$\n-\t即：\n$$\nx^Ty=0 \\\\\n$$\n\n\n-\t子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交\n-\t若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己\n-\t行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分\n-\t图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立\n-\t图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。\n\n## 正交子空间\n-\t例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交\n-\t因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量\n-\t以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$\n-\t$A^TA$是一个$n*n$的方阵，而且对称\n-\t坏方程转换为好方程，两边同乘$A^T$\n-\t$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同\n-\t$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质\n\n# 第十四讲：子空间投影\n## 投影\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG)\n-\t在二维情况下讨论投影\n-\t一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p\n-\tp在a的一维子空间里，是a的x倍，即p=xa\n-\ta垂直于e，即\n$$\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\\n$$\n-\t从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了\n\n## 投影矩阵\n-\t现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$\n-\t用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1\n-\t投影矩阵的另外两条性质：\n -\t对称,即$P^T=P$\n -\t两次投影在相同的位置，即$P^2=P$\n \n## 投影的意义\n-\t下面在高维情况下讨论\n-\t当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解\n-\t如何找？将b微调，使得b在列空间中\n-\t怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{'}=p$,p时b在列空间上的投影\n-\t现在我们要求$x^{'}$,$p=Ax^{'}$，误差向量$e=b-Ax^{'}$，由投影定义可知e需要垂直于A的列空间\n-\t综上可得\n$$\nA^T(b-Ax^{'})=0 \\\\\n$$\n-\t由上式可以看出e在A的左零空间，与列空间正交。解上式可得\n$$\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\\n$$\n-\t即投影矩阵P的n维模式:\n$$\nA(A^TA)^{-1}A^T \\\\\n$$\n-\t投影矩阵P的n维模式依然保留了1维模式的两个性质\n-\t现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线\n-\t已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$\n-\t假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n写成线代形式为:\n$$\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}\n$$\n-\t即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{'}$，这样就可以求出拟合直线。下一讲继续此例\n\n# 第十五讲：投影矩阵和最小二乘法\n\n## 投影矩阵\n-\t回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：\n\tb在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$\n\tb正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$\n\n\n-\tp是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：\n$$\nb=p+e \\\\\np=Pb \\\\\n$$\n-\t所以\n$$\ne=(I-P)b \\\\\n$$\n-\t所以左零空间的投影矩阵为$(I-P)$ \n\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG)\n\t\n## 最小二乘法\n-\t回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG)\n-\t设直线为$y=C+Dt$，代入三个点坐标得到一个方程组\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n-\t此方程组无解但是存在最优价，从代数角度看：\n$$\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\\n$$\n-\t分别对C和D求偏导为0，得到方程组: \n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。\n$$\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n$$\n-\tA满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。\n-\t现在要解出$x^{'},p$，已知$p=Ax^{'}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有\n$$\nA^TAx^{'}=A^Tb\n$$\n-\t代入数值可得\n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线\n-\t回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p\n-\t如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：\n\t如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量\n\t$$\n\tA^TAx=0 \\\\\n\tx^TA^TAx=0 \\\\\n\t(Ax)^T(Ax)=0 \\\\\n\tAx=0 \\\\\n\t$$\n-\t又因为A各列线性无关，所以\n\t$$\n\tx=0\n\t$$\n-\t即证\n-\t对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容\n\n# 第十六讲：正交矩阵和Gram-Schmidt正交化\n\n## 正交矩阵\n-\t已知一组正交向量集\n$$\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}\n$$\n$$\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\\n$$\n-\t所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如\n$$\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n$$\n-\tQ不一定是方阵。Q的各列将是列空间的标准正交基\n-\t对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$\n\n## Gram-Schmidt正交化\n-\t给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：\n$$\nB=e=b-\\frac{A^Tb}{A^TA}A\n$$\n-\t正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$\n-\t扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量\n$$\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B\n$$\n-\t由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,...$与$a,b,....$在同一列空间内，正交化可以写成\n$$\nA=QR \\\\\n$$\n-\t即\n$$\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\\n$$\n-\t其中，因为$QQ^T=I$\n-\t所以$R=Q^TA$\n-\t$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵","source":"_posts/LinearAlgebra2.md","raw":"---\ntitle: MIT线性代数笔记2\ndate: 2017-01-21 19:28:03\ntags: [linearalgebra,math]\ncategories: 数学\nmathjax: true\nhtml: true\n---\n\n***\n# 第九讲：线性相关性、基、维数\n\n## 线性相关性\n-\t背景知识:假设一个矩阵A，m<n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。\n-\t什么条件下，$x_1,x_2,x_3...x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。\n-\t如果向量组里存在一个零向量，则这个向量组不可能线性无关。\n-\t假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。\n-\t对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。\n-\t换一种方式解释：当$v_1,v_2...v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。\n-\t列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。\n\n<!--more-->\n\n\n## 生成空间、基\n-\t$v_1...,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。\n-\t向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。\n-\t举个栗子：求$R^3$的一组基，最容易想到的是\n$$\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}\n$$\n-\t这是一组标准基，另一个栗子:\n$$\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}\n$$\n\n-\t显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。\n-\t如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。\n-\t若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。\n-\t基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。\n\n## 维数\n-\t上面提到的所有基向量的个数相同，这个个数就是空间的维数。**不是基向量的维数，而是基向量的个数**\n\n## 最后举个栗子\n对矩阵A\n$$\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t四列并不线性无关，可取第一列第二列为主列\n-\t2=A的秩=主列数=列空间维数\n-\t第一列和第二列构成列空间的一组基。\n-\t如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。\n-\t零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：\n$$\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1\t\\\\\n\\end{bmatrix}\n$$\n-\t这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。\n\n# 第十讲：四个基本子空间\n-\t列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。\n-\t分别处于$R^m、R^n、R^n、R^m$空间中\n-\t列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r\n-\t列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行\n-\t行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化\n-\t为什么叫做左零空间？\n$$\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\\n$$\n-\t易得rref=E，即EA=R \n-\t通过E可以计算左零空\n-\t求左零空间即找一个产生零行向量的行组合 \n-\t左零空间的基就是R非0行对应的E行,共m-r行 \n\n\n# 第十一讲：矩阵空间、秩1矩阵和小世界图\n\n## 矩阵空间\n-\t可以看成向量空间，可以数乘，可以相加\n-\t以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n-\t再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6\n$$\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}\n$$\n$$\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}\n$$\n-\t对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基\n-\t接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3\n-\t如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9\n-\t整理一下可得\n$$\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\\n$$\n-\t再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间\n$$\n\\frac{d^2y}{dx^2}+y=0 \n$$\n-\t他的几个解为 \n$$\ny=cos(x),y=sin(x) \n$$\n-\t完整解为  \n$$\ny=c_1cos(x)+c_2sin(x) \n$$\n-\t即得到一个向量空间，基为2\n\n\n## 秩1矩阵\n-\t先写一个简单的秩1矩阵\n$$\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}\n$$\n-\t所有的秩1矩阵都可以表示为一列乘一行\n-\t秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成\n-\t再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为\n$$\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}\n$$\n-\t易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为\n$$\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}\n$$\n-\t矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$\n-\t整理一下\n$$\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\\n$$\n\n## 小世界图\n-\t仅仅引入了图的概念，为下一讲准备\n\n# 第十二讲：图和网络\n## 图\n-  图的一些基础概念，略过\n\n## 网络\n\n-  图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0\n\n-  构成回路的几行线性相关，回路意味着相关\n\n-  关联矩阵A描述了图的拓扑结构\n\n-  $dimN(A^T)=m-r​$\n\n-  假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路\n\n-  树就是没有回路的图\n\n-  再来看看$dimN(A^T)=m-r$\n\n-\t$dimN(A^T)$=无关回路数  \n-\t$m$=边数 \n-\t$r=n-1$=节点数-1 (因为零空间是一维的) \n-\t即:节点数-边数+回路数=1(欧拉公式) \n\n\n\n## <font size=4>总结\n\n- 将电势记为e,$e=Ax$\n\n- 电势差导致电流产生，$y=Ce$\n\n- 电流满足基尔霍夫电流方程,$A^Ty=0$\n\n- 将三个方程联立：\n  $$\n  A^TCAx=f\n  $$\n  这就是应用数学中最基本的平衡方程\n\n# 第十三讲：正交向量与子空间\n\n## 正交向量\n-\t正交即垂直，意味着在n维空间内，这些向量的夹角是90度\n-\t当$x^Ty=0$,x与y正交，证明：\n-\t若x与y正交，易得:\n$$\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\\n$$\n-\t即：\n$$\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\\n$$\n-\t即：\n$$\nx^Ty=0 \\\\\n$$\n\n\n-\t子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交\n-\t若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己\n-\t行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分\n-\t图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立\n-\t图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。\n\n## 正交子空间\n-\t例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交\n-\t因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量\n-\t以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$\n-\t$A^TA$是一个$n*n$的方阵，而且对称\n-\t坏方程转换为好方程，两边同乘$A^T$\n-\t$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同\n-\t$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质\n\n# 第十四讲：子空间投影\n## 投影\n\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG)\n-\t在二维情况下讨论投影\n-\t一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p\n-\tp在a的一维子空间里，是a的x倍，即p=xa\n-\ta垂直于e，即\n$$\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\\n$$\n-\t从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了\n\n## 投影矩阵\n-\t现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$\n-\t用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1\n-\t投影矩阵的另外两条性质：\n -\t对称,即$P^T=P$\n -\t两次投影在相同的位置，即$P^2=P$\n \n## 投影的意义\n-\t下面在高维情况下讨论\n-\t当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解\n-\t如何找？将b微调，使得b在列空间中\n-\t怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{'}=p$,p时b在列空间上的投影\n-\t现在我们要求$x^{'}$,$p=Ax^{'}$，误差向量$e=b-Ax^{'}$，由投影定义可知e需要垂直于A的列空间\n-\t综上可得\n$$\nA^T(b-Ax^{'})=0 \\\\\n$$\n-\t由上式可以看出e在A的左零空间，与列空间正交。解上式可得\n$$\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\\n$$\n-\t即投影矩阵P的n维模式:\n$$\nA(A^TA)^{-1}A^T \\\\\n$$\n-\t投影矩阵P的n维模式依然保留了1维模式的两个性质\n-\t现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线\n-\t已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$\n-\t假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n写成线代形式为:\n$$\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}\n$$\n-\t即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{'}$，这样就可以求出拟合直线。下一讲继续此例\n\n# 第十五讲：投影矩阵和最小二乘法\n\n## 投影矩阵\n-\t回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：\n\tb在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$\n\tb正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$\n\n\n-\tp是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：\n$$\nb=p+e \\\\\np=Pb \\\\\n$$\n-\t所以\n$$\ne=(I-P)b \\\\\n$$\n-\t所以左零空间的投影矩阵为$(I-P)$ \n\n\t![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG)\n\t\n## 最小二乘法\n-\t回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图\n![mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG)\n-\t设直线为$y=C+Dt$，代入三个点坐标得到一个方程组\n$$\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\\n$$\n-\t此方程组无解但是存在最优价，从代数角度看：\n$$\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\\n$$\n-\t分别对C和D求偏导为0，得到方程组: \n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。\n$$\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n$$\n-\tA满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。\n-\t现在要解出$x^{'},p$，已知$p=Ax^{'}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有\n$$\nA^TAx^{'}=A^Tb\n$$\n-\t代入数值可得\n$$\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}\n$$\n\n-\t与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线\n-\t回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p\n-\t如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：\n\t如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量\n\t$$\n\tA^TAx=0 \\\\\n\tx^TA^TAx=0 \\\\\n\t(Ax)^T(Ax)=0 \\\\\n\tAx=0 \\\\\n\t$$\n-\t又因为A各列线性无关，所以\n\t$$\n\tx=0\n\t$$\n-\t即证\n-\t对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容\n\n# 第十六讲：正交矩阵和Gram-Schmidt正交化\n\n## 正交矩阵\n-\t已知一组正交向量集\n$$\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}\n$$\n$$\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\\n$$\n-\t所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如\n$$\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}\n$$\n-\tQ不一定是方阵。Q的各列将是列空间的标准正交基\n-\t对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$\n\n## Gram-Schmidt正交化\n-\t给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：\n$$\nB=e=b-\\frac{A^Tb}{A^TA}A\n$$\n-\t正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$\n-\t扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量\n$$\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B\n$$\n-\t由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,...$与$a,b,....$在同一列空间内，正交化可以写成\n$$\nA=QR \\\\\n$$\n-\t即\n$$\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\\n$$\n-\t其中，因为$QQ^T=I$\n-\t所以$R=Q^TA$\n-\t$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵","slug":"LinearAlgebra2","published":1,"updated":"2019-07-22T03:45:22.781Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6va50044q8t5goc0bs6z","content":"<hr>\n<h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul>\n<li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li>\n<li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li>\n<li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li>\n<li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li>\n<li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li>\n<li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li>\n<li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li>\n</ul>\n<a id=\"more\"></a>\n<h2 id=\"生成空间、基\"><a href=\"#生成空间、基\" class=\"headerlink\" title=\"生成空间、基\"></a>生成空间、基</h2><ul>\n<li>$v_1…,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。</li>\n<li>向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。</li>\n<li>举个栗子：求$R^3$的一组基，最容易想到的是<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}</script></li>\n<li><p>这是一组标准基，另一个栗子:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}</script></li>\n<li><p>显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。</p>\n</li>\n<li>如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。</li>\n<li>若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。</li>\n<li>基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。</li>\n</ul>\n<h2 id=\"维数\"><a href=\"#维数\" class=\"headerlink\" title=\"维数\"></a>维数</h2><ul>\n<li>上面提到的所有基向量的个数相同，这个个数就是空间的维数。<strong>不是基向量的维数，而是基向量的个数</strong></li>\n</ul>\n<h2 id=\"最后举个栗子\"><a href=\"#最后举个栗子\" class=\"headerlink\" title=\"最后举个栗子\"></a>最后举个栗子</h2><p>对矩阵A</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}</script><ul>\n<li>四列并不线性无关，可取第一列第二列为主列</li>\n<li>2=A的秩=主列数=列空间维数</li>\n<li>第一列和第二列构成列空间的一组基。</li>\n<li>如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。</li>\n<li>零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1    \\\\\n\\end{bmatrix}</script></li>\n<li>这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。</li>\n</ul>\n<h1 id=\"第十讲：四个基本子空间\"><a href=\"#第十讲：四个基本子空间\" class=\"headerlink\" title=\"第十讲：四个基本子空间\"></a>第十讲：四个基本子空间</h1><ul>\n<li>列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。</li>\n<li>分别处于$R^m、R^n、R^n、R^m$空间中</li>\n<li>列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r</li>\n<li>列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行</li>\n<li>行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化</li>\n<li>为什么叫做左零空间？<script type=\"math/tex; mode=display\">\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\</script></li>\n<li>易得rref=E，即EA=R </li>\n<li>通过E可以计算左零空</li>\n<li>求左零空间即找一个产生零行向量的行组合 </li>\n<li>左零空间的基就是R非0行对应的E行,共m-r行 </li>\n</ul>\n<h1 id=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"><a href=\"#第十一讲：矩阵空间、秩1矩阵和小世界图\" class=\"headerlink\" title=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"></a>第十一讲：矩阵空间、秩1矩阵和小世界图</h1><h2 id=\"矩阵空间\"><a href=\"#矩阵空间\" class=\"headerlink\" title=\"矩阵空间\"></a>矩阵空间</h2><ul>\n<li>可以看成向量空间，可以数乘，可以相加</li>\n<li>以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基</li>\n<li>接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3</li>\n<li>如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9</li>\n<li>整理一下可得<script type=\"math/tex; mode=display\">\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\</script></li>\n<li>再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间<script type=\"math/tex; mode=display\">\n\\frac{d^2y}{dx^2}+y=0</script></li>\n<li>他的几个解为 <script type=\"math/tex; mode=display\">\ny=cos(x),y=sin(x)</script></li>\n<li>完整解为  <script type=\"math/tex; mode=display\">\ny=c_1cos(x)+c_2sin(x)</script></li>\n<li>即得到一个向量空间，基为2</li>\n</ul>\n<h2 id=\"秩1矩阵\"><a href=\"#秩1矩阵\" class=\"headerlink\" title=\"秩1矩阵\"></a>秩1矩阵</h2><ul>\n<li>先写一个简单的秩1矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}</script></li>\n<li>所有的秩1矩阵都可以表示为一列乘一行</li>\n<li>秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成</li>\n<li>再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}</script></li>\n<li>易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}</script></li>\n<li>矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$</li>\n<li>整理一下<script type=\"math/tex; mode=display\">\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\</script></li>\n</ul>\n<h2 id=\"小世界图\"><a href=\"#小世界图\" class=\"headerlink\" title=\"小世界图\"></a>小世界图</h2><ul>\n<li>仅仅引入了图的概念，为下一讲准备</li>\n</ul>\n<h1 id=\"第十二讲：图和网络\"><a href=\"#第十二讲：图和网络\" class=\"headerlink\" title=\"第十二讲：图和网络\"></a>第十二讲：图和网络</h1><h2 id=\"图\"><a href=\"#图\" class=\"headerlink\" title=\"图\"></a>图</h2><ul>\n<li>图的一些基础概念，略过</li>\n</ul>\n<h2 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h2><ul>\n<li><p>图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0</p>\n</li>\n<li><p>构成回路的几行线性相关，回路意味着相关</p>\n</li>\n<li><p>关联矩阵A描述了图的拓扑结构</p>\n</li>\n<li><p>$dimN(A^T)=m-r​$</p>\n</li>\n<li><p>假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路</p>\n</li>\n<li><p>树就是没有回路的图</p>\n</li>\n<li><p>再来看看$dimN(A^T)=m-r$</p>\n</li>\n<li><p>$dimN(A^T)$=无关回路数  </p>\n</li>\n<li>$m$=边数 </li>\n<li>$r=n-1$=节点数-1 (因为零空间是一维的) </li>\n<li>即:节点数-边数+回路数=1(欧拉公式) </li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><font size=\"4\">总结</font></h2><ul>\n<li><p>将电势记为e,$e=Ax$</p>\n</li>\n<li><p>电势差导致电流产生，$y=Ce$</p>\n</li>\n<li><p>电流满足基尔霍夫电流方程,$A^Ty=0$</p>\n</li>\n<li><p>将三个方程联立：</p>\n<script type=\"math/tex; mode=display\">\nA^TCAx=f</script><p>这就是应用数学中最基本的平衡方程</p>\n</li>\n</ul>\n<h1 id=\"第十三讲：正交向量与子空间\"><a href=\"#第十三讲：正交向量与子空间\" class=\"headerlink\" title=\"第十三讲：正交向量与子空间\"></a>第十三讲：正交向量与子空间</h1><h2 id=\"正交向量\"><a href=\"#正交向量\" class=\"headerlink\" title=\"正交向量\"></a>正交向量</h2><ul>\n<li>正交即垂直，意味着在n维空间内，这些向量的夹角是90度</li>\n<li>当$x^Ty=0$,x与y正交，证明：</li>\n<li>若x与y正交，易得:<script type=\"math/tex; mode=display\">\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\</script></li>\n<li>即：<script type=\"math/tex; mode=display\">\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\</script></li>\n<li>即：<script type=\"math/tex; mode=display\">\nx^Ty=0 \\\\</script></li>\n</ul>\n<ul>\n<li>子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交</li>\n<li>若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己</li>\n<li>行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分</li>\n<li>图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立</li>\n<li>图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。</li>\n</ul>\n<h2 id=\"正交子空间\"><a href=\"#正交子空间\" class=\"headerlink\" title=\"正交子空间\"></a>正交子空间</h2><ul>\n<li>例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交</li>\n<li>因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量</li>\n<li>以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$</li>\n<li>$A^TA$是一个$n*n$的方阵，而且对称</li>\n<li>坏方程转换为好方程，两边同乘$A^T$</li>\n<li>$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同</li>\n<li>$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质</li>\n</ul>\n<h1 id=\"第十四讲：子空间投影\"><a href=\"#第十四讲：子空间投影\" class=\"headerlink\" title=\"第十四讲：子空间投影\"></a>第十四讲：子空间投影</h1><h2 id=\"投影\"><a href=\"#投影\" class=\"headerlink\" title=\"投影\"></a>投影</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG\" alt=\"mark\"></p>\n<ul>\n<li>在二维情况下讨论投影</li>\n<li>一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p</li>\n<li>p在a的一维子空间里，是a的x倍，即p=xa</li>\n<li>a垂直于e，即<script type=\"math/tex; mode=display\">\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\</script></li>\n<li>从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了</li>\n</ul>\n<h2 id=\"投影矩阵\"><a href=\"#投影矩阵\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul>\n<li>现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$</li>\n<li>用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1</li>\n<li>投影矩阵的另外两条性质：<ul>\n<li>对称,即$P^T=P$</li>\n<li>两次投影在相同的位置，即$P^2=P$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"投影的意义\"><a href=\"#投影的意义\" class=\"headerlink\" title=\"投影的意义\"></a>投影的意义</h2><ul>\n<li>下面在高维情况下讨论</li>\n<li>当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解</li>\n<li>如何找？将b微调，使得b在列空间中</li>\n<li>怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{‘}=p$,p时b在列空间上的投影</li>\n<li>现在我们要求$x^{‘}$,$p=Ax^{‘}$，误差向量$e=b-Ax^{‘}$，由投影定义可知e需要垂直于A的列空间</li>\n<li>综上可得<script type=\"math/tex; mode=display\">\nA^T(b-Ax^{'})=0 \\\\</script></li>\n<li>由上式可以看出e在A的左零空间，与列空间正交。解上式可得<script type=\"math/tex; mode=display\">\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\</script></li>\n<li>即投影矩阵P的n维模式:<script type=\"math/tex; mode=display\">\nA(A^TA)^{-1}A^T \\\\</script></li>\n<li>投影矩阵P的n维模式依然保留了1维模式的两个性质</li>\n<li>现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线</li>\n<li>已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$</li>\n<li>假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则<script type=\"math/tex; mode=display\">\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\</script>写成线代形式为:<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}</script></li>\n<li>即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{‘}$，这样就可以求出拟合直线。下一讲继续此例</li>\n</ul>\n<h1 id=\"第十五讲：投影矩阵和最小二乘法\"><a href=\"#第十五讲：投影矩阵和最小二乘法\" class=\"headerlink\" title=\"第十五讲：投影矩阵和最小二乘法\"></a>第十五讲：投影矩阵和最小二乘法</h1><h2 id=\"投影矩阵-1\"><a href=\"#投影矩阵-1\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul>\n<li>回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：<br>b在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$<br>b正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$</li>\n</ul>\n<ul>\n<li>p是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：<script type=\"math/tex; mode=display\">\nb=p+e \\\\\np=Pb \\\\</script></li>\n<li>所以<script type=\"math/tex; mode=display\">\ne=(I-P)b \\\\</script></li>\n<li><p>所以左零空间的投影矩阵为$(I-P)$ </p>\n<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG\" alt=\"mark\"></p>\n</li>\n</ul>\n<h2 id=\"最小二乘法\"><a href=\"#最小二乘法\" class=\"headerlink\" title=\"最小二乘法\"></a>最小二乘法</h2><ul>\n<li>回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG\" alt=\"mark\"></li>\n<li>设直线为$y=C+Dt$，代入三个点坐标得到一个方程组<script type=\"math/tex; mode=display\">\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\</script></li>\n<li>此方程组无解但是存在最优价，从代数角度看：<script type=\"math/tex; mode=display\">\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\</script></li>\n<li><p>分别对C和D求偏导为0，得到方程组: </p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}</script></li>\n<li><p>写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。</p>\n<script type=\"math/tex; mode=display\">\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}</script></li>\n<li>A满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。</li>\n<li>现在要解出$x^{‘},p$，已知$p=Ax^{‘}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有<script type=\"math/tex; mode=display\">\nA^TAx^{'}=A^Tb</script></li>\n<li><p>代入数值可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}</script></li>\n<li><p>与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线</p>\n</li>\n<li>回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p</li>\n<li>如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：<br>如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量<script type=\"math/tex; mode=display\">\nA^TAx=0 \\\\\nx^TA^TAx=0 \\\\\n(Ax)^T(Ax)=0 \\\\\nAx=0 \\\\</script></li>\n<li>又因为A各列线性无关，所以<script type=\"math/tex; mode=display\">\nx=0</script></li>\n<li>即证</li>\n<li>对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容</li>\n</ul>\n<h1 id=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"><a href=\"#第十六讲：正交矩阵和Gram-Schmidt正交化\" class=\"headerlink\" title=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"></a>第十六讲：正交矩阵和Gram-Schmidt正交化</h1><h2 id=\"正交矩阵\"><a href=\"#正交矩阵\" class=\"headerlink\" title=\"正交矩阵\"></a>正交矩阵</h2><ul>\n<li>已知一组正交向量集<script type=\"math/tex; mode=display\">\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\</script></li>\n<li>所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如<script type=\"math/tex; mode=display\">\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}</script></li>\n<li>Q不一定是方阵。Q的各列将是列空间的标准正交基</li>\n<li>对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$</li>\n</ul>\n<h2 id=\"Gram-Schmidt正交化\"><a href=\"#Gram-Schmidt正交化\" class=\"headerlink\" title=\"Gram-Schmidt正交化\"></a>Gram-Schmidt正交化</h2><ul>\n<li>给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：<script type=\"math/tex; mode=display\">\nB=e=b-\\frac{A^Tb}{A^TA}A</script></li>\n<li>正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$</li>\n<li>扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量<script type=\"math/tex; mode=display\">\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B</script></li>\n<li>由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,…$与$a,b,….$在同一列空间内，正交化可以写成<script type=\"math/tex; mode=display\">\nA=QR \\\\</script></li>\n<li>即<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>其中，因为$QQ^T=I$</li>\n<li>所以$R=Q^TA$</li>\n<li>$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul>\n<li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li>\n<li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li>\n<li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li>\n<li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li>\n<li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li>\n<li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li>\n<li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li>\n</ul>","more":"<h2 id=\"生成空间、基\"><a href=\"#生成空间、基\" class=\"headerlink\" title=\"生成空间、基\"></a>生成空间、基</h2><ul>\n<li>$v_1…,v_l$生成了一个空间，是指这个空间包含这些向量的所有线性组合。</li>\n<li>向量空间的一组基是指一个向量组，这些向量有两个特性：他们线性无关、他们生成整个空间。</li>\n<li>举个栗子：求$R^3$的一组基，最容易想到的是<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1   \\\\\n0   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n0   \\\\\n0   \\\\\n1   \\\\\n\\end{bmatrix}</script></li>\n<li><p>这是一组标准基，另一个栗子:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1   \\\\\n1   \\\\\n2   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n2   \\\\\n2   \\\\\n5   \\\\\n\\end{bmatrix}</script></li>\n<li><p>显然无法构成一个空间，只要再取一个不在这两个向量构成的平面上的任意一个向量即可。</p>\n</li>\n<li>如何检验他们是一组基？将他们作为列构成一个矩阵，矩阵必须可逆(因为此例中为方阵)。</li>\n<li>若只有例2中2个向量，他们无法构成三维空间的基，那么他们能构成什么空间的基呢？这两个向量所构成的平面。</li>\n<li>基不是唯一确定的，但所有的基都有共同点：基中向量的个数是相同的。</li>\n</ul>\n<h2 id=\"维数\"><a href=\"#维数\" class=\"headerlink\" title=\"维数\"></a>维数</h2><ul>\n<li>上面提到的所有基向量的个数相同，这个个数就是空间的维数。<strong>不是基向量的维数，而是基向量的个数</strong></li>\n</ul>\n<h2 id=\"最后举个栗子\"><a href=\"#最后举个栗子\" class=\"headerlink\" title=\"最后举个栗子\"></a>最后举个栗子</h2><p>对矩阵A</p>\n<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 2 & 3 &1  \\\\\n1 & 1 & 2 & 1   \\\\\n1 & 2 & 3 & 1 \\\\\n\\end{bmatrix}</script><ul>\n<li>四列并不线性无关，可取第一列第二列为主列</li>\n<li>2=A的秩=主列数=列空间维数</li>\n<li>第一列和第二列构成列空间的一组基。</li>\n<li>如果你知道列空间的维数，则确定了向量的个数，再满足线性无关，这些向量就可以构成一组基。</li>\n<li>零空间的维数是多少？在本例中零空间中的两个向量(特殊解)为：<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-1   \\\\\n-1  \\\\\n1   \\\\\n0   \\\\\n\\end{bmatrix}\n,\n\\begin{bmatrix}\n-1   \\\\\n0  \\\\\n0   \\\\\n1    \\\\\n\\end{bmatrix}</script></li>\n<li>这两个特殊解是否构成了零空间的一组基？是的，零空间的维数就是自由变量的个数,即n-r,在本例中是4-2=2。</li>\n</ul>\n<h1 id=\"第十讲：四个基本子空间\"><a href=\"#第十讲：四个基本子空间\" class=\"headerlink\" title=\"第十讲：四个基本子空间\"></a>第十讲：四个基本子空间</h1><ul>\n<li>列空间C(A)，零空间N(A)，行空间C($A^T$)，左零空间N($A^T$)。</li>\n<li>分别处于$R^m、R^n、R^n、R^m$空间中</li>\n<li>列空间与行空间的维数都是秩r，零空间维数是n-r，左零空间维数是m-r</li>\n<li>列空间的基：主列，共r列。零空间的基：特殊解(自由列)，共n-r个。行空间的基：最简形式R的非0行,共r行</li>\n<li>行变换是行向量的线性组合，因此A和R的行空间相同，列空间发生了变化</li>\n<li>为什么叫做左零空间？<script type=\"math/tex; mode=display\">\nrref\\begin{bmatrix}\nA_{m\\*n} & I_{m\\*n}\n\\end{bmatrix}\\rightarrow\n\\begin{bmatrix}\nR_{m\\*n} & E_{m\\*n}\n\\end{bmatrix} \\\\</script></li>\n<li>易得rref=E，即EA=R </li>\n<li>通过E可以计算左零空</li>\n<li>求左零空间即找一个产生零行向量的行组合 </li>\n<li>左零空间的基就是R非0行对应的E行,共m-r行 </li>\n</ul>\n<h1 id=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"><a href=\"#第十一讲：矩阵空间、秩1矩阵和小世界图\" class=\"headerlink\" title=\"第十一讲：矩阵空间、秩1矩阵和小世界图\"></a>第十一讲：矩阵空间、秩1矩阵和小世界图</h1><h2 id=\"矩阵空间\"><a href=\"#矩阵空间\" class=\"headerlink\" title=\"矩阵空间\"></a>矩阵空间</h2><ul>\n<li>可以看成向量空间，可以数乘，可以相加</li>\n<li>以$3*3$矩阵空间M为例，空间的一组基即9个矩阵，每个矩阵中只包含一个元素1,这是一组标准基，可得这个矩阵空间维数是9<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}.....\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}</script></li>\n<li>再来研究$3*3$矩阵空间中对称矩阵子空间S的维数，可以看到原空间基中9个矩阵，有3个矩阵属于对称矩阵子空间，另外还有上三角与下三角对称的三个矩阵，所以对称矩阵子空间的维数是6<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 1 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n0 & 1 & 0  \\\\\n1 & 0 & 0  \\\\\n0 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 1  \\\\\n0 & 0 & 0  \\\\\n1 & 0 & 0  \\\\\n\\end{bmatrix}，\n\\begin{bmatrix}\n0 & 0 & 0  \\\\\n0 & 0 & 1  \\\\\n0 & 1 & 0  \\\\\n\\end{bmatrix}</script></li>\n<li>对于上三角矩阵子空间U，易得维数为6，且元空间的基包含了子空间的基</li>\n<li>接着再来研究$S \\bigcap U$ ，易得这个子空间即对角矩阵D，维度为3</li>\n<li>如果是$S \\bigcup U $呢？他们的并的基可以得到所有M的基，所以其维数是9</li>\n<li>整理一下可得<script type=\"math/tex; mode=display\">\ndim(S)=6,dim(U)=6,dim(S \\bigcap U)=3,dim(S \\bigcup U)=3 \\\\\ndim(S)+dim(U)=dim(S \\bigcap U)+dim(S \\bigcup U) \\\\</script></li>\n<li>再来举一个栗子，说明向量空间不一定有向量，比如下面这个基于微分方程的向量空间<script type=\"math/tex; mode=display\">\n\\frac{d^2y}{dx^2}+y=0</script></li>\n<li>他的几个解为 <script type=\"math/tex; mode=display\">\ny=cos(x),y=sin(x)</script></li>\n<li>完整解为  <script type=\"math/tex; mode=display\">\ny=c_1cos(x)+c_2sin(x)</script></li>\n<li>即得到一个向量空间，基为2</li>\n</ul>\n<h2 id=\"秩1矩阵\"><a href=\"#秩1矩阵\" class=\"headerlink\" title=\"秩1矩阵\"></a>秩1矩阵</h2><ul>\n<li>先写一个简单的秩1矩阵<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n2 & 8 & 10 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n\\end{bmatrix}*\n\\begin{bmatrix}\n1 & 4 & 5 \\\\\n\\end{bmatrix}</script></li>\n<li>所有的秩1矩阵都可以表示为一列乘一行</li>\n<li>秩1矩阵就像积木，比如一个秩为4的矩阵可以由4个秩1矩阵构建而成</li>\n<li>再来看一个秩1矩阵的栗子，在四维空间中，设向量$v=(v_1,v_2,v_3,v_4)$,集合$S=\\{v|v_1+v_2+v_3+v_4=0\\}$,假如把S看成零空间，则相应的方程$Av=0$中的矩阵A为<script type=\"math/tex; mode=display\">\nA=\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n\\end{bmatrix}</script></li>\n<li>易得$dimN(A)=n-r$，所以S的维数是$4-1=3$，S的一组基为<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n-1  \\\\\n1  \\\\\n0  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n1  \\\\\n0  \\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n-1  \\\\\n0  \\\\\n0  \\\\\n1  \\\\\n\\end{bmatrix}</script></li>\n<li>矩阵A的四个子空间:易得行空间和列空间的秩(维数)均为1，行空间$C(A^T)=\\{a,a,a,a\\}​$，列空间$C(A)=R^1​$，零空间$N(A)​$即S的基线性组合，$N(A^T)={0}​$</li>\n<li>整理一下<script type=\"math/tex; mode=display\">\ndim(N(A))+dim(C(A^T))=3+1=4=n \\\\\ndim(C(A))+dim(N(A^T))=1+0=1=m \\\\</script></li>\n</ul>\n<h2 id=\"小世界图\"><a href=\"#小世界图\" class=\"headerlink\" title=\"小世界图\"></a>小世界图</h2><ul>\n<li>仅仅引入了图的概念，为下一讲准备</li>\n</ul>\n<h1 id=\"第十二讲：图和网络\"><a href=\"#第十二讲：图和网络\" class=\"headerlink\" title=\"第十二讲：图和网络\"></a>第十二讲：图和网络</h1><h2 id=\"图\"><a href=\"#图\" class=\"headerlink\" title=\"图\"></a>图</h2><ul>\n<li>图的一些基础概念，略过</li>\n</ul>\n<h2 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h2><ul>\n<li><p>图的关联矩阵A，将列作为图的节点，行作为矩阵的边，起点为-1，终点为1，其余为0</p>\n</li>\n<li><p>构成回路的几行线性相关，回路意味着相关</p>\n</li>\n<li><p>关联矩阵A描述了图的拓扑结构</p>\n</li>\n<li><p>$dimN(A^T)=m-r​$</p>\n</li>\n<li><p>假如图的节点是电势，$Ax$中x即电势，$Ax=0$得到一组电势差方程，零空间是一维的，$A^Ty$中y即边上的电流，电流与电势差的关系即欧姆定律，$A^Ty=0$得到基尔霍夫定律，零空间包含了基尔霍夫电流方程的两个解，从电路图上看即两个小回路</p>\n</li>\n<li><p>树就是没有回路的图</p>\n</li>\n<li><p>再来看看$dimN(A^T)=m-r$</p>\n</li>\n<li><p>$dimN(A^T)$=无关回路数  </p>\n</li>\n<li>$m$=边数 </li>\n<li>$r=n-1$=节点数-1 (因为零空间是一维的) </li>\n<li>即:节点数-边数+回路数=1(欧拉公式) </li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><font size=\"4\">总结</font></h2><ul>\n<li><p>将电势记为e,$e=Ax$</p>\n</li>\n<li><p>电势差导致电流产生，$y=Ce$</p>\n</li>\n<li><p>电流满足基尔霍夫电流方程,$A^Ty=0$</p>\n</li>\n<li><p>将三个方程联立：</p>\n<script type=\"math/tex; mode=display\">\nA^TCAx=f</script><p>这就是应用数学中最基本的平衡方程</p>\n</li>\n</ul>\n<h1 id=\"第十三讲：正交向量与子空间\"><a href=\"#第十三讲：正交向量与子空间\" class=\"headerlink\" title=\"第十三讲：正交向量与子空间\"></a>第十三讲：正交向量与子空间</h1><h2 id=\"正交向量\"><a href=\"#正交向量\" class=\"headerlink\" title=\"正交向量\"></a>正交向量</h2><ul>\n<li>正交即垂直，意味着在n维空间内，这些向量的夹角是90度</li>\n<li>当$x^Ty=0$,x与y正交，证明：</li>\n<li>若x与y正交，易得:<script type=\"math/tex; mode=display\">\n{||x||}^2+{||y||}^2={||x+y||}^2 \\\\</script></li>\n<li>即：<script type=\"math/tex; mode=display\">\nx^Tx+y^Ty={(x+y)}^T(x+y)=x^Tx+y^Ty+x^Ty+xy^T=2x^Ty \\\\</script></li>\n<li>即：<script type=\"math/tex; mode=display\">\nx^Ty=0 \\\\</script></li>\n</ul>\n<ul>\n<li>子空间正交意味着一个子空间内的所有向量与另一个子空间内的每一个向量正交，显然，如果两个二维子空间在某一向量处相交，则这两个空间一定不正交</li>\n<li>若两个子空间正交，则他们一定不会相交于某一个非零向量，因为这个非零向量同时存在于两个子空间内，它不可能自己垂直于自己</li>\n<li>行空间正交于零空间，因为$Ax=0$，即矩阵的每一行以及这些行的线性组合(行空间)与解向量(零空间)点乘都为0。这样就证明了图中左半部分</li>\n<li>图中右半部份，列空间和左零空间分别是矩阵A的转置矩阵的行空间和零空间，刚才的证明同样有效，因此列空间和左零空间正交，图中右半部份成立</li>\n<li>图中给出了n维空间和m维空间的正交子空间，n维空间的正交子空间:r维行空间和n-r维零空间。m维空间的正交子空间:r维列空间和m-r维左零空间。</li>\n</ul>\n<h2 id=\"正交子空间\"><a href=\"#正交子空间\" class=\"headerlink\" title=\"正交子空间\"></a>正交子空间</h2><ul>\n<li>例如三维空间，假如行空间是一维的，则零空间是二维的，行空间是一条直线，零空间是垂直于这个直线的平面，从几何上可以直观看出他们正交</li>\n<li>因为零空间是行空间的正交补集，所以零空间包含了所有正交于行空间的向量</li>\n<li>以上是所有关于解$Ax=0$的知识，如果要解不可解的方程，或者说求最优解，该怎么办呢？我们引入一个重要的矩阵$A^TA$</li>\n<li>$A^TA$是一个$n*n$的方阵，而且对称</li>\n<li>坏方程转换为好方程，两边同乘$A^T$</li>\n<li>$A^TA$不总是可逆，若可逆，则$N(A^TA)=N(A)$，且他们的秩相同</li>\n<li>$A^TA$可逆当且仅当零空间内只有零向量，即各列线性无关，下一讲将证明这些性质</li>\n</ul>\n<h1 id=\"第十四讲：子空间投影\"><a href=\"#第十四讲：子空间投影\" class=\"headerlink\" title=\"第十四讲：子空间投影\"></a>第十四讲：子空间投影</h1><h2 id=\"投影\"><a href=\"#投影\" class=\"headerlink\" title=\"投影\"></a>投影</h2><p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG\" alt=\"mark\"></p>\n<ul>\n<li>在二维情况下讨论投影</li>\n<li>一个点b到另一条直线a的投影，即从这个点做垂直于a的垂线段交a于p点，p即b在a上的投影点，以p为终点的向量即投影p，垂线段即误差e，e=b-p</li>\n<li>p在a的一维子空间里，是a的x倍，即p=xa</li>\n<li>a垂直于e，即<script type=\"math/tex; mode=display\">\na^T(b-xa)=0 \\\\\nxa^Ta=a^Tb \\\\\nx= \\frac {a^Tb}{a^Ta} \\\\\np=a\\frac {a^Tb}{a^Ta} \\\\</script></li>\n<li>从式子中可以看到，若b翻倍，则投影翻倍，若a变化，则投影不变，因为分子分母抵消了</li>\n</ul>\n<h2 id=\"投影矩阵\"><a href=\"#投影矩阵\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul>\n<li>现在可以引入投影矩阵P的一维模式(projection matrix)，$p=Pb$,$P= \\frac {aa^T}{a^Ta}$</li>\n<li>用任意b乘投影矩阵，总会落在通过a的一条线上(即b在a上的投影p),所以投影矩阵的列空间是这条线，矩阵的秩为1</li>\n<li>投影矩阵的另外两条性质：<ul>\n<li>对称,即$P^T=P$</li>\n<li>两次投影在相同的位置，即$P^2=P$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"投影的意义\"><a href=\"#投影的意义\" class=\"headerlink\" title=\"投影的意义\"></a>投影的意义</h2><ul>\n<li>下面在高维情况下讨论</li>\n<li>当方程数大于未知数个数时，经常无解，这时我们只能找出最接近的解</li>\n<li>如何找？将b微调，使得b在列空间中</li>\n<li>怎么微调？将b变成p，即列空间中最接近b的那一个，即转换求解$Ax^{‘}=p$,p时b在列空间上的投影</li>\n<li>现在我们要求$x^{‘}$,$p=Ax^{‘}$，误差向量$e=b-Ax^{‘}$，由投影定义可知e需要垂直于A的列空间</li>\n<li>综上可得<script type=\"math/tex; mode=display\">\nA^T(b-Ax^{'})=0 \\\\</script></li>\n<li>由上式可以看出e在A的左零空间，与列空间正交。解上式可得<script type=\"math/tex; mode=display\">\nx^{'}=(A^TA)^{-1}A^Tb \\\\\np=Ax^{'}=A(A^TA)^{-1}A^Tb \\\\</script></li>\n<li>即投影矩阵P的n维模式:<script type=\"math/tex; mode=display\">\nA(A^TA)^{-1}A^T \\\\</script></li>\n<li>投影矩阵P的n维模式依然保留了1维模式的两个性质</li>\n<li>现在回到求最优解，最常见的一个例子是通过最小二乘法拟合一条直线</li>\n<li>已知三个点$a_1,a_2,a_3$，找出一条直线拟合接近三个点,$b=C+Da$</li>\n<li>假如$a_1=(1,1),a_2=(2,2),a_3=(3,2)$,则<script type=\"math/tex; mode=display\">\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\</script>写成线代形式为:<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\n1 & 1  \\\\\n1 & 2  \\\\\n1 & 3  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC  \\\\\nD  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1  \\\\\n2  \\\\\n2  \\\\\n\\end{bmatrix}</script></li>\n<li>即Ax=b,方程数大于未知数个数，若两边乘以A转置，即求$x^{‘}$，这样就可以求出拟合直线。下一讲继续此例</li>\n</ul>\n<h1 id=\"第十五讲：投影矩阵和最小二乘法\"><a href=\"#第十五讲：投影矩阵和最小二乘法\" class=\"headerlink\" title=\"第十五讲：投影矩阵和最小二乘法\"></a>第十五讲：投影矩阵和最小二乘法</h1><h2 id=\"投影矩阵-1\"><a href=\"#投影矩阵-1\" class=\"headerlink\" title=\"投影矩阵\"></a>投影矩阵</h2><ul>\n<li>回顾，$P=A(A^TA)^{-1}A^T$,$Pb$即b在A的列空间上的投影，现在考虑两种极端情况，b在列空间上和b正交于列空间：<br>b在列空间上：$Pb=b$；证明：若b在列空间上，则可以表示为$b=Ax$，在A各列线性无关的条件下，$(A^TA)$可逆，代入$P=A(A^TA)^{-1}A^T$有$Pb=b$<br>b正交于列空间，$Pb=0$；证明：若b正交于列空间则b在左零空间内，即$A^Tb=0$，显然代入$P=A(A^TA)^{-1}A^T$有$Pb=0$</li>\n</ul>\n<ul>\n<li>p是b在列空间上的投影，因为列空间正交于左零空间，自然e就是b在左零空间上的投影，如图：<script type=\"math/tex; mode=display\">\nb=p+e \\\\\np=Pb \\\\</script></li>\n<li>所以<script type=\"math/tex; mode=display\">\ne=(I-P)b \\\\</script></li>\n<li><p>所以左零空间的投影矩阵为$(I-P)$ </p>\n<p><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/192923067.JPG\" alt=\"mark\"></p>\n</li>\n</ul>\n<h2 id=\"最小二乘法\"><a href=\"#最小二乘法\" class=\"headerlink\" title=\"最小二乘法\"></a>最小二乘法</h2><ul>\n<li>回到上一讲的例子，找到一条最优直线接近三个点，最小化误差，如图<br><img src=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170301/195536899.JPG\" alt=\"mark\"></li>\n<li>设直线为$y=C+Dt$，代入三个点坐标得到一个方程组<script type=\"math/tex; mode=display\">\nC+D=1 \\\\\nC+2D=2 \\\\\nC+3D=2 \\\\</script></li>\n<li>此方程组无解但是存在最优价，从代数角度看：<script type=\"math/tex; mode=display\">\n||e||^2=(C+D-1)^2+(C+2D-2)^2+(C+3D-2)^2 \\\\</script></li>\n<li><p>分别对C和D求偏导为0，得到方程组: </p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}</script></li>\n<li><p>写成矩阵形式，这里的$C,D$仅仅存在一个形式，他们无解，要解出$C,D$是将其作为拟合直线，即b被替换为投影p时的$C,D$。</p>\n<script type=\"math/tex; mode=display\">\nAx=b \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nC \\\\\nD \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n\\end{bmatrix}</script></li>\n<li>A满足各列线性无关，b不在A的列空间里，现在我们想最小化误差$e=Ax-b$，怎么量化误差？求其长度的平方$||e||^2$，在图中即y轴方向上点到拟合直线的距离的平方和。这些点$b_1,b_2,b_3$的误差线段$e_1,e_2,e_3$与拟合直线交于$p_1,p_2,p_3$，当将三个b点用三个p点取代时，方程组有解。</li>\n<li>现在要解出$x^{‘},p$，已知$p=Ax^{‘}=A(A^TA)^{-1}A^Tb$，$Ax=b$，两边同乘$A^T$联立有<script type=\"math/tex; mode=display\">\nA^TAx^{'}=A^Tb</script></li>\n<li><p>代入数值可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n3C+6D=5\\\\\n6C+14D=11\\\\\n\\end{cases}</script></li>\n<li><p>与代数求偏导数结果一样,之后可以解出$C,D$，也就得到了拟合直线</p>\n</li>\n<li>回顾一下上面两幅图，一张解释了$b,p,e$的关系，另一张用$C,D$确定了拟合直线，由$C,D$确定的列组合就是向量p</li>\n<li>如果A的各列线性无关，则$A^TA$是可逆的，这时最小二乘法使用的前提，证明：<br>如果矩阵可逆，则它的零空间仅为零向量，即$A^TAx=0$中x必须是零向量<script type=\"math/tex; mode=display\">\nA^TAx=0 \\\\\nx^TA^TAx=0 \\\\\n(Ax)^T(Ax)=0 \\\\\nAx=0 \\\\</script></li>\n<li>又因为A各列线性无关，所以<script type=\"math/tex; mode=display\">\nx=0</script></li>\n<li>即证</li>\n<li>对于处理相互垂直的单位向量，我们引入标准正交向量组，这个矩阵的各列是标准正交而且是单位向量组，下一讲将介绍更多关于标准正交向量组的内容</li>\n</ul>\n<h1 id=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"><a href=\"#第十六讲：正交矩阵和Gram-Schmidt正交化\" class=\"headerlink\" title=\"第十六讲：正交矩阵和Gram-Schmidt正交化\"></a>第十六讲：正交矩阵和Gram-Schmidt正交化</h1><h2 id=\"正交矩阵\"><a href=\"#正交矩阵\" class=\"headerlink\" title=\"正交矩阵\"></a>正交矩阵</h2><ul>\n<li>已知一组正交向量集<script type=\"math/tex; mode=display\">\nq_i^Tq_j=\n\\begin{cases}\n0 \\quad if \\quad i \\neq j \\\\\n1 \\quad if \\quad i=j \\\\\n\\end{cases}</script><script type=\"math/tex; mode=display\">\nQ=\n\\begin{bmatrix}\nq_1 & q_2 & ... & q_n \\\\\n\\end{bmatrix} \\\\\nQ^TQ=I \\\\</script></li>\n<li>所以，对有标准正交列的方阵，$Q^TQ=I$,$Q^T=Q^{-1}$,即正交矩阵，例如<script type=\"math/tex; mode=display\">\nQ=\\begin{bmatrix}\ncos \\theta & -sin \\theta \\\\\nsin \\theta & cos \\theta \\\\\n\\end{bmatrix}or\n\\frac {1}{\\sqrt 2} \n\\begin{bmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{bmatrix}</script></li>\n<li>Q不一定是方阵。Q的各列将是列空间的标准正交基</li>\n<li>对Q，投影到Q的列空间的投影矩阵P是什么？$P=Q(Q^TQ)^{-1}Q^T=QQ^T$</li>\n</ul>\n<h2 id=\"Gram-Schmidt正交化\"><a href=\"#Gram-Schmidt正交化\" class=\"headerlink\" title=\"Gram-Schmidt正交化\"></a>Gram-Schmidt正交化</h2><ul>\n<li>给定两个不正交的向量a和b，我们希望从a,b中得到两个正交向量A,B，可设A=a，则B就是b投影到A上的误差向量e：<script type=\"math/tex; mode=display\">\nB=e=b-\\frac{A^Tb}{A^TA}A</script></li>\n<li>正交基就是A,B除以他们的长度$q_1=\\frac{A}{||A||}$</li>\n<li>扩展到求三个向量，即A,B,C的情况，从上式我们已知A，B，同理，C需要c剪去在A和B上的投影分量<script type=\"math/tex; mode=display\">\nC=c- \\frac {A^Tc}{A^TA}A- \\frac {B^Tc}{B^TB} B</script></li>\n<li>由a,b组成列向量的矩阵A就通过施密特正交化变成了正交矩阵Q,用公式推导可以看出，Q的各列$q_1,q_2,…$与$a,b,….$在同一列空间内，正交化可以写成<script type=\"math/tex; mode=display\">\nA=QR \\\\</script></li>\n<li>即<script type=\"math/tex; mode=display\">\n\\begin{bmatrix}\na & b \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nq_1 & q_2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^Ta & q_1^Tb \\\\\nq_2^Ta & q_2^Tb \\\\\n\\end{bmatrix} \\\\</script></li>\n<li>其中，因为$QQ^T=I$</li>\n<li>所以$R=Q^TA$</li>\n<li>$q_2$与$q_1$正交，而$q_1$只是$a$的单位化，所以$q_2^Ta=0$，即$R$是上三角矩阵</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:22 GMT+0800 (GMT+08:00)","title":"MIT线性代数笔记2","path":"2017/01/21/LinearAlgebra2/","eyeCatchImage":"http://ojtdnrpmt.bkt.clouddn.com/blog/20170228/203158686.JPG","excerpt":"<hr>\n<h1 id=\"第九讲：线性相关性、基、维数\"><a href=\"#第九讲：线性相关性、基、维数\" class=\"headerlink\" title=\"第九讲：线性相关性、基、维数\"></a>第九讲：线性相关性、基、维数</h1><h2 id=\"线性相关性\"><a href=\"#线性相关性\" class=\"headerlink\" title=\"线性相关性\"></a>线性相关性</h2><ul>\n<li>背景知识:假设一个矩阵A，m&lt;n，即未知数个数大于方程数，因此在零空间内除了零向量之外还有别的向量，最多m个主元，存在n-m个自由向量，整个方程存在非零解。</li>\n<li>什么条件下，$x_1,x_2,x_3…x_n$线性无关?存在一个系数不全为零的组合，使得线性相加结果为0,则为线性相关，反之为线性无关。</li>\n<li>如果向量组里存在一个零向量，则这个向量组不可能线性无关。</li>\n<li>假如在二维空间内随意画三个向量，则他们一定线性相关，为什么？由背景知识可得。</li>\n<li>对一个矩阵A，我们关心各列是否线性相关，如果零空间内存在非零向量，则各列相关。</li>\n<li>换一种方式解释：当$v_1,v_2…v_n$是A的各列，如果他们是无关的，那么A的零空间是怎样的？只有零向量。如果他们是相关的呢？那么零空间内除了零向量还存在一个非零向量。</li>\n<li>列向量无关时，所有的列向量都是主向量，秩为n。列向量相关时，秩小于n。</li>\n</ul>","date":"2017-01-21T11:28:03.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","linearalgebra"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"机器学习入门:K-Means和KNN","date":"2017-03-16T07:51:11.000Z","mathjax":true,"html":true,"_content":"-\t以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较\n-   标题本来是K-Means&KNN，把&改成了和，因为标题中出现特殊符号&会导致我的sitemap生成错误......\n***\n\n<!--more-->\n![i0onl8.jpg](https://s1.ax1x.com/2018/10/20/i0onl8.jpg)\n\n# 简介\n-   K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复\n-   KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别\n\n# K-means++\n-\tk-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点\n-\t算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点\n\t-\t计算每个点$c_i$到已经选出的中心点$k_1,k_2...$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远\n\t-\t将$c_1,c_2,c_3......$的距离归一化，并排成一条线\n\t-\t这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大\n\t-\t在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点\n-\t可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求\n\n# K-Means代码实现\n\n## 数据检视\n-\tIris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性\n\t![i0otpV.jpg](https://s1.ax1x.com/2018/10/20/i0otpV.jpg)\n\n\n\n## 初始化数据\n-\t初始化数据\n```Python\n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n```\n\n## k-means++初始化k个点\n-\tD2是每个点的距离(即之前定义的里其他中心点至少有多远)\n-\tprobs归一化\n-\tcumprobs将归一化的概率累加，排列成一条线\n```Python\n    def initk(X_train, k):\n        C = [X_train[0]]\n        for i in range(1, k):\n            D2 = scipy.array([min([scipy.inner(c - x, c - x) for c in C]) for x in X_train])\n            probs = D2 / D2.sum()\n            cumprobs = probs.cumsum()\n            r = scipy.rand()\n            for j, p in enumerate(cumprobs):\n                if r < p:\n                    i = j\n                    break\n            C.append(X_train[i])\n        return C\n    \n```\n\n## 损失评估\n-\t在这里用每个类内点到中心点距离平方和的总和作为损失评估\n```Python\n    def evaluate(C, X_train, y_predict):\n        sum = 0\n        for i in range(len(X_train)):\n            c = C[y_predict[i]]\n            sum += scipy.inner(c - X_train[i], c - X_train[i])\n        return sum\n```\n\n## 聚类\n-\t初始化k个中心点后，所有的点就可以分类\n-\t重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标\n```Python\n    def cluster(C, X_train, y_predict, k):\n        sum = [0, 0, 0, 0] * k\n        count = [0] * k\n        newC = []\n        for i in range(len(X_train)):\n            min = 32768\n            minj = -1\n            for j in range(k):\n                if scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) < min:\n                    min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])\n                    minj = j\n            y_predict[i] = (minj + 1) % k\n        for i in range(len(X_train)):\n            sum[y_predict[i]] += X_train[i]\n            count[y_predict[i]] += 1\n        for i in range(k):\n            newC.append(sum[i] / count[i])\n        return y_predict, newC\n```\n\n## 主函数\n-\t计算损失，更新k个中心点，再站队(聚类)一次\n-\t重复，直到损失变化小于10%\n-\t每次迭代显示新旧损失，显示损失变化\n-\t最后输出分类结果\n```Python\n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        k = 3\n        total = len(y_train)\n        y_predict = [0] * total\n        C = initk(X_train, k)\n        oldeval = evaluate(C, X_train, y_predict)\n        while (1):\n            y_predict, C = cluster(C, X_train, y_predict, k)\n            neweval = evaluate(C, X_train, y_predict)\n            ratio = (oldeval - neweval) / oldeval * 100\n            print(oldeval, \" -> \", neweval, \"%f %%\" % ratio)\n            oldeval = neweval\n            if ratio < 0.1:\n                break\n    \n        print(y_train)\n        print(y_predict)\n        n = 0\n        m = 0\n        for i in range(len(y_train)):\n            m += 1\n            if y_train[i] == y_predict[i]:\n                n += 1\n        print(n / m)\n        print(classification_report(y_train, y_predict, target_names=iris.target_names))\n    \n```\n\n# KNN代码\n-\t直接使用了sklearn中的KNeighborsClassifier\n```Python\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    \n    \n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n    \n    def KNN(X_train, X_test, y_train, y_test, iris):\n        knc = KNeighborsClassifier()\n        knc.fit(X_train, y_train)\n        y_predict = knc.predict(X_test)\n        print(knc.score(X_test, y_test))\n        print(classification_report(y_test, y_predict, target_names=iris.target_names))\n    \n    \n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        KNN(X_train, X_test, y_train, y_test, iris)\n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 预测结果\n-\t指标说明\n\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n\t$$\n\t精确率:P=\\frac{TP}{TP+FP} \\\\\n\t召回率:R=\\frac{TP}{TP+FN} \\\\\n\t1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n\t$$\n-\tK-Means程序输出\n\t预测正确率:88.39%\n\t平均精确率:89%\n\t召回率:0.88\n\tF1指标:0.88\n\t![i0o1Ts.jpg](https://s1.ax1x.com/2018/10/20/i0o1Ts.jpg)\n\n-\tKNN程序输出\n\t预测正确率:71.05%\n\t平均精确率:86%\n\t召回率:0.71\n\tF1指标:0.70\n\t![i0oKOg.jpg](https://s1.ax1x.com/2018/10/20/i0oKOg.jpg)\n\n-\t原始分类\n\t可以看到这个数据集本身在空间上就比较方便聚类划分\n\t![i0oQmQ.gif](https://s1.ax1x.com/2018/10/20/i0oQmQ.gif)\n\n-\t预测分类\n\t![i0o8kn.gif](https://s1.ax1x.com/2018/10/20/i0o8kn.gif)\n\n\n\n# 改进\n## 未知k的情况\n-\t以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?\n-\t一种方式是canopy算法\n-\t待补充\n\n## 空类的处理\n-\t待补充\n\n## 不同距离计算方式\n-\t待补充\n\n## ANN算法\n","source":"_posts/kmeans.md","raw":"---\ntitle: 机器学习入门:K-Means和KNN\ndate: 2017-03-016 15:51:11\ncategories: 机器学习\ntags:\n  - code\n  - machinelearning\nmathjax: true\nhtml: true\n---\n-\t以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较\n-   标题本来是K-Means&KNN，把&改成了和，因为标题中出现特殊符号&会导致我的sitemap生成错误......\n***\n\n<!--more-->\n![i0onl8.jpg](https://s1.ax1x.com/2018/10/20/i0onl8.jpg)\n\n# 简介\n-   K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复\n-   KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别\n\n# K-means++\n-\tk-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点\n-\t算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点\n\t-\t计算每个点$c_i$到已经选出的中心点$k_1,k_2...$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远\n\t-\t将$c_1,c_2,c_3......$的距离归一化，并排成一条线\n\t-\t这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大\n\t-\t在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点\n-\t可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求\n\n# K-Means代码实现\n\n## 数据检视\n-\tIris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性\n\t![i0otpV.jpg](https://s1.ax1x.com/2018/10/20/i0otpV.jpg)\n\n\n\n## 初始化数据\n-\t初始化数据\n```Python\n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n```\n\n## k-means++初始化k个点\n-\tD2是每个点的距离(即之前定义的里其他中心点至少有多远)\n-\tprobs归一化\n-\tcumprobs将归一化的概率累加，排列成一条线\n```Python\n    def initk(X_train, k):\n        C = [X_train[0]]\n        for i in range(1, k):\n            D2 = scipy.array([min([scipy.inner(c - x, c - x) for c in C]) for x in X_train])\n            probs = D2 / D2.sum()\n            cumprobs = probs.cumsum()\n            r = scipy.rand()\n            for j, p in enumerate(cumprobs):\n                if r < p:\n                    i = j\n                    break\n            C.append(X_train[i])\n        return C\n    \n```\n\n## 损失评估\n-\t在这里用每个类内点到中心点距离平方和的总和作为损失评估\n```Python\n    def evaluate(C, X_train, y_predict):\n        sum = 0\n        for i in range(len(X_train)):\n            c = C[y_predict[i]]\n            sum += scipy.inner(c - X_train[i], c - X_train[i])\n        return sum\n```\n\n## 聚类\n-\t初始化k个中心点后，所有的点就可以分类\n-\t重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标\n```Python\n    def cluster(C, X_train, y_predict, k):\n        sum = [0, 0, 0, 0] * k\n        count = [0] * k\n        newC = []\n        for i in range(len(X_train)):\n            min = 32768\n            minj = -1\n            for j in range(k):\n                if scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) < min:\n                    min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])\n                    minj = j\n            y_predict[i] = (minj + 1) % k\n        for i in range(len(X_train)):\n            sum[y_predict[i]] += X_train[i]\n            count[y_predict[i]] += 1\n        for i in range(k):\n            newC.append(sum[i] / count[i])\n        return y_predict, newC\n```\n\n## 主函数\n-\t计算损失，更新k个中心点，再站队(聚类)一次\n-\t重复，直到损失变化小于10%\n-\t每次迭代显示新旧损失，显示损失变化\n-\t最后输出分类结果\n```Python\n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        k = 3\n        total = len(y_train)\n        y_predict = [0] * total\n        C = initk(X_train, k)\n        oldeval = evaluate(C, X_train, y_predict)\n        while (1):\n            y_predict, C = cluster(C, X_train, y_predict, k)\n            neweval = evaluate(C, X_train, y_predict)\n            ratio = (oldeval - neweval) / oldeval * 100\n            print(oldeval, \" -> \", neweval, \"%f %%\" % ratio)\n            oldeval = neweval\n            if ratio < 0.1:\n                break\n    \n        print(y_train)\n        print(y_predict)\n        n = 0\n        m = 0\n        for i in range(len(y_train)):\n            m += 1\n            if y_train[i] == y_predict[i]:\n                n += 1\n        print(n / m)\n        print(classification_report(y_train, y_predict, target_names=iris.target_names))\n    \n```\n\n# KNN代码\n-\t直接使用了sklearn中的KNeighborsClassifier\n```Python\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    \n    \n    def init():\n        iris = load_iris()\n        X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)\n        ss = StandardScaler()\n        X_train = ss.fit_transform(X_train)\n        X_test = ss.fit_transform(X_test)\n        return X_train, X_test, y_train, y_test, iris\n    \n    \n    def KNN(X_train, X_test, y_train, y_test, iris):\n        knc = KNeighborsClassifier()\n        knc.fit(X_train, y_train)\n        y_predict = knc.predict(X_test)\n        print(knc.score(X_test, y_test))\n        print(classification_report(y_test, y_predict, target_names=iris.target_names))\n    \n    \n    def main():\n        X_train, X_test, y_train, y_test, iris = init()\n        KNN(X_train, X_test, y_train, y_test, iris)\n    \n    if __name__ == \"__main__\":\n        main()\n```\n\n# 预测结果\n-\t指标说明\n\t对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN\n\t$$\n\t精确率:P=\\frac{TP}{TP+FP} \\\\\n\t召回率:R=\\frac{TP}{TP+FN} \\\\\n\t1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\\n\t$$\n-\tK-Means程序输出\n\t预测正确率:88.39%\n\t平均精确率:89%\n\t召回率:0.88\n\tF1指标:0.88\n\t![i0o1Ts.jpg](https://s1.ax1x.com/2018/10/20/i0o1Ts.jpg)\n\n-\tKNN程序输出\n\t预测正确率:71.05%\n\t平均精确率:86%\n\t召回率:0.71\n\tF1指标:0.70\n\t![i0oKOg.jpg](https://s1.ax1x.com/2018/10/20/i0oKOg.jpg)\n\n-\t原始分类\n\t可以看到这个数据集本身在空间上就比较方便聚类划分\n\t![i0oQmQ.gif](https://s1.ax1x.com/2018/10/20/i0oQmQ.gif)\n\n-\t预测分类\n\t![i0o8kn.gif](https://s1.ax1x.com/2018/10/20/i0o8kn.gif)\n\n\n\n# 改进\n## 未知k的情况\n-\t以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?\n-\t一种方式是canopy算法\n-\t待补充\n\n## 空类的处理\n-\t待补充\n\n## 不同距离计算方式\n-\t待补充\n\n## ANN算法\n","slug":"kmeans","published":1,"updated":"2019-07-22T03:45:23.201Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vaj0048q8t5fl1fqbyu","content":"<ul>\n<li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li>\n<li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li>\n</ul>\n<hr>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0onl8.jpg\" alt=\"i0onl8.jpg\"></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><ul>\n<li>K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复</li>\n<li>KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别</li>\n</ul>\n<h1 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means++\"></a>K-means++</h1><ul>\n<li>k-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点</li>\n<li>算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点<ul>\n<li>计算每个点$c_i$到已经选出的中心点$k_1,k_2…$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远</li>\n<li>将$c_1,c_2,c_3……$的距离归一化，并排成一条线</li>\n<li>这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大</li>\n<li>在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点</li>\n</ul>\n</li>\n<li>可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求</li>\n</ul>\n<h1 id=\"K-Means代码实现\"><a href=\"#K-Means代码实现\" class=\"headerlink\" title=\"K-Means代码实现\"></a>K-Means代码实现</h1><h2 id=\"数据检视\"><a href=\"#数据检视\" class=\"headerlink\" title=\"数据检视\"></a>数据检视</h2><ul>\n<li>Iris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性<br><img src=\"https://s1.ax1x.com/2018/10/20/i0otpV.jpg\" alt=\"i0otpV.jpg\"></li>\n</ul>\n<h2 id=\"初始化数据\"><a href=\"#初始化数据\" class=\"headerlink\" title=\"初始化数据\"></a>初始化数据</h2><ul>\n<li>初始化数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"k-means-初始化k个点\"><a href=\"#k-means-初始化k个点\" class=\"headerlink\" title=\"k-means++初始化k个点\"></a>k-means++初始化k个点</h2><ul>\n<li>D2是每个点的距离(即之前定义的里其他中心点至少有多远)</li>\n<li>probs归一化</li>\n<li>cumprobs将归一化的概率累加，排列成一条线<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initk</span><span class=\"params\">(X_train, k)</span>:</span></span><br><span class=\"line\">    C = [X_train[<span class=\"number\">0</span>]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, k):</span><br><span class=\"line\">        D2 = scipy.array([min([scipy.inner(c - x, c - x) <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> C]) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X_train])</span><br><span class=\"line\">        probs = D2 / D2.sum()</span><br><span class=\"line\">        cumprobs = probs.cumsum()</span><br><span class=\"line\">        r = scipy.rand()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, p <span class=\"keyword\">in</span> enumerate(cumprobs):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> r &lt; p:</span><br><span class=\"line\">                i = j</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        C.append(X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a>损失评估</h2><ul>\n<li>在这里用每个类内点到中心点距离平方和的总和作为损失评估<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate</span><span class=\"params\">(C, X_train, y_predict)</span>:</span></span><br><span class=\"line\">    sum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        c = C[y_predict[i]]</span><br><span class=\"line\">        sum += scipy.inner(c - X_train[i], c - X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><ul>\n<li>初始化k个中心点后，所有的点就可以分类</li>\n<li>重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cluster</span><span class=\"params\">(C, X_train, y_predict, k)</span>:</span></span><br><span class=\"line\">    sum = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>] * k</span><br><span class=\"line\">    count = [<span class=\"number\">0</span>] * k</span><br><span class=\"line\">    newC = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        min = <span class=\"number\">32768</span></span><br><span class=\"line\">        minj = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) &lt; min:</span><br><span class=\"line\">                min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])</span><br><span class=\"line\">                minj = j</span><br><span class=\"line\">        y_predict[i] = (minj + <span class=\"number\">1</span>) % k</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        sum[y_predict[i]] += X_train[i]</span><br><span class=\"line\">        count[y_predict[i]] += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">        newC.append(sum[i] / count[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_predict, newC</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"主函数\"><a href=\"#主函数\" class=\"headerlink\" title=\"主函数\"></a>主函数</h2><ul>\n<li>计算损失，更新k个中心点，再站队(聚类)一次</li>\n<li>重复，直到损失变化小于10%</li>\n<li>每次迭代显示新旧损失，显示损失变化</li>\n<li>最后输出分类结果<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    k = <span class=\"number\">3</span></span><br><span class=\"line\">    total = len(y_train)</span><br><span class=\"line\">    y_predict = [<span class=\"number\">0</span>] * total</span><br><span class=\"line\">    C = initk(X_train, k)</span><br><span class=\"line\">    oldeval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_predict, C = cluster(C, X_train, y_predict, k)</span><br><span class=\"line\">        neweval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">        ratio = (oldeval - neweval) / oldeval * <span class=\"number\">100</span></span><br><span class=\"line\">        print(oldeval, <span class=\"string\">\" -&gt; \"</span>, neweval, <span class=\"string\">\"%f %%\"</span> % ratio)</span><br><span class=\"line\">        oldeval = neweval</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ratio &lt; <span class=\"number\">0.1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    print(y_train)</span><br><span class=\"line\">    print(y_predict)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    m = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(y_train)):</span><br><span class=\"line\">        m += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> y_train[i] == y_predict[i]:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">    print(n / m)</span><br><span class=\"line\">    print(classification_report(y_train, y_predict, target_names=iris.target_names))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"KNN代码\"><a href=\"#KNN代码\" class=\"headerlink\" title=\"KNN代码\"></a>KNN代码</h1><ul>\n<li>直接使用了sklearn中的KNeighborsClassifier<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KNN</span><span class=\"params\">(X_train, X_test, y_train, y_test, iris)</span>:</span></span><br><span class=\"line\">    knc = KNeighborsClassifier()</span><br><span class=\"line\">    knc.fit(X_train, y_train)</span><br><span class=\"line\">    y_predict = knc.predict(X_test)</span><br><span class=\"line\">    print(knc.score(X_test, y_test))</span><br><span class=\"line\">    print(classification_report(y_test, y_predict, target_names=iris.target_names))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    KNN(X_train, X_test, y_train, y_test, iris)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h1><ul>\n<li>指标说明<br>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<script type=\"math/tex; mode=display\">\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\</script></li>\n<li><p>K-Means程序输出<br>预测正确率:88.39%<br>平均精确率:89%<br>召回率:0.88<br>F1指标:0.88<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o1Ts.jpg\" alt=\"i0o1Ts.jpg\"></p>\n</li>\n<li><p>KNN程序输出<br>预测正确率:71.05%<br>平均精确率:86%<br>召回率:0.71<br>F1指标:0.70<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oKOg.jpg\" alt=\"i0oKOg.jpg\"></p>\n</li>\n<li><p>原始分类<br>可以看到这个数据集本身在空间上就比较方便聚类划分<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oQmQ.gif\" alt=\"i0oQmQ.gif\"></p>\n</li>\n<li><p>预测分类<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o8kn.gif\" alt=\"i0o8kn.gif\"></p>\n</li>\n</ul>\n<h1 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h1><h2 id=\"未知k的情况\"><a href=\"#未知k的情况\" class=\"headerlink\" title=\"未知k的情况\"></a>未知k的情况</h2><ul>\n<li>以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?</li>\n<li>一种方式是canopy算法</li>\n<li>待补充</li>\n</ul>\n<h2 id=\"空类的处理\"><a href=\"#空类的处理\" class=\"headerlink\" title=\"空类的处理\"></a>空类的处理</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"不同距离计算方式\"><a href=\"#不同距离计算方式\" class=\"headerlink\" title=\"不同距离计算方式\"></a>不同距离计算方式</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"ANN算法\"><a href=\"#ANN算法\" class=\"headerlink\" title=\"ANN算法\"></a>ANN算法</h2>","site":{"data":{}},"excerpt":"<ul>\n<li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li>\n<li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li>\n</ul>\n<hr>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0onl8.jpg\" alt=\"i0onl8.jpg\"></p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><ul>\n<li>K-Means是简单的基于划分的聚类方法，要解决的问题是，现在有n个样本(点集X)，将他们的特征向量投射到高维空间中，根据空间分布可以大致划分成几个子空间，每个子空间中的点属于同一类，现在需要计算出每个点所在的类，大致思想就是随机选择k个点(中心点集C)作为中心点，其余的点自己站队：离k个中心点里哪个点最近就站那个点的队，即和那个中心点划分到同一类中，这样就能形成k个类，重复上过程，期间引入一个损失评估，比如以各个类中的点到这个类中心点距离的和作为评估指标，当指标小于某一程度或者指标变化小于某一程度就停止重复</li>\n<li>KNN则比较简单粗暴，其思想类似于民主投票。KNN不训练数据，选定一个值K，对于每一个需要预测的向量，在已知类别的数据集中找到与这个向量最近的K，这K个点中拥有最多点个数的类别就是预测类别，即让离某个点最近的K个点投票决定这个点的类别，哪个类别的点票数多就是哪个类别</li>\n</ul>\n<h1 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means++\"></a>K-means++</h1><ul>\n<li>k-means++在k-means上优化了初始k个点的选择。原始算法是随机取k个点，显然这样随机不确定性太大，比较好的k个点的选择方案应该是他们离彼此尽量远，但不能太远。尽量远，就能尽可能贴近最终理想中心点分布；不能太远，是为了防止将一些错误点孤立点作为中心点</li>\n<li>算法上的实现是先随机从X集中取第一个中心点，之后反复以下过程取中心点<ul>\n<li>计算每个点$c_i$到已经选出的中心点$k_1,k_2…$的距离，选取最小的一个距离作为$c_i$的距离，这个距离的意义即$c_i$作为下一个中心点时离其他中心点至少有多远</li>\n<li>将$c_1,c_2,c_3……$的距离归一化，并排成一条线</li>\n<li>这条线长度为1，分成了许多段，每一段的长度就代表了这一段所代表的点的距离在归一化中所占的比例，距离越大，比例越大</li>\n<li>在(0,1)之间随机取一个数，这个数所在的段区间所代表的点就是下一个中心点，将其加入中心点集C，接着重复找下一个中心点</li>\n</ul>\n</li>\n<li>可以看出，如果距离够远，在线上被随机抽到的可能越大，符合我们的需求</li>\n</ul>\n<h1 id=\"K-Means代码实现\"><a href=\"#K-Means代码实现\" class=\"headerlink\" title=\"K-Means代码实现\"></a>K-Means代码实现</h1><h2 id=\"数据检视\"><a href=\"#数据检视\" class=\"headerlink\" title=\"数据检视\"></a>数据检视</h2><ul>\n<li>Iris是鸢尾花分类数据集，150个样本，均匀分成3类，每一个样本有4个属性<br><img src=\"https://s1.ax1x.com/2018/10/20/i0otpV.jpg\" alt=\"i0otpV.jpg\"></li>\n</ul>\n<h2 id=\"初始化数据\"><a href=\"#初始化数据\" class=\"headerlink\" title=\"初始化数据\"></a>初始化数据</h2><ul>\n<li>初始化数据<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"k-means-初始化k个点\"><a href=\"#k-means-初始化k个点\" class=\"headerlink\" title=\"k-means++初始化k个点\"></a>k-means++初始化k个点</h2><ul>\n<li>D2是每个点的距离(即之前定义的里其他中心点至少有多远)</li>\n<li>probs归一化</li>\n<li>cumprobs将归一化的概率累加，排列成一条线<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initk</span><span class=\"params\">(X_train, k)</span>:</span></span><br><span class=\"line\">    C = [X_train[<span class=\"number\">0</span>]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, k):</span><br><span class=\"line\">        D2 = scipy.array([min([scipy.inner(c - x, c - x) <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> C]) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X_train])</span><br><span class=\"line\">        probs = D2 / D2.sum()</span><br><span class=\"line\">        cumprobs = probs.cumsum()</span><br><span class=\"line\">        r = scipy.rand()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j, p <span class=\"keyword\">in</span> enumerate(cumprobs):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> r &lt; p:</span><br><span class=\"line\">                i = j</span><br><span class=\"line\">                <span class=\"keyword\">break</span></span><br><span class=\"line\">        C.append(X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"损失评估\"><a href=\"#损失评估\" class=\"headerlink\" title=\"损失评估\"></a>损失评估</h2><ul>\n<li>在这里用每个类内点到中心点距离平方和的总和作为损失评估<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate</span><span class=\"params\">(C, X_train, y_predict)</span>:</span></span><br><span class=\"line\">    sum = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        c = C[y_predict[i]]</span><br><span class=\"line\">        sum += scipy.inner(c - X_train[i], c - X_train[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><ul>\n<li>初始化k个中心点后，所有的点就可以分类</li>\n<li>重新在每个类中取中心点，在这里取一个类中所有点坐标平均作为中心点坐标<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cluster</span><span class=\"params\">(C, X_train, y_predict, k)</span>:</span></span><br><span class=\"line\">    sum = [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>] * k</span><br><span class=\"line\">    count = [<span class=\"number\">0</span>] * k</span><br><span class=\"line\">    newC = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        min = <span class=\"number\">32768</span></span><br><span class=\"line\">        minj = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> scipy.inner(C[j] - X_train[i], C[j] - X_train[i]) &lt; min:</span><br><span class=\"line\">                min = scipy.inner(C[j] - X_train[i], C[j] - X_train[i])</span><br><span class=\"line\">                minj = j</span><br><span class=\"line\">        y_predict[i] = (minj + <span class=\"number\">1</span>) % k</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X_train)):</span><br><span class=\"line\">        sum[y_predict[i]] += X_train[i]</span><br><span class=\"line\">        count[y_predict[i]] += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(k):</span><br><span class=\"line\">        newC.append(sum[i] / count[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_predict, newC</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"主函数\"><a href=\"#主函数\" class=\"headerlink\" title=\"主函数\"></a>主函数</h2><ul>\n<li>计算损失，更新k个中心点，再站队(聚类)一次</li>\n<li>重复，直到损失变化小于10%</li>\n<li>每次迭代显示新旧损失，显示损失变化</li>\n<li>最后输出分类结果<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    k = <span class=\"number\">3</span></span><br><span class=\"line\">    total = len(y_train)</span><br><span class=\"line\">    y_predict = [<span class=\"number\">0</span>] * total</span><br><span class=\"line\">    C = initk(X_train, k)</span><br><span class=\"line\">    oldeval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>):</span><br><span class=\"line\">        y_predict, C = cluster(C, X_train, y_predict, k)</span><br><span class=\"line\">        neweval = evaluate(C, X_train, y_predict)</span><br><span class=\"line\">        ratio = (oldeval - neweval) / oldeval * <span class=\"number\">100</span></span><br><span class=\"line\">        print(oldeval, <span class=\"string\">\" -&gt; \"</span>, neweval, <span class=\"string\">\"%f %%\"</span> % ratio)</span><br><span class=\"line\">        oldeval = neweval</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ratio &lt; <span class=\"number\">0.1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">    print(y_train)</span><br><span class=\"line\">    print(y_predict)</span><br><span class=\"line\">    n = <span class=\"number\">0</span></span><br><span class=\"line\">    m = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(y_train)):</span><br><span class=\"line\">        m += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> y_train[i] == y_predict[i]:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">    print(n / m)</span><br><span class=\"line\">    print(classification_report(y_train, y_predict, target_names=iris.target_names))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"KNN代码\"><a href=\"#KNN代码\" class=\"headerlink\" title=\"KNN代码\"></a>KNN代码</h1><ul>\n<li>直接使用了sklearn中的KNeighborsClassifier<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    iris = load_iris()</span><br><span class=\"line\">    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.25</span>, random_state=<span class=\"number\">33</span>)</span><br><span class=\"line\">    ss = StandardScaler()</span><br><span class=\"line\">    X_train = ss.fit_transform(X_train)</span><br><span class=\"line\">    X_test = ss.fit_transform(X_test)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_train, X_test, y_train, y_test, iris</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">KNN</span><span class=\"params\">(X_train, X_test, y_train, y_test, iris)</span>:</span></span><br><span class=\"line\">    knc = KNeighborsClassifier()</span><br><span class=\"line\">    knc.fit(X_train, y_train)</span><br><span class=\"line\">    y_predict = knc.predict(X_test)</span><br><span class=\"line\">    print(knc.score(X_test, y_test))</span><br><span class=\"line\">    print(classification_report(y_test, y_predict, target_names=iris.target_names))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    X_train, X_test, y_train, y_test, iris = init()</span><br><span class=\"line\">    KNN(X_train, X_test, y_train, y_test, iris)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">\"__main__\"</span>:</span><br><span class=\"line\">    main()</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"预测结果\"><a href=\"#预测结果\" class=\"headerlink\" title=\"预测结果\"></a>预测结果</h1><ul>\n<li>指标说明<br>对于二分类，四种情况的总数:对的预测成对的TP;对的预测成错的FN；错的预测成对的FP；错的预测成错的TN<script type=\"math/tex; mode=display\">\n精确率:P=\\frac{TP}{TP+FP} \\\\\n召回率:R=\\frac{TP}{TP+FN} \\\\\n1F值:\\frac {2}{F_1}=\\frac1P+\\frac1R \\\\</script></li>\n<li><p>K-Means程序输出<br>预测正确率:88.39%<br>平均精确率:89%<br>召回率:0.88<br>F1指标:0.88<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o1Ts.jpg\" alt=\"i0o1Ts.jpg\"></p>\n</li>\n<li><p>KNN程序输出<br>预测正确率:71.05%<br>平均精确率:86%<br>召回率:0.71<br>F1指标:0.70<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oKOg.jpg\" alt=\"i0oKOg.jpg\"></p>\n</li>\n<li><p>原始分类<br>可以看到这个数据集本身在空间上就比较方便聚类划分<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oQmQ.gif\" alt=\"i0oQmQ.gif\"></p>\n</li>\n<li><p>预测分类<br><img src=\"https://s1.ax1x.com/2018/10/20/i0o8kn.gif\" alt=\"i0o8kn.gif\"></p>\n</li>\n</ul>\n<h1 id=\"改进\"><a href=\"#改进\" class=\"headerlink\" title=\"改进\"></a>改进</h1><h2 id=\"未知k的情况\"><a href=\"#未知k的情况\" class=\"headerlink\" title=\"未知k的情况\"></a>未知k的情况</h2><ul>\n<li>以上是我们已知鸢尾花会分成3类，加入我们不知道有几类呢？毕竟k-means是无监督学习，可以在无标签的情况下计算，自然标签的个数我们也极有可能不知道，那么如何确定k?</li>\n<li>一种方式是canopy算法</li>\n<li>待补充</li>\n</ul>\n<h2 id=\"空类的处理\"><a href=\"#空类的处理\" class=\"headerlink\" title=\"空类的处理\"></a>空类的处理</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"不同距离计算方式\"><a href=\"#不同距离计算方式\" class=\"headerlink\" title=\"不同距离计算方式\"></a>不同距离计算方式</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"ANN算法\"><a href=\"#ANN算法\" class=\"headerlink\" title=\"ANN算法\"></a>ANN算法</h2>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0onl8.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"机器学习入门:K-Means和KNN","path":"2017/03/16/kmeans/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0onl8.jpg","excerpt":"<ul>\n<li>以简单的Iris数据集做测试，实现了K-means++聚类算法，并与sklearn中自带的KNN算法进行比较</li>\n<li>标题本来是K-Means&amp;KNN，把&amp;改成了和，因为标题中出现特殊符号&amp;会导致我的sitemap生成错误……</li>\n</ul>\n<hr>","date":"2017-03-16T07:51:11.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["machinelearning","code"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Algorithm 题解目录","date":"2017-03-27T11:47:54.000Z","photo":null,"mathjax":true,"_content":"\n***\n算法刷题目录，方便自己查找回忆复习\n之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧\n\n<!--more--> \n\n！[mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg)\n\n# Leetcode\n-\t[Leetcode算法列表](https://leetcode.com/problemset/algorithms/)\n-\t直接搜每道题的序号即可\n\n## 排序\n-\t75:给出一个只包含0，1，2的数列，排序形成[0,0,...,0,1,...,1,2,...,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。\n-\t164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。\n-\t179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序\n\n## 堆、栈、队列\n-\t23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中\n-\t150:算术表达式，栈，python的switch写法\n-\t224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1\n-\t373:在2对有序数组中找出有最小和的k组数对，优先队列\n\n## 组合数学\n-\t47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重\n\n## 搜索与查找\n-\t74:在一个有有特定顺序的矩阵中查找，两次二分查找\n-\t78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归\n-\t79:在一个字母矩阵中查找单词，深搜\n-\t89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称\n-\t140:将一个字符串按给定词典加空格成句子，记忆化搜索\n-\t153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找\n-\t154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个\n-\t240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法\n\n## 树\n-\t98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈\n-\t101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本\n-\t106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可\n-\t107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs\n-\t108:构造一棵二叉查找树，递归\n-\t114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点\n-\t144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点\n-\t515:求二叉树每一层的最大值，BFS\n\n## 图论\n-\t130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic\n\n## 数学题\n-\t4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] <= A[i] and A[i-1] <= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。\n-\t57：给出一组区间，插入一个新区间并合并。模拟\n-\t122:水，一行代码，找一个序列中的最大相邻元素差\n-\t142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点\n-\t166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复\n-\t172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数\n-\t202:快慢指针，求循环的一种方法\n-\t263:数学题\n-\t264:数学题\n-\t313:数学题\n\n## 字符串\n-\t131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码\n-\t242:水题，python字典的使用\n-\t301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况\n-\t451:按频率统计字母，哈希，python字典\n-\t541:部分字符串翻转，模拟，注意python切片的使用\n\n## 贪心\n-\t134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1\n-\t402:从一个大数中移除k个数字，使得新的数最小，栈，贪心\n-\t452:重叠区间问题，贪心\n\n## 动态规划\n-\t70:经典问题爬梯子，斐波那契数列，dp\n-\t96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left\\*right，即f(i,n)=ans(i-1)\\*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) \\* ans(n-1) + ans(1) \\* ans(n-2) + … + ans(n-1) \\* ans(0)，边界ans(0)=ans(1)=1。 \n-\t139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)\n-\t174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前\n-\t312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]\\*num[i]\\*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。\n-\t338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n) & x=2n \\\\\n\tf(n)+1 & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n-\t397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n)+1 & x=2n \\\\\n\tmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n\t奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时\n\t可以将方程进一步化简\n\tIf n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.\n\tIf n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.\n\t[证明在这里](https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof)\t\n-\t472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见[Python中实现字典树](http://thinkwee.top/2017/05/02/trie/#more)\n\t\n## 分治\n-\t247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法\n\n# Poj\n-\t[C++源码](https://github.com/thinkwee/Poj_Test)\n-\t1062:Dijkstra\n-\t1068:模拟\n-\t1094:拓扑排序\n-\t1328:贪心，换求解变量\n-\t1753:枚举，位运算\n-\t1789:Prim，优先队列\n-\t1860:bellman-ford\n-\t2109:贪心，高精度乘法\n-\t2965:枚举，位运算\n-\t3259:建模，bellman-ford\n-\t3295:模拟，栈\n\n# 校内赛\n-\t[2017pre](http://code.bupt.edu.cn/contest/650/)\n-\tA,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题\n-\tD,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断\n-\tF,题目骗人，不需要回溯，递推公式可以写出特征方程求解\n-\tG,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs\n-\tH,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1\n-\tI,高中物理，求重力加速度，方程不好解，二分法逼近答案\t\n\n# hiho\n-\thiho\n-\t1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i < j, p < q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 \n-\t1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1][j](1.0-a[i]);a[i]为第i次正面朝上的概率，注意对j=0进行特判。\n-\t1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。\n-\t1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。\n","source":"_posts/oj.md","raw":"---\ntitle: Algorithm 题解目录\ndate: 2017-03-27 19:47:54\ntags:\n-\tcode\n-\tpython\n-\tc++\n-\talgorithm\ncategories:\n-\t算法\nphoto: \nmathjax: true\n---\n\n***\n算法刷题目录，方便自己查找回忆复习\n之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧\n\n<!--more--> \n\n！[mark](http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg)\n\n# Leetcode\n-\t[Leetcode算法列表](https://leetcode.com/problemset/algorithms/)\n-\t直接搜每道题的序号即可\n\n## 排序\n-\t75:给出一个只包含0，1，2的数列，排序形成[0,0,...,0,1,...,1,2,...,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。\n-\t164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。\n-\t179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序\n\n## 堆、栈、队列\n-\t23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中\n-\t150:算术表达式，栈，python的switch写法\n-\t224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1\n-\t373:在2对有序数组中找出有最小和的k组数对，优先队列\n\n## 组合数学\n-\t47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重\n\n## 搜索与查找\n-\t74:在一个有有特定顺序的矩阵中查找，两次二分查找\n-\t78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归\n-\t79:在一个字母矩阵中查找单词，深搜\n-\t89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称\n-\t140:将一个字符串按给定词典加空格成句子，记忆化搜索\n-\t153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找\n-\t154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个\n-\t240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法\n\n## 树\n-\t98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈\n-\t101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本\n-\t106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可\n-\t107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs\n-\t108:构造一棵二叉查找树，递归\n-\t114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点\n-\t144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点\n-\t515:求二叉树每一层的最大值，BFS\n\n## 图论\n-\t130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic\n\n## 数学题\n-\t4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] <= A[i] and A[i-1] <= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。\n-\t57：给出一组区间，插入一个新区间并合并。模拟\n-\t122:水，一行代码，找一个序列中的最大相邻元素差\n-\t142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点\n-\t166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复\n-\t172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数\n-\t202:快慢指针，求循环的一种方法\n-\t263:数学题\n-\t264:数学题\n-\t313:数学题\n\n## 字符串\n-\t131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码\n-\t242:水题，python字典的使用\n-\t301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况\n-\t451:按频率统计字母，哈希，python字典\n-\t541:部分字符串翻转，模拟，注意python切片的使用\n\n## 贪心\n-\t134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1\n-\t402:从一个大数中移除k个数字，使得新的数最小，栈，贪心\n-\t452:重叠区间问题，贪心\n\n## 动态规划\n-\t70:经典问题爬梯子，斐波那契数列，dp\n-\t96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left\\*right，即f(i,n)=ans(i-1)\\*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) \\* ans(n-1) + ans(1) \\* ans(n-2) + … + ans(n-1) \\* ans(0)，边界ans(0)=ans(1)=1。 \n-\t139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)\n-\t174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前\n-\t312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]\\*num[i]\\*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。\n-\t338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n) & x=2n \\\\\n\tf(n)+1 & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n-\t397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:\n\t$$\n\tf(x)=\n\t\\begin{cases}\n\tf(n)+1 & x=2n \\\\\n\tmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\t\\end{cases}\n\t$$\n\t奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时\n\t可以将方程进一步化简\n\tIf n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.\n\tIf n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.\n\t[证明在这里](https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof)\t\n-\t472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见[Python中实现字典树](http://thinkwee.top/2017/05/02/trie/#more)\n\t\n## 分治\n-\t247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法\n\n# Poj\n-\t[C++源码](https://github.com/thinkwee/Poj_Test)\n-\t1062:Dijkstra\n-\t1068:模拟\n-\t1094:拓扑排序\n-\t1328:贪心，换求解变量\n-\t1753:枚举，位运算\n-\t1789:Prim，优先队列\n-\t1860:bellman-ford\n-\t2109:贪心，高精度乘法\n-\t2965:枚举，位运算\n-\t3259:建模，bellman-ford\n-\t3295:模拟，栈\n\n# 校内赛\n-\t[2017pre](http://code.bupt.edu.cn/contest/650/)\n-\tA,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题\n-\tD,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断\n-\tF,题目骗人，不需要回溯，递推公式可以写出特征方程求解\n-\tG,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs\n-\tH,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1\n-\tI,高中物理，求重力加速度，方程不好解，二分法逼近答案\t\n\n# hiho\n-\thiho\n-\t1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i < j, p < q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 \n-\t1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1][j](1.0-a[i]);a[i]为第i次正面朝上的概率，注意对j=0进行特判。\n-\t1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。\n-\t1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。\n","slug":"oj","published":1,"updated":"2019-07-22T03:45:23.295Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vaq004bq8t5v507mb95","content":"<hr>\n<p>算法刷题目录，方便自己查找回忆复习<br>之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧</p>\n<a id=\"more\"></a> \n<p>！<a href=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg\" target=\"_blank\" rel=\"noopener\">mark</a></p>\n<h1 id=\"Leetcode\"><a href=\"#Leetcode\" class=\"headerlink\" title=\"Leetcode\"></a>Leetcode</h1><ul>\n<li><a href=\"https://leetcode.com/problemset/algorithms/\" target=\"_blank\" rel=\"noopener\">Leetcode算法列表</a></li>\n<li>直接搜每道题的序号即可</li>\n</ul>\n<h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><ul>\n<li>75:给出一个只包含0，1，2的数列，排序形成[0,0,…,0,1,…,1,2,…,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。</li>\n<li>164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。</li>\n<li>179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序</li>\n</ul>\n<h2 id=\"堆、栈、队列\"><a href=\"#堆、栈、队列\" class=\"headerlink\" title=\"堆、栈、队列\"></a>堆、栈、队列</h2><ul>\n<li>23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中</li>\n<li>150:算术表达式，栈，python的switch写法</li>\n<li>224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1</li>\n<li>373:在2对有序数组中找出有最小和的k组数对，优先队列</li>\n</ul>\n<h2 id=\"组合数学\"><a href=\"#组合数学\" class=\"headerlink\" title=\"组合数学\"></a>组合数学</h2><ul>\n<li>47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重</li>\n</ul>\n<h2 id=\"搜索与查找\"><a href=\"#搜索与查找\" class=\"headerlink\" title=\"搜索与查找\"></a>搜索与查找</h2><ul>\n<li>74:在一个有有特定顺序的矩阵中查找，两次二分查找</li>\n<li>78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归</li>\n<li>79:在一个字母矩阵中查找单词，深搜</li>\n<li>89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称</li>\n<li>140:将一个字符串按给定词典加空格成句子，记忆化搜索</li>\n<li>153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找</li>\n<li>154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个</li>\n<li>240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法</li>\n</ul>\n<h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><ul>\n<li>98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈</li>\n<li>101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本</li>\n<li>106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可</li>\n<li>107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs</li>\n<li>108:构造一棵二叉查找树，递归</li>\n<li>114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点</li>\n<li>144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点</li>\n<li>515:求二叉树每一层的最大值，BFS</li>\n</ul>\n<h2 id=\"图论\"><a href=\"#图论\" class=\"headerlink\" title=\"图论\"></a>图论</h2><ul>\n<li>130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic</li>\n</ul>\n<h2 id=\"数学题\"><a href=\"#数学题\" class=\"headerlink\" title=\"数学题\"></a>数学题</h2><ul>\n<li>4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。</li>\n<li>57：给出一组区间，插入一个新区间并合并。模拟</li>\n<li>122:水，一行代码，找一个序列中的最大相邻元素差</li>\n<li>142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点</li>\n<li>166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复</li>\n<li>172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数</li>\n<li>202:快慢指针，求循环的一种方法</li>\n<li>263:数学题</li>\n<li>264:数学题</li>\n<li>313:数学题</li>\n</ul>\n<h2 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h2><ul>\n<li>131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码</li>\n<li>242:水题，python字典的使用</li>\n<li>301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况</li>\n<li>451:按频率统计字母，哈希，python字典</li>\n<li>541:部分字符串翻转，模拟，注意python切片的使用</li>\n</ul>\n<h2 id=\"贪心\"><a href=\"#贪心\" class=\"headerlink\" title=\"贪心\"></a>贪心</h2><ul>\n<li>134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1</li>\n<li>402:从一个大数中移除k个数字，使得新的数最小，栈，贪心</li>\n<li>452:重叠区间问题，贪心</li>\n</ul>\n<h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><ul>\n<li>70:经典问题爬梯子，斐波那契数列，dp</li>\n<li>96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left*right，即f(i,n)=ans(i-1)*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) * ans(n-1) + ans(1) * ans(n-2) + … + ans(n-1) * ans(0)，边界ans(0)=ans(1)=1。 </li>\n<li>139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)</li>\n<li>174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前</li>\n<li>312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]*num[i]*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。</li>\n<li>338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nf(n) & x=2n \\\\\nf(n)+1 & x=2n+1 \\\\\n\\end{cases}</script></li>\n<li>397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nf(n)+1 & x=2n \\\\\nmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\\end{cases}</script>奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时<br>可以将方程进一步化简<br>If n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.<br>If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.<br><a href=\"https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof\" target=\"_blank\" rel=\"noopener\">证明在这里</a>    </li>\n<li>472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见<a href=\"http://thinkwee.top/2017/05/02/trie/#more\">Python中实现字典树</a></li>\n</ul>\n<h2 id=\"分治\"><a href=\"#分治\" class=\"headerlink\" title=\"分治\"></a>分治</h2><ul>\n<li>247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法</li>\n</ul>\n<h1 id=\"Poj\"><a href=\"#Poj\" class=\"headerlink\" title=\"Poj\"></a>Poj</h1><ul>\n<li><a href=\"https://github.com/thinkwee/Poj_Test\" target=\"_blank\" rel=\"noopener\">C++源码</a></li>\n<li>1062:Dijkstra</li>\n<li>1068:模拟</li>\n<li>1094:拓扑排序</li>\n<li>1328:贪心，换求解变量</li>\n<li>1753:枚举，位运算</li>\n<li>1789:Prim，优先队列</li>\n<li>1860:bellman-ford</li>\n<li>2109:贪心，高精度乘法</li>\n<li>2965:枚举，位运算</li>\n<li>3259:建模，bellman-ford</li>\n<li>3295:模拟，栈</li>\n</ul>\n<h1 id=\"校内赛\"><a href=\"#校内赛\" class=\"headerlink\" title=\"校内赛\"></a>校内赛</h1><ul>\n<li><a href=\"http://code.bupt.edu.cn/contest/650/\" target=\"_blank\" rel=\"noopener\">2017pre</a></li>\n<li>A,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题</li>\n<li>D,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断</li>\n<li>F,题目骗人，不需要回溯，递推公式可以写出特征方程求解</li>\n<li>G,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs</li>\n<li>H,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1</li>\n<li>I,高中物理，求重力加速度，方程不好解，二分法逼近答案    </li>\n</ul>\n<h1 id=\"hiho\"><a href=\"#hiho\" class=\"headerlink\" title=\"hiho\"></a>hiho</h1><ul>\n<li>hiho</li>\n<li>1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i &lt; j, p &lt; q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 </li>\n<li>1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1]<a href=\"1.0-a[i]\">j</a>;a[i]为第i次正面朝上的概率，注意对j=0进行特判。</li>\n<li>1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。</li>\n<li>1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>算法刷题目录，方便自己查找回忆复习<br>之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧</p>","more":"<p>！<a href=\"http://ojtdnrpmt.bkt.clouddn.com/blog/20170327/195153747.jpg\" target=\"_blank\" rel=\"noopener\">mark</a></p>\n<h1 id=\"Leetcode\"><a href=\"#Leetcode\" class=\"headerlink\" title=\"Leetcode\"></a>Leetcode</h1><ul>\n<li><a href=\"https://leetcode.com/problemset/algorithms/\" target=\"_blank\" rel=\"noopener\">Leetcode算法列表</a></li>\n<li>直接搜每道题的序号即可</li>\n</ul>\n<h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><ul>\n<li>75:给出一个只包含0，1，2的数列，排序形成[0,0,…,0,1,…,1,2,…,2]的形式，在原数列上修改。借鉴Lomuto partition algorithm，维护三个区间[0,i],[i,j],[j,k]为0，1，2的范围，按优先级，若当前数为2或1或0，先将nums[k]=2，k++，若当前数为1或0，则nums[j]=1,j++，若当前数为0,则nums[i]=0,i++，越往后能覆盖前面的，优先级越高。</li>\n<li>164：给出一个无序数列，请在O(n)时间复杂度内找到数列有序化后相隔元素差最大值。桶排序，设数列中最大值最小值分别为max,min,易得这个相邻元素差最大值肯定大于(max-min)/(n-1)并设这个值为gap，n是数列长度。将数列的取值区间[min,max]以(max-min)/(n-1)为间隔分成nums个桶，nums是int((max - min) / gap + 1)，把数列各个元素分到桶中，每个桶只存对应区间内的最大元素和最小元素，把那么最大间隔肯定在相邻两个桶的边界处（前一个桶的最大值和后一个桶的最小值），因为桶内的最大间隔小于gap。最后遍历一遍桶就可以了。</li>\n<li>179：水题，给出一个数列，要求把各个数连起来组合成一个最大的数。字符串排序</li>\n</ul>\n<h2 id=\"堆、栈、队列\"><a href=\"#堆、栈、队列\" class=\"headerlink\" title=\"堆、栈、队列\"></a>堆、栈、队列</h2><ul>\n<li>23:合并K个有序链表为1个有序链表，用最小堆存所有链表头节点，依次取出插入到新链表中</li>\n<li>150:算术表达式，栈，python的switch写法</li>\n<li>224:带括号的算术表达式，栈,巧妙地方法处理括号，如果括号外是负号则符号乘-1否则乘1</li>\n<li>373:在2对有序数组中找出有最小和的k组数对，优先队列</li>\n</ul>\n<h2 id=\"组合数学\"><a href=\"#组合数学\" class=\"headerlink\" title=\"组合数学\"></a>组合数学</h2><ul>\n<li>47:生成n个数字的所有组合，数字有重复，依次插入数字到已生成k个数字组合的k+1个空中，排重直接用集合，Top Solution中用插入处后一个数字是否重复来排重</li>\n</ul>\n<h2 id=\"搜索与查找\"><a href=\"#搜索与查找\" class=\"headerlink\" title=\"搜索与查找\"></a>搜索与查找</h2><ul>\n<li>74:在一个有有特定顺序的矩阵中查找，两次二分查找</li>\n<li>78:生成子集，递归或者迭代，每个数字分加入子集或者不加入子集进行递归</li>\n<li>79:在一个字母矩阵中查找单词，深搜</li>\n<li>89:生成格雷码，迭代，每次在最前面加1或0，格雷码上下对称</li>\n<li>140:将一个字符串按给定词典加空格成句子，记忆化搜索</li>\n<li>153:一个有序数组拆成两部分再倒接一起，查找，特殊的二分查找</li>\n<li>154:153如果加入重复数字的变形，加入一个条件，二分的start和end是相邻不重复的第一个</li>\n<li>240:74的改版，每一行每一列有序，查数，对行和列分别做二分，数肯定在满足条件的行与列的交集处，还有其他解法</li>\n</ul>\n<h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><ul>\n<li>98:判断一颗二叉树是不是查找树，即左子树都小于节点，右子树都大于节点，中序遍历看看是不是递增数列即可，递归或者栈</li>\n<li>101:判断一棵二叉树是不是对称的，递归，注意递归的是(left.left,right.right)和(left.right,right.left)即内外两队子节点，另外可以用栈实现迭代版本</li>\n<li>106:给出中序遍历后序遍历，求前序遍历。后序遍历的最后一个是根节点，在中序遍历中找到这个根节点并在后序遍历中删除，根节点以左是左子树，以右是右子树，这时后序遍历的最后一个节点就是左子树的根节点，递归即可</li>\n<li>107:从下往上从左往右输出树每一层的节点，用二维数组记录答案，如果二维数组的一维长度即层数比当前遍历的节点层数少就新建一层存答案。可以直接dfs，或者用栈和队列替代递归，如果用栈就是dfs，如果用队列就是bfs</li>\n<li>108:构造一棵二叉查找树，递归</li>\n<li>114:给一棵树，按前序遍历将其压扁，变成一棵只有右节点的树。将一个节点压扁操作定义为，把左子树置空，右子树变成原左子树，原右子树接在原左子树的尾节点上，原左子树的尾节点就是按前序遍历这个子树遍历到的最后一个点。递归，先压扁左节点，再压扁右节点，再回溯压父节点</li>\n<li>144:求一棵树的前序遍历，利用栈，弹出一个节点记录值，并先压入右节点，再压入左节点</li>\n<li>515:求二叉树每一层的最大值，BFS</li>\n</ul>\n<h2 id=\"图论\"><a href=\"#图论\" class=\"headerlink\" title=\"图论\"></a>图论</h2><ul>\n<li>130:水题，把一个图中被黑子包围的白子全部替换成黑子，直接从边上找出所有白子并存起来，再将整个棋盘涂黑，把白子复原。存的是top solution，写法很pythonic</li>\n</ul>\n<h2 id=\"数学题\"><a href=\"#数学题\" class=\"headerlink\" title=\"数学题\"></a>数学题</h2><ul>\n<li>4:给出两个有序数列，求出两个数列合并后的中位数，要求时间复杂度O(log(m+n))，所以不能直接一个一个比较。中位数的意义是把序列拆成长度相等两部分，大部分中最小数比小部分中最大数大，根据这个原理将两个序列拆成两部分，设两个数列长度分别为m,n，设数列1在i位置拆开，数列2在j位置拆开，则需要保证：1：两个长度一样，则$i+j=m-i+n-j$；2：两个数列的小部分的并集的任意数比两个数列大部分的并集的任意数要小，因为拆开后的部分依然有序，因此这个条件就是$B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]$，i的位置确定，j的位置就确定，因此只需二分查找i的位置即可。注意奇偶性和边界判断。</li>\n<li>57：给出一组区间，插入一个新区间并合并。模拟</li>\n<li>122:水，一行代码，找一个序列中的最大相邻元素差</li>\n<li>142:给出一个链表，找出链表中环的起点。依然是快慢钟算法，设一快一慢两个指针一起从链表开始走，第一次相遇时说明存在环且他们步数之差是环的长度，而且可以推算出链表起点到环起点的距离等于相遇点到环起点的距离，就可以找到环的起点</li>\n<li>166:给出除数被除数写出结果，包括循环小数显示，数学分析题，小数不断乘10除除数，更新余数，余数出现重复即小数出现重复</li>\n<li>172:求n!中0的个数，数学分析题，0来自5*2，就看n中有多少5及其倍数</li>\n<li>202:快慢指针，求循环的一种方法</li>\n<li>263:数学题</li>\n<li>264:数学题</li>\n<li>313:数学题</li>\n</ul>\n<h2 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h2><ul>\n<li>131:将一个字符串分成回文子串，用python的[i::-1]很好找，也是一行代码</li>\n<li>242:水题，python字典的使用</li>\n<li>301:给定字符串，删除最小数目的括号使得字符串中所有括号匹配，输出所有情况。注意两点：正序逆序各做一遍，因为有删左括号和右括号两种方案；所有情况都要输出，用集合存取所有情况</li>\n<li>451:按频率统计字母，哈希，python字典</li>\n<li>541:部分字符串翻转，模拟，注意python切片的使用</li>\n</ul>\n<h2 id=\"贪心\"><a href=\"#贪心\" class=\"headerlink\" title=\"贪心\"></a>贪心</h2><ul>\n<li>134:加油站排成一圈，给定各个加油站可以加的油和站与站之间路程所耗的油，问可以走完一圈需要从哪个加油站出发，数据保证答案唯一，贪心，如果前i个加油站走不完了，那么出发点就设成i+1</li>\n<li>402:从一个大数中移除k个数字，使得新的数最小，栈，贪心</li>\n<li>452:重叠区间问题，贪心</li>\n</ul>\n<h2 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h2><ul>\n<li>70:经典问题爬梯子，斐波那契数列，dp</li>\n<li>96:给一个1到n的数列，问可以构造出多少不同的bst。设ans(i)为长度为i的数列能构造不同的bst的个数，我们需要找到ans(i)与ans(i-1)的关系，引入中间变量f(i,n)，指以第i个数字为根构造bst，长度为n的数列能构造不同的bst的个数，这里可以找到递归关系，根左边的子数列构造出根的左子树，设有left种不同的左子树，右数列right同理，则f(i,n)应该是left*right，即f(i,n)=ans(i-1)*ans(n-i)，同时以不同的i为根会有完全不同的划分，则$ans(n)=\\sum _{i=1}^n f(i,n)$，合并可以得到ans(i)=ans(0) * ans(n-1) + ans(1) * ans(n-2) + … + ans(n-1) * ans(0)，边界ans(0)=ans(1)=1。 </li>\n<li>139:给出词典，给出一个字符串，判断字符串是否可以完全由词典内的单词构成。设f[i]==true即前字符串前i位都可以完全由单词构成，则遍历每个单词，单词长度为k，若f[i-k]为真则f[i]为真，也可以转换为图用dfs做，i与j之间有一条边说明s[i:j]是一个单词，我们要找到一条路径从0到len(s)</li>\n<li>174:矩阵中有正数负数，在中途不为0的情况下求最小路径。动态规划，从后往前</li>\n<li>312:给出一个数列，问怎样依次消去数能得到最多硬币，得硬币的规则：消掉一个数得到这个数和相邻两个数累乘的数量的硬币。区间dp(分治)，一段区间内能得到的最多硬币f[x,y]是看其最后一次消去位置在i，得到的硬币就是f[x,i-1]+num[i-1]*num[i]*num[i+1]+f[i+1,y]，在这个区间内枚举位置i,其中需要左右两个小区间的位置已知，因此将区间从小到大枚举。总共三层循环：外循环区间长度，中循环区间在整个序列中的开始位置，内循环枚举区间内位置i。</li>\n<li>338:求出[1,num]内每个数字二进制中有多少个1，列举几个数分析发现对偶数n，它的二进制就是n/2的二进制左移一位，1的个数不变，对奇数n，它的二进制就是n/2的二进制左移一位并末尾加1，即1的个数多一个，显然状态转移方程<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nf(n) & x=2n \\\\\nf(n)+1 & x=2n+1 \\\\\n\\end{cases}</script></li>\n<li>397:一个数是偶数就除2，是奇数就变到相邻数，问最少几次变成1，根据题意可以写出状态转移方程:<script type=\"math/tex; mode=display\">\nf(x)=\n\\begin{cases}\nf(n)+1 & x=2n \\\\\nmin(f(2n+2)+1,f(2n)+1) & x=2n+1 \\\\\n\\end{cases}</script>奇数情况可以化简为$min(f(2n)+1,f(n+1)+2)$，这样就可以从1到n动规了，但是会超时<br>可以将方程进一步化简<br>If n % 4 = 3 and n != 3, then f(n) = f(n + 1) + 1.<br>If n % 4 = 1 or n = 3, then f(n) = f(n - 1) + 1.<br><a href=\"https://discuss.leetcode.com/topic/59350/python-o-log-n-time-o-1-space-with-explanation-and-proof\" target=\"_blank\" rel=\"noopener\">证明在这里</a>    </li>\n<li>472:是139的变式，依然是判断字符串是否可以由单词构成，但单词和要判断的字符串都在一个词典里，每个单词都需要判断是否完全能被其他单词组成。因为词典中无重复单词，所以先按单词长度排序，每个单词只能由它前面的单词构成，然后问题就转变成了139，第i个单词为要查询的字符串，前i-1个单词构成了查询所需的词典，一样的进行动态规划。top solution中利用了trie字典树加速，见<a href=\"http://thinkwee.top/2017/05/02/trie/#more\">Python中实现字典树</a></li>\n</ul>\n<h2 id=\"分治\"><a href=\"#分治\" class=\"headerlink\" title=\"分治\"></a>分治</h2><ul>\n<li>247:给出一个算式，无括号，问加括号的情况下有多少种可能的解(不要求去重)，分治，取第i个运算符号，对符号左边和右边分别递归求解部分解，python中map的用法</li>\n</ul>\n<h1 id=\"Poj\"><a href=\"#Poj\" class=\"headerlink\" title=\"Poj\"></a>Poj</h1><ul>\n<li><a href=\"https://github.com/thinkwee/Poj_Test\" target=\"_blank\" rel=\"noopener\">C++源码</a></li>\n<li>1062:Dijkstra</li>\n<li>1068:模拟</li>\n<li>1094:拓扑排序</li>\n<li>1328:贪心，换求解变量</li>\n<li>1753:枚举，位运算</li>\n<li>1789:Prim，优先队列</li>\n<li>1860:bellman-ford</li>\n<li>2109:贪心，高精度乘法</li>\n<li>2965:枚举，位运算</li>\n<li>3259:建模，bellman-ford</li>\n<li>3295:模拟，栈</li>\n</ul>\n<h1 id=\"校内赛\"><a href=\"#校内赛\" class=\"headerlink\" title=\"校内赛\"></a>校内赛</h1><ul>\n<li><a href=\"http://code.bupt.edu.cn/contest/650/\" target=\"_blank\" rel=\"noopener\">2017pre</a></li>\n<li>A,集合中找有序三个数，满足和为0，求这些数平方和，Leetcode原题</li>\n<li>D,求一个数的连续质数和分解，poj原题，数据大，因此没必要打表，直接对每一个输入判断</li>\n<li>F,题目骗人，不需要回溯，递推公式可以写出特征方程求解</li>\n<li>G,找一个集合中的最大子集，有绑定规则，选一个数必须选其他几个数，bfs</li>\n<li>H,给出字符变换规则和代价，求把两个字符串变成一样的最小代价，flyod，注意两个字母可以变成第三个字母，不一定是互相变，两个字符串可能不等长，不等长直接输出-1</li>\n<li>I,高中物理，求重力加速度，方程不好解，二分法逼近答案    </li>\n</ul>\n<h1 id=\"hiho\"><a href=\"#hiho\" class=\"headerlink\" title=\"hiho\"></a>hiho</h1><ul>\n<li>hiho</li>\n<li>1505：给定一个组，问有多少种下标四元组(i, j, p, q)满足i, j, p, q两两不同，并且i &lt; j, p &lt; q, Ai + Aj = Ap + Aq。2sum问题，数据范围不大，直接开哈希数组，sum[i]记录两个数之和为i有多少组，利用容斥定理解。 </li>\n<li>1506：投掷硬币，投n次m次朝上的概率是多少，每次投正面朝上概率不同。动态规划，记dp[i][j]为投前i次j次朝上的概率，状态转移方程为:dp[i][j]=dp[i-1][j-1]a[i]+dp[i-1]<a href=\"1.0-a[i]\">j</a>;a[i]为第i次正面朝上的概率，注意对j=0进行特判。</li>\n<li>1507：错误的记录，给出一棵树，给出根节点编号，在中间加一条错误边，找出所有可能的错误边。注意因为只加了一条错误边，因此情况可以分为两种：1.这条错误边连向根节点，直接输出。2.根节点正常，则这条错误边一定连在入度为2的节点上，且这样的节点只有一个，找到这个节点，将连接它的两条边分别去掉并从根节点开始dfs，如果dfs遍历完次数为n，说明这条边去掉后树依然成立，这是错误边，如果次数小于n，说明去掉这条边树就断了，形成森林，这条边不是错误边。注意边重复的情况，这时候只去掉其中一条，但是两条边是分别当错误边输出。因为错误边数小于等于2，所以dfs的次数小于等于2，加上构造去掉边的图，时间复杂度是O(n)。</li>\n<li>1515：给定一些同学之间的分数关系（A比B高或低多少分），最后对q个查询给出答案。带权并查集，每个集维护同学到这个集某一个根同学的分数差。每次输入一条关系便合并两个同学所在的并查集，并对每个并查集做一次更新,加入y被x合并，x,y之间关系值是s,更新公式是d[root of y]=d[x]-d[y]-s，并在下一次循环中更新整个并查集的值。最后直接查询。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"Algorithm 题解目录","path":"2017/03/27/oj/","eyeCatchImage":null,"excerpt":"<hr>\n<p>算法刷题目录，方便自己查找回忆复习<br>之后(2018.9.27)只更新leetcode上的题了，也懒得整理源码了，leetcode上都存了，只记录思路吧</p>","date":"2017-03-27T11:47:54.000Z","pv":0,"totalPV":0,"categories":"算法","tags":["code","python","c++","algorithm"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"基于端到端模型的生成式自动文摘研究","date":"2018-07-04T07:58:59.000Z","mathjax":true,"html":true,"_content":"\n本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制\n现在对整个模型做一个简单的总结\n\n<!--more-->\n \n![i0TGHH.png](https://s1.ax1x.com/2018/10/20/i0TGHH.png)\n\n# 任务\n-\t自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。\n-\t自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。\n\n![i0TYEd.jpg](https://s1.ax1x.com/2018/10/20/i0TYEd.jpg)\n\n# 预备知识\n##\t循环神经网络\n-\t循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。\n-\t不展开时形式如下：\n![i0TtUA.jpg](https://s1.ax1x.com/2018/10/20/i0TtUA.jpg)\n-\t按时间步展开之后：\n![i0TN4I.jpg](https://s1.ax1x.com/2018/10/20/i0TN4I.jpg)\n\n##\tLSTM和GRU\n-\t循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。\n-\tLSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。\n![i0TaCt.jpg](https://s1.ax1x.com/2018/10/20/i0TaCt.jpg)\n-\tGRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。\n![i0Td8P.jpg](https://s1.ax1x.com/2018/10/20/i0Td8P.jpg)\n##\t词嵌入\n-\t深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。\n-\t词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维\n度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。\n-\t下图展示了word2vec中的skipgram模型：\n![i0Twgf.jpg](https://s1.ax1x.com/2018/10/20/i0Twgf.jpg)\n-\t得到的词嵌入矩阵如下：\n![i0T0v8.jpg](https://s1.ax1x.com/2018/10/20/i0T0v8.jpg)\n-\tMikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。\n-\tWord2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，\n-\t而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。\n\n##\t注意力\n-\t在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。\n![i0TDKS.jpg](https://s1.ax1x.com/2018/10/20/i0TDKS.jpg)\n\n##\t序列到序列\n-\tseq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。\n![i0TrDg.jpg](https://s1.ax1x.com/2018/10/20/i0TrDg.jpg)\n-\t对于序列到序列模型的一些个人理解：\n\t-\t（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。\n\t-\t（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。\n\t-\t（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。\n\n## 序列损失\n-\t解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。\n\n# 基本模型\n-\t预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：\n![i0TsbQ.jpg](https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg)\n-\t训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征\n![i0T6Ej.jpg](https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg)\n-\t特征输入编码器得到中间表示\n![i0Tg5n.png](https://s1.ax1x.com/2018/10/20/i0Tg5n.png)\n-\t拿到中间表示和输出文摘(相当于label)，输入解码器进行解码\n![i0TIrF.png](https://s1.ax1x.com/2018/10/20/i0TIrF.png)\n-\t加入注意力机制后完整的序列到序列模型结构如下：\n![i0TRCq.jpg](https://s1.ax1x.com/2018/10/20/i0TRCq.jpg)\n\n# 情感融合机制\n-\t情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。\n-\t先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型\n![i0TW80.jpg](https://s1.ax1x.com/2018/10/20/i0TW80.jpg)\n-\t查找词典得到情感向量（即情感特征）\n![i0Tf2V.jpg](https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg)\n-\t将情感特征直接拼接在中间表示之后，输入解码器\n![i0ThvT.jpg](https://s1.ax1x.com/2018/10/20/i0ThvT.jpg)\n\n# 结果\n-\t结果由ROUGE-F1值形式记录，情感语料下各种方法对比\n![i0T5KU.png](https://s1.ax1x.com/2018/10/20/i0T5KU.png)\n-\t普通语料下情感融合方案对比\n![i0Tb5R.png](https://s1.ax1x.com/2018/10/20/i0Tb5R.png)\n-\t情感分类准确率，作为参考，之前训练的情感分类器准确率为74%\n![i0Tob4.png](https://s1.ax1x.com/2018/10/20/i0Tob4.png)\n-\t因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果\n![i0T7VJ.png](https://s1.ax1x.com/2018/10/20/i0T7VJ.png)\n\n# 问题\n-\tunk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值\n![i0THa9.png](https://s1.ax1x.com/2018/10/20/i0THa9.png)\n-\t语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“...... 周三...... 黄金价格上涨”和“......周四......黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：\n\t-\t不去重：保留原文本，不去重，进行训练和测试\n\t-\t去重：删除语料中所有重复文摘对应的的短文本-短文摘对。\n\t-\t训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。\n\t-\t测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。\n\t重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。\n![i0TLP1.png](https://s1.ax1x.com/2018/10/20/i0TLP1.png)\n\n# 实现环境\n-\t这里是github地址：-\t[Abstract_Summarization_Tensorflow](https://github.com/thinkwee/Abstract_Summarization_Tensorflow)\n-\tUbuntu 16.04\n-\tTensorflow 1.6\n-\tCUDA 9.0\n-\tCudnn 7.1.2\n-\tGigawords数据集，训练了部分数据，约30万\n-\tGTX1066，训练时间3到4个小时\n\n# 参考文献\n![i0TX26.jpg](https://s1.ax1x.com/2018/10/20/i0TX26.jpg)\n![i0TO8x.jpg](https://s1.ax1x.com/2018/10/20/i0TO8x.jpg)\n\n","source":"_posts/seq2seq-summarization.md","raw":"---\ntitle: 基于端到端模型的生成式自动文摘研究\ndate: 2018-07-04 15:58:59\ntags:\n  - abstractive summarization\n  - seq2seq\n  - machinelearning\n  -\trnn\n  -\tnlp\n  - lstm\n  - gru\ncategories:\n  - 自然语言处理\nmathjax: true\nhtml: true\n---\n\n本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制\n现在对整个模型做一个简单的总结\n\n<!--more-->\n \n![i0TGHH.png](https://s1.ax1x.com/2018/10/20/i0TGHH.png)\n\n# 任务\n-\t自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。\n-\t自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。\n\n![i0TYEd.jpg](https://s1.ax1x.com/2018/10/20/i0TYEd.jpg)\n\n# 预备知识\n##\t循环神经网络\n-\t循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。\n-\t不展开时形式如下：\n![i0TtUA.jpg](https://s1.ax1x.com/2018/10/20/i0TtUA.jpg)\n-\t按时间步展开之后：\n![i0TN4I.jpg](https://s1.ax1x.com/2018/10/20/i0TN4I.jpg)\n\n##\tLSTM和GRU\n-\t循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。\n-\tLSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。\n![i0TaCt.jpg](https://s1.ax1x.com/2018/10/20/i0TaCt.jpg)\n-\tGRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。\n![i0Td8P.jpg](https://s1.ax1x.com/2018/10/20/i0Td8P.jpg)\n##\t词嵌入\n-\t深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。\n-\t词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维\n度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。\n-\t下图展示了word2vec中的skipgram模型：\n![i0Twgf.jpg](https://s1.ax1x.com/2018/10/20/i0Twgf.jpg)\n-\t得到的词嵌入矩阵如下：\n![i0T0v8.jpg](https://s1.ax1x.com/2018/10/20/i0T0v8.jpg)\n-\tMikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。\n-\tWord2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，\n-\t而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。\n\n##\t注意力\n-\t在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。\n![i0TDKS.jpg](https://s1.ax1x.com/2018/10/20/i0TDKS.jpg)\n\n##\t序列到序列\n-\tseq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。\n![i0TrDg.jpg](https://s1.ax1x.com/2018/10/20/i0TrDg.jpg)\n-\t对于序列到序列模型的一些个人理解：\n\t-\t（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。\n\t-\t（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。\n\t-\t（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。\n\n## 序列损失\n-\t解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。\n\n# 基本模型\n-\t预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：\n![i0TsbQ.jpg](https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg)\n-\t训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征\n![i0T6Ej.jpg](https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg)\n-\t特征输入编码器得到中间表示\n![i0Tg5n.png](https://s1.ax1x.com/2018/10/20/i0Tg5n.png)\n-\t拿到中间表示和输出文摘(相当于label)，输入解码器进行解码\n![i0TIrF.png](https://s1.ax1x.com/2018/10/20/i0TIrF.png)\n-\t加入注意力机制后完整的序列到序列模型结构如下：\n![i0TRCq.jpg](https://s1.ax1x.com/2018/10/20/i0TRCq.jpg)\n\n# 情感融合机制\n-\t情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。\n-\t先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型\n![i0TW80.jpg](https://s1.ax1x.com/2018/10/20/i0TW80.jpg)\n-\t查找词典得到情感向量（即情感特征）\n![i0Tf2V.jpg](https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg)\n-\t将情感特征直接拼接在中间表示之后，输入解码器\n![i0ThvT.jpg](https://s1.ax1x.com/2018/10/20/i0ThvT.jpg)\n\n# 结果\n-\t结果由ROUGE-F1值形式记录，情感语料下各种方法对比\n![i0T5KU.png](https://s1.ax1x.com/2018/10/20/i0T5KU.png)\n-\t普通语料下情感融合方案对比\n![i0Tb5R.png](https://s1.ax1x.com/2018/10/20/i0Tb5R.png)\n-\t情感分类准确率，作为参考，之前训练的情感分类器准确率为74%\n![i0Tob4.png](https://s1.ax1x.com/2018/10/20/i0Tob4.png)\n-\t因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果\n![i0T7VJ.png](https://s1.ax1x.com/2018/10/20/i0T7VJ.png)\n\n# 问题\n-\tunk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值\n![i0THa9.png](https://s1.ax1x.com/2018/10/20/i0THa9.png)\n-\t语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“...... 周三...... 黄金价格上涨”和“......周四......黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：\n\t-\t不去重：保留原文本，不去重，进行训练和测试\n\t-\t去重：删除语料中所有重复文摘对应的的短文本-短文摘对。\n\t-\t训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。\n\t-\t测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。\n\t重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。\n![i0TLP1.png](https://s1.ax1x.com/2018/10/20/i0TLP1.png)\n\n# 实现环境\n-\t这里是github地址：-\t[Abstract_Summarization_Tensorflow](https://github.com/thinkwee/Abstract_Summarization_Tensorflow)\n-\tUbuntu 16.04\n-\tTensorflow 1.6\n-\tCUDA 9.0\n-\tCudnn 7.1.2\n-\tGigawords数据集，训练了部分数据，约30万\n-\tGTX1066，训练时间3到4个小时\n\n# 参考文献\n![i0TX26.jpg](https://s1.ax1x.com/2018/10/20/i0TX26.jpg)\n![i0TO8x.jpg](https://s1.ax1x.com/2018/10/20/i0TO8x.jpg)\n\n","slug":"seq2seq-summarization","published":1,"updated":"2019-07-22T03:45:23.338Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vay004eq8t51ugdr1g9","content":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TGHH.png\" alt=\"i0TGHH.png\"></p>\n<h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul>\n<li>自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。</li>\n<li>自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TYEd.jpg\" alt=\"i0TYEd.jpg\"></p>\n<h1 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h1><h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2><ul>\n<li>循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。</li>\n<li>不展开时形式如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TtUA.jpg\" alt=\"i0TtUA.jpg\"></li>\n<li>按时间步展开之后：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TN4I.jpg\" alt=\"i0TN4I.jpg\"></li>\n</ul>\n<h2 id=\"LSTM和GRU\"><a href=\"#LSTM和GRU\" class=\"headerlink\" title=\"LSTM和GRU\"></a>LSTM和GRU</h2><ul>\n<li>循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。</li>\n<li>LSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TaCt.jpg\" alt=\"i0TaCt.jpg\"></li>\n<li>GRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Td8P.jpg\" alt=\"i0Td8P.jpg\"><h2 id=\"词嵌入\"><a href=\"#词嵌入\" class=\"headerlink\" title=\"词嵌入\"></a>词嵌入</h2></li>\n<li>深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。</li>\n<li>词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维<br>度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。</li>\n<li>下图展示了word2vec中的skipgram模型：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Twgf.jpg\" alt=\"i0Twgf.jpg\"></li>\n<li>得到的词嵌入矩阵如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T0v8.jpg\" alt=\"i0T0v8.jpg\"></li>\n<li>Mikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。</li>\n<li>Word2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，</li>\n<li>而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。</li>\n</ul>\n<h2 id=\"注意力\"><a href=\"#注意力\" class=\"headerlink\" title=\"注意力\"></a>注意力</h2><ul>\n<li>在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TDKS.jpg\" alt=\"i0TDKS.jpg\"></li>\n</ul>\n<h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul>\n<li>seq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TrDg.jpg\" alt=\"i0TrDg.jpg\"></li>\n<li>对于序列到序列模型的一些个人理解：<ul>\n<li>（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。</li>\n<li>（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。</li>\n<li>（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"序列损失\"><a href=\"#序列损失\" class=\"headerlink\" title=\"序列损失\"></a>序列损失</h2><ul>\n<li>解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。</li>\n</ul>\n<h1 id=\"基本模型\"><a href=\"#基本模型\" class=\"headerlink\" title=\"基本模型\"></a>基本模型</h1><ul>\n<li>预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg\" alt=\"i0TsbQ.jpg\"></li>\n<li>训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg\" alt=\"i0T6Ej.jpg\"></li>\n<li>特征输入编码器得到中间表示<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tg5n.png\" alt=\"i0Tg5n.png\"></li>\n<li>拿到中间表示和输出文摘(相当于label)，输入解码器进行解码<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TIrF.png\" alt=\"i0TIrF.png\"></li>\n<li>加入注意力机制后完整的序列到序列模型结构如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TRCq.jpg\" alt=\"i0TRCq.jpg\"></li>\n</ul>\n<h1 id=\"情感融合机制\"><a href=\"#情感融合机制\" class=\"headerlink\" title=\"情感融合机制\"></a>情感融合机制</h1><ul>\n<li>情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。</li>\n<li>先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TW80.jpg\" alt=\"i0TW80.jpg\"></li>\n<li>查找词典得到情感向量（即情感特征）<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg\" alt=\"i0Tf2V.jpg\"></li>\n<li>将情感特征直接拼接在中间表示之后，输入解码器<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ThvT.jpg\" alt=\"i0ThvT.jpg\"></li>\n</ul>\n<h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul>\n<li>结果由ROUGE-F1值形式记录，情感语料下各种方法对比<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T5KU.png\" alt=\"i0T5KU.png\"></li>\n<li>普通语料下情感融合方案对比<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tb5R.png\" alt=\"i0Tb5R.png\"></li>\n<li>情感分类准确率，作为参考，之前训练的情感分类器准确率为74%<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tob4.png\" alt=\"i0Tob4.png\"></li>\n<li>因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T7VJ.png\" alt=\"i0T7VJ.png\"></li>\n</ul>\n<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><ul>\n<li>unk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值<br><img src=\"https://s1.ax1x.com/2018/10/20/i0THa9.png\" alt=\"i0THa9.png\"></li>\n<li>语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“…… 周三…… 黄金价格上涨”和“……周四……黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：<ul>\n<li>不去重：保留原文本，不去重，进行训练和测试</li>\n<li>去重：删除语料中所有重复文摘对应的的短文本-短文摘对。</li>\n<li>训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。</li>\n<li>测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。<br>重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TLP1.png\" alt=\"i0TLP1.png\"></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"实现环境\"><a href=\"#实现环境\" class=\"headerlink\" title=\"实现环境\"></a>实现环境</h1><ul>\n<li>这里是github地址：-    <a href=\"https://github.com/thinkwee/Abstract_Summarization_Tensorflow\" target=\"_blank\" rel=\"noopener\">Abstract_Summarization_Tensorflow</a></li>\n<li>Ubuntu 16.04</li>\n<li>Tensorflow 1.6</li>\n<li>CUDA 9.0</li>\n<li>Cudnn 7.1.2</li>\n<li>Gigawords数据集，训练了部分数据，约30万</li>\n<li>GTX1066，训练时间3到4个小时</li>\n</ul>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0TX26.jpg\" alt=\"i0TX26.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TO8x.jpg\" alt=\"i0TO8x.jpg\"></p>\n","site":{"data":{}},"excerpt":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TGHH.png\" alt=\"i0TGHH.png\"></p>\n<h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul>\n<li>自动文摘是一类自然语言处理（Natural Language Processin，NLP）任务。对于一段较长的文本，产生一段能够覆盖原文核心意义的短文本，这段短文本就是原文本的摘要。自动文摘技术是指在计算机上构建数学模型，将长文本输入模型，通过计算之后模型能自动生成短摘要。根据需要产生摘要的语料规模和摘要的规模，可以将摘要分为多文本摘要、长文摘要、短文摘要。本文主要研究的是短文摘要：对于一句或者几句文本，生成一句短摘要，概括原文关键信息，且流畅可读，尽量达到人类作者撰写摘要的水平。</li>\n<li>自动文摘分抽取式和生成式，前者是抽取原文句子构成文摘，后者是通过深度学习模型逐字生成文摘。本文主要研究生成文摘，并将问题抽象成对一个平均长度为28词的长句生成一个平均长度为8词的短句。</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0TYEd.jpg\" alt=\"i0TYEd.jpg\"></p>\n<h1 id=\"预备知识\"><a href=\"#预备知识\" class=\"headerlink\" title=\"预备知识\"></a>预备知识</h1><h2 id=\"循环神经网络\"><a href=\"#循环神经网络\" class=\"headerlink\" title=\"循环神经网络\"></a>循环神经网络</h2><ul>\n<li>循环神经网络（Recurrent Neural Network，RNN），是神经网络的一种变形，能够有效处理序列型数据，其所有的隐藏层共享参数，每一隐藏层不仅依赖当前时刻的输入还依赖上一时刻的隐藏层状态，数据流并不是如传统神经网络那样在网络层之间传播，而是作为状态在自身网络中循环传递。</li>\n<li>不展开时形式如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TtUA.jpg\" alt=\"i0TtUA.jpg\"></li>\n<li>按时间步展开之后：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TN4I.jpg\" alt=\"i0TN4I.jpg\"></li>\n</ul>\n<h2 id=\"LSTM和GRU\"><a href=\"#LSTM和GRU\" class=\"headerlink\" title=\"LSTM和GRU\"></a>LSTM和GRU</h2><ul>\n<li>循环神经网络能够有效捕捉序列数据的顺序信息且能够构造很深的神经网络而不产生大量需要学习的参数；但也因为参数共享，梯度通过时间步进行链式求导时相当于进行矩阵幂运算，若参数矩阵的特征值过小会造成梯度弥散，特征值过大会造成梯度爆炸，影响反向传播过程，即RNN的长期依赖问题。在处理长序列数据时长期依赖问题会导致序列的长期记忆信息丢失，虽然人们通过引入梯度截断和跳跃链接技术试图缓解此问题，但效果并不显著，直到长短期记忆神经网络和门控循环神经网络作为 RNN 的扩展形式的出现，有效地解决了这个问题。</li>\n<li>LSTM即长短期记忆神经网络。为了捕捉长期记忆信息，LSTM在其节点中引入了门控单元，作为网络参数的一部分参与训练。门控单元控制了当前隐藏层节点记忆（遗忘）过去信息，接受当前输入新记忆的程度。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TaCt.jpg\" alt=\"i0TaCt.jpg\"></li>\n<li>GRU即门控神经网络，与LSTM不同的是，GRU 将遗忘门和输入门整合为重置门，遗忘门的门控值和输入门的门控值和为 1，因此 GRU 在 LSTM 的基础上简化了参数，从而使得模型能够更快得收敛。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Td8P.jpg\" alt=\"i0Td8P.jpg\"><h2 id=\"词嵌入\"><a href=\"#词嵌入\" class=\"headerlink\" title=\"词嵌入\"></a>词嵌入</h2></li>\n<li>深度学习的一大好处就是能自动学习特征，在自然语言处理中，我们专门用word2vec之类的技术学习词语的特征表示，即词嵌入。</li>\n<li>词向量（Word Vector）表示，又称词嵌入（Word Embedding），指以连续值向量形式表示（Distributed Representation）词语，而不是用离散的方式表示（One-hot Representation）。在传统的离散表示中，一个词用一个长度为 V 的向量表示，其中 V 是词典大小。向量中只有一个元素为 1，其余元素为 0，元素 1的位置代表这个词在词典中的下标。用离散的方式存储单词效率低下，且向量无法反映单词的语义语法特征，而词向量可以解决以上问题。词向量将向量的维<br>度从 V 降低到 $\\sqrt[k] V$（一般 k 取 4），每个元素的值不再是 1 和 0，而是连续值。词向量是建立神经网络语言模型 (Neural Network Language Model，NNLM)对语料进行监督学习得到的副产物，此模型的提出基于一个语言学上的假设：具有相近语义的词有相似的上下文，即 NNLM 模型能够在给定上下文环境的条件下求出相应中心词。</li>\n<li>下图展示了word2vec中的skipgram模型：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Twgf.jpg\" alt=\"i0Twgf.jpg\"></li>\n<li>得到的词嵌入矩阵如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T0v8.jpg\" alt=\"i0T0v8.jpg\"></li>\n<li>Mikolov等人在NNLM基础上提出了 Word2Vec 模型，此模型进行监督学习的输入输出分别为中心词与上下文（即 Skip Gram 模型) 或者上下文与中心词（即 CBOW 模型）。两种方式均能有效训练出高质量的词向量，但是 CBOW 模型是根据上下文求中心词，训练速度快，适合在大语料上训练；Skip Gram 模型能充分利用训练语料，其本身含义为“跳跃的语法模型”，不仅使用相邻词构成中心词的上下文环境，隔一个词的跳跃词也构成上下文环境，以图 2-1 为例，语料中的一个中心词的上下文包括四个词，假如语料中有 Wtotal 个词语，则 Skip Gram 模型能计算 4 · Wtotal 次损失并进行反向传播学习，对于语料的学习次数是 CBOW 模型的四倍，因此该模型适合于充分利用小语料训练词向量。</li>\n<li>Word2Vec 模型训练完成时，输入层与隐藏层之间的权重矩阵即词嵌入矩阵（Word Embedding Matrix）。离散表示单词的向量与矩阵相乘即得到词向量，这项操作实际上等效于在词嵌入矩阵中查找对应词向量（Embedding Look Up）。词向量能够有效表示词的语义关系，其本质是为机器学习模型提供了一种提取文本信息特征的方法，方便将单词数值化输入模型进行处理。传统的语言模型训练词向量在输出 Softmax 层开销太大，</li>\n<li>而 Word2Vec 模型采用了分层 Softmax（Hierarchical Softmax）和噪声对比估计（Noise Contrastive Estimation）两种技术，大大加速了训练，使得在自然语言处理中使用大规模语料训练出的优质词向量成为可能。</li>\n</ul>\n<h2 id=\"注意力\"><a href=\"#注意力\" class=\"headerlink\" title=\"注意力\"></a>注意力</h2><ul>\n<li>在 NLP 任务中注意力机制最早应用于机器翻译，即引入一个权重矩阵代表编码器序列中各个元素对解码器生成的每一个词的贡献程度。在实际实现时注意力机制会生成一个注意力权重，对各个编码器元素的隐藏层状态进行注意力加权生成中间表示，而不是简单的采用最后一个元素的隐藏层状态。最简单的注意力即解码器对编码器的注意力，分为全局和局部注意力。全局注意力生成对整个编码器序列的注意力权重，而局部注意力先训练出一个注意力对齐位置，再对此位置附近取一个窗口，仅对窗口内的序列加权，使得注意力更加精确、集中。注意力机制带来的一个副产品即词与词之间的对齐Alignment）。在机器翻译中，对齐关系可以理解为词到翻译词的关联程度。在自动文摘中采用注意力机制，可以有效缓解长序列经过编码器编码成中间表示时的信息损失问题。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TDKS.jpg\" alt=\"i0TDKS.jpg\"></li>\n</ul>\n<h2 id=\"序列到序列\"><a href=\"#序列到序列\" class=\"headerlink\" title=\"序列到序列\"></a>序列到序列</h2><ul>\n<li>seq2seq模型，即使用一个RNN作为编码器，编码输入的序列数据得到中间语义表示，再利用另外一个RNN作为解码器，利用中间语义表示得到序列化的输出。广义的序列到序列以及端到端学习不一定使用RNN，也可以用CNN或者纯注意力机制。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TrDg.jpg\" alt=\"i0TrDg.jpg\"></li>\n<li>对于序列到序列模型的一些个人理解：<ul>\n<li>（信息论）如果不考虑编码器和解码器的具体实现形式，只认为编码器可以将序列数据转化为中间表示，解码器可以将中间表示转化为序列数据，则整个序列到序列模型相当于对文摘信息的一次编码解码，由于中间表示的维度远小于编码器的总维度，因此这种编码是有损编码。序列到序列模型要使得有损编码的结果是提取原文中的文摘信息，因此训练网络的目标是让损失的部分为文摘不需要的冗余信息。解码器相当于编码器的逆过程，从包含文摘信息的中间表示中还原出文摘的序列形式数据。</li>\n<li>（学习与应用）将整个模型比作人类的大脑，则编码器的隐藏层状态相当于大脑内存储的知识，解码器则是利用这些知识解决问题，即编码器是一个学习过程而解码器是一个应用过程。这种比喻能够形象地解释后来对序列到序列模型的各种改进：时间步上的学习对应现实时间线上的学习，越早学习的知识在大脑中越容易忘记（靠前的时间步隐藏层信息传递到最终步时信息丢失严重），且大脑的容量一定（中间表示的信息存储容量一定），因此学习时我们选择性记忆和忘记信息（LSTM 和 GRU 的应用），并且在记忆的过程中划重点（注意力机制），甚至还可以借助向外界环境查询信息，不完全靠自己的记忆解决问题（记忆网络）。</li>\n<li>（循环神经网络）完全从数据流和网络结构的角度看，整个序列到序列模型可以看作一个长的、限定了某些时间步输入输出的 RNN，以图 3-1 为例，模型就是一个具有 8 个时间步的 RNN，前 5 时间步没有输出，后 3 个时间步将输出传递到下一时间步指导状态变化。编码器和解码器联合训练即相当于对这个大型RNN 进行训练。这种只在部分节点进行输入输出的 RNN 从结构上就适合处理序列到序列任务。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"序列损失\"><a href=\"#序列损失\" class=\"headerlink\" title=\"序列损失\"></a>序列损失</h2><ul>\n<li>解码器每一步解码出来的实际上时词典的概率分布，取最大概率的词输出（或者做束搜索），损失是每一步的概率和这一步标准词语的01分布做交叉熵，求和再平均。实际上还要应用一个mask来解决句子长度不一定的问题。</li>\n</ul>\n<h1 id=\"基本模型\"><a href=\"#基本模型\" class=\"headerlink\" title=\"基本模型\"></a>基本模型</h1><ul>\n<li>预处理：应用序列到序列模型需要对数据进行一定的预处理，除了常用的去停用词加UNK，以及padding之外，还需要设计解码的开始与结束符号，如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TsbQ.jpg\" alt=\"i0TsbQ.jpg\"></li>\n<li>训练好词嵌入之后，对输入做一个embedding lookup的操作得到特征<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T6Ej.jpg\" alt=\"i0T6Ej.jpg\"></li>\n<li>特征输入编码器得到中间表示<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tg5n.png\" alt=\"i0Tg5n.png\"></li>\n<li>拿到中间表示和输出文摘(相当于label)，输入解码器进行解码<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TIrF.png\" alt=\"i0TIrF.png\"></li>\n<li>加入注意力机制后完整的序列到序列模型结构如下：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TRCq.jpg\" alt=\"i0TRCq.jpg\"></li>\n</ul>\n<h1 id=\"情感融合机制\"><a href=\"#情感融合机制\" class=\"headerlink\" title=\"情感融合机制\"></a>情感融合机制</h1><ul>\n<li>情感机制主要是为文本补充了情感特征，通过查找情感词典的方式手动构造了一个六维的特征，未来希望能够用自动构造特征的方式来进行这方面工作。</li>\n<li>先训练情感分类器，对原语料进行了筛选形成情感语料，在情感语料和普通语料上都测试了模型<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TW80.jpg\" alt=\"i0TW80.jpg\"></li>\n<li>查找词典得到情感向量（即情感特征）<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tf2V.jpg\" alt=\"i0Tf2V.jpg\"></li>\n<li>将情感特征直接拼接在中间表示之后，输入解码器<br><img src=\"https://s1.ax1x.com/2018/10/20/i0ThvT.jpg\" alt=\"i0ThvT.jpg\"></li>\n</ul>\n<h1 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h1><ul>\n<li>结果由ROUGE-F1值形式记录，情感语料下各种方法对比<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T5KU.png\" alt=\"i0T5KU.png\"></li>\n<li>普通语料下情感融合方案对比<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tb5R.png\" alt=\"i0Tb5R.png\"></li>\n<li>情感分类准确率，作为参考，之前训练的情感分类器准确率为74%<br><img src=\"https://s1.ax1x.com/2018/10/20/i0Tob4.png\" alt=\"i0Tob4.png\"></li>\n<li>因为是大语料小batch训练，只训练了十次迭代，各个迭代的测试集效果<br><img src=\"https://s1.ax1x.com/2018/10/20/i0T7VJ.png\" alt=\"i0T7VJ.png\"></li>\n</ul>\n<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><ul>\n<li>unk替换问题：在很多文献中都提到了使用指针开关技术解决生成文摘中的罕见词（unk)，即从原文中挑选词语替换掉文摘中的unk，但是由于ROUGE评测的是词的共现程度，因此只要替换了原文的词，哪怕位置、词语不对，都有可能是ROUGE值上升，造成评测结果偏高，本文设计了对比试验，发现即便是没有采用任何机制的随机替换都能提高ROUGE值<br><img src=\"https://s1.ax1x.com/2018/10/20/i0THa9.png\" alt=\"i0THa9.png\"></li>\n<li>语料重复问题：在检查语料时我们发现了大量短文本不同但短文摘相同的语料，重复的语料来自于对于相同事件的不同描述，或者一些功能性的文字，翻译过来例如“…… 周三…… 黄金价格上涨”和“……周四……黄金价格上涨”均生成相同的文摘“黄金价格上涨”。短文摘重复的语料会导致模型在解码端学到一些不应该固化的短语，而如果训练集和测试集中有重复的相同的文摘，则固化的文摘反而会导致生成文摘正确率虚高。对于这类文本，本文进行了四组实验：<ul>\n<li>不去重：保留原文本，不去重，进行训练和测试</li>\n<li>去重：删除语料中所有重复文摘对应的的短文本-短文摘对。</li>\n<li>训练去重：部分去重，只将训练语料中的重复数据删除，即学习好的模型已经没有受到重复文本的影响。</li>\n<li>测试去重：部分去重，只将测试集中与训练集重复的部分删除，学习好的模型受到了重复文本的影响，但测试集中没有与重复文本中相对应的数据。<br>重复的语料训练下 ROUGE-1 和 ROUGE-L 都突破了 30，远超正常训练的模型，去重之后便恢复到正常水平。两种部分去重的结果则分别表明：训练去重时，没有受重复语料影响的模型对于测试集中的重复数据没有明显的反应，与完全去重的普通模型近似；测试去重时，虽然模型受到了重复语料影响，但是测试集中没有重复的数据让其利用学习到的固化的文摘，因此结果也不会虚高，且由于学习到了不同短文对应相同文摘的模式，编码端实际上结构更加灵活，导致 ROUGE 指标高于训练去重的结果。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0TLP1.png\" alt=\"i0TLP1.png\"></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"实现环境\"><a href=\"#实现环境\" class=\"headerlink\" title=\"实现环境\"></a>实现环境</h1><ul>\n<li>这里是github地址：-    <a href=\"https://github.com/thinkwee/Abstract_Summarization_Tensorflow\" target=\"_blank\" rel=\"noopener\">Abstract_Summarization_Tensorflow</a></li>\n<li>Ubuntu 16.04</li>\n<li>Tensorflow 1.6</li>\n<li>CUDA 9.0</li>\n<li>Cudnn 7.1.2</li>\n<li>Gigawords数据集，训练了部分数据，约30万</li>\n<li>GTX1066，训练时间3到4个小时</li>\n</ul>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><img src=\"https://s1.ax1x.com/2018/10/20/i0TX26.jpg\" alt=\"i0TX26.jpg\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0TO8x.jpg\" alt=\"i0TO8x.jpg\"></p>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0TGHH.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"基于端到端模型的生成式自动文摘研究","path":"2018/07/04/seq2seq-summarization/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0TGHH.png","excerpt":"<p>本科毕业设计是做一个基于seq2seq的短句文摘模型，并设计了一种情感融合机制<br>现在对整个模型做一个简单的总结</p>","date":"2018-07-04T07:58:59.000Z","pv":0,"totalPV":0,"categories":"自然语言处理","tags":["abstractive summarization","machinelearning","nlp","seq2seq","rnn","lstm","gru"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"变分自编码器学习笔记","date":"2019-03-20T01:53:31.000Z","mathjax":true,"html":true,"_content":"***\n-\t变分自编码器学习笔记\n-\t参考文章：\n\t-\t[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)\n\t-\t[Daniel Daza ，The Variational Autoencoder](https://dfdazac.github.io/01-vae.html)\n\t-\t[苏神的VAE系列](https://spaces.ac.cn/tag/vae/)\n- 关于VAE，上面的原论文以及两篇博客已经讲的很清楚了，我写也就是复读转述，自己捋一遍，如果有人看到这篇博客，建议优先读这三个参考来源\n<!--more-->\n\n# 直接看网络结构\n-\t变分自编码器用了变分推断，但是因为其中的参数估计部分用的是神经网络的梯度下降方法，因此可以直接画出其网络结构——实际上我们称其为自编码器，也是因为其结构上和自编码器有许多相通之处，如果不从贝叶斯的角度出发，甚至可以将VAE直接看成一类特殊的自编码器。\n- \t以原论文的mnist实验为例，我们直接看VAE的网络结构，之后再一般化模型并解释细节：\n\t-\t整体和自编码器一样，一个encoder和一个decoder,目标是重构误差，获取有用的编码\n\t-\t然而变分自编码器不对输入直接编码，而是假定编码服从多维正态分布，encoder编码的是这个多维正态分布的均值和方差\n\t-\t也就是VAE假定编码很简单，就是服从正态分布，而我要训练出来并利用的是解码，这个解码器能从正态分布的采样中解码还原出输入，或者说，生成输出\n-\tmnist的输入为28\\*28，batch_size为128，假定隐层dim为200，参数dim为10，则整个网络为：\n1. 输入$x$[128,28\\*28]，过一个linear+ReLu，得到编码器隐层$h$[128,200]\n2. $h$分别过两个linear，得到正态分布的参数$\\mu _e$[128,10]，$\\log \\sigma _e$[128,10]\n3. 从一个标准多维正态分布中采样$\\epsilon \\sim N(0,I)$，得到$\\epsilon$[128,10]\n4. 组合通过网络得到的参数$\\mu _e$，$\\log \\sigma _e$到标准多维正态分布的采样值中，$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$[128,10]，$z$即编码\n5. encoder部分就此完成，接下来是decoder部分：编码$z$过linear+ReLu得到解码隐层状态h[128,200]\n6. 解码隐层状态h经过linear+sigmoid得到$\\mu _d$[128,28\\*28]，即解码后的输出\n7. 解码后输出$\\mu _d$与输入$x$计算伯努利交叉熵损失\n8. 此外还要加上一个类似正则项的损失$\\frac 12 \\sum _{i=1}^{10} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)$\n\n# 直接对网络分析\n-\t从网络可以看到，VAE相比AE最大的区别就是不直接对输入编码，而是将概率的思想引入了网络结构，对每一个输入单独构建一个满足多维正态分布的编码\n-\t这么做的一个好处是，编码可以做插值，实现生成图像连续的变化：原始的AE对于每一个确定输入有确定的编码，而VAE中的网络只负责生成确定的均值和方差，也就是确定的正态分布，实际的编码只是在这个确定的正态分布中采样得到，依然是不确定的。在训练过程中VAE是对一片区域进行训练，而不是点训练，因此得到的编码具有连续性，在区域中心点附近也能生成相似的图像，甚至可以在两类图像输入确定的两个编码区域之间插值，实现两种生成图像的平滑过渡。\n-\t步骤1，2，3，4实现的是编码器从一个$N(\\mu _e,\\sigma _e ^2)$的分布中采样得到编码，然而这里存在一个reparameterization的技巧，即：\n\t-\t本来应该是让编码器神经网络拟合一个满足$N(\\mu _e,\\sigma _e ^2)$的分布，再从分布中采样\n\t-\t但是采样得到的值无法进行反向传播\n\t-\t因此改成神经网络只拟合分布的参数，然后从一个简单的标准多维正态分布中采样，采样值经过拟合参数处理，即$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$，来达到$z$仿佛直接从$N(\\mu _e,\\sigma _e ^2)$采样得到的效果，而神经网络仅仅拟合参数，可以进行反向传播，采样在其中相当于做了一些指定权值的加权，参与进网络的训练\n-\t步骤5，6，7则是普通的解码，因为输出是28\\*28的黑白图像，因此直接解码成28\\*28的二值向量，与输入比对计算交叉熵\n-\t关键是8，这个正则项如何得到？\n\n# 正则项\n-\t这个正则项实际上是编码得到的正态分布和标准正态分布之间的KL散度，即（其中K是多维正态分布的维度，在上例中是10）：\n$$\nKL(N(\\mu _e,\\sigma _e ^2)||N(0,I)) =  \\frac 12 \\sum _{i=1}^{K} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)\n$$\n-\t也就是说，我们希望编码的正态分布接近标准正态分布，为什么？\n-\t这里就有很多种说法了：\n\t-\t第一种：我们希望的是对于不同类的输入，编码能编码到同一个大区域，即不同区域内部紧凑的同时，区域之间的距离也不应该太远，最好能体现图像特征上的距离，比如以mnist为例，4和9的图像比较近似，和0的图像差异比较大，则他们的编码区域之间的距离能反映相似的关系；或者说从0变到8的过程中中间状态会像9，那么9的编码区域能在0和8的编码区域之间最好。然而实际上是，编码器网络可能会学到这样的编码方法：对于不同类的输入，$\\mu$相差很大，它将不同类输入（准确的说是不近似的输入，这里是无监督学习，没有类别）的编码区域隔得很开。神经网络这么做是有道理的：让解码器方便区别不同的输入进行解码。这就与我们希望它编码成连续区域方便插值的初衷相悖，因此我们强制希望所学到的编码分布都近似标准正态分布，这样都在一个大区域中，当然也不能太近似，不然大家都一样，解码器负担太大，根本解码不出来区别，这就是前面的重构损失的作用。\n\t-\t第二种：VAE的效果相当于在标准的自编码器中加入了高斯噪声，使得decoder对噪声具有鲁棒性。KL散度的大小代表了噪声的强弱：KL散度小，噪声越贴近标准高斯噪声，即强度大；KL散度大，噪声强度就小，这里理解为噪声被同化了，而不是说方差变小了，因为噪声应该与输入信号无关，一直保持高斯噪声或者其他指定的分布，如果噪声变得和指定分布越来越远，和输入越来越相关，那其作为噪声的作用也就越来越小了。\n\t-\t第三种：也是最严谨的一种理解，这个KL散度是从变分推断的角度出发得到的，整个模型也是从贝叶斯框架推理得到的。其之所以有网络结构是因为作者用了神经网络来拟合参数，神经网络的超参、分布的指定也是该框架在mnist生成任务中的一种特例，毕竟原文叫自编码变分贝叶斯（一种方法），而不是变分自编码网络（一种结构）。接下来我们从原论文的角度来看看整个模型如何推导出来，并自然而然得到这个KL散度正则项。\n\n# 变分自编码贝叶斯\n-\t整个解码器部分我们可以看成一个生成模型，其概率图为：\n![AKu5FA.png](https://s2.ax1x.com/2019/03/20/AKu5FA.png)\n-\t$z$即编码，$\\theta$是我们希望得到的解码器参数，控制解码器从编码中解码出（生成出）$x$\n-\t现在问题回归到概率图模型的推断：已知观测变量x，怎么得到参数$\\theta$？\n-\t作者采取的思路并不是完全照搬[变分推断](https://thinkwee.top/2018/08/28/inference-algorithm/#more)，在VAE中也采用了$q$分布来近似后验分布$p(z|x)$，并将观测量的对数似然拆分成ELBO和KL(q||p(z|x))，不同的是变分推断中用EM的方式得到q，而在VAE中用神经网络的方式拟合q（神经网络输入为$z$，因此$q$本身也是后验分布$q(z|x)$。完整写下来：\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n= \\log \\frac{p(x,z|\\theta)}{q(z|x,\\phi)} - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= \\log p(x,z|\\theta) - \\log q(z|x,\\phi) - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= [ \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz ] + [- \\int _z \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} q(z|x,\\phi) dz ]\\\\\n$$\n- \t注意，我们实际希望得到的是使得观测量对数似然最大的参数$\\theta$和$\\phi$，而隐变量$z$可以在输入确定的情况下随模型得到。\n-\t可以看到，在观测量即对数似然确定的情况下，前一个中括号内即ELBO值越大，则后面的KL散度，即后验$q(z|x,\\phi)$分布和后验真实分布$p(z|x,\\theta)$越相近。这个后验分布，即已知$x$得到$z$实际上就是编码器，因此这个KL散度越小则编码器效果越好，既然如此我们就应该最大化ELBO，ELBO可以改写成：\n$$\nELBO = \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz \\\\\n= E_{q(z|x,\\phi)}[\\log p(x,z|\\theta)-\\log q(z|x,\\phi)] \\\\\n= E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]-KL(q(z|x,\\phi)||(p(z|\\theta))) \\\\\n$$\n-\t又出现了一个KL散度！这个KL散度是编码器编码出的隐变量后验分布和隐变量先验分布之间的KL散度。而前半部分，$p(x|z,\\theta)$，已知隐变量求出观测量的分布，实际上就是解码器。因此$\\phi$和$\\theta$分别对应编码器和解码器的参数，实际上即神经网络的参数。前者称为variational parameter，后者称为generative parameter\n-\t我们要使得这个ELBO最大，VAE就直接将其作为网络结构的目标函数，做梯度下降，分别对$\\theta$和$\\phi$求导。在这里前半部分$E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]$求期望，用的是蒙特卡罗方法，即从$z \\sim q(z|x,\\phi)$中采样多个$z$，再求均值求期望，这里用到了上面说到的reparameterization技巧。\n-\t此时整个概率图模型，加上推断部分，变成了\n[![AKuhod.png](https://s2.ax1x.com/2019/03/20/AKuhod.png)](https://imgchr.com/i/AKuhod)\n-\t流程：得到观测量x->通过reparameterization得到z的样本->将z的样本带入目标函数（ELBO）求导->梯度下降，更新参数$\\theta$和$\\phi$\n\n# 回到mnist\n-\t在mnist实验中，作者设置隐变量的先验、q分布、reparameterization中的基础分布$\\epsilon$、观测量的后验分布分别为：\n$$\np(z) = N(z|0,I) \\\\\nq(z|x,\\phi) = N(z|\\mu _e , diag(\\sigma _e)) \\\\\n\\epsilon \\sim N(0,I) \\\\\np(x|z,\\theta) = \\prod _{i=1}^D \\mu _{d_i}^{x_i} (1-\\mu _{d_i})^{1-x_i} \\\\\n$$\n-\t其中模型参数$\\phi = [\\mu_e , \\sigma _e]$,$\\theta=\\mu _d$通过神经网络学习得到\n-\t而目标函数ELBO的前半部分，求期望部分已经通过reparameterization完成，内部的\n$$\n\\log p(x|z,\\theta) = \\sum _{i=1}^D x_i \\log \\mu _{d_i} + (1-x_i) \\log (1- \\mu _{d_i}) \\\\\n$$\n-\t即伯努利交叉熵，在网络设计是最后一层增加sigmoid函数也就是为了输出的$mu_d$满足为概率。\n-\t目标函数ELBO的后半部分，即隐变量的后验q分布和先验p分布之间的KL散度，就成为了上面所说的正则项,使得近似分布靠近先验分布\n-\t整个模型既考虑了重构损失，也考虑了先验信息\n-\t[因此ELBO可以写成](https://dfdazac.github.io/01-vae.html)：\n$$\nELBO = -重构误差损失-正则惩罚\n$$\n\n# 效果\n-\t在mnist数据集上的重构效果\n[![AKufdH.png](https://s2.ax1x.com/2019/03/20/AKufdH.png)](https://imgchr.com/i/AKufdH)\n-\t对方差扰动得到的效果\n[![AKuWee.png](https://s2.ax1x.com/2019/03/20/AKuWee.png)](https://imgchr.com/i/AKuWee)\n-\t对均值扰动得到的效果\n[![AKu2LD.png](https://s2.ax1x.com/2019/03/20/AKu2LD.png)](https://imgchr.com/i/AKu2LD)\n-\t对4和9进行插值的结果\n[![AKuIJI.png](https://s2.ax1x.com/2019/03/20/AKuIJI.png)](https://imgchr.com/i/AKuIJI)","source":"_posts/vae.md","raw":"---\ntitle: 变分自编码器学习笔记\ndate: 2019-03-20 09:53:31\ncategories: 机器学习\ntags:\n  - vae\n  - math\n  -\tmcmc\nmathjax: true\nhtml: true\n---\n***\n-\t变分自编码器学习笔记\n-\t参考文章：\n\t-\t[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)\n\t-\t[Daniel Daza ，The Variational Autoencoder](https://dfdazac.github.io/01-vae.html)\n\t-\t[苏神的VAE系列](https://spaces.ac.cn/tag/vae/)\n- 关于VAE，上面的原论文以及两篇博客已经讲的很清楚了，我写也就是复读转述，自己捋一遍，如果有人看到这篇博客，建议优先读这三个参考来源\n<!--more-->\n\n# 直接看网络结构\n-\t变分自编码器用了变分推断，但是因为其中的参数估计部分用的是神经网络的梯度下降方法，因此可以直接画出其网络结构——实际上我们称其为自编码器，也是因为其结构上和自编码器有许多相通之处，如果不从贝叶斯的角度出发，甚至可以将VAE直接看成一类特殊的自编码器。\n- \t以原论文的mnist实验为例，我们直接看VAE的网络结构，之后再一般化模型并解释细节：\n\t-\t整体和自编码器一样，一个encoder和一个decoder,目标是重构误差，获取有用的编码\n\t-\t然而变分自编码器不对输入直接编码，而是假定编码服从多维正态分布，encoder编码的是这个多维正态分布的均值和方差\n\t-\t也就是VAE假定编码很简单，就是服从正态分布，而我要训练出来并利用的是解码，这个解码器能从正态分布的采样中解码还原出输入，或者说，生成输出\n-\tmnist的输入为28\\*28，batch_size为128，假定隐层dim为200，参数dim为10，则整个网络为：\n1. 输入$x$[128,28\\*28]，过一个linear+ReLu，得到编码器隐层$h$[128,200]\n2. $h$分别过两个linear，得到正态分布的参数$\\mu _e$[128,10]，$\\log \\sigma _e$[128,10]\n3. 从一个标准多维正态分布中采样$\\epsilon \\sim N(0,I)$，得到$\\epsilon$[128,10]\n4. 组合通过网络得到的参数$\\mu _e$，$\\log \\sigma _e$到标准多维正态分布的采样值中，$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$[128,10]，$z$即编码\n5. encoder部分就此完成，接下来是decoder部分：编码$z$过linear+ReLu得到解码隐层状态h[128,200]\n6. 解码隐层状态h经过linear+sigmoid得到$\\mu _d$[128,28\\*28]，即解码后的输出\n7. 解码后输出$\\mu _d$与输入$x$计算伯努利交叉熵损失\n8. 此外还要加上一个类似正则项的损失$\\frac 12 \\sum _{i=1}^{10} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)$\n\n# 直接对网络分析\n-\t从网络可以看到，VAE相比AE最大的区别就是不直接对输入编码，而是将概率的思想引入了网络结构，对每一个输入单独构建一个满足多维正态分布的编码\n-\t这么做的一个好处是，编码可以做插值，实现生成图像连续的变化：原始的AE对于每一个确定输入有确定的编码，而VAE中的网络只负责生成确定的均值和方差，也就是确定的正态分布，实际的编码只是在这个确定的正态分布中采样得到，依然是不确定的。在训练过程中VAE是对一片区域进行训练，而不是点训练，因此得到的编码具有连续性，在区域中心点附近也能生成相似的图像，甚至可以在两类图像输入确定的两个编码区域之间插值，实现两种生成图像的平滑过渡。\n-\t步骤1，2，3，4实现的是编码器从一个$N(\\mu _e,\\sigma _e ^2)$的分布中采样得到编码，然而这里存在一个reparameterization的技巧，即：\n\t-\t本来应该是让编码器神经网络拟合一个满足$N(\\mu _e,\\sigma _e ^2)$的分布，再从分布中采样\n\t-\t但是采样得到的值无法进行反向传播\n\t-\t因此改成神经网络只拟合分布的参数，然后从一个简单的标准多维正态分布中采样，采样值经过拟合参数处理，即$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$，来达到$z$仿佛直接从$N(\\mu _e,\\sigma _e ^2)$采样得到的效果，而神经网络仅仅拟合参数，可以进行反向传播，采样在其中相当于做了一些指定权值的加权，参与进网络的训练\n-\t步骤5，6，7则是普通的解码，因为输出是28\\*28的黑白图像，因此直接解码成28\\*28的二值向量，与输入比对计算交叉熵\n-\t关键是8，这个正则项如何得到？\n\n# 正则项\n-\t这个正则项实际上是编码得到的正态分布和标准正态分布之间的KL散度，即（其中K是多维正态分布的维度，在上例中是10）：\n$$\nKL(N(\\mu _e,\\sigma _e ^2)||N(0,I)) =  \\frac 12 \\sum _{i=1}^{K} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)\n$$\n-\t也就是说，我们希望编码的正态分布接近标准正态分布，为什么？\n-\t这里就有很多种说法了：\n\t-\t第一种：我们希望的是对于不同类的输入，编码能编码到同一个大区域，即不同区域内部紧凑的同时，区域之间的距离也不应该太远，最好能体现图像特征上的距离，比如以mnist为例，4和9的图像比较近似，和0的图像差异比较大，则他们的编码区域之间的距离能反映相似的关系；或者说从0变到8的过程中中间状态会像9，那么9的编码区域能在0和8的编码区域之间最好。然而实际上是，编码器网络可能会学到这样的编码方法：对于不同类的输入，$\\mu$相差很大，它将不同类输入（准确的说是不近似的输入，这里是无监督学习，没有类别）的编码区域隔得很开。神经网络这么做是有道理的：让解码器方便区别不同的输入进行解码。这就与我们希望它编码成连续区域方便插值的初衷相悖，因此我们强制希望所学到的编码分布都近似标准正态分布，这样都在一个大区域中，当然也不能太近似，不然大家都一样，解码器负担太大，根本解码不出来区别，这就是前面的重构损失的作用。\n\t-\t第二种：VAE的效果相当于在标准的自编码器中加入了高斯噪声，使得decoder对噪声具有鲁棒性。KL散度的大小代表了噪声的强弱：KL散度小，噪声越贴近标准高斯噪声，即强度大；KL散度大，噪声强度就小，这里理解为噪声被同化了，而不是说方差变小了，因为噪声应该与输入信号无关，一直保持高斯噪声或者其他指定的分布，如果噪声变得和指定分布越来越远，和输入越来越相关，那其作为噪声的作用也就越来越小了。\n\t-\t第三种：也是最严谨的一种理解，这个KL散度是从变分推断的角度出发得到的，整个模型也是从贝叶斯框架推理得到的。其之所以有网络结构是因为作者用了神经网络来拟合参数，神经网络的超参、分布的指定也是该框架在mnist生成任务中的一种特例，毕竟原文叫自编码变分贝叶斯（一种方法），而不是变分自编码网络（一种结构）。接下来我们从原论文的角度来看看整个模型如何推导出来，并自然而然得到这个KL散度正则项。\n\n# 变分自编码贝叶斯\n-\t整个解码器部分我们可以看成一个生成模型，其概率图为：\n![AKu5FA.png](https://s2.ax1x.com/2019/03/20/AKu5FA.png)\n-\t$z$即编码，$\\theta$是我们希望得到的解码器参数，控制解码器从编码中解码出（生成出）$x$\n-\t现在问题回归到概率图模型的推断：已知观测变量x，怎么得到参数$\\theta$？\n-\t作者采取的思路并不是完全照搬[变分推断](https://thinkwee.top/2018/08/28/inference-algorithm/#more)，在VAE中也采用了$q$分布来近似后验分布$p(z|x)$，并将观测量的对数似然拆分成ELBO和KL(q||p(z|x))，不同的是变分推断中用EM的方式得到q，而在VAE中用神经网络的方式拟合q（神经网络输入为$z$，因此$q$本身也是后验分布$q(z|x)$。完整写下来：\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n= \\log \\frac{p(x,z|\\theta)}{q(z|x,\\phi)} - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= \\log p(x,z|\\theta) - \\log q(z|x,\\phi) - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= [ \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz ] + [- \\int _z \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} q(z|x,\\phi) dz ]\\\\\n$$\n- \t注意，我们实际希望得到的是使得观测量对数似然最大的参数$\\theta$和$\\phi$，而隐变量$z$可以在输入确定的情况下随模型得到。\n-\t可以看到，在观测量即对数似然确定的情况下，前一个中括号内即ELBO值越大，则后面的KL散度，即后验$q(z|x,\\phi)$分布和后验真实分布$p(z|x,\\theta)$越相近。这个后验分布，即已知$x$得到$z$实际上就是编码器，因此这个KL散度越小则编码器效果越好，既然如此我们就应该最大化ELBO，ELBO可以改写成：\n$$\nELBO = \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz \\\\\n= E_{q(z|x,\\phi)}[\\log p(x,z|\\theta)-\\log q(z|x,\\phi)] \\\\\n= E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]-KL(q(z|x,\\phi)||(p(z|\\theta))) \\\\\n$$\n-\t又出现了一个KL散度！这个KL散度是编码器编码出的隐变量后验分布和隐变量先验分布之间的KL散度。而前半部分，$p(x|z,\\theta)$，已知隐变量求出观测量的分布，实际上就是解码器。因此$\\phi$和$\\theta$分别对应编码器和解码器的参数，实际上即神经网络的参数。前者称为variational parameter，后者称为generative parameter\n-\t我们要使得这个ELBO最大，VAE就直接将其作为网络结构的目标函数，做梯度下降，分别对$\\theta$和$\\phi$求导。在这里前半部分$E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]$求期望，用的是蒙特卡罗方法，即从$z \\sim q(z|x,\\phi)$中采样多个$z$，再求均值求期望，这里用到了上面说到的reparameterization技巧。\n-\t此时整个概率图模型，加上推断部分，变成了\n[![AKuhod.png](https://s2.ax1x.com/2019/03/20/AKuhod.png)](https://imgchr.com/i/AKuhod)\n-\t流程：得到观测量x->通过reparameterization得到z的样本->将z的样本带入目标函数（ELBO）求导->梯度下降，更新参数$\\theta$和$\\phi$\n\n# 回到mnist\n-\t在mnist实验中，作者设置隐变量的先验、q分布、reparameterization中的基础分布$\\epsilon$、观测量的后验分布分别为：\n$$\np(z) = N(z|0,I) \\\\\nq(z|x,\\phi) = N(z|\\mu _e , diag(\\sigma _e)) \\\\\n\\epsilon \\sim N(0,I) \\\\\np(x|z,\\theta) = \\prod _{i=1}^D \\mu _{d_i}^{x_i} (1-\\mu _{d_i})^{1-x_i} \\\\\n$$\n-\t其中模型参数$\\phi = [\\mu_e , \\sigma _e]$,$\\theta=\\mu _d$通过神经网络学习得到\n-\t而目标函数ELBO的前半部分，求期望部分已经通过reparameterization完成，内部的\n$$\n\\log p(x|z,\\theta) = \\sum _{i=1}^D x_i \\log \\mu _{d_i} + (1-x_i) \\log (1- \\mu _{d_i}) \\\\\n$$\n-\t即伯努利交叉熵，在网络设计是最后一层增加sigmoid函数也就是为了输出的$mu_d$满足为概率。\n-\t目标函数ELBO的后半部分，即隐变量的后验q分布和先验p分布之间的KL散度，就成为了上面所说的正则项,使得近似分布靠近先验分布\n-\t整个模型既考虑了重构损失，也考虑了先验信息\n-\t[因此ELBO可以写成](https://dfdazac.github.io/01-vae.html)：\n$$\nELBO = -重构误差损失-正则惩罚\n$$\n\n# 效果\n-\t在mnist数据集上的重构效果\n[![AKufdH.png](https://s2.ax1x.com/2019/03/20/AKufdH.png)](https://imgchr.com/i/AKufdH)\n-\t对方差扰动得到的效果\n[![AKuWee.png](https://s2.ax1x.com/2019/03/20/AKuWee.png)](https://imgchr.com/i/AKuWee)\n-\t对均值扰动得到的效果\n[![AKu2LD.png](https://s2.ax1x.com/2019/03/20/AKu2LD.png)](https://imgchr.com/i/AKu2LD)\n-\t对4和9进行插值的结果\n[![AKuIJI.png](https://s2.ax1x.com/2019/03/20/AKuIJI.png)](https://imgchr.com/i/AKuIJI)","slug":"vae","published":1,"updated":"2019-07-22T03:45:23.415Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vb4004jq8t5vzuld000","content":"<hr>\n<ul>\n<li>变分自编码器学习笔记</li>\n<li>参考文章：<ul>\n<li><a href=\"https://arxiv.org/pdf/1312.6114.pdf\" target=\"_blank\" rel=\"noopener\">Auto-Encoding Variational Bayes</a></li>\n<li><a href=\"https://dfdazac.github.io/01-vae.html\" target=\"_blank\" rel=\"noopener\">Daniel Daza ，The Variational Autoencoder</a></li>\n<li><a href=\"https://spaces.ac.cn/tag/vae/\" target=\"_blank\" rel=\"noopener\">苏神的VAE系列</a></li>\n</ul>\n</li>\n<li>关于VAE，上面的原论文以及两篇博客已经讲的很清楚了，我写也就是复读转述，自己捋一遍，如果有人看到这篇博客，建议优先读这三个参考来源<a id=\"more\"></a>\n</li>\n</ul>\n<h1 id=\"直接看网络结构\"><a href=\"#直接看网络结构\" class=\"headerlink\" title=\"直接看网络结构\"></a>直接看网络结构</h1><ul>\n<li>变分自编码器用了变分推断，但是因为其中的参数估计部分用的是神经网络的梯度下降方法，因此可以直接画出其网络结构——实际上我们称其为自编码器，也是因为其结构上和自编码器有许多相通之处，如果不从贝叶斯的角度出发，甚至可以将VAE直接看成一类特殊的自编码器。</li>\n<li>以原论文的mnist实验为例，我们直接看VAE的网络结构，之后再一般化模型并解释细节：<ul>\n<li>整体和自编码器一样，一个encoder和一个decoder,目标是重构误差，获取有用的编码</li>\n<li>然而变分自编码器不对输入直接编码，而是假定编码服从多维正态分布，encoder编码的是这个多维正态分布的均值和方差</li>\n<li>也就是VAE假定编码很简单，就是服从正态分布，而我要训练出来并利用的是解码，这个解码器能从正态分布的采样中解码还原出输入，或者说，生成输出</li>\n</ul>\n</li>\n<li>mnist的输入为28*28，batch_size为128，假定隐层dim为200，参数dim为10，则整个网络为：</li>\n</ul>\n<ol>\n<li>输入$x$[128,28*28]，过一个linear+ReLu，得到编码器隐层$h$[128,200]</li>\n<li>$h$分别过两个linear，得到正态分布的参数$\\mu _e$[128,10]，$\\log \\sigma _e$[128,10]</li>\n<li>从一个标准多维正态分布中采样$\\epsilon \\sim N(0,I)$，得到$\\epsilon$[128,10]</li>\n<li>组合通过网络得到的参数$\\mu _e$，$\\log \\sigma _e$到标准多维正态分布的采样值中，$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$[128,10]，$z$即编码</li>\n<li>encoder部分就此完成，接下来是decoder部分：编码$z$过linear+ReLu得到解码隐层状态h[128,200]</li>\n<li>解码隐层状态h经过linear+sigmoid得到$\\mu _d$[128,28*28]，即解码后的输出</li>\n<li>解码后输出$\\mu _d$与输入$x$计算伯努利交叉熵损失</li>\n<li>此外还要加上一个类似正则项的损失$\\frac 12 \\sum _{i=1}^{10} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)$</li>\n</ol>\n<h1 id=\"直接对网络分析\"><a href=\"#直接对网络分析\" class=\"headerlink\" title=\"直接对网络分析\"></a>直接对网络分析</h1><ul>\n<li>从网络可以看到，VAE相比AE最大的区别就是不直接对输入编码，而是将概率的思想引入了网络结构，对每一个输入单独构建一个满足多维正态分布的编码</li>\n<li>这么做的一个好处是，编码可以做插值，实现生成图像连续的变化：原始的AE对于每一个确定输入有确定的编码，而VAE中的网络只负责生成确定的均值和方差，也就是确定的正态分布，实际的编码只是在这个确定的正态分布中采样得到，依然是不确定的。在训练过程中VAE是对一片区域进行训练，而不是点训练，因此得到的编码具有连续性，在区域中心点附近也能生成相似的图像，甚至可以在两类图像输入确定的两个编码区域之间插值，实现两种生成图像的平滑过渡。</li>\n<li>步骤1，2，3，4实现的是编码器从一个$N(\\mu _e,\\sigma _e ^2)$的分布中采样得到编码，然而这里存在一个reparameterization的技巧，即：<ul>\n<li>本来应该是让编码器神经网络拟合一个满足$N(\\mu _e,\\sigma _e ^2)$的分布，再从分布中采样</li>\n<li>但是采样得到的值无法进行反向传播</li>\n<li>因此改成神经网络只拟合分布的参数，然后从一个简单的标准多维正态分布中采样，采样值经过拟合参数处理，即$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$，来达到$z$仿佛直接从$N(\\mu _e,\\sigma _e ^2)$采样得到的效果，而神经网络仅仅拟合参数，可以进行反向传播，采样在其中相当于做了一些指定权值的加权，参与进网络的训练</li>\n</ul>\n</li>\n<li>步骤5，6，7则是普通的解码，因为输出是28*28的黑白图像，因此直接解码成28*28的二值向量，与输入比对计算交叉熵</li>\n<li>关键是8，这个正则项如何得到？</li>\n</ul>\n<h1 id=\"正则项\"><a href=\"#正则项\" class=\"headerlink\" title=\"正则项\"></a>正则项</h1><ul>\n<li>这个正则项实际上是编码得到的正态分布和标准正态分布之间的KL散度，即（其中K是多维正态分布的维度，在上例中是10）：<script type=\"math/tex; mode=display\">\nKL(N(\\mu _e,\\sigma _e ^2)||N(0,I)) =  \\frac 12 \\sum _{i=1}^{K} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)</script></li>\n<li>也就是说，我们希望编码的正态分布接近标准正态分布，为什么？</li>\n<li>这里就有很多种说法了：<ul>\n<li>第一种：我们希望的是对于不同类的输入，编码能编码到同一个大区域，即不同区域内部紧凑的同时，区域之间的距离也不应该太远，最好能体现图像特征上的距离，比如以mnist为例，4和9的图像比较近似，和0的图像差异比较大，则他们的编码区域之间的距离能反映相似的关系；或者说从0变到8的过程中中间状态会像9，那么9的编码区域能在0和8的编码区域之间最好。然而实际上是，编码器网络可能会学到这样的编码方法：对于不同类的输入，$\\mu$相差很大，它将不同类输入（准确的说是不近似的输入，这里是无监督学习，没有类别）的编码区域隔得很开。神经网络这么做是有道理的：让解码器方便区别不同的输入进行解码。这就与我们希望它编码成连续区域方便插值的初衷相悖，因此我们强制希望所学到的编码分布都近似标准正态分布，这样都在一个大区域中，当然也不能太近似，不然大家都一样，解码器负担太大，根本解码不出来区别，这就是前面的重构损失的作用。</li>\n<li>第二种：VAE的效果相当于在标准的自编码器中加入了高斯噪声，使得decoder对噪声具有鲁棒性。KL散度的大小代表了噪声的强弱：KL散度小，噪声越贴近标准高斯噪声，即强度大；KL散度大，噪声强度就小，这里理解为噪声被同化了，而不是说方差变小了，因为噪声应该与输入信号无关，一直保持高斯噪声或者其他指定的分布，如果噪声变得和指定分布越来越远，和输入越来越相关，那其作为噪声的作用也就越来越小了。</li>\n<li>第三种：也是最严谨的一种理解，这个KL散度是从变分推断的角度出发得到的，整个模型也是从贝叶斯框架推理得到的。其之所以有网络结构是因为作者用了神经网络来拟合参数，神经网络的超参、分布的指定也是该框架在mnist生成任务中的一种特例，毕竟原文叫自编码变分贝叶斯（一种方法），而不是变分自编码网络（一种结构）。接下来我们从原论文的角度来看看整个模型如何推导出来，并自然而然得到这个KL散度正则项。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"变分自编码贝叶斯\"><a href=\"#变分自编码贝叶斯\" class=\"headerlink\" title=\"变分自编码贝叶斯\"></a>变分自编码贝叶斯</h1><ul>\n<li>整个解码器部分我们可以看成一个生成模型，其概率图为：<br><img src=\"https://s2.ax1x.com/2019/03/20/AKu5FA.png\" alt=\"AKu5FA.png\"></li>\n<li>$z$即编码，$\\theta$是我们希望得到的解码器参数，控制解码器从编码中解码出（生成出）$x$</li>\n<li>现在问题回归到概率图模型的推断：已知观测变量x，怎么得到参数$\\theta$？</li>\n<li>作者采取的思路并不是完全照搬<a href=\"https://thinkwee.top/2018/08/28/inference-algorithm/#more\">变分推断</a>，在VAE中也采用了$q$分布来近似后验分布$p(z|x)$，并将观测量的对数似然拆分成ELBO和KL(q||p(z|x))，不同的是变分推断中用EM的方式得到q，而在VAE中用神经网络的方式拟合q（神经网络输入为$z$，因此$q$本身也是后验分布$q(z|x)$。完整写下来：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n= \\log \\frac{p(x,z|\\theta)}{q(z|x,\\phi)} - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= \\log p(x,z|\\theta) - \\log q(z|x,\\phi) - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= [ \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz ] + [- \\int _z \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} q(z|x,\\phi) dz ]\\\\</script></li>\n<li>注意，我们实际希望得到的是使得观测量对数似然最大的参数$\\theta$和$\\phi$，而隐变量$z$可以在输入确定的情况下随模型得到。</li>\n<li>可以看到，在观测量即对数似然确定的情况下，前一个中括号内即ELBO值越大，则后面的KL散度，即后验$q(z|x,\\phi)$分布和后验真实分布$p(z|x,\\theta)$越相近。这个后验分布，即已知$x$得到$z$实际上就是编码器，因此这个KL散度越小则编码器效果越好，既然如此我们就应该最大化ELBO，ELBO可以改写成：<script type=\"math/tex; mode=display\">\nELBO = \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz \\\\\n= E_{q(z|x,\\phi)}[\\log p(x,z|\\theta)-\\log q(z|x,\\phi)] \\\\\n= E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]-KL(q(z|x,\\phi)||(p(z|\\theta))) \\\\</script></li>\n<li>又出现了一个KL散度！这个KL散度是编码器编码出的隐变量后验分布和隐变量先验分布之间的KL散度。而前半部分，$p(x|z,\\theta)$，已知隐变量求出观测量的分布，实际上就是解码器。因此$\\phi$和$\\theta$分别对应编码器和解码器的参数，实际上即神经网络的参数。前者称为variational parameter，后者称为generative parameter</li>\n<li>我们要使得这个ELBO最大，VAE就直接将其作为网络结构的目标函数，做梯度下降，分别对$\\theta$和$\\phi$求导。在这里前半部分$E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]$求期望，用的是蒙特卡罗方法，即从$z \\sim q(z|x,\\phi)$中采样多个$z$，再求均值求期望，这里用到了上面说到的reparameterization技巧。</li>\n<li>此时整个概率图模型，加上推断部分，变成了<br><a href=\"https://imgchr.com/i/AKuhod\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuhod.png\" alt=\"AKuhod.png\"></a></li>\n<li>流程：得到观测量x-&gt;通过reparameterization得到z的样本-&gt;将z的样本带入目标函数（ELBO）求导-&gt;梯度下降，更新参数$\\theta$和$\\phi$</li>\n</ul>\n<h1 id=\"回到mnist\"><a href=\"#回到mnist\" class=\"headerlink\" title=\"回到mnist\"></a>回到mnist</h1><ul>\n<li>在mnist实验中，作者设置隐变量的先验、q分布、reparameterization中的基础分布$\\epsilon$、观测量的后验分布分别为：<script type=\"math/tex; mode=display\">\np(z) = N(z|0,I) \\\\\nq(z|x,\\phi) = N(z|\\mu _e , diag(\\sigma _e)) \\\\\n\\epsilon \\sim N(0,I) \\\\\np(x|z,\\theta) = \\prod _{i=1}^D \\mu _{d_i}^{x_i} (1-\\mu _{d_i})^{1-x_i} \\\\</script></li>\n<li>其中模型参数$\\phi = [\\mu_e , \\sigma _e]$,$\\theta=\\mu _d$通过神经网络学习得到</li>\n<li>而目标函数ELBO的前半部分，求期望部分已经通过reparameterization完成，内部的<script type=\"math/tex; mode=display\">\n\\log p(x|z,\\theta) = \\sum _{i=1}^D x_i \\log \\mu _{d_i} + (1-x_i) \\log (1- \\mu _{d_i}) \\\\</script></li>\n<li>即伯努利交叉熵，在网络设计是最后一层增加sigmoid函数也就是为了输出的$mu_d$满足为概率。</li>\n<li>目标函数ELBO的后半部分，即隐变量的后验q分布和先验p分布之间的KL散度，就成为了上面所说的正则项,使得近似分布靠近先验分布</li>\n<li>整个模型既考虑了重构损失，也考虑了先验信息</li>\n<li><a href=\"https://dfdazac.github.io/01-vae.html\" target=\"_blank\" rel=\"noopener\">因此ELBO可以写成</a>：<script type=\"math/tex; mode=display\">\nELBO = -重构误差损失-正则惩罚</script></li>\n</ul>\n<h1 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h1><ul>\n<li>在mnist数据集上的重构效果<br><a href=\"https://imgchr.com/i/AKufdH\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKufdH.png\" alt=\"AKufdH.png\"></a></li>\n<li>对方差扰动得到的效果<br><a href=\"https://imgchr.com/i/AKuWee\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuWee.png\" alt=\"AKuWee.png\"></a></li>\n<li>对均值扰动得到的效果<br><a href=\"https://imgchr.com/i/AKu2LD\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKu2LD.png\" alt=\"AKu2LD.png\"></a></li>\n<li>对4和9进行插值的结果<br><a href=\"https://imgchr.com/i/AKuIJI\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuIJI.png\" alt=\"AKuIJI.png\"></a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<ul>\n<li>变分自编码器学习笔记</li>\n<li>参考文章：<ul>\n<li><a href=\"https://arxiv.org/pdf/1312.6114.pdf\" target=\"_blank\" rel=\"noopener\">Auto-Encoding Variational Bayes</a></li>\n<li><a href=\"https://dfdazac.github.io/01-vae.html\" target=\"_blank\" rel=\"noopener\">Daniel Daza ，The Variational Autoencoder</a></li>\n<li><a href=\"https://spaces.ac.cn/tag/vae/\" target=\"_blank\" rel=\"noopener\">苏神的VAE系列</a></li>\n</ul>\n</li>\n<li>关于VAE，上面的原论文以及两篇博客已经讲的很清楚了，我写也就是复读转述，自己捋一遍，如果有人看到这篇博客，建议优先读这三个参考来源</li></ul>","more":"\n\n<h1 id=\"直接看网络结构\"><a href=\"#直接看网络结构\" class=\"headerlink\" title=\"直接看网络结构\"></a>直接看网络结构</h1><ul>\n<li>变分自编码器用了变分推断，但是因为其中的参数估计部分用的是神经网络的梯度下降方法，因此可以直接画出其网络结构——实际上我们称其为自编码器，也是因为其结构上和自编码器有许多相通之处，如果不从贝叶斯的角度出发，甚至可以将VAE直接看成一类特殊的自编码器。</li>\n<li>以原论文的mnist实验为例，我们直接看VAE的网络结构，之后再一般化模型并解释细节：<ul>\n<li>整体和自编码器一样，一个encoder和一个decoder,目标是重构误差，获取有用的编码</li>\n<li>然而变分自编码器不对输入直接编码，而是假定编码服从多维正态分布，encoder编码的是这个多维正态分布的均值和方差</li>\n<li>也就是VAE假定编码很简单，就是服从正态分布，而我要训练出来并利用的是解码，这个解码器能从正态分布的采样中解码还原出输入，或者说，生成输出</li>\n</ul>\n</li>\n<li>mnist的输入为28*28，batch_size为128，假定隐层dim为200，参数dim为10，则整个网络为：</li>\n</ul>\n<ol>\n<li>输入$x$[128,28*28]，过一个linear+ReLu，得到编码器隐层$h$[128,200]</li>\n<li>$h$分别过两个linear，得到正态分布的参数$\\mu _e$[128,10]，$\\log \\sigma _e$[128,10]</li>\n<li>从一个标准多维正态分布中采样$\\epsilon \\sim N(0,I)$，得到$\\epsilon$[128,10]</li>\n<li>组合通过网络得到的参数$\\mu _e$，$\\log \\sigma _e$到标准多维正态分布的采样值中，$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$[128,10]，$z$即编码</li>\n<li>encoder部分就此完成，接下来是decoder部分：编码$z$过linear+ReLu得到解码隐层状态h[128,200]</li>\n<li>解码隐层状态h经过linear+sigmoid得到$\\mu _d$[128,28*28]，即解码后的输出</li>\n<li>解码后输出$\\mu _d$与输入$x$计算伯努利交叉熵损失</li>\n<li>此外还要加上一个类似正则项的损失$\\frac 12 \\sum _{i=1}^{10} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)$</li>\n</ol>\n<h1 id=\"直接对网络分析\"><a href=\"#直接对网络分析\" class=\"headerlink\" title=\"直接对网络分析\"></a>直接对网络分析</h1><ul>\n<li>从网络可以看到，VAE相比AE最大的区别就是不直接对输入编码，而是将概率的思想引入了网络结构，对每一个输入单独构建一个满足多维正态分布的编码</li>\n<li>这么做的一个好处是，编码可以做插值，实现生成图像连续的变化：原始的AE对于每一个确定输入有确定的编码，而VAE中的网络只负责生成确定的均值和方差，也就是确定的正态分布，实际的编码只是在这个确定的正态分布中采样得到，依然是不确定的。在训练过程中VAE是对一片区域进行训练，而不是点训练，因此得到的编码具有连续性，在区域中心点附近也能生成相似的图像，甚至可以在两类图像输入确定的两个编码区域之间插值，实现两种生成图像的平滑过渡。</li>\n<li>步骤1，2，3，4实现的是编码器从一个$N(\\mu _e,\\sigma _e ^2)$的分布中采样得到编码，然而这里存在一个reparameterization的技巧，即：<ul>\n<li>本来应该是让编码器神经网络拟合一个满足$N(\\mu _e,\\sigma _e ^2)$的分布，再从分布中采样</li>\n<li>但是采样得到的值无法进行反向传播</li>\n<li>因此改成神经网络只拟合分布的参数，然后从一个简单的标准多维正态分布中采样，采样值经过拟合参数处理，即$z = \\mu _e + \\sigma _e \\bigodot \\epsilon$，来达到$z$仿佛直接从$N(\\mu _e,\\sigma _e ^2)$采样得到的效果，而神经网络仅仅拟合参数，可以进行反向传播，采样在其中相当于做了一些指定权值的加权，参与进网络的训练</li>\n</ul>\n</li>\n<li>步骤5，6，7则是普通的解码，因为输出是28*28的黑白图像，因此直接解码成28*28的二值向量，与输入比对计算交叉熵</li>\n<li>关键是8，这个正则项如何得到？</li>\n</ul>\n<h1 id=\"正则项\"><a href=\"#正则项\" class=\"headerlink\" title=\"正则项\"></a>正则项</h1><ul>\n<li>这个正则项实际上是编码得到的正态分布和标准正态分布之间的KL散度，即（其中K是多维正态分布的维度，在上例中是10）：<script type=\"math/tex; mode=display\">\nKL(N(\\mu _e,\\sigma _e ^2)||N(0,I)) =  \\frac 12 \\sum _{i=1}^{K} (\\sigma _{ei}^2 + \\mu _{ei}^2 -log(\\sigma _{ei}^2) - 1)</script></li>\n<li>也就是说，我们希望编码的正态分布接近标准正态分布，为什么？</li>\n<li>这里就有很多种说法了：<ul>\n<li>第一种：我们希望的是对于不同类的输入，编码能编码到同一个大区域，即不同区域内部紧凑的同时，区域之间的距离也不应该太远，最好能体现图像特征上的距离，比如以mnist为例，4和9的图像比较近似，和0的图像差异比较大，则他们的编码区域之间的距离能反映相似的关系；或者说从0变到8的过程中中间状态会像9，那么9的编码区域能在0和8的编码区域之间最好。然而实际上是，编码器网络可能会学到这样的编码方法：对于不同类的输入，$\\mu$相差很大，它将不同类输入（准确的说是不近似的输入，这里是无监督学习，没有类别）的编码区域隔得很开。神经网络这么做是有道理的：让解码器方便区别不同的输入进行解码。这就与我们希望它编码成连续区域方便插值的初衷相悖，因此我们强制希望所学到的编码分布都近似标准正态分布，这样都在一个大区域中，当然也不能太近似，不然大家都一样，解码器负担太大，根本解码不出来区别，这就是前面的重构损失的作用。</li>\n<li>第二种：VAE的效果相当于在标准的自编码器中加入了高斯噪声，使得decoder对噪声具有鲁棒性。KL散度的大小代表了噪声的强弱：KL散度小，噪声越贴近标准高斯噪声，即强度大；KL散度大，噪声强度就小，这里理解为噪声被同化了，而不是说方差变小了，因为噪声应该与输入信号无关，一直保持高斯噪声或者其他指定的分布，如果噪声变得和指定分布越来越远，和输入越来越相关，那其作为噪声的作用也就越来越小了。</li>\n<li>第三种：也是最严谨的一种理解，这个KL散度是从变分推断的角度出发得到的，整个模型也是从贝叶斯框架推理得到的。其之所以有网络结构是因为作者用了神经网络来拟合参数，神经网络的超参、分布的指定也是该框架在mnist生成任务中的一种特例，毕竟原文叫自编码变分贝叶斯（一种方法），而不是变分自编码网络（一种结构）。接下来我们从原论文的角度来看看整个模型如何推导出来，并自然而然得到这个KL散度正则项。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"变分自编码贝叶斯\"><a href=\"#变分自编码贝叶斯\" class=\"headerlink\" title=\"变分自编码贝叶斯\"></a>变分自编码贝叶斯</h1><ul>\n<li>整个解码器部分我们可以看成一个生成模型，其概率图为：<br><img src=\"https://s2.ax1x.com/2019/03/20/AKu5FA.png\" alt=\"AKu5FA.png\"></li>\n<li>$z$即编码，$\\theta$是我们希望得到的解码器参数，控制解码器从编码中解码出（生成出）$x$</li>\n<li>现在问题回归到概率图模型的推断：已知观测变量x，怎么得到参数$\\theta$？</li>\n<li>作者采取的思路并不是完全照搬<a href=\"https://thinkwee.top/2018/08/28/inference-algorithm/#more\">变分推断</a>，在VAE中也采用了$q$分布来近似后验分布$p(z|x)$，并将观测量的对数似然拆分成ELBO和KL(q||p(z|x))，不同的是变分推断中用EM的方式得到q，而在VAE中用神经网络的方式拟合q（神经网络输入为$z$，因此$q$本身也是后验分布$q(z|x)$。完整写下来：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n= \\log \\frac{p(x,z|\\theta)}{q(z|x,\\phi)} - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= \\log p(x,z|\\theta) - \\log q(z|x,\\phi) - \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} \\\\\n= [ \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz ] + [- \\int _z \\log \\frac{p(z|x,\\theta)}{q(z|x,\\phi)} q(z|x,\\phi) dz ]\\\\</script></li>\n<li>注意，我们实际希望得到的是使得观测量对数似然最大的参数$\\theta$和$\\phi$，而隐变量$z$可以在输入确定的情况下随模型得到。</li>\n<li>可以看到，在观测量即对数似然确定的情况下，前一个中括号内即ELBO值越大，则后面的KL散度，即后验$q(z|x,\\phi)$分布和后验真实分布$p(z|x,\\theta)$越相近。这个后验分布，即已知$x$得到$z$实际上就是编码器，因此这个KL散度越小则编码器效果越好，既然如此我们就应该最大化ELBO，ELBO可以改写成：<script type=\"math/tex; mode=display\">\nELBO = \\int _z q(z|x,\\phi) \\log p(x,z|\\theta)dz - \\int _z q(z|x,\\phi) \\log q(z|x,\\phi)dz \\\\\n= E_{q(z|x,\\phi)}[\\log p(x,z|\\theta)-\\log q(z|x,\\phi)] \\\\\n= E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]-KL(q(z|x,\\phi)||(p(z|\\theta))) \\\\</script></li>\n<li>又出现了一个KL散度！这个KL散度是编码器编码出的隐变量后验分布和隐变量先验分布之间的KL散度。而前半部分，$p(x|z,\\theta)$，已知隐变量求出观测量的分布，实际上就是解码器。因此$\\phi$和$\\theta$分别对应编码器和解码器的参数，实际上即神经网络的参数。前者称为variational parameter，后者称为generative parameter</li>\n<li>我们要使得这个ELBO最大，VAE就直接将其作为网络结构的目标函数，做梯度下降，分别对$\\theta$和$\\phi$求导。在这里前半部分$E_{q(z|x,\\phi)}[\\log p(x|z,\\theta)]$求期望，用的是蒙特卡罗方法，即从$z \\sim q(z|x,\\phi)$中采样多个$z$，再求均值求期望，这里用到了上面说到的reparameterization技巧。</li>\n<li>此时整个概率图模型，加上推断部分，变成了<br><a href=\"https://imgchr.com/i/AKuhod\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuhod.png\" alt=\"AKuhod.png\"></a></li>\n<li>流程：得到观测量x-&gt;通过reparameterization得到z的样本-&gt;将z的样本带入目标函数（ELBO）求导-&gt;梯度下降，更新参数$\\theta$和$\\phi$</li>\n</ul>\n<h1 id=\"回到mnist\"><a href=\"#回到mnist\" class=\"headerlink\" title=\"回到mnist\"></a>回到mnist</h1><ul>\n<li>在mnist实验中，作者设置隐变量的先验、q分布、reparameterization中的基础分布$\\epsilon$、观测量的后验分布分别为：<script type=\"math/tex; mode=display\">\np(z) = N(z|0,I) \\\\\nq(z|x,\\phi) = N(z|\\mu _e , diag(\\sigma _e)) \\\\\n\\epsilon \\sim N(0,I) \\\\\np(x|z,\\theta) = \\prod _{i=1}^D \\mu _{d_i}^{x_i} (1-\\mu _{d_i})^{1-x_i} \\\\</script></li>\n<li>其中模型参数$\\phi = [\\mu_e , \\sigma _e]$,$\\theta=\\mu _d$通过神经网络学习得到</li>\n<li>而目标函数ELBO的前半部分，求期望部分已经通过reparameterization完成，内部的<script type=\"math/tex; mode=display\">\n\\log p(x|z,\\theta) = \\sum _{i=1}^D x_i \\log \\mu _{d_i} + (1-x_i) \\log (1- \\mu _{d_i}) \\\\</script></li>\n<li>即伯努利交叉熵，在网络设计是最后一层增加sigmoid函数也就是为了输出的$mu_d$满足为概率。</li>\n<li>目标函数ELBO的后半部分，即隐变量的后验q分布和先验p分布之间的KL散度，就成为了上面所说的正则项,使得近似分布靠近先验分布</li>\n<li>整个模型既考虑了重构损失，也考虑了先验信息</li>\n<li><a href=\"https://dfdazac.github.io/01-vae.html\" target=\"_blank\" rel=\"noopener\">因此ELBO可以写成</a>：<script type=\"math/tex; mode=display\">\nELBO = -重构误差损失-正则惩罚</script></li>\n</ul>\n<h1 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h1><ul>\n<li>在mnist数据集上的重构效果<br><a href=\"https://imgchr.com/i/AKufdH\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKufdH.png\" alt=\"AKufdH.png\"></a></li>\n<li>对方差扰动得到的效果<br><a href=\"https://imgchr.com/i/AKuWee\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuWee.png\" alt=\"AKuWee.png\"></a></li>\n<li>对均值扰动得到的效果<br><a href=\"https://imgchr.com/i/AKu2LD\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKu2LD.png\" alt=\"AKu2LD.png\"></a></li>\n<li>对4和9进行插值的结果<br><a href=\"https://imgchr.com/i/AKuIJI\" target=\"_blank\" rel=\"noopener\"><img src=\"https://s2.ax1x.com/2019/03/20/AKuIJI.png\" alt=\"AKuIJI.png\"></a></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s2.ax1x.com/2019/03/20/AKu5FA.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"变分自编码器学习笔记","path":"2019/03/20/vae/","eyeCatchImage":"https://s2.ax1x.com/2019/03/20/AKu5FA.png","excerpt":"<hr>\n<ul>\n<li>变分自编码器学习笔记</li>\n<li>参考文章：<ul>\n<li><a href=\"https://arxiv.org/pdf/1312.6114.pdf\" target=\"_blank\" rel=\"noopener\">Auto-Encoding Variational Bayes</a></li>\n<li><a href=\"https://dfdazac.github.io/01-vae.html\" target=\"_blank\" rel=\"noopener\">Daniel Daza ，The Variational Autoencoder</a></li>\n<li><a href=\"https://spaces.ac.cn/tag/vae/\" target=\"_blank\" rel=\"noopener\">苏神的VAE系列</a></li>\n</ul>\n</li>\n<li>关于VAE，上面的原论文以及两篇博客已经讲的很清楚了，我写也就是复读转述，自己捋一遍，如果有人看到这篇博客，建议优先读这三个参考来源</li></ul>","date":"2019-03-20T01:53:31.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","vae","mcmc"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"搭站总结","author":"Thinkwee","date":"2017-01-16T04:01:00.000Z","_content":"***\n\n\n\n一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器\n后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管\n而且本来官方说明也推荐用这个写博客，于是就开始试试\n大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站\nHexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository\n为了备份，我们将在repository中建立两个branch\n一个master用于让hexo上传静态网页文件\n一个hexo用于保存本地hexo项目\n下面分享一些经验和踩到的坑\n-\t\t2017.2.8更新md写作软件\n-\t\t2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)\n-\t\t2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)\n-\t\t2017.3.30更新置顶说明原文地址\n-\t\t2017.12.27更新异地恢复\n-\t\t2018.7.6更新一个比较全面的参考网址\n<!--more-->\n\n![i07MIs.png](https://s1.ax1x.com/2018/10/20/i07MIs.png)\n\n# 前提\n-\t安装好Node.js\n-\t安装好git\n-\t安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。\n-\t可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)\n\n# GitHub&Hexo初始化\n选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些\n最后的过程是这样的\n-\t注册github账号，这个就省略了\n-\t新建一个repository，命名必须为“你的账户名.github.io\"\n-\t对这个repository新建一个hexo分支并把hexo设置为默认分支\n-\tgit bash cd到你本地新建的一个文件夹（用于存放博客项目）\n-\t依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git\n-\t此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文\n-\t此时你的网址就是：https://你的账户名.github.io/\n-\t现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件\n\n\n\n**之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支**\n\n# Hexo配置\nHexo博客的目录格式如下\n\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n\n-\tconfig.yml在这里设置博客的总体参数，博客名，url，git什么的\n-\t_posts里面存放你的文章\n-\ttheme顾名思义，存放博客界面主题\n-\t在config.yml中关键配置以下几个参数\n *\ttitle: 博客网站标题\n *\tsubtitle: \t副标题\n *\tdescription: \t一句话简介\n *\tauthor: \t作者名\n *\tlanguage: zh-Hans\t语言根据你选择的主题看，在主题目录的language里查看支持哪些语言\n *\ttimezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区\n *\turl: https://你的用户名.github.io/\n *\ttype: git\n *\trepo: https://github.com/你的用户名/你的用户名.github.io.git\n *\t**branch: master**\n\n# 更新Hexo项目\n依次执行\n-\tgit add .  (检查所有文件是否更新)\n-\tgit commit -m \"更新报告\" (提交commit)\n-\tgit push origin hexo (上传更新到github)\n**第一次更新可能出现一些错误**\n-\t第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错\n-\tpermission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：https://www.github.com/你的用户名/你的用户名.github.io.git\n-\trefusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo --allow-unrelated-histories，之后再ush\n**所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的**\n\n\n# Hexo 写博客及更新网页\n主要命令如下\n-\thexo clean 清除缓存和静态文件\n-\thexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate\n-\thexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭\n-\thexo new\"文件名\" 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了\n-\thexo d或者hexo deploy，写好博客，clean&generate之后，部署博客到GitHub Pages上，就更新了博客网页\n**其中有几点要注意**\n-\t第一次新建目录后执行#npm install hexo-deployer-git --save安装git分发，否则上传不了\n-\thexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格\n-\t我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s\n\n# Hexo写作的几种选择(Windows)\n-\t**马克飞象**:这里推荐用马克飞象的chrome app版本，可以离线打开。**优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年**,它长这个样子：\n![i078zV.jpg](https://s1.ax1x.com/2018/10/20/i078zV.jpg)\n-\t**hexo admin插件**:安装hexo-admin插件，可视化管理博客，安装方法：\n```Github\nnpm install --save hexo-admin\nhexo server -d\nopen http://localhost:4000/admin/\n```\n*http://localhost:4000/admin/*就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，**优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好**,它长这个样子：\n![i073R0.jpg](https://s1.ax1x.com/2018/10/20/i073R0.jpg)\n-\t推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。\n-\t**Typora**:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：**Typora**。**优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。**他长这个样子：\n![i071Gq.jpg](https://s1.ax1x.com/2018/10/20/i071Gq.jpg)\n-\tMac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。\n-\t常备一个notepad++,出现什么问题没有notepad++解决不了的。\n\n\n\n# 主题设置\n-\t一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可\n-\t每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改\n\n# 写作语法\n-\t语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明\n\n# 插入图片\n-\t插入图片分本地和在线链接插入，推荐使用在线链接\n-\t我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。\n\n# 插入公式\n-\t用一对$$将公式围起来，语法支持latex\n-\t必须在文章首部中声明如mathjax: true\n-\t因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法\n-\t建议更换cdn以加速:\n```\nmathjax:\n  enable: true\n  per_page: true\n  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n  ```\n\n# 标签与分类\n-\t标签与分类的区别在于1篇文章归于一类，但可能有多个标签\n-\t比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签\n-\t分类需在首部加上categories: 分类名\n-\t标签需在首部加上tags: 标签名或者tags: [标签1,标签2....],**注意别打成中文逗号**\t\n\n# 其他美化\n-\t我用的是next主题，可以设置背景之类的，百度一下方法就能找到\n-\t更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network\n\n# 首页文章数量设置\n- 安装插件\n```Git\n\tnpm install --save hexo-generator-index\n\tnpm install --save hexo-generator-archive\n\tnpm install --save hexo-generator-tag\n```\n-\t在站点配置文件config.yml中添加如下字段\n```Markdown\n\tindex_generator:\n\tper_page: 5\n\n\tarchive_generator:\n\tper_page: 20\n\tyearly: true\n\tmonthly: true\n\n\ttag_generator:\n\tper_page: 10\n```\n-\tindex, archive及tag开头分表代表主页，归档页面和标签页面。\n\n# 添加评论功能\n-\t~~视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可~~\n-\t现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论\n\n# 优化插件\n-\thexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率\n\t[hexo-all-minifier](https://github.com/chenzhutian/hexo-all-minifier)\n\n# 置顶\n-\t感谢Netcan_Space提供解决方案，希望官方theme加入此功能：[添加Hexo置顶功能](http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/)\n\n# cnpm\n-\t使用淘宝镜像安装插件提速，详情百度cnpm安装\n\n# RSS\n-\t使用hexo-generator-feed使用rss:[hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed)\n\n# 异地恢复\n-\t最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下\n *\tclone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml\n *\t这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除\n *\t记得安装hexo-deployer-git\n *\tnext更新了，但是新功能的一些依赖还是需要看注释，自己npm install\n *\t发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作\n *\t上面那2张图已经过时了，可以去我的github里看博客的文件目录\n *\t有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱\n *  升级mathjax到2.7.5\n *  改用hexo-renderer-kramed，并修改inline.js转义\n\n# 这位大佬靠谱\n-\t网址在这：[HEXO建站备忘录](https://www.vincentqin.tech/posts/build-a-website-using-hexo/)\n-\t里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍[hexo-git-backup](https://github.com/coneycode/hexo-git-backup)\n\n","source":"_posts/setupmywebsite.md","raw":"title: 搭站总结\ntags:\n  - web\n  - hexo\n  - github\ncategories:\n  - 瞎折腾\nauthor: Thinkwee\ndate: 2017-01-16 12:01:00\n---\n***\n\n\n\n一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器\n后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管\n而且本来官方说明也推荐用这个写博客，于是就开始试试\n大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站\nHexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository\n为了备份，我们将在repository中建立两个branch\n一个master用于让hexo上传静态网页文件\n一个hexo用于保存本地hexo项目\n下面分享一些经验和踩到的坑\n-\t\t2017.2.8更新md写作软件\n-\t\t2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)\n-\t\t2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)\n-\t\t2017.3.30更新置顶说明原文地址\n-\t\t2017.12.27更新异地恢复\n-\t\t2018.7.6更新一个比较全面的参考网址\n<!--more-->\n\n![i07MIs.png](https://s1.ax1x.com/2018/10/20/i07MIs.png)\n\n# 前提\n-\t安装好Node.js\n-\t安装好git\n-\t安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。\n-\t可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)\n\n# GitHub&Hexo初始化\n选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些\n最后的过程是这样的\n-\t注册github账号，这个就省略了\n-\t新建一个repository，命名必须为“你的账户名.github.io\"\n-\t对这个repository新建一个hexo分支并把hexo设置为默认分支\n-\tgit bash cd到你本地新建的一个文件夹（用于存放博客项目）\n-\t依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git\n-\t此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文\n-\t此时你的网址就是：https://你的账户名.github.io/\n-\t现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件\n\n\n\n**之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支**\n\n# Hexo配置\nHexo博客的目录格式如下\n\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n\n-\tconfig.yml在这里设置博客的总体参数，博客名，url，git什么的\n-\t_posts里面存放你的文章\n-\ttheme顾名思义，存放博客界面主题\n-\t在config.yml中关键配置以下几个参数\n *\ttitle: 博客网站标题\n *\tsubtitle: \t副标题\n *\tdescription: \t一句话简介\n *\tauthor: \t作者名\n *\tlanguage: zh-Hans\t语言根据你选择的主题看，在主题目录的language里查看支持哪些语言\n *\ttimezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区\n *\turl: https://你的用户名.github.io/\n *\ttype: git\n *\trepo: https://github.com/你的用户名/你的用户名.github.io.git\n *\t**branch: master**\n\n# 更新Hexo项目\n依次执行\n-\tgit add .  (检查所有文件是否更新)\n-\tgit commit -m \"更新报告\" (提交commit)\n-\tgit push origin hexo (上传更新到github)\n**第一次更新可能出现一些错误**\n-\t第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错\n-\tpermission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：https://www.github.com/你的用户名/你的用户名.github.io.git\n-\trefusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo --allow-unrelated-histories，之后再ush\n**所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的**\n\n\n# Hexo 写博客及更新网页\n主要命令如下\n-\thexo clean 清除缓存和静态文件\n-\thexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate\n-\thexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭\n-\thexo new\"文件名\" 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了\n-\thexo d或者hexo deploy，写好博客，clean&generate之后，部署博客到GitHub Pages上，就更新了博客网页\n**其中有几点要注意**\n-\t第一次新建目录后执行#npm install hexo-deployer-git --save安装git分发，否则上传不了\n-\thexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格\n-\t我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s\n\n# Hexo写作的几种选择(Windows)\n-\t**马克飞象**:这里推荐用马克飞象的chrome app版本，可以离线打开。**优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年**,它长这个样子：\n![i078zV.jpg](https://s1.ax1x.com/2018/10/20/i078zV.jpg)\n-\t**hexo admin插件**:安装hexo-admin插件，可视化管理博客，安装方法：\n```Github\nnpm install --save hexo-admin\nhexo server -d\nopen http://localhost:4000/admin/\n```\n*http://localhost:4000/admin/*就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，**优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好**,它长这个样子：\n![i073R0.jpg](https://s1.ax1x.com/2018/10/20/i073R0.jpg)\n-\t推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。\n-\t**Typora**:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：**Typora**。**优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。**他长这个样子：\n![i071Gq.jpg](https://s1.ax1x.com/2018/10/20/i071Gq.jpg)\n-\tMac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。\n-\t常备一个notepad++,出现什么问题没有notepad++解决不了的。\n\n\n\n# 主题设置\n-\t一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可\n-\t每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改\n\n# 写作语法\n-\t语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明\n\n# 插入图片\n-\t插入图片分本地和在线链接插入，推荐使用在线链接\n-\t我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。\n\n# 插入公式\n-\t用一对$$将公式围起来，语法支持latex\n-\t必须在文章首部中声明如mathjax: true\n-\t因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法\n-\t建议更换cdn以加速:\n```\nmathjax:\n  enable: true\n  per_page: true\n  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\n  ```\n\n# 标签与分类\n-\t标签与分类的区别在于1篇文章归于一类，但可能有多个标签\n-\t比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签\n-\t分类需在首部加上categories: 分类名\n-\t标签需在首部加上tags: 标签名或者tags: [标签1,标签2....],**注意别打成中文逗号**\t\n\n# 其他美化\n-\t我用的是next主题，可以设置背景之类的，百度一下方法就能找到\n-\t更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network\n\n# 首页文章数量设置\n- 安装插件\n```Git\n\tnpm install --save hexo-generator-index\n\tnpm install --save hexo-generator-archive\n\tnpm install --save hexo-generator-tag\n```\n-\t在站点配置文件config.yml中添加如下字段\n```Markdown\n\tindex_generator:\n\tper_page: 5\n\n\tarchive_generator:\n\tper_page: 20\n\tyearly: true\n\tmonthly: true\n\n\ttag_generator:\n\tper_page: 10\n```\n-\tindex, archive及tag开头分表代表主页，归档页面和标签页面。\n\n# 添加评论功能\n-\t~~视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可~~\n-\t现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论\n\n# 优化插件\n-\thexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率\n\t[hexo-all-minifier](https://github.com/chenzhutian/hexo-all-minifier)\n\n# 置顶\n-\t感谢Netcan_Space提供解决方案，希望官方theme加入此功能：[添加Hexo置顶功能](http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/)\n\n# cnpm\n-\t使用淘宝镜像安装插件提速，详情百度cnpm安装\n\n# RSS\n-\t使用hexo-generator-feed使用rss:[hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed)\n\n# 异地恢复\n-\t最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下\n *\tclone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml\n *\t这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除\n *\t记得安装hexo-deployer-git\n *\tnext更新了，但是新功能的一些依赖还是需要看注释，自己npm install\n *\t发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作\n *\t上面那2张图已经过时了，可以去我的github里看博客的文件目录\n *\t有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱\n *  升级mathjax到2.7.5\n *  改用hexo-renderer-kramed，并修改inline.js转义\n\n# 这位大佬靠谱\n-\t网址在这：[HEXO建站备忘录](https://www.vincentqin.tech/posts/build-a-website-using-hexo/)\n-\t里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍[hexo-git-backup](https://github.com/coneycode/hexo-git-backup)\n\n","slug":"setupmywebsite","published":1,"updated":"2019-07-23T02:37:53.142Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vbe004mq8t5pszv4ess","content":"<hr>\n<p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p>\n<ul>\n<li>2017.2.8更新md写作软件</li>\n<li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li>\n<li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li>\n<li>2017.3.30更新置顶说明原文地址</li>\n<li>2017.12.27更新异地恢复</li>\n<li>2018.7.6更新一个比较全面的参考网址<a id=\"more\"></a>\n</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07MIs.png\" alt=\"i07MIs.png\"></p>\n<h1 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h1><ul>\n<li>安装好Node.js</li>\n<li>安装好git</li>\n<li>安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。</li>\n<li>可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)</li>\n</ul>\n<h1 id=\"GitHub-amp-Hexo初始化\"><a href=\"#GitHub-amp-Hexo初始化\" class=\"headerlink\" title=\"GitHub&amp;Hexo初始化\"></a>GitHub&amp;Hexo初始化</h1><p>选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些<br>最后的过程是这样的</p>\n<ul>\n<li>注册github账号，这个就省略了</li>\n<li>新建一个repository，命名必须为“你的账户名.github.io”</li>\n<li>对这个repository新建一个hexo分支并把hexo设置为默认分支</li>\n<li>git bash cd到你本地新建的一个文件夹（用于存放博客项目）</li>\n<li>依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git</li>\n<li>此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文</li>\n<li>此时你的网址就是：<a href=\"https://你的账户名.github.io/\" target=\"_blank\" rel=\"noopener\">https://你的账户名.github.io/</a></li>\n<li>现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件</li>\n</ul>\n<p><strong>之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支</strong></p>\n<h1 id=\"Hexo配置\"><a href=\"#Hexo配置\" class=\"headerlink\" title=\"Hexo配置\"></a>Hexo配置</h1><p>Hexo博客的目录格式如下</p>\n<p>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br>|   ├── _drafts<br>|   └── _posts<br>└── themes</p>\n<ul>\n<li>config.yml在这里设置博客的总体参数，博客名，url，git什么的</li>\n<li>_posts里面存放你的文章</li>\n<li>theme顾名思义，存放博客界面主题</li>\n<li>在config.yml中关键配置以下几个参数<ul>\n<li>title: 博客网站标题</li>\n<li>subtitle:     副标题</li>\n<li>description:     一句话简介</li>\n<li>author:     作者名</li>\n<li>language: zh-Hans    语言根据你选择的主题看，在主题目录的language里查看支持哪些语言</li>\n<li>timezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区</li>\n<li>url: <a href=\"https://你的用户名.github.io/\" target=\"_blank\" rel=\"noopener\">https://你的用户名.github.io/</a></li>\n<li>type: git</li>\n<li>repo: <a href=\"https://github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://github.com/你的用户名/你的用户名.github.io.git</a></li>\n<li><strong>branch: master</strong></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"更新Hexo项目\"><a href=\"#更新Hexo项目\" class=\"headerlink\" title=\"更新Hexo项目\"></a>更新Hexo项目</h1><p>依次执行</p>\n<ul>\n<li>git add .  (检查所有文件是否更新)</li>\n<li>git commit -m “更新报告” (提交commit)</li>\n<li>git push origin hexo (上传更新到github)<br><strong>第一次更新可能出现一些错误</strong></li>\n<li>第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错</li>\n<li>permission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：<a href=\"https://www.github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://www.github.com/你的用户名/你的用户名.github.io.git</a></li>\n<li>refusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo —allow-unrelated-histories，之后再ush<br><strong>所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的</strong></li>\n</ul>\n<h1 id=\"Hexo-写博客及更新网页\"><a href=\"#Hexo-写博客及更新网页\" class=\"headerlink\" title=\"Hexo 写博客及更新网页\"></a>Hexo 写博客及更新网页</h1><p>主要命令如下</p>\n<ul>\n<li>hexo clean 清除缓存和静态文件</li>\n<li>hexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate</li>\n<li>hexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭</li>\n<li>hexo new”文件名” 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了</li>\n<li>hexo d或者hexo deploy，写好博客，clean&amp;generate之后，部署博客到GitHub Pages上，就更新了博客网页<br><strong>其中有几点要注意</strong></li>\n<li>第一次新建目录后执行#npm install hexo-deployer-git —save安装git分发，否则上传不了</li>\n<li>hexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格</li>\n<li>我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s</li>\n</ul>\n<h1 id=\"Hexo写作的几种选择-Windows\"><a href=\"#Hexo写作的几种选择-Windows\" class=\"headerlink\" title=\"Hexo写作的几种选择(Windows)\"></a>Hexo写作的几种选择(Windows)</h1><ul>\n<li><strong>马克飞象</strong>:这里推荐用马克飞象的chrome app版本，可以离线打开。<strong>优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年</strong>,它长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i078zV.jpg\" alt=\"i078zV.jpg\"></li>\n<li><strong>hexo admin插件</strong>:安装hexo-admin插件，可视化管理博客，安装方法：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-admin</span><br><span class=\"line\">hexo server -d</span><br><span class=\"line\">open http://localhost:4000/admin/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><em><a href=\"http://localhost:4000/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/admin/</a></em>就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，<strong>优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好</strong>,它长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i073R0.jpg\" alt=\"i073R0.jpg\"></p>\n<ul>\n<li>推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。</li>\n<li><strong>Typora</strong>:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：<strong>Typora</strong>。<strong>优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。</strong>他长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i071Gq.jpg\" alt=\"i071Gq.jpg\"></li>\n<li>Mac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。</li>\n<li>常备一个notepad++,出现什么问题没有notepad++解决不了的。</li>\n</ul>\n<h1 id=\"主题设置\"><a href=\"#主题设置\" class=\"headerlink\" title=\"主题设置\"></a>主题设置</h1><ul>\n<li>一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可</li>\n<li>每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改</li>\n</ul>\n<h1 id=\"写作语法\"><a href=\"#写作语法\" class=\"headerlink\" title=\"写作语法\"></a>写作语法</h1><ul>\n<li>语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明</li>\n</ul>\n<h1 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h1><ul>\n<li>插入图片分本地和在线链接插入，推荐使用在线链接</li>\n<li>我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。</li>\n</ul>\n<h1 id=\"插入公式\"><a href=\"#插入公式\" class=\"headerlink\" title=\"插入公式\"></a>插入公式</h1><ul>\n<li>用一对$$将公式围起来，语法支持latex</li>\n<li>必须在文章首部中声明如mathjax: true</li>\n<li>因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法</li>\n<li>建议更换cdn以加速:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mathjax:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  per_page: true</span><br><span class=\"line\">  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"标签与分类\"><a href=\"#标签与分类\" class=\"headerlink\" title=\"标签与分类\"></a>标签与分类</h1><ul>\n<li>标签与分类的区别在于1篇文章归于一类，但可能有多个标签</li>\n<li>比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签</li>\n<li>分类需在首部加上categories: 分类名</li>\n<li>标签需在首部加上tags: 标签名或者tags: [标签1,标签2….],<strong>注意别打成中文逗号</strong>    </li>\n</ul>\n<h1 id=\"其他美化\"><a href=\"#其他美化\" class=\"headerlink\" title=\"其他美化\"></a>其他美化</h1><ul>\n<li>我用的是next主题，可以设置背景之类的，百度一下方法就能找到</li>\n<li>更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network</li>\n</ul>\n<h1 id=\"首页文章数量设置\"><a href=\"#首页文章数量设置\" class=\"headerlink\" title=\"首页文章数量设置\"></a>首页文章数量设置</h1><ul>\n<li><p>安装插件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-generator-index</span><br><span class=\"line\">npm install --save hexo-generator-archive</span><br><span class=\"line\">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在站点配置文件config.yml中添加如下字段</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index_generator:</span><br><span class=\"line\">per_page: 5</span><br><span class=\"line\"></span><br><span class=\"line\">archive_generator:</span><br><span class=\"line\">per_page: 20</span><br><span class=\"line\">yearly: true</span><br><span class=\"line\">monthly: true</span><br><span class=\"line\"></span><br><span class=\"line\">tag_generator:</span><br><span class=\"line\">per_page: 10</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>index, archive及tag开头分表代表主页，归档页面和标签页面。</p>\n</li>\n</ul>\n<h1 id=\"添加评论功能\"><a href=\"#添加评论功能\" class=\"headerlink\" title=\"添加评论功能\"></a>添加评论功能</h1><ul>\n<li><del>视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可</del></li>\n<li>现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论</li>\n</ul>\n<h1 id=\"优化插件\"><a href=\"#优化插件\" class=\"headerlink\" title=\"优化插件\"></a>优化插件</h1><ul>\n<li>hexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率<br><a href=\"https://github.com/chenzhutian/hexo-all-minifier\" target=\"_blank\" rel=\"noopener\">hexo-all-minifier</a></li>\n</ul>\n<h1 id=\"置顶\"><a href=\"#置顶\" class=\"headerlink\" title=\"置顶\"></a>置顶</h1><ul>\n<li>感谢Netcan_Space提供解决方案，希望官方theme加入此功能：<a href=\"http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">添加Hexo置顶功能</a></li>\n</ul>\n<h1 id=\"cnpm\"><a href=\"#cnpm\" class=\"headerlink\" title=\"cnpm\"></a>cnpm</h1><ul>\n<li>使用淘宝镜像安装插件提速，详情百度cnpm安装</li>\n</ul>\n<h1 id=\"RSS\"><a href=\"#RSS\" class=\"headerlink\" title=\"RSS\"></a>RSS</h1><ul>\n<li>使用hexo-generator-feed使用rss:<a href=\"https://github.com/hexojs/hexo-generator-feed\" target=\"_blank\" rel=\"noopener\">hexo-generator-feed</a></li>\n</ul>\n<h1 id=\"异地恢复\"><a href=\"#异地恢复\" class=\"headerlink\" title=\"异地恢复\"></a>异地恢复</h1><ul>\n<li>最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下<ul>\n<li>clone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml</li>\n<li>这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除</li>\n<li>记得安装hexo-deployer-git</li>\n<li>next更新了，但是新功能的一些依赖还是需要看注释，自己npm install</li>\n<li>发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作</li>\n<li>上面那2张图已经过时了，可以去我的github里看博客的文件目录</li>\n<li>有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱</li>\n<li>升级mathjax到2.7.5</li>\n<li>改用hexo-renderer-kramed，并修改inline.js转义</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"这位大佬靠谱\"><a href=\"#这位大佬靠谱\" class=\"headerlink\" title=\"这位大佬靠谱\"></a>这位大佬靠谱</h1><ul>\n<li>网址在这：<a href=\"https://www.vincentqin.tech/posts/build-a-website-using-hexo/\" target=\"_blank\" rel=\"noopener\">HEXO建站备忘录</a></li>\n<li>里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍<a href=\"https://github.com/coneycode/hexo-git-backup\" target=\"_blank\" rel=\"noopener\">hexo-git-backup</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p>\n<ul>\n<li>2017.2.8更新md写作软件</li>\n<li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li>\n<li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li>\n<li>2017.3.30更新置顶说明原文地址</li>\n<li>2017.12.27更新异地恢复</li>\n<li>2018.7.6更新一个比较全面的参考网址</li></ul>","more":"\n\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i07MIs.png\" alt=\"i07MIs.png\"></p>\n<h1 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h1><ul>\n<li>安装好Node.js</li>\n<li>安装好git</li>\n<li>安装好一个你最喜欢的文字编辑软件，用于写博客，比如notepad++。或者使用插件和第三方markdown写作软件(推荐)。</li>\n<li>可选：安装chrome插件极简图床或MPic(图床软件)。安装geogebra(画图像)</li>\n</ul>\n<h1 id=\"GitHub-amp-Hexo初始化\"><a href=\"#GitHub-amp-Hexo初始化\" class=\"headerlink\" title=\"GitHub&amp;Hexo初始化\"></a>GitHub&amp;Hexo初始化</h1><p>选择hexo作为博客工具，并没有用官方推荐的jeykell，其实差不多，一个基于ruby,一个基于node.js,hexo据说快一些<br>最后的过程是这样的</p>\n<ul>\n<li>注册github账号，这个就省略了</li>\n<li>新建一个repository，命名必须为“你的账户名.github.io”</li>\n<li>对这个repository新建一个hexo分支并把hexo设置为默认分支</li>\n<li>git bash cd到你本地新建的一个文件夹（用于存放博客项目）</li>\n<li>依次执行 npm install hexo、hexo init、npm install、npm install hexo-deployer-git</li>\n<li>此时你的文件夹中就已经初始化了Hexo博客项目，在文件夹中找到      _config.yml，修改deploy参数为master, 见后文</li>\n<li>此时你的网址就是：<a href=\"https://你的账户名.github.io/\" target=\"_blank\" rel=\"noopener\">https://你的账户名.github.io/</a></li>\n<li>现在的状态就是，你的博客项目存在hexo分支下，所有更改都会保存到github上Hexo分支中，但因为deploy参数为master,所以用hexo命令生成博客网页文件时会更新master分支，因此每次更新博客后两个分支都会更新，一个hexo项目，一个更新网页文件</li>\n</ul>\n<p><strong>之后所有的命令都是在Git Bash命令行环境下，工作目录就是这个Hexo博客目录,并且注意切换到hexo分支</strong></p>\n<h1 id=\"Hexo配置\"><a href=\"#Hexo配置\" class=\"headerlink\" title=\"Hexo配置\"></a>Hexo配置</h1><p>Hexo博客的目录格式如下</p>\n<p>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br>|   ├── _drafts<br>|   └── _posts<br>└── themes</p>\n<ul>\n<li>config.yml在这里设置博客的总体参数，博客名，url，git什么的</li>\n<li>_posts里面存放你的文章</li>\n<li>theme顾名思义，存放博客界面主题</li>\n<li>在config.yml中关键配置以下几个参数<ul>\n<li>title: 博客网站标题</li>\n<li>subtitle:     副标题</li>\n<li>description:     一句话简介</li>\n<li>author:     作者名</li>\n<li>language: zh-Hans    语言根据你选择的主题看，在主题目录的language里查看支持哪些语言</li>\n<li>timezone: Asia/Shanghai 时区，这个有规范，中国就写上海时区</li>\n<li>url: <a href=\"https://你的用户名.github.io/\" target=\"_blank\" rel=\"noopener\">https://你的用户名.github.io/</a></li>\n<li>type: git</li>\n<li>repo: <a href=\"https://github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://github.com/你的用户名/你的用户名.github.io.git</a></li>\n<li><strong>branch: master</strong></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"更新Hexo项目\"><a href=\"#更新Hexo项目\" class=\"headerlink\" title=\"更新Hexo项目\"></a>更新Hexo项目</h1><p>依次执行</p>\n<ul>\n<li>git add .  (检查所有文件是否更新)</li>\n<li>git commit -m “更新报告” (提交commit)</li>\n<li>git push origin hexo (上传更新到github)<br><strong>第一次更新可能出现一些错误</strong></li>\n<li>第一次先pull再push,否则GitHub发现本地没有服务器上的一些文件会报错</li>\n<li>permission denied 去博客目录/.git/config中，找到Url,将ssh链接改成html格式：<a href=\"https://www.github.com/你的用户名/你的用户名.github.io.git\" target=\"_blank\" rel=\"noopener\">https://www.github.com/你的用户名/你的用户名.github.io.git</a></li>\n<li>refusing to merge unrelated histories 因为没有公共祖先分支无法合并，这就只能强制合并了，执行命令：git pull origin hexo —allow-unrelated-histories，之后再ush<br><strong>所有的操作都在Hexo分支下进行，因为master只存静态网页文件，不需要你更改，它是由Hexo生成的</strong></li>\n</ul>\n<h1 id=\"Hexo-写博客及更新网页\"><a href=\"#Hexo-写博客及更新网页\" class=\"headerlink\" title=\"Hexo 写博客及更新网页\"></a>Hexo 写博客及更新网页</h1><p>主要命令如下</p>\n<ul>\n<li>hexo clean 清除缓存和静态文件</li>\n<li>hexo g或者hexo generate 生成框架文件 建议每次更新网站之前先clean再generate</li>\n<li>hexo s或者hexo server 打开本地服务器进行预览，网址输入localhost:4000即可，按crtl+c关闭</li>\n<li>hexo new”文件名” 新建博客，这个文件名不是文章标题名，新建之后在_post文件夹里打开就可以写博客了</li>\n<li>hexo d或者hexo deploy，写好博客，clean&amp;generate之后，部署博客到GitHub Pages上，就更新了博客网页<br><strong>其中有几点要注意</strong></li>\n<li>第一次新建目录后执行#npm install hexo-deployer-git —save安装git分发，否则上传不了</li>\n<li>hexo的语法规范很严格，博客里tags:或者级标题符号#后面必须接一个空格</li>\n<li>我在next主题下更新有点问题，最好是两次generate两次server进行预览，预览时用server -s</li>\n</ul>\n<h1 id=\"Hexo写作的几种选择-Windows\"><a href=\"#Hexo写作的几种选择-Windows\" class=\"headerlink\" title=\"Hexo写作的几种选择(Windows)\"></a>Hexo写作的几种选择(Windows)</h1><ul>\n<li><strong>马克飞象</strong>:这里推荐用马克飞象的chrome app版本，可以离线打开。<strong>优点：界面简洁明了，编辑自由，预览功能完整，常见操作(加粗字体插入图片链接代码块引用)方便,可以绑定印象笔记给文章备份。缺点：操作麻烦一点，需要把文本拷回去。 多个文章打开不方便，免费试用10天，78元一年</strong>,它长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i078zV.jpg\" alt=\"i078zV.jpg\"></li>\n<li><strong>hexo admin插件</strong>:安装hexo-admin插件，可视化管理博客，安装方法：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-admin</span><br><span class=\"line\">hexo server -d</span><br><span class=\"line\">open http://localhost:4000/admin/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><em><a href=\"http://localhost:4000/admin/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/admin/</a></em>就是一个网页管理界面，可以管理文章，同样也能写文章，实时预览，加标签什么的，相当于把部分功能提出来做成了GUI，<strong>优点：多文章管理，操作方便，自动保存,编辑时根据md语法可以在编辑文本改字号字体。缺点：预览不是很完全。不支持mathjax数学公式预览，自定义程度不高,编辑有一点小bug,新建文章时文件名有点小问题，对中文支持不是很好</strong>,它长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i073R0.jpg\" alt=\"i073R0.jpg\"></p>\n<ul>\n<li>推荐是两个都装上，写新文章或者写需要插入公式代码的文章用第三方编辑器，如果只是普通码字或者更新一下以前写过的文章可以用hexo admin。</li>\n<li><strong>Typora</strong>:markdown写作软件有许多，大部分都是一边编辑一边预览，这里强推一款极简设计，提倡所见即所得概念的软件：<strong>Typora</strong>。<strong>优点：所见即所得，桌面软件，功能完整，近似于word的便捷体验(尤其在插入表格图片方面很方便)。缺点:不支持同步备份，html需要单独划区域,编辑略不自由。</strong>他长这个样子：<br><img src=\"https://s1.ax1x.com/2018/10/20/i071Gq.jpg\" alt=\"i071Gq.jpg\"></li>\n<li>Mac系统网上推荐使用Mou或者Sublime，我没有用过，也不做评价。</li>\n<li>常备一个notepad++,出现什么问题没有notepad++解决不了的。</li>\n</ul>\n<h1 id=\"主题设置\"><a href=\"#主题设置\" class=\"headerlink\" title=\"主题设置\"></a>主题设置</h1><ul>\n<li>一般来说Hexo的主题制作人都会将主题开源到GitHub上，直接用命令clone将其拷到本地，然后在config.yml中将theme改成这个主题名即可</li>\n<li>每一个主题下面有自己的config.yml，用于对每个主题进行进一步的配置，根据作者说明或者GitHub上的readme进行更改</li>\n</ul>\n<h1 id=\"写作语法\"><a href=\"#写作语法\" class=\"headerlink\" title=\"写作语法\"></a>写作语法</h1><ul>\n<li>语法支持Markdown,可以用html，latex等其他语法，但必须在文章首部中声明</li>\n</ul>\n<h1 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h1><ul>\n<li>插入图片分本地和在线链接插入，推荐使用在线链接</li>\n<li>我的方法是申请七牛云账户，新建一个存储空间，拿到域名，ak,sk,然后chrome装一个插件，极简图床，在这个插件上配置好刚刚拿到的域名，ak,sk，之后直接点击上传图片，就可以了，插件会自动生成用于插入图片的markdown语句或者在线链接，直接写进正文即可。MPic同理。</li>\n</ul>\n<h1 id=\"插入公式\"><a href=\"#插入公式\" class=\"headerlink\" title=\"插入公式\"></a>插入公式</h1><ul>\n<li>用一对$$将公式围起来，语法支持latex</li>\n<li>必须在文章首部中声明如mathjax: true</li>\n<li>因为hexo先用marked.js进行预处理，然后才通过mathjax处理，所以\\\\可能被转义为\\，从而导致公式显示不正确，解决方法有两种，多打两个\\\\，另外一种是修改marked.js文件，无视转义，可以自行百度方法</li>\n<li>建议更换cdn以加速:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mathjax:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  per_page: true</span><br><span class=\"line\">  cdn: //cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"标签与分类\"><a href=\"#标签与分类\" class=\"headerlink\" title=\"标签与分类\"></a>标签与分类</h1><ul>\n<li>标签与分类的区别在于1篇文章归于一类，但可能有多个标签</li>\n<li>比如机器学习的一片文章归于机器学习这个category，但可以有代码、数学、原创等多个标签</li>\n<li>分类需在首部加上categories: 分类名</li>\n<li>标签需在首部加上tags: 标签名或者tags: [标签1,标签2….],<strong>注意别打成中文逗号</strong>    </li>\n</ul>\n<h1 id=\"其他美化\"><a href=\"#其他美化\" class=\"headerlink\" title=\"其他美化\"></a>其他美化</h1><ul>\n<li>我用的是next主题，可以设置背景之类的，百度一下方法就能找到</li>\n<li>更换字体，视主题而定，next里面是用config里直接改字体，用的是google fonts,由于国内访问有问题，所以host用//fonts.css.network</li>\n</ul>\n<h1 id=\"首页文章数量设置\"><a href=\"#首页文章数量设置\" class=\"headerlink\" title=\"首页文章数量设置\"></a>首页文章数量设置</h1><ul>\n<li><p>安装插件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install --save hexo-generator-index</span><br><span class=\"line\">npm install --save hexo-generator-archive</span><br><span class=\"line\">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>在站点配置文件config.yml中添加如下字段</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index_generator:</span><br><span class=\"line\">per_page: 5</span><br><span class=\"line\"></span><br><span class=\"line\">archive_generator:</span><br><span class=\"line\">per_page: 20</span><br><span class=\"line\">yearly: true</span><br><span class=\"line\">monthly: true</span><br><span class=\"line\"></span><br><span class=\"line\">tag_generator:</span><br><span class=\"line\">per_page: 10</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>index, archive及tag开头分表代表主页，归档页面和标签页面。</p>\n</li>\n</ul>\n<h1 id=\"添加评论功能\"><a href=\"#添加评论功能\" class=\"headerlink\" title=\"添加评论功能\"></a>添加评论功能</h1><ul>\n<li><del>视主题而定，我用的next主题，默认支持多说，只需要在主题配置文件中写入自己的duoshou_shortname即可</del></li>\n<li>现在多说那些啥的都关了，最好自己在leancloud上开后台空间，使用gitcomment、Disqus、来必力都行，现在我用的是valine，就冲它不用登录即可评论</li>\n</ul>\n<h1 id=\"优化插件\"><a href=\"#优化插件\" class=\"headerlink\" title=\"优化插件\"></a>优化插件</h1><ul>\n<li>hexo生成的html文件有许多冗余，这里推荐安装一款插件压缩文件，提高效率<br><a href=\"https://github.com/chenzhutian/hexo-all-minifier\" target=\"_blank\" rel=\"noopener\">hexo-all-minifier</a></li>\n</ul>\n<h1 id=\"置顶\"><a href=\"#置顶\" class=\"headerlink\" title=\"置顶\"></a>置顶</h1><ul>\n<li>感谢Netcan_Space提供解决方案，希望官方theme加入此功能：<a href=\"http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/\" target=\"_blank\" rel=\"noopener\">添加Hexo置顶功能</a></li>\n</ul>\n<h1 id=\"cnpm\"><a href=\"#cnpm\" class=\"headerlink\" title=\"cnpm\"></a>cnpm</h1><ul>\n<li>使用淘宝镜像安装插件提速，详情百度cnpm安装</li>\n</ul>\n<h1 id=\"RSS\"><a href=\"#RSS\" class=\"headerlink\" title=\"RSS\"></a>RSS</h1><ul>\n<li>使用hexo-generator-feed使用rss:<a href=\"https://github.com/hexojs/hexo-generator-feed\" target=\"_blank\" rel=\"noopener\">hexo-generator-feed</a></li>\n</ul>\n<h1 id=\"异地恢复\"><a href=\"#异地恢复\" class=\"headerlink\" title=\"异地恢复\"></a>异地恢复</h1><ul>\n<li>最近重装系统，重新恢复了本地博客，但是从远程clone下来并在本地hexo操作之后发现了许多问题，总结如下<ul>\n<li>clone之后直接安装依赖项，不需要hexo init，否则会情况博客的config.yml</li>\n<li>这一次其实是完全重新建了博客，因为以前的设置怎么导入怎么有问题，后来才发现当时的本地博客备份里根本没有theme!因为我的theme就是从别人的repository那里clone过来的，而整个博客又是通过git备份的，一个repository中不能包含另外1个repository，所以其实主题及所有设置一直没备份，以后得将主题目录下的.git文件夹删除</li>\n<li>记得安装hexo-deployer-git</li>\n<li>next更新了，但是新功能的一些依赖还是需要看注释，自己npm install</li>\n<li>发现其实之前的博客太花哨了，这次干脆把所有附加功能都整没了，专注写作</li>\n<li>上面那2张图已经过时了，可以去我的github里看博客的文件目录</li>\n<li>有很多module因为名字太长所以也没有完成备份，感觉还是百度云靠谱</li>\n<li>升级mathjax到2.7.5</li>\n<li>改用hexo-renderer-kramed，并修改inline.js转义</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"这位大佬靠谱\"><a href=\"#这位大佬靠谱\" class=\"headerlink\" title=\"这位大佬靠谱\"></a>这位大佬靠谱</h1><ul>\n<li>网址在这：<a href=\"https://www.vincentqin.tech/posts/build-a-website-using-hexo/\" target=\"_blank\" rel=\"noopener\">HEXO建站备忘录</a></li>\n<li>里面提到了一个非常好的版本管理的工具，解决了theme下repository包含的问题，可以直接看介绍<a href=\"https://github.com/coneycode/hexo-git-backup\" target=\"_blank\" rel=\"noopener\">hexo-git-backup</a></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i07MIs.png","popularPost_tmp_gaData":{"updated":"Tue Jul 23 2019 10:37:53 GMT+0800 (GMT+08:00)","title":"搭站总结","path":"2017/01/16/setupmywebsite/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i07MIs.png","excerpt":"<hr>\n<p>一直想建一个自己的博客，之前想用wordpress，但是苦于自己懒癌，不想折腾服务器<br>后来偶尔发现了GitHub Pages,上传js项目自动生成网站，全部由GitHub托管<br>而且本来官方说明也推荐用这个写博客，于是就开始试试<br>大体框架应该是GitHubPages由你在GitHub的github.io项目生成网站<br>Hexo由你的博客内容和自定义设置生成静态网页项目并上传到你的repository<br>为了备份，我们将在repository中建立两个branch<br>一个master用于让hexo上传静态网页文件<br>一个hexo用于保存本地hexo项目<br>下面分享一些经验和踩到的坑</p>\n<ul>\n<li>2017.2.8更新md写作软件</li>\n<li>2017.2.10更新mathjax cdn，加入长廊，更新域名,国内外访问分流(blog2.0)</li>\n<li>2017.2.13更新优化插件，更新置顶说明,优化长廊，加宽文章宽度(blog3.0)</li>\n<li>2017.3.30更新置顶说明原文地址</li>\n<li>2017.12.27更新异地恢复</li>\n<li>2018.7.6更新一个比较全面的参考网址</li></ul>","date":"2017-01-16T04:01:00.000Z","pv":0,"totalPV":0,"categories":"瞎折腾","tags":["web","hexo","github"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"Android:Melodia客户端","date":"2017-03-09T09:19:53.000Z","_content":"***\n学校大创项目简单的app\n实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。\n\n<!--more-->\n\n![i0o26O.gif](https://s1.ax1x.com/2018/10/20/i0o26O.gif)\n封面图使用[qiao](https://github.com/qiao)的midi在线可视化工具[euphony](https://github.com/qiao/euphony)\n\n# midi播放\n调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放\n-\tMediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放\n-\t从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)\n-\tUri或者外部存储读取new->setDataSource->prepare->start\n\n# 录制声音并重放\n参考[android中AudioRecord使用](http://blog.csdn.net/jiangliloveyou/article/details/11218555)\n```Java\n\tprivate class RecordTask extends AsyncTask<Void, Integer, Void> {\n\t\t@Override\n\t\tprotected Void doInBackground(Void... arg0) {\n\t\t\tisRecording = true;\n\t\t\ttry {\n\t\t\t\t//开通输出流到指定的文件\n\t\t\t\tDataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(pcmFile)));\n\t\t\t\t//根据定义好的几个配置，来获取合适的缓冲大小\n\t\t\t\tint bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);\n\t\t\t\t//实例化AudioRecord\n\t\t\t\tAudioRecord record = new AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);\n\t\t\t\t//定义缓冲\n\t\t\t\tshort[] buffer = new short[bufferSize];\n\n\t\t\t\t//开始录制\n\t\t\t\trecord.startRecording();\n\n\t\t\t\tint r = 0; //存储录制进度\n\t\t\t\t//定义循环，根据isRecording的值来判断是否继续录制\n\t\t\t\twhile (isRecording) {\n\t\t\t\t\t//从bufferSize中读取字节，返回读取的short个数\n\t\t\t\t\t//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决\n\t\t\t\t\tint bufferReadResult = record.read(buffer, 0, buffer.length);\n\t\t\t\t\t//循环将buffer中的音频数据写入到OutputStream中\n\t\t\t\t\tfor (int i = 0; i < bufferReadResult; i++) {\n\t\t\t\t\t\tdos.writeShort(buffer[i]);\n\t\t\t\t\t}\n\t\t\t\t\tpublishProgress(new Integer(r)); //向UI线程报告当前进度\n\t\t\t\t\tr++; //自增进度值\n\t\t\t\t}\n\t\t\t\t//录制结束\n\t\t\t\trecord.stop();\n\t\t\t\tconvertWaveFile();\n\t\t\t\tdos.close();\n\t\t\t} catch (Exception e) {\n\t\t\t\t// TODO: handle exception\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n```\n\n# pcm写头文件转成wav\n因为录制的是裸文件，pcm格式，需要自己加上wav头\n```Java\n\tprivate void WriteWaveFileHeader(FileOutputStream out, long totalAudioLen, long totalDataLen, long longSampleRate,\n\t\t\t\t\t\t\t\t\t int channels, long byteRate) throws IOException {\n\t\tbyte[] header = new byte[45];\n\t\theader[0] = 'R'; // RIFF\n\t\theader[1] = 'I';\n\t\theader[2] = 'F';\n\t\theader[3] = 'F';\n\t\theader[4] = (byte) (totalDataLen & 0xff);//数据大小\n\t\theader[5] = (byte) ((totalDataLen >> 8) & 0xff);\n\t\theader[6] = (byte) ((totalDataLen >> 16) & 0xff);\n\t\theader[7] = (byte) ((totalDataLen >> 24) & 0xff);\n\t\theader[8] = 'W';//WAVE\n\t\theader[9] = 'A';\n\t\theader[10] = 'V';\n\t\theader[11] = 'E';\n\t\t//FMT Chunk\n\t\theader[12] = 'f'; // 'fmt '\n\t\theader[13] = 'm';\n\t\theader[14] = 't';\n\t\theader[15] = ' ';//过渡字节\n\t\t//数据大小\n\t\theader[16] = 16; // 4 bytes: size of 'fmt ' chunk\n\t\theader[17] = 0;\n\t\theader[18] = 0;\n\t\theader[19] = 0;\n\t\t//编码方式 10H为PCM编码格式\n\t\theader[20] = 1; // format = 1\n\t\theader[21] = 0;\n\t\t//通道数\n\t\theader[22] = (byte) channels;\n\t\theader[23] = 0;\n\t\t//采样率，每个通道的播放速度\n\t\theader[24] = (byte) (longSampleRate & 0xff);\n\t\theader[25] = (byte) ((longSampleRate >> 8) & 0xff);\n\t\theader[26] = (byte) ((longSampleRate >> 16) & 0xff);\n\t\theader[27] = (byte) ((longSampleRate >> 24) & 0xff);\n\t\t//音频数据传送速率,采样率*通道数*采样深度/8\n\t\theader[28] = (byte) (byteRate & 0xff);\n\t\theader[29] = (byte) ((byteRate >> 8) & 0xff);\n\t\theader[30] = (byte) ((byteRate >> 16) & 0xff);\n\t\theader[31] = (byte) ((byteRate >> 24) & 0xff);\n\t\t// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数\n\t\theader[32] = (byte) (1 * 16 / 8);\n\t\theader[33] = 0;\n\t\t//每个样本的数据位数\n\t\theader[34] = 16;\n\t\theader[35] = 0;\n\t\t//Data chunk\n\t\theader[36] = 'd';//data\n\t\theader[37] = 'a';\n\t\theader[38] = 't';\n\t\theader[39] = 'a';\n\t\theader[40] = (byte) (totalAudioLen & 0xff);\n\t\theader[41] = (byte) ((totalAudioLen >> 8) & 0xff);\n\t\theader[42] = (byte) ((totalAudioLen >> 16) & 0xff);\n\t\theader[43] = (byte) ((totalAudioLen >> 24) & 0xff);\n\t\theader[44] = 0;\n\t\tout.write(header, 0, 45);\n\t}\n```\n\n# json收发\n根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中\njson发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件\n```Java\n    private JSONObject makejson(int request, String identifycode, String data) {\n        if (identifycode == \"a\") {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                JSONObject config = new JSONObject();\n                config.put(\"n\", lowf);\n                config.put(\"m\", highf);\n                config.put(\"w\", interval);\n                pack.put(\"config\", config);\n                pack.put(\"data\", data);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n        } else {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                pack.put(\"config\", \"\");\n                pack.put(\"data\", identifycode);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n\n        }\n        return null;\n    }\n```\n# socket通信\n单开一个线程用于启动socket，再开一个线程写两次json收发\n注意收发json时将json字符串用base64解码编码，java自己的string会存在错误\n另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，\"endbidou\"，不要问我是什么意思，做转换算法的兄弟想的\n```Java\nprivate class MsgThread extends Thread {\n        @Override\n        public void run() {\n            File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.wav\");\n            FileInputStream reader = null;\n            try {\n                reader = new FileInputStream(file);\n                int len = reader.available();\n                byte[] buff = new byte[len];\n                reader.read(buff);\n                String data = Base64.encodeToString(buff, Base64.DEFAULT);\n                String senda = makejson(1, \"a\", data).toString();\n                Log.i(TAG, \"request1: \" + senda);\n                OutputStream os = null;\n                InputStream is = null;\n                DataInputStream in = null;\n                try {\n                    os = soc.getOutputStream();\n                    BufferedReader bra = null;\n                    os.write(senda.getBytes());\n                    os.write(\"endbidou1\".getBytes());\n                    os.flush();\n                    Log.i(TAG, \"request1 send successful\");\n                    if (soc.isConnected()) {\n                        is = soc.getInputStream();\n                        bra = new BufferedReader(new InputStreamReader(is));\n                        md5 = bra.readLine();\n                        Log.i(TAG, \"md5: \" + md5);\n                        bra.close();\n                    } else\n                        Log.i(TAG, \"socket closed while reading\");\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n                soc.close();\n                startflag = 1;\n\n                StartThread st = new StartThread();\n                st.start();\n\n                while (soc.isClosed()) ;\n\n                String sendb = makejson(2, md5, \"request2\").toString();\n                Log.i(TAG, \"request2: \" + sendb);\n                os = soc.getOutputStream();\n                os.write(sendb.getBytes());\n                os.write(\"endbidou1\".getBytes());\n                os.flush();\n                Log.i(TAG, \"request2 send successful\");\n\n                is = soc.getInputStream();\n                byte buffer[] = new byte[1024 * 100];\n                is.read(buffer);\n                Log.i(TAG, \"midifilecontent: \" + buffer.toString());\n                soc.close();\n                File filemid = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.mid\");\n                FileOutputStream writer = null;\n                writer = new FileOutputStream(filemid);\n                writer.write(buffer);\n                writer.close();\n                Message msg = myhandler.obtainMessage();\n                msg.what = 1;\n                myhandler.sendMessage(msg);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n\n\n        }\n    }\n```\n\n# 录音特效\n录音图像动画效果来自Github：[ShineButton](https://github.com/ChadCSong/ShineButton)\n另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消\n```Java\n\tfabrecord.setOnTouchListener(new View.OnTouchListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onTouch(View v, MotionEvent event) {\n\t\t\t\t\tswitch (event.getAction()) {\n\t\t\t\t\t\tcase MotionEvent.ACTION_DOWN:\n\t\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);\n\t\t\t\t\t\t\tif (isUploadingIcon) {\n\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_DOWN\");\n\t\t\t\t\t\t\tif (!shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tox = event.getX();\n\t\t\t\t\t\t\toy = event.getY();\n\n\t\t\t\t\t\t\tisRecording = true;\n\t\t\t\t\t\t\trecLen = 0;\n\t\t\t\t\t\t\trecTime = 0;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"开始录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\t\t\trecorder = new RecordTask();\n\t\t\t\t\t\t\trecorder.execute();\n\t\t\t\t\t\t\thandler.postDelayed(runrecord, 0);\n\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_UP:\n\t\t\t\t\t\t\thandler.removeCallbacks(runrecord);\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_UP\");\n\t\t\t\t\t\t\tif (shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfloat x1 = event.getX();\n\t\t\t\t\t\t\tfloat y1 = event.getY();\n\t\t\t\t\t\t\tfloat dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);\n\n\t\t\t\t\t\t\tisRecording = false;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);\n\t\t\t\t\t\t\tif (dis1 > 30000) {\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"取消录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tif (!isUploadingIcon) {\n\t\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);\n\t\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t\t} else {\n\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"录音完成\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t\thandler.postDelayed(runreplay, 0);\n\t\t\t\t\t\t\t\treplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_MOVE:\n\t\t\t\t\t\t\tfloat x2 = event.getX();\n\t\t\t\t\t\t\tfloat y2 = event.getY();\n\t\t\t\t\t\t\tfloat dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);\n\t\t\t\t\t\t\tif (dis2 > 30000) {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```\n# 展示乐谱\n-\t本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享\n```Java\n    public void init() {\n        md5 = getArguments().getString(\"md5\");\n        final String imageUri = \"服务器地址\" + md5 + \"_1.png\";\n        Log.i(\"play\", \"pngfile: \" + imageUri);\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                imageLoader.displayImage(imageUri, showpic);\n            }\n        }, 2000);\n\n    }\n```\n\n# 与电子琴通信\n-\t类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接\n```Java\n\tpianobt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tif (!isconnected) {\n\t\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\t\tparam[0] = 0x30;\n\t\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\t\tst.start();\n\t\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\t\tms.start();\n\t\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tisconnected = false;\n\t\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\t\t\t\t\t}\n\n\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tsamplebt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\tparam[0] = 0x31;\n\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\tst.start();\n\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\tms.start();\n\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tisconnected = false;\n\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\n\t\t\t\t}\n\t\t\t});\n\n\n\t\t}\n\n\t\tprivate class StartThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tsoc = new Socket(pianoaddr, pianoport);\n\t\t\t\t\tif (soc.isConnected()) {//成功连接获取soc对象则发送成功消息\n\t\t\t\t\t\tLog.i(\"piano\", \"piano is Connected\");\n\t\t\t\t\t\tif (!isconnected)\n\t\t\t\t\t\t\tisconnected = !isconnected;\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tprivate class MsgThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tOutputStream os = soc.getOutputStream();\n\t\t\t\t\tos.write(param);\n\t\t\t\t\tos.flush();\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful\");\n\t\t\t\t\tSnackbar.make(pianobt, \"正在启动启动电子琴教学\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\tsoc.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful failed\");\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n\n# 乐谱分享\n-\t显示乐谱的是Github上一个魔改的ImageView:[PinchImageView](https://github.com/boycy815/PinchImageView)\n-\t定义其长按事件，触发一个分享的intent\n```Java\n\tshowpic.setOnLongClickListener(new View.OnLongClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onLongClick(View v) {\n\t\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);\n\t\t\t\t\tif (drawingCache == null) {\n\t\t\t\t\t\tLog.i(\"play\", \"no img to save\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFile imageFile = new File(Environment.getExternalStorageDirectory(), \"saveImageview.jpg\");\n\t\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),\n\t\t\t\t\t\t\t\t\t\"\", Toast.LENGTH_LONG);\n\t\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, 0, 200);\n\t\t\t\t\t\t\ttoast.setText(\"分享图片\");\n\t\t\t\t\t\t\ttoast.show();\n\t\t\t\t\t\t\tFileOutputStream outStream;\n\t\t\t\t\t\t\toutStream = new FileOutputStream(imageFile);\n\t\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, 100, outStream);\n\t\t\t\t\t\t\toutStream.flush();\n\t\t\t\t\t\t\toutStream.close();\n\n\t\t\t\t\t\t\tIntent sendIntent = new Intent();\n\t\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);\n\t\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));\n\t\t\t\t\t\t\tsendIntent.setType(\"image/png\");\n\t\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, \"分享到\"));\n\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\tLog.i(\"play\", \"share img wrong\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```","source":"_posts/dachuang.md","raw":"---\ntitle: Android:Melodia客户端\ndate: 2017-03-09 17:19:53\ntags: [code,android]\ncategories: Android\n---\n***\n学校大创项目简单的app\n实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。\n\n<!--more-->\n\n![i0o26O.gif](https://s1.ax1x.com/2018/10/20/i0o26O.gif)\n封面图使用[qiao](https://github.com/qiao)的midi在线可视化工具[euphony](https://github.com/qiao/euphony)\n\n# midi播放\n调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放\n-\tMediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放\n-\t从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)\n-\tUri或者外部存储读取new->setDataSource->prepare->start\n\n# 录制声音并重放\n参考[android中AudioRecord使用](http://blog.csdn.net/jiangliloveyou/article/details/11218555)\n```Java\n\tprivate class RecordTask extends AsyncTask<Void, Integer, Void> {\n\t\t@Override\n\t\tprotected Void doInBackground(Void... arg0) {\n\t\t\tisRecording = true;\n\t\t\ttry {\n\t\t\t\t//开通输出流到指定的文件\n\t\t\t\tDataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(pcmFile)));\n\t\t\t\t//根据定义好的几个配置，来获取合适的缓冲大小\n\t\t\t\tint bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);\n\t\t\t\t//实例化AudioRecord\n\t\t\t\tAudioRecord record = new AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);\n\t\t\t\t//定义缓冲\n\t\t\t\tshort[] buffer = new short[bufferSize];\n\n\t\t\t\t//开始录制\n\t\t\t\trecord.startRecording();\n\n\t\t\t\tint r = 0; //存储录制进度\n\t\t\t\t//定义循环，根据isRecording的值来判断是否继续录制\n\t\t\t\twhile (isRecording) {\n\t\t\t\t\t//从bufferSize中读取字节，返回读取的short个数\n\t\t\t\t\t//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决\n\t\t\t\t\tint bufferReadResult = record.read(buffer, 0, buffer.length);\n\t\t\t\t\t//循环将buffer中的音频数据写入到OutputStream中\n\t\t\t\t\tfor (int i = 0; i < bufferReadResult; i++) {\n\t\t\t\t\t\tdos.writeShort(buffer[i]);\n\t\t\t\t\t}\n\t\t\t\t\tpublishProgress(new Integer(r)); //向UI线程报告当前进度\n\t\t\t\t\tr++; //自增进度值\n\t\t\t\t}\n\t\t\t\t//录制结束\n\t\t\t\trecord.stop();\n\t\t\t\tconvertWaveFile();\n\t\t\t\tdos.close();\n\t\t\t} catch (Exception e) {\n\t\t\t\t// TODO: handle exception\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n```\n\n# pcm写头文件转成wav\n因为录制的是裸文件，pcm格式，需要自己加上wav头\n```Java\n\tprivate void WriteWaveFileHeader(FileOutputStream out, long totalAudioLen, long totalDataLen, long longSampleRate,\n\t\t\t\t\t\t\t\t\t int channels, long byteRate) throws IOException {\n\t\tbyte[] header = new byte[45];\n\t\theader[0] = 'R'; // RIFF\n\t\theader[1] = 'I';\n\t\theader[2] = 'F';\n\t\theader[3] = 'F';\n\t\theader[4] = (byte) (totalDataLen & 0xff);//数据大小\n\t\theader[5] = (byte) ((totalDataLen >> 8) & 0xff);\n\t\theader[6] = (byte) ((totalDataLen >> 16) & 0xff);\n\t\theader[7] = (byte) ((totalDataLen >> 24) & 0xff);\n\t\theader[8] = 'W';//WAVE\n\t\theader[9] = 'A';\n\t\theader[10] = 'V';\n\t\theader[11] = 'E';\n\t\t//FMT Chunk\n\t\theader[12] = 'f'; // 'fmt '\n\t\theader[13] = 'm';\n\t\theader[14] = 't';\n\t\theader[15] = ' ';//过渡字节\n\t\t//数据大小\n\t\theader[16] = 16; // 4 bytes: size of 'fmt ' chunk\n\t\theader[17] = 0;\n\t\theader[18] = 0;\n\t\theader[19] = 0;\n\t\t//编码方式 10H为PCM编码格式\n\t\theader[20] = 1; // format = 1\n\t\theader[21] = 0;\n\t\t//通道数\n\t\theader[22] = (byte) channels;\n\t\theader[23] = 0;\n\t\t//采样率，每个通道的播放速度\n\t\theader[24] = (byte) (longSampleRate & 0xff);\n\t\theader[25] = (byte) ((longSampleRate >> 8) & 0xff);\n\t\theader[26] = (byte) ((longSampleRate >> 16) & 0xff);\n\t\theader[27] = (byte) ((longSampleRate >> 24) & 0xff);\n\t\t//音频数据传送速率,采样率*通道数*采样深度/8\n\t\theader[28] = (byte) (byteRate & 0xff);\n\t\theader[29] = (byte) ((byteRate >> 8) & 0xff);\n\t\theader[30] = (byte) ((byteRate >> 16) & 0xff);\n\t\theader[31] = (byte) ((byteRate >> 24) & 0xff);\n\t\t// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数\n\t\theader[32] = (byte) (1 * 16 / 8);\n\t\theader[33] = 0;\n\t\t//每个样本的数据位数\n\t\theader[34] = 16;\n\t\theader[35] = 0;\n\t\t//Data chunk\n\t\theader[36] = 'd';//data\n\t\theader[37] = 'a';\n\t\theader[38] = 't';\n\t\theader[39] = 'a';\n\t\theader[40] = (byte) (totalAudioLen & 0xff);\n\t\theader[41] = (byte) ((totalAudioLen >> 8) & 0xff);\n\t\theader[42] = (byte) ((totalAudioLen >> 16) & 0xff);\n\t\theader[43] = (byte) ((totalAudioLen >> 24) & 0xff);\n\t\theader[44] = 0;\n\t\tout.write(header, 0, 45);\n\t}\n```\n\n# json收发\n根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中\njson发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件\n```Java\n    private JSONObject makejson(int request, String identifycode, String data) {\n        if (identifycode == \"a\") {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                JSONObject config = new JSONObject();\n                config.put(\"n\", lowf);\n                config.put(\"m\", highf);\n                config.put(\"w\", interval);\n                pack.put(\"config\", config);\n                pack.put(\"data\", data);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n        } else {\n            try {\n                JSONObject pack = new JSONObject();\n                pack.put(\"request\", request);\n                pack.put(\"config\", \"\");\n                pack.put(\"data\", identifycode);\n                return pack;\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n\n        }\n        return null;\n    }\n```\n# socket通信\n单开一个线程用于启动socket，再开一个线程写两次json收发\n注意收发json时将json字符串用base64解码编码，java自己的string会存在错误\n另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，\"endbidou\"，不要问我是什么意思，做转换算法的兄弟想的\n```Java\nprivate class MsgThread extends Thread {\n        @Override\n        public void run() {\n            File file = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.wav\");\n            FileInputStream reader = null;\n            try {\n                reader = new FileInputStream(file);\n                int len = reader.available();\n                byte[] buff = new byte[len];\n                reader.read(buff);\n                String data = Base64.encodeToString(buff, Base64.DEFAULT);\n                String senda = makejson(1, \"a\", data).toString();\n                Log.i(TAG, \"request1: \" + senda);\n                OutputStream os = null;\n                InputStream is = null;\n                DataInputStream in = null;\n                try {\n                    os = soc.getOutputStream();\n                    BufferedReader bra = null;\n                    os.write(senda.getBytes());\n                    os.write(\"endbidou1\".getBytes());\n                    os.flush();\n                    Log.i(TAG, \"request1 send successful\");\n                    if (soc.isConnected()) {\n                        is = soc.getInputStream();\n                        bra = new BufferedReader(new InputStreamReader(is));\n                        md5 = bra.readLine();\n                        Log.i(TAG, \"md5: \" + md5);\n                        bra.close();\n                    } else\n                        Log.i(TAG, \"socket closed while reading\");\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n                soc.close();\n                startflag = 1;\n\n                StartThread st = new StartThread();\n                st.start();\n\n                while (soc.isClosed()) ;\n\n                String sendb = makejson(2, md5, \"request2\").toString();\n                Log.i(TAG, \"request2: \" + sendb);\n                os = soc.getOutputStream();\n                os.write(sendb.getBytes());\n                os.write(\"endbidou1\".getBytes());\n                os.flush();\n                Log.i(TAG, \"request2 send successful\");\n\n                is = soc.getInputStream();\n                byte buffer[] = new byte[1024 * 100];\n                is.read(buffer);\n                Log.i(TAG, \"midifilecontent: \" + buffer.toString());\n                soc.close();\n                File filemid = new File(Environment.getExternalStorageDirectory().getAbsolutePath() + \"/data/files/Melodia.mid\");\n                FileOutputStream writer = null;\n                writer = new FileOutputStream(filemid);\n                writer.write(buffer);\n                writer.close();\n                Message msg = myhandler.obtainMessage();\n                msg.what = 1;\n                myhandler.sendMessage(msg);\n            } catch (IOException e) {\n                e.printStackTrace();\n            }\n\n\n        }\n    }\n```\n\n# 录音特效\n录音图像动画效果来自Github：[ShineButton](https://github.com/ChadCSong/ShineButton)\n另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消\n```Java\n\tfabrecord.setOnTouchListener(new View.OnTouchListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onTouch(View v, MotionEvent event) {\n\t\t\t\t\tswitch (event.getAction()) {\n\t\t\t\t\t\tcase MotionEvent.ACTION_DOWN:\n\t\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);\n\t\t\t\t\t\t\tif (isUploadingIcon) {\n\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_DOWN\");\n\t\t\t\t\t\t\tif (!shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tox = event.getX();\n\t\t\t\t\t\t\toy = event.getY();\n\n\t\t\t\t\t\t\tisRecording = true;\n\t\t\t\t\t\t\trecLen = 0;\n\t\t\t\t\t\t\trecTime = 0;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"开始录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\t\t\trecorder = new RecordTask();\n\t\t\t\t\t\t\trecorder.execute();\n\t\t\t\t\t\t\thandler.postDelayed(runrecord, 0);\n\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_UP:\n\t\t\t\t\t\t\thandler.removeCallbacks(runrecord);\n\t\t\t\t\t\t\tLog.i(TAG, \"ACTION_UP\");\n\t\t\t\t\t\t\tif (shinebtstatus) {\n\t\t\t\t\t\t\t\tshinebt.performClick();\n\t\t\t\t\t\t\t\tshinebtstatus = false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfloat x1 = event.getX();\n\t\t\t\t\t\t\tfloat y1 = event.getY();\n\t\t\t\t\t\t\tfloat dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);\n\n\t\t\t\t\t\t\tisRecording = false;\n\t\t\t\t\t\t\tpb.setValue(0);\n\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);\n\t\t\t\t\t\t\tif (dis1 > 30000) {\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"取消录音\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tif (!isUploadingIcon) {\n\t\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);\n\t\t\t\t\t\t\t\t\tisPressUpload = false;\n\t\t\t\t\t\t\t\t\tuploadbt.performClick();\n\t\t\t\t\t\t\t\t\tisPressUpload = true;\n\t\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;\n\t\t\t\t\t\t\t\t} else {\n\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tSnackbar.make(fabrecord, \"录音完成\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\t\t\thandler.postDelayed(runreplay, 0);\n\t\t\t\t\t\t\t\treplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase MotionEvent.ACTION_MOVE:\n\t\t\t\t\t\t\tfloat x2 = event.getX();\n\t\t\t\t\t\t\tfloat y2 = event.getY();\n\t\t\t\t\t\t\tfloat dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);\n\t\t\t\t\t\t\tif (dis2 > 30000) {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```\n# 展示乐谱\n-\t本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享\n```Java\n    public void init() {\n        md5 = getArguments().getString(\"md5\");\n        final String imageUri = \"服务器地址\" + md5 + \"_1.png\";\n        Log.i(\"play\", \"pngfile: \" + imageUri);\n        new Handler().postDelayed(new Runnable() {\n            public void run() {\n                //execute the task\n                imageLoader.displayImage(imageUri, showpic);\n            }\n        }, 2000);\n\n    }\n```\n\n# 与电子琴通信\n-\t类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接\n```Java\n\tpianobt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tif (!isconnected) {\n\t\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\t\tparam[0] = 0x30;\n\t\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\t\tst.start();\n\t\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\t\tms.start();\n\t\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tisconnected = false;\n\t\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\t\t\t\t\t}\n\n\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tsamplebt.setOnClickListener(new View.OnClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic void onClick(View v) {\n\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();\n\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());\n\t\t\t\t\tparam[0] = 0x31;\n\t\t\t\t\tStartThread st = new StartThread();\n\t\t\t\t\tst.start();\n\t\t\t\t\twhile (!isconnected) ;\n\t\t\t\t\tMsgThread ms = new MsgThread();\n\t\t\t\t\tms.start();\n\t\t\t\t\tYoYo.with(Techniques.Wobble)\n\t\t\t\t\t\t\t.duration(300)\n\t\t\t\t\t\t\t.repeat(6)\n\t\t\t\t\t\t\t.playOn(seekBaroctave);\n\t\t\t\t\twhile (soc.isConnected()) ;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t\tisconnected = false;\n\t\t\t\t\tLog.i(\"piano\", \"socket closed\");\n\n\t\t\t\t}\n\t\t\t});\n\n\n\t\t}\n\n\t\tprivate class StartThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tsoc = new Socket(pianoaddr, pianoport);\n\t\t\t\t\tif (soc.isConnected()) {//成功连接获取soc对象则发送成功消息\n\t\t\t\t\t\tLog.i(\"piano\", \"piano is Connected\");\n\t\t\t\t\t\tif (!isconnected)\n\t\t\t\t\t\t\tisconnected = !isconnected;\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\t\tsoc.close();\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\tLog.i(\"piano\", \"Connect Failed\");\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tprivate class MsgThread extends Thread {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tOutputStream os = soc.getOutputStream();\n\t\t\t\t\tos.write(param);\n\t\t\t\t\tos.flush();\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful\");\n\t\t\t\t\tSnackbar.make(pianobt, \"正在启动启动电子琴教学\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\n\t\t\t\t\tsoc.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLog.i(\"piano\", \"piano msg send successful failed\");\n\t\t\t\t\tSnackbar.make(pianobt, \"启动电子琴教学失败\", Snackbar.LENGTH_SHORT)\n\t\t\t\t\t\t\t.setAction(\"Action\", null).show();\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n\n# 乐谱分享\n-\t显示乐谱的是Github上一个魔改的ImageView:[PinchImageView](https://github.com/boycy815/PinchImageView)\n-\t定义其长按事件，触发一个分享的intent\n```Java\n\tshowpic.setOnLongClickListener(new View.OnLongClickListener() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean onLongClick(View v) {\n\t\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);\n\t\t\t\t\tif (drawingCache == null) {\n\t\t\t\t\t\tLog.i(\"play\", \"no img to save\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tFile imageFile = new File(Environment.getExternalStorageDirectory(), \"saveImageview.jpg\");\n\t\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),\n\t\t\t\t\t\t\t\t\t\"\", Toast.LENGTH_LONG);\n\t\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, 0, 200);\n\t\t\t\t\t\t\ttoast.setText(\"分享图片\");\n\t\t\t\t\t\t\ttoast.show();\n\t\t\t\t\t\t\tFileOutputStream outStream;\n\t\t\t\t\t\t\toutStream = new FileOutputStream(imageFile);\n\t\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, 100, outStream);\n\t\t\t\t\t\t\toutStream.flush();\n\t\t\t\t\t\t\toutStream.close();\n\n\t\t\t\t\t\t\tIntent sendIntent = new Intent();\n\t\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);\n\t\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));\n\t\t\t\t\t\t\tsendIntent.setType(\"image/png\");\n\t\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, \"分享到\"));\n\n\t\t\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t\t\tLog.i(\"play\", \"share img wrong\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t});\n```","slug":"dachuang","published":1,"updated":"2019-07-22T03:45:23.063Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vig005sq8t53xxmwcmf","content":"<hr>\n<p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o26O.gif\" alt=\"i0o26O.gif\"><br>封面图使用<a href=\"https://github.com/qiao\" target=\"_blank\" rel=\"noopener\">qiao</a>的midi在线可视化工具<a href=\"https://github.com/qiao/euphony\" target=\"_blank\" rel=\"noopener\">euphony</a></p>\n<h1 id=\"midi播放\"><a href=\"#midi播放\" class=\"headerlink\" title=\"midi播放\"></a>midi播放</h1><p>调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放</p>\n<ul>\n<li>MediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放</li>\n<li>从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)</li>\n<li>Uri或者外部存储读取new-&gt;setDataSource-&gt;prepare-&gt;start</li>\n</ul>\n<h1 id=\"录制声音并重放\"><a href=\"#录制声音并重放\" class=\"headerlink\" title=\"录制声音并重放\"></a>录制声音并重放</h1><p>参考<a href=\"http://blog.csdn.net/jiangliloveyou/article/details/11218555\" target=\"_blank\" rel=\"noopener\">android中AudioRecord使用</a><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">AsyncTask</span>&lt;<span class=\"title\">Void</span>, <span class=\"title\">Integer</span>, <span class=\"title\">Void</span>&gt; </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">protected</span> Void <span class=\"title\">doInBackground</span><span class=\"params\">(Void... arg0)</span> </span>&#123;</span><br><span class=\"line\">\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开通输出流到指定的文件</span></span><br><span class=\"line\">\t\t\tDataOutputStream dos = <span class=\"keyword\">new</span> DataOutputStream(<span class=\"keyword\">new</span> BufferedOutputStream(<span class=\"keyword\">new</span> FileOutputStream(pcmFile)));</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//根据定义好的几个配置，来获取合适的缓冲大小</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//实例化AudioRecord</span></span><br><span class=\"line\">\t\t\tAudioRecord record = <span class=\"keyword\">new</span> AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义缓冲</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">short</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">short</span>[bufferSize];</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开始录制</span></span><br><span class=\"line\">\t\t\trecord.startRecording();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> r = <span class=\"number\">0</span>; <span class=\"comment\">//存储录制进度</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义循环，根据isRecording的值来判断是否继续录制</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (isRecording) &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//从bufferSize中读取字节，返回读取的short个数</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">int</span> bufferReadResult = record.read(buffer, <span class=\"number\">0</span>, buffer.length);</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//循环将buffer中的音频数据写入到OutputStream中</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; bufferReadResult; i++) &#123;</span><br><span class=\"line\">\t\t\t\t\tdos.writeShort(buffer[i]);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tpublishProgress(<span class=\"keyword\">new</span> Integer(r)); <span class=\"comment\">//向UI线程报告当前进度</span></span><br><span class=\"line\">\t\t\t\tr++; <span class=\"comment\">//自增进度值</span></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//录制结束</span></span><br><span class=\"line\">\t\t\trecord.stop();</span><br><span class=\"line\">\t\t\tconvertWaveFile();</span><br><span class=\"line\">\t\t\tdos.close();</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"pcm写头文件转成wav\"><a href=\"#pcm写头文件转成wav\" class=\"headerlink\" title=\"pcm写头文件转成wav\"></a>pcm写头文件转成wav</h1><p>因为录制的是裸文件，pcm格式，需要自己加上wav头<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">WriteWaveFileHeader</span><span class=\"params\">(FileOutputStream out, <span class=\"keyword\">long</span> totalAudioLen, <span class=\"keyword\">long</span> totalDataLen, <span class=\"keyword\">long</span> longSampleRate,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\t\t\t\t\t\t\t <span class=\"keyword\">int</span> channels, <span class=\"keyword\">long</span> byteRate)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] header = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">45</span>];</span><br><span class=\"line\">\theader[<span class=\"number\">0</span>] = <span class=\"string\">'R'</span>; <span class=\"comment\">// RIFF</span></span><br><span class=\"line\">\theader[<span class=\"number\">1</span>] = <span class=\"string\">'I'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">2</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">3</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">4</span>] = (<span class=\"keyword\">byte</span>) (totalDataLen &amp; <span class=\"number\">0xff</span>);<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">5</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">6</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">7</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">8</span>] = <span class=\"string\">'W'</span>;<span class=\"comment\">//WAVE</span></span><br><span class=\"line\">\theader[<span class=\"number\">9</span>] = <span class=\"string\">'A'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">10</span>] = <span class=\"string\">'V'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">11</span>] = <span class=\"string\">'E'</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//FMT Chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">12</span>] = <span class=\"string\">'f'</span>; <span class=\"comment\">// 'fmt '</span></span><br><span class=\"line\">\theader[<span class=\"number\">13</span>] = <span class=\"string\">'m'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">14</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">15</span>] = <span class=\"string\">' '</span>;<span class=\"comment\">//过渡字节</span></span><br><span class=\"line\">\t<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">16</span>] = <span class=\"number\">16</span>; <span class=\"comment\">// 4 bytes: size of 'fmt ' chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">17</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">18</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">19</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//编码方式 10H为PCM编码格式</span></span><br><span class=\"line\">\theader[<span class=\"number\">20</span>] = <span class=\"number\">1</span>; <span class=\"comment\">// format = 1</span></span><br><span class=\"line\">\theader[<span class=\"number\">21</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//通道数</span></span><br><span class=\"line\">\theader[<span class=\"number\">22</span>] = (<span class=\"keyword\">byte</span>) channels;</span><br><span class=\"line\">\theader[<span class=\"number\">23</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//采样率，每个通道的播放速度</span></span><br><span class=\"line\">\theader[<span class=\"number\">24</span>] = (<span class=\"keyword\">byte</span>) (longSampleRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">25</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">26</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">27</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">//音频数据传送速率,采样率*通道数*采样深度/8</span></span><br><span class=\"line\">\theader[<span class=\"number\">28</span>] = (<span class=\"keyword\">byte</span>) (byteRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">29</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">30</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">31</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">32</span>] = (<span class=\"keyword\">byte</span>) (<span class=\"number\">1</span> * <span class=\"number\">16</span> / <span class=\"number\">8</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">33</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//每个样本的数据位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">34</span>] = <span class=\"number\">16</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">35</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//Data chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">36</span>] = <span class=\"string\">'d'</span>;<span class=\"comment\">//data</span></span><br><span class=\"line\">\theader[<span class=\"number\">37</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">38</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">39</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">40</span>] = (<span class=\"keyword\">byte</span>) (totalAudioLen &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">41</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">42</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">43</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">44</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tout.write(header, <span class=\"number\">0</span>, <span class=\"number\">45</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"json收发\"><a href=\"#json收发\" class=\"headerlink\" title=\"json收发\"></a>json收发</h1><p>根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中<br>json发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> JSONObject <span class=\"title\">makejson</span><span class=\"params\">(<span class=\"keyword\">int</span> request, String identifycode, String data)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (identifycode == <span class=\"string\">\"a\"</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            JSONObject config = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            config.put(<span class=\"string\">\"n\"</span>, lowf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"m\"</span>, highf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"w\"</span>, interval);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, config);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, data);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, identifycode);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"socket通信\"><a href=\"#socket通信\" class=\"headerlink\" title=\"socket通信\"></a>socket通信</h1><p>单开一个线程用于启动socket，再开一个线程写两次json收发<br>注意收发json时将json字符串用base64解码编码，java自己的string会存在错误<br>另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，”endbidou”，不要问我是什么意思，做转换算法的兄弟想的<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            File file = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.wav\"</span>);</span><br><span class=\"line\">            FileInputStream reader = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                reader = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> len = reader.available();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span>[] buff = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[len];</span><br><span class=\"line\">                reader.read(buff);</span><br><span class=\"line\">                String data = Base64.encodeToString(buff, Base64.DEFAULT);</span><br><span class=\"line\">                String senda = makejson(<span class=\"number\">1</span>, <span class=\"string\">\"a\"</span>, data).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request1: \"</span> + senda);</span><br><span class=\"line\">                OutputStream os = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                InputStream is = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                DataInputStream in = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    os = soc.getOutputStream();</span><br><span class=\"line\">                    BufferedReader bra = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                    os.write(senda.getBytes());</span><br><span class=\"line\">                    os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                    os.flush();</span><br><span class=\"line\">                    Log.i(TAG, <span class=\"string\">\"request1 send successful\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (soc.isConnected()) &#123;</span><br><span class=\"line\">                        is = soc.getInputStream();</span><br><span class=\"line\">                        bra = <span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(is));</span><br><span class=\"line\">                        md5 = bra.readLine();</span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"md5: \"</span> + md5);</span><br><span class=\"line\">                        bra.close();</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"socket closed while reading\"</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    e.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                startflag = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                StartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">                st.start();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">while</span> (soc.isClosed()) ;</span><br><span class=\"line\"></span><br><span class=\"line\">                String sendb = makejson(<span class=\"number\">2</span>, md5, <span class=\"string\">\"request2\"</span>).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2: \"</span> + sendb);</span><br><span class=\"line\">                os = soc.getOutputStream();</span><br><span class=\"line\">                os.write(sendb.getBytes());</span><br><span class=\"line\">                os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                os.flush();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2 send successful\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                is = soc.getInputStream();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span> buffer[] = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024</span> * <span class=\"number\">100</span>];</span><br><span class=\"line\">                is.read(buffer);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"midifilecontent: \"</span> + buffer.toString());</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                File filemid = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.mid\"</span>);</span><br><span class=\"line\">                FileOutputStream writer = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                writer = <span class=\"keyword\">new</span> FileOutputStream(filemid);</span><br><span class=\"line\">                writer.write(buffer);</span><br><span class=\"line\">                writer.close();</span><br><span class=\"line\">                Message msg = myhandler.obtainMessage();</span><br><span class=\"line\">                msg.what = <span class=\"number\">1</span>;</span><br><span class=\"line\">                myhandler.sendMessage(msg);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"录音特效\"><a href=\"#录音特效\" class=\"headerlink\" title=\"录音特效\"></a>录音特效</h1><p>录音图像动画效果来自Github：<a href=\"https://github.com/ChadCSong/ShineButton\" target=\"_blank\" rel=\"noopener\">ShineButton</a><br>另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fabrecord.setOnTouchListener(<span class=\"keyword\">new</span> View.OnTouchListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onTouch</span><span class=\"params\">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">switch</span> (event.getAction()) &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_DOWN:</span><br><span class=\"line\">\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_DOWN\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (!shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tox = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\toy = event.getY();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecTime = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"开始录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\trecorder = <span class=\"keyword\">new</span> RecordTask();</span><br><span class=\"line\">\t\t\t\t\t\trecorder.execute();</span><br><span class=\"line\">\t\t\t\t\t\thandler.postDelayed(runrecord, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_UP:</span><br><span class=\"line\">\t\t\t\t\t\thandler.removeCallbacks(runrecord);</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_UP\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x1 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y1 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis1 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"取消录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">if</span> (!isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"录音完成\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t\thandler.postDelayed(runreplay, <span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\t\treplay();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_MOVE:</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x2 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y2 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis2 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"展示乐谱\"><a href=\"#展示乐谱\" class=\"headerlink\" title=\"展示乐谱\"></a>展示乐谱</h1><ul>\n<li>本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    md5 = getArguments().getString(<span class=\"string\">\"md5\"</span>);</span><br><span class=\"line\">    <span class=\"keyword\">final</span> String imageUri = <span class=\"string\">\"服务器地址\"</span> + md5 + <span class=\"string\">\"_1.png\"</span>;</span><br><span class=\"line\">    Log.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"pngfile: \"</span> + imageUri);</span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            imageLoader.displayImage(imageUri, showpic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"与电子琴通信\"><a href=\"#与电子琴通信\" class=\"headerlink\" title=\"与电子琴通信\"></a>与电子琴通信</h1><ul>\n<li>类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pianobt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isconnected) &#123;</span><br><span class=\"line\">\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x30</span>;</span><br><span class=\"line\">\t\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsamplebt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x31</span>;</span><br><span class=\"line\">\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StartThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tsoc = <span class=\"keyword\">new</span> Socket(pianoaddr, pianoport);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (soc.isConnected()) &#123;<span class=\"comment\">//成功连接获取soc对象则发送成功消息</span></span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano is Connected\"</span>);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> (!isconnected)</span><br><span class=\"line\">\t\t\t\t\t\tisconnected = !isconnected;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tOutputStream os = soc.getOutputStream();</span><br><span class=\"line\">\t\t\t\tos.write(param);</span><br><span class=\"line\">\t\t\t\tos.flush();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"正在启动启动电子琴教学\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful failed\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"乐谱分享\"><a href=\"#乐谱分享\" class=\"headerlink\" title=\"乐谱分享\"></a>乐谱分享</h1><ul>\n<li>显示乐谱的是Github上一个魔改的ImageView:<a href=\"https://github.com/boycy815/PinchImageView\" target=\"_blank\" rel=\"noopener\">PinchImageView</a></li>\n<li>定义其长按事件，触发一个分享的intent<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">showpic.setOnLongClickListener(<span class=\"keyword\">new</span> View.OnLongClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onLongClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (drawingCache == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"no img to save\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tFile imageFile = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory(), <span class=\"string\">\"saveImageview.jpg\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t<span class=\"string\">\"\"</span>, Toast.LENGTH_LONG);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, <span class=\"number\">0</span>, <span class=\"number\">200</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setText(<span class=\"string\">\"分享图片\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.show();</span><br><span class=\"line\">\t\t\t\t\t\tFileOutputStream outStream;</span><br><span class=\"line\">\t\t\t\t\t\toutStream = <span class=\"keyword\">new</span> FileOutputStream(imageFile);</span><br><span class=\"line\">\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, <span class=\"number\">100</span>, outStream);</span><br><span class=\"line\">\t\t\t\t\t\toutStream.flush();</span><br><span class=\"line\">\t\t\t\t\t\toutStream.close();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tIntent sendIntent = <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setType(<span class=\"string\">\"image/png\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, <span class=\"string\">\"分享到\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"share img wrong\"</span>);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0o26O.gif\" alt=\"i0o26O.gif\"><br>封面图使用<a href=\"https://github.com/qiao\" target=\"_blank\" rel=\"noopener\">qiao</a>的midi在线可视化工具<a href=\"https://github.com/qiao/euphony\" target=\"_blank\" rel=\"noopener\">euphony</a></p>\n<h1 id=\"midi播放\"><a href=\"#midi播放\" class=\"headerlink\" title=\"midi播放\"></a>midi播放</h1><p>调用MediaPlayer类播放，因为不可抗因素，只能用android5.1，没有midi库，就做简单的播放</p>\n<ul>\n<li>MediaPlayer可以用外部存储，assert,自建raw文件夹或者uri四种方式访问媒体文件并播放</li>\n<li>从raw文件夹中读取可以直接用player = MediaPlayer.create(this, R.raw.test1)</li>\n<li>Uri或者外部存储读取new-&gt;setDataSource-&gt;prepare-&gt;start</li>\n</ul>\n<h1 id=\"录制声音并重放\"><a href=\"#录制声音并重放\" class=\"headerlink\" title=\"录制声音并重放\"></a>录制声音并重放</h1><p>参考<a href=\"http://blog.csdn.net/jiangliloveyou/article/details/11218555\" target=\"_blank\" rel=\"noopener\">android中AudioRecord使用</a><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordTask</span> <span class=\"keyword\">extends</span> <span class=\"title\">AsyncTask</span>&lt;<span class=\"title\">Void</span>, <span class=\"title\">Integer</span>, <span class=\"title\">Void</span>&gt; </span>&#123;</span><br><span class=\"line\">\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">protected</span> Void <span class=\"title\">doInBackground</span><span class=\"params\">(Void... arg0)</span> </span>&#123;</span><br><span class=\"line\">\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开通输出流到指定的文件</span></span><br><span class=\"line\">\t\t\tDataOutputStream dos = <span class=\"keyword\">new</span> DataOutputStream(<span class=\"keyword\">new</span> BufferedOutputStream(<span class=\"keyword\">new</span> FileOutputStream(pcmFile)));</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//根据定义好的几个配置，来获取合适的缓冲大小</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> bufferSize = AudioRecord.getMinBufferSize(audioRate, channelConfig, audioEncoding);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//实例化AudioRecord</span></span><br><span class=\"line\">\t\t\tAudioRecord record = <span class=\"keyword\">new</span> AudioRecord(MediaRecorder.AudioSource.MIC, audioRate, channelConfig, audioEncoding, bufferSize);</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义缓冲</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">short</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">short</span>[bufferSize];</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//开始录制</span></span><br><span class=\"line\">\t\t\trecord.startRecording();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">int</span> r = <span class=\"number\">0</span>; <span class=\"comment\">//存储录制进度</span></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//定义循环，根据isRecording的值来判断是否继续录制</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">while</span> (isRecording) &#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//从bufferSize中读取字节，返回读取的short个数</span></span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//这里老是出现buffer overflow，不知道是什么原因，试了好几个值，都没用，TODO：待解决</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">int</span> bufferReadResult = record.read(buffer, <span class=\"number\">0</span>, buffer.length);</span><br><span class=\"line\">\t\t\t\t<span class=\"comment\">//循环将buffer中的音频数据写入到OutputStream中</span></span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; bufferReadResult; i++) &#123;</span><br><span class=\"line\">\t\t\t\t\tdos.writeShort(buffer[i]);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tpublishProgress(<span class=\"keyword\">new</span> Integer(r)); <span class=\"comment\">//向UI线程报告当前进度</span></span><br><span class=\"line\">\t\t\t\tr++; <span class=\"comment\">//自增进度值</span></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//录制结束</span></span><br><span class=\"line\">\t\t\trecord.stop();</span><br><span class=\"line\">\t\t\tconvertWaveFile();</span><br><span class=\"line\">\t\t\tdos.close();</span><br><span class=\"line\">\t\t&#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"pcm写头文件转成wav\"><a href=\"#pcm写头文件转成wav\" class=\"headerlink\" title=\"pcm写头文件转成wav\"></a>pcm写头文件转成wav</h1><p>因为录制的是裸文件，pcm格式，需要自己加上wav头<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">WriteWaveFileHeader</span><span class=\"params\">(FileOutputStream out, <span class=\"keyword\">long</span> totalAudioLen, <span class=\"keyword\">long</span> totalDataLen, <span class=\"keyword\">long</span> longSampleRate,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">\t\t\t\t\t\t\t\t <span class=\"keyword\">int</span> channels, <span class=\"keyword\">long</span> byteRate)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">byte</span>[] header = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">45</span>];</span><br><span class=\"line\">\theader[<span class=\"number\">0</span>] = <span class=\"string\">'R'</span>; <span class=\"comment\">// RIFF</span></span><br><span class=\"line\">\theader[<span class=\"number\">1</span>] = <span class=\"string\">'I'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">2</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">3</span>] = <span class=\"string\">'F'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">4</span>] = (<span class=\"keyword\">byte</span>) (totalDataLen &amp; <span class=\"number\">0xff</span>);<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">5</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">6</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">7</span>] = (<span class=\"keyword\">byte</span>) ((totalDataLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">8</span>] = <span class=\"string\">'W'</span>;<span class=\"comment\">//WAVE</span></span><br><span class=\"line\">\theader[<span class=\"number\">9</span>] = <span class=\"string\">'A'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">10</span>] = <span class=\"string\">'V'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">11</span>] = <span class=\"string\">'E'</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//FMT Chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">12</span>] = <span class=\"string\">'f'</span>; <span class=\"comment\">// 'fmt '</span></span><br><span class=\"line\">\theader[<span class=\"number\">13</span>] = <span class=\"string\">'m'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">14</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">15</span>] = <span class=\"string\">' '</span>;<span class=\"comment\">//过渡字节</span></span><br><span class=\"line\">\t<span class=\"comment\">//数据大小</span></span><br><span class=\"line\">\theader[<span class=\"number\">16</span>] = <span class=\"number\">16</span>; <span class=\"comment\">// 4 bytes: size of 'fmt ' chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">17</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">18</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">19</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//编码方式 10H为PCM编码格式</span></span><br><span class=\"line\">\theader[<span class=\"number\">20</span>] = <span class=\"number\">1</span>; <span class=\"comment\">// format = 1</span></span><br><span class=\"line\">\theader[<span class=\"number\">21</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//通道数</span></span><br><span class=\"line\">\theader[<span class=\"number\">22</span>] = (<span class=\"keyword\">byte</span>) channels;</span><br><span class=\"line\">\theader[<span class=\"number\">23</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//采样率，每个通道的播放速度</span></span><br><span class=\"line\">\theader[<span class=\"number\">24</span>] = (<span class=\"keyword\">byte</span>) (longSampleRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">25</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">26</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">27</span>] = (<span class=\"keyword\">byte</span>) ((longSampleRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">//音频数据传送速率,采样率*通道数*采样深度/8</span></span><br><span class=\"line\">\theader[<span class=\"number\">28</span>] = (<span class=\"keyword\">byte</span>) (byteRate &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">29</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">30</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">31</span>] = (<span class=\"keyword\">byte</span>) ((byteRate &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 确定系统一次要处理多少个这样字节的数据，确定缓冲区，通道数*采样位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">32</span>] = (<span class=\"keyword\">byte</span>) (<span class=\"number\">1</span> * <span class=\"number\">16</span> / <span class=\"number\">8</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">33</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//每个样本的数据位数</span></span><br><span class=\"line\">\theader[<span class=\"number\">34</span>] = <span class=\"number\">16</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">35</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"comment\">//Data chunk</span></span><br><span class=\"line\">\theader[<span class=\"number\">36</span>] = <span class=\"string\">'d'</span>;<span class=\"comment\">//data</span></span><br><span class=\"line\">\theader[<span class=\"number\">37</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">38</span>] = <span class=\"string\">'t'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">39</span>] = <span class=\"string\">'a'</span>;</span><br><span class=\"line\">\theader[<span class=\"number\">40</span>] = (<span class=\"keyword\">byte</span>) (totalAudioLen &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">41</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">8</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">42</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">16</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">43</span>] = (<span class=\"keyword\">byte</span>) ((totalAudioLen &gt;&gt; <span class=\"number\">24</span>) &amp; <span class=\"number\">0xff</span>);</span><br><span class=\"line\">\theader[<span class=\"number\">44</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">\tout.write(header, <span class=\"number\">0</span>, <span class=\"number\">45</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"json收发\"><a href=\"#json收发\" class=\"headerlink\" title=\"json收发\"></a>json收发</h1><p>根据我们的实际情况，发送时使用json，存三个参数和wav内容，因为录音的wav时长较短，可以把整个wav写入json中<br>json发送两次，第一次发送参数和文件，拿到md5编码的时间戳，第二次把这个时间戳加入json中请求相应的midi文件<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> JSONObject <span class=\"title\">makejson</span><span class=\"params\">(<span class=\"keyword\">int</span> request, String identifycode, String data)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (identifycode == <span class=\"string\">\"a\"</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            JSONObject config = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            config.put(<span class=\"string\">\"n\"</span>, lowf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"m\"</span>, highf);</span><br><span class=\"line\">            config.put(<span class=\"string\">\"w\"</span>, interval);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, config);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, data);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            JSONObject pack = <span class=\"keyword\">new</span> JSONObject();</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"request\"</span>, request);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"config\"</span>, <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">            pack.put(<span class=\"string\">\"data\"</span>, identifycode);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> pack;</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> (JSONException e) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"socket通信\"><a href=\"#socket通信\" class=\"headerlink\" title=\"socket通信\"></a>socket通信</h1><p>单开一个线程用于启动socket，再开一个线程写两次json收发<br>注意收发json时将json字符串用base64解码编码，java自己的string会存在错误<br>另外因为wav字符串较长，服务器接收时分块接收，正常做法是加一个字典项存wav长度，按长度读取wav，然后这里我们偷懒直接在文件尾加了一个特殊字符段用于判断是否接收完成，”endbidou”，不要问我是什么意思，做转换算法的兄弟想的<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            File file = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.wav\"</span>);</span><br><span class=\"line\">            FileInputStream reader = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">            <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                reader = <span class=\"keyword\">new</span> FileInputStream(file);</span><br><span class=\"line\">                <span class=\"keyword\">int</span> len = reader.available();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span>[] buff = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[len];</span><br><span class=\"line\">                reader.read(buff);</span><br><span class=\"line\">                String data = Base64.encodeToString(buff, Base64.DEFAULT);</span><br><span class=\"line\">                String senda = makejson(<span class=\"number\">1</span>, <span class=\"string\">\"a\"</span>, data).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request1: \"</span> + senda);</span><br><span class=\"line\">                OutputStream os = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                InputStream is = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                DataInputStream in = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                    os = soc.getOutputStream();</span><br><span class=\"line\">                    BufferedReader bra = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                    os.write(senda.getBytes());</span><br><span class=\"line\">                    os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                    os.flush();</span><br><span class=\"line\">                    Log.i(TAG, <span class=\"string\">\"request1 send successful\"</span>);</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (soc.isConnected()) &#123;</span><br><span class=\"line\">                        is = soc.getInputStream();</span><br><span class=\"line\">                        bra = <span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(is));</span><br><span class=\"line\">                        md5 = bra.readLine();</span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"md5: \"</span> + md5);</span><br><span class=\"line\">                        bra.close();</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span></span><br><span class=\"line\">                        Log.i(TAG, <span class=\"string\">\"socket closed while reading\"</span>);</span><br><span class=\"line\">                &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                    e.printStackTrace();</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                startflag = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">                StartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">                st.start();</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">while</span> (soc.isClosed()) ;</span><br><span class=\"line\"></span><br><span class=\"line\">                String sendb = makejson(<span class=\"number\">2</span>, md5, <span class=\"string\">\"request2\"</span>).toString();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2: \"</span> + sendb);</span><br><span class=\"line\">                os = soc.getOutputStream();</span><br><span class=\"line\">                os.write(sendb.getBytes());</span><br><span class=\"line\">                os.write(<span class=\"string\">\"endbidou1\"</span>.getBytes());</span><br><span class=\"line\">                os.flush();</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"request2 send successful\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">                is = soc.getInputStream();</span><br><span class=\"line\">                <span class=\"keyword\">byte</span> buffer[] = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024</span> * <span class=\"number\">100</span>];</span><br><span class=\"line\">                is.read(buffer);</span><br><span class=\"line\">                Log.i(TAG, <span class=\"string\">\"midifilecontent: \"</span> + buffer.toString());</span><br><span class=\"line\">                soc.close();</span><br><span class=\"line\">                File filemid = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory().getAbsolutePath() + <span class=\"string\">\"/data/files/Melodia.mid\"</span>);</span><br><span class=\"line\">                FileOutputStream writer = <span class=\"keyword\">null</span>;</span><br><span class=\"line\">                writer = <span class=\"keyword\">new</span> FileOutputStream(filemid);</span><br><span class=\"line\">                writer.write(buffer);</span><br><span class=\"line\">                writer.close();</span><br><span class=\"line\">                Message msg = myhandler.obtainMessage();</span><br><span class=\"line\">                msg.what = <span class=\"number\">1</span>;</span><br><span class=\"line\">                myhandler.sendMessage(msg);</span><br><span class=\"line\">            &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">                e.printStackTrace();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"录音特效\"><a href=\"#录音特效\" class=\"headerlink\" title=\"录音特效\"></a>录音特效</h1><p>录音图像动画效果来自Github：<a href=\"https://github.com/ChadCSong/ShineButton\" target=\"_blank\" rel=\"noopener\">ShineButton</a><br>另外录音按钮做了个效果，按住录音，松开完成，往外滑一定距离取消<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fabrecord.setOnTouchListener(<span class=\"keyword\">new</span> View.OnTouchListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onTouch</span><span class=\"params\">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">switch</span> (event.getAction()) &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_DOWN:</span><br><span class=\"line\">\t\t\t\t\t\tuploadbt.setVisibility(View.INVISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_DOWN\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (!shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tox = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\toy = event.getY();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\trecTime = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"开始录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\trecorder = <span class=\"keyword\">new</span> RecordTask();</span><br><span class=\"line\">\t\t\t\t\t\trecorder.execute();</span><br><span class=\"line\">\t\t\t\t\t\thandler.postDelayed(runrecord, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_UP:</span><br><span class=\"line\">\t\t\t\t\t\thandler.removeCallbacks(runrecord);</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(TAG, <span class=\"string\">\"ACTION_UP\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (shinebtstatus) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\tshinebtstatus = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x1 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y1 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis1 = (x1 - ox) * (x1 - ox) + (y1 - oy) * (y1 - oy);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tisRecording = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\tpb.setValue(<span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_fiber_manual_record_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis1 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"取消录音\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t<span class=\"keyword\">if</span> (!isUploadingIcon) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.setVisibility(View.VISIBLE);</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tuploadbt.performClick();</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisPressUpload = <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t\t\t\t\t\tisUploadingIcon = !isUploadingIcon;</span><br><span class=\"line\">\t\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\t\tSnackbar.make(fabrecord, <span class=\"string\">\"录音完成\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\t\t\thandler.postDelayed(runreplay, <span class=\"number\">0</span>);</span><br><span class=\"line\">\t\t\t\t\t\t\treplay();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">case</span> MotionEvent.ACTION_MOVE:</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> x2 = event.getX();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> y2 = event.getY();</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">float</span> dis2 = (x2 - ox) * (x2 - ox) + (y2 - oy) * (y2 - oy);</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">if</span> (dis2 &gt; <span class=\"number\">30000</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_cancel_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tfabrecord.setImageResource(R.drawable.ic_stop_white_24dp);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\t<span class=\"keyword\">break</span>;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"展示乐谱\"><a href=\"#展示乐谱\" class=\"headerlink\" title=\"展示乐谱\"></a>展示乐谱</h1><ul>\n<li>本来是想通过socket收发图片，后来觉得太麻烦于是把方案改成Apache对每一次转换生成相应的图片链接，通过时间戳md5直接在线访问，如果需要分享图片则先存到本地再分享<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    md5 = getArguments().getString(<span class=\"string\">\"md5\"</span>);</span><br><span class=\"line\">    <span class=\"keyword\">final</span> String imageUri = <span class=\"string\">\"服务器地址\"</span> + md5 + <span class=\"string\">\"_1.png\"</span>;</span><br><span class=\"line\">    Log.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"pngfile: \"</span> + imageUri);</span><br><span class=\"line\">    <span class=\"keyword\">new</span> Handler().postDelayed(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"comment\">//execute the task</span></span><br><span class=\"line\">            imageLoader.displayImage(imageUri, showpic);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;, <span class=\"number\">2000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"与电子琴通信\"><a href=\"#与电子琴通信\" class=\"headerlink\" title=\"与电子琴通信\"></a>与电子琴通信</h1><ul>\n<li>类似于上传服务器，也是socket通信，电子琴改装了之后从手机客户端接收八度、速度两个参数，arduino接收到参数就播放，并由arduino断开连接<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pianobt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (!isconnected) &#123;</span><br><span class=\"line\">\t\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x30</span>;</span><br><span class=\"line\">\t\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tsamplebt.setOnClickListener(<span class=\"keyword\">new</span> View.OnClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tpianoaddr = etpianoaddr.getText().toString();</span><br><span class=\"line\">\t\t\t\tpianoport = Integer.valueOf(etpianoport.getText().toString());</span><br><span class=\"line\">\t\t\t\tparam[<span class=\"number\">0</span>] = <span class=\"number\">0x31</span>;</span><br><span class=\"line\">\t\t\t\tStartThread st = <span class=\"keyword\">new</span> StartThread();</span><br><span class=\"line\">\t\t\t\tst.start();</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (!isconnected) ;</span><br><span class=\"line\">\t\t\t\tMsgThread ms = <span class=\"keyword\">new</span> MsgThread();</span><br><span class=\"line\">\t\t\t\tms.start();</span><br><span class=\"line\">\t\t\t\tYoYo.with(Techniques.Wobble)</span><br><span class=\"line\">\t\t\t\t\t\t.duration(<span class=\"number\">300</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.repeat(<span class=\"number\">6</span>)</span><br><span class=\"line\">\t\t\t\t\t\t.playOn(seekBaroctave);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">while</span> (soc.isConnected()) ;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\tisconnected = <span class=\"keyword\">false</span>;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"socket closed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StartThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tsoc = <span class=\"keyword\">new</span> Socket(pianoaddr, pianoport);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (soc.isConnected()) &#123;<span class=\"comment\">//成功连接获取soc对象则发送成功消息</span></span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano is Connected\"</span>);</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">if</span> (!isconnected)</span><br><span class=\"line\">\t\t\t\t\t\tisconnected = !isconnected;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"Connect Failed\"</span>);</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MsgThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\tOutputStream os = soc.getOutputStream();</span><br><span class=\"line\">\t\t\t\tos.write(param);</span><br><span class=\"line\">\t\t\t\tos.flush();</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"正在启动启动电子琴教学\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tsoc.close();</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\tLog.i(<span class=\"string\">\"piano\"</span>, <span class=\"string\">\"piano msg send successful failed\"</span>);</span><br><span class=\"line\">\t\t\t\tSnackbar.make(pianobt, <span class=\"string\">\"启动电子琴教学失败\"</span>, Snackbar.LENGTH_SHORT)</span><br><span class=\"line\">\t\t\t\t\t\t.setAction(<span class=\"string\">\"Action\"</span>, <span class=\"keyword\">null</span>).show();</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"乐谱分享\"><a href=\"#乐谱分享\" class=\"headerlink\" title=\"乐谱分享\"></a>乐谱分享</h1><ul>\n<li>显示乐谱的是Github上一个魔改的ImageView:<a href=\"https://github.com/boycy815/PinchImageView\" target=\"_blank\" rel=\"noopener\">PinchImageView</a></li>\n<li>定义其长按事件，触发一个分享的intent<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">showpic.setOnLongClickListener(<span class=\"keyword\">new</span> View.OnLongClickListener() &#123;</span><br><span class=\"line\">\t\t\t<span class=\"meta\">@Override</span></span><br><span class=\"line\">\t\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">onLongClick</span><span class=\"params\">(View v)</span> </span>&#123;</span><br><span class=\"line\">\t\t\t\tBitmap drawingCache = getViewBitmap(showpic);</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">if</span> (drawingCache == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"no img to save\"</span>);</span><br><span class=\"line\">\t\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t<span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">\t\t\t\t\t\tFile imageFile = <span class=\"keyword\">new</span> File(Environment.getExternalStorageDirectory(), <span class=\"string\">\"saveImageview.jpg\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tToast toast = Toast.makeText(getActivity(),</span><br><span class=\"line\">\t\t\t\t\t\t\t\t<span class=\"string\">\"\"</span>, Toast.LENGTH_LONG);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setGravity(Gravity.TOP, <span class=\"number\">0</span>, <span class=\"number\">200</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.setText(<span class=\"string\">\"分享图片\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\ttoast.show();</span><br><span class=\"line\">\t\t\t\t\t\tFileOutputStream outStream;</span><br><span class=\"line\">\t\t\t\t\t\toutStream = <span class=\"keyword\">new</span> FileOutputStream(imageFile);</span><br><span class=\"line\">\t\t\t\t\t\tdrawingCache.compress(Bitmap.CompressFormat.JPEG, <span class=\"number\">100</span>, outStream);</span><br><span class=\"line\">\t\t\t\t\t\toutStream.flush();</span><br><span class=\"line\">\t\t\t\t\t\toutStream.close();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t\tIntent sendIntent = <span class=\"keyword\">new</span> Intent();</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setAction(Intent.ACTION_SEND);</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.putExtra(Intent.EXTRA_STREAM, Uri.fromFile(imageFile));</span><br><span class=\"line\">\t\t\t\t\t\tsendIntent.setType(<span class=\"string\">\"image/png\"</span>);</span><br><span class=\"line\">\t\t\t\t\t\tgetActivity().startActivity(Intent.createChooser(sendIntent, <span class=\"string\">\"分享到\"</span>));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\t\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tLog.i(<span class=\"string\">\"play\"</span>, <span class=\"string\">\"share img wrong\"</span>);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br></pre></td></tr></table></figure></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o26O.gif","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"Android:Melodia客户端","path":"2017/03/09/dachuang/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0o26O.gif","excerpt":"<hr>\n<p>学校大创项目简单的app<br>实现功能，录制声音存为wav，利用json与服务器通信，上传wav到服务器，服务器转为midi文件，从服务器下载midi和乐谱并播放，同时电子琴改装后也可以与服务器通信，由手机给电子琴辅助参数，电子琴通过arduino从服务器上读取乐曲中间键值文件播放。</p>","date":"2017-03-09T09:19:53.000Z","pv":0,"totalPV":0,"categories":"Android","tags":["code","android"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"深度贝叶斯习题","date":"2018-09-22T02:26:48.000Z","mathjax":true,"html":true,"password":"kengbi","_content":"\nDeep-Bayes 2018 Summer Camp的习题\n发现自己代码能力果然弱......\n<!--more-->  \n\n# Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm\n- 第一题就是应用EM算法还原图像，人像和背景叠加在一起，灰度值的概率分布形式已知，设计人像在背景中的位置为隐变量，进行EM迭代推断。\n- 具体说明在官网和下面的notebook注释中有，实际上公式已经给出，想要完成作业就是把公式打上去，可以自己推一下公式。\n\nOne of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.\n\nLet's find the prankster!\n\nWhen you recognize [him or her](http://deepbayes.ru/#speakers), send:\n* name\n* reconstructed photo\n* this notebook with your code (doesn't matter how awful it is :)\n\n__privately__ to [Nadia Chirkova](https://www.facebook.com/nadiinchi) at Facebook or to info@deepbayes.ru. The first three participants will receive a present. Do not make spoilers to other participants!\n\nPlease, note that you have only __one attempt__ to send a message!\n\n\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n```\n\n\n```python\nDATA_FILE = \"data_em\"\nw = 73 # face_width\n```\n\n### Data\n\nWe are given a set of $K$ images with shape $H \\times W$.\n\nIt is represented by a numpy-array with shape $H \\times W \\times K$:\n\n\n```python\nX = np.load(DATA_FILE)\n```\n\n\n```python\nX.shape # H, W, K\n```\n\n\n\n\n    (100, 200, 1000)\n\n\n\nExample of noisy image:\n\n\n```python\nplt.imshow(X[:, :, 0], cmap=\"Greys_r\")\nplt.axis(\"off\")\nprint(X[1,:,0])\n```\n\n    [255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n       0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n     148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n       0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n       0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n       0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n       0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n       0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n     255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n     233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n     226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n       0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n     255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n     255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n     102.  25.  30.  73.]\n    \n\n\n![i0Ihiq.png](https://s1.ax1x.com/2018/10/20/i0Ihiq.png)\n\n\n### Goal and plan\n\nOur goal is to find face $F$ ($H \\times w$).\n\nAlso, we will find:\n* $B$: background  ($H \\times W$)\n* $s$: noise standard deviation (float)\n* $a$: discrete prior over face positions ($W-w+1$)\n* $q(d)$: discrete posterior over face positions for each image  (($W-w+1$) x $K$)\n\nImplementation plan:\n1. calculating $log\\, p(X  \\mid d,\\,F,\\,B,\\,s)$\n1. calculating objective\n1. E-step: finding $q(d)$\n1. M-step: estimating $F,\\, B, \\,s, \\,a$\n1. composing EM-algorithm from E- and M-step\n\n\n### Implementation\n\n\n```python\n### Variables to test implementation\ntH, tW, tw, tK = 2, 3, 1, 2\ntX = np.arange(tH*tW*tK).reshape(tH, tW, tK)\ntF = np.arange(tH*tw).reshape(tH, tw)\ntB = np.arange(tH*tW).reshape(tH, tW)\nts = 0.1\nta = np.arange(1, (tW-tw+1)+1)\nta = ta / ta.sum()\ntq = np.arange(1, (tW-tw+1)*tK+1).reshape(tW-tw+1, tK)\ntq = tq / tq.sum(axis=0)[np.newaxis, :]\n```\n\n#### 1. Implement calculate_log_probability\nFor $k$-th image $X_k$ and some face position $d_k$:\n$$p(X_k  \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}\n    \\begin{cases} \n      \\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2), \n      & \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\\n      \\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), & \\text{else}\n    \\end{cases}$$\n\nImportant notes:\n* Do not forget about logarithm!\n* This implementation should use no more than 1 cycle!\n\n\n```python\ndef calculate_log_probability(X, F, B, s):\n    \"\"\"\n    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and\n    all possible face position d_k.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n\n    Returns\n    -------\n    ll : array, shape(W-w+1, K)\n        ll[dw, k] - log-likelihood of observing image X_k given\n        that the prankster's face F is located at position dw\n    \"\"\"\n    # your code here\n    H = X.shape[0]\n    W = X.shape[1]\n    K = X.shape[2]\n    w = F.shape[1]\n    ll = np.zeros((W-w+1,K))\n    for k in range(K):\n        X_minus_B = X[:,:,k]-B[:,:]\n        XB = np.multiply(X_minus_B,X_minus_B)\n        for dk in range(W-w+1):\n            F_temp = np.zeros((H,W))\n            F_temp[:,dk:dk+w] = F\n            X_minus_F = X[:,:,k] - F_temp[:,:]\n            XF = np.multiply(X_minus_F,X_minus_F)\n            XB_mask = np.ones((H,W))\n            XB_mask[:,dk:dk+w] = 0\n            XF_mask = 1-XB_mask\n            XB_temp = np.multiply(XB,XB_mask)\n            XF_temp = np.multiply(XF,XF_mask)   \n            Sum = (np.sum(XB_temp+XF_temp))*(-1/(2*s**2))-H*W*np.log(np.sqrt(2*np.pi)*s)\n            ll[dk][k]=Sum    \n    return ll\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[-3541.69812064, -5541.69812064],\n       [-4541.69812064, -6741.69812064],\n       [-6141.69812064, -8541.69812064]])\nactual = calculate_log_probability(tX, tF, tB, ts)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 2. Implement calculate_lower_bound\n$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s) + \n    \\log p( d_k  \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$\n    \nImportant notes:\n* Use already implemented calculate_log_probability! \n* Note that distributions $q( d_k)$ and $p( d_k  \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.\n* This implementation should not use cycles!\n\n\n```python\ndef calculate_lower_bound(X, F, B, s, a, q):\n    \"\"\"\n    Calculates the lower bound L(q, F, B, s, a) for \n    the marginal log likelihood.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    q : array\n        q[dw, k] - estimate of posterior \n                   of position dw\n                   of prankster's face given image Xk\n\n    Returns\n    -------\n    L : float\n        The lower bound L(q, F, B, s, a) \n        for the marginal log likelihood.\n    \"\"\"\n    # your code here\n    K = X.shape[2]\n    ll = calculate_log_probability(X,F,B,s)\n    ll_expectation = np.multiply(ll,q)\n    q_expectation = np.multiply(np.log(q),q)\n    dk_expection = 0\n    for k in range(K):\n        dk_expection += np.multiply(np.log(a),q[:,k])\n    L = np.sum(ll_expectation)-np.sum(q_expectation)+np.sum(dk_expection)\n    \n    return L\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = -12761.1875\nactual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 3. Implement E-step\n$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) = \n\\frac {p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}\n{\\sum_{d'_k} p(  X_{k}  \\mid d'_k , \\,F,\\,B,\\,s) \\,p(d'_k \\mid a)}$$\n\nImportant notes:\n* Use already implemented calculate_log_probability!\n* For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:\n$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n  \\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} = \n  \\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$\n* This implementation should not use cycles!\n\n\n```python\ndef run_e_step(X, F, B, s, a):\n    \"\"\"\n    Given the current esitmate of the parameters, for each image Xk\n    esitmates the probability p(d_k|X_k, F, B, s, a).\n\n    Parameters\n    ----------\n    X : array, shape(H, W, K)\n        K images of size H x W.\n    F  : array_like, shape(H, w)\n        Estimate of prankster's face.\n    B : array shape(H, W)\n        Estimate of background.\n    s : float\n        Eestimate of standard deviation of Gaussian noise.\n    a : array, shape(W-w+1)\n        Estimate of prior on face position in any image.\n\n    Returns\n    -------\n    q : array\n        shape (W-w+1, K)\n        q[dw, k] - estimate of posterior of position dw\n        of prankster's face given image Xk\n    \"\"\"\n    # your code here\n    ll = calculate_log_probability(X,F,B,s)\n    K = X.shape[2]\n    for k in range(K):\n        max_ll = ll[:,k].max()\n        ll[:,k] -= max_ll\n        ll[:,k] = np.exp(ll[:,k])*a\n        denominator = np.sum(ll[:,k])\n        ll[:,k] /= denominator\n    q = ll\n    return q\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[ 1.,  1.],\n                   [ 0.,  0.],\n                   [ 0.,  0.]])\nactual = run_e_step(tX, tF, tB, ts, ta)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 4. Implement M-step\n$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j'}  \\sum_{k'} q( d_{k'} = j')}$$\n$$F[i, m] = \\frac 1 K  \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$\n$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]} \n      {\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$\n$$s^2 = \\frac 1 {HWK}   \\sum_k \\sum_{d_k} q(d_k)\n      \\sum_{i,\\, j}  (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$\n\nwhere $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.\n\nImportant notes:\n* Update parameters in the following order: $a$, $F$, $B$, $s$.\n* When the parameter is updated, its __new__ value is used to update other parameters.\n* This implementation should use no more than 3 cycles and no embedded cycles!\n\n\n```python\ndef run_m_step(X, q, w):\n    \"\"\"\n    Estimates F, B, s, a given esitmate of posteriors defined by q.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    q  :\n        q[dw, k] - estimate of posterior of position dw\n                   of prankster's face given image Xk\n    w : int\n        Face mask width.\n\n    Returns\n    -------\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    \"\"\"\n    # your code here\n    K = X.shape[2]\n    W = X.shape[1]\n    H = X.shape[0]\n    \n    a = np.sum(q,axis=1) / np.sum(q)\n\n    F = np.zeros((H,w))\n    for k in range(K):\n        for dk in range(W-w+1):\n            F+=q[dk][k]*X[:,dk:dk+w,k]\n    F = F / K\n            \n    \n    B = np.zeros((H,W))\n    denominator = np.zeros((H,W))\n    for k in range(K):\n        for dk in range(W-w+1):\n            mask = np.ones((H,W))\n            mask[:,dk:dk+w] = 0\n            B += np.multiply(q[dk][k]*X[:,:,k],mask)\n            denominator += q[dk][k]*mask\n    denominator = 1/denominator\n    B = B * denominator\n    \n    s = 0\n    for k in range(K):\n        for dk in range(W-w+1):\n            F_B = np.zeros((H,W))\n            F_B[:,dk:dk+w]=F\n            mask = np.ones((H,W))\n            mask[:,dk:dk+w] = 0\n            Model = F_B + np.multiply(B,mask)\n            temp = X[:,:,k]-Model[:,:]\n            temp = np.multiply(temp,temp)\n            temp = np.sum(temp)\n            temp *= q[dk][k]\n            s += temp\n    s = np.sqrt(s /(H*W*K))          \n    \n    return F,B,s,a\n    \n    \n```\n\n\n```python\n# run this cell to test your implementation\nexpected = [np.array([[ 3.27777778],\n                      [ 9.27777778]]),\n np.array([[  0.48387097,   2.5       ,   4.52941176],\n           [  6.48387097,   8.5       ,  10.52941176]]),\n  0.94868,\n np.array([ 0.13888889,  0.33333333,  0.52777778])]\nactual = run_m_step(tX, tq, tw)\nfor a, e in zip(actual, expected):\n    assert np.allclose(a, e)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 5. Implement EM_algorithm\nInitialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.\n\nPlease note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.\n\n\n```python\ndef run_EM(X, w, F=None, B=None, s=None, a=None, tolerance=0.001,\n           max_iter=50):\n    \"\"\"\n    Runs EM loop until the likelihood of observing X given current\n    estimate of parameters is idempotent as defined by a fixed\n    tolerance.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    w : int\n        Face mask width.\n    F : array, shape (H, w), optional\n        Initial estimate of prankster's face.\n    B : array, shape (H, W), optional\n        Initial estimate of background.\n    s : float, optional\n        Initial estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1), optional\n        Initial estimate of prior on position of face in any image.\n    tolerance : float, optional\n        Parameter for stopping criterion.\n    max_iter  : int, optional\n        Maximum number of iterations.\n\n    Returns\n    -------\n    F, B, s, a : trained parameters.\n    LL : array, shape(number_of_iters + 2,)\n        L(q, F, B, s, a) at initial guess, \n        after each EM iteration and after\n        final estimate of posteriors;\n        number_of_iters is actual number of iterations that was done.\n    \"\"\"\n    H, W, N = X.shape\n    if F is None:\n        F = np.random.randint(0, 255, (H, w))\n    if B is None:\n        B = np.random.randint(0, 255, (H, W))\n    if a is None:\n        a = np.ones(W - w + 1)\n        a /= np.sum(a)\n    if s is None:\n        s = np.random.rand()*pow(64,2)\n    # your code here\n    LL = [-100000]\n    for i in range(max_iter):\n        q = run_e_step(X,F,B,s,a)\n        F,B,s,a = run_m_step(X,q,w)\n        LL.append(calculate_lower_bound(X,F,B,s,a,q))\n        if LL[-1]-LL[-2] < tolerance :\n            break\n    LL = np.array(LL)\n    return F,B,s,a,LL\n        \n    \n```\n\n\n```python\n# run this cell to test your implementation\nres = run_EM(tX, tw, max_iter=10)\nLL = res[-1]\nassert np.alltrue(LL[1:] - LL[:-1] > 0)\nprint(\"OK\")\n```\n\n    OK\n    \n\n### Who is the prankster?\n\nTo speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.\n\nIf everything is implemented correctly, you will recognize the prankster (remember he is the one from [DeepBayes team](http://deepbayes.ru/#speakers)).\n\nRun EM-algorithm:\n\n\n```python\ndef show(F, i=1, n=1):\n    \"\"\"\n    shows face F at subplot i out of n\n    \"\"\"\n    plt.subplot(1, n, i)\n    plt.imshow(F, cmap=\"Greys_r\")\n    plt.axis(\"off\")\n```\n\n\n```python\nF, B, s, a = [None] * 4\nLL = []\nlens = [50, 100, 300, 500, 1000]\niters = [5, 1, 1, 1, 1]\nplt.figure(figsize=(20, 5))\nfor i, (l, it) in enumerate(zip(lens, iters)):\n    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n    show(F, i+1, 5)\n```\n\n\n![i0omSf.png](https://s1.ax1x.com/2018/10/20/i0omSf.png)\n\n\nAnd this is the background:\n\n\n```python\nshow(B)\n```\n\n\n![i0I4J0.png](https://s1.ax1x.com/2018/10/20/i0I4J0.png)\n\n\n### Optional part: hard-EM\n\nIf you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:\n$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$\nThis modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.\n\nIf you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:\n* calculate_lower_bound\n* run_e_step\n* run_m_step\n* run_EM\n\nAfter implementation, compare overall computation time for EM and hard-EM till recognizable F.\n\n","source":"_posts/deepbayes2018.md","raw":"---\ntitle: 深度贝叶斯习题\ndate: 2018-09-22 10:26:48\ntags: [bayes,math,machinelearning]\ncategories: 数学\nmathjax: true\nhtml: true\npassword: kengbi\n---\n\nDeep-Bayes 2018 Summer Camp的习题\n发现自己代码能力果然弱......\n<!--more-->  \n\n# Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm\n- 第一题就是应用EM算法还原图像，人像和背景叠加在一起，灰度值的概率分布形式已知，设计人像在背景中的位置为隐变量，进行EM迭代推断。\n- 具体说明在官网和下面的notebook注释中有，实际上公式已经给出，想要完成作业就是把公式打上去，可以自己推一下公式。\n\nOne of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.\n\nLet's find the prankster!\n\nWhen you recognize [him or her](http://deepbayes.ru/#speakers), send:\n* name\n* reconstructed photo\n* this notebook with your code (doesn't matter how awful it is :)\n\n__privately__ to [Nadia Chirkova](https://www.facebook.com/nadiinchi) at Facebook or to info@deepbayes.ru. The first three participants will receive a present. Do not make spoilers to other participants!\n\nPlease, note that you have only __one attempt__ to send a message!\n\n\n```python\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n```\n\n\n```python\nDATA_FILE = \"data_em\"\nw = 73 # face_width\n```\n\n### Data\n\nWe are given a set of $K$ images with shape $H \\times W$.\n\nIt is represented by a numpy-array with shape $H \\times W \\times K$:\n\n\n```python\nX = np.load(DATA_FILE)\n```\n\n\n```python\nX.shape # H, W, K\n```\n\n\n\n\n    (100, 200, 1000)\n\n\n\nExample of noisy image:\n\n\n```python\nplt.imshow(X[:, :, 0], cmap=\"Greys_r\")\nplt.axis(\"off\")\nprint(X[1,:,0])\n```\n\n    [255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n       0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n     148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n       0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n       0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n       0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n       0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n       0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n     255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n     233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n     226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n       0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n     255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n     255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n     102.  25.  30.  73.]\n    \n\n\n![i0Ihiq.png](https://s1.ax1x.com/2018/10/20/i0Ihiq.png)\n\n\n### Goal and plan\n\nOur goal is to find face $F$ ($H \\times w$).\n\nAlso, we will find:\n* $B$: background  ($H \\times W$)\n* $s$: noise standard deviation (float)\n* $a$: discrete prior over face positions ($W-w+1$)\n* $q(d)$: discrete posterior over face positions for each image  (($W-w+1$) x $K$)\n\nImplementation plan:\n1. calculating $log\\, p(X  \\mid d,\\,F,\\,B,\\,s)$\n1. calculating objective\n1. E-step: finding $q(d)$\n1. M-step: estimating $F,\\, B, \\,s, \\,a$\n1. composing EM-algorithm from E- and M-step\n\n\n### Implementation\n\n\n```python\n### Variables to test implementation\ntH, tW, tw, tK = 2, 3, 1, 2\ntX = np.arange(tH*tW*tK).reshape(tH, tW, tK)\ntF = np.arange(tH*tw).reshape(tH, tw)\ntB = np.arange(tH*tW).reshape(tH, tW)\nts = 0.1\nta = np.arange(1, (tW-tw+1)+1)\nta = ta / ta.sum()\ntq = np.arange(1, (tW-tw+1)*tK+1).reshape(tW-tw+1, tK)\ntq = tq / tq.sum(axis=0)[np.newaxis, :]\n```\n\n#### 1. Implement calculate_log_probability\nFor $k$-th image $X_k$ and some face position $d_k$:\n$$p(X_k  \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}\n    \\begin{cases} \n      \\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2), \n      & \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\\n      \\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), & \\text{else}\n    \\end{cases}$$\n\nImportant notes:\n* Do not forget about logarithm!\n* This implementation should use no more than 1 cycle!\n\n\n```python\ndef calculate_log_probability(X, F, B, s):\n    \"\"\"\n    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and\n    all possible face position d_k.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n\n    Returns\n    -------\n    ll : array, shape(W-w+1, K)\n        ll[dw, k] - log-likelihood of observing image X_k given\n        that the prankster's face F is located at position dw\n    \"\"\"\n    # your code here\n    H = X.shape[0]\n    W = X.shape[1]\n    K = X.shape[2]\n    w = F.shape[1]\n    ll = np.zeros((W-w+1,K))\n    for k in range(K):\n        X_minus_B = X[:,:,k]-B[:,:]\n        XB = np.multiply(X_minus_B,X_minus_B)\n        for dk in range(W-w+1):\n            F_temp = np.zeros((H,W))\n            F_temp[:,dk:dk+w] = F\n            X_minus_F = X[:,:,k] - F_temp[:,:]\n            XF = np.multiply(X_minus_F,X_minus_F)\n            XB_mask = np.ones((H,W))\n            XB_mask[:,dk:dk+w] = 0\n            XF_mask = 1-XB_mask\n            XB_temp = np.multiply(XB,XB_mask)\n            XF_temp = np.multiply(XF,XF_mask)   \n            Sum = (np.sum(XB_temp+XF_temp))*(-1/(2*s**2))-H*W*np.log(np.sqrt(2*np.pi)*s)\n            ll[dk][k]=Sum    \n    return ll\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[-3541.69812064, -5541.69812064],\n       [-4541.69812064, -6741.69812064],\n       [-6141.69812064, -8541.69812064]])\nactual = calculate_log_probability(tX, tF, tB, ts)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 2. Implement calculate_lower_bound\n$$\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s) + \n    \\log p( d_k  \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr) $$\n    \nImportant notes:\n* Use already implemented calculate_log_probability! \n* Note that distributions $q( d_k)$ and $p( d_k  \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.\n* This implementation should not use cycles!\n\n\n```python\ndef calculate_lower_bound(X, F, B, s, a, q):\n    \"\"\"\n    Calculates the lower bound L(q, F, B, s, a) for \n    the marginal log likelihood.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    q : array\n        q[dw, k] - estimate of posterior \n                   of position dw\n                   of prankster's face given image Xk\n\n    Returns\n    -------\n    L : float\n        The lower bound L(q, F, B, s, a) \n        for the marginal log likelihood.\n    \"\"\"\n    # your code here\n    K = X.shape[2]\n    ll = calculate_log_probability(X,F,B,s)\n    ll_expectation = np.multiply(ll,q)\n    q_expectation = np.multiply(np.log(q),q)\n    dk_expection = 0\n    for k in range(K):\n        dk_expection += np.multiply(np.log(a),q[:,k])\n    L = np.sum(ll_expectation)-np.sum(q_expectation)+np.sum(dk_expection)\n    \n    return L\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = -12761.1875\nactual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 3. Implement E-step\n$$q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) = \n\\frac {p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}\n{\\sum_{d'_k} p(  X_{k}  \\mid d'_k , \\,F,\\,B,\\,s) \\,p(d'_k \\mid a)}$$\n\nImportant notes:\n* Use already implemented calculate_log_probability!\n* For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:\n$$\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n  \\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} = \n  \\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}$$\n* This implementation should not use cycles!\n\n\n```python\ndef run_e_step(X, F, B, s, a):\n    \"\"\"\n    Given the current esitmate of the parameters, for each image Xk\n    esitmates the probability p(d_k|X_k, F, B, s, a).\n\n    Parameters\n    ----------\n    X : array, shape(H, W, K)\n        K images of size H x W.\n    F  : array_like, shape(H, w)\n        Estimate of prankster's face.\n    B : array shape(H, W)\n        Estimate of background.\n    s : float\n        Eestimate of standard deviation of Gaussian noise.\n    a : array, shape(W-w+1)\n        Estimate of prior on face position in any image.\n\n    Returns\n    -------\n    q : array\n        shape (W-w+1, K)\n        q[dw, k] - estimate of posterior of position dw\n        of prankster's face given image Xk\n    \"\"\"\n    # your code here\n    ll = calculate_log_probability(X,F,B,s)\n    K = X.shape[2]\n    for k in range(K):\n        max_ll = ll[:,k].max()\n        ll[:,k] -= max_ll\n        ll[:,k] = np.exp(ll[:,k])*a\n        denominator = np.sum(ll[:,k])\n        ll[:,k] /= denominator\n    q = ll\n    return q\n```\n\n\n```python\n# run this cell to test your implementation\nexpected = np.array([[ 1.,  1.],\n                   [ 0.,  0.],\n                   [ 0.,  0.]])\nactual = run_e_step(tX, tF, tB, ts, ta)\nassert np.allclose(actual, expected)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 4. Implement M-step\n$$a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j'}  \\sum_{k'} q( d_{k'} = j')}$$\n$$F[i, m] = \\frac 1 K  \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]$$\n$$B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]} \n      {\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}$$\n$$s^2 = \\frac 1 {HWK}   \\sum_k \\sum_{d_k} q(d_k)\n      \\sum_{i,\\, j}  (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2$$\n\nwhere $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.\n\nImportant notes:\n* Update parameters in the following order: $a$, $F$, $B$, $s$.\n* When the parameter is updated, its __new__ value is used to update other parameters.\n* This implementation should use no more than 3 cycles and no embedded cycles!\n\n\n```python\ndef run_m_step(X, q, w):\n    \"\"\"\n    Estimates F, B, s, a given esitmate of posteriors defined by q.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    q  :\n        q[dw, k] - estimate of posterior of position dw\n                   of prankster's face given image Xk\n    w : int\n        Face mask width.\n\n    Returns\n    -------\n    F : array, shape (H, w)\n        Estimate of prankster's face.\n    B : array, shape (H, W)\n        Estimate of background.\n    s : float\n        Estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1)\n        Estimate of prior on position of face in any image.\n    \"\"\"\n    # your code here\n    K = X.shape[2]\n    W = X.shape[1]\n    H = X.shape[0]\n    \n    a = np.sum(q,axis=1) / np.sum(q)\n\n    F = np.zeros((H,w))\n    for k in range(K):\n        for dk in range(W-w+1):\n            F+=q[dk][k]*X[:,dk:dk+w,k]\n    F = F / K\n            \n    \n    B = np.zeros((H,W))\n    denominator = np.zeros((H,W))\n    for k in range(K):\n        for dk in range(W-w+1):\n            mask = np.ones((H,W))\n            mask[:,dk:dk+w] = 0\n            B += np.multiply(q[dk][k]*X[:,:,k],mask)\n            denominator += q[dk][k]*mask\n    denominator = 1/denominator\n    B = B * denominator\n    \n    s = 0\n    for k in range(K):\n        for dk in range(W-w+1):\n            F_B = np.zeros((H,W))\n            F_B[:,dk:dk+w]=F\n            mask = np.ones((H,W))\n            mask[:,dk:dk+w] = 0\n            Model = F_B + np.multiply(B,mask)\n            temp = X[:,:,k]-Model[:,:]\n            temp = np.multiply(temp,temp)\n            temp = np.sum(temp)\n            temp *= q[dk][k]\n            s += temp\n    s = np.sqrt(s /(H*W*K))          \n    \n    return F,B,s,a\n    \n    \n```\n\n\n```python\n# run this cell to test your implementation\nexpected = [np.array([[ 3.27777778],\n                      [ 9.27777778]]),\n np.array([[  0.48387097,   2.5       ,   4.52941176],\n           [  6.48387097,   8.5       ,  10.52941176]]),\n  0.94868,\n np.array([ 0.13888889,  0.33333333,  0.52777778])]\nactual = run_m_step(tX, tq, tw)\nfor a, e in zip(actual, expected):\n    assert np.allclose(a, e)\nprint(\"OK\")\n```\n\n    OK\n    \n\n#### 5. Implement EM_algorithm\nInitialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.\n\nPlease note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.\n\n\n```python\ndef run_EM(X, w, F=None, B=None, s=None, a=None, tolerance=0.001,\n           max_iter=50):\n    \"\"\"\n    Runs EM loop until the likelihood of observing X given current\n    estimate of parameters is idempotent as defined by a fixed\n    tolerance.\n\n    Parameters\n    ----------\n    X : array, shape (H, W, K)\n        K images of size H x W.\n    w : int\n        Face mask width.\n    F : array, shape (H, w), optional\n        Initial estimate of prankster's face.\n    B : array, shape (H, W), optional\n        Initial estimate of background.\n    s : float, optional\n        Initial estimate of standard deviation of Gaussian noise.\n    a : array, shape (W-w+1), optional\n        Initial estimate of prior on position of face in any image.\n    tolerance : float, optional\n        Parameter for stopping criterion.\n    max_iter  : int, optional\n        Maximum number of iterations.\n\n    Returns\n    -------\n    F, B, s, a : trained parameters.\n    LL : array, shape(number_of_iters + 2,)\n        L(q, F, B, s, a) at initial guess, \n        after each EM iteration and after\n        final estimate of posteriors;\n        number_of_iters is actual number of iterations that was done.\n    \"\"\"\n    H, W, N = X.shape\n    if F is None:\n        F = np.random.randint(0, 255, (H, w))\n    if B is None:\n        B = np.random.randint(0, 255, (H, W))\n    if a is None:\n        a = np.ones(W - w + 1)\n        a /= np.sum(a)\n    if s is None:\n        s = np.random.rand()*pow(64,2)\n    # your code here\n    LL = [-100000]\n    for i in range(max_iter):\n        q = run_e_step(X,F,B,s,a)\n        F,B,s,a = run_m_step(X,q,w)\n        LL.append(calculate_lower_bound(X,F,B,s,a,q))\n        if LL[-1]-LL[-2] < tolerance :\n            break\n    LL = np.array(LL)\n    return F,B,s,a,LL\n        \n    \n```\n\n\n```python\n# run this cell to test your implementation\nres = run_EM(tX, tw, max_iter=10)\nLL = res[-1]\nassert np.alltrue(LL[1:] - LL[:-1] > 0)\nprint(\"OK\")\n```\n\n    OK\n    \n\n### Who is the prankster?\n\nTo speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.\n\nIf everything is implemented correctly, you will recognize the prankster (remember he is the one from [DeepBayes team](http://deepbayes.ru/#speakers)).\n\nRun EM-algorithm:\n\n\n```python\ndef show(F, i=1, n=1):\n    \"\"\"\n    shows face F at subplot i out of n\n    \"\"\"\n    plt.subplot(1, n, i)\n    plt.imshow(F, cmap=\"Greys_r\")\n    plt.axis(\"off\")\n```\n\n\n```python\nF, B, s, a = [None] * 4\nLL = []\nlens = [50, 100, 300, 500, 1000]\niters = [5, 1, 1, 1, 1]\nplt.figure(figsize=(20, 5))\nfor i, (l, it) in enumerate(zip(lens, iters)):\n    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)\n    show(F, i+1, 5)\n```\n\n\n![i0omSf.png](https://s1.ax1x.com/2018/10/20/i0omSf.png)\n\n\nAnd this is the background:\n\n\n```python\nshow(B)\n```\n\n\n![i0I4J0.png](https://s1.ax1x.com/2018/10/20/i0I4J0.png)\n\n\n### Optional part: hard-EM\n\nIf you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:\n$$q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}$$\nThis modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.\n\nIf you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:\n* calculate_lower_bound\n* run_e_step\n* run_m_step\n* run_EM\n\nAfter implementation, compare overall computation time for EM and hard-EM till recognizable F.\n\n","slug":"deepbayes2018","published":1,"updated":"2019-07-22T03:45:23.118Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vik005tq8t51c4pn1iw","content":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\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\n</div>\n<script src=\"/lib/crypto-js.js\"></script><script src=\"/lib/blog-encrypt.js\"></script><link href=\"/css/blog-encrypt.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"这个坑还没填完，正努力填坑中......</br>","more":"这个坑还没填完，正努力填坑中......</br>","origin":"<p>Deep-Bayes 2018 Summer Camp的习题<br>发现自己代码能力果然弱……<br><a id=\"more\"></a>  </p>\n<h1 id=\"Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\"><a href=\"#Deep-Bayes-summer-school-Practical-session-on-EM-algorithm\" class=\"headerlink\" title=\"Deep|Bayes summer school. Practical session on EM algorithm\"></a>Deep<span style=\"color:green\">|</span>Bayes summer school. Practical session on EM algorithm</h1><ul>\n<li>第一题就是应用EM算法还原图像，人像和背景叠加在一起，灰度值的概率分布形式已知，设计人像在背景中的位置为隐变量，进行EM迭代推断。</li>\n<li>具体说明在官网和下面的notebook注释中有，实际上公式已经给出，想要完成作业就是把公式打上去，可以自己推一下公式。</li>\n</ul>\n<p>One of the school organisers decided to prank us and hid all games for our Thursday Game Night somewhere.</p>\n<p>Let’s find the prankster!</p>\n<p>When you recognize <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">him or her</a>, send:</p>\n<ul>\n<li>name</li>\n<li>reconstructed photo</li>\n<li>this notebook with your code (doesn’t matter how awful it is :)</li>\n</ul>\n<p><strong>privately</strong> to <a href=\"https://www.facebook.com/nadiinchi\" target=\"_blank\" rel=\"noopener\">Nadia Chirkova</a> at Facebook or to info@deepbayes.ru. The first three participants will receive a present. Do not make spoilers to other participants!</p>\n<p>Please, note that you have only <strong>one attempt</strong> to send a message!</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">%matplotlib inline</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DATA_FILE = <span class=\"string\">\"data_em\"</span></span><br><span class=\"line\">w = <span class=\"number\">73</span> <span class=\"comment\"># face_width</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Data\"><a href=\"#Data\" class=\"headerlink\" title=\"Data\"></a>Data</h3><p>We are given a set of $K$ images with shape $H \\times W$.</p>\n<p>It is represented by a numpy-array with shape $H \\times W \\times K$:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.load(DATA_FILE)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X.shape <span class=\"comment\"># H, W, K</span></span><br></pre></td></tr></table></figure>\n<pre><code>(100, 200, 1000)\n</code></pre><p>Example of noisy image:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.imshow(X[:, :, <span class=\"number\">0</span>], cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">print(X[<span class=\"number\">1</span>,:,<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n<pre><code>[255. 255.  41. 255.   0.  51. 255.  15. 255.  59.   0.   0.   0. 255.\n   0. 255. 255.   0. 175.  74. 184.   0.   0. 150. 255. 255.   0.   0.\n 148.   0. 255. 181. 255. 255. 255.   0. 255. 255.  30.   0.   0. 255.\n   0. 255. 255. 206. 234. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n   0. 255. 255. 175.  30. 255.   0.   0. 255.   0. 255.  48.   0.   0.\n   0. 232. 162. 255.  26.   0.   0. 255.   0. 255. 173. 255. 255.   0.\n   0. 255. 255. 119.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n   0.   0. 248.   5. 149. 255.   0. 255. 255. 255.   0. 108.   0.   0.\n 255.   0. 255. 255. 255.   0.   0. 193.  79.   0. 255.   0.   0.   0.\n 233. 255.   0.  65. 255. 255. 255.   0. 255.   0.   0.   0. 255.  58.\n 226. 255.   0. 242. 255. 255.   0. 255.   4. 255. 255.  97. 255.   0.\n   0. 255.   0. 255.   0.   0.   0. 255.   0.  43. 219.   0. 255. 255.\n 255. 166. 255.   0. 255.  42. 255.  44. 255. 255. 255. 255. 255. 255.\n 255. 255.  28.   0.  52. 255.  81. 104. 255. 255.  48. 255. 255. 255.\n 102.  25.  30.  73.]\n</code></pre><p><img src=\"https://s1.ax1x.com/2018/10/20/i0Ihiq.png\" alt=\"i0Ihiq.png\"></p>\n<h3 id=\"Goal-and-plan\"><a href=\"#Goal-and-plan\" class=\"headerlink\" title=\"Goal and plan\"></a>Goal and plan</h3><p>Our goal is to find face $F$ ($H \\times w$).</p>\n<p>Also, we will find:</p>\n<ul>\n<li>$B$: background  ($H \\times W$)</li>\n<li>$s$: noise standard deviation (float)</li>\n<li>$a$: discrete prior over face positions ($W-w+1$)</li>\n<li>$q(d)$: discrete posterior over face positions for each image  (($W-w+1$) x $K$)</li>\n</ul>\n<p>Implementation plan:</p>\n<ol>\n<li>calculating $log\\, p(X  \\mid d,\\,F,\\,B,\\,s)$</li>\n<li>calculating objective</li>\n<li>E-step: finding $q(d)$</li>\n<li>M-step: estimating $F,\\, B, \\,s, \\,a$</li>\n<li>composing EM-algorithm from E- and M-step</li>\n</ol>\n<h3 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### Variables to test implementation</span></span><br><span class=\"line\">tH, tW, tw, tK = <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span></span><br><span class=\"line\">tX = np.arange(tH*tW*tK).reshape(tH, tW, tK)</span><br><span class=\"line\">tF = np.arange(tH*tw).reshape(tH, tw)</span><br><span class=\"line\">tB = np.arange(tH*tW).reshape(tH, tW)</span><br><span class=\"line\">ts = <span class=\"number\">0.1</span></span><br><span class=\"line\">ta = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)+<span class=\"number\">1</span>)</span><br><span class=\"line\">ta = ta / ta.sum()</span><br><span class=\"line\">tq = np.arange(<span class=\"number\">1</span>, (tW-tw+<span class=\"number\">1</span>)*tK+<span class=\"number\">1</span>).reshape(tW-tw+<span class=\"number\">1</span>, tK)</span><br><span class=\"line\">tq = tq / tq.sum(axis=<span class=\"number\">0</span>)[np.newaxis, :]</span><br></pre></td></tr></table></figure>\n<h4 id=\"1-Implement-calculate-log-probability\"><a href=\"#1-Implement-calculate-log-probability\" class=\"headerlink\" title=\"1. Implement calculate_log_probability\"></a>1. Implement calculate_log_probability</h4><p>For $k$-th image $X_k$ and some face position $d_k$:</p>\n<script type=\"math/tex; mode=display\">p(X_k  \\mid d_k,\\,F,\\,B,\\,s) = \\prod_{ij}\n    \\begin{cases} \n      \\mathcal{N}(X_k[i,j]\\mid F[i,\\,j-d_k],\\,s^2), \n      & \\text{if}\\, (i,j)\\in faceArea(d_k)\\\\\n      \\mathcal{N}(X_k[i,j]\\mid B[i,j],\\,s^2), & \\text{else}\n    \\end{cases}</script><p>Important notes:</p>\n<ul>\n<li>Do not forget about logarithm!</li>\n<li>This implementation should use no more than 1 cycle!</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_log_probability</span><span class=\"params\">(X, F, B, s)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates log p(X_k|d_k, F, B, s) for all images X_k in X and</span></span><br><span class=\"line\"><span class=\"string\">    all possible face position d_k.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    ll : array, shape(W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        ll[dw, k] - log-likelihood of observing image X_k given</span></span><br><span class=\"line\"><span class=\"string\">        that the prankster's face F is located at position dw</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br><span class=\"line\">    H = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    W = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    K = X.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    w = F.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    ll = np.zeros((W-w+<span class=\"number\">1</span>,K))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        X_minus_B = X[:,:,k]-B[:,:]</span><br><span class=\"line\">        XB = np.multiply(X_minus_B,X_minus_B)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dk <span class=\"keyword\">in</span> range(W-w+<span class=\"number\">1</span>):</span><br><span class=\"line\">            F_temp = np.zeros((H,W))</span><br><span class=\"line\">            F_temp[:,dk:dk+w] = F</span><br><span class=\"line\">            X_minus_F = X[:,:,k] - F_temp[:,:]</span><br><span class=\"line\">            XF = np.multiply(X_minus_F,X_minus_F)</span><br><span class=\"line\">            XB_mask = np.ones((H,W))</span><br><span class=\"line\">            XB_mask[:,dk:dk+w] = <span class=\"number\">0</span></span><br><span class=\"line\">            XF_mask = <span class=\"number\">1</span>-XB_mask</span><br><span class=\"line\">            XB_temp = np.multiply(XB,XB_mask)</span><br><span class=\"line\">            XF_temp = np.multiply(XF,XF_mask)   </span><br><span class=\"line\">            Sum = (np.sum(XB_temp+XF_temp))*(<span class=\"number\">-1</span>/(<span class=\"number\">2</span>*s**<span class=\"number\">2</span>))-H*W*np.log(np.sqrt(<span class=\"number\">2</span>*np.pi)*s)</span><br><span class=\"line\">            ll[dk][k]=Sum    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> ll</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[<span class=\"number\">-3541.69812064</span>, <span class=\"number\">-5541.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-4541.69812064</span>, <span class=\"number\">-6741.69812064</span>],</span><br><span class=\"line\">       [<span class=\"number\">-6141.69812064</span>, <span class=\"number\">-8541.69812064</span>]])</span><br><span class=\"line\">actual = calculate_log_probability(tX, tF, tB, ts)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>OK\n</code></pre><h4 id=\"2-Implement-calculate-lower-bound\"><a href=\"#2-Implement-calculate-lower-bound\" class=\"headerlink\" title=\"2. Implement calculate_lower_bound\"></a>2. Implement calculate_lower_bound</h4><script type=\"math/tex; mode=display\">\\mathcal{L}(q, \\,F, \\,B,\\, s,\\, a) = \\sum_k \\biggl (\\mathbb{E} _ {q( d_k)}\\bigl ( \\log p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s) + \n    \\log p( d_k  \\mid a)\\bigr) - \\mathbb{E} _ {q( d_k)} \\log q( d_k)\\biggr)</script><p>Important notes:</p>\n<ul>\n<li>Use already implemented calculate_log_probability! </li>\n<li>Note that distributions $q( d_k)$ and $p( d_k  \\mid a)$ are discrete. For example, $P(d_k=i \\mid a) = a[i]$.</li>\n<li>This implementation should not use cycles!</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_lower_bound</span><span class=\"params\">(X, F, B, s, a, q)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Calculates the lower bound L(q, F, B, s, a) for </span></span><br><span class=\"line\"><span class=\"string\">    the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior </span></span><br><span class=\"line\"><span class=\"string\">                   of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    L : float</span></span><br><span class=\"line\"><span class=\"string\">        The lower bound L(q, F, B, s, a) </span></span><br><span class=\"line\"><span class=\"string\">        for the marginal log likelihood.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br><span class=\"line\">    K = X.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    ll = calculate_log_probability(X,F,B,s)</span><br><span class=\"line\">    ll_expectation = np.multiply(ll,q)</span><br><span class=\"line\">    q_expectation = np.multiply(np.log(q),q)</span><br><span class=\"line\">    dk_expection = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        dk_expection += np.multiply(np.log(a),q[:,k])</span><br><span class=\"line\">    L = np.sum(ll_expectation)-np.sum(q_expectation)+np.sum(dk_expection)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> L</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = <span class=\"number\">-12761.1875</span></span><br><span class=\"line\">actual = calculate_lower_bound(tX, tF, tB, ts, ta, tq)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>OK\n</code></pre><h4 id=\"3-Implement-E-step\"><a href=\"#3-Implement-E-step\" class=\"headerlink\" title=\"3. Implement E-step\"></a>3. Implement E-step</h4><script type=\"math/tex; mode=display\">q(d_k) = p(d_k \\mid X_k, \\,F, \\,B, \\,s,\\, a) = \n\\frac {p(  X_{k}  \\mid {d}_{k} , \\,F,\\,B,\\,s)\\, p(d_k \\mid a)}\n{\\sum_{d'_k} p(  X_{k}  \\mid d'_k , \\,F,\\,B,\\,s) \\,p(d'_k \\mid a)}</script><p>Important notes:</p>\n<ul>\n<li>Use already implemented calculate_log_probability!</li>\n<li>For computational stability, perform all computations with logarithmic values and apply exp only before return. Also, we recommend using this trick:<script type=\"math/tex; mode=display\">\\beta_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n\\frac{e^{\\beta_i}}{\\sum_k e^{\\beta_k}} = \n\\frac{e^{(\\beta_i - \\max_j \\beta_j)}}{\\sum_k e^{(\\beta_k- \\max_j \\beta_j)}}</script></li>\n<li>This implementation should not use cycles!</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_e_step</span><span class=\"params\">(X, F, B, s, a)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Given the current esitmate of the parameters, for each image Xk</span></span><br><span class=\"line\"><span class=\"string\">    esitmates the probability p(d_k|X_k, F, B, s, a).</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape(H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    F  : array_like, shape(H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array shape(H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Eestimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape(W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on face position in any image.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    q : array</span></span><br><span class=\"line\"><span class=\"string\">        shape (W-w+1, K)</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">        of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br><span class=\"line\">    ll = calculate_log_probability(X,F,B,s)</span><br><span class=\"line\">    K = X.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        max_ll = ll[:,k].max()</span><br><span class=\"line\">        ll[:,k] -= max_ll</span><br><span class=\"line\">        ll[:,k] = np.exp(ll[:,k])*a</span><br><span class=\"line\">        denominator = np.sum(ll[:,k])</span><br><span class=\"line\">        ll[:,k] /= denominator</span><br><span class=\"line\">    q = ll</span><br><span class=\"line\">    <span class=\"keyword\">return</span> q</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = np.array([[ <span class=\"number\">1.</span>,  <span class=\"number\">1.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>],</span><br><span class=\"line\">                   [ <span class=\"number\">0.</span>,  <span class=\"number\">0.</span>]])</span><br><span class=\"line\">actual = run_e_step(tX, tF, tB, ts, ta)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.allclose(actual, expected)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>OK\n</code></pre><h4 id=\"4-Implement-M-step\"><a href=\"#4-Implement-M-step\" class=\"headerlink\" title=\"4. Implement M-step\"></a>4. Implement M-step</h4><script type=\"math/tex; mode=display\">a[j] = \\frac{\\sum_k q( d_k = j )}{\\sum_{j'}  \\sum_{k'} q( d_{k'} = j')}</script><script type=\"math/tex; mode=display\">F[i, m] = \\frac 1 K  \\sum_k \\sum_{d_k} q(d_k)\\, X^k[i,\\, m+d_k]</script><script type=\"math/tex; mode=display\">B[i, j] = \\frac {\\sum_k \\sum_{ d_k:\\, (i, \\,j) \\,\\not\\in faceArea(d_k)} q(d_k)\\, X^k[i, j]} \n      {\\sum_k \\sum_{d_k: \\,(i, \\,j)\\, \\not\\in faceArea(d_k)} q(d_k)}</script><script type=\"math/tex; mode=display\">s^2 = \\frac 1 {HWK}   \\sum_k \\sum_{d_k} q(d_k)\n      \\sum_{i,\\, j}  (X^k[i, \\,j] - Model^{d_k}[i, \\,j])^2</script><p>where $Model^{d_k}[i, j]$ is an image composed from background and face located at $d_k$.</p>\n<p>Important notes:</p>\n<ul>\n<li>Update parameters in the following order: $a$, $F$, $B$, $s$.</li>\n<li>When the parameter is updated, its <strong>new</strong> value is used to update other parameters.</li>\n<li>This implementation should use no more than 3 cycles and no embedded cycles!</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_m_step</span><span class=\"params\">(X, q, w)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Estimates F, B, s, a given esitmate of posteriors defined by q.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    q  :</span></span><br><span class=\"line\"><span class=\"string\">        q[dw, k] - estimate of posterior of position dw</span></span><br><span class=\"line\"><span class=\"string\">                   of prankster's face given image Xk</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1)</span></span><br><span class=\"line\"><span class=\"string\">        Estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br><span class=\"line\">    K = X.shape[<span class=\"number\">2</span>]</span><br><span class=\"line\">    W = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    H = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    a = np.sum(q,axis=<span class=\"number\">1</span>) / np.sum(q)</span><br><span class=\"line\"></span><br><span class=\"line\">    F = np.zeros((H,w))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dk <span class=\"keyword\">in</span> range(W-w+<span class=\"number\">1</span>):</span><br><span class=\"line\">            F+=q[dk][k]*X[:,dk:dk+w,k]</span><br><span class=\"line\">    F = F / K</span><br><span class=\"line\">            </span><br><span class=\"line\">    </span><br><span class=\"line\">    B = np.zeros((H,W))</span><br><span class=\"line\">    denominator = np.zeros((H,W))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dk <span class=\"keyword\">in</span> range(W-w+<span class=\"number\">1</span>):</span><br><span class=\"line\">            mask = np.ones((H,W))</span><br><span class=\"line\">            mask[:,dk:dk+w] = <span class=\"number\">0</span></span><br><span class=\"line\">            B += np.multiply(q[dk][k]*X[:,:,k],mask)</span><br><span class=\"line\">            denominator += q[dk][k]*mask</span><br><span class=\"line\">    denominator = <span class=\"number\">1</span>/denominator</span><br><span class=\"line\">    B = B * denominator</span><br><span class=\"line\">    </span><br><span class=\"line\">    s = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> dk <span class=\"keyword\">in</span> range(W-w+<span class=\"number\">1</span>):</span><br><span class=\"line\">            F_B = np.zeros((H,W))</span><br><span class=\"line\">            F_B[:,dk:dk+w]=F</span><br><span class=\"line\">            mask = np.ones((H,W))</span><br><span class=\"line\">            mask[:,dk:dk+w] = <span class=\"number\">0</span></span><br><span class=\"line\">            Model = F_B + np.multiply(B,mask)</span><br><span class=\"line\">            temp = X[:,:,k]-Model[:,:]</span><br><span class=\"line\">            temp = np.multiply(temp,temp)</span><br><span class=\"line\">            temp = np.sum(temp)</span><br><span class=\"line\">            temp *= q[dk][k]</span><br><span class=\"line\">            s += temp</span><br><span class=\"line\">    s = np.sqrt(s /(H*W*K))          </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> F,B,s,a</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">expected = [np.array([[ <span class=\"number\">3.27777778</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">9.27777778</span>]]),</span><br><span class=\"line\"> np.array([[  <span class=\"number\">0.48387097</span>,   <span class=\"number\">2.5</span>       ,   <span class=\"number\">4.52941176</span>],</span><br><span class=\"line\">           [  <span class=\"number\">6.48387097</span>,   <span class=\"number\">8.5</span>       ,  <span class=\"number\">10.52941176</span>]]),</span><br><span class=\"line\">  <span class=\"number\">0.94868</span>,</span><br><span class=\"line\"> np.array([ <span class=\"number\">0.13888889</span>,  <span class=\"number\">0.33333333</span>,  <span class=\"number\">0.52777778</span>])]</span><br><span class=\"line\">actual = run_m_step(tX, tq, tw)</span><br><span class=\"line\"><span class=\"keyword\">for</span> a, e <span class=\"keyword\">in</span> zip(actual, expected):</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> np.allclose(a, e)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>OK\n</code></pre><h4 id=\"5-Implement-EM-algorithm\"><a href=\"#5-Implement-EM-algorithm\" class=\"headerlink\" title=\"5. Implement EM_algorithm\"></a>5. Implement EM_algorithm</h4><p>Initialize parameters, if they are not passed, and then repeat E- and M-steps till convergence.</p>\n<p>Please note that $\\mathcal{L}(q, \\,F, \\,B, \\,s, \\,a)$ must increase after each iteration.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_EM</span><span class=\"params\">(X, w, F=None, B=None, s=None, a=None, tolerance=<span class=\"number\">0.001</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           max_iter=<span class=\"number\">50</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Runs EM loop until the likelihood of observing X given current</span></span><br><span class=\"line\"><span class=\"string\">    estimate of parameters is idempotent as defined by a fixed</span></span><br><span class=\"line\"><span class=\"string\">    tolerance.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    X : array, shape (H, W, K)</span></span><br><span class=\"line\"><span class=\"string\">        K images of size H x W.</span></span><br><span class=\"line\"><span class=\"string\">    w : int</span></span><br><span class=\"line\"><span class=\"string\">        Face mask width.</span></span><br><span class=\"line\"><span class=\"string\">    F : array, shape (H, w), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prankster's face.</span></span><br><span class=\"line\"><span class=\"string\">    B : array, shape (H, W), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of background.</span></span><br><span class=\"line\"><span class=\"string\">    s : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of standard deviation of Gaussian noise.</span></span><br><span class=\"line\"><span class=\"string\">    a : array, shape (W-w+1), optional</span></span><br><span class=\"line\"><span class=\"string\">        Initial estimate of prior on position of face in any image.</span></span><br><span class=\"line\"><span class=\"string\">    tolerance : float, optional</span></span><br><span class=\"line\"><span class=\"string\">        Parameter for stopping criterion.</span></span><br><span class=\"line\"><span class=\"string\">    max_iter  : int, optional</span></span><br><span class=\"line\"><span class=\"string\">        Maximum number of iterations.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    F, B, s, a : trained parameters.</span></span><br><span class=\"line\"><span class=\"string\">    LL : array, shape(number_of_iters + 2,)</span></span><br><span class=\"line\"><span class=\"string\">        L(q, F, B, s, a) at initial guess, </span></span><br><span class=\"line\"><span class=\"string\">        after each EM iteration and after</span></span><br><span class=\"line\"><span class=\"string\">        final estimate of posteriors;</span></span><br><span class=\"line\"><span class=\"string\">        number_of_iters is actual number of iterations that was done.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    H, W, N = X.shape</span><br><span class=\"line\">    <span class=\"keyword\">if</span> F <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        F = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, w))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> B <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        B = np.random.randint(<span class=\"number\">0</span>, <span class=\"number\">255</span>, (H, W))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> a <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        a = np.ones(W - w + <span class=\"number\">1</span>)</span><br><span class=\"line\">        a /= np.sum(a)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> s <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">        s = np.random.rand()*pow(<span class=\"number\">64</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># your code here</span></span><br><span class=\"line\">    LL = [<span class=\"number\">-100000</span>]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(max_iter):</span><br><span class=\"line\">        q = run_e_step(X,F,B,s,a)</span><br><span class=\"line\">        F,B,s,a = run_m_step(X,q,w)</span><br><span class=\"line\">        LL.append(calculate_lower_bound(X,F,B,s,a,q))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> LL[<span class=\"number\">-1</span>]-LL[<span class=\"number\">-2</span>] &lt; tolerance :</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\">    LL = np.array(LL)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> F,B,s,a,LL</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># run this cell to test your implementation</span></span><br><span class=\"line\">res = run_EM(tX, tw, max_iter=<span class=\"number\">10</span>)</span><br><span class=\"line\">LL = res[<span class=\"number\">-1</span>]</span><br><span class=\"line\"><span class=\"keyword\">assert</span> np.alltrue(LL[<span class=\"number\">1</span>:] - LL[:<span class=\"number\">-1</span>] &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"OK\"</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>OK\n</code></pre><h3 id=\"Who-is-the-prankster\"><a href=\"#Who-is-the-prankster\" class=\"headerlink\" title=\"Who is the prankster?\"></a>Who is the prankster?</h3><p>To speed up the computation, we will perform 5 iterations over small subset of images and then gradually increase the subset.</p>\n<p>If everything is implemented correctly, you will recognize the prankster (remember he is the one from <a href=\"http://deepbayes.ru/#speakers\" target=\"_blank\" rel=\"noopener\">DeepBayes team</a>).</p>\n<p>Run EM-algorithm:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show</span><span class=\"params\">(F, i=<span class=\"number\">1</span>, n=<span class=\"number\">1</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    shows face F at subplot i out of n</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, n, i)</span><br><span class=\"line\">    plt.imshow(F, cmap=<span class=\"string\">\"Greys_r\"</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F, B, s, a = [<span class=\"literal\">None</span>] * <span class=\"number\">4</span></span><br><span class=\"line\">LL = []</span><br><span class=\"line\">lens = [<span class=\"number\">50</span>, <span class=\"number\">100</span>, <span class=\"number\">300</span>, <span class=\"number\">500</span>, <span class=\"number\">1000</span>]</span><br><span class=\"line\">iters = [<span class=\"number\">5</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">20</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i, (l, it) <span class=\"keyword\">in</span> enumerate(zip(lens, iters)):</span><br><span class=\"line\">    F, B, s, a, _ = run_EM(X[:, :, :l], w, F, B, s, a, max_iter=it)</span><br><span class=\"line\">    show(F, i+<span class=\"number\">1</span>, <span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0omSf.png\" alt=\"i0omSf.png\"></p>\n<p>And this is the background:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show(B)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0I4J0.png\" alt=\"i0I4J0.png\"></p>\n<h3 id=\"Optional-part-hard-EM\"><a href=\"#Optional-part-hard-EM\" class=\"headerlink\" title=\"Optional part: hard-EM\"></a>Optional part: hard-EM</h3><p>If you have some time left, you can implement simplified version of EM-algorithm called hard-EM. In hard-EM, instead of finding posterior distribution $p(d_k|X_k, F, B, s, A)$ at E-step, we just remember its argmax $\\tilde d_k$ for each image $k$. Thus, the distribution q is replaced with a singular distribution:</p>\n<script type=\"math/tex; mode=display\">q(d_k) = \\begin{cases} 1, \\, if d_k = \\tilde d_k \\\\ 0, \\, otherwise\\end{cases}</script><p>This modification simplifies formulas for $\\mathcal{L}$ and M-step and speeds their computation up. However, the convergence of hard-EM is usually slow.</p>\n<p>If you implement hard-EM, add binary flag hard_EM to the parameters of the following functions:</p>\n<ul>\n<li>calculate_lower_bound</li>\n<li>run_e_step</li>\n<li>run_m_step</li>\n<li>run_EM</li>\n</ul>\n<p>After implementation, compare overall computation time for EM and hard-EM till recognizable F.</p>\n","encrypt":true,"abstract":"这个坑还没填完，正努力填坑中......</br>","template":"<script src=\"https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js\"></script>\n<div id=\"hbe-security\">\n  <div class=\"hbe-input-container\">\n  <input type=\"password\" class=\"hbe-form-control\" id=\"pass\" placeholder=\"文章还没写完，稍后再读，或者输入kengbi看看草稿\" />\n    <label for=\"pass\">文章还没写完，稍后再读，或者输入kengbi看看草稿</label>\n    <div class=\"bottom-line\"></div>\n  </div>\n</div>\n<div id=\"decryptionError\" style=\"display: none;\">Incorrect Password!</div>\n<div id=\"noContentError\" style=\"display: none;\">No content to display!</div>\n<div id=\"encrypt-blog\" style=\"display:none\">\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\n</div>\n","message":"文章还没写完，稍后再读，或者输入kengbi看看草稿","decryptionError":"Incorrect Password!","noContentError":"No content to display!","popularPost_tmp_postPath":true,"eyeCatchImage":null,"popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"深度贝叶斯习题","path":"2018/09/22/deepbayes2018/","eyeCatchImage":null,"excerpt":"这个坑还没填完，正努力填坑中......</br>","date":"2018-09-22T02:26:48.000Z","pv":0,"totalPV":0,"categories":"数学","tags":["math","machinelearning","bayes"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"推断算法笔记","date":"2018-08-28T01:55:10.000Z","mathjax":true,"_content":"\n记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。\n很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。\n徐老师的课程讲义地址：[roboticcam/machine-learning-notes](https://github.com/roboticcam/machine-learning-notes)，如果不额外说明，一些截图和代码均来自徐老师的讲义。\n其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。\n\n<!--more-->  \n![iwWPun.png](https://s1.ax1x.com/2018/10/19/iwWPun.png)\n\n# Bayesian Inference\n-\t在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）\n-\t统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计\n-\t在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$\n-\t后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布\n-\t在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和\n-\t后验实际上是在最大似然估计和先验之间权衡\n-\t当数据非常多时，后验渐渐不再依赖于先验\n-\t很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布\n-\t有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计\n\n# Markov Chain Monte Carlo\n-\tMCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数\n-\t最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布\n-\t蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量\n-\t蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）\n\n## 采样\n-\t直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量\n-\t在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数\n-\t最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：\n$$\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\\n$$\n\n## 拒绝采样\n-\t但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling\n-\t对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线\n![i0oFwd.jpg](https://s1.ax1x.com/2018/10/20/i0oFwd.jpg)\n-\t我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接\n-\t显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：\n```\ni=0\nwhile i!= N\n\tx(i)~q(x) and u~U(0,1)\n\tif u< p(x(i))/Mq(x(i)) then\n\t\taccept x(i)\n\t\ti=i+1\n\telse\n\t\treject x(i)\n\tend\nend\n```\n-\trejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。\n\n## 适应性拒绝采样\n-\t当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高\n-\t基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域\n-\t但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。\n![i0oEFI.jpg](https://s1.ax1x.com/2018/10/20/i0oEFI.jpg)\n\n## 重要性采样\n-\t上面提到的采样算法是从简单分布（提议分布）采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布\n-\timportance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。\n-\t例如我们希望通过采样得到某个分布的期望\n$$\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\\n$$\n-\tp(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。\n-\t这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。\n\n## 马尔可夫蒙特卡洛和Metropolis-Hasting算法\n-\tmcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关\n-\t不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。\n-\t我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{'} | x)$，从样本$x$转移到样本$x^{'}$。\n-\t在马尔可夫链中，有如下Chapman-Kologronvo等式：\n$$\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx\n$$\n-\t这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：\n$$\n\\pi _t (x) = \\pi _{t-1} (x)\n$$\n-\t实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：\n$$\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})\n$$\n-\t由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。\n-\t当满足细致平稳条件时，马氏链是收敛的\n-\t在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：\n![i0okTA.jpg](https://s1.ax1x.com/2018/10/20/i0okTA.jpg)\n-\t在mh中，我们没有改变转移矩阵来适应给定分布，而是用给定分布来修正转移矩阵，因此，转移矩阵是我们自己设计的。一般将转移矩阵（提议分布）设计为以当前状态为中心的高斯分布，对于这个高斯分布，当方差很小时，概率集中在本次采样点附近，那么转移到下次采样时大概率位置不会变动很多，接受率高（因为本次采样点就是通过了接收得到的，大概率是处于高接受率的位置），但这会造成随机游走缓慢；如果方差很大，到处走，接受率就会降低。\n-\t尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。\n-\t而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。\n\n## Hybrid Metropolis-Hasting\n-\t待补充\n\n## 吉布斯采样\n-\t吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布\n-\t先看直觉上为啥吉布斯采样通过条件概率迭代抽样的过程中不改变联合概率分布。首先在排除第i个参数计算条件概率时，这被排除的n-1个变量的边缘分布与真实联合概率分布针对这n-1个变量的边缘分布是一样的，因为它们的值没有改变；条件概率依据的条件相比真实分布是不变的，那条件概率分布也是不变的。边缘分布和条件概率分布都是不变（真实）的，那相乘得到的联合分布自然也是不变的，因此每一步迭代里都是按照真实分布采样且迭代不会改变这个分布。\n-\t吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：\n![i0oVYt.jpg](https://s1.ax1x.com/2018/10/20/i0oVYt.jpg)\n-\t工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的（存在疑问，另一种关于collapse的说法是忽略一些条件变量，基本的gibbs采样就是collapsed gibbs sampling，而这种几个分量看成一个整体的做法是blocked gibbs sampling）：\n```\nu~p(u|x,y,z)\nx,y,z~p(x,y,z|u)\n=p(x|u)p(y|u)p(z|u)\n```\n-\t上面关于x,y,z的三个条件概率可以并行计算。\n-\t现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率\n$$\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})\n$$\n-\t在gibbs中\n$$\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\\n$$\n-\t而且实际上从$x_{¬i}$到$x_{¬i}^{'}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此\n$$\nx_{¬i}^{'}=x_{¬i}\n$$\n-\t接下来看看gibbs的接受率\n$$\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\\n$$\n\n\n# Expectation Maximization\n## 更新\n-\t看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。\n\n## 公式\n-\t对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：\n\n$$\n\\theta=\\mathop{argmax}_{\\theta} L(X | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\\n$$\n\n-\t之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导\n-\t这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。\n-\t每一次迭代分为两个部分，E和M，也就求期望和最大化\n\t-\t求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数\n\t-\t最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果\n-\t合起来就得到EM算法的公式：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz\n$$\n## 为何有效\n-\t也就是证明，每次迭代后最大似然会增加\n-\t要证明：\n$$\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})\n$$\n-\t先改写对数似然\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n$$\n-\t两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：\n$$\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\\n$$\n-\t其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量\n-\t迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？\n-\t我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$\n$$\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\\n$$\n-\t其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入\tH,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性\n\n## 从ELBO的角度理解\n-\t我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式\n-\t在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：\n$$\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\\n$$\n-\t也就是\n$$\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))\n$$\n-\t其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然\n-\t可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤\n-\t我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：\n$$\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))\n$$\n-\tq取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：\n$$\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\\n$$\n-\t其中第二项与$\\theta$无关，因此：\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\\n$$\n-\t代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz\n$$\n-\t同样得到了EM算法的迭代公式\n-\t下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。\n![i0oZfP.png](https://s1.ax1x.com/2018/10/20/i0oZfP.png)\n![i0ou6S.png](https://s1.ax1x.com/2018/10/20/i0ou6S.png)\n-\t剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入\n\n## 从假设隐变量为可观察的角度\n-\t这种理解来自Chuong B Do & Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?\n-\tEM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。\n-\tEM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。\n-\t猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。\n-\t所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。\n\n## 从假设隐变量为缺失值的角度\n-\t一般如何处理缺失值？用随机值、平均值、0值、聚类中心值代替等等\n-\tEM相当于用均值代替缺失值，也就是隐变量，但是利用了更多的信息：这个均值是在已知的x分布上求期望得到\n-\tEM的迭代就是反复处理缺失值（隐变量），然后基于完整的数据再调整x的分布，再将隐变量看成缺失值进行调整\n\n## EM算法与K-means\n-\tK-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。\n\n## 隐变量引入的好处\n-\t其实应该反过来说，很多时候我们凭借逻辑设计了隐变量，然后利用EM算法推断隐变量，而不是刻意设计隐变量来简化运算。\n-\t对于GMM来说，引入隐变量的一个好处是化简了最大似然估计的计算（当然这是假设我们已知隐变量的情况下），将log与求和运算交换，参考了pluskid大神的博客：[漫谈 Clustering (番外篇): Expectation Maximization](http://blog.pluskid.org/?p=81)\n-\t对于GMM，引入隐变量作为示性函数之前，最大似然估计是：\n$$\n\\sum _{i=1}^N \\log (\\sum _{k=1}^K \\pi _k N(x_i | \\mu _k , \\Sigma _k))\n$$\n-\t引入隐变量之后，令第i个样本$x_i$对应的示性函数为$z_i$，这是一个k维one-hot向量，代表第i个样本属于k个高斯模型中哪一个，假设属于第m个模型，则$z_i^m$等于1，其余等于0。现在最大似然估计是：\n$$\n\\log \\prod _{i=1}^N p(x_i,z_i) \\\\\n= \\log \\prod _{i=1}^N p(z_i) \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K \\pi _k ^{z_i^k} \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K ( \\pi _k N(x_i | \\mu _k , \\Sigma _k)) ^{z_i^k} \\\\\n= \\sum _{i=1}^N \\sum _{k=1}^K z_i^k(\\log \\pi _k + \\log N(x_i | \\mu _k , \\Sigma _k)) \\\\\n$$\n\n## 在EM算法中应用蒙特卡罗方法\n-\t当E步骤无法解析的计算时，可以使用蒙特卡洛近似M步骤的积分：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz\n$$\n-\t我们根据现在得到的隐变量后验估计$p(z|x,\\theta ^{(t)})$来采样有限个$Z^l$，之后将这些$Z^l$代入$\\log p(x,z|\\theta)$来近似积分：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\approx \\frac 1L \\sum_{l=1}^L  \\log p(x,Z^l|\\theta)\n$$\n-\t蒙特卡洛EM算法的一个极端的例子是随机EM算法，相当于每次迭代只在E步骤只采样一个样本点。在混合模型求解中，隐变量作为示性函数，只采样一个隐变量意味着hard assignment，每个样本点以1概率分配到某个component，\n-\t蒙特卡洛EM算法推广到贝叶斯框架，就得到IP算法\n\t-\tI步骤：\n\t$$\n\tp(Z|X)=\\int p(Z | \\theta ,X)p(\\theta | X)d\\theta\n\t$$\n\t先从$p(\\theta | X)$中采样$\\theta ^l$，再将其代入，接着从$p(Z | \\theta ^l ,X)$中采样$Z^l$。\n\t-\tP步骤：\n\t从I步骤采样得到的$Z^l$用于估计参数后验：\n\t$$\n\tp(\\theta | X) = \\int p(\\theta | Z,X)p(Z|X) dZ  \\\\\n\t\\approx \\frac 1L \\sum _{l=1}^L p(\\theta | Z^l,X) \\\\\n\t$$\n\n## 广义EM算法\n-\t不会鸽\n\n## Wake-Sleep算法\n-\t鸽德哲学\n\n## 广义EM算法与吉布斯采样\n-\t当你认为我不会鸽的时候鸽了，亦是一种不鸽\n\n# Variational Inference\n\n## ELBO\n-\t接下来介绍变分推断，可以看到，EM算法可以推广到变分推断\n-\t重新推出ELBO与对数似然的关系：\n$$\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\\n$$\n-\t两边对隐分布$q(z)$求期望\n$$\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\\n$$\n-\t我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。\n-\t接下来就是讨论如何使得ELBO最大化\n\n## 任意分布上的变分推断\n-\t对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量\n-\t我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：\n$$\nq(z)=\\prod _{i=1}^M q_i(z_i)\n$$\n-\t因此ELBO可以写成两部分\n$$\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\\n$$\n-\t其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成\n$$\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\\n$$\n-\t在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：\n$$\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n$$\n-\t这样part1用伪分布的形式可以改写成\n$$\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\\n$$\n-\tpart2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：\n$$\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n$$\n-\t再把part1和part2合起来，得到ELBO关于分量j的形式：\n$$\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\\n$$\n-\t也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度\n-\t何时这个KL散度最小？也就是：\n$$\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\\n$$\n-\t到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。\n\n## 指数家族分布\n-\t定义指数家族分布：\n$$\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\\n$$\n-\t其中\n\t-\t$T(x)$:sufficient statistics\n\t-\t$\\theta$:parameter of the family\n\t-\t$\\eta$:natural parameter\n\t-\t$h(x)$:underlying measure\n\t-\t$A(\\theta)$:log normalizer / partition function\n-\t注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：\n$$\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))\n$$\n-\t当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：\n$$\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\\n$$\n-\t继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：\n$$\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n$$\n-\t举个例子，高斯分布写成指数家族分布形式：\n$$\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )\n$$\n-\t用自然参数去替代方差和均值，写成指数家族分布形式：\n$$\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))\n$$\n-\t其中：\n\t-\t$T(x)$:$[x \\ x^2]$\n\t-\t$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$\n\t-\t$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$\n-\t接下来我们利用指数家族的性质来快速计算均值和方差\n$$\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\\n$$\n-\t为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：\n$$\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\\n$$\n-\t下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：\n$$\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\\n$$\n-\t用向量组的方式改写：\n$$\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\\n$$\n- 原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：\n$$\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\\n$$\n-\t我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：\n$$\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\\n$$\n-\t这样先验和后验形式一致，也就是共轭\n-\t这样我们用统一的形式写下似然和先验\n$$\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\\n$$\n-\t这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：\n$$\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\\n$$\n-\t上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。\n\n## 指数族分布下的变分推断\n-\t接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁\n-\t我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$\n-\t我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大\n-\t首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：\n$$\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\\n$$\n-\t其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：\n$$\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\\n$$\n-\t现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：\n$$\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\\n$$\n-\t代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：\n$$\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda) \n$$\n-\t利用之前log normalizer关于参数求导的结论，有:\n$$\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)\n$$\n-\t对上式求导，令其为0，有：\n$$\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\\n$$\n-\t我们就得到了$\\lambda$的迭代式！同理可以得到：\n$$\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\\n$$\n-\t写完整应该是：\n$$\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\\n$$\n-\t观察这两个迭代式，变量更新的路径是:\n$$\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda\n$$","source":"_posts/inference-algorithm.md","raw":"---\ntitle: 推断算法笔记\ndate: 2018-08-28 09:55:10\ncategories: 机器学习\ntags:\n  - inference\n  - math\n  -\tmcmc\n  - variational inference\n  - em\nmathjax: true\n---\n\n记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。\n很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。\n徐老师的课程讲义地址：[roboticcam/machine-learning-notes](https://github.com/roboticcam/machine-learning-notes)，如果不额外说明，一些截图和代码均来自徐老师的讲义。\n其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。\n\n<!--more-->  \n![iwWPun.png](https://s1.ax1x.com/2018/10/19/iwWPun.png)\n\n# Bayesian Inference\n-\t在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）\n-\t统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计\n-\t在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$\n-\t后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布\n-\t在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和\n-\t后验实际上是在最大似然估计和先验之间权衡\n-\t当数据非常多时，后验渐渐不再依赖于先验\n-\t很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布\n-\t有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计\n\n# Markov Chain Monte Carlo\n-\tMCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数\n-\t最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布\n-\t蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量\n-\t蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）\n\n## 采样\n-\t直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量\n-\t在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数\n-\t最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：\n$$\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\\n$$\n\n## 拒绝采样\n-\t但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling\n-\t对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线\n![i0oFwd.jpg](https://s1.ax1x.com/2018/10/20/i0oFwd.jpg)\n-\t我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接\n-\t显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：\n```\ni=0\nwhile i!= N\n\tx(i)~q(x) and u~U(0,1)\n\tif u< p(x(i))/Mq(x(i)) then\n\t\taccept x(i)\n\t\ti=i+1\n\telse\n\t\treject x(i)\n\tend\nend\n```\n-\trejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。\n\n## 适应性拒绝采样\n-\t当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高\n-\t基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域\n-\t但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。\n![i0oEFI.jpg](https://s1.ax1x.com/2018/10/20/i0oEFI.jpg)\n\n## 重要性采样\n-\t上面提到的采样算法是从简单分布（提议分布）采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布\n-\timportance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。\n-\t例如我们希望通过采样得到某个分布的期望\n$$\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\\n$$\n-\tp(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。\n-\t这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。\n\n## 马尔可夫蒙特卡洛和Metropolis-Hasting算法\n-\tmcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关\n-\t不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。\n-\t我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{'} | x)$，从样本$x$转移到样本$x^{'}$。\n-\t在马尔可夫链中，有如下Chapman-Kologronvo等式：\n$$\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx\n$$\n-\t这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：\n$$\n\\pi _t (x) = \\pi _{t-1} (x)\n$$\n-\t实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：\n$$\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})\n$$\n-\t由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。\n-\t当满足细致平稳条件时，马氏链是收敛的\n-\t在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：\n![i0okTA.jpg](https://s1.ax1x.com/2018/10/20/i0okTA.jpg)\n-\t在mh中，我们没有改变转移矩阵来适应给定分布，而是用给定分布来修正转移矩阵，因此，转移矩阵是我们自己设计的。一般将转移矩阵（提议分布）设计为以当前状态为中心的高斯分布，对于这个高斯分布，当方差很小时，概率集中在本次采样点附近，那么转移到下次采样时大概率位置不会变动很多，接受率高（因为本次采样点就是通过了接收得到的，大概率是处于高接受率的位置），但这会造成随机游走缓慢；如果方差很大，到处走，接受率就会降低。\n-\t尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。\n-\t而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。\n\n## Hybrid Metropolis-Hasting\n-\t待补充\n\n## 吉布斯采样\n-\t吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布\n-\t先看直觉上为啥吉布斯采样通过条件概率迭代抽样的过程中不改变联合概率分布。首先在排除第i个参数计算条件概率时，这被排除的n-1个变量的边缘分布与真实联合概率分布针对这n-1个变量的边缘分布是一样的，因为它们的值没有改变；条件概率依据的条件相比真实分布是不变的，那条件概率分布也是不变的。边缘分布和条件概率分布都是不变（真实）的，那相乘得到的联合分布自然也是不变的，因此每一步迭代里都是按照真实分布采样且迭代不会改变这个分布。\n-\t吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：\n![i0oVYt.jpg](https://s1.ax1x.com/2018/10/20/i0oVYt.jpg)\n-\t工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的（存在疑问，另一种关于collapse的说法是忽略一些条件变量，基本的gibbs采样就是collapsed gibbs sampling，而这种几个分量看成一个整体的做法是blocked gibbs sampling）：\n```\nu~p(u|x,y,z)\nx,y,z~p(x,y,z|u)\n=p(x|u)p(y|u)p(z|u)\n```\n-\t上面关于x,y,z的三个条件概率可以并行计算。\n-\t现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率\n$$\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})\n$$\n-\t在gibbs中\n$$\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\\n$$\n-\t而且实际上从$x_{¬i}$到$x_{¬i}^{'}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此\n$$\nx_{¬i}^{'}=x_{¬i}\n$$\n-\t接下来看看gibbs的接受率\n$$\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\\n$$\n\n\n# Expectation Maximization\n## 更新\n-\t看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。\n\n## 公式\n-\t对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：\n\n$$\n\\theta=\\mathop{argmax}_{\\theta} L(X | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\\n$$\n\n-\t之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导\n-\t这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。\n-\t每一次迭代分为两个部分，E和M，也就求期望和最大化\n\t-\t求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数\n\t-\t最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果\n-\t合起来就得到EM算法的公式：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz\n$$\n## 为何有效\n-\t也就是证明，每次迭代后最大似然会增加\n-\t要证明：\n$$\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})\n$$\n-\t先改写对数似然\n$$\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\\n$$\n-\t两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：\n$$\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\\n$$\n-\t其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量\n-\t迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？\n-\t我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$\n$$\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\\n$$\n-\t其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入\tH,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性\n\n## 从ELBO的角度理解\n-\t我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式\n-\t在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：\n$$\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\\n$$\n-\t也就是\n$$\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))\n$$\n-\t其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然\n-\t可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤\n-\t我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：\n$$\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))\n$$\n-\tq取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：\n$$\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\\n$$\n-\t其中第二项与$\\theta$无关，因此：\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\\n$$\n-\t代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到\n$$\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz\n$$\n-\t同样得到了EM算法的迭代公式\n-\t下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。\n![i0oZfP.png](https://s1.ax1x.com/2018/10/20/i0oZfP.png)\n![i0ou6S.png](https://s1.ax1x.com/2018/10/20/i0ou6S.png)\n-\t剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入\n\n## 从假设隐变量为可观察的角度\n-\t这种理解来自Chuong B Do & Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?\n-\tEM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。\n-\tEM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。\n-\t猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。\n-\t所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。\n\n## 从假设隐变量为缺失值的角度\n-\t一般如何处理缺失值？用随机值、平均值、0值、聚类中心值代替等等\n-\tEM相当于用均值代替缺失值，也就是隐变量，但是利用了更多的信息：这个均值是在已知的x分布上求期望得到\n-\tEM的迭代就是反复处理缺失值（隐变量），然后基于完整的数据再调整x的分布，再将隐变量看成缺失值进行调整\n\n## EM算法与K-means\n-\tK-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。\n\n## 隐变量引入的好处\n-\t其实应该反过来说，很多时候我们凭借逻辑设计了隐变量，然后利用EM算法推断隐变量，而不是刻意设计隐变量来简化运算。\n-\t对于GMM来说，引入隐变量的一个好处是化简了最大似然估计的计算（当然这是假设我们已知隐变量的情况下），将log与求和运算交换，参考了pluskid大神的博客：[漫谈 Clustering (番外篇): Expectation Maximization](http://blog.pluskid.org/?p=81)\n-\t对于GMM，引入隐变量作为示性函数之前，最大似然估计是：\n$$\n\\sum _{i=1}^N \\log (\\sum _{k=1}^K \\pi _k N(x_i | \\mu _k , \\Sigma _k))\n$$\n-\t引入隐变量之后，令第i个样本$x_i$对应的示性函数为$z_i$，这是一个k维one-hot向量，代表第i个样本属于k个高斯模型中哪一个，假设属于第m个模型，则$z_i^m$等于1，其余等于0。现在最大似然估计是：\n$$\n\\log \\prod _{i=1}^N p(x_i,z_i) \\\\\n= \\log \\prod _{i=1}^N p(z_i) \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K \\pi _k ^{z_i^k} \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K ( \\pi _k N(x_i | \\mu _k , \\Sigma _k)) ^{z_i^k} \\\\\n= \\sum _{i=1}^N \\sum _{k=1}^K z_i^k(\\log \\pi _k + \\log N(x_i | \\mu _k , \\Sigma _k)) \\\\\n$$\n\n## 在EM算法中应用蒙特卡罗方法\n-\t当E步骤无法解析的计算时，可以使用蒙特卡洛近似M步骤的积分：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz\n$$\n-\t我们根据现在得到的隐变量后验估计$p(z|x,\\theta ^{(t)})$来采样有限个$Z^l$，之后将这些$Z^l$代入$\\log p(x,z|\\theta)$来近似积分：\n$$\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\approx \\frac 1L \\sum_{l=1}^L  \\log p(x,Z^l|\\theta)\n$$\n-\t蒙特卡洛EM算法的一个极端的例子是随机EM算法，相当于每次迭代只在E步骤只采样一个样本点。在混合模型求解中，隐变量作为示性函数，只采样一个隐变量意味着hard assignment，每个样本点以1概率分配到某个component，\n-\t蒙特卡洛EM算法推广到贝叶斯框架，就得到IP算法\n\t-\tI步骤：\n\t$$\n\tp(Z|X)=\\int p(Z | \\theta ,X)p(\\theta | X)d\\theta\n\t$$\n\t先从$p(\\theta | X)$中采样$\\theta ^l$，再将其代入，接着从$p(Z | \\theta ^l ,X)$中采样$Z^l$。\n\t-\tP步骤：\n\t从I步骤采样得到的$Z^l$用于估计参数后验：\n\t$$\n\tp(\\theta | X) = \\int p(\\theta | Z,X)p(Z|X) dZ  \\\\\n\t\\approx \\frac 1L \\sum _{l=1}^L p(\\theta | Z^l,X) \\\\\n\t$$\n\n## 广义EM算法\n-\t不会鸽\n\n## Wake-Sleep算法\n-\t鸽德哲学\n\n## 广义EM算法与吉布斯采样\n-\t当你认为我不会鸽的时候鸽了，亦是一种不鸽\n\n# Variational Inference\n\n## ELBO\n-\t接下来介绍变分推断，可以看到，EM算法可以推广到变分推断\n-\t重新推出ELBO与对数似然的关系：\n$$\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\\n$$\n-\t两边对隐分布$q(z)$求期望\n$$\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\\n$$\n-\t我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。\n-\t接下来就是讨论如何使得ELBO最大化\n\n## 任意分布上的变分推断\n-\t对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量\n-\t我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：\n$$\nq(z)=\\prod _{i=1}^M q_i(z_i)\n$$\n-\t因此ELBO可以写成两部分\n$$\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\\n$$\n-\t其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成\n$$\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\\n$$\n-\t在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：\n$$\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n$$\n-\t这样part1用伪分布的形式可以改写成\n$$\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\\n$$\n-\tpart2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：\n$$\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n$$\n-\t再把part1和part2合起来，得到ELBO关于分量j的形式：\n$$\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\\n$$\n-\t也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度\n-\t何时这个KL散度最小？也就是：\n$$\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\\n$$\n-\t到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。\n\n## 指数家族分布\n-\t定义指数家族分布：\n$$\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\\n$$\n-\t其中\n\t-\t$T(x)$:sufficient statistics\n\t-\t$\\theta$:parameter of the family\n\t-\t$\\eta$:natural parameter\n\t-\t$h(x)$:underlying measure\n\t-\t$A(\\theta)$:log normalizer / partition function\n-\t注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：\n$$\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))\n$$\n-\t当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：\n$$\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\\n$$\n-\t继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：\n$$\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n$$\n-\t举个例子，高斯分布写成指数家族分布形式：\n$$\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )\n$$\n-\t用自然参数去替代方差和均值，写成指数家族分布形式：\n$$\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))\n$$\n-\t其中：\n\t-\t$T(x)$:$[x \\ x^2]$\n\t-\t$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$\n\t-\t$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$\n-\t接下来我们利用指数家族的性质来快速计算均值和方差\n$$\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\\n$$\n-\t为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：\n$$\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\\n$$\n-\t下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：\n$$\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\\n$$\n-\t用向量组的方式改写：\n$$\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\\n$$\n- 原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：\n$$\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\\n$$\n-\t我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：\n$$\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\\n$$\n-\t这样先验和后验形式一致，也就是共轭\n-\t这样我们用统一的形式写下似然和先验\n$$\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\\n$$\n-\t这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：\n$$\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\\n$$\n-\t上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。\n\n## 指数族分布下的变分推断\n-\t接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁\n-\t我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$\n-\t我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大\n-\t首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：\n$$\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\\n$$\n-\t其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：\n$$\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\\n$$\n-\t现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：\n$$\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\\n$$\n-\t代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：\n$$\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda) \n$$\n-\t利用之前log normalizer关于参数求导的结论，有:\n$$\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)\n$$\n-\t对上式求导，令其为0，有：\n$$\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\\n$$\n-\t我们就得到了$\\lambda$的迭代式！同理可以得到：\n$$\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\\n$$\n-\t写完整应该是：\n$$\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\\n$$\n-\t观察这两个迭代式，变量更新的路径是:\n$$\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda\n$$","slug":"inference-algorithm","published":1,"updated":"2019-07-22T03:45:23.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vlw0062q8t5uwn5behq","content":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p>\n<a id=\"more\"></a>  \n<p><img src=\"https://s1.ax1x.com/2018/10/19/iwWPun.png\" alt=\"iwWPun.png\"></p>\n<h1 id=\"Bayesian-Inference\"><a href=\"#Bayesian-Inference\" class=\"headerlink\" title=\"Bayesian Inference\"></a>Bayesian Inference</h1><ul>\n<li>在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）</li>\n<li>统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计</li>\n<li>在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$</li>\n<li>后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布</li>\n<li>在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和</li>\n<li>后验实际上是在最大似然估计和先验之间权衡</li>\n<li>当数据非常多时，后验渐渐不再依赖于先验</li>\n<li>很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布</li>\n<li>有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计</li>\n</ul>\n<h1 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h1><ul>\n<li>MCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数</li>\n<li>最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布</li>\n<li>蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量</li>\n<li>蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）</li>\n</ul>\n<h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul>\n<li>直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量</li>\n<li>在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数</li>\n<li>最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：<script type=\"math/tex; mode=display\">\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\</script></li>\n</ul>\n<h2 id=\"拒绝采样\"><a href=\"#拒绝采样\" class=\"headerlink\" title=\"拒绝采样\"></a>拒绝采样</h2><ul>\n<li>但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling</li>\n<li>对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oFwd.jpg\" alt=\"i0oFwd.jpg\"></li>\n<li>我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接</li>\n<li><p>显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=0</span><br><span class=\"line\">while i!= N</span><br><span class=\"line\">\tx(i)~q(x) and u~U(0,1)</span><br><span class=\"line\">\tif u&lt; p(x(i))/Mq(x(i)) then</span><br><span class=\"line\">\t\taccept x(i)</span><br><span class=\"line\">\t\ti=i+1</span><br><span class=\"line\">\telse</span><br><span class=\"line\">\t\treject x(i)</span><br><span class=\"line\">\tend</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>rejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。</p>\n</li>\n</ul>\n<h2 id=\"适应性拒绝采样\"><a href=\"#适应性拒绝采样\" class=\"headerlink\" title=\"适应性拒绝采样\"></a>适应性拒绝采样</h2><ul>\n<li>当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高</li>\n<li>基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域</li>\n<li>但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oEFI.jpg\" alt=\"i0oEFI.jpg\"></li>\n</ul>\n<h2 id=\"重要性采样\"><a href=\"#重要性采样\" class=\"headerlink\" title=\"重要性采样\"></a>重要性采样</h2><ul>\n<li>上面提到的采样算法是从简单分布（提议分布）采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布</li>\n<li>importance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。</li>\n<li>例如我们希望通过采样得到某个分布的期望<script type=\"math/tex; mode=display\">\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\</script></li>\n<li>p(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。</li>\n<li>这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。</li>\n</ul>\n<h2 id=\"马尔可夫蒙特卡洛和Metropolis-Hasting算法\"><a href=\"#马尔可夫蒙特卡洛和Metropolis-Hasting算法\" class=\"headerlink\" title=\"马尔可夫蒙特卡洛和Metropolis-Hasting算法\"></a>马尔可夫蒙特卡洛和Metropolis-Hasting算法</h2><ul>\n<li>mcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关</li>\n<li>不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。</li>\n<li>我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{‘} | x)$，从样本$x$转移到样本$x^{‘}$。</li>\n<li>在马尔可夫链中，有如下Chapman-Kologronvo等式：<script type=\"math/tex; mode=display\">\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx</script></li>\n<li>这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：<script type=\"math/tex; mode=display\">\n\\pi _t (x) = \\pi _{t-1} (x)</script></li>\n<li>实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：<script type=\"math/tex; mode=display\">\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})</script></li>\n<li>由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。</li>\n<li>当满足细致平稳条件时，马氏链是收敛的</li>\n<li>在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0okTA.jpg\" alt=\"i0okTA.jpg\"></li>\n<li>在mh中，我们没有改变转移矩阵来适应给定分布，而是用给定分布来修正转移矩阵，因此，转移矩阵是我们自己设计的。一般将转移矩阵（提议分布）设计为以当前状态为中心的高斯分布，对于这个高斯分布，当方差很小时，概率集中在本次采样点附近，那么转移到下次采样时大概率位置不会变动很多，接受率高（因为本次采样点就是通过了接收得到的，大概率是处于高接受率的位置），但这会造成随机游走缓慢；如果方差很大，到处走，接受率就会降低。</li>\n<li>尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。</li>\n<li>而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。</li>\n</ul>\n<h2 id=\"Hybrid-Metropolis-Hasting\"><a href=\"#Hybrid-Metropolis-Hasting\" class=\"headerlink\" title=\"Hybrid Metropolis-Hasting\"></a>Hybrid Metropolis-Hasting</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"吉布斯采样\"><a href=\"#吉布斯采样\" class=\"headerlink\" title=\"吉布斯采样\"></a>吉布斯采样</h2><ul>\n<li>吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布</li>\n<li>先看直觉上为啥吉布斯采样通过条件概率迭代抽样的过程中不改变联合概率分布。首先在排除第i个参数计算条件概率时，这被排除的n-1个变量的边缘分布与真实联合概率分布针对这n-1个变量的边缘分布是一样的，因为它们的值没有改变；条件概率依据的条件相比真实分布是不变的，那条件概率分布也是不变的。边缘分布和条件概率分布都是不变（真实）的，那相乘得到的联合分布自然也是不变的，因此每一步迭代里都是按照真实分布采样且迭代不会改变这个分布。</li>\n<li>吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oVYt.jpg\" alt=\"i0oVYt.jpg\"></li>\n<li><p>工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的（存在疑问，另一种关于collapse的说法是忽略一些条件变量，基本的gibbs采样就是collapsed gibbs sampling，而这种几个分量看成一个整体的做法是blocked gibbs sampling）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u~p(u|x,y,z)</span><br><span class=\"line\">x,y,z~p(x,y,z|u)</span><br><span class=\"line\">=p(x|u)p(y|u)p(z|u)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>上面关于x,y,z的三个条件概率可以并行计算。</p>\n</li>\n<li>现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率<script type=\"math/tex; mode=display\">\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})</script></li>\n<li>在gibbs中<script type=\"math/tex; mode=display\">\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\</script></li>\n<li>而且实际上从$x_{¬i}$到$x_{¬i}^{‘}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此<script type=\"math/tex; mode=display\">\nx_{¬i}^{'}=x_{¬i}</script></li>\n<li>接下来看看gibbs的接受率<script type=\"math/tex; mode=display\">\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\</script></li>\n</ul>\n<h1 id=\"Expectation-Maximization\"><a href=\"#Expectation-Maximization\" class=\"headerlink\" title=\"Expectation Maximization\"></a>Expectation Maximization</h1><h2 id=\"更新\"><a href=\"#更新\" class=\"headerlink\" title=\"更新\"></a>更新</h2><ul>\n<li>看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。</li>\n</ul>\n<h2 id=\"公式\"><a href=\"#公式\" class=\"headerlink\" title=\"公式\"></a>公式</h2><ul>\n<li>对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta=\\mathop{argmax}_{\\theta} L(X | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\</script><ul>\n<li>之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导</li>\n<li>这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。</li>\n<li>每一次迭代分为两个部分，E和M，也就求期望和最大化<ul>\n<li>求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数</li>\n<li>最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果</li>\n</ul>\n</li>\n<li>合起来就得到EM算法的公式：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz</script><h2 id=\"为何有效\"><a href=\"#为何有效\" class=\"headerlink\" title=\"为何有效\"></a>为何有效</h2></li>\n<li>也就是证明，每次迭代后最大似然会增加</li>\n<li>要证明：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})</script></li>\n<li>先改写对数似然<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\</script></li>\n<li>两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\</script></li>\n<li>其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量</li>\n<li>迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？</li>\n<li>我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$<script type=\"math/tex; mode=display\">\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\</script></li>\n<li>其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入    H,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性</li>\n</ul>\n<h2 id=\"从ELBO的角度理解\"><a href=\"#从ELBO的角度理解\" class=\"headerlink\" title=\"从ELBO的角度理解\"></a>从ELBO的角度理解</h2><ul>\n<li>我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式</li>\n<li>在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：<script type=\"math/tex; mode=display\">\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\</script></li>\n<li>也就是<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))</script></li>\n<li>其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然</li>\n<li>可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤</li>\n<li>我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：<script type=\"math/tex; mode=display\">\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))</script></li>\n<li>q取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：<script type=\"math/tex; mode=display\">\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\</script></li>\n<li>其中第二项与$\\theta$无关，因此：<script type=\"math/tex; mode=display\">\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\</script></li>\n<li>代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到<script type=\"math/tex; mode=display\">\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz</script></li>\n<li>同样得到了EM算法的迭代公式</li>\n<li>下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oZfP.png\" alt=\"i0oZfP.png\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0ou6S.png\" alt=\"i0ou6S.png\"></li>\n<li>剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入</li>\n</ul>\n<h2 id=\"从假设隐变量为可观察的角度\"><a href=\"#从假设隐变量为可观察的角度\" class=\"headerlink\" title=\"从假设隐变量为可观察的角度\"></a>从假设隐变量为可观察的角度</h2><ul>\n<li>这种理解来自Chuong B Do &amp; Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?</li>\n<li>EM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。</li>\n<li>EM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。</li>\n<li>猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。</li>\n<li>所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。</li>\n</ul>\n<h2 id=\"从假设隐变量为缺失值的角度\"><a href=\"#从假设隐变量为缺失值的角度\" class=\"headerlink\" title=\"从假设隐变量为缺失值的角度\"></a>从假设隐变量为缺失值的角度</h2><ul>\n<li>一般如何处理缺失值？用随机值、平均值、0值、聚类中心值代替等等</li>\n<li>EM相当于用均值代替缺失值，也就是隐变量，但是利用了更多的信息：这个均值是在已知的x分布上求期望得到</li>\n<li>EM的迭代就是反复处理缺失值（隐变量），然后基于完整的数据再调整x的分布，再将隐变量看成缺失值进行调整</li>\n</ul>\n<h2 id=\"EM算法与K-means\"><a href=\"#EM算法与K-means\" class=\"headerlink\" title=\"EM算法与K-means\"></a>EM算法与K-means</h2><ul>\n<li>K-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。</li>\n</ul>\n<h2 id=\"隐变量引入的好处\"><a href=\"#隐变量引入的好处\" class=\"headerlink\" title=\"隐变量引入的好处\"></a>隐变量引入的好处</h2><ul>\n<li>其实应该反过来说，很多时候我们凭借逻辑设计了隐变量，然后利用EM算法推断隐变量，而不是刻意设计隐变量来简化运算。</li>\n<li>对于GMM来说，引入隐变量的一个好处是化简了最大似然估计的计算（当然这是假设我们已知隐变量的情况下），将log与求和运算交换，参考了pluskid大神的博客：<a href=\"http://blog.pluskid.org/?p=81\" target=\"_blank\" rel=\"noopener\">漫谈 Clustering (番外篇): Expectation Maximization</a></li>\n<li>对于GMM，引入隐变量作为示性函数之前，最大似然估计是：<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^N \\log (\\sum _{k=1}^K \\pi _k N(x_i | \\mu _k , \\Sigma _k))</script></li>\n<li>引入隐变量之后，令第i个样本$x_i$对应的示性函数为$z_i$，这是一个k维one-hot向量，代表第i个样本属于k个高斯模型中哪一个，假设属于第m个模型，则$z_i^m$等于1，其余等于0。现在最大似然估计是：<script type=\"math/tex; mode=display\">\n\\log \\prod _{i=1}^N p(x_i,z_i) \\\\\n= \\log \\prod _{i=1}^N p(z_i) \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K \\pi _k ^{z_i^k} \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K ( \\pi _k N(x_i | \\mu _k , \\Sigma _k)) ^{z_i^k} \\\\\n= \\sum _{i=1}^N \\sum _{k=1}^K z_i^k(\\log \\pi _k + \\log N(x_i | \\mu _k , \\Sigma _k)) \\\\</script></li>\n</ul>\n<h2 id=\"在EM算法中应用蒙特卡罗方法\"><a href=\"#在EM算法中应用蒙特卡罗方法\" class=\"headerlink\" title=\"在EM算法中应用蒙特卡罗方法\"></a>在EM算法中应用蒙特卡罗方法</h2><ul>\n<li>当E步骤无法解析的计算时，可以使用蒙特卡洛近似M步骤的积分：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz</script></li>\n<li>我们根据现在得到的隐变量后验估计$p(z|x,\\theta ^{(t)})$来采样有限个$Z^l$，之后将这些$Z^l$代入$\\log p(x,z|\\theta)$来近似积分：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\approx \\frac 1L \\sum_{l=1}^L  \\log p(x,Z^l|\\theta)</script></li>\n<li>蒙特卡洛EM算法的一个极端的例子是随机EM算法，相当于每次迭代只在E步骤只采样一个样本点。在混合模型求解中，隐变量作为示性函数，只采样一个隐变量意味着hard assignment，每个样本点以1概率分配到某个component，</li>\n<li>蒙特卡洛EM算法推广到贝叶斯框架，就得到IP算法<ul>\n<li>I步骤：<script type=\"math/tex; mode=display\">\np(Z|X)=\\int p(Z | \\theta ,X)p(\\theta | X)d\\theta</script>先从$p(\\theta | X)$中采样$\\theta ^l$，再将其代入，接着从$p(Z | \\theta ^l ,X)$中采样$Z^l$。</li>\n<li>P步骤：<br>从I步骤采样得到的$Z^l$用于估计参数后验：<script type=\"math/tex; mode=display\">\np(\\theta | X) = \\int p(\\theta | Z,X)p(Z|X) dZ  \\\\\n\\approx \\frac 1L \\sum _{l=1}^L p(\\theta | Z^l,X) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"广义EM算法\"><a href=\"#广义EM算法\" class=\"headerlink\" title=\"广义EM算法\"></a>广义EM算法</h2><ul>\n<li>不会鸽</li>\n</ul>\n<h2 id=\"Wake-Sleep算法\"><a href=\"#Wake-Sleep算法\" class=\"headerlink\" title=\"Wake-Sleep算法\"></a>Wake-Sleep算法</h2><ul>\n<li>鸽德哲学</li>\n</ul>\n<h2 id=\"广义EM算法与吉布斯采样\"><a href=\"#广义EM算法与吉布斯采样\" class=\"headerlink\" title=\"广义EM算法与吉布斯采样\"></a>广义EM算法与吉布斯采样</h2><ul>\n<li>当你认为我不会鸽的时候鸽了，亦是一种不鸽</li>\n</ul>\n<h1 id=\"Variational-Inference\"><a href=\"#Variational-Inference\" class=\"headerlink\" title=\"Variational Inference\"></a>Variational Inference</h1><h2 id=\"ELBO\"><a href=\"#ELBO\" class=\"headerlink\" title=\"ELBO\"></a>ELBO</h2><ul>\n<li>接下来介绍变分推断，可以看到，EM算法可以推广到变分推断</li>\n<li>重新推出ELBO与对数似然的关系：<script type=\"math/tex; mode=display\">\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\</script></li>\n<li>两边对隐分布$q(z)$求期望<script type=\"math/tex; mode=display\">\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\</script></li>\n<li>我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。</li>\n<li>接下来就是讨论如何使得ELBO最大化</li>\n</ul>\n<h2 id=\"任意分布上的变分推断\"><a href=\"#任意分布上的变分推断\" class=\"headerlink\" title=\"任意分布上的变分推断\"></a>任意分布上的变分推断</h2><ul>\n<li>对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量</li>\n<li>我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：<script type=\"math/tex; mode=display\">\nq(z)=\\prod _{i=1}^M q_i(z_i)</script></li>\n<li>因此ELBO可以写成两部分<script type=\"math/tex; mode=display\">\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\</script></li>\n<li>其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成<script type=\"math/tex; mode=display\">\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\</script></li>\n<li>在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：<script type=\"math/tex; mode=display\">\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\</script></li>\n<li>这样part1用伪分布的形式可以改写成<script type=\"math/tex; mode=display\">\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\</script></li>\n<li>part2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：<script type=\"math/tex; mode=display\">\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\</script></li>\n<li>再把part1和part2合起来，得到ELBO关于分量j的形式：<script type=\"math/tex; mode=display\">\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\</script></li>\n<li>也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度</li>\n<li>何时这个KL散度最小？也就是：<script type=\"math/tex; mode=display\">\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\</script></li>\n<li>到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。</li>\n</ul>\n<h2 id=\"指数家族分布\"><a href=\"#指数家族分布\" class=\"headerlink\" title=\"指数家族分布\"></a>指数家族分布</h2><ul>\n<li>定义指数家族分布：<script type=\"math/tex; mode=display\">\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\</script></li>\n<li>其中<ul>\n<li>$T(x)$:sufficient statistics</li>\n<li>$\\theta$:parameter of the family</li>\n<li>$\\eta$:natural parameter</li>\n<li>$h(x)$:underlying measure</li>\n<li>$A(\\theta)$:log normalizer / partition function</li>\n</ul>\n</li>\n<li>注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：<script type=\"math/tex; mode=display\">\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))</script></li>\n<li>当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：<script type=\"math/tex; mode=display\">\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\</script></li>\n<li>继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：<script type=\"math/tex; mode=display\">\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\</script></li>\n<li>举个例子，高斯分布写成指数家族分布形式：<script type=\"math/tex; mode=display\">\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )</script></li>\n<li>用自然参数去替代方差和均值，写成指数家族分布形式：<script type=\"math/tex; mode=display\">\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))</script></li>\n<li>其中：<ul>\n<li>$T(x)$:$[x \\ x^2]$</li>\n<li>$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$</li>\n<li>$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$</li>\n</ul>\n</li>\n<li>接下来我们利用指数家族的性质来快速计算均值和方差<script type=\"math/tex; mode=display\">\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\</script></li>\n<li>为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：<script type=\"math/tex; mode=display\">\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\</script></li>\n<li>下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：<script type=\"math/tex; mode=display\">\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\</script></li>\n<li>用向量组的方式改写：<script type=\"math/tex; mode=display\">\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\</script></li>\n<li>原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：<script type=\"math/tex; mode=display\">\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\</script></li>\n<li>我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：<script type=\"math/tex; mode=display\">\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\</script></li>\n<li>这样先验和后验形式一致，也就是共轭</li>\n<li>这样我们用统一的形式写下似然和先验<script type=\"math/tex; mode=display\">\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\</script></li>\n<li>这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：<script type=\"math/tex; mode=display\">\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\</script></li>\n<li>上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。</li>\n</ul>\n<h2 id=\"指数族分布下的变分推断\"><a href=\"#指数族分布下的变分推断\" class=\"headerlink\" title=\"指数族分布下的变分推断\"></a>指数族分布下的变分推断</h2><ul>\n<li>接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁</li>\n<li>我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$</li>\n<li>我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大</li>\n<li>首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：<script type=\"math/tex; mode=display\">\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\</script></li>\n<li>其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：<script type=\"math/tex; mode=display\">\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\</script></li>\n<li>现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\</script></li>\n<li>代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda)</script></li>\n<li>利用之前log normalizer关于参数求导的结论，有:<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)</script></li>\n<li>对上式求导，令其为0，有：<script type=\"math/tex; mode=display\">\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\</script></li>\n<li>我们就得到了$\\lambda$的迭代式！同理可以得到：<script type=\"math/tex; mode=display\">\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\</script></li>\n<li>写完整应该是：<script type=\"math/tex; mode=display\">\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\</script></li>\n<li>观察这两个迭代式，变量更新的路径是:<script type=\"math/tex; mode=display\">\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda</script></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/19/iwWPun.png\" alt=\"iwWPun.png\"></p>\n<h1 id=\"Bayesian-Inference\"><a href=\"#Bayesian-Inference\" class=\"headerlink\" title=\"Bayesian Inference\"></a>Bayesian Inference</h1><ul>\n<li>在贝叶斯推断中，需要区别可观察量（数据）和未知变量（可能是统计参数、缺失数据、隐变量）</li>\n<li>统计参数在贝叶斯框架中被看成是随机变量，我们需要对模型的参数进行概率估计，而在频率学派的框架下，参数是确定的非随机的量，主要针对数据做概率估计</li>\n<li>在频率学派框架中只关注似然$p(x|\\theta)$，而贝叶斯学派认为应将参数$\\theta$作为变量，在观察到数据之前，对参数做出先验假设$p(\\theta)$</li>\n<li>后验正比与似然乘以先验，代表观察到数据后我们对参数先验调整，得到的参数概率分布</li>\n<li>在贝叶斯框架中我们更关注精确度，它是方差的倒数，例如在正态分布的后验中，精确度是先验和数据的精确度之和</li>\n<li>后验实际上是在最大似然估计和先验之间权衡</li>\n<li>当数据非常多时，后验渐渐不再依赖于先验</li>\n<li>很多时候我们并没有先验知识，这时一般采用平坦的、分散的分布作为先验分布，例如范围很大的均匀分布，或者方差很大的正态分布</li>\n<li>有时我们并不需要知道整个后验分布，而仅仅做点估计或者区间估计</li>\n</ul>\n<h1 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h1><ul>\n<li>MCMC，前一个MC代表如何采样，使得采样点满足分布，后一个MC代表用随机采样来估计分布的参数</li>\n<li>最大似然估计和EM算法都是点估计，而MCMC是通过采样找出完整的后验分布</li>\n<li>蒙特卡洛单纯做抽样，是已知分布，但无法直接求得某些函数在此分布上的统计量，因此间接的通过对此分布随机抽样产生样本，通过样本计算统计量</li>\n<li>蒙特卡洛做推断，则是分布未知，已知样本（数据），通过样本反推分布（？待确定）</li>\n</ul>\n<h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul>\n<li>直接通过分布函数很难（或者分布未知）推出一些统计量，我们可以通过产生一系列符合这个分布的样本，通过样本统计计算统计量，即随机采样的方式获得统计量</li>\n<li>在参数推断中，我们可以随机采样出一系列满足参数的后验分布的样本，从而依靠样本估计参数</li>\n<li>最简单的采样：从累计分布函数的逆采样，也就是先从[0,1]做一个均匀分布的采样，然后这个值作为cdf函数的输出值，采样值即cdf的输入值：<script type=\"math/tex; mode=display\">\nu = U(0,1) \\\\\nx= cdf ^{-1} (u) \\\\</script></li>\n</ul>\n<h2 id=\"拒绝采样\"><a href=\"#拒绝采样\" class=\"headerlink\" title=\"拒绝采样\"></a>拒绝采样</h2><ul>\n<li>但是不是所有分布的累积分布函数取逆都容易得到。另外一种采样方法叫做rejection sampling</li>\n<li>对于一个概率密度函数，我们无法直接采样，那么就做一个处处大于概率密度函数的分布，包围着这个函数，如图中红色线包住了绿色线<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oFwd.jpg\" alt=\"i0oFwd.jpg\"></li>\n<li>我们计算出每个点到红线和绿线的距离，将其分为接受和拒绝区域，这样，我们先从红色分布采样得到样本，然后做一个[0,1]均匀分布采样，如果落在接收区域则接收该采样，否则拒接</li>\n<li><p>显然红色分布处处比绿色大是不可能的，积分不为1，因此需要按比例放缩一下，乘以一个系数M，算法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i=0</span><br><span class=\"line\">while i!= N</span><br><span class=\"line\">\tx(i)~q(x) and u~U(0,1)</span><br><span class=\"line\">\tif u&lt; p(x(i))/Mq(x(i)) then</span><br><span class=\"line\">\t\taccept x(i)</span><br><span class=\"line\">\t\ti=i+1</span><br><span class=\"line\">\telse</span><br><span class=\"line\">\t\treject x(i)</span><br><span class=\"line\">\tend</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>rejection sampling效率太低，因为若红色分布选择不好，不能紧紧包住绿色分布时，接受率太低，大部分采样会被拒绝。</p>\n</li>\n</ul>\n<h2 id=\"适应性拒绝采样\"><a href=\"#适应性拒绝采样\" class=\"headerlink\" title=\"适应性拒绝采样\"></a>适应性拒绝采样</h2><ul>\n<li>当分布是log-concave的时候，我们能够有效的构造绿色分布的包络，也就是红色分布比较贴近绿色分布，接受率较高</li>\n<li>基本思想是，将要采样的绿色分布分为k个区域，每个区域最左边的点作为起始点，如果在每个区域能够用绿色分布在起始点的切线来包络的话，我们就可以用这个k个区域上的切线来组成红色区域</li>\n<li>但是这要求在各个区域内原始分布是凹的，但是例如高斯分布的概率密度函数并不是凹函数，但是高斯分布取对数之后是凹的，也就是所谓log-concave，因此我们先取对数，作出切线，然后计算指数还原到原分布，得到原分布的k段切线。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oEFI.jpg\" alt=\"i0oEFI.jpg\"></li>\n</ul>\n<h2 id=\"重要性采样\"><a href=\"#重要性采样\" class=\"headerlink\" title=\"重要性采样\"></a>重要性采样</h2><ul>\n<li>上面提到的采样算法是从简单分布（提议分布）采样，通过简单分布和复杂分布之间的关系计算每个样本的接受率，拒绝掉一些样本，使得剩下的样本满足复杂分布</li>\n<li>importance sampling的思路是对样本点加权而不是简单粗暴的拒绝或者接收，这样可以充分利用每个样本点。</li>\n<li>例如我们希望通过采样得到某个分布的期望<script type=\"math/tex; mode=display\">\nE_{p(x)}(f(x)) = \\int _x f(x)p(x)dx \\\\\nE_{p(x)}(f(x)) = \\int _x f(x) \\frac{p(x)}{q(x)} q(x) dx \\\\\nE_{p(x)}(f(x)) = \\int _x g(x)q(x)dx \\\\</script></li>\n<li>p(x)难以采样，我们就转化为从q(x)采样。其中$\\frac{p(x)}{q(x)}$就是importance weight。</li>\n<li>这样我们消除了红色分布必须包络住绿色分布的限制，只要计算出重要性权重，对采样出的样本点进行重要性加权，就可以得到绿色分布下的一些统计量。</li>\n</ul>\n<h2 id=\"马尔可夫蒙特卡洛和Metropolis-Hasting算法\"><a href=\"#马尔可夫蒙特卡洛和Metropolis-Hasting算法\" class=\"headerlink\" title=\"马尔可夫蒙特卡洛和Metropolis-Hasting算法\"></a>马尔可夫蒙特卡洛和Metropolis-Hasting算法</h2><ul>\n<li>mcmc是另一种采样方法，他将样本序列看作马尔可夫链，通过mcmc采样出的样本之间不是独立的，下一个样本的概率分布与上一个样本有关</li>\n<li>不同于普通采样的接收或者拒绝的概念，在每采样一个样本之后，mcmc会计算在当前样本的前提下，下一个样本的位置的概率分布，也就是关键的转移概率。</li>\n<li>我们抽样一个样本之后，按照转移概率我们抽下一个，得到一系列样本，符合给定的分布，显然这个转移概率是需要和给定分布相关的。我们利用马尔可夫链的收敛性，希望收敛之后的分布$\\pi$就是给定分布，假定转移概率为$k(x^{‘} | x)$，从样本$x$转移到样本$x^{‘}$。</li>\n<li>在马尔可夫链中，有如下Chapman-Kologronvo等式：<script type=\"math/tex; mode=display\">\n\\pi _t (x^{'}) = \\int _x \\pi _{t-1}(x) k(x^{'} | x) dx</script></li>\n<li>这个公式的意义显而易见。我们希望得到马氏链收敛，收敛之后无论怎么转移，得到的一系列样本都满足同一给定分布，则要求：<script type=\"math/tex; mode=display\">\n\\pi _t (x) = \\pi _{t-1} (x)</script></li>\n<li>实际使用时我们依赖于另一个重要的公式，叫做细致平稳条件，the detailed balance：<script type=\"math/tex; mode=display\">\n\\pi (x) k(x^{'} | x) = \\pi (x^{'}) k(x | x^{'})</script></li>\n<li>由detailed balance可以推出Chapman-Kologronvo等式，反之不一定。</li>\n<li>当满足细致平稳条件时，马氏链是收敛的</li>\n<li>在LDA的博客里介绍了mh和gibbs，Metropolis-Hasting就是基本mcmc将一边的接受率提到1的结果：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0okTA.jpg\" alt=\"i0okTA.jpg\"></li>\n<li>在mh中，我们没有改变转移矩阵来适应给定分布，而是用给定分布来修正转移矩阵，因此，转移矩阵是我们自己设计的。一般将转移矩阵（提议分布）设计为以当前状态为中心的高斯分布，对于这个高斯分布，当方差很小时，概率集中在本次采样点附近，那么转移到下次采样时大概率位置不会变动很多，接受率高（因为本次采样点就是通过了接收得到的，大概率是处于高接受率的位置），但这会造成随机游走缓慢；如果方差很大，到处走，接受率就会降低。</li>\n<li>尽管一边的样本接受率提到了1，但总有一边低于1，如果被拒绝，则mcmc会原地重复采样一次，再继续。</li>\n<li>而gibbs则将两边的接受率都提到了1，可以看出，gibbs是mh的一种特例。mh没有修改转移概率，而是添加了接受率，将原先的转移概率和需要采样的分布联系起来。但是显然如果我们自己选择转移概率，且使得转移概率和原始分布的联系越密切，那效果越好，gibbs就是这样的思路。</li>\n</ul>\n<h2 id=\"Hybrid-Metropolis-Hasting\"><a href=\"#Hybrid-Metropolis-Hasting\" class=\"headerlink\" title=\"Hybrid Metropolis-Hasting\"></a>Hybrid Metropolis-Hasting</h2><ul>\n<li>待补充</li>\n</ul>\n<h2 id=\"吉布斯采样\"><a href=\"#吉布斯采样\" class=\"headerlink\" title=\"吉布斯采样\"></a>吉布斯采样</h2><ul>\n<li>吉布斯采样的一个动机：对于多个参数的联合分布，很难直接采样，但是如果固定其他参数作为条件，仅仅对一个参数的条件分布做采样，这时采样会简单许多，且可以证明收敛之后这样采样出来的样本满足联合分布</li>\n<li>先看直觉上为啥吉布斯采样通过条件概率迭代抽样的过程中不改变联合概率分布。首先在排除第i个参数计算条件概率时，这被排除的n-1个变量的边缘分布与真实联合概率分布针对这n-1个变量的边缘分布是一样的，因为它们的值没有改变；条件概率依据的条件相比真实分布是不变的，那条件概率分布也是不变的。边缘分布和条件概率分布都是不变（真实）的，那相乘得到的联合分布自然也是不变的，因此每一步迭代里都是按照真实分布采样且迭代不会改变这个分布。</li>\n<li>吉本斯采样是类似变分推断的coordinate descent方法，一次更新样本的一个分量，依据的转移概率是在给定其他分量情况下当前更新分量所在维度的条件概率：<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oVYt.jpg\" alt=\"i0oVYt.jpg\"></li>\n<li><p>工业上吉布斯采样用的很广，因为它快，事实上这样一种迭代算法不能并行，但是利用collapsed gibbs sampling可以并行化迭代。其原理是将几个分量看成一个整体，collapse成一个分量，当其他分量用这组分量更新时，看成独立的（存在疑问，另一种关于collapse的说法是忽略一些条件变量，基本的gibbs采样就是collapsed gibbs sampling，而这种几个分量看成一个整体的做法是blocked gibbs sampling）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u~p(u|x,y,z)</span><br><span class=\"line\">x,y,z~p(x,y,z|u)</span><br><span class=\"line\">=p(x|u)p(y|u)p(z|u)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>上面关于x,y,z的三个条件概率可以并行计算。</p>\n</li>\n<li>现在我们证明gibbs是mh的一种特例且接受率为1，先看看mh的接受率<script type=\"math/tex; mode=display\">\n\\alpha = min(1,\\frac{\\pi (x^{'}),q(x| x^{'})}{\\pi (x) q(x^{'} | x)})</script></li>\n<li>在gibbs中<script type=\"math/tex; mode=display\">\nq(x|x^{'})=\\pi (x_i | x_{¬i}^{'}) \\\\\nq(x^{'}|x)=\\pi (x_i ^{'} | x_{¬i}) \\\\</script></li>\n<li>而且实际上从$x_{¬i}$到$x_{¬i}^{‘}$，只有第i个分量变了，除了第i个分量之外的其他分量没有改变，因此<script type=\"math/tex; mode=display\">\nx_{¬i}^{'}=x_{¬i}</script></li>\n<li>接下来看看gibbs的接受率<script type=\"math/tex; mode=display\">\n\\alpha _{gibbs} =  min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i}^{'})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'}) \\pi (x_i | x_{¬i})}{\\pi (x) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}^{'}) \\pi( x_{¬i}^{'}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,\\frac{\\pi (x^{'} |  x_{¬i}) \\pi( x_{¬i}) \\pi (x_i | x_{¬i})}{\\pi (x_i | x_{¬i}) \\pi( x_{¬i}) (x_i ^{'} | x_{¬i})}) \\\\\n= min(1,1) \\\\\n= 1 \\\\</script></li>\n</ul>\n<h1 id=\"Expectation-Maximization\"><a href=\"#Expectation-Maximization\" class=\"headerlink\" title=\"Expectation Maximization\"></a>Expectation Maximization</h1><h2 id=\"更新\"><a href=\"#更新\" class=\"headerlink\" title=\"更新\"></a>更新</h2><ul>\n<li>看毛子的deep|bayes2018，提到了用随机梯度下降做EM的M步骤，因为是随机的，所以E步骤只针对一部分数据进行，开销小，可以实现大规模数据上的隐变量模型推断，当时应用在word2vec上，为每一个词添加了一个定性隐变量，指示该词多个意思当中的一个，以期解决歧义问题，甚至还可以用中国餐馆过程将词意个数参数化。有时间再详细看。</li>\n</ul>\n<h2 id=\"公式\"><a href=\"#公式\" class=\"headerlink\" title=\"公式\"></a>公式</h2><ul>\n<li>对于简单的分布，我们想要做参数推断，只需要做最大似然估计，先求对数似然：</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\theta=\\mathop{argmax}_{\\theta} L(X | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\log \\prod p(x_i | \\theta) \\\\\n=\\mathop{argmax}_{\\theta} \\sum \\log p(x_i | \\theta) \\\\</script><ul>\n<li>之后对这个对数似然求导计算极值即可，但是对于复杂的分布，可能并不方便求导</li>\n<li>这时我们可以用EM算法迭代求解。EM算法考虑了概率生成模型当中的隐变量，并为其分配概率，每次迭代更新其概率分布并同时更新参数$\\theta$，可以证明，每一次迭代之后得到的$\\theta$都会使对数似然增加。</li>\n<li>每一次迭代分为两个部分，E和M，也就求期望和最大化<ul>\n<li>求期望，是求$\\log p(x,z|\\theta)$在分布$p(z|x,\\theta ^{(t)})$上的期望，其中$\\theta ^{(t)}$是第t次迭代时计算出的参数</li>\n<li>最大化，也就是求使这个期望最大的$\\theta$，作为本次参数迭代更新的结果</li>\n</ul>\n</li>\n<li>合起来就得到EM算法的公式：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz</script><h2 id=\"为何有效\"><a href=\"#为何有效\" class=\"headerlink\" title=\"为何有效\"></a>为何有效</h2></li>\n<li>也就是证明，每次迭代后最大似然会增加</li>\n<li>要证明：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta ^{(t+1)}) \\geq \\log p(x|\\theta ^{(t)})</script></li>\n<li>先改写对数似然<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\log p(x,z|\\theta) - \\log p(z|x,\\theta) \\\\</script></li>\n<li>两边对分布$p(z|x,\\theta ^{(t)})$求期望，注意到等式左边与z无关，因此求期望之后不变：<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = \\int _z \\log p(x,z|\\theta) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n=Q(\\theta,\\theta ^{(t)})-H(\\theta,\\theta ^{(t)}) \\\\</script></li>\n<li>其中Q部分就是EM算法中的E部分，注意在这里$\\theta$是变量，$\\theta ^{(t)}$是常量</li>\n<li>迭代之后，由于EM算法中M部分作用，Q部分肯定变大了（大于等于），那么使Q部分变大的这个迭代之后新的$\\theta$，代入H部分，H部分会怎么变化呢？</li>\n<li>我们先计算，假如H部分的$\\theta$不变，直接用上一次的$\\theta ^{(t)}$带入，即$H(\\theta ^{(t)},\\theta ^{(t)})$<script type=\"math/tex; mode=display\">\nH(\\theta ^{(t)},\\theta ^{(t)})-H(\\theta,\\theta ^{(t)})= \\\\\n\\int _z \\log p(z|x,\\theta ^{(t)}) p(z|x,\\theta ^{(t)}) dz - \\int _z \\log p(z|x,\\theta) p(z|x,\\theta ^{(t)}) dz \\\\\n= \\int _z \\log (\\frac {p(z|x,\\theta ^{(t)})} {p(z|x,\\theta)} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\int _z \\log (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n\\geq - \\log \\int _z  (\\frac {p(z|x,\\theta)} {p(z|x,\\theta ^{(t)})} ) p(z|x,\\theta ^{(t)}) dz \\\\\n= - \\log 1 \\\\\n= 0 \\\\</script></li>\n<li>其中那个不等式是利用了Jensen不等式。也就是说，直接用上一次的$\\theta ^{(t)}$作为$\\theta$代入H，就是H的最大值!那么无论新的由argmax Q部分得到的$\\theta ^{(t+1)}$是多少，带入    H,H部分都会减小（小于等于）！被减数变大，减数变小，那么得到的结果就是对数似然肯定变大，也就证明了EM算法的有效性</li>\n</ul>\n<h2 id=\"从ELBO的角度理解\"><a href=\"#从ELBO的角度理解\" class=\"headerlink\" title=\"从ELBO的角度理解\"></a>从ELBO的角度理解</h2><ul>\n<li>我们还可以从ELBO（Evidence Lower Bound）的角度推出EM算法的公式</li>\n<li>在之前改写对数似然时我们得到了两个式子$p(x,z|\\theta)$和$p(z|x,\\theta)$，我们引入隐变量的一个分布$q(z)$，对这个两个式子做其与$q(z)$之间的KL散度，可以证明对数似然是这两个KL散度之差：<script type=\"math/tex; mode=display\">\nKL(q(z)||p(z|x,\\theta)) = \\int q(z) [\\log q(z) - \\log p(z|x,\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta) + \\log p(x|\\theta)] dz \\\\\n= \\int q(z) [\\log q(z) - \\log p(x|z,\\theta) - \\log (z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= \\int q(z) [\\log q(z) - \\log p(x,z|\\theta)] dz + \\log p(x|\\theta) \\\\\n= KL(q(z)||p(x,z|\\theta)) + \\log p(x|\\theta) \\\\</script></li>\n<li>也就是<script type=\"math/tex; mode=display\">\n\\log p(x|\\theta) = - KL(q(z)||p(x,z|\\theta)) + KL(q(z)||p(z|x,\\theta))</script></li>\n<li>其中$- KL(q(z)||p(x,z|\\theta))$就是ELBO，因为$ KL(q(z)||p(z|x,\\theta)) \\geq 0 $，因此ELBO是对数似然的下界。我们可以通过最大化这个下界来最大化对数似然</li>\n<li>可以看到，ELBO有两个参数，$q$和$\\theta$，首先我们固定$\\theta ^{(t-1)}$，找到使ELBO最大化的$q^{(t)}$，这一步实际上是EM算法的E步骤，接下来固定$q^{(t)}$，找到使ELBO最大化的$\\theta ^{(t)}$，这一步对应的就是EM算法的M步骤</li>\n<li>我们把$\\theta = \\theta ^{(t-1)}$带入ELBO的表达式：<script type=\"math/tex; mode=display\">\nELBO=\\log p(x|\\theta ^{(t-1)}) - KL(q(z)||p(z|x,\\theta ^{(t-1)}))</script></li>\n<li>q取什么值时ELBO最大？显然当KL散度为0时，ELBO取到最大值，也就是下界达到对数似然本身，这时$q(z)=p(z|x,\\theta ^{(t-1)})$，接下来我们固定$q$，求使ELBO最大的$\\theta$，先把ELBO的定义式改写：<script type=\"math/tex; mode=display\">\nELBO = - KL(q(z)||p(x,z|\\theta)) \\\\\n= \\int q^{(t)}(z) [ \\log p(x,z|\\theta) - \\log q^{(t)}(z)] dz \\\\\n= - \\int q^{(t)}(z) \\log p(x,z|\\theta) - q^{(t)}(z) \\log q^{(t)}(z) dz \\\\</script></li>\n<li>其中第二项与$\\theta$无关，因此：<script type=\"math/tex; mode=display\">\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int q^{(t)}(z) \\log p(x,z|\\theta) dz \\\\</script></li>\n<li>代入上一步得到的$q(z)=p(z|x,\\theta ^{(t-1)})$，得到<script type=\"math/tex; mode=display\">\n\\theta ^{(t)} = \\mathop{argmax} _{\\theta} \\int \\log p(x,z|\\theta)p(z|x,\\theta ^{(t-1)}) dz</script></li>\n<li>同样得到了EM算法的迭代公式</li>\n<li>下面两张图截取自Christopher M. Bishop的Pattern Recognition and Machine Learning，说明了E步骤和M步骤实际在做什么：E步骤将下界ELBO提高到对数似然，但是这时只更新了隐变量，因此对数似然没有变化，而当利用更新的隐变量更新参数$\\theta$，也就是M步骤执行后，我们继续获得了更高的ELBO，以及其对应的对数似然，此时q没有变化，但p发生改变，因此KL不为0，对数似然一定大于ELBO，也就是会提升。直观的来说，我们在E和M步骤都提高了ELBO，E步骤先一口气将ELBO提满到对数似然，之后M步骤依然可以提高ELBO，但对数似然肯定会大于等于（在M步骤时实际上是大于）ELBO，因此对数似然就被M步骤提升的ELBO给“顶上去了”。<br><img src=\"https://s1.ax1x.com/2018/10/20/i0oZfP.png\" alt=\"i0oZfP.png\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0ou6S.png\" alt=\"i0ou6S.png\"></li>\n<li>剩下的问题就是，如何选择z以及q，在混合模型中，可以将z作为示性函数引入，其他在设计时包含隐变量的概率模型里，可以直接将隐变量引入</li>\n</ul>\n<h2 id=\"从假设隐变量为可观察的角度\"><a href=\"#从假设隐变量为可观察的角度\" class=\"headerlink\" title=\"从假设隐变量为可观察的角度\"></a>从假设隐变量为可观察的角度</h2><ul>\n<li>这种理解来自Chuong B Do &amp; Serafim Batzoglou的tutorial:What is the expectation maximization algorithm?</li>\n<li>EM用于包含不可观察隐变量的概率模型推断，事实上，如果我们将隐变量从不可观察变为可观察，针对隐变量每一种可能的取值做最大似然估计，一样可以得到结果，但其时间代价是相当高的。</li>\n<li>EM则改进了这种朴素的算法。一种对EM算法的理解是：EM算法在每次迭代中先猜想一种隐变量的取值概率分布，创造一个考虑了所有隐变量取值可能的加权的训练集，然后在这上面做一个魔改版本的最大似然估计。</li>\n<li>猜想一种隐变量的取值概率分布就是E步骤，但是我们不需要知道具体的概率分布，我们只需要求充分统计量在这个分布上的期望（Expectation）。</li>\n<li>所以说EM算法是最大似然估计在包含隐变量的数据（或者说包含部分不可观察样本的数据）上的自然泛化。</li>\n</ul>\n<h2 id=\"从假设隐变量为缺失值的角度\"><a href=\"#从假设隐变量为缺失值的角度\" class=\"headerlink\" title=\"从假设隐变量为缺失值的角度\"></a>从假设隐变量为缺失值的角度</h2><ul>\n<li>一般如何处理缺失值？用随机值、平均值、0值、聚类中心值代替等等</li>\n<li>EM相当于用均值代替缺失值，也就是隐变量，但是利用了更多的信息：这个均值是在已知的x分布上求期望得到</li>\n<li>EM的迭代就是反复处理缺失值（隐变量），然后基于完整的数据再调整x的分布，再将隐变量看成缺失值进行调整</li>\n</ul>\n<h2 id=\"EM算法与K-means\"><a href=\"#EM算法与K-means\" class=\"headerlink\" title=\"EM算法与K-means\"></a>EM算法与K-means</h2><ul>\n<li>K-means是一种Hard-EM算法，它一样对隐变量的各种可能做出假设（样本属于的类），但是他并不是在类上计算概率和期望，而是比较Hard，只指定一个类作为样本的类，只有这个类概率为1，其余均为0。</li>\n</ul>\n<h2 id=\"隐变量引入的好处\"><a href=\"#隐变量引入的好处\" class=\"headerlink\" title=\"隐变量引入的好处\"></a>隐变量引入的好处</h2><ul>\n<li>其实应该反过来说，很多时候我们凭借逻辑设计了隐变量，然后利用EM算法推断隐变量，而不是刻意设计隐变量来简化运算。</li>\n<li>对于GMM来说，引入隐变量的一个好处是化简了最大似然估计的计算（当然这是假设我们已知隐变量的情况下），将log与求和运算交换，参考了pluskid大神的博客：<a href=\"http://blog.pluskid.org/?p=81\" target=\"_blank\" rel=\"noopener\">漫谈 Clustering (番外篇): Expectation Maximization</a></li>\n<li>对于GMM，引入隐变量作为示性函数之前，最大似然估计是：<script type=\"math/tex; mode=display\">\n\\sum _{i=1}^N \\log (\\sum _{k=1}^K \\pi _k N(x_i | \\mu _k , \\Sigma _k))</script></li>\n<li>引入隐变量之后，令第i个样本$x_i$对应的示性函数为$z_i$，这是一个k维one-hot向量，代表第i个样本属于k个高斯模型中哪一个，假设属于第m个模型，则$z_i^m$等于1，其余等于0。现在最大似然估计是：<script type=\"math/tex; mode=display\">\n\\log \\prod _{i=1}^N p(x_i,z_i) \\\\\n= \\log \\prod _{i=1}^N p(z_i) \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K \\pi _k ^{z_i^k} \\prod _{k=1}^K N(x_i | \\mu _k , \\Sigma _k)^{z_i^k} \\\\\n= \\log \\prod _{i=1}^N  \\prod _{k=1}^K ( \\pi _k N(x_i | \\mu _k , \\Sigma _k)) ^{z_i^k} \\\\\n= \\sum _{i=1}^N \\sum _{k=1}^K z_i^k(\\log \\pi _k + \\log N(x_i | \\mu _k , \\Sigma _k)) \\\\</script></li>\n</ul>\n<h2 id=\"在EM算法中应用蒙特卡罗方法\"><a href=\"#在EM算法中应用蒙特卡罗方法\" class=\"headerlink\" title=\"在EM算法中应用蒙特卡罗方法\"></a>在EM算法中应用蒙特卡罗方法</h2><ul>\n<li>当E步骤无法解析的计算时，可以使用蒙特卡洛近似M步骤的积分：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\int p(z|x,\\theta ^{(t)}) \\log p(x,z|\\theta) dz</script></li>\n<li>我们根据现在得到的隐变量后验估计$p(z|x,\\theta ^{(t)})$来采样有限个$Z^l$，之后将这些$Z^l$代入$\\log p(x,z|\\theta)$来近似积分：<script type=\"math/tex; mode=display\">\n\\theta ^{(t+1)} = \\mathop{argmax} _{\\theta} \\approx \\frac 1L \\sum_{l=1}^L  \\log p(x,Z^l|\\theta)</script></li>\n<li>蒙特卡洛EM算法的一个极端的例子是随机EM算法，相当于每次迭代只在E步骤只采样一个样本点。在混合模型求解中，隐变量作为示性函数，只采样一个隐变量意味着hard assignment，每个样本点以1概率分配到某个component，</li>\n<li>蒙特卡洛EM算法推广到贝叶斯框架，就得到IP算法<ul>\n<li>I步骤：<script type=\"math/tex; mode=display\">\np(Z|X)=\\int p(Z | \\theta ,X)p(\\theta | X)d\\theta</script>先从$p(\\theta | X)$中采样$\\theta ^l$，再将其代入，接着从$p(Z | \\theta ^l ,X)$中采样$Z^l$。</li>\n<li>P步骤：<br>从I步骤采样得到的$Z^l$用于估计参数后验：<script type=\"math/tex; mode=display\">\np(\\theta | X) = \\int p(\\theta | Z,X)p(Z|X) dZ  \\\\\n\\approx \\frac 1L \\sum _{l=1}^L p(\\theta | Z^l,X) \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"广义EM算法\"><a href=\"#广义EM算法\" class=\"headerlink\" title=\"广义EM算法\"></a>广义EM算法</h2><ul>\n<li>不会鸽</li>\n</ul>\n<h2 id=\"Wake-Sleep算法\"><a href=\"#Wake-Sleep算法\" class=\"headerlink\" title=\"Wake-Sleep算法\"></a>Wake-Sleep算法</h2><ul>\n<li>鸽德哲学</li>\n</ul>\n<h2 id=\"广义EM算法与吉布斯采样\"><a href=\"#广义EM算法与吉布斯采样\" class=\"headerlink\" title=\"广义EM算法与吉布斯采样\"></a>广义EM算法与吉布斯采样</h2><ul>\n<li>当你认为我不会鸽的时候鸽了，亦是一种不鸽</li>\n</ul>\n<h1 id=\"Variational-Inference\"><a href=\"#Variational-Inference\" class=\"headerlink\" title=\"Variational Inference\"></a>Variational Inference</h1><h2 id=\"ELBO\"><a href=\"#ELBO\" class=\"headerlink\" title=\"ELBO\"></a>ELBO</h2><ul>\n<li>接下来介绍变分推断，可以看到，EM算法可以推广到变分推断</li>\n<li>重新推出ELBO与对数似然的关系：<script type=\"math/tex; mode=display\">\n\\log p(x) = \\log p(x,z) - \\log p(z|x) \\\\\n= \\log \\frac{p(x,z)}{q(z)} - \\log \\frac{p(z|x)}{q(z)} \\\\\n= \\log p(x,z) - \\log q(z) - \\log \\frac{p(z|x)}{q(z)} \\\\</script></li>\n<li>两边对隐分布$q(z)$求期望<script type=\"math/tex; mode=display\">\n\\log p(x) = \\\\\n[ \\int _z q(z) \\log p(x,z)dz - \\int _z q(z) \\log q(z)dz ] + [- \\int _z \\log \\frac{p(z|x)}{q(z)} q(z) dz ]\\\\\n= ELBO+KL(q||p(z|x)) \\\\</script></li>\n<li>我们希望推断隐变量$z$的后验分布$p(z|x)$，为此我们引入一个分布$q(z)$来近似这个后验。当目前观测量也就是对数似然确定的前提下，近似后验等价于使得$q(z)$和$p(z|x)$的KL散度最小，由上式可以看出，当ELBO最大时，KL散度最小。</li>\n<li>接下来就是讨论如何使得ELBO最大化</li>\n</ul>\n<h2 id=\"任意分布上的变分推断\"><a href=\"#任意分布上的变分推断\" class=\"headerlink\" title=\"任意分布上的变分推断\"></a>任意分布上的变分推断</h2><ul>\n<li>对任意分布使用，一次选取隐变量一个分量更新，比如第j个分量</li>\n<li>我们自己选取的$q(z)$当然要比近似的分布简单，这里假设分布是独立的，隐变量是$M$维的：<script type=\"math/tex; mode=display\">\nq(z)=\\prod _{i=1}^M q_i(z_i)</script></li>\n<li>因此ELBO可以写成两部分<script type=\"math/tex; mode=display\">\nELBO=\\int \\prod q_i(z_i) \\log p(x,z) dz - \\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n=part1-part2 \\\\</script></li>\n<li>其中part1可以写成对隐变量各个维度求多重积分的形式，我们挑出第j个维度将其改写成<script type=\"math/tex; mode=display\">\npart1=\\int \\prod q_i(z_i) \\log p(x,z) dz \\\\\n= \\int _{z_1} \\int _{z_2} ... \\int _{z_M} \\prod _{i=1}^M q_i(z_i) \\log p(x,z) d z_1 , d z_2 , ... ,d z_M \\\\\n= \\int _{z_j} q_j(z_j) ( \\int _{z_{i \\neq j}} \\log (p(x,z)) \\prod _{z_{i \\neq j}} q_i(z_i) d z_i) d z_j \\\\\n= \\int _{z_j}  q_j(z_j) [E_{i \\neq j} [\\log (p(x,z))]] d z_j \\\\</script></li>\n<li>在此我们定义一种伪分布的形式，一种分布的伪分布就是对其对数求积分再求指数：<script type=\"math/tex; mode=display\">\np_j(z_j) = \\int _{i \\neq j} p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\np_j^{'}(z_j) = exp \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\\n\\log p_j^{'}(z_j)  = \\int _{i \\neq j} \\log p(z_1,...,z_i) d z_1 , d z_2 ,..., d z_i \\\\</script></li>\n<li>这样part1用伪分布的形式可以改写成<script type=\"math/tex; mode=display\">\npart1= \\int _{z_j} q_j(z_j) \\log p_j^{'}(x,z_j) \\\\</script></li>\n<li>part2中因为隐变量各个分量独立，可以把函数的和在联合分布上的期望改写成各个函数在边缘分布上的期望的和，在这些和中我们关注第j个变量，其余看成常量：<script type=\"math/tex; mode=display\">\npart2=\\int \\prod q_j(z_j) \\sum \\log q_j(z_j) dz \\\\\n= \\sum ( \\int q_i(z_i) \\log (q_i(z_i)) d z_i ) \\\\\n= \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\</script></li>\n<li>再把part1和part2合起来，得到ELBO关于分量j的形式：<script type=\"math/tex; mode=display\">\nELBO = \\int _{z_j} \\log \\log p_j^{'}(x,z_j) -  \\int q_j(z_j) \\log (q_j(z_j)) d z_j + const \\\\\n= \\int _{z_j} q_j(z_j) \\log \\frac{p_j^{'}(x,z_j)}{q_j(z_j)} + const \\\\\n= - KL(p_j^{'}(x,z_j) || q_j(z_j)) + const\\\\</script></li>\n<li>也就是将ELBO写成了伪分布和近似分布之间的负KL散度，最大化ELBO就是最小化这个KL散度</li>\n<li>何时这个KL散度最小？也就是：<script type=\"math/tex; mode=display\">\nq_j(z_j) = p_j^{'}(x,z_j) \\\\\n\\log q_j(z_j) = E_{i \\neq j} [\\log (p(x,z))] \\\\</script></li>\n<li>到此我们就得到了变分推断下对于隐变量单一分量的近似分布迭代公式，在计算第j个分量的概率时，用到了$\\log (p(x,z))$在其他所有分量$q_i(z_i)$上的期望，之后这个新的第j个分量的概率就参与下一次迭代，计算出其他分量的概率。</li>\n</ul>\n<h2 id=\"指数家族分布\"><a href=\"#指数家族分布\" class=\"headerlink\" title=\"指数家族分布\"></a>指数家族分布</h2><ul>\n<li>定义指数家族分布：<script type=\"math/tex; mode=display\">\np(x | \\theta)=h(x) exp(\\eta (\\theta) \\cdot T(x)-A(\\theta)) \\\\</script></li>\n<li>其中<ul>\n<li>$T(x)$:sufficient statistics</li>\n<li>$\\theta$:parameter of the family</li>\n<li>$\\eta$:natural parameter</li>\n<li>$h(x)$:underlying measure</li>\n<li>$A(\\theta)$:log normalizer / partition function</li>\n</ul>\n</li>\n<li>注意parameter of the family和natural parameter都是向量，当指数家族分布处于标量化参数形式，即$\\eta _i (\\theta) = \\theta _i$的时候，指数家族分布可以写成：<script type=\"math/tex; mode=display\">\np(x | \\eta)=h(x) exp(\\eta (T(x) ^T \\eta - A(\\eta))</script></li>\n<li>当我们把概率密度函数写成指数家族形式，求最大对数似然时，有：<script type=\"math/tex; mode=display\">\n\\eta = \\mathop{argmax} _ {\\eta} [\\log p(X | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log \\prod p(x_i | \\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} [\\log [\\prod h(x_i) exp [(\\sum T(x_i))^T \\eta - n A(\\eta)]]] \\\\\n= \\mathop{argmax} _ {\\eta} (\\sum T(x_i))^T \\eta - n A(\\eta)] \\\\\n= \\mathop{argmax} _ {\\eta} L(\\eta) \\\\</script></li>\n<li>继续求极值，我们就可以得到指数家族分布关于log normalizer和sufficient statistics的很重要的一个性质：<script type=\"math/tex; mode=display\">\n\\frac{\\partial L (\\eta)}{\\partial \\eta} = \\sum T(x_i) - n A^{'}(\\eta) =0 \\\\\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\</script></li>\n<li>举个例子，高斯分布写成指数家族分布形式：<script type=\"math/tex; mode=display\">\np(x) = exp[- \\frac{1}{2 \\sigma ^2}x^2 + \\frac{\\mu}{\\sigma ^2}x - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2)] \\\\\n=exp ( [x \\ x^2] [\\frac{\\mu}{\\sigma ^2} \\ \\frac{-1}{2 \\sigma ^2}] ^T - \\frac{\\mu ^2}{2 \\sigma ^2} - \\frac 12 \\log(2 \\pi \\sigma ^2) )</script></li>\n<li>用自然参数去替代方差和均值，写成指数家族分布形式：<script type=\"math/tex; mode=display\">\np(x) = exp( [x \\ x^2] [ \\eta _1 \\ \\eta _2] ^T + \\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 ) - \\frac 12 \\log (2 \\pi))</script></li>\n<li>其中：<ul>\n<li>$T(x)$:$[x \\ x^2]$</li>\n<li>$\\eta$:$[ \\eta _1 \\ \\eta _2] ^T$</li>\n<li>$-A(\\eta)$:$\\frac{\\eta _1 ^2}{4 \\eta _2} + \\frac 12 \\log (-2 \\eta _2 )$</li>\n</ul>\n</li>\n<li>接下来我们利用指数家族的性质来快速计算均值和方差<script type=\"math/tex; mode=display\">\nA^{'}(\\eta) = \\sum \\frac{T(x_i)}{n} \\\\\n[\\frac{\\partial A}{\\eta _1} \\ \\frac{\\partial A}{\\eta _2}] = [\\frac{- \\eta _1}{2 \\eta _2} \\ \\frac{\\eta _1 ^2 }{2 \\eta _2}-\\frac{1}{2 \\eta _2}] \\\\\n= [\\frac{\\sum x_i}{n} \\ \\frac{\\sum x_i^2}{n}] \\\\\n= [\\mu \\ \\mu ^2 + \\sigma ^2] \\\\</script></li>\n<li>为什么$A(\\eta)$叫做log normalizer？因为把概率密度的指数族分布积分有：<script type=\"math/tex; mode=display\">\n\\int _x \\frac{h(x)exp(T(x)^T \\eta)}{exp(A(\\eta))} = 1 \\\\\nA(\\eta) = \\log \\int _x h(x)exp(T(x)^T \\eta) \\\\</script></li>\n<li>下面讨论指数族分布的共轭关系，假设似然和先验均是指数族分布：<script type=\"math/tex; mode=display\">\np(\\beta | x) ∝ p(x | \\beta) p(\\beta) \\\\\n∝ h(x) exp(T(x) \\beta ^T - A_l (\\beta)) h(\\beta) exp(T(\\beta) \\alpha ^T - A(\\alpha)) \\\\</script></li>\n<li>用向量组的方式改写：<script type=\"math/tex; mode=display\">\nT(\\beta) = [\\beta \\ -g(\\beta)] \\\\\n\\alpha = [\\alpha _1 \\ \\alpha _2] \\\\</script></li>\n<li>原式中关于$\\beta$，$h(x)$和$A(\\alpha)$都是常数，从正比式中消去，带入向量组有：<script type=\"math/tex; mode=display\">\n∝ h(\\beta) exp(T(x) \\beta - A_l(\\beta) + \\alpha _1 \\beta - \\alpha _2 g(\\beta)) \\\\</script></li>\n<li>我们注意到，如果令$-g(\\beta)=-A_l (\\beta)$，原式就可以写成：<script type=\"math/tex; mode=display\">\n∝ h(\\beta) exp((T(x)+\\alpha _1)\\beta - (1+\\alpha _2) A_l (\\beta)) \\\\\n∝ h(\\beta) exp(\\alpha _1 ^{'} \\beta - \\alpha _2 ^{'} A_l (\\beta)) \\\\</script></li>\n<li>这样先验和后验形式一致，也就是共轭</li>\n<li>这样我们用统一的形式写下似然和先验<script type=\"math/tex; mode=display\">\np(\\beta | x, \\alpha) ∝ p(x | \\beta) p(\\beta | \\alpha) \\\\\n∝ h(x)exp[T(x)^T\\beta - A_l(\\beta)] h(\\beta) exp[T(\\beta)^T\\alpha - A_l(\\alpha)] \\\\</script></li>\n<li>这里我们可以计算log normalizer关于参数求导的结果，注意，这是计算得到，不同于之前求指数族分布的最大似然估计得到的关于log normalizer和sufficient statistics的性质：<script type=\"math/tex; mode=display\">\n\\frac{\\partial A_l(\\beta)}{\\partial \\beta}=\\int _x T(x) p(x | \\beta)dx \\\\\n= E_{p(x|\\beta)} [T(x)] \\\\</script></li>\n<li>上式可以通过指数族分布积分为1，积分对$\\beta$求导为0，将这个等式变换证明。</li>\n</ul>\n<h2 id=\"指数族分布下的变分推断\"><a href=\"#指数族分布下的变分推断\" class=\"headerlink\" title=\"指数族分布下的变分推断\"></a>指数族分布下的变分推断</h2><ul>\n<li>接下来我们将ELBO中的参数后验写成指数族分布形式，可以看到最后的迭代公式相当简洁</li>\n<li>我们假定要优化的参数有两个，x和z，我们用$\\lambda$和$\\phi$来近似$\\eta(z,x)$和$\\eta(\\beta ,x)$，依然是要使ELBO最大，这时调整的参数是$q(\\lambda , \\phi)$，实际上是$\\lambda$和$\\phi$</li>\n<li>我们采用固定一个参数，优化另一个参数的方法，相互迭代使得ELBO变大</li>\n<li>首先我们改写ELBO，注意$q(z,\\beta)=q(z)q(\\beta)$：<script type=\"math/tex; mode=display\">\nELBO=E_{q(z,\\beta)}[\\log p(x,z,\\beta)] - E_{q(z,\\beta)}[\\log p(z,\\beta)] \\\\\n= E_{q(z,\\beta)}[\\log p(\\beta | x,z) + \\log p(z | x) + \\log p(x)] - E_{q(z,\\beta)}[\\log q(\\beta)] - E_{q(z,\\beta)}[\\log q(z)] \\\\</script></li>\n<li>其中后验为指数家族分布，且q分布用简单的参数$\\lambda$和$\\phi$去近似：<script type=\"math/tex; mode=display\">\np(\\beta | x,z) = h(\\beta) exp [ T(\\beta) ^T \\eta (z,x) - A_g (\\eta(z,x))] \\\\\n\\approx q(\\beta | \\lambda) \\\\\n= h(\\beta) exp [ T(\\beta) ^T \\eta (\\lambda - A_g (\\eta(\\lambda))] \\\\\np(z | x,\\beta) = h(z) exp [ T(z) ^T \\eta (\\beta,x) - A_l (\\eta(\\beta,x))] \\\\\n\\approx q(\\beta | \\phi) \\\\\n= h(z) exp [ T(z) ^T \\eta (\\phi - A_l (\\eta(\\phi))] \\\\</script></li>\n<li>现在我们固定$\\phi$，优化$\\lambda$，将ELBO中无关常量除去，有：<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = E_{q(z,\\beta)}[\\log p(\\beta | x,z)] - E_{q(z,\\beta)}[\\log q(\\beta)] \\\\</script></li>\n<li>代入指数家族分布，消去无关常量$- E_{q(z)}[A_g(\\eta(x,z))]$，化简得到：<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = E_{q(\\beta)}[T(\\beta)^T] E_{q(z)}[\\eta(z,x)]  -E_{q(\\beta)} [T(\\beta)^T \\lambda] + A_g(\\lambda)</script></li>\n<li>利用之前log normalizer关于参数求导的结论，有:<script type=\"math/tex; mode=display\">\nELBO_{\\lambda} = A_g^{'}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - \\lambda A_g^{'}(\\lambda) ^T + A_g (\\lambda)</script></li>\n<li>对上式求导，令其为0，有：<script type=\"math/tex; mode=display\">\nA_g^{''}(\\lambda)^T[E_{q(z)}[\\eta(z,x)]] - A_g^{'}(\\lambda)-\\lambda A_g^{''}(\\lambda) ^T + A_g^{} (\\lambda) = 0 \\\\\n\\lambda = E_{q(z)}[\\eta(z,x)] \\\\</script></li>\n<li>我们就得到了$\\lambda$的迭代式！同理可以得到：<script type=\"math/tex; mode=display\">\n\\phi = E_{q(\\beta)}[\\eta(\\beta,x)] \\\\</script></li>\n<li>写完整应该是：<script type=\"math/tex; mode=display\">\n\\lambda = E_{q(z | \\phi)}[\\eta(z,x)] \\\\\n\\phi = E_{q(\\beta | \\lambda)}[\\eta(\\beta,x)] \\\\</script></li>\n<li>观察这两个迭代式，变量更新的路径是:<script type=\"math/tex; mode=display\">\n\\lambda \\rightarrow q(\\beta | \\lambda) \\rightarrow \\phi \\rightarrow q(z | \\phi) \\rightarrow \\lambda</script></li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/19/iwWPun.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"推断算法笔记","path":"2018/08/28/inference-algorithm/","eyeCatchImage":"https://s1.ax1x.com/2018/10/19/iwWPun.png","excerpt":"<p>记录Variational Inference、Expectation Maximization、Markov Chain Monte Carlo等用于概率机器学习中未知变量推断的算法的原理、推导。<br>很多内容和推导、图片来自悉尼科技大学徐亦达教授的在线课程及其讲义，徐老师讲非参贝叶斯的一系列视频非常好，可以直接在b站或者优酷搜索他的名字找到视频。<br>徐老师的课程讲义地址：<a href=\"https://github.com/roboticcam/machine-learning-notes\" target=\"_blank\" rel=\"noopener\">roboticcam/machine-learning-notes</a>，如果不额外说明，一些截图和代码均来自徐老师的讲义。<br>其他一些内容来自各种书或者tutorial，引用出处我会在文中说明。</p>","date":"2018-08-28T01:55:10.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","mcmc","inference","variational inference","em"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"LDA学习笔记","date":"2018-07-23T01:56:41.000Z","mathjax":true,"html":true,"_content":"***\nLatent Dirichlet Allocation 文档主题生成模型学习笔记\n主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。\n依然没有彻底弄懂，每次都发现点新东西......\n\n<!--more-->\n![i0oNlT.jpg](https://s1.ax1x.com/2018/10/20/i0oNlT.jpg)\n\n# LDA用来做什么\n-\tLDA是一种主题模型，问题实际上是主题模型是用来做什么？用来表示文档 。在这里将文档看成一个词袋。\n-\t如果将词典里每一个词看成一个特征，tfidf值作为特征值大小来表示文档，则文档的特征向量太过稀疏，且维度太高\n-\tLSI的解决办法是，将文档-词的矩阵进行奇异值分解，降维，但是这样得到的降维空间，即词到文档之间的隐变量无法解释，纯数学的方法，太暴力\n- \tPLSA提出了隐变量应该是主题，可以把文档表示为主题向量，而主题定义为在词典上的某一种多项式分布，这样PLSA中包含了两层多项式分布：文档到主题的多项式分布（文档中各个文档的混合比例，即文档的特征向量），主题到词的多项式分布（在整个词典上的概率分布，表示不同主题下各个词出现的概率）\n-\tLDA则对这两个多项式分布的参数制定了迪利克雷先验，为PLSA引入贝叶斯框架\n\n# 数学基础\n## Gamma函数\n-\t定义 \n$$\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt\n$$\n-\t因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数\n-\tBohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足\n\t-\t$f(1)=1$\n\t-\t$f(x+1)=xf(x)$\n\t-\t$log f(x)$是凸函数\n\t那么$f(x)=\\Gamma(x)$\n-\tDigamma函数\n$$\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}\n$$\n\t其具有以下性质\n$$\n\\psi (x+1)=\\psi (x)+\\frac 1x\n$$\n-\t在用变分推断对LDA进行推断时结果就是digamma函数的形式\n\n## Gamma分布\n-\t将上式变换\n$$\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1\n$$\n\t因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：\n$$\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}\n$$\n-\t指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。\n-\tGamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。\n$$\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}\n$$\n-\t令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。\n\n## Beta分布\n-\t背景：\n\t-\t现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？\n\t-\t为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$\n\t-\t将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为\n$$\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n    &=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}\n$$\n\t-\t若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数\n$$\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]\n$$\n\t用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:\n$$\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n    &=Beta(p|k,n-k+1) \\\\\n    &=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}\n$$\n\t这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。\n-\tBeta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。\n-\t因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。\n\n## Beta-Binomial共轭\n-\t这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。\n-\t假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$\n-\t补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$\n\n| 先验      |  数据知识 | 后验  |\n| :-------: | :-------:| :--: |\n| Beta分布  | 二项分布 |   Beta分布   |\n-\t因此我们可以得到Beta-Binomial共轭\n$$\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)\n$$\n\t即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。\n-\t可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$\n-\t通过这个共轭，我们可以推出关于二项分布的一个重要公式：\n$$\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)\n$$\n\t现在可以证明如下：\n\t-\t式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分\n\t-\t取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$\n\t-\t此时可以得到$P(C \\leq k)=P(X_{k+1}>p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个\n\t-\t这时利用我们对第k大数的概率密度计算出为Beta分布，带入有\n$$\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n\t\t\t&=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n\t\t\t&=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}\n$$\n\t即证\n-\t通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。\n-\t在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。\n\n## Dirichlet-Multinomial共轭\n-\t假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？\n-\t同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}\n$$\n-\t整理一下可以写成\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}\n$$\n\t这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。\n-\t同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布\n$$\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)\n$$\n\t上式中的参数均是向量，对应多维情况。\n-\t无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。\n\n## 总结\n-\t总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。\n-\tLDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。\n-\t为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：\n\n![i0orkR.png](https://s1.ax1x.com/2018/10/20/i0orkR.png)\n\n-\t$\\alpha$控制了多项式分布参数的mean shape和sparsity。\n-\t最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。\n-\t中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。\n-\t最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）\n-\t因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。\n-\t当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。\n-\t再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：\n{% blockquote Dirichlet_distribution https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters Intuitive interpretations of the parameters %}\nThe concentration parameter\nDirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how \"concentrated\" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.\n{% endblockquote %}\n-\t当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。\n-\t在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。\n-\t对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。\n\n# 马尔可夫链蒙特卡洛和吉步斯采样\n## 随机模拟\n-\t即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本，并用这些样本的统计量来估计原分布一些不好直接解析计算的参数。\n-\t马尔可夫是指产生随机样本的方法依赖于马氏链的性质，通过构造马氏链当中的转移矩阵，使得马氏链收敛时能够产生满足给定分布的样本序列。\n-\tMCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。\n\n## 马氏链\n-\t马氏链即状态转移的概率只依赖于前一个状态\n-\t因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果\n-\t矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布\n-\t关于马氏链收敛的定义\n\t-\t如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$\n\t-\t$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$\n\t-\t$\\pi$是方程$\\pi P = \\pi $的唯一非负解\n\t-\t$\\pi$称为马氏链的平稳分布\n\n## Markov Chain Monte Carlo\n-\t回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。\n-\t因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。\n-\t设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：\n$$\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)\n$$\n\t为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。\n-\t如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。\n-\t因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。\n-\t这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。\n\n## Gibbs Sampling\n-\t之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。\n-\t对二维概率分布$p(x,y)$，易得到\n$$\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}\n$$\n-\t从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：\n\t-\t若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$\n\t-\t若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$\n\t-\t若两点连线既不垂直也水平，则$Q=0$\n\t这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二维平面上的马氏链将收敛到$p(x,y)$。\n-\tgibbs采样得到新的x维度之后，在计算新的y维度时是依赖了新的x维度，因为是在之前选定坐标轴转换的基础上再进行转移，不然无法跳转到新状态$(x_2,y_2)$，你得到的实际上是$(x_1,y_2)$和$(x_2,y_1)$。\n-\t因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，从细致平稳条件的公式可以看到这个平稳是可以传递的，如果从某一个维度的转移满足平稳条件，之后接着另一个维度，那么两次转移所等效的一次转移也是平稳的。\n-\t等到所有维度都转移了一次，就得到了一个新的样本。等到马氏链收敛之后形成的样本序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。虽然每次随机选择坐标轴会导致中途计算出来的新的维度值不一样，但是平稳条件没有打破，最终能够收敛到一样的给定分布。\n-\t同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。\n\n## 总结\n-\t首先明确，MCMC方法是产生已知分布的样本，即便gibbs采样使用了完全条件概率，也是产生满足联合分布的样本。\n-\t应用于LDA中，其采样的是每个token分配的主题，这样的样本序列可以得到每个主题下各个词的统计量，以及每篇文档下各个主题的统计量，用这两个统计量来估计主题分布和词分布两个多项式分布的参数，其巧妙的地方在于gibbs采样主题分配的完全条件概率里，避开了主题分布和词分布两个多项式分布（这两个是未知的，要推断的，当然不能从未知分布中采样），利用迪利克雷分布的性质，即使用两个迪利克雷分布的参数比例值来替换两个多项式分布的参数，这样完全条件概率中只包含了人为设定好的迪利克雷参数（超参），以及要迭代的统计量，符合gibbs采样用于推断的要求。\n-\tgibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。\n-\t下文可以看到gibbs采样公式，可以解释为根据其他词的主题分配情况决定自己的主题分配，迭代更新所有词的主题分配；具体如何决定，包含了两个部分，这两个部分类似于tf和idf提供的信息。\n-\t通过采样出来的词的主题分配，用统计量估计参数，这里就是实现了模型的参数推断。\n\n# 文本建模\n-\t接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：\n\t-\t模型是怎样的？\n\t-\t各个词的生成概率或者说模型参数是多少？\n\n## Unigram模型\n-\t模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率。\n-\t这里为unigram引入一层贝叶斯框架，为后文LDA两层贝叶斯框架的推导铺垫。贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。\n-\t也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：\n$$\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}\n$$\n-\t按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：\n$$\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}\n$$\n-\t既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：\n$$\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}\n$$\n\t其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：\n$$\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}\n$$\n-\t得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：\n$$\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}\n$$\n\t这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。\n-\t现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：\n$$\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}\n$$\n\t将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。\n$$\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}\n$$\n\t其中$\\Delta$是归一化因子：\n$$\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}\n$$\n\n## PLSA模型\n-\tPLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。\n-\t事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。\n-\tPLSA模型可以用EM算法迭代学习到参数。\n\n## 总结\n-\t现在整理一下，Unigram模型中主要包含两部分\n\t-\t词生成概率分布\n\t-\t词生成概率分布的分布\n-\tPLSA模型主要包含两部分\n\t-\t词生成概率分布\n\t-\t主题生成概率分布\n-\tUnigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了\n-\tPLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。\n\n# LDA文本建模\n## 模型概述\n-\tLDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设\n\t-\t词生成概率分布（暂记A）\n\t-\t词生成概率分布的分布（暂记B）\n\t-\t主题生成概率分布（暂记C）\n\t-\t主题生成概率分布的分布（暂记D）\n-\t这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：\n\t-\t先在B分布条件下抽样得到K个A分布\n\t-\t对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：\n\t\t-\t从C分布中抽样得到一个主题z\n\t\t-\t选择K个A分布中第z个，从这个A分布中抽样得到一个单词\n-\t假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：\n\n![i0oGYq.png](https://s1.ax1x.com/2018/10/20/i0oGYq.png)\n![i0oJf0.jpg](https://s1.ax1x.com/2018/10/20/i0oJf0.jpg)\n\n## 建立分布\n-\t现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：\n$$\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}\n$$\n-\t最终得到词与主题的联合分布:\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n\n\n## 采样\n-\t首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。\n-\t在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。\n\t-\t其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。\n\t-\t采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。\n-\t公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导\n-\t基于共轭关系推导如下：\n-\t采样的对象是词所对应的主题，概率为：\n$$\np(z_i=k|\\mathop{w}^{\\rightarrow})\n$$\n-\t使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})\n$$\n-\t由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：\n$$\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})\n$$\n-\t把这个公式按主题分布和词分布展开：\n$$\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t由于所有的共轭都是独立的，上式可以写成：\n$$\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：\n$$\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：\n$$\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\\n$$\n-\t而根据共轭关系，有\n$$\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\\n$$\n-\t因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，就在这里绕过了多项式分布！因此最后的概率计算出来就是（注意是正比于）：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t这个概率可以理解为：\n$$\n\\frac{文档d_i中词汇w_i已经被分配到主题j次数+\\alpha}{文档d_i中已经被分配到主题j的词汇\n总数+T\\alpha} \\frac{词汇w_i分配到主题j的次数+\\beta}{所有分配到主题j的词汇总数+W\\beta} \n$$\n-\t其中$w_i$代表第i个token对应的词，$d_i$代表第i个token所在的文档。\n-\t我们再看看基于联合分布如何推导\n-\t之前我们已经得到词和主题的联合分布：\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n-\t根据贝叶斯公式有\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\\n$$\n-\t因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：\n$$\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}\n$$\n-\t将$\\Delta$的表达式带入计算，也可以得到：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：\n\n![i0oU6U.png](https://s1.ax1x.com/2018/10/20/i0oU6U.png)\n\n-\t可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：\n$$\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}\n$$\n## 训练与推断\n-\t接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：\n\t-\t迭代什么？采样并更新词对应的主题\n\t-\t根据什么迭代？gibbs采样的完全条件概率\n\t-\t迭代之后的效果？主题分配改变、统计量改变、下一次gibbs采样的完全条件概率改变\n\t-\t迭代到什么时候为止？Gibbs采样收敛，即采样一段时间区间内主题分布稳定不变，或者根据困惑度等指标来衡量模型收敛的程度。\n-\t训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。\n-\t事实上两个超参$\\alpha$和$\\beta$在训练完之后是经过了很多次后验替换先验的迭代，参数值很大了，$\\alpha$就抛弃了这最后的后验结果，在对新文档产生主题分布时重新用最开始的先验值，这样的话中途得到的训练集上的文档到主题的分布在推断新文档时实际上是用不上的，我们利用的是主题到词的分布：因为只有主题集合是针对整个文档空间的（训练集和测试集），主题到词的分布也是建立在整个文档空间的词典上的，这一部分的k个$\\beta$向量我们保留最后的后验结果，因为这个后验吸收了数据的似然知识后参数值很大，不确定度很小了，基本上每个$\\beta$向量就等同于确定了一个主题到词的多项式分布，也就是确定了一个主题。我们利用这确定的主题，来推断一篇新文档在各个主题上的分布。因此在推断新文档时参数$\\alpha$一般设置为对称的，即各个分量相同（没有先验偏好那个主题），且值很小（即不确定度大，否则生成的主题分布是均匀分布），这里类似于最大熵的思想。推断是利用已知的固定的主题去得到文档到主题的分布。\n-\tLDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。\n\n# LDA in Gensim\n-\tGensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：\n{% blockquote Gensim https://radimrehurek.com/gensim/models/ldamodel.html models.ldamodel – Latent Dirichlet Allocation %}\nalpha ({numpy.ndarray, str}, optional) –\nCan be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n’default’: Learns an asymmetric prior from the corpus.\n{% endblockquote %}\n-\tgensim中没有暴露$\\beta$给用户，用户只能设置$\\alpha$，可以自定义，也可以设置对称或者不对称。其中对称设置即全为1，不对称设置则拟合了zipf law（？），可能$\\beta$的默认设置就是不对称。","source":"_posts/lda.md","raw":"---\ntitle: LDA学习笔记\ndate: 2018-07-23 09:56:41\ncategories: 机器学习\ntags:\n  - lda\n  - math\n  -\tmcmc\nmathjax: true\nhtml: true\n---\n***\nLatent Dirichlet Allocation 文档主题生成模型学习笔记\n主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。\n依然没有彻底弄懂，每次都发现点新东西......\n\n<!--more-->\n![i0oNlT.jpg](https://s1.ax1x.com/2018/10/20/i0oNlT.jpg)\n\n# LDA用来做什么\n-\tLDA是一种主题模型，问题实际上是主题模型是用来做什么？用来表示文档 。在这里将文档看成一个词袋。\n-\t如果将词典里每一个词看成一个特征，tfidf值作为特征值大小来表示文档，则文档的特征向量太过稀疏，且维度太高\n-\tLSI的解决办法是，将文档-词的矩阵进行奇异值分解，降维，但是这样得到的降维空间，即词到文档之间的隐变量无法解释，纯数学的方法，太暴力\n- \tPLSA提出了隐变量应该是主题，可以把文档表示为主题向量，而主题定义为在词典上的某一种多项式分布，这样PLSA中包含了两层多项式分布：文档到主题的多项式分布（文档中各个文档的混合比例，即文档的特征向量），主题到词的多项式分布（在整个词典上的概率分布，表示不同主题下各个词出现的概率）\n-\tLDA则对这两个多项式分布的参数制定了迪利克雷先验，为PLSA引入贝叶斯框架\n\n# 数学基础\n## Gamma函数\n-\t定义 \n$$\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt\n$$\n-\t因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数\n-\tBohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足\n\t-\t$f(1)=1$\n\t-\t$f(x+1)=xf(x)$\n\t-\t$log f(x)$是凸函数\n\t那么$f(x)=\\Gamma(x)$\n-\tDigamma函数\n$$\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}\n$$\n\t其具有以下性质\n$$\n\\psi (x+1)=\\psi (x)+\\frac 1x\n$$\n-\t在用变分推断对LDA进行推断时结果就是digamma函数的形式\n\n## Gamma分布\n-\t将上式变换\n$$\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1\n$$\n\t因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：\n$$\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}\n$$\n-\t指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。\n-\tGamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。\n$$\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}\n$$\n-\t令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。\n\n## Beta分布\n-\t背景：\n\t-\t现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？\n\t-\t为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$\n\t-\t将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为\n$$\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n    &=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}\n$$\n\t-\t若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数\n$$\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]\n$$\n\t用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:\n$$\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n    &=Beta(p|k,n-k+1) \\\\\n    &=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}\n$$\n\t这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。\n-\tBeta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。\n-\t因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。\n\n## Beta-Binomial共轭\n-\t这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。\n-\t假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$\n-\t补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$\n\n| 先验      |  数据知识 | 后验  |\n| :-------: | :-------:| :--: |\n| Beta分布  | 二项分布 |   Beta分布   |\n-\t因此我们可以得到Beta-Binomial共轭\n$$\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)\n$$\n\t即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。\n-\t可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$\n-\t通过这个共轭，我们可以推出关于二项分布的一个重要公式：\n$$\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)\n$$\n\t现在可以证明如下：\n\t-\t式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分\n\t-\t取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$\n\t-\t此时可以得到$P(C \\leq k)=P(X_{k+1}>p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个\n\t-\t这时利用我们对第k大数的概率密度计算出为Beta分布，带入有\n$$\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n\t\t\t&=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n\t\t\t&=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}\n$$\n\t即证\n-\t通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。\n-\t在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。\n\n## Dirichlet-Multinomial共轭\n-\t假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？\n-\t同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}\n$$\n-\t整理一下可以写成\n$$\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}\n$$\n\t这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。\n-\t同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布\n$$\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)\n$$\n\t上式中的参数均是向量，对应多维情况。\n-\t无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。\n\n## 总结\n-\t总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。\n-\tLDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。\n-\t为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：\n\n![i0orkR.png](https://s1.ax1x.com/2018/10/20/i0orkR.png)\n\n-\t$\\alpha$控制了多项式分布参数的mean shape和sparsity。\n-\t最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。\n-\t中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。\n-\t最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）\n-\t因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。\n-\t当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。\n-\t再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：\n{% blockquote Dirichlet_distribution https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters Intuitive interpretations of the parameters %}\nThe concentration parameter\nDirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how \"concentrated\" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.\n{% endblockquote %}\n-\t当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。\n-\t在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。\n-\t对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。\n\n# 马尔可夫链蒙特卡洛和吉步斯采样\n## 随机模拟\n-\t即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本，并用这些样本的统计量来估计原分布一些不好直接解析计算的参数。\n-\t马尔可夫是指产生随机样本的方法依赖于马氏链的性质，通过构造马氏链当中的转移矩阵，使得马氏链收敛时能够产生满足给定分布的样本序列。\n-\tMCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。\n\n## 马氏链\n-\t马氏链即状态转移的概率只依赖于前一个状态\n-\t因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果\n-\t矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布\n-\t关于马氏链收敛的定义\n\t-\t如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$\n\t-\t$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$\n\t-\t$\\pi$是方程$\\pi P = \\pi $的唯一非负解\n\t-\t$\\pi$称为马氏链的平稳分布\n\n## Markov Chain Monte Carlo\n-\t回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。\n-\t因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。\n-\t设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：\n$$\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)\n$$\n\t为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。\n-\t如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。\n-\t因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。\n-\t这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。\n\n## Gibbs Sampling\n-\t之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。\n-\t对二维概率分布$p(x,y)$，易得到\n$$\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}\n$$\n-\t从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：\n\t-\t若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$\n\t-\t若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$\n\t-\t若两点连线既不垂直也水平，则$Q=0$\n\t这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二维平面上的马氏链将收敛到$p(x,y)$。\n-\tgibbs采样得到新的x维度之后，在计算新的y维度时是依赖了新的x维度，因为是在之前选定坐标轴转换的基础上再进行转移，不然无法跳转到新状态$(x_2,y_2)$，你得到的实际上是$(x_1,y_2)$和$(x_2,y_1)$。\n-\t因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，从细致平稳条件的公式可以看到这个平稳是可以传递的，如果从某一个维度的转移满足平稳条件，之后接着另一个维度，那么两次转移所等效的一次转移也是平稳的。\n-\t等到所有维度都转移了一次，就得到了一个新的样本。等到马氏链收敛之后形成的样本序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。虽然每次随机选择坐标轴会导致中途计算出来的新的维度值不一样，但是平稳条件没有打破，最终能够收敛到一样的给定分布。\n-\t同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。\n\n## 总结\n-\t首先明确，MCMC方法是产生已知分布的样本，即便gibbs采样使用了完全条件概率，也是产生满足联合分布的样本。\n-\t应用于LDA中，其采样的是每个token分配的主题，这样的样本序列可以得到每个主题下各个词的统计量，以及每篇文档下各个主题的统计量，用这两个统计量来估计主题分布和词分布两个多项式分布的参数，其巧妙的地方在于gibbs采样主题分配的完全条件概率里，避开了主题分布和词分布两个多项式分布（这两个是未知的，要推断的，当然不能从未知分布中采样），利用迪利克雷分布的性质，即使用两个迪利克雷分布的参数比例值来替换两个多项式分布的参数，这样完全条件概率中只包含了人为设定好的迪利克雷参数（超参），以及要迭代的统计量，符合gibbs采样用于推断的要求。\n-\tgibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。\n-\t下文可以看到gibbs采样公式，可以解释为根据其他词的主题分配情况决定自己的主题分配，迭代更新所有词的主题分配；具体如何决定，包含了两个部分，这两个部分类似于tf和idf提供的信息。\n-\t通过采样出来的词的主题分配，用统计量估计参数，这里就是实现了模型的参数推断。\n\n# 文本建模\n-\t接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：\n\t-\t模型是怎样的？\n\t-\t各个词的生成概率或者说模型参数是多少？\n\n## Unigram模型\n-\t模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率。\n-\t这里为unigram引入一层贝叶斯框架，为后文LDA两层贝叶斯框架的推导铺垫。贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。\n-\t也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：\n$$\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}\n$$\n-\t按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：\n$$\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}\n$$\n-\t既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：\n$$\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}\n$$\n\t其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：\n$$\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}\n$$\n-\t得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：\n$$\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}\n$$\n\t这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。\n-\t现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：\n$$\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}\n$$\n\t将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。\n$$\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}\n$$\n\t其中$\\Delta$是归一化因子：\n$$\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}\n$$\n\n## PLSA模型\n-\tPLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。\n-\t事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。\n-\tPLSA模型可以用EM算法迭代学习到参数。\n\n## 总结\n-\t现在整理一下，Unigram模型中主要包含两部分\n\t-\t词生成概率分布\n\t-\t词生成概率分布的分布\n-\tPLSA模型主要包含两部分\n\t-\t词生成概率分布\n\t-\t主题生成概率分布\n-\tUnigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了\n-\tPLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。\n\n# LDA文本建模\n## 模型概述\n-\tLDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设\n\t-\t词生成概率分布（暂记A）\n\t-\t词生成概率分布的分布（暂记B）\n\t-\t主题生成概率分布（暂记C）\n\t-\t主题生成概率分布的分布（暂记D）\n-\t这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：\n\t-\t先在B分布条件下抽样得到K个A分布\n\t-\t对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：\n\t\t-\t从C分布中抽样得到一个主题z\n\t\t-\t选择K个A分布中第z个，从这个A分布中抽样得到一个单词\n-\t假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：\n\n![i0oGYq.png](https://s1.ax1x.com/2018/10/20/i0oGYq.png)\n![i0oJf0.jpg](https://s1.ax1x.com/2018/10/20/i0oJf0.jpg)\n\n## 建立分布\n-\t现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：\n$$\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}\n$$\n-\t最终得到词与主题的联合分布:\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n\n\n## 采样\n-\t首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。\n-\t在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。\n\t-\t其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。\n\t-\t采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。\n-\t公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导\n-\t基于共轭关系推导如下：\n-\t采样的对象是词所对应的主题，概率为：\n$$\np(z_i=k|\\mathop{w}^{\\rightarrow})\n$$\n-\t使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})\n$$\n-\t由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：\n$$\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})\n$$\n-\t把这个公式按主题分布和词分布展开：\n$$\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t由于所有的共轭都是独立的，上式可以写成：\n$$\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：\n$$\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}\n$$\n-\t已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：\n$$\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\\n$$\n-\t而根据共轭关系，有\n$$\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\\n$$\n-\t因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，就在这里绕过了多项式分布！因此最后的概率计算出来就是（注意是正比于）：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t这个概率可以理解为：\n$$\n\\frac{文档d_i中词汇w_i已经被分配到主题j次数+\\alpha}{文档d_i中已经被分配到主题j的词汇\n总数+T\\alpha} \\frac{词汇w_i分配到主题j的次数+\\beta}{所有分配到主题j的词汇总数+W\\beta} \n$$\n-\t其中$w_i$代表第i个token对应的词，$d_i$代表第i个token所在的文档。\n-\t我们再看看基于联合分布如何推导\n-\t之前我们已经得到词和主题的联合分布：\n$$\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}\n$$\n-\t根据贝叶斯公式有\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\\n$$\n-\t因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：\n$$\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}\n$$\n-\t将$\\Delta$的表达式带入计算，也可以得到：\n$$\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}\n$$\n-\t最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：\n\n![i0oU6U.png](https://s1.ax1x.com/2018/10/20/i0oU6U.png)\n\n-\t可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：\n$$\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}\n$$\n## 训练与推断\n-\t接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：\n\t-\t迭代什么？采样并更新词对应的主题\n\t-\t根据什么迭代？gibbs采样的完全条件概率\n\t-\t迭代之后的效果？主题分配改变、统计量改变、下一次gibbs采样的完全条件概率改变\n\t-\t迭代到什么时候为止？Gibbs采样收敛，即采样一段时间区间内主题分布稳定不变，或者根据困惑度等指标来衡量模型收敛的程度。\n-\t训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。\n-\t事实上两个超参$\\alpha$和$\\beta$在训练完之后是经过了很多次后验替换先验的迭代，参数值很大了，$\\alpha$就抛弃了这最后的后验结果，在对新文档产生主题分布时重新用最开始的先验值，这样的话中途得到的训练集上的文档到主题的分布在推断新文档时实际上是用不上的，我们利用的是主题到词的分布：因为只有主题集合是针对整个文档空间的（训练集和测试集），主题到词的分布也是建立在整个文档空间的词典上的，这一部分的k个$\\beta$向量我们保留最后的后验结果，因为这个后验吸收了数据的似然知识后参数值很大，不确定度很小了，基本上每个$\\beta$向量就等同于确定了一个主题到词的多项式分布，也就是确定了一个主题。我们利用这确定的主题，来推断一篇新文档在各个主题上的分布。因此在推断新文档时参数$\\alpha$一般设置为对称的，即各个分量相同（没有先验偏好那个主题），且值很小（即不确定度大，否则生成的主题分布是均匀分布），这里类似于最大熵的思想。推断是利用已知的固定的主题去得到文档到主题的分布。\n-\tLDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。\n\n# LDA in Gensim\n-\tGensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：\n{% blockquote Gensim https://radimrehurek.com/gensim/models/ldamodel.html models.ldamodel – Latent Dirichlet Allocation %}\nalpha ({numpy.ndarray, str}, optional) –\nCan be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n’default’: Learns an asymmetric prior from the corpus.\n{% endblockquote %}\n-\tgensim中没有暴露$\\beta$给用户，用户只能设置$\\alpha$，可以自定义，也可以设置对称或者不对称。其中对称设置即全为1，不对称设置则拟合了zipf law（？），可能$\\beta$的默认设置就是不对称。","slug":"lda","published":1,"updated":"2019-07-22T03:45:23.225Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vwm006cq8t5vxwvq4in","content":"<hr>\n<p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。<br>依然没有彻底弄懂，每次都发现点新东西……</p>\n<a id=\"more\"></a>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oNlT.jpg\" alt=\"i0oNlT.jpg\"></p>\n<h1 id=\"LDA用来做什么\"><a href=\"#LDA用来做什么\" class=\"headerlink\" title=\"LDA用来做什么\"></a>LDA用来做什么</h1><ul>\n<li>LDA是一种主题模型，问题实际上是主题模型是用来做什么？用来表示文档 。在这里将文档看成一个词袋。</li>\n<li>如果将词典里每一个词看成一个特征，tfidf值作为特征值大小来表示文档，则文档的特征向量太过稀疏，且维度太高</li>\n<li>LSI的解决办法是，将文档-词的矩阵进行奇异值分解，降维，但是这样得到的降维空间，即词到文档之间的隐变量无法解释，纯数学的方法，太暴力</li>\n<li>PLSA提出了隐变量应该是主题，可以把文档表示为主题向量，而主题定义为在词典上的某一种多项式分布，这样PLSA中包含了两层多项式分布：文档到主题的多项式分布（文档中各个文档的混合比例，即文档的特征向量），主题到词的多项式分布（在整个词典上的概率分布，表示不同主题下各个词出现的概率）</li>\n<li>LDA则对这两个多项式分布的参数制定了迪利克雷先验，为PLSA引入贝叶斯框架</li>\n</ul>\n<h1 id=\"数学基础\"><a href=\"#数学基础\" class=\"headerlink\" title=\"数学基础\"></a>数学基础</h1><h2 id=\"Gamma函数\"><a href=\"#Gamma函数\" class=\"headerlink\" title=\"Gamma函数\"></a>Gamma函数</h2><ul>\n<li>定义 <script type=\"math/tex; mode=display\">\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt</script></li>\n<li>因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数</li>\n<li>Bohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足<ul>\n<li>$f(1)=1$</li>\n<li>$f(x+1)=xf(x)$</li>\n<li>$log f(x)$是凸函数<br>那么$f(x)=\\Gamma(x)$</li>\n</ul>\n</li>\n<li>Digamma函数<script type=\"math/tex; mode=display\">\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}</script>其具有以下性质<script type=\"math/tex; mode=display\">\n\\psi (x+1)=\\psi (x)+\\frac 1x</script></li>\n<li>在用变分推断对LDA进行推断时结果就是digamma函数的形式</li>\n</ul>\n<h2 id=\"Gamma分布\"><a href=\"#Gamma分布\" class=\"headerlink\" title=\"Gamma分布\"></a>Gamma分布</h2><ul>\n<li>将上式变换<script type=\"math/tex; mode=display\">\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1</script>因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：<script type=\"math/tex; mode=display\">\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}</script></li>\n<li>指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。</li>\n<li>Gamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。<script type=\"math/tex; mode=display\">\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}</script></li>\n<li>令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。</li>\n</ul>\n<h2 id=\"Beta分布\"><a href=\"#Beta分布\" class=\"headerlink\" title=\"Beta分布\"></a>Beta分布</h2><ul>\n<li>背景：<ul>\n<li>现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？</li>\n<li>为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$</li>\n<li>将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n&=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}</script></li>\n<li>若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数<script type=\"math/tex; mode=display\">\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]</script>用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n&=Beta(p|k,n-k+1) \\\\\n&=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}</script>这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。</li>\n</ul>\n</li>\n<li>Beta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。</li>\n<li>因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。</li>\n</ul>\n<h2 id=\"Beta-Binomial共轭\"><a href=\"#Beta-Binomial共轭\" class=\"headerlink\" title=\"Beta-Binomial共轭\"></a>Beta-Binomial共轭</h2><ul>\n<li>这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。</li>\n<li>假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$</li>\n<li>补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$</li>\n</ul>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">先验</th>\n<th style=\"text-align:center\">数据知识</th>\n<th style=\"text-align:center\">后验</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Beta分布</td>\n<td style=\"text-align:center\">二项分布</td>\n<td style=\"text-align:center\">Beta分布</td>\n</tr>\n</tbody>\n</table>\n</div>\n<ul>\n<li>因此我们可以得到Beta-Binomial共轭<script type=\"math/tex; mode=display\">\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)</script>即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。</li>\n<li>可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$</li>\n<li>通过这个共轭，我们可以推出关于二项分布的一个重要公式：<script type=\"math/tex; mode=display\">\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)</script>现在可以证明如下：<ul>\n<li>式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分</li>\n<li>取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$</li>\n<li>此时可以得到$P(C \\leq k)=P(X_{k+1}&gt;p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个</li>\n<li>这时利用我们对第k大数的概率密度计算出为Beta分布，带入有<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n  &=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n  &=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}</script>即证</li>\n</ul>\n</li>\n<li>通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。</li>\n<li>在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。</li>\n</ul>\n<h2 id=\"Dirichlet-Multinomial共轭\"><a href=\"#Dirichlet-Multinomial共轭\" class=\"headerlink\" title=\"Dirichlet-Multinomial共轭\"></a>Dirichlet-Multinomial共轭</h2><ul>\n<li>假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？</li>\n<li>同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到<script type=\"math/tex; mode=display\">\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}</script></li>\n<li>整理一下可以写成<script type=\"math/tex; mode=display\">\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}</script>这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。</li>\n<li>同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布<script type=\"math/tex; mode=display\">\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)</script>上式中的参数均是向量，对应多维情况。</li>\n<li>无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。</li>\n<li>LDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。</li>\n<li>为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0orkR.png\" alt=\"i0orkR.png\"></p>\n<ul>\n<li>$\\alpha$控制了多项式分布参数的mean shape和sparsity。</li>\n<li>最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。</li>\n<li>中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。</li>\n<li>最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）</li>\n<li>因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。</li>\n<li>当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。</li>\n<li>再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：<blockquote><p>The concentration parameter<br>Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how “concentrated” the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.</p>\n<footer><strong>Dirichlet_distribution</strong><cite><a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters\" target=\"_blank\" rel=\"noopener\">Intuitive interpretations of the parameters</a></cite></footer></blockquote></li>\n<li>当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。</li>\n<li>在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。</li>\n<li>对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。</li>\n</ul>\n<h1 id=\"马尔可夫链蒙特卡洛和吉步斯采样\"><a href=\"#马尔可夫链蒙特卡洛和吉步斯采样\" class=\"headerlink\" title=\"马尔可夫链蒙特卡洛和吉步斯采样\"></a>马尔可夫链蒙特卡洛和吉步斯采样</h1><h2 id=\"随机模拟\"><a href=\"#随机模拟\" class=\"headerlink\" title=\"随机模拟\"></a>随机模拟</h2><ul>\n<li>即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本，并用这些样本的统计量来估计原分布一些不好直接解析计算的参数。</li>\n<li>马尔可夫是指产生随机样本的方法依赖于马氏链的性质，通过构造马氏链当中的转移矩阵，使得马氏链收敛时能够产生满足给定分布的样本序列。</li>\n<li>MCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。</li>\n</ul>\n<h2 id=\"马氏链\"><a href=\"#马氏链\" class=\"headerlink\" title=\"马氏链\"></a>马氏链</h2><ul>\n<li>马氏链即状态转移的概率只依赖于前一个状态</li>\n<li>因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果</li>\n<li>矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布</li>\n<li>关于马氏链收敛的定义<ul>\n<li>如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$</li>\n<li>$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$</li>\n<li>$\\pi$是方程$\\pi P = \\pi $的唯一非负解</li>\n<li>$\\pi$称为马氏链的平稳分布</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h2><ul>\n<li>回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。</li>\n<li>因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。</li>\n<li>设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：<script type=\"math/tex; mode=display\">\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)</script>为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。</li>\n<li>如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。</li>\n<li>因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。</li>\n<li>这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。</li>\n</ul>\n<h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul>\n<li>之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。</li>\n<li>对二维概率分布$p(x,y)$，易得到<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}</script></li>\n<li>从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：<ul>\n<li>若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$</li>\n<li>若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$</li>\n<li>若两点连线既不垂直也水平，则$Q=0$<br>这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二维平面上的马氏链将收敛到$p(x,y)$。</li>\n</ul>\n</li>\n<li>gibbs采样得到新的x维度之后，在计算新的y维度时是依赖了新的x维度，因为是在之前选定坐标轴转换的基础上再进行转移，不然无法跳转到新状态$(x_2,y_2)$，你得到的实际上是$(x_1,y_2)$和$(x_2,y_1)$。</li>\n<li>因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，从细致平稳条件的公式可以看到这个平稳是可以传递的，如果从某一个维度的转移满足平稳条件，之后接着另一个维度，那么两次转移所等效的一次转移也是平稳的。</li>\n<li>等到所有维度都转移了一次，就得到了一个新的样本。等到马氏链收敛之后形成的样本序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。虽然每次随机选择坐标轴会导致中途计算出来的新的维度值不一样，但是平稳条件没有打破，最终能够收敛到一样的给定分布。</li>\n<li>同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。</li>\n</ul>\n<h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>首先明确，MCMC方法是产生已知分布的样本，即便gibbs采样使用了完全条件概率，也是产生满足联合分布的样本。</li>\n<li>应用于LDA中，其采样的是每个token分配的主题，这样的样本序列可以得到每个主题下各个词的统计量，以及每篇文档下各个主题的统计量，用这两个统计量来估计主题分布和词分布两个多项式分布的参数，其巧妙的地方在于gibbs采样主题分配的完全条件概率里，避开了主题分布和词分布两个多项式分布（这两个是未知的，要推断的，当然不能从未知分布中采样），利用迪利克雷分布的性质，即使用两个迪利克雷分布的参数比例值来替换两个多项式分布的参数，这样完全条件概率中只包含了人为设定好的迪利克雷参数（超参），以及要迭代的统计量，符合gibbs采样用于推断的要求。</li>\n<li>gibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。</li>\n<li>下文可以看到gibbs采样公式，可以解释为根据其他词的主题分配情况决定自己的主题分配，迭代更新所有词的主题分配；具体如何决定，包含了两个部分，这两个部分类似于tf和idf提供的信息。</li>\n<li>通过采样出来的词的主题分配，用统计量估计参数，这里就是实现了模型的参数推断。</li>\n</ul>\n<h1 id=\"文本建模\"><a href=\"#文本建模\" class=\"headerlink\" title=\"文本建模\"></a>文本建模</h1><ul>\n<li>接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：<ul>\n<li>模型是怎样的？</li>\n<li>各个词的生成概率或者说模型参数是多少？</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Unigram模型\"><a href=\"#Unigram模型\" class=\"headerlink\" title=\"Unigram模型\"></a>Unigram模型</h2><ul>\n<li>模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率。</li>\n<li>这里为unigram引入一层贝叶斯框架，为后文LDA两层贝叶斯框架的推导铺垫。贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。</li>\n<li>也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：<script type=\"math/tex; mode=display\">\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}</script></li>\n<li>按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}</script></li>\n<li>既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：<script type=\"math/tex; mode=display\">\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}</script>其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}</script></li>\n<li>得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：<script type=\"math/tex; mode=display\">\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}</script>这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。</li>\n<li>现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：<script type=\"math/tex; mode=display\">\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}</script>将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。<script type=\"math/tex; mode=display\">\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}</script>其中$\\Delta$是归一化因子：<script type=\"math/tex; mode=display\">\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}</script></li>\n</ul>\n<h2 id=\"PLSA模型\"><a href=\"#PLSA模型\" class=\"headerlink\" title=\"PLSA模型\"></a>PLSA模型</h2><ul>\n<li>PLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。</li>\n<li>事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。</li>\n<li>PLSA模型可以用EM算法迭代学习到参数。</li>\n</ul>\n<h2 id=\"总结-2\"><a href=\"#总结-2\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>现在整理一下，Unigram模型中主要包含两部分<ul>\n<li>词生成概率分布</li>\n<li>词生成概率分布的分布</li>\n</ul>\n</li>\n<li>PLSA模型主要包含两部分<ul>\n<li>词生成概率分布</li>\n<li>主题生成概率分布</li>\n</ul>\n</li>\n<li>Unigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了</li>\n<li>PLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。</li>\n</ul>\n<h1 id=\"LDA文本建模\"><a href=\"#LDA文本建模\" class=\"headerlink\" title=\"LDA文本建模\"></a>LDA文本建模</h1><h2 id=\"模型概述\"><a href=\"#模型概述\" class=\"headerlink\" title=\"模型概述\"></a>模型概述</h2><ul>\n<li>LDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设<ul>\n<li>词生成概率分布（暂记A）</li>\n<li>词生成概率分布的分布（暂记B）</li>\n<li>主题生成概率分布（暂记C）</li>\n<li>主题生成概率分布的分布（暂记D）</li>\n</ul>\n</li>\n<li>这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：<ul>\n<li>先在B分布条件下抽样得到K个A分布</li>\n<li>对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：<ul>\n<li>从C分布中抽样得到一个主题z</li>\n<li>选择K个A分布中第z个，从这个A分布中抽样得到一个单词</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oGYq.png\" alt=\"i0oGYq.png\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0oJf0.jpg\" alt=\"i0oJf0.jpg\"></p>\n<h2 id=\"建立分布\"><a href=\"#建立分布\" class=\"headerlink\" title=\"建立分布\"></a>建立分布</h2><ul>\n<li>现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}</script></li>\n<li>最终得到词与主题的联合分布:<script type=\"math/tex; mode=display\">\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}</script></li>\n</ul>\n<h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul>\n<li>首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。</li>\n<li>在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。<ul>\n<li>其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。</li>\n<li>采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。</li>\n</ul>\n</li>\n<li>公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导</li>\n<li>基于共轭关系推导如下：</li>\n<li>采样的对象是词所对应的主题，概率为：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{w}^{\\rightarrow})</script></li>\n<li>使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})</script></li>\n<li>由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：<script type=\"math/tex; mode=display\">\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})</script></li>\n<li>把这个公式按主题分布和词分布展开：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>由于所有的共轭都是独立的，上式可以写成：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\</script></li>\n<li>而根据共轭关系，有<script type=\"math/tex; mode=display\">\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\</script></li>\n<li>因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，就在这里绕过了多项式分布！因此最后的概率计算出来就是（注意是正比于）：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>这个概率可以理解为：<script type=\"math/tex; mode=display\">\n\\frac{文档d_i中词汇w_i已经被分配到主题j次数+\\alpha}{文档d_i中已经被分配到主题j的词汇\n总数+T\\alpha} \\frac{词汇w_i分配到主题j的次数+\\beta}{所有分配到主题j的词汇总数+W\\beta}</script></li>\n<li>其中$w_i$代表第i个token对应的词，$d_i$代表第i个token所在的文档。</li>\n<li>我们再看看基于联合分布如何推导</li>\n<li>之前我们已经得到词和主题的联合分布：<script type=\"math/tex; mode=display\">\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}</script></li>\n<li>根据贝叶斯公式有<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\</script></li>\n<li>因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：<script type=\"math/tex; mode=display\">\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}</script></li>\n<li>将$\\Delta$的表达式带入计算，也可以得到：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oU6U.png\" alt=\"i0oU6U.png\"></p>\n<ul>\n<li>可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：<script type=\"math/tex; mode=display\">\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}</script><h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2></li>\n<li>接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：<ul>\n<li>迭代什么？采样并更新词对应的主题</li>\n<li>根据什么迭代？gibbs采样的完全条件概率</li>\n<li>迭代之后的效果？主题分配改变、统计量改变、下一次gibbs采样的完全条件概率改变</li>\n<li>迭代到什么时候为止？Gibbs采样收敛，即采样一段时间区间内主题分布稳定不变，或者根据困惑度等指标来衡量模型收敛的程度。</li>\n</ul>\n</li>\n<li>训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。</li>\n<li>事实上两个超参$\\alpha$和$\\beta$在训练完之后是经过了很多次后验替换先验的迭代，参数值很大了，$\\alpha$就抛弃了这最后的后验结果，在对新文档产生主题分布时重新用最开始的先验值，这样的话中途得到的训练集上的文档到主题的分布在推断新文档时实际上是用不上的，我们利用的是主题到词的分布：因为只有主题集合是针对整个文档空间的（训练集和测试集），主题到词的分布也是建立在整个文档空间的词典上的，这一部分的k个$\\beta$向量我们保留最后的后验结果，因为这个后验吸收了数据的似然知识后参数值很大，不确定度很小了，基本上每个$\\beta$向量就等同于确定了一个主题到词的多项式分布，也就是确定了一个主题。我们利用这确定的主题，来推断一篇新文档在各个主题上的分布。因此在推断新文档时参数$\\alpha$一般设置为对称的，即各个分量相同（没有先验偏好那个主题），且值很小（即不确定度大，否则生成的主题分布是均匀分布），这里类似于最大熵的思想。推断是利用已知的固定的主题去得到文档到主题的分布。</li>\n<li>LDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。</li>\n</ul>\n<h1 id=\"LDA-in-Gensim\"><a href=\"#LDA-in-Gensim\" class=\"headerlink\" title=\"LDA in Gensim\"></a>LDA in Gensim</h1><ul>\n<li>Gensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：<blockquote><p>alpha ({numpy.ndarray, str}, optional) –<br>Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:<br>’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.<br>’default’: Learns an asymmetric prior from the corpus.</p>\n<footer><strong>Gensim</strong><cite><a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\" target=\"_blank\" rel=\"noopener\">models.ldamodel – Latent Dirichlet Allocation</a></cite></footer></blockquote></li>\n<li>gensim中没有暴露$\\beta$给用户，用户只能设置$\\alpha$，可以自定义，也可以设置对称或者不对称。其中对称设置即全为1，不对称设置则拟合了zipf law（？），可能$\\beta$的默认设置就是不对称。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<hr>\n<p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。<br>依然没有彻底弄懂，每次都发现点新东西……</p>","more":"<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oNlT.jpg\" alt=\"i0oNlT.jpg\"></p>\n<h1 id=\"LDA用来做什么\"><a href=\"#LDA用来做什么\" class=\"headerlink\" title=\"LDA用来做什么\"></a>LDA用来做什么</h1><ul>\n<li>LDA是一种主题模型，问题实际上是主题模型是用来做什么？用来表示文档 。在这里将文档看成一个词袋。</li>\n<li>如果将词典里每一个词看成一个特征，tfidf值作为特征值大小来表示文档，则文档的特征向量太过稀疏，且维度太高</li>\n<li>LSI的解决办法是，将文档-词的矩阵进行奇异值分解，降维，但是这样得到的降维空间，即词到文档之间的隐变量无法解释，纯数学的方法，太暴力</li>\n<li>PLSA提出了隐变量应该是主题，可以把文档表示为主题向量，而主题定义为在词典上的某一种多项式分布，这样PLSA中包含了两层多项式分布：文档到主题的多项式分布（文档中各个文档的混合比例，即文档的特征向量），主题到词的多项式分布（在整个词典上的概率分布，表示不同主题下各个词出现的概率）</li>\n<li>LDA则对这两个多项式分布的参数制定了迪利克雷先验，为PLSA引入贝叶斯框架</li>\n</ul>\n<h1 id=\"数学基础\"><a href=\"#数学基础\" class=\"headerlink\" title=\"数学基础\"></a>数学基础</h1><h2 id=\"Gamma函数\"><a href=\"#Gamma函数\" class=\"headerlink\" title=\"Gamma函数\"></a>Gamma函数</h2><ul>\n<li>定义 <script type=\"math/tex; mode=display\">\n\\Gamma (x) = \\int _0 ^{\\infty} t^{x-1} e^{-t} dt</script></li>\n<li>因为其递归性质$\\Gamma(x+1)=x\\Gamma(x)$，可以将阶乘的定义扩展到实数域，进而将函数导数的定义扩展到实数集，例如计算1/2阶导数</li>\n<li>Bohr-Mullerup定理：如果$f:(0,\\infty) \\rightarrow (0,\\infty)$，且满足<ul>\n<li>$f(1)=1$</li>\n<li>$f(x+1)=xf(x)$</li>\n<li>$log f(x)$是凸函数<br>那么$f(x)=\\Gamma(x)$</li>\n</ul>\n</li>\n<li>Digamma函数<script type=\"math/tex; mode=display\">\n\\psi (x)=\\frac{d log \\Gamma(x)}{dx}</script>其具有以下性质<script type=\"math/tex; mode=display\">\n\\psi (x+1)=\\psi (x)+\\frac 1x</script></li>\n<li>在用变分推断对LDA进行推断时结果就是digamma函数的形式</li>\n</ul>\n<h2 id=\"Gamma分布\"><a href=\"#Gamma分布\" class=\"headerlink\" title=\"Gamma分布\"></a>Gamma分布</h2><ul>\n<li>将上式变换<script type=\"math/tex; mode=display\">\n\\int _0 ^{\\infty} \\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}dx = 1</script>因此可取积分中的函数作为概率密度，得到形式最简单的Gamma分布的密度函数：<script type=\"math/tex; mode=display\">\nGamma(x|\\alpha)=\\frac{x^{\\alpha -1}e^{-x}}{\\Gamma(\\alpha)}</script></li>\n<li>指数分布和$\\chi ^2$分布都是特殊的Gamma分布，且作为先验分布非常有用，广泛应用于贝叶斯分析当中。</li>\n<li>Gamma分布与泊松分布具有形式上的一致性，实际上在两者的通常表示中，仅仅只有参数差1的区别，且之前说到阶乘可以用Gamma函数表示，因此可以直观的认为Gamma分布是Poisson分布在正实数集上的连续化版本。<script type=\"math/tex; mode=display\">\nPoisson(X=k|\\lambda)=\\frac{\\lambda ^k e^{-\\lambda}}{k!}</script></li>\n<li>令二项分布中$np=\\lambda$，当n趋向于无穷时，泊松分布就是二项分布的极限分布。经常用于解释泊松分布的一个例子是交换机接收呼叫，假设将整个时间分成若干个时间段，每个时间段内至多达到一次呼叫，概率为p，则总体呼叫数符合一个二项分布，当np为定值$\\lambda$时，将时间分为无穷个段，几乎是连续的，取$p=\\frac{\\lambda}{n}$带入二项分布，取极限后即得到泊松分布。在此基础上分布的取值连续化（即将泊松分布中k的阶乘用Gamma函数替代）就得到Gamma分布。</li>\n</ul>\n<h2 id=\"Beta分布\"><a href=\"#Beta分布\" class=\"headerlink\" title=\"Beta分布\"></a>Beta分布</h2><ul>\n<li>背景：<ul>\n<li>现在有n个在[0,1]区间上均匀分布的随机变量，将这n个随机变量按大小排序后，如何求得第k个顺序统计量$p=x_k$的分布？</li>\n<li>为了求分布，利用极限的思想，我们求这个变量落在一小段区间上的概率$P(x) \\leq X_{k} \\leq x+\\Delta x$</li>\n<li>将整个区间分为三部分：小区间以前，小区间，小区间以后，若小区间内只有第k大的数，则小区间以前应该有k-1个数，小区间以后应该有n-k个数,这种情况的概率为<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(E)&=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x \\\\\n&=x^{k-1}(1-x)^{n-k}\\Delta x+\\omicron (\\Delta x) \\\\\n\\end{aligned}</script></li>\n<li>若小区间内有两个及两个以上的数，计算可得这种情况的概率是$\\omicron (\\Delta x)$，因此只考虑小区间内只有第k大的数，此时令$\\Delta x$趋向于0，则得到第k大数的概率密度函数<script type=\"math/tex; mode=display\">\nf(x)=\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]</script>用Gamma函数表示（令$\\alpha =k,\\beta = n-k+1$）得到:<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nf(x)&=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\\n&=Beta(p|k,n-k+1) \\\\\n&=Beta(\\alpha,\\beta) \\\\\n\\end{aligned}</script>这就是Beta分布，我们可以取概率分布最大的点作为第k大数的预测值。</li>\n</ul>\n</li>\n<li>Beta分布实际上是对分布的预测，即分布的分布。在背景里，我们要找第k大顺序统计量的概率分布，我们把这个第k大顺序统计量记作p，现在知道有n个在[0,1]区间上均匀分布的值，n和k建立了一个在[0,1]区间内的相对位置，这也就是$\\alpha$和$\\beta$的作用（因为$\\alpha$和$\\beta$就是n和k计算出来的），代表我倾向于认为p在[0,1]中的哪里。因为n个统计量是均匀分布的，而p是其中第k大的，那么p就倾向于在$\\frac kn$这个位置。</li>\n<li>因此Beta分布的参数（$\\alpha$和$\\beta$）意义很明显了，就是我在什么都不知道的情况下，先入为主的认为p可能所在的位置，即先验，而Beta分布的结果就是在这种先验影响下，计算出来的p的分布，这个p的实际含义，是二项式分布的参数。因此，Beta分布可以作为二项式分布的参数先验。</li>\n</ul>\n<h2 id=\"Beta-Binomial共轭\"><a href=\"#Beta-Binomial共轭\" class=\"headerlink\" title=\"Beta-Binomial共轭\"></a>Beta-Binomial共轭</h2><ul>\n<li>这样我们可以对第k大数的分布建立一个Beta分布的先验，如果现在我们知道有m个在[0,1]之间均匀分布的数，且其中有几个比第k大数大，有几个比第k大数小，则可以将其作为一种数据知识补充进去，形成后验分布。</li>\n<li>假设要猜的数$p=X_k$，现在我们知道了有$m_1$个数比$p$小，$m_2$个数比$p$大，显然此时$p$的概率密度函数就变成了$Beta(p|k+m_1,n-k+1+m_2)$</li>\n<li>补充进m个数的数据知识相对于做了m次贝努利实验，因为我们讨论的范围在[0,1]上，m个数只关心比$p$大还是小，$p=X_k$的数值便可以代表这一概率，因此$m_1$服从二项分布$B(m,p)$</li>\n</ul>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">先验</th>\n<th style=\"text-align:center\">数据知识</th>\n<th style=\"text-align:center\">后验</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Beta分布</td>\n<td style=\"text-align:center\">二项分布</td>\n<td style=\"text-align:center\">Beta分布</td>\n</tr>\n</tbody>\n</table>\n</div>\n<ul>\n<li>因此我们可以得到Beta-Binomial共轭<script type=\"math/tex; mode=display\">\nBeta(p|\\alpha,\\beta)+BinomCount(m_1,m_2)=Beta(p|\\alpha+m_1,\\beta+m_2)</script>即数据符合二项分布时，参数的先验分布和后验分布都能保持Beta分布的形式，我们能够在先验分布中赋予参数明确的物理意义，并将其延续到后验分布中进行解释，因此一般将Beta分布中的参数$\\alpha,\\beta$称为伪计数，表示物理计数。</li>\n<li>可以验证的是当Beta分布的两个参数均为1时，就是均匀分布，这时对共轭关系可以看成：开始对硬币不均匀性不知道，假设硬币正面向上的概率为均匀分布，在投掷了m次硬币后，获得了$m_1$次向上其他次向下的数据知识，通过贝叶斯公式计算后验概率，可以算出正面向上的概率正好是$Beta(p|m_1+1,m_2+1)$</li>\n<li>通过这个共轭，我们可以推出关于二项分布的一个重要公式：<script type=\"math/tex; mode=display\">\nP(C \\leq k) = \\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\quad C \\sim B(n,p)</script>现在可以证明如下：<ul>\n<li>式子左边是二项分布的概率累积，式子右边时是$Beta(t|k+1,n-k)$分布的概率积分</li>\n<li>取n个随机变量，均匀分布于[0,1]，对于二项分布$B(n,p)$，若数小于$p$则是成功，否则失败，则n个随机变量小于$p$的个数C符合二项分布$B(n,p)$</li>\n<li>此时可以得到$P(C \\leq k)=P(X_{k+1}&gt;p)$，即n个随机变量按顺序排好后，小于$p$的有$k$个</li>\n<li>这时利用我们对第k大数的概率密度计算出为Beta分布，带入有<script type=\"math/tex; mode=display\">\n\\begin{aligned}\nP(C \\leq k) &=P(X_{k+1} > p) \\\\\n  &=\\int _p ^1 Beta(t|k+1,n-k)dt \\\\\n  &=\\frac{n!}{k!(n-k-1)!}\\int _p ^1 t^k(1-t)^{n-k-1}dt \\\\\n\\end{aligned}</script>即证</li>\n</ul>\n</li>\n<li>通过这个式子，将n取极限到无穷大，转换为泊松分布，可以推导出Gamma分布。</li>\n<li>在本节中，我们为先验引入了其他信息，就是有几个数比第k大数大和小，这些信息相当于是告诉我：我可以修改p的先验，我之前倾向于认为的p的位置改动了。假如我知道了几个数比p大，那么p的先验位置应该往后移，如果我知道了几个数比p小，那么p的先验位置应该往前移，如果我同时知道了100个数比p大，100个数比p小呢？p的位置不变，但是我更加确信了p真实的位置就是现在这个先验位置，因此Beta分布在这个先验位置上更集中，从后文分析Dirichlet参数的意义中我们会再次看到这一点。先验加上数据知识修改之后，就形成了后验，这就是贝叶斯公式的基本内容。</li>\n</ul>\n<h2 id=\"Dirichlet-Multinomial共轭\"><a href=\"#Dirichlet-Multinomial共轭\" class=\"headerlink\" title=\"Dirichlet-Multinomial共轭\"></a>Dirichlet-Multinomial共轭</h2><ul>\n<li>假设我们不仅要猜一个数，还要猜两个数$x_{k_1},x_{k_1+k_2}$的联合分布，该如何计算？</li>\n<li>同理，我们设置两个极小区间$\\Delta x$，将整个区间分为五块，极小区间之间分别为$x_1,x_2,x_3$计算之后可以得到<script type=\"math/tex; mode=display\">\nf(x_1,x_2,x_3)=\\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}</script></li>\n<li>整理一下可以写成<script type=\"math/tex; mode=display\">\nf(x_1,x_2,x_3)=\\frac{\\Gamma(\\alpha _1+\\alpha _2+\\alpha _3)}{\\Gamma(\\alpha _1)\\Gamma(\\alpha _2)\\Gamma(\\alpha _3)}x_1^{\\alpha _1-1}x_2^{\\alpha _2-1}x_3^{\\alpha _3-1}</script>这就是3维形式的Dirichlet分布。其中$x_1,x_2,x_3$（实际上只有两个变量）确定了两个顺序数联合分布，$f$代表概率密度。</li>\n<li>同样，我们也可以对Dirichlet分布的先验加入数据知识，其后验依然是Dirichlet分布<script type=\"math/tex; mode=display\">\nDir(p|\\alpha)+MultCount(m)=Dir(p|\\alpha+m)</script>上式中的参数均是向量，对应多维情况。</li>\n<li>无论是Beta分布还是Dirichlet分布，都有一个很重要的性质，即他们的均值可以用参数的比例表示，例如对于Beta分布，$E(p)=\\frac{\\alpha}{\\alpha+\\beta}$，对于Dirichlet分布，均值是一个向量，对应于各个参数比例组成的向量。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>总结一下共轭：设有分布A，A的参数分布（或者叫分布的分布）为B，若B的先验在获得A的数据知识之后得到的后验与先验属于同一类分布，则A与B共轭，B称为A的参数共轭分布（或者叫先验共轭） ，在上文提到的例子里，Beta分布是二项分布的参数共轭分布，Dirichlet分布是多项式分布的参数共轭分布。</li>\n<li>LDA实际上是将文本生成建模为一个概率生成模型，具体而言是一个三层贝叶斯模型，并且针对文档-主题分布和主题-词语分布分别假设其先验分布为Dirichlet分布，用Dirichlet-Multinomial共轭来利用数据知识更新其后验，因此先介绍Gamma函数及其分布，Gamma函数将阶乘扩展到实数域，之后介绍了能估计分布的Beta函数，在引入了Gamma函数之后Beta分布的参数能扩展到实数域。之后介绍了Beta-Binomial共轭，这种共轭带来的好处是在用数据训练修正时，我们已经知道了后验分布的形式，之后将这种共轭关系扩展到高维（估计多个分布），就得到了Dirichlet-Multinomial共轭。</li>\n<li>为文档到主题和主题到词的两个多项式分布加入Dirichlet分布作为参数先验的好处是：将多项式参数作为变量，先验信息指导了参数在哪个范围内变动，而不是具体的值，使得模型在小训练样本内的泛化能力更强。根据Dirichlet的性质，其参数比例代表了[0,1]上的一个划分，决定了dirichlet分布高概率的位置，其参数大小决定了高概率的比例（陡峭），例如下图，多项式分布有三项，参数分别为$p_1,p_2,p_3$，他们的和为一且各项大于零，在三维空间内便是一个三角面，面上每一点代表一种多项式分布，红色区域概率高，蓝色区域概率低：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0orkR.png\" alt=\"i0orkR.png\"></p>\n<ul>\n<li>$\\alpha$控制了多项式分布参数的mean shape和sparsity。</li>\n<li>最左边，Dirichlet的三个参数$\\alpha _1,\\alpha _2,\\alpha _3$相等，代表其红色区域位置居中，且三个$\\alpha$的值均较大，红色区域较小，把热力图看成等高线图的话就代表红色区域较陡峭，说明Dirichlet非常确认多项式分布的参数会在居中的位置。对于三个多项式分布的参数$p_1,p_2,p_3$来说，较大可能取到三个p等值的情况。</li>\n<li>中间的图，三个$\\alpha$不相等，某一个$\\alpha$偏大，导致红色区域偏向三角面某一角，导致某一个p取值较大，其余两个p取值较小的可能性比较大，这时体现参数先验的参数作为concentration的作用，将概率注意力集中于某些项。</li>\n<li>最右边，同最左边，三个$\\alpha$相等，红色区域居中，但是$\\alpha$的值均偏小，导致红色区域发散，也就是Dirichlet认为三个p的值应该在最中间，但是不那么确定。结果就是三个p之间有差别，但差别不会太大（依然在最中间附近），不会出现非常陡峭的情况（最陡峭也就是最中间无限高，概率为1，三个p值一定相同）</li>\n<li>因此可以看出，$\\alpha$的比例决定了多项式分布高概率的位置，也就是主要确定了各个$p$的比例，定好concentration，而$\\alpha$的大小决定了这个位置的集中情况，$\\alpha$越小，位置越集中，p的分布越确定，反之p的分布由红色区域位置大致确定，但是变化范围较大。</li>\n<li>当$\\alpha$远小于1时，Dirichelt分布会走向另一个极端，在上面这个三角面的例子里，红色区域依然会陡峭，但是陡峭在三角面的三个角上，可以想成$alpha$从大变到1，再变小，大概率密度区域从最中间慢慢分散到整个面，然后又聚集在三个角。</li>\n<li>再来看看维基百科中关于Dirichlet参数$\\alpha$的描述：<blockquote><p>The concentration parameter<br>Dirichlet distributions are very often used as prior distributions in Bayesian inference. The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter. If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution, then intuitively the concentration parameter can be thought of as determining how “concentrated” the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.</p>\n<footer><strong>Dirichlet_distribution</strong><cite><a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution#Intuitive_interpretations_of_the_parameters\" target=\"_blank\" rel=\"noopener\">Intuitive interpretations of the parameters</a></cite></footer></blockquote></li>\n<li>当$\\alpha$远小于1时，概率密度会主要堆积在一个或少数几个项上，也就是红色区域聚集在三个角的情况，这时Dirichlet分布抽样得到的多项式分布大概率在角上，也就是概率密度堆积在一个项上，其余两个项概率近似为0。$\\alpha$远大于1时，概率密度会分散到各个部分，就是对应三图中最左边的图，三个项概率相差不大的可能性比较大。</li>\n<li>在Dirichlet Process中$\\alpha$的大小体现了Dirichlet分布拟合Base Measure时的离散程度，$\\alpha$越大，越不离散，各个项均能得到差不多的概率。</li>\n<li>对应到LDA模型中，这种离散程度就代表了文档是集中在某几个主题上还是在所有主题上较均匀的分布，或者主题是集中在少数词上还是在整体的词上较均匀的分布。</li>\n</ul>\n<h1 id=\"马尔可夫链蒙特卡洛和吉步斯采样\"><a href=\"#马尔可夫链蒙特卡洛和吉步斯采样\" class=\"headerlink\" title=\"马尔可夫链蒙特卡洛和吉步斯采样\"></a>马尔可夫链蒙特卡洛和吉步斯采样</h1><h2 id=\"随机模拟\"><a href=\"#随机模拟\" class=\"headerlink\" title=\"随机模拟\"></a>随机模拟</h2><ul>\n<li>即蒙特卡洛的含义，用于已知分布，需要生成一系列满足此分布的随机样本，并用这些样本的统计量来估计原分布一些不好直接解析计算的参数。</li>\n<li>马尔可夫是指产生随机样本的方法依赖于马氏链的性质，通过构造马氏链当中的转移矩阵，使得马氏链收敛时能够产生满足给定分布的样本序列。</li>\n<li>MCMC的一种方法是随机采样，利用已知分布计算出接受率，只接收部分采样，而吉布斯采样是接受率为1的一种MCMC方法。它提升了接受率，但是加长了采样过程。</li>\n</ul>\n<h2 id=\"马氏链\"><a href=\"#马氏链\" class=\"headerlink\" title=\"马氏链\"></a>马氏链</h2><ul>\n<li>马氏链即状态转移的概率只依赖于前一个状态</li>\n<li>因为转移概率只依赖于前一个状态，因此可以将状态转移概率写成转移概率矩阵，经过n次转移之后的各状态概率分布即初始状态概率分布向量乘以矩阵的n次幂得到的结果</li>\n<li>矩阵的幂从某一次数之后不变，即每一行收敛成相同的概率分布，同时初始概率转移足够多次之后也收敛成相同的分布</li>\n<li>关于马氏链收敛的定义<ul>\n<li>如果一个非周期马氏链具有转移概率矩阵P（可以有无穷多状态），且它的两个任何状态是联通（任何两个状态可以通过有限步转移到达）的，则$lim_{n \\rightarrow \\infty}P_{ij}^n$存在且与$i$无关，记这个收敛的矩阵为$\\pi(j)$</li>\n<li>$\\pi (j)=\\sum _{i=0}^{\\infty} \\pi (i) P_{ij}$</li>\n<li>$\\pi$是方程$\\pi P = \\pi $的唯一非负解</li>\n<li>$\\pi$称为马氏链的平稳分布</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Markov-Chain-Monte-Carlo\"><a href=\"#Markov-Chain-Monte-Carlo\" class=\"headerlink\" title=\"Markov Chain Monte Carlo\"></a>Markov Chain Monte Carlo</h2><ul>\n<li>回到随机生成，对于给定的概率分布，我们希望能生成它对应的样本，一个想法就是构造一个马氏链，其平稳分布刚好就是这个概率分布：因为当马氏链收敛以后，其平稳分布就是各个状态的概率分布，收敛后无论经过状态转移生成多长的状态序列，这个序列里状态的分布一直保持着平稳分布。若平稳分布是要生成的概率分布，则状态序列就是给定概率分布下的一个随机样本序列。</li>\n<li>因此，问题在于，如何已知平稳分布构造马氏链的状态转移矩阵。主要利用了非周期马氏链的细致平稳条件：若$\\pi(i)P_{ij}=\\pi(j)P_{ij} \\quad for \\quad all \\quad i,j$，则$\\pi(x)$是马氏链的平稳分布。这个定理一个物理解释是：若状态i的概率质量是稳定的，则从状态i转移到状态j变化的概率质量恰好和从状态j转移到状态i变化的概率质量互补。</li>\n<li>设从状态i转移到状态j的概率为$q(i,j)$，若$p(i)q(i,j)=p(j)q(j,i)$，那么此时$p(x)$就是这个马氏链的平稳分布，转移矩阵也不用改了，但一般情况下你的运气没有那么好，在已知$p(x)$的情况下，我们需要对$q$进行改造，为此我们乘上一个接受率$\\alpha$，使得：<script type=\"math/tex; mode=display\">\np(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha (j,i)</script>为什么叫接受率？因为可以理解为这个$\\alpha$是在原来的状态转移之后再乘一个概率，代表是否接受这次转移。</li>\n<li>如何确定接受率？其实显而易见$\\alpha (i,j)=p(j)q(j,i)$，对称构造即可。</li>\n<li>因此在每次转移之后，我们从均匀分布采样一个变量，若变量小于接受率则按原始状态转移矩阵进行转移，否则不转移。</li>\n<li>这样的MCMC采样算法存在一个问题，我们其实没有改动原始状态转移矩阵q，而是根据q计算了接受率来保证收敛到p，但是接受率可能计算出来很小，导致状态长期原地踏步，收敛缓慢。事实上，将式$p(i)q(i,j)\\alpha (i,j)=p(j)q(j,i)\\alpha(j,i)$两边同时乘以一个倍数，细致平稳条件没有被打破，但是接受率获得了提高，因此只要将两边接受率乘以一个倍数并保证两个接受率翻倍之后不大于1即可，一般做法是将较大的接受率乘到1。这时就得到了最常见的一种MCMC方法：Metropolis-Hastings算法。</li>\n</ul>\n<h2 id=\"Gibbs-Sampling\"><a href=\"#Gibbs-Sampling\" class=\"headerlink\" title=\"Gibbs Sampling\"></a>Gibbs Sampling</h2><ul>\n<li>之前说到了MCMC实际上没有对状态转移矩阵改动，因此需要一个接受率补充，即便放缩之后总有一个接受率小于1，降低了收敛效率。吉步斯采样希望找到一个转移矩阵Q使得接受率等于1。</li>\n<li>对二维概率分布$p(x,y)$，易得到<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(x_1,y_1)p(y_2|x_1) & =p(x_1)p(y_1|x_1)p(y_2|x_1) \\\\\n& =p(x_1)p(y_2|x_1)p(y_1|x_1) \\\\\n& =p(x_1,y_2)p(y_1|x_1) \\\\\n\\end{aligned}</script></li>\n<li>从左式到最终式，这种形式非常像细致平稳条件！实际上假如固定$x=x_1$，则$p(y|x_1)$可以作为直线$x=x_1$上任意两个不同y值的点之间的转移概率，且这种转移满足细致平稳条件。固定y我们能得到相同的结论，因此在这个二位平面上任意两点之间我们可以构造转移概率矩阵：<ul>\n<li>若两点在垂直线$x=x_1$上，则$Q=p(y|x_1)$</li>\n<li>若两点在水平线$y=y_1$上，则$Q=p(x|y_1)$</li>\n<li>若两点连线既不垂直也水平，则$Q=0$<br>这样对平面上任意两点应用转移矩阵Q，均满足细致平稳条件，这个二维平面上的马氏链将收敛到$p(x,y)$。</li>\n</ul>\n</li>\n<li>gibbs采样得到新的x维度之后，在计算新的y维度时是依赖了新的x维度，因为是在之前选定坐标轴转换的基础上再进行转移，不然无法跳转到新状态$(x_2,y_2)$，你得到的实际上是$(x_1,y_2)$和$(x_2,y_1)$。</li>\n<li>因此，给定二维概率分布，可以计算出这个平面上沿水平或者垂直方向的满足细致平稳条件的转移概率，从平面上任何一点状态出发，它一次转移只改变横坐标或者纵坐标，也就是水平或者垂直移动，从细致平稳条件的公式可以看到这个平稳是可以传递的，如果从某一个维度的转移满足平稳条件，之后接着另一个维度，那么两次转移所等效的一次转移也是平稳的。</li>\n<li>等到所有维度都转移了一次，就得到了一个新的样本。等到马氏链收敛之后形成的样本序列就是我们所需要的随机生成样本序列。状态的转移可以是坐标轴轮流变换的，即这次水平转换，下次垂直转换，也可以每次随机选择坐标轴。虽然每次随机选择坐标轴会导致中途计算出来的新的维度值不一样，但是平稳条件没有打破，最终能够收敛到一样的给定分布。</li>\n<li>同样，上述算法也可以推广到多维。扩展到多维时，在$x$轴上构建的的转移概率就是$Q=p(x|¬ x)$。值得注意的是，上述得到的采样样本并不是相互独立的，只是符合给定的概率分布。</li>\n</ul>\n<h2 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>首先明确，MCMC方法是产生已知分布的样本，即便gibbs采样使用了完全条件概率，也是产生满足联合分布的样本。</li>\n<li>应用于LDA中，其采样的是每个token分配的主题，这样的样本序列可以得到每个主题下各个词的统计量，以及每篇文档下各个主题的统计量，用这两个统计量来估计主题分布和词分布两个多项式分布的参数，其巧妙的地方在于gibbs采样主题分配的完全条件概率里，避开了主题分布和词分布两个多项式分布（这两个是未知的，要推断的，当然不能从未知分布中采样），利用迪利克雷分布的性质，即使用两个迪利克雷分布的参数比例值来替换两个多项式分布的参数，这样完全条件概率中只包含了人为设定好的迪利克雷参数（超参），以及要迭代的统计量，符合gibbs采样用于推断的要求。</li>\n<li>gibbs采样中在不断更新参数，例如本次迭代更新$p(x_1^{t+1})=p(x_1|x_2^t,x_3^t)$，则下一次迭代为$p(x_2^{t+1})=p(x_2|x_1^{t+1},x_3^t)$，即使用更新之后的$x_1^{t+1}$来计算。在LDA中，这一过程通过更新被采样单词的主题实现。</li>\n<li>下文可以看到gibbs采样公式，可以解释为根据其他词的主题分配情况决定自己的主题分配，迭代更新所有词的主题分配；具体如何决定，包含了两个部分，这两个部分类似于tf和idf提供的信息。</li>\n<li>通过采样出来的词的主题分配，用统计量估计参数，这里就是实现了模型的参数推断。</li>\n</ul>\n<h1 id=\"文本建模\"><a href=\"#文本建模\" class=\"headerlink\" title=\"文本建模\"></a>文本建模</h1><ul>\n<li>接下来讨论如何对文本进行概率建模，基本思想是我们假设一个文档中所有的词是按照一种预先设置的概率分布生成的，我们希望找到这个概率分布。具体而言分为以下两个任务：<ul>\n<li>模型是怎样的？</li>\n<li>各个词的生成概率或者说模型参数是多少？</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Unigram模型\"><a href=\"#Unigram模型\" class=\"headerlink\" title=\"Unigram模型\"></a>Unigram模型</h2><ul>\n<li>模型是怎样的？传统的unigram模型即一元语法模型，认为各个词之间的生成是独立的，文档之间、词之间都是独立可交换的，无所谓顺序，就像所有的词放在一个袋子里，每次按概率分布取出一个词，因此也叫词袋模型(BoW)。词袋模型的参数就是各个词的生成概率，频率派认为可以通过词频统计确定生成概率。</li>\n<li>这里为unigram引入一层贝叶斯框架，为后文LDA两层贝叶斯框架的推导铺垫。贝叶斯学派认为词的生成不止一层：词的概率分布有很多种，即概率分布本身也服从一种概率分布，就像是上帝有许多骰子，他挑选了一个骰子再扔，生成了词。</li>\n<li>也就是说，Unigram模型下一篇文档只是一袋子词，这些词的生成遵循一个分布，设为$\\mathop{p}^{\\rightarrow}$，同时这个词生成分布也遵循一个分布，设为$p(\\mathop{p}^{\\rightarrow})$。用数学公式把上面说的两层分布翻译下，就是一篇文档的生成概率：<script type=\"math/tex; mode=display\">\np(W)=\\int p(W|\\mathop{p}^{\\rightarrow})p(\\mathop{p}^{\\rightarrow})d\\mathop{p}^{\\rightarrow}</script></li>\n<li>按照贝叶斯学派的观点，我们应该先假设一个先验分布，再用训练数据修正它，此处我们需要假设$p(\\mathop{p}^{\\rightarrow})$，也就是分布的分布的先验，训练数据是什么？是我们从语料中提取的词频分布，假设$\\mathop{n}^{\\rightarrow}$是所有词的词频序列，则这个序列满足多项分布：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{n}^{\\rightarrow}) &= Mult(\\mathop{n}^{\\rightarrow}|\\mathop{p}^{\\rightarrow},N) \\\\\n&= C_N ^{\\mathop{n}^{\\rightarrow}} \\prod_{k=1}^V p_k^{n_k} \\\\\n\\end{aligned}</script></li>\n<li>既然训练数据满足多项分布，我们自然而然想利用Dirichlet-Multinomial共轭，因此假设$p(\\mathop{p}^{\\rightarrow})$的先验分布为Dirichlet分布：<script type=\"math/tex; mode=display\">\nDir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})=\\frac{1}{\\int \\prod_{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}} \\prod_{k=1}^V p_k^{\\alpha _k -1}</script>其中$V$是语料词典大小，Dirichlet分布的参数$\\alpha$需要自己设置。之后根据共轭得到数据修正之后$p(\\mathop{p}^{\\rightarrow})$的后验分布：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{p}^{\\rightarrow}|W,\\mathop{\\alpha}^{\\rightarrow}) &= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow})+MultCount(\\mathop{n}^{\\rightarrow}) \\\\\n&= Dir(\\mathop{p}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow}) \\\\\n\\end{aligned}</script></li>\n<li>得到后验之后，可以使用极大似然估计或者均值估计来计算$\\mathop{p}^{\\rightarrow}$，这里我们使用后验里Dirichlet分布的均值来估计，结合之前提到了Dirichlet的性质，有：<script type=\"math/tex; mode=display\">\n\\mathop{p_i}^{~}=\\frac{n_i+\\alpha _i}{\\sum _{i=1}^{V} (n_i+\\alpha _i)}</script>这个式子的物理解释：不同于一般的使用词频作为估计，我们首先假设了词频（即先验的伪计数$\\alpha _i$），然后加上数据给出的词频$n_i$，再归一化作为概率。</li>\n<li>现在得到了词语的生成概率分布$\\mathop{p}^{\\rightarrow}$，那么在此分布下的文档的生成概率显然为：<script type=\"math/tex; mode=display\">\np(W|\\mathop{p}^{\\rightarrow})=\\prod _{k=1}^V p_k^{n_k}</script>将词生成概率分布的分布，文档在词生成分布下的条件生成概率带入之前提到的文档概率积分式，就得到所有分布情况下，文档的生成概率。代入化简之后可以得到一个很漂亮的式子。<script type=\"math/tex; mode=display\">\np(W|\\mathop{\\alpha}^{\\rightarrow})=\\frac{\\Delta(\\mathop{\\alpha}^{\\rightarrow}+\\mathop{n}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})}</script>其中$\\Delta$是归一化因子：<script type=\"math/tex; mode=display\">\n\\Delta(\\mathop{\\alpha}^{\\rightarrow})=\\int \\prod _{k=1}^V p_k^{\\alpha _k -1}d\\mathop{p}^{\\rightarrow}</script></li>\n</ul>\n<h2 id=\"PLSA模型\"><a href=\"#PLSA模型\" class=\"headerlink\" title=\"PLSA模型\"></a>PLSA模型</h2><ul>\n<li>PLSA即概率隐含语义分析模型，这个模型认为文档到词之间存在一个隐含的主题层次，文档包含多个主题，每个主题对应一种词的分布，生成词时，先选主题，再从主题中选词生成（实际计算时是各个主题的概率叠加）。</li>\n<li>事实上这种模型就是将Unigram中词生成分布对应成主题，在往上加了一层文档到主题的分布。</li>\n<li>PLSA模型可以用EM算法迭代学习到参数。</li>\n</ul>\n<h2 id=\"总结-2\"><a href=\"#总结-2\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>现在整理一下，Unigram模型中主要包含两部分<ul>\n<li>词生成概率分布</li>\n<li>词生成概率分布的分布</li>\n</ul>\n</li>\n<li>PLSA模型主要包含两部分<ul>\n<li>词生成概率分布</li>\n<li>主题生成概率分布</li>\n</ul>\n</li>\n<li>Unigram模型展示了分布的分布，即为词分布引入参数先验的意义：使得词的分布是一个变量，从掷色子选出一个词变成了先选一个色子，再掷色子选词。至于引入先验究竟有没有用是贝叶斯学派和频率学派之间争吵的话题了</li>\n<li>PLSA模型为人类语言生成提供了一个很直观的建模，引入了话题作为隐含语义，提出了主题代表词的分布，一篇文章包含多个主题的观点。</li>\n</ul>\n<h1 id=\"LDA文本建模\"><a href=\"#LDA文本建模\" class=\"headerlink\" title=\"LDA文本建模\"></a>LDA文本建模</h1><h2 id=\"模型概述\"><a href=\"#模型概述\" class=\"headerlink\" title=\"模型概述\"></a>模型概述</h2><ul>\n<li>LDA整合了两种观Unigram和PLSA的优点，对于词和主题这两个骰子分别加上了Dirichlet先验假设<ul>\n<li>词生成概率分布（暂记A）</li>\n<li>词生成概率分布的分布（暂记B）</li>\n<li>主题生成概率分布（暂记C）</li>\n<li>主题生成概率分布的分布（暂记D）</li>\n</ul>\n</li>\n<li>这里有一个坑需要避免，即主题生成概率分布并不是词生成概率分布的分布！也就是区分LDA模型中的层次关系和各个层次里共轭关系。另外主题和词并不是一对多的层次关系，两者是多对多的关系，事实上，在LDA模型中一篇文档是这么生成的（假设有K个主题)：<ul>\n<li>先在B分布条件下抽样得到K个A分布</li>\n<li>对每一篇文档，在符合D分布条件下抽取一个C分布，重复如下过程生成词：<ul>\n<li>从C分布中抽样得到一个主题z</li>\n<li>选择K个A分布中第z个，从这个A分布中抽样得到一个单词</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>假设有$m$篇文档，$n$个词，$k$个主题，则$D+C$是$m$个独立的Dirichlet-Multinomial共轭，$B+A$是$k$个独立的Dirichlet-Multinomial共轭。两个dirichlet参数分别为1个k维向量($\\alpha$)和1个n维向量($\\beta$)。现在我们可以理解本文最开始的配图，我们将符号用其实际意义表述，与标题配图对应，这幅图实际上描述了LDA中这$m+k$个独立的Dirichlet-Multinomial共轭：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oGYq.png\" alt=\"i0oGYq.png\"><br><img src=\"https://s1.ax1x.com/2018/10/20/i0oJf0.jpg\" alt=\"i0oJf0.jpg\"></p>\n<h2 id=\"建立分布\"><a href=\"#建立分布\" class=\"headerlink\" title=\"建立分布\"></a>建立分布</h2><ul>\n<li>现在我们可以用$m+k$个Dirichlet-Multinomial共轭对LDA主题模型建模了，借鉴之前推导Unigram模型时得到最终的文档生成分布，我们可以分别计算：<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\\\\np(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})} \\\\\n\\end{aligned}</script></li>\n<li>最终得到词与主题的联合分布:<script type=\"math/tex; mode=display\">\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}</script></li>\n</ul>\n<h2 id=\"采样\"><a href=\"#采样\" class=\"headerlink\" title=\"采样\"></a>采样</h2><ul>\n<li>首先我们需要推导出采样公式，即$p(z_i=k|\\mathop{w}^{\\rightarrow})$，$z_i$代表第i个词的主题（这里下标i代表第m篇文档第n个词），而向量w代表我们现在观察到的所有词。</li>\n<li>在建立了整个概率模型之后，我们通过以下方法训练：设定好超参，随机初始化各个词频统计（包括文章m下主题k的词数，词汇t属于主题k的词数，文章m的总词数，主题k的总词数），然后对语料中所有词，依次进行吉布斯采样，采样其主题，并分配给该词这个主题，并更新四个词频（即利用共轭更新后验），循环采样直到收敛，即采样之后的主题分布基本符合后验概率下的模型产生的主题分布，数据已经不能提供给模型更多的知识（不再进行更新）。<ul>\n<li>其中吉布斯采样是需要限定某一维，按照其他维度的条件概率进行采样，在文本主题建模中维度就是词语，按其他维度的条件概率计算就是在四个词频中除去当前词语及其主题的计数。</li>\n<li>采样后主题后将这个主题分配给词语，四个词频计数增加，如果已经收敛，则采样前后主题相同，词频没有改变，则相当于后验没有从数据知识中获得更新。</li>\n</ul>\n</li>\n<li>公式推导：下面分别介绍两种推导方法，第一种是LDA数学八卦一文中基于共轭关系做出的推导，另一种是Parameter estimation for text analysis 一文中基于联合分布做出的推导</li>\n<li>基于共轭关系推导如下：</li>\n<li>采样的对象是词所对应的主题，概率为：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{w}^{\\rightarrow})</script></li>\n<li>使用吉布斯采样来采样某一个词的主题，则需要用其他词的主题作为条件计算条件概率：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})</script></li>\n<li>由贝叶斯公式，这个条件概率正比于（采样我们可以按正比扩大各个概率采样）：<script type=\"math/tex; mode=display\">\np(z_i=k,w_i=t|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})</script></li>\n<li>把这个公式按主题分布和词分布展开：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k,w_i=t,\\mathop{\\vartheta _m}^{\\rightarrow},\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>由于所有的共轭都是独立的，上式可以写成：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k,\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})p(w_i=t,\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>把概率链式分解下，又因为两个式子分别和主题分布和词分布相关，因此可以写成两个积分相乘：<script type=\"math/tex; mode=display\">\n\\int p(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})p(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\vartheta _m}^{\\rightarrow} \\cdot \\int p(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})p(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})d\\mathop{\\varphi _k}^{\\rightarrow}</script></li>\n<li>已知主题分布和词分布，求第i个词为t的概率和第i个词对应主题为k的概率，那么显然：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{\\vartheta _m}^{\\rightarrow})=\\mathop{\\vartheta _{mk}} \\\\\np(w_i=t|\\mathop{\\varphi _k}^{\\rightarrow})=\\mathop{\\varphi _{kt}} \\\\</script></li>\n<li>而根据共轭关系，有<script type=\"math/tex; mode=display\">\np(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\vartheta _m}^{\\rightarrow}|\\mathop{n_{m,¬ i}}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow}) \\\\\np(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w_{¬ i}}^{\\rightarrow})=Dir(\\mathop{\\varphi _k}^{\\rightarrow}|\\mathop{n_{k,¬ i}}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow}) \\\\</script></li>\n<li>因此整个式子可以看作$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$分别在两个Dirichlet分布上的期望相乘。而根据之前Dirichlet的性质，易得这两个期望是按Dirichlet参数比例得到的分式，就在这里绕过了多项式分布！因此最后的概率计算出来就是（注意是正比于）：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝\\frac{n_{m,¬ i}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m,¬ i}^{(t)}+\\alpha _k)} \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>注意到第一项的分母是对k求和，实际上和k无关，因此可以写成：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>这个概率可以理解为：<script type=\"math/tex; mode=display\">\n\\frac{文档d_i中词汇w_i已经被分配到主题j次数+\\alpha}{文档d_i中已经被分配到主题j的词汇\n总数+T\\alpha} \\frac{词汇w_i分配到主题j的次数+\\beta}{所有分配到主题j的词汇总数+W\\beta}</script></li>\n<li>其中$w_i$代表第i个token对应的词，$d_i$代表第i个token所在的文档。</li>\n<li>我们再看看基于联合分布如何推导</li>\n<li>之前我们已经得到词和主题的联合分布：<script type=\"math/tex; mode=display\">\np(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow}|\\mathop{\\alpha}^{\\rightarrow},\\mathop{\\beta}^{\\rightarrow}) = \\prod _{m=1}^M \\frac{\\Delta(\\mathop{n_m}^{\\rightarrow}+\\mathop{\\alpha}^{\\rightarrow})}{\\Delta(\\mathop{\\alpha}^{\\rightarrow})} \\prod _{k=1}^K \\frac{\\Delta(\\mathop{n_k}^{\\rightarrow}+\\mathop{\\beta}^{\\rightarrow})}{\\Delta(\\mathop{\\beta}^{\\rightarrow})}</script></li>\n<li>根据贝叶斯公式有<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})=\\frac{p(\\mathop{w}^{\\rightarrow},\\mathop{z}^{\\rightarrow})}{p(\\mathop{w}^{\\rightarrow},\\mathop{z_{¬ i}}^{\\rightarrow})} \\\\\n=\\frac{p(\\mathop{w}^{\\rightarrow}|\\mathop{z}^{\\rightarrow})} {p(\\mathop{w_{¬ i}}^{\\rightarrow}|\\mathop{z_{¬ i}}^{\\rightarrow})p(w_i)} \\cdot \\frac{p(\\mathop{z}^{\\rightarrow})} {\\mathop{z_{¬ i}}^{\\rightarrow}} \\\\</script></li>\n<li>因为$p(w_i)$是可观测变量，我们省略它，得到一个正比于的式子，将这个式子用之前的$\\Delta$形式表示（分式除以分式，分母相同抵消了）：<script type=\"math/tex; mode=display\">\n∝ \\frac{\\Delta(\\mathop{n_{z}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}}{\\Delta(\\mathop{n_{z,¬ i}}^{\\rightarrow})+\\mathop{\\beta}^{\\rightarrow}} \\cdot \\frac{\\Delta(\\mathop{n_{m}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}{\\Delta(\\mathop{n_{m,¬ i}}^{\\rightarrow})+\\mathop{\\alpha}^{\\rightarrow}}</script></li>\n<li>将$\\Delta$的表达式带入计算，也可以得到：<script type=\"math/tex; mode=display\">\np(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})∝ (n_{m,¬ i}^{(k)}+\\alpha _k) \\cdot \\frac{n_{k,¬ i}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k,¬ i}^{(t)}+\\beta _t)}</script></li>\n<li>最后附上Parameter estimation for text analysis一文中吉布斯采样的伪算法图：</li>\n</ul>\n<p><img src=\"https://s1.ax1x.com/2018/10/20/i0oU6U.png\" alt=\"i0oU6U.png\"></p>\n<ul>\n<li>可以看到主要通过记录四个n值（两个矩阵两个向量）来计算条件概率，更新主题时也是更新四个n值进行增量更新。算法先通过随机均匀采样赋初值，然后按采样公式更新主题（先减去旧的主题分配，再加上新的主题分配），其中公式78即之前我们计算得到的$p(z_i=k|\\mathop{z_{¬ i}}^{\\rightarrow},\\mathop{w}^{\\rightarrow})$，公式81和82分别为$\\mathop{\\vartheta _{mk}},\\mathop{\\varphi _{kt}}$，我们可以直接通过四个n值得到，不用考虑采样时的$¬ i$条件了，具体是：<script type=\"math/tex; mode=display\">\n\\mathop{\\vartheta _{mk}} = \\frac{n_{m}^{(k)}+\\alpha _k}{\\sum _{k=1}^K (n_{m}^{(t)}+\\alpha _k)} \\\\\n\\mathop{\\varphi _{kt}} = \\frac{n_{k}^{(t)}+\\beta _t}{\\sum _{t=1}^V (n_{k}^{(t)}+\\beta _t)}</script><h2 id=\"训练与推断\"><a href=\"#训练与推断\" class=\"headerlink\" title=\"训练与推断\"></a>训练与推断</h2></li>\n<li>接下来我们使用共轭关系和Gibbs采样两大工具来训练LDA模型，首先对LDA的Dir参数随机初始化（先验），然后使用文本进行数据知识补充，得到最终正确的后验，训练是一个迭代过程：<ul>\n<li>迭代什么？采样并更新词对应的主题</li>\n<li>根据什么迭代？gibbs采样的完全条件概率</li>\n<li>迭代之后的效果？主题分配改变、统计量改变、下一次gibbs采样的完全条件概率改变</li>\n<li>迭代到什么时候为止？Gibbs采样收敛，即采样一段时间区间内主题分布稳定不变，或者根据困惑度等指标来衡量模型收敛的程度。</li>\n</ul>\n</li>\n<li>训练和推断的区别在于，训练是针对全文档集进行采样更新，文档到主题和主题到词的分布都在更新，而推断则保留主题到词的分布不变，只针对当前推断的文档采样到收敛，得到该文档的主题分布。</li>\n<li>事实上两个超参$\\alpha$和$\\beta$在训练完之后是经过了很多次后验替换先验的迭代，参数值很大了，$\\alpha$就抛弃了这最后的后验结果，在对新文档产生主题分布时重新用最开始的先验值，这样的话中途得到的训练集上的文档到主题的分布在推断新文档时实际上是用不上的，我们利用的是主题到词的分布：因为只有主题集合是针对整个文档空间的（训练集和测试集），主题到词的分布也是建立在整个文档空间的词典上的，这一部分的k个$\\beta$向量我们保留最后的后验结果，因为这个后验吸收了数据的似然知识后参数值很大，不确定度很小了，基本上每个$\\beta$向量就等同于确定了一个主题到词的多项式分布，也就是确定了一个主题。我们利用这确定的主题，来推断一篇新文档在各个主题上的分布。因此在推断新文档时参数$\\alpha$一般设置为对称的，即各个分量相同（没有先验偏好那个主题），且值很小（即不确定度大，否则生成的主题分布是均匀分布），这里类似于最大熵的思想。推断是利用已知的固定的主题去得到文档到主题的分布。</li>\n<li>LDA的训练实际上是一种无参贝叶斯推断，可以采用MCMC方法和非MCMC方法，MCMC方法中经常采用的就是Gibbs采样，而非MCMC方法还可以用变分推断等方法来迭代得到参数。</li>\n</ul>\n<h1 id=\"LDA-in-Gensim\"><a href=\"#LDA-in-Gensim\" class=\"headerlink\" title=\"LDA in Gensim\"></a>LDA in Gensim</h1><ul>\n<li>Gensim中的LDA提供了几个参数，其中$\\alpha$的默认值如下：<blockquote><p>alpha ({numpy.ndarray, str}, optional) –<br>Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:<br>’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.<br>’default’: Learns an asymmetric prior from the corpus.</p>\n<footer><strong>Gensim</strong><cite><a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\" target=\"_blank\" rel=\"noopener\">models.ldamodel – Latent Dirichlet Allocation</a></cite></footer></blockquote></li>\n<li>gensim中没有暴露$\\beta$给用户，用户只能设置$\\alpha$，可以自定义，也可以设置对称或者不对称。其中对称设置即全为1，不对称设置则拟合了zipf law（？），可能$\\beta$的默认设置就是不对称。</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0oNlT.jpg","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"LDA学习笔记","path":"2018/07/23/lda/","eyeCatchImage":"https://s1.ax1x.com/2018/10/20/i0oNlT.jpg","excerpt":"<hr>\n<p>Latent Dirichlet Allocation 文档主题生成模型学习笔记<br>主要摘录自《LDA数学八卦》，原文写的非常精彩，有许多抛砖引玉之处，本文梳理了其一步一步推出LDA的脉络，删除了不相关的一些扩展，并给出了一些总结。没有读过原文的同学建议先读原文过一遍。<br>依然没有彻底弄懂，每次都发现点新东西……</p>","date":"2018-07-23T01:56:41.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","mcmc","lda"],"internalLinks":[],"keywords":[],"keywordsLength":0}},{"title":"计算语言学笔记","date":"2018-11-16T02:15:34.000Z","author":"Thinkwee","mathjax":true,"html":true,"_content":"\n计算语言学课程笔记\n参考教材：Speech and Language Processing：An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition\n一些公式待修订\n<!--more--> \n\n# 第二章：正则表达式与自动机\n-\t正则表达式：一种用于查找符合特定模式的子串或者用于以标准形式定义语言的工具，本章主要讨论其用于查找子串的功能。正则表达式用代数的形式来表示一些字符串集合。\n-\t正则表达式接收一个模式，然后在整个语料中查找符合这个模式的子串，这个功能可以通过设计有限状态自动机实现。\n-\t字符串看成符号的序列，所有的字符，数字，空格，制表符，标点和空格均看成符号。\n\n## 基本正则表达式模式\n-\t用双斜线表示正则表达式开始和结束（perl中的形式）\n\t-\t查找子串，大小写敏感：/woodchuck/-> woodchuck\n\t-\t用方括号代表取其中一个，或：/[Ww]oodchuck/->woodchuck or Woodchuck\n\t-\t方括号加减号，范围内取或：/[2-5]/->/[2345]\n\t-\t插入符号放在左方括号后，代表模式中不出现后接的所有符号，取非: /^Ss/ ->既不是大写S也不是小写s\n\t-\t问号代表之前的符号出现一个或不出现：/colou?r/->color or colour\n\t-\t星号代表之前的符号出现多个或不出现：/ba*/->b or ba or baa or baaa......\n\t-\t加号代表之前的符号出现至少一次：/ba+/->ba or baa or baaa.......\n\t-\t小数点代表通配符，与任何除了回车符之外的符号匹配：/beg.n/->begin or begun or beg’n or .......\n\t-\t锚符号，用来表示特定位置的子串，插入符号代表行首，美元符号代表行尾，\\b代表单词分界线，\\B代表单词非分界线，perl将单词的定义为数字、下划线、字母的序列，不在其中的符号便可以作为单词的分界。\n\n## 析取、组合和优先\n-\t用竖线代表析取，字符串之间的或：/cat|dog/->cat or dog\n-\t用圆括号代表部分析取（组合），圆括号内也可以用基本算符：/gupp(y|ies)/->guppy or guppies\n-\t优先级：圆括号>计数符>序列与锚>析取符\n\n## 高级算符\n-\t\\d：任何数字\n-\t\\D：任何非数字字符\n-\t\\w：任何字母、数字、空格\n-\t\\W：与\\w相反\n-\t\\s：空白区域\n-\t\\S：与\\s相反\n-\t{n}：前面的模式出现n个\n-\t{n,m}：前面的模式出现n到m个\n-\t{n,}：前面的模式至少出现n个\n-\t\\n：换行\n-\t\\t：表格符\n\n## 替换、寄存器\n-\t替换s/A/B/：A替换成B\n-\ts/(A)/<\\1>/：用数字算符\\1指代A，在A的两边加上尖括号\n-\t在查找中也可以用数字算符，指代圆括号内内容，可以多个算符指代多个圆括号内内容\n-\t这里数字算符起到了寄存器的作用\n\n## 有限状态自动机\n-\t有限状态自动机和正则表达式彼此对称，正则表达式是刻画正则语言的一种方法。正则表达式、正则语法和自动状态机都是表达正则语言的形式。FSA用有向图表示，圆圈或点代表状态，箭头或者弧代表状态转移，用双圈表示最终状态，如下图表示识别/baa+!/的状态机图： \n![FoVj3V.png](https://s2.ax1x.com/2019/01/03/FoVj3V.png)\n-\t状态机从初始状态出发，依次读入符号，若满足条件，则进行状态转移，若读入的符号序列满足模式，则状态机可以到达最终状态；若符号序列不满足模式，或者自动机在某个非最终状态卡住，则称自动机拒绝了此次输入。\n-\t另一种表示方式是状态转移表：\n![FoVqNn.png](https://s2.ax1x.com/2019/01/03/FoVqNn.png)\n-\t一个有限自动机可以用5个参数定义：\n\t-\t$Q$：状态{q_i}的有限集合\n\t-\t\\sum ：有限的输入符号字母表\n\t-\t$q_0$：初始状态\n\t-\t$F$：终极状态集合\n\t-\t$\\delta (q,i)$：状态之间的转移函数或者转移矩阵，是从$Q × \\Sigma$到$2^Q$的一个关系\n-\t以上描述的自动机是确定性的，即DFSA，在已知的记录在状态转移表上的状态时，根据查表自动机总能知道如何进行状态转移。算法如下，给定输入和自动机模型，算法确定输入是否被状态机接受：\n![FoZpB4.png](https://s2.ax1x.com/2019/01/03/FoZpB4.png)\n-\t当出现了表中没有的状态时自动机就会出错，可以添加一个失败状态处理这些情况。\n\n## 形式语言\n-\t形式语言是一个模型，能且只能生成和识别一些满足形式语言定义的某一语言的符号串。形式语言是一种特殊的正则语言。通常使用形式语言来模拟自然语言的某些部分。以上例/baa+!/为例，设对应的自动机模型为m，输入符号表$\\Sigma = {a,b,!}$，$L(m)$代表由m刻画的形式语言，是一个无限集合${baa!,baaa!,baaaa!,…}$\n\n## 非确定有限自动机\n-\t非确定的有限自动机NFSA,把之前的例子稍微改动，自返圈移动到状态2，就形成了NFSA，因为此时在状态2，输入a，有两种转移可选，自动机无法确定转移路径：\n![FoVLhq.png](https://s2.ax1x.com/2019/01/03/FoVLhq.png)\n-\t另一种NFSA的形式是引入$\\epsilon$转移，即不需要输入符号也可以通过此$\\epsilon$转移进行转移，如下图，在状态3时依然不确定如何进行转移：\n![FoVX90.png](https://s2.ax1x.com/2019/01/03/FoVX90.png)\n-\t在NFSA时，面临转移选择时自动机可能做出错误的选择，此时存在三种解决方法：\n\t-\t回退：标记此时状态，当确定发生错误选择之后，回退到此状态\n\t-\t前瞻：在输入中向前看，帮助判定进行选择\n\t-\t并行：并行的进行所有可能的转移\n-\t在自动机中，采用回退算法时需要标记的状态称为搜索状态，包括两部分：状态节点和输入位置。对于NFSA，其状态转移表也有相应改变，如图，添加了代表$\\epsilon$转移的$\\epsilon$列，且转移可以转移到多个状态：\n![FoZE36.png](https://s2.ax1x.com/2019/01/03/FoZE36.png)\n-\t采用回退策略的非确定自动机算法如下，是一种搜索算法： \n![FoZSuF.png](https://s2.ax1x.com/2019/01/03/FoZSuF.png)\n-\t子函数GENERATE-NEW-STATES接受一个搜索状态，提取出状态节点和输入位置，查找这个状态节点上的所有状态转移可能，生成一个搜索状态列表作为返回值；\n-\t子函数ACCEPT-STATE接受一个搜索状态，判断是否接受，接受时的搜索状态应该是最终状态和输入结束位置的二元组。\n-\t算法使用进程表（agenda）记录所有的搜索状态，初始只包括初始的搜索状态，即自动机初始状态节点和输入起始。之后不断循环，从进程表中调出搜索状态，先调用ACCEPT-STATE判断是否搜索成功，之后再调用GENERATE-NEW-STATES生成新的搜索状态加入进程表。循环直到搜索成功或者进程表为空（所有可能转移均尝试且未成功）返回拒绝。\n-\t可以注意到NFSA算法就是一种状态空间搜索，可以通过改变搜索状态的顺序提升搜索效率，例如用栈实现进程表，进行深度优先搜索DFS；或者使用队列实现进程表，进行宽度优先搜索BFS。\n-\t对于任何NFSA，存在一个完全等价的DFSA。\n\n## 正则语言和NFSA\n-\t定义字母表\\sum 为所有输入符号集合；空符号串$\\epsilon$，空符号串不包含再字母表中；空集∅。在\\sum 上的正则语言的类（或者正则集）可以形式的定义如下：\n\t-\t∅是正则语言\n\t-\t∀a ∈ $\\sum$ ∪$\\epsilon$,{a}是形式语言\n\t-\t如果$L_1$和$L_2$是正则语言，那么：\n\t-\t$L_1$和$L_2$的拼接是正则语言\n\t-\t$L_1$和$L_2$的合取、析取也是正则语言\n\t-\t$L_1$^*，即$L_1$的Kleene闭包也是正则语言\n-\t可见正则语言的三种基本算符：拼接、合取及析取、Kleene闭包。任何正则表达式可以写成只使用这三种基本算符的形式。\n-\t正则语言对以下运算也封闭（$L_1$和$L_2$均为正则语言）：\n\t-\t交：$L_1$和$L_2$的符号串集合的交构成的语言也是正则语言\n\t-\t差：$L_1$和$L_2$的符号串集合的差构成的语言也是正则语言\n\t-\t补：不在$L_1$的符号串集合中的集合构成的语言也是正则语言\n\t-\t逆：$L_1$所有符号串的逆构成的集合构成的语言也是正则语言\n-\t可以证明正则表达式和自动机等价，一个证明任何正则表达式可以建立对应的自动机的方法是，根据正则语言的定义，构造基础自动机代表$\\epsilon$、∅以及$\\sum$中的单个符号a，然后将三种基本算符表示为自动机上的操作，归纳性的，在基础自动机上应用这些操作，得到新的基础自动机，这样就可以构造满足任何正则表达式的自动机，如下图：\n![FoVxjU.png](https://s2.ax1x.com/2019/01/03/FoVxjU.png)\n基础自动机\n![FoZPE9.png](https://s2.ax1x.com/2019/01/03/FoZPE9.png) \n拼接算符\n![FoZ9HJ.png](https://s2.ax1x.com/2019/01/03/FoZ9HJ.png)\nKleene闭包算符\n![FoZiNR.png](https://s2.ax1x.com/2019/01/03/FoZiNR.png)\n合取析取算符\n\n# 第三章：形态学与有限状态转录机\n-\t剖析：取一个输入并产生关于这个输入的各类结构\n\n## 英语形态学概论\n-\t形态学研究词的构成，词可以进一步拆解为语素，语素可分为词干和词缀，词缀可分为前缀、中缀、后缀、位缀。\n-\t屈折形态学：英语中，名词只包括两种屈折变化：一个词缀表示复数，一个词缀表示领属：\n\t-\t复数：-s，-es，不规则复数形式\n\t-\t领属：-‘s，-s’\n-\t动词的屈折变化包括规则动词和非规则动词的变化：\n\t-\t规则动词：主要动词和基础动词，-s，-ing，-ed，\n\t-\t非规则动词\n-\t派生形态学：派生将词干和一个语法语素结合起来，形成新的单词\n\t-\t名词化：-ation，-ee，-er，-ness\n\t-\t派生出形容词：-al，-able，-less\n\n## 形态剖析\n-\t例子：我们希望建立一个形态剖析器，输入单词，输出其词干和有关的形态特征，如下表，我们的目标是产生第二列和第四列：\n![FoZA9x.png](https://s2.ax1x.com/2019/01/03/FoZA9x.png)\n-\t我们至少需要：\n\t-\t词表（lexicon）：词干和词缀表及其基本信息\n\t-\t形态顺序规则（morphotactics）：什么样的语素跟在什么样的语素之后\n\t-\t正词法规则（orthographic rule）：语素结合时拼写规则的变化\n-\t一般不直接构造词表，而是根据形态顺序规则，设计FSA对词干进行屈折变化生成词语。例如一个名词复数化的简单自动机如下图：\n![FoZmuD.png](https://s2.ax1x.com/2019/01/03/FoZmuD.png)\n-\t其中reg-noun代表规则名词，可以通过加s形成复数形式，并且忽略了非规则单数名词(irreg-sg-noun)和非规则复数名词(irreg-pl-noun)。另外一个模拟动词屈折变化的自动机如下图：\n![FoZQUA.png](https://s2.ax1x.com/2019/01/03/FoZQUA.png)\n-\t使用FSA解决形态识别问题（判断输入符号串是否合法）的一种方法是，将状态转移细分到字母层次，但是这样仍然会存在一些问题：\n![FoZZjO.png](https://s2.ax1x.com/2019/01/03/FoZZjO.png)\n\n## 有限状态转录机\n-\t双层形态学：将一个词表示为词汇层和表层，词汇层表示该词语素之间的简单毗连（拼接，concatenation），表层表示单词实际最终的拼写，有限状态转录机是一种有限状态自动机，但其实现的是转录，实现词汇层和表层之间的对应，它有两个输入，产生和识别字符串对，每一个状态转移的弧上有两个标签，代表两个输入。\n![FoZVgK.png](https://s2.ax1x.com/2019/01/03/FoZVgK.png)\n-\t从四个途径看待FST：\n\t-\t作为识别器：FST接受一对字符串，作为输入，如果这对字符串在语言的字符串对中则输出接受否则拒绝\n\t-\t作为生成器：生成语言的字符串对\n\t-\t作为翻译器：读入一个字符串，输出另一个\n\t-\t作为关联器：计算两个集合之间的关系\n-\t定义有限状态转录机：\n\t-\tQ：状态{q_i}的有限集合\n\t-\t\\sum ：有限的输入符号字母表\n\t-\t∆：有限的输出符号字母表\n\t-\t$q_0 \\in Q$：初始状态\n\t-\t$F⊆Q$：终极状态集合\n\t-\t$\\delta (q,w)$：状态之间的转移函数或者转移矩阵，是从Q×\\sum 到2^Q的一个关系，q是状态，w是字符串，返回新状态集合\n\t-\t$\\sigma (q,w)$：输出函数，给定每一个状态和输入，返回可能输出字符串的集合，是从$Q × \\Sigma$到$2^∆$的一个关系\n-\t在FST中，字母表的元素不是单个符号，而是符号对，称为可行偶对。类比于FSA和正则语言，FST和正则关系同构，对于并运算封闭，一般对于差、补、交运算不封闭。\n-\t此外，FST，\n\t-\t关于逆反（逆的逆）闭包，逆反用于方便的实现作为剖析器的FST到作为生成器的FST的转换\n\t-\t关于组合（嵌套）闭包，用于将多个转录机用一个更复杂的转录机替换。\n-\t转录机一般是非确定性的，如果用FSA的搜索算法会很慢，如果用非确定性到确定性的转换算法，则有些FST本身是不可以被转换为为确定的。\n-\t顺序转录机是一种输入确定的转录机，每个状态转移在给定状态和输入之后是确定的，不像上图中的FST，状态0在输入b时有两种状态转移（转移到相同的状态，但是输出不同）。顺序转录机可以使用$\\epsilon$符号，但是只能加在输出字符串上，不能加在输入字符串上，如下图： \n![FoZuHH.png](https://s2.ax1x.com/2019/01/03/FoZuHH.png)\n-\t顺序转录机输出不一定是序列的，即从同一状态发出的不同转移可能产生相同输出，因此顺序转录机的逆不一定是顺序转录机，所以在定义顺序转录机时需要定义方向，且转移函数和输出函数需要稍微修改，输出空间缩小为Q和∆。\n-\t顺序转录机的一种泛化形式是并发转录机，其在最终状态额外输出一个字符串，拼接到已经输出的字符串之后。顺序和并发转录机的效率高，且有有效的算法对其进行确定化和最小化，因此很重要。P并发转录机在此基础上可以解决歧义问题。\n\n## 用有限状态转录机进行形态剖析\n-\t将单词看成词汇层和表层之间的关系，如下图： \n![FoZnDe.png](https://s2.ax1x.com/2019/01/03/FoZnDe.png)\n-\t在之前双层形态学的基础定义上，定义自己到自己的映射为基本对，用一个字母表示；用^代表语素边界；用#代表单词边界，在任务中提到需要输出+SG之类的语素特征，这些特征在另一个输出上没有对应的输出符号，因此映射到空字符串或边界符号。我们把输入输出对用冒号连接，也可以写在弧的上下。一个抽象的表示英语名词复数屈折变化的转录机如下图：\n![FoZl4I.png](https://s2.ax1x.com/2019/01/03/FoZl4I.png)\n-\t之后我们需要更新词表，使得非规则复数名词能够被剖析为正确的词干：\n![FoZMEd.png](https://s2.ax1x.com/2019/01/03/FoZMEd.png)\n-\t之后将抽象的转录机写成具体的，由字母组成转移弧的转录机，如下图，只展示了具体化部分非规则复数和单数名词之后的转录机：\n![FoZ3Ct.png](https://s2.ax1x.com/2019/01/03/FoZ3Ct.png)\n\n## 转录机和正词法规则\n-\t用拼写规则，也就是正词法规则来处理英语中经常在语素边界发生拼写错误的问题。\n-\t以下是一些拼写规则实例：\n\t-\t辅音重叠：beg/beggin\n\t-\tE的删除：make/making\n\t-\tE的插入：watch/watches\n\t-\tY的替换：try/tries\n\t-\tK的插入：panic/panicked\n-\t为了实现拼写规则，我们在词汇层和表层之间加入中间层，以符合特定规则的语素毗连作为输入，以修改之后的正确的语素毗连作为输出，例如fox +N +PL输入到中间层即第一次转录，得到fox ^ s #，之后中间层到表层的第二次转录检测到特殊语素毗连：x^和s#，就在表层的x和s之间插入一个e，得到foxes。下面的转录机示意图展示了这个过程：\n![FoZ88P.png](https://s2.ax1x.com/2019/01/03/FoZ88P.png)\n-\t这个转录机只考虑x^和s#毗连需插入e这一正词法规则\n-\t其他的词能正常通过\n-\t$Q_0$代表无关词通过，是接受状态\n-\t$Q_1$代表看见了zsx，作为中间状态保存，一直保存的是最后的与语素毗连的z,s,x，如果出现了其他字母则返回到q0，其本身也可以作为接受态\n-\t$Q_2$代表看见了与z,s,x毗连的语素，这之后有四种转移\n\t-\t接了$x$,$z$，回到$q_1$，也就是认为重新接到了可能和语素毗连的x,z\n\t-\t接了$s$，分为两种情况，一种是正常需要插入e，这时通过$\\epsilon$转移到$q_3$再到$q_4$；另一种是本来就需要插入$e$，这就到达$q_5$，之后视情况回退了$q_1$、$q_0$，或者$s$又毗连语素回到$q_2$。两种情况不确定，需要通过搜索解决\n\t-\t接单词边界和其他符号，回到$q_0$\n\t-\t$q_2$本身也可以作为接受态\n\n## 结合\n-\t现在可以通过三层结构，结合产生语素和进行正词法规则矫正的转录机。从词汇层到中间层用一个转录机产生语素，从中间层到表层可并行使用多个转录机进行正词法规则的矫正。\n-\t两类转录机叠加时可以改写成一类转录机，这时需要对两类状态机状态集合计算笛卡尔积，对新集合内每一个元素建立状态。\n-\t这种三层结构是可逆的，但是进行剖析时（从表层到词汇层）会出现歧义问题，即一个单词可能剖析出多种语素结合，这时单纯依靠转录机无法消歧，需要借助上下文。\n\n## 其他应用（简单介绍）\n-\t不需要词表的FST，PORTER词干处理器：将层叠式重写规则用FST实现，提取出单词的词干。\n-\t分词和分句：一个简单的英文分词可以基于正则表达式实现，一个简单的中文分词可以通过maxmatch（一种基于最大长度匹配的贪婪搜索算法）实现。\n-\t拼写检查与矫正：使用了投影操作的FST可以完成非词错误的检测，然后基于最小编辑距离（使用动态规划算法实现）可以矫正。正常词错误检测和矫正需借助N元语法模型。\n\n## 人如何进行形态处理\n-\t研究表明，人的心理词表存储了一部分形态机构，其他的结构不组合在心理词表中，而需要分别提取并组合。研究说明了两个问题：\n\t-\t形态尤其是屈折变化之类的能产性形态在人的心理词表中起作用，且人的语音词表和正词法词表可能具有相同结构。\n\t-\t例如形态这种语言处理的很多性质，可以应用于语言的理解和生成。\n\n# 第四章：N元语法\n-\t语言模型是关于单词序列的统计模型，N元语法模型是其中的一种，它根据之前N-1个单词推测第N个单词，且这样的条件概率可以组成整个单词序列（句子）的联合概率。\n\n## 在语料库中统计单词\n-\t区别：word type或者叫 vocabulary size V，代表语料中不同单词的个数，而tokens，不去重，代表语料的大小。有研究认为词典大小不低于tokens数目的平方根。\n非平滑N元语法模型\n-\t任务：根据以前的单词推断下一个单词的概率：$P(w|h)$，以及计算整个句子的概率$P(W)$。\n-\t最朴素的做法是用古典概型，统计所有历史h和当前词w组成的片段在整个语料中出现的次数，并除以历史h片段在整个语料中出现的次数。句子的概率也用相似的方法产生。缺点：依赖大语料，且语言本身多变，这样的计算限制过于严格。\n-\t接下来引入N元语法模型，首先通过概率的链式法则，可以得到条件概率$P(w|h)$和整个句子的联合概率$P(W)$之间的关系：\n$$\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\\\\n= \\prod _{k=1}^n P(w_k|w_1^{k-1}) \\\\\n$$\n-\tN元语法模型放松了条件概率的限制，做出一个马尔可夫假设：每个单词的概率只和它之前N-1个单词相关，例如二元语法模型，只和前一个单词相关，用这个条件概率去近似$P(w|h)$:\n$$\nP(w_n|w_1^{n-1}) \\approx P(w_n|w_{n-1}) \\\\\n$$\n-\tN元语法模型里的条件概率用最大似然估计来估算，统计语料中各种N元语法的个数，并归一化，其中可以简化的一点是：以二元语法为例，所有给定单词开头的二元语法总数必定等于该单词一元语法的计数：\n$$\nP(w_n|w_{n-1}) = \\frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\\\\n$$\n-\t使用N元语法之后，句子概率的链式分解变得容易计算，我们可以通过计算各种句子的概率来判断句子是否包含错字，或者计算某些句子在给定上下文中出现的可能，因为N元语法能捕捉一些语言学上的特征，或者一些用语习惯。在语料充足的时候，我们可以使用三元语法模型获得更好的效果。\n\n## 训练集和测试集\n-\tN元语法模型对训练集非常敏感。N元语法的N越大，依赖的上下文信息越多，利用N元语法模型生成的句子就越流畅，但这些未必“过于流畅”，其原因在于N元语法概率矩阵非常大且非常稀疏，在N较大例如四元语法中，一旦生成了第一个单词，之后可供的选择非常少，接着生成第二个单词之后选择更少了，往往只有一个选择，这样生成的就和原文中某一个四元语法一模一样。过于依赖训练集会使得模型的泛化能力变差。因此我们选择的训练集和测试集应来自同一细分领域。\n-\t有时候测试集中会出现训练集词典里没有的词，即出现未登录词（Out Of Vocabulty,OOV）。在开放词典系统中，我们先固定词典大小，并将所有未登录词用特殊符号<UNK>代替，然后才进行训练。\n\n## 评价N元语法模型：困惑度\n-\t模型的评价分两种：外在评价和内在评价。外在评价是一种端到端的评价，看看某一模块的改进是否改进了整个模型的效果。内在评价的目的是快速衡量模块的潜在改进效果。内在评价的潜在改进效果不一定会使得端到端的外在评价提高，但是一般两者都存在某种正相关关系。\n-\t困惑度（Perplexsity,PP）是一种关于概率模型的内在评价方法。语言模型的在测试集上的困惑度是语言模型给测试集分配的概率的函数。以二元语法为例，测试集上的困惑度为：\n$$\nPP(W) = \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i|w_{i-1})}} \\\\\n$$\n-\t概率越高，困惑度越低。困惑度的两种解释：\n\t-\t加权的平均分支因子：分支因子是指可能接在任何上文之后的单词的数目。显然，如果我们的模型啥也没学习到，那么测试集任何单词可以接在任何上文之后，分支因子很高，困惑度很高；相反，如果我们的模型学习到了具体的规则，那么单词被限制接在一些指定上文之后，困惑度变低。困惑度使用了概率加权分支因子，分支因子的大小在模型学习前后不变，”morning”仍然可以接到任何上文之后，但是它接到”good”之后的概率变大了，因此是加权的分支因子。\n\t-\t熵：对于语言序列，我们定义一个序列的熵为：\t$$H(w_1,w_2,…,w_n )=-\\sum _{W_1^n \\in L} p(W_1^n) \\log ⁡p(W_1^n)$$也就是这个序列中所有前缀子序列的熵之和，其均值是序列的熵率。计算整个语言的熵，假设语言是一个产生单词序列的随机过程，单词序列无限长，则其熵率是：$$H(L)=\\lim _{n \\rightarrow \\infty}⁡ \\frac 1n H(w_1,w_2,…,w_n) =\\lim _{n \\rightarrow \\infty} -⁡\\frac 1n \\sum _{W \\in L} p(W_1^n)  \\log ⁡p(W_1^n)$$根据Shannon-McMillan-Breiman理论，在n趋于无穷的情况下，如果语言既是平稳又是正则的，上面这些子串的和的熵，可以用最大串代替每一个子串得到，这里的代替是指log后面求的是最大串的概率，log之前的概率依然是各个子串的概率？假如是这样的话提出最大串的概率对数，对所有子串概率求和得到：$$H(L)=\\lim _{n \\rightarrow \\infty} -⁡ \\frac 1n \\log ⁡p(w_1,w_2,…,w_n)$$交叉熵可以衡量我们的模型生成的概率分布到指定概率分布之间的距离，我们希望模型生成概率分布尽可能近似真实分布，即交叉熵小。具体衡量时是对相同的语言序列，计算训练得到的模型m和理想模型p在生成这个序列上的概率的交叉熵：$$H(p,m) = \\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\sum _{W \\in L} p(W_1^n) \\log⁡ m(W_1^n)$$但是我们不知道理想的分布p，这时根据之前的Shannon-McMillan-Breiman定理，得到了只包含一个概率分布的序列交叉熵（？）：$$H(p,m)=\\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\log⁡ m(W_1^n)$$在测试数据上我们没有无限长的序列，就用有限长的序列的交叉熵近似这个无限长序列的交叉熵。困惑度则是这个（近似的？只包含一个概率分布的？）交叉熵取指数运算：\n$$\nPerplexity(W) = 2^{H(W)} \\\\\n= P(w_1 w_2 ... w_N)^{\\frac {-1}{N}} \\\\\n= \\sqrt[n]{\\frac {1}{P(w_1 w_2 ... w_N)}} \\\\\n= \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i | w_1 ... w_{i-1})}} \\\\\n$$\n\n## 平滑\n-\t因为N元语法模型依赖语料，一般而言对于N越高的N元语法，语料提供的数据越稀疏。这种情况下N元语法对于那些计数很小的语法估计很差，且如果测试集中某一句包含了训练集中没有出现的N元语法时，我们无法使用困惑度进行评价。因此我们使用平滑作为一种改进方法，使得N元语法的最大似然估计能够适应这些存在0概率的情况。\n-\t接下来介绍了两种平滑：\n\t-\t拉普拉斯平滑（加1平滑）\n\t-\tGood-Turing 打折法\n\n### 拉普拉斯平滑\n-\t加1平滑就是在计算概率归一化之前，给每个计数加1，对应的，归一化时分母整体加了一个词典大小:\n$$\nP_{Laplace}(w_i) = \\frac {c_i + 1}{N+V} \\\\\n$$\n-\t为了表现平滑的作用，引入调整计数$c^{\\*}$，将平滑后的概率写成和平滑之前一样的形式：\n$$\nP_{Laplace} (w_i) = \\frac {(C_i^{\\*})}{N} \\\\\nC_i^{\\*} = \\frac {(C_i+1)N}{(N+V)} \\\\\n$$\n-\t一种看待平滑的角度是：对每个非0计数打折，分一些概率给0计数，定义相对打折$d_c$（定义在非0计数上），\n$$\nd_c = \\frac {c^{\\*}} {c}\n$$\n-\t$d_c$代表了打折前后单词计数的变化。平滑之后，对于非0计数，当$C_i < \\frac NV$时，计数增加；否则计数减少。计数越大，打折越多，增加越少（减少越多）。当0计数很多时，N/V较小，这时大部分非0计数都会减少，且减少较多。\n-\t而0计数则没有收到打折的影响。因此在一轮不同程度的增长之后，再归一化的结果就是非0计数分享了一些概率给0计数。写成调整计数的形式，就是非0计数减少数值，0计数变化（一般是减少）数值（但不是减少的完全等于增加的）。 书中给出了一个例子，下图是一部分语料的二元语法平滑之后的计数，蓝色代表平滑加1之后的0计数：\n![FoZNDg.png](https://s2.ax1x.com/2019/01/03/FoZNDg.png)\n如果把表写成调整计数的形式：\n![FoZtKS.png](https://s2.ax1x.com/2019/01/03/FoZtKS.png) \n-\t可以看到，本来的0计数（蓝色）从0变大，而其他的计数减少，例如< i want>，从827减少到527，<want to>从608减少到238。\n-\t当0计数很多时，非0计数减少的数值很多，可以使用一个小于1的小数$\\delta$代替1，即加$\\delta$平滑。通常这个$\\delta$是动态变化的。\n\n### GT打折法\n-\t类似于Good-Turing打折法, Witten-Bell打折法， Kneyser-Ney 平滑一类的方法，它们的基本动机是用只出现一次的事物的计数来估计从未出现的事物的计数。只出现一次的语法称为单件（singleton）或者罕见语（hapax legomena）。Good-Turing打折法用单件的频率来估计0计数二元语法。\n-\t定义N_c为出现c次的N元语法的总个数（不是总个数乘以c），并称之为频度c的频度。对N_c中的c的最大似然估计是c。这样相当于将N元语法按其出现次数分成了多个桶，GT打折法用c+1号桶里语法概率的最大似然估计来重新估计c号桶内语法的概率。因此GT估计之后最大似然估计得到的c被替换成：\n$$\nc^{\\*}=(c+1) \\frac {N_{c+1}}{N_c} \n$$\n-\t之后计算某N元语法的概率：\n\t-\t从未出现：$P_{GT}^{\\*}=\\frac{N_1}{N}$。其中N是所有N元语法数$(\\sum _i N_i \\* i)$。这里假设了我们已知$N_0$，则此式表示某一具体未知计数N元语法概率时还应除以$N_0$。\n\t-\t已出现（已知计数）：$P_{GT}^{\\*} = \\frac{c^{\\*}}{N}$\n-\t这样计算，$N_1$的一些概率转移到了$N_0$上。GT打折法假设所有的N元语法概率分布满足二项式分布，且假设我们已知$N_0$，以二元语法为例：\n$$\nN_0 = V^2 - \\sum _{i>0} N_i \\\\ \n$$\n-\t其他注意事项：\n\t-\t有些$N_c$为0，这时我们无法用这些$N_c$来计算平滑后的c。这种情况下我们直接放弃平滑，令$c^{\\*} = c$，再根据正常的数据计算出一个对数线性映射，$log⁡(N_c) = a + b \\log(c)$，代入放弃平滑的c并用其倒推计算计数为0的$N_c$，使得这些$N_c$有值，不会影响更高阶的c的计算。\n\t-\t只对较小c的$N_c$进行平滑，较大c的$N_c$认为足够可靠，设定一个阈值k，对$c < k$的$N_c$计算：\n$$\nc^{\\*} = \\frac {(c+1) \\frac {N_c+1}{N_c} - c \\frac {(k+1) N_{k+1} }{N_1} } {1- \\frac {(k+1)N_{k+1}} {N_1}} \\\\\n$$\n\t-\t计算较小的c如c=1时，也看成c=0的情况进行平滑\n-\t一个例子：\n![FoZGgf.png](https://s2.ax1x.com/2019/01/03/FoZGgf.png)\n \n## 插值与回退\n-\t上述的平滑只考虑了如何转移概率到计数为0的语法上去，对于条件概率$p(w|h)$，我们也可以采用类似的思想，假如不存在某个三元语法帮助计算$p(w_n |w_{n-1} w_{n-2})$，则可以用阶数较低的语法$p(w_n |w_{n-1})$帮助计算，有两种方案：\n\t-\t回退：用低阶数语法的替代0计数的高阶语法\n\t-\t插值：用低阶数语法的加权估计高阶语法\n-\t在Katz回退中，我们使用GT打折作为方法的一部分：GT打折告诉我们有多少概率可以从已知语法中分出来，Katz回退告诉我们如何将这些分出来的概率分配给未知语法。在之前的GT打折法中，我们将分出的概率均匀分给每一个未知语法，而Katz回退则依靠低阶语法的信息来分配：\n![FoZJv8.png](https://s2.ax1x.com/2019/01/03/FoZJv8.png)\n-\t其中$P^{\\*}$是打折之后得到的概率；\\alpha是归一化系数，保证分出去的概率等于未知语法分配得到的概率。\n-\t插值则是用低阶语法概率加权求和得到未知高阶语法概率：\n![FoZUbQ.png](https://s2.ax1x.com/2019/01/03/FoZUbQ.png)\n-\t加权的系数还可以通过上下文动态计算。具体系数的计算有两种方法：\n\t-\t尝试各种系数，用在验证集上表现最好的系数组合\n\t-\t将系数看成是概率生成模型的隐变量，使用EM算法进行推断\n\n## 实际问题：工具和数据格式\n-\t在语言模型计算中，一般将概率取对数进行计算，原因有二：防止数值下溢；取对数能将累乘运算变成累加，加速计算。\n-\t回退N元语法模型一般采用ARPA格式。ARPA格式文件由一些头部信息和各类N元语法的列表组成，列表中包含了该类N元语法下所有语法，概率，和回退的归一化系数。只有能够称为高阶语法前缀的低阶语法才能在回退中被利用，并拥有归一化系数。\n-\t两种计算N元语法模型的工具包：SRILM toolkit 和Cambridge-CMU toolkit\n\n## 语言建模中的高级问题\n### 高级平滑方法：Kneser-Ney平滑\n-\t注意到在GT打折法当中，打折之后估计的c值比最大似然估计得到的c值近似多出一个定值d。绝对打折法便考虑了这一点，在每个计数中减去这个d：\n![FoZwUs.png](https://s2.ax1x.com/2019/01/03/FoZwUs.png)\n-\tKneser-Ney平滑吸收了这种观点，并且还考虑了连续性：在不同上文中出现的单词更有可能出现在新的上文之后，在回退时，我们应该优先考虑这种在多种上文环境里出现的词，而不是那些出现次数很多，但仅仅在特定上文中出现的词。\n![FoZdEj.png](https://s2.ax1x.com/2019/01/03/FoZdEj.png)\n-\t在Kneser-Ney中，插值法能够比回退法取得更加好的效果：\n![FoZ05n.png](https://s2.ax1x.com/2019/01/03/FoZ05n.png)\n\n### 基于分类的N元语法\n-\t这种方法是为了解决训练数据的稀疏性。例如IBM聚类，每个单词只能属于一类，以二元语法为例，某个二元语法的条件概率的计算变为给定上文所在类，某个单词的条件概率，还可以进一步链式分解为两个类的条件概率乘以某个单词在给定其类条件下的条件概率：\n$$\np(w_i│w_{i-1} ) \\approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \\cdot p(w_i |c_i)\n$$\n\n### 语言模型适应和网络应用\n-\t适应是指在大型宽泛的语料库上训练语言模型，并在小的细分领域的语言模型上进一步改进。网络是大型语料库的一个重要来源。在实际应用时我们不可能搜索每一个语法并统计搜索得到所有页面上的所有语法，我们用搜索得到的页面数来近似计数。\n\n### 利用更长距离的上文信息\n-\t通常我们使用二元和三元语法模型，但是更大的N能够带来更好的效果。为了捕捉更长距离的上文信息，有以下几种方法：\n\t-\t基于缓存机制的N元语法模型\n\t-\t基于主题建模的N元语法模型，对不同主题建模语言模型，再加权求和\n\t-\t不一定使用相邻的上文信息，例如skip N-grams或者不一定使用定长的上文信息，例如变长N-grams\n\n# 第十六章：语言的复杂性\n## Chomsky层级\n-\tChomsky层级反映了不同形式化方法描述的语法之间的蕴含关系，较强生成能力或者说更复杂的语法在层级的外层。从外到内，加在可重写语法规则上的约束增加，语言的生成能力逐渐降低。\n![FoZXad.png](https://s2.ax1x.com/2019/01/03/FoZXad.png)\n-\t五种语法对应的规则和应用实例：\n![Foepxf.png](https://s2.ax1x.com/2019/01/03/Foepxf.png)\n\t-\t0型语法：规则上只有一个限制，即规则左侧不能为空字符串。0型语法刻画了递归可枚举语言\n\t-\t上下文相关语法：可以把上下文\\alpha，\\beta之间的非终极符号A重写成任意非空符号串\n\t-\t温和的上下文相关语法\n\t-\t上下文无关语法：可以把任何单独的非终极符号重写为由终极符号和非终极符号构成的字符串，也可以重写为空字符串\n\t-\t正则语法：可以是右线性也可以是左线性，以右线性为例，非终极符号可以重写为左边加了若干终极符号的另一个非终极符号，右线性不断地在字符串左侧生成终极符号。\n\n## 自然语言是否正则\n-\t判断语言是否正则能够让我们了解应该用哪一层次的语法来描述一门语言，且这个问题能够帮助我们了解自然语言的不同方面的某些形式特性。\n-\t抽吸引理：用来证明一门语言不是正则语言。\n\t-\t如果一门语言可以被有限状态自动机来描述，则与自动机对应有一个记忆约束量。这个约束量对于不同的符号串不会增长的很大，因为其状态数目是固定的，更长的符号串应该是通过状态之间转移产生而不是增加状态数目。因此这个记忆量不一定和输入的长度成比例。\n\t-\t如果一个正则语言能够描述任意长的符号序列，比自动机的状态数目还多，则该语言的自动机中必然存在回路。\n![FoZxPI.png](https://s2.ax1x.com/2019/01/03/FoZxPI.png)\n-\t如图所示自动机，可以表述xyz,xyyz,xyyyz.....，当然也可以将中间无限长的y序列“抽吸掉”，表述xz。抽吸引理表述如下：\n-\t设L是一个有限的正则语言，那么必然存在符号串x,y,z,使得对于任意n≥0，y≠$\\epsilon$，且xy^n z∈L\n-\t即假如一门语言是正则语言，则存在某一个符号串y，可以被适当的“抽吸”。这个定理是一门语言是正则语言的必要非充分条件。\n-\t有学者证明英语不是一门正则语言：\n\t-\t具有镜像性质的句子通过抽吸原理可以证明不是正则语言，而英语中一个特殊的子集合和这种镜像性质的句子是同态的。\n\t-\t另一种证明基于某些带有中心-嵌套结构的句子。这种句子可以由英语和某一类简单的正则表达式相交得到，通过抽吸原理可以得到这种句子不是正则语言。英语和正则语言的交不是正则语言，则英语不是正则语言。\n\n## 自然语言是否上下文无关\n-\t既然自然语言不是正则语言，我们接着考虑更宽松的限定，自然语言是否是上下文无关的？\n-\t不是......\n\n## 计算复杂性和人的语言处理\n-\t人对中心嵌套句子处理很困难，因为人们剖析时利用的栈记忆有限，且栈中不同层次记忆容易混淆。\n\n# 第五章：词类标注\n-\t各种表述：POS（Part Of Speech）、word classes（词类）、morphological classes（形态类）、lexical tags（词汇标记）。\n-\tPOS的意义在于：\n\t-\t能够提供关于单词及其上下文的大量信息。\n\t-\t同一单词在不同词类下发音不同，因此POS还能为语音处理提供信息。\n\t-\t进行词干分割（stemming），辅助信息检索\n-\t本章介绍三种词类标注算法：\n\t-\t基于规则的算法\n\t-\t基于概率的算法，隐马尔科夫模型\n\t-\t基于变换的算法\n\n## 一般词类\n-\tPOS分为封闭集和开放集，封闭集集合相对稳定，例如介词，开放集的词语则不断动态扩充，例如名词和动词。特定某个说话人或者某个语料的开放集可能不同，但是所有说一种语言以及各种大规模语料库可能共享相同的封闭集。封闭集的单词称为虚词（功能词，function word），这些词是语法词，一般很短，出现频次很高。\n-\t四大开放类：名词、动词、形容词、副词。\n-\t名词是从功能上定义的而不是从语义上定义的，因此名词一般表示人、地点、事物，但既不充分也不必要。定义名词：\n\t-\t与限定词同时出现\n\t-\t可以受主有代词修饰\n\t-\t大多数可以以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词\n-\t动词，表示行为和过程的词，包括第三人称单数、非第三人称单数、进行时、过去分词几种形态\n-\t形容词，描述性质和质量\n-\t副词，用于修饰，副词可以修饰动词、动词短语、其它副词。\n-\t英语中的一些封闭类：\n\t-\t介词 prepositions：出现在名词短语之前，表示关系\n\t-\t限定词 determiners 冠词 articles：与有定性（definiteness）相关\n\t-\t代词 pronouns：简短的援引某些名词短语、实体、或事件的一种形式\n\t-\t连接词 conjunctions：用于连接和补足（complementation）\n\t-\t助动词 auxiliary verbs：标志主要动词的某些语义特征，包括：时态、完成体、极性对立、情态\n\t-\t小品词 particles：与动词结合形成短语动词\n\t-\t数词 numerals\n\n## 词类标注\n-\t标注算法的输入是单词的符号串和标记集，输出要让每一个单词标注上一个单独且最佳的标记。如果每个单词只对应一种词性，那么根据已有的标记集，词类标注就是一个简单的查表打标的过程，但是很多词存在多种词性，例如book既可以是名词也可以是动词，因此要进行消歧，词类标注是歧义消解的一个重要方面。\n\n## 基于规则的词类标注\n-\t介绍了ENGTWOL系统，根据双层形态学构建，对于每一个词的每一种词类分别立条，计算时不计屈折形式和派生形式.\n-\t标注算法的第一阶段是将单词通过双层转录机，得到该单词的所有可能词类\n-\t之后通过施加约束规则排除不正确的词类。这些规则通过上下文的类型来决定排除哪些词类。\n\n## 基于隐马尔科夫模型的词类标注\n-\t使用隐马尔科夫模型做词类标注是一类贝叶斯推断，这种方法将词类标注看成是序列分类任务。观察量为一个词序列（比如句子），任务是给这个序列分配一个标注序列。\n-\t给定一个句子，贝叶斯推断想要在所有标注序列可能中选择最好的一个序列，即\n$$\n{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)\n$$\n-\t使用贝叶斯法则将其转化为：\n$$\n{t_1^n} _{best}={argmax} _{t_1^n}  \\frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n} P(w_1^n│t_1^n)P(t_1^n)\n$$\n-\t隐马尔科夫模型在此基础上做了两点假设\n\t-\t一个词出现的概率只与该词的词类标注有关，与上下文其他词和其他标注无关，从而将序列的联合概率拆解为元素概率之积，即：P(w_1^n│t_1^n) \\approx \\prod _{i=1}^n P(w_i |t_i)\n\t-\t一个标注出现的概率只与前一个标注相关，类似于二元语法的假设：P(t_1^n ) \\approx \\prod _{i=1}^n P(t_i |t_{i-1})\n-\t在两种假设下简化后的最好标注序列表达式为：\n$$\n{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})\n$$\n-\t上面这个概率表达式实际上将HMM模型的联合概率拆成了各个部分转移概率的乘积，具体而言分为标签转移概率（隐变量之间转移）和词似然（隐变量转移到可观察变量）。通过最大似然估计，我们可以通过古典概型的方法从已标注的语料中计算出这两类概率：\n$$\nP(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\\\\nP(w_i│t_i ) = \\frac{C(t_i,w_i)}{C(t_i)} \\\\\n$$\n-\t一个例子：HMM模型如何正确的将下句中的race识别为动词而不是名词：\n-\tSecretariat is expected to race tomorrow.\n-\t画出上句中race被识别为动词和名词两种情况下的HMM模型，可以看到两个模型对比只有三个转移概率不同，用加粗线标出：\n![FoZDCq.png](https://s2.ax1x.com/2019/01/03/FoZDCq.png)\n-\tHMM词类标注器消歧的方式是全局的而不是局部的。我们在语料中统计得到这三种转移概率，再累乘，结果是(a)的概率是(b)概率的843倍。显然race应该被标注为动词。\n\n## 形式化隐马尔科夫模型标注器\n-\tHMM模型是有限自动机的扩展，具体而言是一种加权有限自动机，马尔可夫链的扩展，这种模型允许我们考虑观察量和隐变量，考虑包含隐变量的概率模型。HMM包含以下组件：\n\t-\tQ：大小为N的状态集\n\t-\tA：大小为N*N的转移概率矩阵\n\t-\tO：大小为T的观察事件集\n\t-\tB：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察o_t的概率\n\t-\t$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量\n-\tA中的概率和B中的概率对应着之前式子中每一个累乘项里的先验$P(w_i│t_i )$和似然$P(t_i |t _{i-1})$概率：\n$$\n{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})\n$$\n\n## HMM标注的维特比算法\n-\t在HMM模型中，已知转移概率和观察序列，求隐变量的任务叫做解码。解码的一种算法即维特比算法，实质上是一种动态规划算法，与之前求最小编辑距离的算法类似。\n-\t首先我们从语料中计算得到A和B两个矩阵，即模型的转移概率已知，对于给定的观察序列，按照以下步骤执行维特比算法：\n![FoZyvT.png](https://s2.ax1x.com/2019/01/03/FoZyvT.png)\n-\t算法维护一个$(N+2)\\*T$的概率矩阵viterbi，加了2代表初始状态和结束状态，viterbi[s,t]代表了在第t步状态为s时的最佳路径概率，而backpointer[s,t]对应着保存了该最佳路径的上一步是什么状态，用于回溯输出整个最佳路径。\n-\t关键的转移在于$viterbi[s,t] \\leftarrow max _{s^{\\*}=1}^N⁡ viterbi[s^{\\*},t-1] \\* a_{s^{\\*},s} \\* b_s (o_t)$即当前时间步最佳路径是由上一时间步各个状态的最佳路径转移过来的，选择上一步最佳路径概率与转移概率乘积最大的路径作为当前时间步的最佳路径。从动态规划的角度而言，即长度为t的最佳路径，必定是从长度为t-1的最佳路径里选择一条转移得到，否则肯定可以从另一条概率更大的路径转移获得更优解。这样就限制了最佳路径的生成可能，减少了计算量。\n\n## 将HMM算法扩展到三元语法\n-\t现代的HMM标注器一般在标注转移概率上考虑更长的上文历史：\n$$\nP(t_1^n ) \\approx \\prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})\n$$\n-\t这样的话需要在序列开头和结尾做一些边界处理。使用三元语法的一个问题是数据稀疏：例如我们从没有在训练集中见过标注序列PRP VB TO，则我们无法计算P(TO|PRP,VB)。一种解决办法是线性插值：\n$$\nP(t_i│t _{i-1} t _{i-2} ) = \\lambda _1 P ̂(t_i│t _{i-1} t _{i-2} )+\\lambda _2 P ̂(t_i│t _{i-1} )+\\lambda _3 P ̂(t_i)\n$$\n-\t使用删除插值的办法确定系数$\\lambda$：\n![FoZr80.png](https://s2.ax1x.com/2019/01/03/FoZr80.png)\n\n## 基于变换的标注\n-\t基于变换的方法结合了基于规则和基于概率方法的优点。基于变换的方法依然需要规则，但是从数据中总结出规则，是一种监督学习方法，称为基于变换的学习（Transformation Based Learning，TBL）。在TBL算法中，语料库首先用比较宽的规则来标注，然后再选择稍微特殊的规则来修改，接着再使用更窄的规则来修改数量更少的标记。\n\n## 如何应用TBL规则\n-\t首先应用最宽泛的规则，就是根据概率给每个词标注，选择概率最大的词类作为标注。之后应用变换规则，即如果满足某一条件，就将之前标注的某一词类变换（纠正）为正确的词类，之后不断应用更严格的变换，在上一次变换的基础上进行小部分的修改。\n-\t如何学习到TBL规则\n\t-\t首先给每个词打上最可能的标签\n\t-\t检查每一个可能的变换，选择效果提升最多的变换，此处需要直到每一个词正确的标签来衡量变换带来的提升效果，因此是监督学习。\n\t-\t根据这个被选择的变换给数据重新打标，重复步骤2，直到收敛（提升效果小于某一阈值）\n-\t以上过程输出的结果是一有序变换序列，用来组成一个标注过程，在新语料上应用。虽然可以穷举所有的规则，但是那样复杂度太高，因此我们需要限制变换集合的大小。解决方案是设计一个小的模板集合（抽象变换）,每一个允许的变换都是其中一个模板的实例化。\n\n## 评价和错误分析\n-\t一般分为训练集、验证集、测试集，在训练集内做十折交叉验证。\n-\t与人类标注的黄金标准比较计算准确率作为衡量指标。\n-\t一般用人类表现作为ceiling，用一元语法最大概率标注的结果作为baseline。\n-\t通过含混矩阵或者列联表来进行错误分析。在N分类任务中，一个N*N的含混矩阵的第i行第j列元素指示第i类被错分为第j类的次数在总分错次数中的占比。一些常见的容易分错的词性包括：\n\t-\t单数名词、专有名词、形容词\n\t-\t副词、小品词、介词\n\t-\t动词过去式、动词过去分词、形容词\n\n## 词性标注中的一些其他问题\n-\t标注不确定性：一个词在多个词性之间存在歧义，很难区分。这种情况下有些标注器允许一个词被打上多个词性标注。在训练和测试的时候，有三种方式解决这种多标注词：\n\t-\t通过某种方式从这些候选标注中选择一个标注\n\t-\t训练时指定一个词性，测试时只要打上了候选词性中任意一个就认为标注正确\n\t-\t将整个不确定的词性集看成一个新的复杂词性\n-\t多部分词：在标注之前需要先分词，一些多部分词是否应该被分为一部分，例如New York City应该分成三部分还是一个整体，也是各个标注系统需要考虑的。\n-\t未知词：不在词典中的词称为未知词。对于未知词，训练集无法给出它的似然P(w_i |t_i)，可以通过以下几种方式解决：\n\t-\t只依赖上下文的POS信息预测\n\t-\t用只出现一次的词来估计未知词的分布，类似于Good Turing打折法\n\t-\t使用未知词的单词拼写信息，正词法信息。例如连字符、ed结尾、首字母大写等特征。之后在训练集中计算每个特征的似然，并假设特征之间独立，然后累乘特征似然作为未知词的似然：$P(w_i│t_i )=p(unknown word│t_i ) \\* p(capital│t_i ) \\* p(endings/hyph|t_i)$\n\t-\t使用最大熵马尔可夫模型\n\t-\t使用对数线性模型\n\n## 噪声信道模型\n-\t贝叶斯推断用于标注可以认为是一种噪声信道模型的应用，本节介绍如何用噪声信道模型来完成拼写纠正任务。\n之前对于非单词错误，通过词典查找可以检测到错误，并根据最小编辑距离纠正错误，但这种方法对于真实单词错误无能为力。噪声信道模型可以纠正这两种类型的拼写错误。\n-\t噪声信道模型的动机在于将错误拼写的单词看成是一个正确拼写的单词经过一个噪声信道时受到干扰扭曲得到。我们尝试所有可能的正确的词，将其输入信道，最后得到的干扰之后的词与错误拼写的词比较，最相似的例子对应的输入词就认为是正确的词。这类噪声信道模型，比如之前的HMM标注模型，是贝叶斯推断的一种特例。我们看到一个观察两（错误拼写词）并希望找到生成这个观察量的隐变量（正确拼写词），也就是找最大后验。\n-\t将噪声信道模型应用于拼写纠正：首先假设各种拼写错误类型，错拼一个、错拼两个、漏拼一个等，然后产生所有可能的纠正，除去词典中不存在的，最后分别计算后验概率，选择后验概率最大的作为纠正。其中需要根据局部上下文特征来计算似然。\n-\t另一种纠正算法是通过迭代来改进的方法：先假设拼写纠正的含混矩阵是均匀分布的，之后根据含混矩阵运行纠正算法，根据纠正之后的数据集更新含混矩阵，反复迭代。这种迭代的算法是一种EM算法。\n\n## 根据上下文进行拼写纠正\n-\t即真实单词拼写错误的纠正。为了解决这类任务需要对噪声信道模型进行扩展：在产生候选纠正词时，需要包括该单词本身以及同音异形词。之后根据整个句子的最大似然来选择正确的纠正词。\n\n# 第六章：隐马尔科夫模型和最大熵模型\n-\t隐马尔科夫模型用来解决序列标注（序列分类问题）。\n-\t最大熵方法是一种分类思想，在满足给定条件下分类应满足限制最小（熵最大），满足奥卡姆剃刀原理。\n-\t最大熵马尔可夫模型是最大熵方法在序列标注任务上的扩展。\n\n## 马尔可夫链\n-\t加权有限自动状态机是对有限自动状态机的扩展，每条转移路径上加上了概率作为权重，说明从这条路径转移的可能性。马尔可夫链是加权有限状态自动机的一种特殊情况，其输入序列唯一确定了自动机会经过的状态序列。马尔可夫链只能对确定性序列分配概率。\n-\t我们将马尔可夫链看作一种概率图模型，一个马尔可夫链由下面的成分确定：\n$$\nQ=q_1 q_2…q_N \\\\\nA=a_{01} a_{02} … a_{n1} … a_{nn} \\\\\nq_0,q_F \\\\\n$$\n-\t分别是\n\t-\t状态集合\n\t-\t转移概率矩阵，其中a_ij代表了从状态i转移到状态j的概率$P(q_j |q_i)$\n\t-\t特殊的开始状态和结束状态\n-\t概率图表示将状态看成图中的点，将转移看成边。\n-\t一阶马尔可夫对转移做了很强的假设：某一状态的概率只与前一状态相关：\n$$\nP(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})\n$$\n-\t马尔可夫链的另一种表示不需要开始和结束状态：\n$$\n\\pi = \\pi _1,\\pi _2 , … , \\pi _N \\\\\nQA={q_x,q_y…} \\\\\n$$\n-\t分别是：\n\t-\t状态的初始概率分布，马尔可夫链以概率$\\pi _i$从状态i开始\n\t-\t集合QA是Q的子集，代表合法的接受状态\n-\t因此状态1作为初始状态的概率既可以写成$a_{01}$也可以写成$\\pi _1$。\n\n## 隐马尔科夫模型\n-\t当马尔可夫链已知时，我们可以用其计算一个观测序列出现的概率。但是观测序列可能依赖于一些不可观测的隐变量，我们可能感兴趣的是推断出这些隐变量。隐马尔科夫模型允许我们同时考虑观测变量和隐变量。\n-\t如之前一样定义隐马尔科夫模型：\n\t-\tQ：大小为N的状态集\n\t-\tA：大小为N*N的转移概率矩阵\n\t-\tO：大小为T的观察事件集\n\t-\tB：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察$o_t$的概率\n\t-\t$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量\n-\t同样的，隐马尔科夫也可以用另一种不依赖初始和结束状态的方式表示。隐马尔科夫模型也做了两个假设，分别是隐状态之间转移和隐状态到观察量转移的一阶马尔可夫性。\n-\t对于隐马尔科夫模型需要解决三类问题：\n\t-\t似然计算：已知参数和观测序列，求似然$P(O|\\lambda)$\n\t-\t解码：已知参数和观测序列，求隐状态序列\n\t-\t学习：已知观测序列和隐状态集合，求解模型参数\n\n## 计算似然：前向算法\n-\t对于马尔可夫链，其没有隐状态到观测量的转移概率矩阵，可以看成观察量与隐状态相同。在隐马尔科夫模型中不能直接计算似然，我们需要直到隐状态序列。\n-\t先假设隐状态序列已知，则似然计算为：\n$$\nP(O│Q) = \\prod _{i=1}^T P(o_i |q_i)\n$$\n-\t根据隐状态转移的一阶马尔可夫性，可以求得隐状态的先验，乘以似然得到观测序列和隐状态序列的联合概率：\n$$\nP(O,Q)=P(O│Q) \\* P(Q) = \\prod _{i=1}^n P(o_i│q_i )  \\prod _{i=1}^n P(q_i |q _{i-1})\n$$\n-\t对于联合概率积分掉隐状态序列，就可以得到观测概率的似然：\n$$\nP(O) = \\sum _Q P(O,Q) = \\sum _Q P(O|Q)P(Q) \n$$\n-\t这样计算相当于考虑了所有的隐状态可能，并对每一种可能从隐状态序列开始到结束计算一次似然，实际上可以保留每次计算的中间状态来减少重复计算，也就是动态规划。在前向计算HMM观测似然使用的动态规划算法称为前向算法：\n\t-\t令$\\alpha _t (j)$代表在得到前t个观测量之后当前时刻隐变量处于状态j的概率,\\lambda为模型参数：\n\t$$\n\t\\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\\lambda)\n\t$$\n\t-\t这个概率值可以根据前一时间步的\\alpha值计算出来，避免了每次从头开始计算：\n\t$$\n\t\\alpha _t (j) = \\sum _{i=1}^N \\alpha _{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t初始化$\\alpha _1 (j)$：\n\t$$\n\t\\alpha _1 (j)=a_{0s} b_s (o_1)\n\t$$\n\t-\t终止状态：\n\t$$\n\tP(O│\\lambda) = \\alpha _T (q_F) = \\sum _{i=1}^N \\alpha _T (i) \\alpha _{iF}\n\t$$\n\n## 解码：维特比算法\n-\t解码任务是根据观测序列和参数推断出最有可能隐状态序列。最朴素的做法：对于每种可能的隐状态序列，计算观测序列的似然，取似然最大时对应的隐状态序列。但是这样做就如同朴素的计算似然方法一样，时间复杂度过高，同样的，我们使用动态规划来缩小求解的规模。在解码时使用了一种维特比算法。\n\t-\t令$v_t (j)$代表已知前t个观测量（1~t）和已知前t个隐状态（0~t-1）的条件下，当前时刻隐状态为j的概率：\n\t$$\n\tv_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 … o_t,q_t=j|\\lambda)\n\t$$\n\t-\t其中我们已知了前t个时间步最大可能的隐状态序列，这些状态序列也是通过动态规划得到的：\n\t$$\n\tv_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t为了得到最佳的隐状态序列，还需要记录每一步的最佳选择，方便回溯得到路径：\n\t$$\n\t{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t初始化：\n\t$$\n\tv_1 (j) = a_{0j} b_j (o_1) \\ \\  1 \\leq j \\leq N \\\\\n\t{bt}_1 (j) = 0 \\\\\n\t$$\n\t-\t终止，分别得到最佳隐状态序列（回溯开始值）及其似然值：\n\t$$\n\tP \\* = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) \\* a_{i,F} \\\\\n\tq_{T\\*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) \\* a_{i,F} \\\\\n\t$$\n-\t维特比算法减小时间复杂度的原因在于其并没有计算所有的隐状态路径，而是利用了每一时间步的最佳路径只能从上一时间步的最佳路径中延伸而来这一条件，减少了路径候选，避免了许多不必要的路径计算。并且每一步利用上一步的结果也是用了动态规划的思想减少了计算量。\n\n## 训练隐马尔科夫模型：前向后向算法\n-\t学习问题是指已知观测序列和隐状态集合，求解模型参数。\n-\t前向后向算法，又称Baum-Welch算法，是EM算法的一种特例，用来求解包含隐变量的概率生成模型的参数。该算法通过迭代的方式反复更新转移概率和生成概率，直到收敛。BW算法通过设计计数值之比作为隐变量，将转移概率矩阵和生成概率矩阵一起迭代更新。\n-\t先考虑马尔科夫链的学习问题。马尔科夫链可以看作是退化的隐马尔科夫模型，即每个隐变量只生成和自己一样的观测量，生成其他观测量的概率为0。因此只需学习转移概率。\n-\t对于马尔可夫链，可以通过古典概型统计出转移概率：\n$$\na_{ij} = \\frac {Count(i \\rightarrow j)} {\\sum _{q \\in Q} Count(i \\rightarrow q)}\n$$\n-\t我们可以这样直接计算概率是因为在马尔可夫链中我们知道当前所处的状态。对于隐马尔科夫模型我们无法这样直接计算是因为对于给定输入，隐状态序列无法确定。Badum-Welch算法使用了两种简洁的直觉来解决这一问题：\n\t-\t迭代估计，先假设一种转移概率和生成概率，再根据假设的概率推出更好的概率\n\t-\t计算某一观测量的前向概率，并将这个概率分摊到不同的路径上，通过这种方式估计概率\n-\t首先类似于前向概率，我们定义后向概率：\n\t-\t令$\\beta _t (i)$代表在得到后t个观测量之后当前时刻隐变量处于状态i的概率,$\\lambda$为模型参数：\n\t$$\n\t\\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\\lambda)\n\t$$\n\t-\t类似于后向概率的归纳计算：\n\t$$\n\t\\beta_t (i) = \\sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j),  \\ \\   1≤i≤N,1≤t<T\n\t$$\n\t-\t初始化$\\alpha _1 (j)$：\n\t$$\n\t\\beta _T (i)=\\alpha _(i,F)\n\t$$\n\t-\t终止状态：\n\t$$\n\tP(O│\\lambda)=\\alpha _t (q_F )=\\beta_1 (0)= \\sum _{i=1}^N a_{0j} b_j (o_1) \\beta _1 (j)\n\t$$\n-\t类似的，我们希望马尔可夫链中的古典概率能帮助我们估计转移概率：\n$$\na_{ij}^{\\*} = \\frac{从状态i转移到状态j的计数值期望}{从状态i转移出去的计数值期望}\n$$\n-\t如何估计计数值：我们将整个序列的转移路径计数值转化为时间步之间转移路径计数值之和，时间步之间某一条转移路径的概率为：\n$$\nP(q_t=i,q_{t+1}=j)\n$$\n-\t首先考虑所有的观测序列和这一转移路径的联合概率（省略了以参数$\\lambda$为条件）：\n$$\nP(q_t=i,q_{t+1}=j,O)\n$$\n-\t观察下面的概率图：\n![FoZWVJ.png](https://s2.ax1x.com/2019/01/03/FoZWVJ.png)\n-\t可以看到这一联合概率包含了三个部分：\n\t-\tT时刻隐状态为i的前向概率\n\t-\tT+1时刻隐状态为j的后向概率\n\t-\tT时刻与T+1时刻的状态转移概率以及生成对应观测量的生成概率\n-\t所以有：\n$$\nP(q_t=i,q_{t+1}=j,O)=\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j)\n$$\n-\t为了从联合分布中得到已知观测序列求转移路径的联合概率，需要计算观测序列的概率，可以通过前向概率或者后向概率求得：\n$$\nP(O)=\\alpha _t (N)=\\beta _T (1) = \\sum _{j=1}^N \\alpha _t (j) \\beta_t (j)\n$$\n-\t最终得到\n$$\nξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \\frac {(\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))}{(\\alpha _t (N))}\n$$\n-\t最后，对所有时间步求和就可以得到从状态i转移到状态j的期望计数值，从而进一步得到转移概率的估计：\n$$\na_{ij}^{\\*} = \\frac {\\sum _{t=1}^{T-1} ξ_t (i,j)}{\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1} ξ_t (i,j)}\n$$\n-\t同样的，我们还希望得到生成概率的估计：\n$$\nb_{j}^{\\*} (v_k) = \\frac {在状态j观测到符号v_k 的计数值期望}{状态j观测到所有符号的计数值期望}\n$$\n-\t类似的，通过先计算联合分布再计算条件分布的方式得到在t时刻处于隐状态j的概率：\n$$\nγ_t (j)=P(q_t=j│O) = \\frac {P(q_t=j,O)}{P(O)}\n$$\n-\t联合概率包含两个部分，即t时刻处于状态j的前向概率和后向概率，所以有：\n$$\nγ_t (j) = \\frac {\\alpha _t (j) \\beta_t (j)}{\\alpha _t (N)}\n$$\n-\t类似的，对所有时间步累加，进而得到生成概率的估计：\n$$\nb_{j}^{\\*} (v_k) = \\frac{\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) }{\\sum _{t=1}^T   γ_t (j) }\n$$\n-\t这两个式子是在已知前向概率和后向概率$(\\alpha,\\beta)$的情况下，计算出中间变量（隐变量）(ξ,γ),引入隐变量的动机是将a、b估计值的期望计数值之比转化为概率之比，且这两个隐变量可以用a,b表示。再由隐变量计算出转移概率和生成概率，因此形成了一个迭代的循环，可以用EM算法求解：\n$$\na,b→\\alpha,\\beta→ξ,γ→a,b\n$$\n-\tE-step:\n$$\nγ_t (j) = (\\alpha _t (j) \\beta_t (j))/(\\alpha _t (N)) ξ_t (i,j) \\\\\n= (\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))/(\\alpha _t (N)) \\\\\n$$\n-\tM-step（最大化的目标是什么）:\n$$\na _{ij} = (\\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1}   ξ_t (i,j)  ) \\\\\nb ̂_j(v_k) = (\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\\sum _{t=1}^T   γ_t (j) ) \\\\\n$$\n-\t迭代时需重新计算：\n$$\n\\alpha _t (j) = \\sum _{i=1}^N   \\alpha_{t-1} (i) a_ij b_j (o_t) \\\\\n\\beta_t (i) = \\sum _{j=1}^N   a_ij b_j (o_{t+1} ) \\beta_{t+1} (j)  \\\\\n$$\n-\t迭代的初始状态对于EM算法来说很重要，经常是通过引入一些外部信息来设计一个好的初始状态。\n\n## 最大熵模型：背景\n-\t最大熵模型另一种广为人知的形式是多项Logistic回归（Softmax?）。\n-\t最大熵模型解决分类问题，最大熵模型作为一种概率分类器，能够根据样本的特征求出样本属于每一个类别的概率，进而进行分类。\n-\t最大熵模型属于指数家族（对数线性）分类器，通过将特征线性组合，取指数得到分类概率：\n$$\np(c│x)=\\frac 1Z exp⁡(\\sum _i   weight_i feature_i) \n$$\n-\tZ是一个归一化系数，使得生成的概率之和为1。\n\n## 最大熵建模\n-\t将二分类Logistic回归推广到多分类问题就得到：\n$$\nP(c│x) = \\frac {exp⁡(\\sum _(i=0)^N   w_ci f_i) } {\\sum _{c^{\\*} in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i)  }\n$$\n-\t语音和语言处理中的特征通常是二值的（是否有该特征），因此使用指示函数表示特征\n$$\nP(c│x) = \\frac {exp⁡(\\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\\sum _{c^{\\*} \\in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i (c^{\\*},x))  }\n$$\n-\t注意到在该模型中每一个类都有其独立的线性权重w_c。相比于硬分布，最大熵模型能够给出分到每一类的概率，因此可以求出每一时刻的分类概率进而求出整体分类概率，得到全局最优分类结果。注意到不同于支持向量机等模型，最大熵模型无法利用特征之间的组合，必须手动构造组合作为新的特征。\n-\t一般使用加了正则化的最大似然作为优化的目标函数：\n$$\nw ̂={argmax} _w \\sum _i   \\log P(y^{(i)}│x^{(i) } ) - \\alpha \\sum _{j=1}^N w_j^2  \n$$\n-\t这种正则化相当于给权重的概率分布加了一个零均值高斯先验，权重越偏离均值，即权重越大，其概率越低。\n-\t为什么多分类Logistic回归是最大熵模型：最大熵模型保证在满足给定约束下，无约束的部分分类应该是等概率分配，例如在两个约束下：\n$$\nP(NN)+P(JJ)+P(NNS)+P(VB)=1 \\\\\nP(t_i=NN or t_i=NNS)=8/10 \\\\\n$$\n-\t则满足这两个约束，最大熵模型分配的概率结果为：\n$$\np(NN)=4/10  \\\\\np(JJ)=1/10  \\\\\np(NNS)=4/10  \\\\\np(VB)=1/10 \\\\\n$$\n-\t在The equivalence of logistic regression and maximum entropy models一文中证明了在广义线性回归模型的平衡条件约束下，满足最大熵分布的非线性激活函数就是sigmoid，即logistic回归。\n\n## 最大熵马尔可夫模型\n-\t最大熵模型只能对单一观测量分类，使用最大熵马尔可夫模型可以将其扩展到序列分类问题上。\n-\t最大熵马尔可夫比隐马尔科夫模型好在哪儿？隐马尔科夫模型对于每个观测量的分类依赖于转移概率和生成概率，假如我们想要在标注过程中引入外部知识，则需要将外部知识编码进这两类概率中，不方便。最大熵马尔可夫模型能够更简单的引入外部知识。\n-\t在隐马尔科夫模型中我们优化似然，并且乘以先验来估计后验：\n$$\nT ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )   \n$$\n-\t在最大熵隐马尔科夫模型中，我们直接计算后验。因为我们直接训练模型来分类，即最大熵马尔可夫模型是一类判别模型，而不是生成模型：\n$$\nT ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1}) \n$$\n-\t因此在最大熵隐马尔科夫模型中没有分别对似然和先验建模，而是通过一个单一的概率模型来估计后验。两者的区别如下图所示：\n![FoZgrF.png](https://s2.ax1x.com/2019/01/03/FoZgrF.png) \n-\t另外最大熵马尔可夫模型可以依赖的特征更多，依赖方式更灵活，如下图：\n![FoZcKU.png](https://s2.ax1x.com/2019/01/03/FoZcKU.png)\n-\t用公式表示这一差别：\n$$\nHMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\\\\nMEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\\\\n$$\n-\t当估计单一转移概率（从状态q^{\\*}转移到状态q，产生观测量o）时，我们使用以下的最大熵模型：\n$$\nP(q│q^{\\*},o)=\\frac{1}{Z(o,q^{\\*})} exp⁡(\\sum _i   w_i f_i (o,q)) \n$$\n\n## 最大熵马尔可夫模型的解码（推断）\n-\tMEMM同样使用维特比算法进行解码\n-\t使用维特比算法解码的通用框架是：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j) \n$$\n-\t在HMM模型中这一框架具体化为：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t) \n$$\n-\t在MEMM中直接将似然和先验替换为后验：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t) \n$$\n\n## 最大熵马尔可夫模型的训练\n-\tMEMM作为最大熵模型的推广，训练过程使用和最大熵模型一样的监督算法。如果训练数据的标签序列存在缺失，也可以通过EM算法进行半监督学习。\n\n\n# 第十二章：英语的形式语法\n## 组成性\n-\t英语中的单词是如何组成一个词组的呢？\n-\t换句话说，我们如何判断一些单词组合成了一个部分？一种可能是这种组合都能在相似的句法环境中出现，例如名词词组都能在一个动词之前出现。另一种可能依据来自于前置和后置结构，例如前置短语on September seventeenth可以放在句子的前面，中间或者后面，但是组合成这个短语的各个部分不能拆出来放在句子的不同位置，因此我们判断on September seventeenth这三个词组成了一个短语。\n\n## 上下文无关法则\n-\t上下文无关语法，简称CFG，又称为短语结构语法，其形式化方法等价于Backus-Naur范式。一个上下文无关语法包含两个部分：规则或者产生式，词表。\n-\t例如，用上下文无关语法描述名词词组，一种描述方式是名词词组可以由一个专有名词构成，也可以由一个限定词加一个名词性成分构成，而名词性成分可以是一个或多个名词，此CFG的规则为：\n\t-\tNP→Det Nominal\n\t-\tNP→ProperNoun\n\t-\tNominal→Noun|Noun Nominal\n-\tCFG可以层级嵌套，因此上面的规则可以与下面表示词汇事实的规则（词表）结合起来：\n\t-\tDet→a\n\t-\tDet→the\n\t-\tNoun→flight\n-\t符号分为两类：\n\t-\t终极符号：与现实中单词对应的符号，词表是引入终极符号的规则的集合\n\t-\t非终极符号：表示终极符号的聚类或者概括性符号\n-\t在每个规则里箭头右边包含一个或多个终极符号和非终极符号，箭头左边为一个非终极符号，与每个单词相关联的是其词类范畴（词类）。\n-\tCFG既可以看成是生成句子的一种机制，也可以看成是给一个句子分配结构的机制。\n-\t以之前提到的CFG为例，对一个符号串NP，可以逐步生成：\n$$\nNP→Det Nominal→Det Noun→a flight\n$$\n-\t称 a flight是NP的一个推导，一般用一个剖析树表示一种推导：\n![FoZ5P1.png](https://s2.ax1x.com/2019/01/03/FoZ5P1.png)\n一个CFG定义了一个形式语言，形式语言是符号串的集合，如果有一个语法推导出的句子处于由该语法定义的形式语言中，这个句子就是合语法的。使用形式语言来模拟自然语言的语法成为生成式语法。\n-\t上下文无关语法的正式定义：\n\t-\tN：非终止符号（或者变量）的集合\n\t-\tSigma：终止符号的集合，与N不相交\n\t-\tR：规则或者产生式的集合\n\t-\tS：指定的开始符号\n-\t一些约定定义：\n\t-\t大写字母：代表非终止符号\n\t-\tS：开始符号\n\t-\t小写希腊字母：从非终止符号和终止符号的并集中抽取出来的符号串\n\t-\t小写罗马字母：终止符号串\n-\t直接导出的定义：\n**公式待补充**\n-\t导出是直接导出的泛化。之后我们可以正式定义由语法G生成的语言L是一个由终止符号组成的字符串集合，这些终止符号可以从指定的开始符号S通过语法G导出：\n**公式待补充**\n-\t将一个单词序列映射到其对应的剖析树成为句法剖析。\n\n## 英语的一些语法规则\n-\t英语中最常用最重要的四种句子结构：\n\t-\t陈述式结构：主语名词短语加一个动词短语\n\t-\t命令式结构：通常以一个动词短语开头，并且没有主语\n\t-\tYes-no疑问式结构：通常用于提问，并且以一个助动词开头，后面紧跟一个主语NP，再跟一个VP\n\t-\tWh疑问式结构：包含一个wh短语成分\n-\t在之前的描述中开始符号用于单独生成整个句子，但是S也可以出现在语法生成规则的右边，嵌入到更大的句子当中。这样的S称为从句，拥有完整的语义。拥有完整的语义是指这个S在整体句子的语法剖析树当中，其子树当中的主要动词拥有所需的所有论元。\n\n## 名词短语\n-\t限定词Det：名词短语可以以一些简单的词法限定词开始，例如a,the,this,those,any,some等等，限定词的位置也可以被更复杂的表示替代，例如所有格。这样的表示是可以递归定义的，例如所有格加名词短语可以构成更大的名词短语的限定词。在复数名词、物质名词之前不需要加限定词。\n-\t名词性词Nominal：包含一些名词前或者名词后修饰语\n-\t名词之前，限定词之后：一些特殊的词类可以出现在名词之前限定词之后，包括基数词Card、序数词Ord、数量修饰语Quant。\n-\t形容词短语AP：形容词短语之前可以出现副词\n-\t可以讲名词短语的前修饰语规则化如下（括号内代表可选）：\n-\tNP->(Det)(Card)(Ord)(Quant)(AP)Nominal\n-\t后修饰语主要包含三种：\n\t-\t介词短语PP：Nominal->Nominal PP(PP)(PP)\n\t-\t非限定从句：动名词后修饰语GerundVP,GerundVP->GerundV NP | GerundV PP | GerundV | GerundV NP PP\n\t-\t关系从句：以关系代词开头的从句 Nominal ->Nominal RelCaluse;RelCaluse -> (who|that) VP\n\n## 一致关系\n-\t每当动词有一个名词作为它的主语时，就会发生一致关系的现象，凡是主语和他的动词不一致的句子都是不合语法的句子，例如第三人称单数动词没有加-s。可以使用多个规则的集合来扩充原有的语法，使得语法可以处理一致关系。例如yes-no疑问句的规则是\n$$\nS \\rightarrow Aux \\ NP \\ VP\n$$\n-\t可以用如下形式的两个规则来替代：\n$$\nS \\rightarrow 3sgAux \\ 3sgNP \\ VP \\\\\nS \\rightarrow Non3sgAux \\ Non3sgNP \\ VP \\\\\n$$\n-\t再分别指定第三人称单数和非第三人称单数的助动词形态。这样的方法会导致语法规模增加。\n\n## 动词短语和次范畴化\n-\t动词短语包括动词和其他一些成分的组合，包括NP和PP以及两者的组合。整个的嵌入句子也可以跟随在动词之后，成为句子补语。\n-\t动词短语的另一个潜在成分是另一个动词短语。\n-\t动词后面也可以跟随一个小品词，小品词类似于借此，但与动词组合在一起是构成一个短语动词，与动词不可分割。\n-\t次范畴化即再分类。传统语法把动词次范畴化为及物动词和不及物动词，而现代语法已经把动词区分为100个次范畴。讨论动词和可能的成分之间的关系是将动词看成一个谓词，而成分想象成这个谓词的论元(argument)。\n-\t对于动词和它的补语之间的关系，我们可以用上下文无关语法表示一致关系特征，且需要区分动词的各个次类。\n\n## 助动词\n-\t助动词是动词的一个次类，具有特殊的句法约束。助动词包括情态动词、完成时助动词、进行时助动词、被动式助动词。每一个助动词都给他后面的动词形式一个约束，且需要按照一定的顺序进行结合。\n-\t四种助动词给VP次范畴化时，VP的中心动词分别是光杆动词、过去分词形式、现在分词形式、过去分词形式。\n-\t一个句子可以用多个助动词，但是要按照情态助动词、完成时助动词、进行式助动词、被动式助动词的顺序。\n\n## 树图资料库\n-\t上下文无关语法可以将一个句子剖析成一个句法剖析树，如果一个语料中所有句子都以句法剖析树的形式表示，这样的句法标注了的语料就称为树图资料库(treebank)。\n-\t树图资料库中的句子隐含的组成了一种语言的语法，我们可以对于每一棵句法剖析树提取其中的CFG规则。从宾州树库中提取出来的CFG规则非常扁平化，使得规则数量很多且规则很长。\n-\t在树库中搜索需要一种特殊的表达式，能够表示关于节点和连接的约束，用来搜索特定的模式。例如tgrep或者TGrep2。\n-\t在tgrep、TGrep2中的一个模式由一个关于节点的描述组成，一个节点描述可以用来返回一个以此节点为根的子树。\n-\t可以使用双斜线对某一类模式命名：\n$$\n/NNS?/\tNN|NNS\n$$\n-\tTgrep/Tgrep2模式的好处在于能够描述连接的信息。小于号代表直接支配，远小于符号代表支配，小数点代表线性次序。这种对于连接的描述反应在剖析树中的关系如下：\n![FoZ2b4.png](https://s2.ax1x.com/2019/01/03/FoZ2b4.png)\n \n## 中心词和中心词查找\n-\t句法成分能够与一个词法中心词相关联。在一个简单的词法中心词模型中，每一个上下文无关规则与一个中心词相关联，中心词传递给剖析树，因此剖析树中每一个非终止符号都被一个单一单词所标注，这个单一单词就是这个非终止符号的中心词。一个例子如下：\n![FoZfa9.png](https://s2.ax1x.com/2019/01/03/FoZfa9.png)\n-\t为了生成这样一棵树，每一个CFG规则都必须扩充来识别一个右手方向的组成成分来作为中心词子女节点。一个节点的中心词词被设置为其子女中心词的中心词。\n-\t另一种方式是通过一个计算系统来完成中心词查找。在这种方式下是依据树的上下文来寻找指定的句子，从而动态的识别中心词。一旦一个句子被解析出来，树将会被遍历一遍并使用合适的中心词来装饰每一个节点。\n\n## 语法等价与范式\n-\t语法等价包括两种：强等价，即两个语法生成相同的符号串集合，且他们对于每个句子都指派同样的短语结构；弱等价，即两个语法生成相同的符号串集合，但是不给每个句子指派相同的短语结构。\n-\t语法都使用一个范式，在范式中每个产生式都使用一个特定的形式。例如一个上下文五官与法是sigma自由的，并且如果他们的每个产生式的形式为A->BC或者是A->a，就说明这个上下文无关语法是符合Chomsky范式的，简称CNF。凡是Chomsky范式的语法都具有二叉树形式。任何上下文无关语法都可以转变成一个弱等价的Chomsky范式语法。\n-\t使用二叉树形式的剖析树能够产生更小的语法。形如A->A B的规则称为Chomsky并连。\n\n## 有限状态语法和上下文无关语法\n-\t复杂的语法模型必须表示组成性，因而不适合用有限状态模型来描述语法。\n-\t当一个非终止符号的展开式中也包含了这个非终止符号时，就会产生语法的递归问题。\n-\t例如，使用正则表达式来描述以Nominal为中心的名词短语：\n(Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*\n-\t为了完成这个正则表达式，只需要按顺序展开PP，展开结果为(P NP)*，这样就出现了地柜问题，因为此时出现了NP，在NP的正则表达式中出现了NP。\n-\t一个上下文无关语法能够被有限自动机生成，当且仅当存在一个生成语言L的没有任何中心自嵌入递归的上下文无关语法。\n\n## 依存语法\n-\t依存语法与上下文无关语法相对，其句法结构完全由词、词与词之间的语义或句法关系描述。一个例子如下：\n![FoZOVH.png](https://s2.ax1x.com/2019/01/03/FoZOVH.png)\n-\t其中没有非终止符号或者短语节点，树中的连接只将两个词语相连。连接即依存关系，代表着语法功能或者一般的语义联系，例如句法主语、直接对象、间接宾语、时间状语等等。\n-\t依存语法具有很强的预测剖析能力，且在处理具有相对自由词序的语言时表现更好。\n\n# 第十三章：基于上下文无关语法的剖析\n## 剖析即搜索\n\n-\t在句法剖析中，剖析可以看成对一个句子搜索一切可能的剖析树空间并发现正确的剖析树。\n-\t对于某一个句子（输入符号串），剖析搜索的目标是发现以初始符号S为根并且恰好覆盖整个输入符号串的一切剖析树。搜索算法的约束来自两方面：\n\t-\t来自数据的约束，即输入句子本身，搜索出来的剖析树的叶子应该是原句的所有单词。\n\t-\t来自语法的约束，搜索出来的剖析树应该有一个根，即初始符号S\n-\t根据这两种约束，产生了两种搜索策略：自顶向下，目标制导的搜索；自下而上，数据制导的搜索。\n-\t对于自顶向下的搜索，从根开始，我们通过生成式不断生成下一层的所有可能子节点，搜索每一层的每一种可能，如下图（对于句子book that flight）：\n![FoZh5R.png](https://s2.ax1x.com/2019/01/03/FoZh5R.png)\n-\t对于自底向上的搜索，剖析从输入的单词开始，每次都使用语法中的规则，试图从底部的单词向上构造剖析树，如果剖析树成功的构造了以初始符号S为根的树，而且这个树覆盖了整个输入，那么就剖析成功。首先通过词表将每个单词连接到对应的词类，如果一个单词有不止一个词类，就需要考虑所有可能。与自顶向下相反，每次进入下一层时，自底向上需要考虑被剖析的成分是否与某个规则的右手边相匹配，而自顶向下是与左手边相匹配。中途如果无法匹配到规则则将这个树枝从搜索空间中删除，如下图所示：\n![FoZI8x.png](https://s2.ax1x.com/2019/01/03/FoZI8x.png) \n-\t两者对比：\n\t-\t自顶向下是从S开始搜索的，因此不会搜索那些在以S为根的树中找不到位置的子树，而自底向上会产生许多不可能的搜索树\n\t-\t相对应的，自顶向下把搜索浪费在了不可能产生输入单词序列的树上\n\t-\t综上，我们需要将自顶向下和自底向上相结合\n\n## 歧义\n-\t在句法剖析中需要解决的一个问题是结构歧义，即语法会给一个句子多种剖析结果可能。\n-\t最常见的两种歧义：附着歧义和并列连接歧义。\n-\t如果一个特定的成分可以附着在剖析树的一个以上的位置，句子就会出现附着歧义。例如We saw the Eiffel Tower flying to Paris一句中,flying to Paris可以修饰Eiffel Tower也可以修饰We。\n-\t在并列连接歧义中，存在着不同的短语，这些短语之间用and这样的连接词相连。例如old men and women可以是老年男性和老年女性，或者老年男性和普通女性，即old是否同时分配到men和women上。\n-\t以上两种歧义还能相互组合嵌套形成更复杂的歧义。假如我们不消歧，仅仅返回所有的可能，留给用户或者人工判断，则随着剖析句子结构变复杂或者剖析规则的增加，得到的可能是成指数级增长的，具体而言，这种剖析句子可能的增长数和算术表达式插入括号问题相同，以Catalan数按指数增长：\n$$\nC(n)=\\frac{1}{1+n} C_{2n}^n\n$$\n-\t摆脱这种指数爆炸的方法有两个：\n\t-\t动态规划，研究搜索空间的规律性，使得常见的部分只推导一次，减少与歧义相关的开销\n\t-\t使用试探性的方法来改善剖析器的搜索策略\n-\t使用例如深度优先搜索或者宽度优先搜索之类的有计划与回溯的搜索算法是在复杂搜索空间中搜索常用的算法，然而在复杂语法空间中无处不在的歧义使得这一类搜索算法效率低下，因为有许多重复的搜索过程。\n\n## 动态规划剖析方法\n-\t在动态规划中，我们维护一个表，系统的将对于子问题的解填入表中，利用已经存储的子问题的解解决更大的子问题，而不用重复从头开始计算。\n-\t在剖析中，这样的表用来存储输入中各个部分的子树，当子树被发现时就存入表中，以便以后调用，就这样解决了重复剖析的问题（只需查找子树而不需要重新剖析）和歧义问题（剖析表隐含的存储着所有可能的剖析结果）。\n-\t主要的三种动态规划剖析方法有三种，CKY算法、Earley算法和表剖析算法。\n\n### CKY剖析\n-\tCKY剖析要求语法必须满足Chomsky范式，即生成式右边要么时两个非终止符号要么是一个终止符号。如果不是Chomsky范式，则需要把一个一般的CFG转换成CNF：\n\t-\t右边有终止符号也有非终止符号：给右边的终止符号单独建一个非终止符号，例如：INF-VP → to VP，改成INF-VP → TO VP和TO → to\n\t-\t右边只有一个非终止符号：这种非终止符号称为单元产物，它们最终会生成非单元产物，用最终生成的非单元产物规则来替换掉单元产物\n\t-\t右边不止2个符号：引入新的非终止符号将规则分解\n\t-\t词法规则保持不变，但是在转换的过程中可能会生成新的词法规则\n-\t当所有的规则都转换成CNF之后，表中的非终止符号在剖析中有两个子节点，且表中每一个入口代表了输入中的某个区间，对于某个入口例如[0,3]，其可以被拆分成两部分，假如一部分为[0,2]，则另一部分为[2,3]，前者在[0,3]的左边，后者在[0,3]的正下方，如下图：\n![FoZo26.png](https://s2.ax1x.com/2019/01/03/FoZo26.png)\n-\t接下来就是如何填表，我们通过自底向上的方法来剖析，对于每个入口[i,j]，包含了输入中i到j这一区间部分的表格单元都会对这个入口值做出贡献，即入口[i,j]左边的单元和下边的单元。下表中的CKY伪算法图描述了这一过程：\n![FoZjIA.png](https://s2.ax1x.com/2019/01/03/FoZjIA.png)\n-\t外层循环从左往右循环列，内层循环从下往上循环行，而最里面的循环式遍历串[i,j]的所有可能二分子串，表中存的是可以代表[i,j]区间符号串的非终止符号集合，因为是集合，所以不会出现重复的非终止符号。\n-\t现在我们完成了识别任务，接下来是剖析。剖析即在[0,N]入口，对应整个句子，找到一个非终止符号作为起始符号S。首先我们要对算法做两点更改：\n\t-\t存入表中的不仅仅是非终止符号，还有其对应的指针，指向生成这个非终止符号的表入口\n\t-\t允许一个入口中存在同一个非终止符号的不同版本\n-\t做了这些改动之后，这张表就包含了一个给定输入的所有可能剖析信息。我们可以选择[0,N]入口中任意一个非终止符号作为起始符号S，然后根据指针迭代提取出剖析信息。\n-\t当然，返回所有的可能剖析会遇到指数爆炸的问题，因此我们在完整的表上应用维特比算法，计算概率最大的剖析并返回这个剖析结果。\n\n### Early算法\n-\t相比CKY自底向上的剖析，Early算法采用了自顶向下的剖析，而且只用了一维的表保存状态，每个状态包含三类信息：\n\t-\t对应某一单一语法规则的子树\n\t-\t子树的完成状态\n\t-\t子树对应于输入中的位置\n-\t算法流程图如下：\n![FoZHKO.png](https://s2.ax1x.com/2019/01/03/FoZHKO.png)\n-\t算法对于状态的操作有三种：\n\t-\t预测：造出一个新的状态来表示在剖析过程中生成的自顶向下的预测。当待剖析的状态为非终极符号但又不是词类范畴时，对于这个非终极符号的不同展开，预测操作都造出一个新的状态。\n\t-\t扫描：当待剖析的状态是词类范畴时，就检查输入符号串，并把对应于所预测的词类范畴的状态加入线图中。\n\t-\t完成：当右边所有状态剖析完成时，完成操作查找输入中在这个位置的语法范畴，发现并推进前面造出的所有状态。\n\n### 表剖析\n-\t表剖析允许动态的决定表格处理的顺序，算法动态的依照计划依次删除图中的一条边，而计划中的元素排序是由规则决定的。\n![FoZTxK.png](https://s2.ax1x.com/2019/01/03/FoZTxK.png)\n\n## 部分剖析\n-\t有时我们只需要输入句子的部分剖析信息\n-\t可以用有限状态自动机级联的方式完成部分剖析，这样会产生比之前提到的方法更加“平”的剖析树。\n-\t另一种有效的部分剖析的方法是分块。使用最广泛覆盖的语法给句子做词类标注，将其分为有主要词类标注信息且不没有递归结构的子块，子块之间不重叠，就是分块。\n-\t我们用中括号将每一个分块框起来，有可能一些词并没有被框住，属于分块之外。\n-\t分块中最重要的是基本分块中不能递归包含相同类型的成分。\n\n### 基于规则的有限状态分块\n-\t利用有限状态方式分块，需要为了特定目的手动构造规则，之后从左到右，找到最长匹配分块，并接着依次分块下去。这是一个贪心的分块过程，不保证全局最优解。\n-\t这些分块规则的主要限制是不能包含递归。\n-\t使用有限状态分块的优点在于可以利用之前转录机的输出作为输入来组成级联，在部分剖析中，这种方法能够有效近似真正的上下文无关剖析器。\n\n### 基于机器学习的分块\n-\t分块可以看成序列分类任务，每个位置分类为1（分块）或者0（不分块）。用于训练序列分类器的机器学习方法都能应用于分块中。\n-\t一种卓有成效的方法是将分块看成类似于词类标注的序列标注任务，用一个小的标注符号集同时编码分块信息和每一个块的标注信息，这种方式称为IOB标注，用B表示分块开始，I表示块内，O表示块外。其中B和I接了后缀，代表该块的句法信息。\n-\t机器学习需要训练数据，而分块的已标数据很难获得，一种方法是使用已有的树图资料库，例如宾州树库。\n\n### 评价分块系统\n-\t准确率：模型给出的正确分块数/模型给出的总分块数\n-\t召回率：模型给出的正确分块数/文本中总的正确分块数\n-\tF1值：准确率和召回率的调和平均\n\n\n\n# 第十四章：统计剖析\n## 概率上下文无关语法\n-\t概率上下文无关语法PCFG是上下文无关语法的一种简单扩展，又称随机上下文无关语法。PCFG在定义上做出了一点改变：\n\t-\tN：非终止符号集合\n\t-\tΣ：终止符号集合\n\t-\tR：规则集合，与上下文无关语法相同，只不过多了一个概率p，代表某一项规则执行的条件概率$P(\\beta|A)$\n\t-\tS：一个指定的开始符号\n-\t当某个语言中所有句子的概率和为1时，我们称这个PCFG时一致的。一些递归规则可能导致PCFG不一致。\n\n## 用于消歧的PCFG\n-\t对于一个给定句子，其某一特定剖析的概率是所有规则概率的乘积，这个乘积既是一个剖析的概率，也是剖析和句子的联合概率。这样，对于出现剖析歧义的句子，其不同剖析的概率不同，通过选择概率大的剖析可以消歧。\n\n## 用于语言建模的PCFG\n-\tPCFG为一个句子分配了一个概率（即剖析的概率），因此可以用于语言建模。相比n元语法模型，PCFG在计算生成每一个词的条件概率时考虑了整个句子，效果更好。对于含歧义的句子，其概率是所有可能剖析的概率之和。\n\n## PCFG的概率CKY剖析\n-\tPCFG的概率剖析问题：为一个句子产生概率最大的剖析\n-\t概率CKY算法扩展了CKY算法，CKY剖析树中的每一个部分被编码进一个$(n+1)\\*(n+1)$的矩阵（只用上三角部分），矩阵中每一个元素包含一个非终止符号集合上的概率分布，可以看成每一个元素也是V维，因此整个存储空间为$(n+1)\\*(n+1)\\*V$，其中[i,j,A]代表非终止符号A可以用来表示句子的i位置到j位置这一段的概率。\n-\t算法伪代码：\n![FoZbrD.png](https://s2.ax1x.com/2019/01/03/FoZbrD.png)\n-\t可以看到也是用k对某一区间[i,j]做分割遍历，取最大的概率组合作为该区间的概率，并向右扩展区间进行动态规划。\n\n## 学习到PCFG的规则概率\n-\t上面的伪算法图用到了每一个规则的概率。如何获取这个概率？两种方法，第一种朴素的方法是在一个已知的树库数据集上用古典概型统计出概率：\n$$\nP(\\alpha \\rightarrow \\beta | \\alpha) = \\frac{Count(\\alpha \\rightarrow \\beta)}{\\sum _{\\gamma} Count(\\alpha \\rightarrow \\gamma)}\n$$\n-\t假如我们没有树库，则可以用非概率剖析算法来剖析一个数据集，再统计出概率。但是非概率剖析算法在剖析歧义句子时，需要对每一种可能剖析计算概率，但是计算概率需要概率剖析算法，这样就陷入了鸡生蛋蛋生鸡的死循环。一种解决方案是先用等概率的剖析算法，剖析句子，得出每一种剖析得概率，然后用概率加权统计量，然后重新估计剖析规则的概率，继续剖析，反复迭代直到收敛。这种算法称为inside-outside算法，是前向后向算法的扩展，同样也是EM算法的一种特例。\n\n## PCFG的问题\n-\t独立性假设导致不能很好的建模剖析树的结构性依存：每个PCFG规则被假定为与其他规则独立，例如，统计结果表明代词比名词更有可能称为主语，因此当NP被展开时，如果NP是主语，则展开为代词的可能性较高——这里需要考虑NP在句子种的位置，然而这种概率依存关系是PCFG所不允许的，\n-\t缺乏对特定单词的敏感，导致次范畴化歧义、介词附着、联合结构歧义的问题：例如在介词附着问题中，某一个介词短语into Afghanistan附着于哪一个部分，在PCFG中计算时被抽象化为介词短语应该附着一个哪一个部分，而抽象化的概率来自于对语料的统计，这种统计不会考虑特定的单词。又例如联合结构歧义，假如一个句子的两种可能剖析树使用了相同的规则，而规则在树中的位置不同，则PCFG对两种剖析计算出相同的概率：因为PCFG假定规则之间是独立的，联合概率是各个概率的乘积。\n\n## 通过拆分和合并非终止符号来改进PCFG\n-\t先解决结构性依存的问题。之前提到了我们希望NP作为主语和宾语时有不同概率的规则，一种想法就是将NP拆分成主语NP和宾语NP。实现这种拆分的方法是父节点标注，及每个节点标注了其父节点，对于主语NP其父节点是S，对于宾语NP，其父节点是VP，因此不同的NP就得到了区分。除此之外，还可以通过词性拆分的方式增强剖析树。\n-\t拆分会导致规则增多，用来训练每一条规则的数据变少，引起过拟合。因此要通过一个手写规则或者自动算法来根据每个训练集合并一些拆分。\n\n## 概率词汇化的CFG\n-\t概率CKY剖析更改了语法规则，而概率词汇化模型更改了概率模型本身。对于每一条规则，不仅要产生成分的规则变化，还要在每个成分上标注其中心词和词性，如下图：\n![FoeSRP.png](https://s2.ax1x.com/2019/01/03/FoeSRP.png)\n-\t为了产生这样的剖析树，每一条PCFG规则右侧需要选择一个成分作为中心词子节点，用子节点的中心词和词性作为该节点的中心词和词性。\n其中，规则被分成了两类，内部规则和词法规则，后者是确定的，前者是需要我们估计的：\n![FoZqqe.png](https://s2.ax1x.com/2019/01/03/FoZqqe.png)\n-\t我们可以用类似父节点标注的思想来拆分规则，拆分后每一部分都对应一种可能的中心词选择。假如我们将概率词汇话的CFG看成一个大的有很多规则CFG，则可以用之前的古典概型来估计概率。但是这样的效果不会很好，因为这样的规则划分太细了，没有足够的数据来估计概率。因此我们需要做出一些独立性假设，将概率分解为更小的概率乘积，这些更小的概率能容易从语料中估计出来。\n-\t不同的统计剖析器区别在于做出怎样的独立性假设。\n-\tCollins剖析如下图所示：\n![FoZzGt.png](https://s2.ax1x.com/2019/01/03/FoZzGt.png)\n-\t其概率拆解为：\n$$\nP(VP(dumped,VBD)→VBD(dumped,VBD)NP(sacks,NNS)PP(into,P))= \\\\\nP_H (VBD│VP,dumped)\\* \\\\\nP_L (STOP│VP,VBD,dumped)\\* \\\\\nP_R (NP(sacks,NNS)│VP,VBD,dumped)\\* \\\\\nP_R (PP(into,P)│VP,VBD,dumped)\\* \\\\\nP_R (STOP|VP,VBD,dumped) \\\\\n$$\n-\t给出生成式左边之后，首先生成规则的中心词，之后一个一个从里到外生成中心词的依赖。先从中心词左侧一直生成直到遇到STOP符号，之后生成右边。如上式做出概率拆分之后，每一个概率都很容易从较小的数据量中统计出来。完整的Collins剖析器更为复杂，还考虑了词的距离关系、平滑技术、未知词等等。\n\n## 评价剖析器\n-\t剖析器评价的标准方法叫做PARSEVAL测度，对于每一个句子s：\n\t-\t标记召回率=(Count(s的候选剖析中正确成分数）)/(Count(s的树库中正确成分数）)\n\t-\t标记准确率=(Count(s的候选剖析中正确成分数）)/(Count(s的候选剖析中全部成分数）)\n\n## 判别式重排序\n-\tPCFG剖析和Collins词法剖析都属于生成式剖析器。生成式模型的缺点在于很难引入任意信息，即很难加入对某一个PCFG规则局部不相关的特征。例如剖析树倾向于右生成这一特征就不方便加入生成式模型当中。\n-\t对于句法剖析，有两类判别式模型，基于动态规划的和基于判别式重排序的。\n-\t判别式重排包含两个阶段，第一个阶段我们用一般的统计剖析器产生前N个最可能的剖析及其对应的概率序列。第二个阶段我们引入一个分类器，将一系列句子以及每个句子的前N个剖析-概率对作为输入，抽取一些特征的大集合并针对每一个句子选择最好的剖析。特征包括：剖析概率、剖析树中的CFG规则、平行并列结构的数量、每个成分的大小、树右生成的程度、相邻非终止符号的二元语法、树的不同部分出现的次数等等。\n\n## 基于剖析的语言建模\n-\t使用统计剖析器来进行语言建模的最简单方式就是利用之前提到的二阶段算法。第一阶段我们运行一个普通的语音识别解码器或者机器翻译解码器（基于普通的N元语法），产生N个最好的候选；第二阶段，我们运行统计剖析器并为每一个候选句分配一个概率，选择概率最佳的。\n\n## 人类剖析\n-\t人类在识别句子时也用到了类似的概率剖析思想，两个例子：\n\t-\t对于出现频率高的二元语法，人们阅读这个二元语法所花的时间就更少\n\t-\t一些实验表明人类在消歧时倾向于选择统计概率大的剖析\n\n\n\n","source":"_posts/coling.md","raw":"---\ntitle: 计算语言学笔记\ndate: 2018-11-16 10:15:34\ntags:\n  - math\n  - machinelearning\n  -\ttheory\n  -\tnlp\ncategories:\n  - 机器学习\nauthor: Thinkwee\nmathjax: true\nhtml: true\n---\n\n计算语言学课程笔记\n参考教材：Speech and Language Processing：An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition\n一些公式待修订\n<!--more--> \n\n# 第二章：正则表达式与自动机\n-\t正则表达式：一种用于查找符合特定模式的子串或者用于以标准形式定义语言的工具，本章主要讨论其用于查找子串的功能。正则表达式用代数的形式来表示一些字符串集合。\n-\t正则表达式接收一个模式，然后在整个语料中查找符合这个模式的子串，这个功能可以通过设计有限状态自动机实现。\n-\t字符串看成符号的序列，所有的字符，数字，空格，制表符，标点和空格均看成符号。\n\n## 基本正则表达式模式\n-\t用双斜线表示正则表达式开始和结束（perl中的形式）\n\t-\t查找子串，大小写敏感：/woodchuck/-> woodchuck\n\t-\t用方括号代表取其中一个，或：/[Ww]oodchuck/->woodchuck or Woodchuck\n\t-\t方括号加减号，范围内取或：/[2-5]/->/[2345]\n\t-\t插入符号放在左方括号后，代表模式中不出现后接的所有符号，取非: /^Ss/ ->既不是大写S也不是小写s\n\t-\t问号代表之前的符号出现一个或不出现：/colou?r/->color or colour\n\t-\t星号代表之前的符号出现多个或不出现：/ba*/->b or ba or baa or baaa......\n\t-\t加号代表之前的符号出现至少一次：/ba+/->ba or baa or baaa.......\n\t-\t小数点代表通配符，与任何除了回车符之外的符号匹配：/beg.n/->begin or begun or beg’n or .......\n\t-\t锚符号，用来表示特定位置的子串，插入符号代表行首，美元符号代表行尾，\\b代表单词分界线，\\B代表单词非分界线，perl将单词的定义为数字、下划线、字母的序列，不在其中的符号便可以作为单词的分界。\n\n## 析取、组合和优先\n-\t用竖线代表析取，字符串之间的或：/cat|dog/->cat or dog\n-\t用圆括号代表部分析取（组合），圆括号内也可以用基本算符：/gupp(y|ies)/->guppy or guppies\n-\t优先级：圆括号>计数符>序列与锚>析取符\n\n## 高级算符\n-\t\\d：任何数字\n-\t\\D：任何非数字字符\n-\t\\w：任何字母、数字、空格\n-\t\\W：与\\w相反\n-\t\\s：空白区域\n-\t\\S：与\\s相反\n-\t{n}：前面的模式出现n个\n-\t{n,m}：前面的模式出现n到m个\n-\t{n,}：前面的模式至少出现n个\n-\t\\n：换行\n-\t\\t：表格符\n\n## 替换、寄存器\n-\t替换s/A/B/：A替换成B\n-\ts/(A)/<\\1>/：用数字算符\\1指代A，在A的两边加上尖括号\n-\t在查找中也可以用数字算符，指代圆括号内内容，可以多个算符指代多个圆括号内内容\n-\t这里数字算符起到了寄存器的作用\n\n## 有限状态自动机\n-\t有限状态自动机和正则表达式彼此对称，正则表达式是刻画正则语言的一种方法。正则表达式、正则语法和自动状态机都是表达正则语言的形式。FSA用有向图表示，圆圈或点代表状态，箭头或者弧代表状态转移，用双圈表示最终状态，如下图表示识别/baa+!/的状态机图： \n![FoVj3V.png](https://s2.ax1x.com/2019/01/03/FoVj3V.png)\n-\t状态机从初始状态出发，依次读入符号，若满足条件，则进行状态转移，若读入的符号序列满足模式，则状态机可以到达最终状态；若符号序列不满足模式，或者自动机在某个非最终状态卡住，则称自动机拒绝了此次输入。\n-\t另一种表示方式是状态转移表：\n![FoVqNn.png](https://s2.ax1x.com/2019/01/03/FoVqNn.png)\n-\t一个有限自动机可以用5个参数定义：\n\t-\t$Q$：状态{q_i}的有限集合\n\t-\t\\sum ：有限的输入符号字母表\n\t-\t$q_0$：初始状态\n\t-\t$F$：终极状态集合\n\t-\t$\\delta (q,i)$：状态之间的转移函数或者转移矩阵，是从$Q × \\Sigma$到$2^Q$的一个关系\n-\t以上描述的自动机是确定性的，即DFSA，在已知的记录在状态转移表上的状态时，根据查表自动机总能知道如何进行状态转移。算法如下，给定输入和自动机模型，算法确定输入是否被状态机接受：\n![FoZpB4.png](https://s2.ax1x.com/2019/01/03/FoZpB4.png)\n-\t当出现了表中没有的状态时自动机就会出错，可以添加一个失败状态处理这些情况。\n\n## 形式语言\n-\t形式语言是一个模型，能且只能生成和识别一些满足形式语言定义的某一语言的符号串。形式语言是一种特殊的正则语言。通常使用形式语言来模拟自然语言的某些部分。以上例/baa+!/为例，设对应的自动机模型为m，输入符号表$\\Sigma = {a,b,!}$，$L(m)$代表由m刻画的形式语言，是一个无限集合${baa!,baaa!,baaaa!,…}$\n\n## 非确定有限自动机\n-\t非确定的有限自动机NFSA,把之前的例子稍微改动，自返圈移动到状态2，就形成了NFSA，因为此时在状态2，输入a，有两种转移可选，自动机无法确定转移路径：\n![FoVLhq.png](https://s2.ax1x.com/2019/01/03/FoVLhq.png)\n-\t另一种NFSA的形式是引入$\\epsilon$转移，即不需要输入符号也可以通过此$\\epsilon$转移进行转移，如下图，在状态3时依然不确定如何进行转移：\n![FoVX90.png](https://s2.ax1x.com/2019/01/03/FoVX90.png)\n-\t在NFSA时，面临转移选择时自动机可能做出错误的选择，此时存在三种解决方法：\n\t-\t回退：标记此时状态，当确定发生错误选择之后，回退到此状态\n\t-\t前瞻：在输入中向前看，帮助判定进行选择\n\t-\t并行：并行的进行所有可能的转移\n-\t在自动机中，采用回退算法时需要标记的状态称为搜索状态，包括两部分：状态节点和输入位置。对于NFSA，其状态转移表也有相应改变，如图，添加了代表$\\epsilon$转移的$\\epsilon$列，且转移可以转移到多个状态：\n![FoZE36.png](https://s2.ax1x.com/2019/01/03/FoZE36.png)\n-\t采用回退策略的非确定自动机算法如下，是一种搜索算法： \n![FoZSuF.png](https://s2.ax1x.com/2019/01/03/FoZSuF.png)\n-\t子函数GENERATE-NEW-STATES接受一个搜索状态，提取出状态节点和输入位置，查找这个状态节点上的所有状态转移可能，生成一个搜索状态列表作为返回值；\n-\t子函数ACCEPT-STATE接受一个搜索状态，判断是否接受，接受时的搜索状态应该是最终状态和输入结束位置的二元组。\n-\t算法使用进程表（agenda）记录所有的搜索状态，初始只包括初始的搜索状态，即自动机初始状态节点和输入起始。之后不断循环，从进程表中调出搜索状态，先调用ACCEPT-STATE判断是否搜索成功，之后再调用GENERATE-NEW-STATES生成新的搜索状态加入进程表。循环直到搜索成功或者进程表为空（所有可能转移均尝试且未成功）返回拒绝。\n-\t可以注意到NFSA算法就是一种状态空间搜索，可以通过改变搜索状态的顺序提升搜索效率，例如用栈实现进程表，进行深度优先搜索DFS；或者使用队列实现进程表，进行宽度优先搜索BFS。\n-\t对于任何NFSA，存在一个完全等价的DFSA。\n\n## 正则语言和NFSA\n-\t定义字母表\\sum 为所有输入符号集合；空符号串$\\epsilon$，空符号串不包含再字母表中；空集∅。在\\sum 上的正则语言的类（或者正则集）可以形式的定义如下：\n\t-\t∅是正则语言\n\t-\t∀a ∈ $\\sum$ ∪$\\epsilon$,{a}是形式语言\n\t-\t如果$L_1$和$L_2$是正则语言，那么：\n\t-\t$L_1$和$L_2$的拼接是正则语言\n\t-\t$L_1$和$L_2$的合取、析取也是正则语言\n\t-\t$L_1$^*，即$L_1$的Kleene闭包也是正则语言\n-\t可见正则语言的三种基本算符：拼接、合取及析取、Kleene闭包。任何正则表达式可以写成只使用这三种基本算符的形式。\n-\t正则语言对以下运算也封闭（$L_1$和$L_2$均为正则语言）：\n\t-\t交：$L_1$和$L_2$的符号串集合的交构成的语言也是正则语言\n\t-\t差：$L_1$和$L_2$的符号串集合的差构成的语言也是正则语言\n\t-\t补：不在$L_1$的符号串集合中的集合构成的语言也是正则语言\n\t-\t逆：$L_1$所有符号串的逆构成的集合构成的语言也是正则语言\n-\t可以证明正则表达式和自动机等价，一个证明任何正则表达式可以建立对应的自动机的方法是，根据正则语言的定义，构造基础自动机代表$\\epsilon$、∅以及$\\sum$中的单个符号a，然后将三种基本算符表示为自动机上的操作，归纳性的，在基础自动机上应用这些操作，得到新的基础自动机，这样就可以构造满足任何正则表达式的自动机，如下图：\n![FoVxjU.png](https://s2.ax1x.com/2019/01/03/FoVxjU.png)\n基础自动机\n![FoZPE9.png](https://s2.ax1x.com/2019/01/03/FoZPE9.png) \n拼接算符\n![FoZ9HJ.png](https://s2.ax1x.com/2019/01/03/FoZ9HJ.png)\nKleene闭包算符\n![FoZiNR.png](https://s2.ax1x.com/2019/01/03/FoZiNR.png)\n合取析取算符\n\n# 第三章：形态学与有限状态转录机\n-\t剖析：取一个输入并产生关于这个输入的各类结构\n\n## 英语形态学概论\n-\t形态学研究词的构成，词可以进一步拆解为语素，语素可分为词干和词缀，词缀可分为前缀、中缀、后缀、位缀。\n-\t屈折形态学：英语中，名词只包括两种屈折变化：一个词缀表示复数，一个词缀表示领属：\n\t-\t复数：-s，-es，不规则复数形式\n\t-\t领属：-‘s，-s’\n-\t动词的屈折变化包括规则动词和非规则动词的变化：\n\t-\t规则动词：主要动词和基础动词，-s，-ing，-ed，\n\t-\t非规则动词\n-\t派生形态学：派生将词干和一个语法语素结合起来，形成新的单词\n\t-\t名词化：-ation，-ee，-er，-ness\n\t-\t派生出形容词：-al，-able，-less\n\n## 形态剖析\n-\t例子：我们希望建立一个形态剖析器，输入单词，输出其词干和有关的形态特征，如下表，我们的目标是产生第二列和第四列：\n![FoZA9x.png](https://s2.ax1x.com/2019/01/03/FoZA9x.png)\n-\t我们至少需要：\n\t-\t词表（lexicon）：词干和词缀表及其基本信息\n\t-\t形态顺序规则（morphotactics）：什么样的语素跟在什么样的语素之后\n\t-\t正词法规则（orthographic rule）：语素结合时拼写规则的变化\n-\t一般不直接构造词表，而是根据形态顺序规则，设计FSA对词干进行屈折变化生成词语。例如一个名词复数化的简单自动机如下图：\n![FoZmuD.png](https://s2.ax1x.com/2019/01/03/FoZmuD.png)\n-\t其中reg-noun代表规则名词，可以通过加s形成复数形式，并且忽略了非规则单数名词(irreg-sg-noun)和非规则复数名词(irreg-pl-noun)。另外一个模拟动词屈折变化的自动机如下图：\n![FoZQUA.png](https://s2.ax1x.com/2019/01/03/FoZQUA.png)\n-\t使用FSA解决形态识别问题（判断输入符号串是否合法）的一种方法是，将状态转移细分到字母层次，但是这样仍然会存在一些问题：\n![FoZZjO.png](https://s2.ax1x.com/2019/01/03/FoZZjO.png)\n\n## 有限状态转录机\n-\t双层形态学：将一个词表示为词汇层和表层，词汇层表示该词语素之间的简单毗连（拼接，concatenation），表层表示单词实际最终的拼写，有限状态转录机是一种有限状态自动机，但其实现的是转录，实现词汇层和表层之间的对应，它有两个输入，产生和识别字符串对，每一个状态转移的弧上有两个标签，代表两个输入。\n![FoZVgK.png](https://s2.ax1x.com/2019/01/03/FoZVgK.png)\n-\t从四个途径看待FST：\n\t-\t作为识别器：FST接受一对字符串，作为输入，如果这对字符串在语言的字符串对中则输出接受否则拒绝\n\t-\t作为生成器：生成语言的字符串对\n\t-\t作为翻译器：读入一个字符串，输出另一个\n\t-\t作为关联器：计算两个集合之间的关系\n-\t定义有限状态转录机：\n\t-\tQ：状态{q_i}的有限集合\n\t-\t\\sum ：有限的输入符号字母表\n\t-\t∆：有限的输出符号字母表\n\t-\t$q_0 \\in Q$：初始状态\n\t-\t$F⊆Q$：终极状态集合\n\t-\t$\\delta (q,w)$：状态之间的转移函数或者转移矩阵，是从Q×\\sum 到2^Q的一个关系，q是状态，w是字符串，返回新状态集合\n\t-\t$\\sigma (q,w)$：输出函数，给定每一个状态和输入，返回可能输出字符串的集合，是从$Q × \\Sigma$到$2^∆$的一个关系\n-\t在FST中，字母表的元素不是单个符号，而是符号对，称为可行偶对。类比于FSA和正则语言，FST和正则关系同构，对于并运算封闭，一般对于差、补、交运算不封闭。\n-\t此外，FST，\n\t-\t关于逆反（逆的逆）闭包，逆反用于方便的实现作为剖析器的FST到作为生成器的FST的转换\n\t-\t关于组合（嵌套）闭包，用于将多个转录机用一个更复杂的转录机替换。\n-\t转录机一般是非确定性的，如果用FSA的搜索算法会很慢，如果用非确定性到确定性的转换算法，则有些FST本身是不可以被转换为为确定的。\n-\t顺序转录机是一种输入确定的转录机，每个状态转移在给定状态和输入之后是确定的，不像上图中的FST，状态0在输入b时有两种状态转移（转移到相同的状态，但是输出不同）。顺序转录机可以使用$\\epsilon$符号，但是只能加在输出字符串上，不能加在输入字符串上，如下图： \n![FoZuHH.png](https://s2.ax1x.com/2019/01/03/FoZuHH.png)\n-\t顺序转录机输出不一定是序列的，即从同一状态发出的不同转移可能产生相同输出，因此顺序转录机的逆不一定是顺序转录机，所以在定义顺序转录机时需要定义方向，且转移函数和输出函数需要稍微修改，输出空间缩小为Q和∆。\n-\t顺序转录机的一种泛化形式是并发转录机，其在最终状态额外输出一个字符串，拼接到已经输出的字符串之后。顺序和并发转录机的效率高，且有有效的算法对其进行确定化和最小化，因此很重要。P并发转录机在此基础上可以解决歧义问题。\n\n## 用有限状态转录机进行形态剖析\n-\t将单词看成词汇层和表层之间的关系，如下图： \n![FoZnDe.png](https://s2.ax1x.com/2019/01/03/FoZnDe.png)\n-\t在之前双层形态学的基础定义上，定义自己到自己的映射为基本对，用一个字母表示；用^代表语素边界；用#代表单词边界，在任务中提到需要输出+SG之类的语素特征，这些特征在另一个输出上没有对应的输出符号，因此映射到空字符串或边界符号。我们把输入输出对用冒号连接，也可以写在弧的上下。一个抽象的表示英语名词复数屈折变化的转录机如下图：\n![FoZl4I.png](https://s2.ax1x.com/2019/01/03/FoZl4I.png)\n-\t之后我们需要更新词表，使得非规则复数名词能够被剖析为正确的词干：\n![FoZMEd.png](https://s2.ax1x.com/2019/01/03/FoZMEd.png)\n-\t之后将抽象的转录机写成具体的，由字母组成转移弧的转录机，如下图，只展示了具体化部分非规则复数和单数名词之后的转录机：\n![FoZ3Ct.png](https://s2.ax1x.com/2019/01/03/FoZ3Ct.png)\n\n## 转录机和正词法规则\n-\t用拼写规则，也就是正词法规则来处理英语中经常在语素边界发生拼写错误的问题。\n-\t以下是一些拼写规则实例：\n\t-\t辅音重叠：beg/beggin\n\t-\tE的删除：make/making\n\t-\tE的插入：watch/watches\n\t-\tY的替换：try/tries\n\t-\tK的插入：panic/panicked\n-\t为了实现拼写规则，我们在词汇层和表层之间加入中间层，以符合特定规则的语素毗连作为输入，以修改之后的正确的语素毗连作为输出，例如fox +N +PL输入到中间层即第一次转录，得到fox ^ s #，之后中间层到表层的第二次转录检测到特殊语素毗连：x^和s#，就在表层的x和s之间插入一个e，得到foxes。下面的转录机示意图展示了这个过程：\n![FoZ88P.png](https://s2.ax1x.com/2019/01/03/FoZ88P.png)\n-\t这个转录机只考虑x^和s#毗连需插入e这一正词法规则\n-\t其他的词能正常通过\n-\t$Q_0$代表无关词通过，是接受状态\n-\t$Q_1$代表看见了zsx，作为中间状态保存，一直保存的是最后的与语素毗连的z,s,x，如果出现了其他字母则返回到q0，其本身也可以作为接受态\n-\t$Q_2$代表看见了与z,s,x毗连的语素，这之后有四种转移\n\t-\t接了$x$,$z$，回到$q_1$，也就是认为重新接到了可能和语素毗连的x,z\n\t-\t接了$s$，分为两种情况，一种是正常需要插入e，这时通过$\\epsilon$转移到$q_3$再到$q_4$；另一种是本来就需要插入$e$，这就到达$q_5$，之后视情况回退了$q_1$、$q_0$，或者$s$又毗连语素回到$q_2$。两种情况不确定，需要通过搜索解决\n\t-\t接单词边界和其他符号，回到$q_0$\n\t-\t$q_2$本身也可以作为接受态\n\n## 结合\n-\t现在可以通过三层结构，结合产生语素和进行正词法规则矫正的转录机。从词汇层到中间层用一个转录机产生语素，从中间层到表层可并行使用多个转录机进行正词法规则的矫正。\n-\t两类转录机叠加时可以改写成一类转录机，这时需要对两类状态机状态集合计算笛卡尔积，对新集合内每一个元素建立状态。\n-\t这种三层结构是可逆的，但是进行剖析时（从表层到词汇层）会出现歧义问题，即一个单词可能剖析出多种语素结合，这时单纯依靠转录机无法消歧，需要借助上下文。\n\n## 其他应用（简单介绍）\n-\t不需要词表的FST，PORTER词干处理器：将层叠式重写规则用FST实现，提取出单词的词干。\n-\t分词和分句：一个简单的英文分词可以基于正则表达式实现，一个简单的中文分词可以通过maxmatch（一种基于最大长度匹配的贪婪搜索算法）实现。\n-\t拼写检查与矫正：使用了投影操作的FST可以完成非词错误的检测，然后基于最小编辑距离（使用动态规划算法实现）可以矫正。正常词错误检测和矫正需借助N元语法模型。\n\n## 人如何进行形态处理\n-\t研究表明，人的心理词表存储了一部分形态机构，其他的结构不组合在心理词表中，而需要分别提取并组合。研究说明了两个问题：\n\t-\t形态尤其是屈折变化之类的能产性形态在人的心理词表中起作用，且人的语音词表和正词法词表可能具有相同结构。\n\t-\t例如形态这种语言处理的很多性质，可以应用于语言的理解和生成。\n\n# 第四章：N元语法\n-\t语言模型是关于单词序列的统计模型，N元语法模型是其中的一种，它根据之前N-1个单词推测第N个单词，且这样的条件概率可以组成整个单词序列（句子）的联合概率。\n\n## 在语料库中统计单词\n-\t区别：word type或者叫 vocabulary size V，代表语料中不同单词的个数，而tokens，不去重，代表语料的大小。有研究认为词典大小不低于tokens数目的平方根。\n非平滑N元语法模型\n-\t任务：根据以前的单词推断下一个单词的概率：$P(w|h)$，以及计算整个句子的概率$P(W)$。\n-\t最朴素的做法是用古典概型，统计所有历史h和当前词w组成的片段在整个语料中出现的次数，并除以历史h片段在整个语料中出现的次数。句子的概率也用相似的方法产生。缺点：依赖大语料，且语言本身多变，这样的计算限制过于严格。\n-\t接下来引入N元语法模型，首先通过概率的链式法则，可以得到条件概率$P(w|h)$和整个句子的联合概率$P(W)$之间的关系：\n$$\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\\\\n= \\prod _{k=1}^n P(w_k|w_1^{k-1}) \\\\\n$$\n-\tN元语法模型放松了条件概率的限制，做出一个马尔可夫假设：每个单词的概率只和它之前N-1个单词相关，例如二元语法模型，只和前一个单词相关，用这个条件概率去近似$P(w|h)$:\n$$\nP(w_n|w_1^{n-1}) \\approx P(w_n|w_{n-1}) \\\\\n$$\n-\tN元语法模型里的条件概率用最大似然估计来估算，统计语料中各种N元语法的个数，并归一化，其中可以简化的一点是：以二元语法为例，所有给定单词开头的二元语法总数必定等于该单词一元语法的计数：\n$$\nP(w_n|w_{n-1}) = \\frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\\\\n$$\n-\t使用N元语法之后，句子概率的链式分解变得容易计算，我们可以通过计算各种句子的概率来判断句子是否包含错字，或者计算某些句子在给定上下文中出现的可能，因为N元语法能捕捉一些语言学上的特征，或者一些用语习惯。在语料充足的时候，我们可以使用三元语法模型获得更好的效果。\n\n## 训练集和测试集\n-\tN元语法模型对训练集非常敏感。N元语法的N越大，依赖的上下文信息越多，利用N元语法模型生成的句子就越流畅，但这些未必“过于流畅”，其原因在于N元语法概率矩阵非常大且非常稀疏，在N较大例如四元语法中，一旦生成了第一个单词，之后可供的选择非常少，接着生成第二个单词之后选择更少了，往往只有一个选择，这样生成的就和原文中某一个四元语法一模一样。过于依赖训练集会使得模型的泛化能力变差。因此我们选择的训练集和测试集应来自同一细分领域。\n-\t有时候测试集中会出现训练集词典里没有的词，即出现未登录词（Out Of Vocabulty,OOV）。在开放词典系统中，我们先固定词典大小，并将所有未登录词用特殊符号<UNK>代替，然后才进行训练。\n\n## 评价N元语法模型：困惑度\n-\t模型的评价分两种：外在评价和内在评价。外在评价是一种端到端的评价，看看某一模块的改进是否改进了整个模型的效果。内在评价的目的是快速衡量模块的潜在改进效果。内在评价的潜在改进效果不一定会使得端到端的外在评价提高，但是一般两者都存在某种正相关关系。\n-\t困惑度（Perplexsity,PP）是一种关于概率模型的内在评价方法。语言模型的在测试集上的困惑度是语言模型给测试集分配的概率的函数。以二元语法为例，测试集上的困惑度为：\n$$\nPP(W) = \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i|w_{i-1})}} \\\\\n$$\n-\t概率越高，困惑度越低。困惑度的两种解释：\n\t-\t加权的平均分支因子：分支因子是指可能接在任何上文之后的单词的数目。显然，如果我们的模型啥也没学习到，那么测试集任何单词可以接在任何上文之后，分支因子很高，困惑度很高；相反，如果我们的模型学习到了具体的规则，那么单词被限制接在一些指定上文之后，困惑度变低。困惑度使用了概率加权分支因子，分支因子的大小在模型学习前后不变，”morning”仍然可以接到任何上文之后，但是它接到”good”之后的概率变大了，因此是加权的分支因子。\n\t-\t熵：对于语言序列，我们定义一个序列的熵为：\t$$H(w_1,w_2,…,w_n )=-\\sum _{W_1^n \\in L} p(W_1^n) \\log ⁡p(W_1^n)$$也就是这个序列中所有前缀子序列的熵之和，其均值是序列的熵率。计算整个语言的熵，假设语言是一个产生单词序列的随机过程，单词序列无限长，则其熵率是：$$H(L)=\\lim _{n \\rightarrow \\infty}⁡ \\frac 1n H(w_1,w_2,…,w_n) =\\lim _{n \\rightarrow \\infty} -⁡\\frac 1n \\sum _{W \\in L} p(W_1^n)  \\log ⁡p(W_1^n)$$根据Shannon-McMillan-Breiman理论，在n趋于无穷的情况下，如果语言既是平稳又是正则的，上面这些子串的和的熵，可以用最大串代替每一个子串得到，这里的代替是指log后面求的是最大串的概率，log之前的概率依然是各个子串的概率？假如是这样的话提出最大串的概率对数，对所有子串概率求和得到：$$H(L)=\\lim _{n \\rightarrow \\infty} -⁡ \\frac 1n \\log ⁡p(w_1,w_2,…,w_n)$$交叉熵可以衡量我们的模型生成的概率分布到指定概率分布之间的距离，我们希望模型生成概率分布尽可能近似真实分布，即交叉熵小。具体衡量时是对相同的语言序列，计算训练得到的模型m和理想模型p在生成这个序列上的概率的交叉熵：$$H(p,m) = \\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\sum _{W \\in L} p(W_1^n) \\log⁡ m(W_1^n)$$但是我们不知道理想的分布p，这时根据之前的Shannon-McMillan-Breiman定理，得到了只包含一个概率分布的序列交叉熵（？）：$$H(p,m)=\\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\log⁡ m(W_1^n)$$在测试数据上我们没有无限长的序列，就用有限长的序列的交叉熵近似这个无限长序列的交叉熵。困惑度则是这个（近似的？只包含一个概率分布的？）交叉熵取指数运算：\n$$\nPerplexity(W) = 2^{H(W)} \\\\\n= P(w_1 w_2 ... w_N)^{\\frac {-1}{N}} \\\\\n= \\sqrt[n]{\\frac {1}{P(w_1 w_2 ... w_N)}} \\\\\n= \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i | w_1 ... w_{i-1})}} \\\\\n$$\n\n## 平滑\n-\t因为N元语法模型依赖语料，一般而言对于N越高的N元语法，语料提供的数据越稀疏。这种情况下N元语法对于那些计数很小的语法估计很差，且如果测试集中某一句包含了训练集中没有出现的N元语法时，我们无法使用困惑度进行评价。因此我们使用平滑作为一种改进方法，使得N元语法的最大似然估计能够适应这些存在0概率的情况。\n-\t接下来介绍了两种平滑：\n\t-\t拉普拉斯平滑（加1平滑）\n\t-\tGood-Turing 打折法\n\n### 拉普拉斯平滑\n-\t加1平滑就是在计算概率归一化之前，给每个计数加1，对应的，归一化时分母整体加了一个词典大小:\n$$\nP_{Laplace}(w_i) = \\frac {c_i + 1}{N+V} \\\\\n$$\n-\t为了表现平滑的作用，引入调整计数$c^{\\*}$，将平滑后的概率写成和平滑之前一样的形式：\n$$\nP_{Laplace} (w_i) = \\frac {(C_i^{\\*})}{N} \\\\\nC_i^{\\*} = \\frac {(C_i+1)N}{(N+V)} \\\\\n$$\n-\t一种看待平滑的角度是：对每个非0计数打折，分一些概率给0计数，定义相对打折$d_c$（定义在非0计数上），\n$$\nd_c = \\frac {c^{\\*}} {c}\n$$\n-\t$d_c$代表了打折前后单词计数的变化。平滑之后，对于非0计数，当$C_i < \\frac NV$时，计数增加；否则计数减少。计数越大，打折越多，增加越少（减少越多）。当0计数很多时，N/V较小，这时大部分非0计数都会减少，且减少较多。\n-\t而0计数则没有收到打折的影响。因此在一轮不同程度的增长之后，再归一化的结果就是非0计数分享了一些概率给0计数。写成调整计数的形式，就是非0计数减少数值，0计数变化（一般是减少）数值（但不是减少的完全等于增加的）。 书中给出了一个例子，下图是一部分语料的二元语法平滑之后的计数，蓝色代表平滑加1之后的0计数：\n![FoZNDg.png](https://s2.ax1x.com/2019/01/03/FoZNDg.png)\n如果把表写成调整计数的形式：\n![FoZtKS.png](https://s2.ax1x.com/2019/01/03/FoZtKS.png) \n-\t可以看到，本来的0计数（蓝色）从0变大，而其他的计数减少，例如< i want>，从827减少到527，<want to>从608减少到238。\n-\t当0计数很多时，非0计数减少的数值很多，可以使用一个小于1的小数$\\delta$代替1，即加$\\delta$平滑。通常这个$\\delta$是动态变化的。\n\n### GT打折法\n-\t类似于Good-Turing打折法, Witten-Bell打折法， Kneyser-Ney 平滑一类的方法，它们的基本动机是用只出现一次的事物的计数来估计从未出现的事物的计数。只出现一次的语法称为单件（singleton）或者罕见语（hapax legomena）。Good-Turing打折法用单件的频率来估计0计数二元语法。\n-\t定义N_c为出现c次的N元语法的总个数（不是总个数乘以c），并称之为频度c的频度。对N_c中的c的最大似然估计是c。这样相当于将N元语法按其出现次数分成了多个桶，GT打折法用c+1号桶里语法概率的最大似然估计来重新估计c号桶内语法的概率。因此GT估计之后最大似然估计得到的c被替换成：\n$$\nc^{\\*}=(c+1) \\frac {N_{c+1}}{N_c} \n$$\n-\t之后计算某N元语法的概率：\n\t-\t从未出现：$P_{GT}^{\\*}=\\frac{N_1}{N}$。其中N是所有N元语法数$(\\sum _i N_i \\* i)$。这里假设了我们已知$N_0$，则此式表示某一具体未知计数N元语法概率时还应除以$N_0$。\n\t-\t已出现（已知计数）：$P_{GT}^{\\*} = \\frac{c^{\\*}}{N}$\n-\t这样计算，$N_1$的一些概率转移到了$N_0$上。GT打折法假设所有的N元语法概率分布满足二项式分布，且假设我们已知$N_0$，以二元语法为例：\n$$\nN_0 = V^2 - \\sum _{i>0} N_i \\\\ \n$$\n-\t其他注意事项：\n\t-\t有些$N_c$为0，这时我们无法用这些$N_c$来计算平滑后的c。这种情况下我们直接放弃平滑，令$c^{\\*} = c$，再根据正常的数据计算出一个对数线性映射，$log⁡(N_c) = a + b \\log(c)$，代入放弃平滑的c并用其倒推计算计数为0的$N_c$，使得这些$N_c$有值，不会影响更高阶的c的计算。\n\t-\t只对较小c的$N_c$进行平滑，较大c的$N_c$认为足够可靠，设定一个阈值k，对$c < k$的$N_c$计算：\n$$\nc^{\\*} = \\frac {(c+1) \\frac {N_c+1}{N_c} - c \\frac {(k+1) N_{k+1} }{N_1} } {1- \\frac {(k+1)N_{k+1}} {N_1}} \\\\\n$$\n\t-\t计算较小的c如c=1时，也看成c=0的情况进行平滑\n-\t一个例子：\n![FoZGgf.png](https://s2.ax1x.com/2019/01/03/FoZGgf.png)\n \n## 插值与回退\n-\t上述的平滑只考虑了如何转移概率到计数为0的语法上去，对于条件概率$p(w|h)$，我们也可以采用类似的思想，假如不存在某个三元语法帮助计算$p(w_n |w_{n-1} w_{n-2})$，则可以用阶数较低的语法$p(w_n |w_{n-1})$帮助计算，有两种方案：\n\t-\t回退：用低阶数语法的替代0计数的高阶语法\n\t-\t插值：用低阶数语法的加权估计高阶语法\n-\t在Katz回退中，我们使用GT打折作为方法的一部分：GT打折告诉我们有多少概率可以从已知语法中分出来，Katz回退告诉我们如何将这些分出来的概率分配给未知语法。在之前的GT打折法中，我们将分出的概率均匀分给每一个未知语法，而Katz回退则依靠低阶语法的信息来分配：\n![FoZJv8.png](https://s2.ax1x.com/2019/01/03/FoZJv8.png)\n-\t其中$P^{\\*}$是打折之后得到的概率；\\alpha是归一化系数，保证分出去的概率等于未知语法分配得到的概率。\n-\t插值则是用低阶语法概率加权求和得到未知高阶语法概率：\n![FoZUbQ.png](https://s2.ax1x.com/2019/01/03/FoZUbQ.png)\n-\t加权的系数还可以通过上下文动态计算。具体系数的计算有两种方法：\n\t-\t尝试各种系数，用在验证集上表现最好的系数组合\n\t-\t将系数看成是概率生成模型的隐变量，使用EM算法进行推断\n\n## 实际问题：工具和数据格式\n-\t在语言模型计算中，一般将概率取对数进行计算，原因有二：防止数值下溢；取对数能将累乘运算变成累加，加速计算。\n-\t回退N元语法模型一般采用ARPA格式。ARPA格式文件由一些头部信息和各类N元语法的列表组成，列表中包含了该类N元语法下所有语法，概率，和回退的归一化系数。只有能够称为高阶语法前缀的低阶语法才能在回退中被利用，并拥有归一化系数。\n-\t两种计算N元语法模型的工具包：SRILM toolkit 和Cambridge-CMU toolkit\n\n## 语言建模中的高级问题\n### 高级平滑方法：Kneser-Ney平滑\n-\t注意到在GT打折法当中，打折之后估计的c值比最大似然估计得到的c值近似多出一个定值d。绝对打折法便考虑了这一点，在每个计数中减去这个d：\n![FoZwUs.png](https://s2.ax1x.com/2019/01/03/FoZwUs.png)\n-\tKneser-Ney平滑吸收了这种观点，并且还考虑了连续性：在不同上文中出现的单词更有可能出现在新的上文之后，在回退时，我们应该优先考虑这种在多种上文环境里出现的词，而不是那些出现次数很多，但仅仅在特定上文中出现的词。\n![FoZdEj.png](https://s2.ax1x.com/2019/01/03/FoZdEj.png)\n-\t在Kneser-Ney中，插值法能够比回退法取得更加好的效果：\n![FoZ05n.png](https://s2.ax1x.com/2019/01/03/FoZ05n.png)\n\n### 基于分类的N元语法\n-\t这种方法是为了解决训练数据的稀疏性。例如IBM聚类，每个单词只能属于一类，以二元语法为例，某个二元语法的条件概率的计算变为给定上文所在类，某个单词的条件概率，还可以进一步链式分解为两个类的条件概率乘以某个单词在给定其类条件下的条件概率：\n$$\np(w_i│w_{i-1} ) \\approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \\cdot p(w_i |c_i)\n$$\n\n### 语言模型适应和网络应用\n-\t适应是指在大型宽泛的语料库上训练语言模型，并在小的细分领域的语言模型上进一步改进。网络是大型语料库的一个重要来源。在实际应用时我们不可能搜索每一个语法并统计搜索得到所有页面上的所有语法，我们用搜索得到的页面数来近似计数。\n\n### 利用更长距离的上文信息\n-\t通常我们使用二元和三元语法模型，但是更大的N能够带来更好的效果。为了捕捉更长距离的上文信息，有以下几种方法：\n\t-\t基于缓存机制的N元语法模型\n\t-\t基于主题建模的N元语法模型，对不同主题建模语言模型，再加权求和\n\t-\t不一定使用相邻的上文信息，例如skip N-grams或者不一定使用定长的上文信息，例如变长N-grams\n\n# 第十六章：语言的复杂性\n## Chomsky层级\n-\tChomsky层级反映了不同形式化方法描述的语法之间的蕴含关系，较强生成能力或者说更复杂的语法在层级的外层。从外到内，加在可重写语法规则上的约束增加，语言的生成能力逐渐降低。\n![FoZXad.png](https://s2.ax1x.com/2019/01/03/FoZXad.png)\n-\t五种语法对应的规则和应用实例：\n![Foepxf.png](https://s2.ax1x.com/2019/01/03/Foepxf.png)\n\t-\t0型语法：规则上只有一个限制，即规则左侧不能为空字符串。0型语法刻画了递归可枚举语言\n\t-\t上下文相关语法：可以把上下文\\alpha，\\beta之间的非终极符号A重写成任意非空符号串\n\t-\t温和的上下文相关语法\n\t-\t上下文无关语法：可以把任何单独的非终极符号重写为由终极符号和非终极符号构成的字符串，也可以重写为空字符串\n\t-\t正则语法：可以是右线性也可以是左线性，以右线性为例，非终极符号可以重写为左边加了若干终极符号的另一个非终极符号，右线性不断地在字符串左侧生成终极符号。\n\n## 自然语言是否正则\n-\t判断语言是否正则能够让我们了解应该用哪一层次的语法来描述一门语言，且这个问题能够帮助我们了解自然语言的不同方面的某些形式特性。\n-\t抽吸引理：用来证明一门语言不是正则语言。\n\t-\t如果一门语言可以被有限状态自动机来描述，则与自动机对应有一个记忆约束量。这个约束量对于不同的符号串不会增长的很大，因为其状态数目是固定的，更长的符号串应该是通过状态之间转移产生而不是增加状态数目。因此这个记忆量不一定和输入的长度成比例。\n\t-\t如果一个正则语言能够描述任意长的符号序列，比自动机的状态数目还多，则该语言的自动机中必然存在回路。\n![FoZxPI.png](https://s2.ax1x.com/2019/01/03/FoZxPI.png)\n-\t如图所示自动机，可以表述xyz,xyyz,xyyyz.....，当然也可以将中间无限长的y序列“抽吸掉”，表述xz。抽吸引理表述如下：\n-\t设L是一个有限的正则语言，那么必然存在符号串x,y,z,使得对于任意n≥0，y≠$\\epsilon$，且xy^n z∈L\n-\t即假如一门语言是正则语言，则存在某一个符号串y，可以被适当的“抽吸”。这个定理是一门语言是正则语言的必要非充分条件。\n-\t有学者证明英语不是一门正则语言：\n\t-\t具有镜像性质的句子通过抽吸原理可以证明不是正则语言，而英语中一个特殊的子集合和这种镜像性质的句子是同态的。\n\t-\t另一种证明基于某些带有中心-嵌套结构的句子。这种句子可以由英语和某一类简单的正则表达式相交得到，通过抽吸原理可以得到这种句子不是正则语言。英语和正则语言的交不是正则语言，则英语不是正则语言。\n\n## 自然语言是否上下文无关\n-\t既然自然语言不是正则语言，我们接着考虑更宽松的限定，自然语言是否是上下文无关的？\n-\t不是......\n\n## 计算复杂性和人的语言处理\n-\t人对中心嵌套句子处理很困难，因为人们剖析时利用的栈记忆有限，且栈中不同层次记忆容易混淆。\n\n# 第五章：词类标注\n-\t各种表述：POS（Part Of Speech）、word classes（词类）、morphological classes（形态类）、lexical tags（词汇标记）。\n-\tPOS的意义在于：\n\t-\t能够提供关于单词及其上下文的大量信息。\n\t-\t同一单词在不同词类下发音不同，因此POS还能为语音处理提供信息。\n\t-\t进行词干分割（stemming），辅助信息检索\n-\t本章介绍三种词类标注算法：\n\t-\t基于规则的算法\n\t-\t基于概率的算法，隐马尔科夫模型\n\t-\t基于变换的算法\n\n## 一般词类\n-\tPOS分为封闭集和开放集，封闭集集合相对稳定，例如介词，开放集的词语则不断动态扩充，例如名词和动词。特定某个说话人或者某个语料的开放集可能不同，但是所有说一种语言以及各种大规模语料库可能共享相同的封闭集。封闭集的单词称为虚词（功能词，function word），这些词是语法词，一般很短，出现频次很高。\n-\t四大开放类：名词、动词、形容词、副词。\n-\t名词是从功能上定义的而不是从语义上定义的，因此名词一般表示人、地点、事物，但既不充分也不必要。定义名词：\n\t-\t与限定词同时出现\n\t-\t可以受主有代词修饰\n\t-\t大多数可以以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词\n-\t动词，表示行为和过程的词，包括第三人称单数、非第三人称单数、进行时、过去分词几种形态\n-\t形容词，描述性质和质量\n-\t副词，用于修饰，副词可以修饰动词、动词短语、其它副词。\n-\t英语中的一些封闭类：\n\t-\t介词 prepositions：出现在名词短语之前，表示关系\n\t-\t限定词 determiners 冠词 articles：与有定性（definiteness）相关\n\t-\t代词 pronouns：简短的援引某些名词短语、实体、或事件的一种形式\n\t-\t连接词 conjunctions：用于连接和补足（complementation）\n\t-\t助动词 auxiliary verbs：标志主要动词的某些语义特征，包括：时态、完成体、极性对立、情态\n\t-\t小品词 particles：与动词结合形成短语动词\n\t-\t数词 numerals\n\n## 词类标注\n-\t标注算法的输入是单词的符号串和标记集，输出要让每一个单词标注上一个单独且最佳的标记。如果每个单词只对应一种词性，那么根据已有的标记集，词类标注就是一个简单的查表打标的过程，但是很多词存在多种词性，例如book既可以是名词也可以是动词，因此要进行消歧，词类标注是歧义消解的一个重要方面。\n\n## 基于规则的词类标注\n-\t介绍了ENGTWOL系统，根据双层形态学构建，对于每一个词的每一种词类分别立条，计算时不计屈折形式和派生形式.\n-\t标注算法的第一阶段是将单词通过双层转录机，得到该单词的所有可能词类\n-\t之后通过施加约束规则排除不正确的词类。这些规则通过上下文的类型来决定排除哪些词类。\n\n## 基于隐马尔科夫模型的词类标注\n-\t使用隐马尔科夫模型做词类标注是一类贝叶斯推断，这种方法将词类标注看成是序列分类任务。观察量为一个词序列（比如句子），任务是给这个序列分配一个标注序列。\n-\t给定一个句子，贝叶斯推断想要在所有标注序列可能中选择最好的一个序列，即\n$$\n{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)\n$$\n-\t使用贝叶斯法则将其转化为：\n$$\n{t_1^n} _{best}={argmax} _{t_1^n}  \\frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n} P(w_1^n│t_1^n)P(t_1^n)\n$$\n-\t隐马尔科夫模型在此基础上做了两点假设\n\t-\t一个词出现的概率只与该词的词类标注有关，与上下文其他词和其他标注无关，从而将序列的联合概率拆解为元素概率之积，即：P(w_1^n│t_1^n) \\approx \\prod _{i=1}^n P(w_i |t_i)\n\t-\t一个标注出现的概率只与前一个标注相关，类似于二元语法的假设：P(t_1^n ) \\approx \\prod _{i=1}^n P(t_i |t_{i-1})\n-\t在两种假设下简化后的最好标注序列表达式为：\n$$\n{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})\n$$\n-\t上面这个概率表达式实际上将HMM模型的联合概率拆成了各个部分转移概率的乘积，具体而言分为标签转移概率（隐变量之间转移）和词似然（隐变量转移到可观察变量）。通过最大似然估计，我们可以通过古典概型的方法从已标注的语料中计算出这两类概率：\n$$\nP(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\\\\nP(w_i│t_i ) = \\frac{C(t_i,w_i)}{C(t_i)} \\\\\n$$\n-\t一个例子：HMM模型如何正确的将下句中的race识别为动词而不是名词：\n-\tSecretariat is expected to race tomorrow.\n-\t画出上句中race被识别为动词和名词两种情况下的HMM模型，可以看到两个模型对比只有三个转移概率不同，用加粗线标出：\n![FoZDCq.png](https://s2.ax1x.com/2019/01/03/FoZDCq.png)\n-\tHMM词类标注器消歧的方式是全局的而不是局部的。我们在语料中统计得到这三种转移概率，再累乘，结果是(a)的概率是(b)概率的843倍。显然race应该被标注为动词。\n\n## 形式化隐马尔科夫模型标注器\n-\tHMM模型是有限自动机的扩展，具体而言是一种加权有限自动机，马尔可夫链的扩展，这种模型允许我们考虑观察量和隐变量，考虑包含隐变量的概率模型。HMM包含以下组件：\n\t-\tQ：大小为N的状态集\n\t-\tA：大小为N*N的转移概率矩阵\n\t-\tO：大小为T的观察事件集\n\t-\tB：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察o_t的概率\n\t-\t$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量\n-\tA中的概率和B中的概率对应着之前式子中每一个累乘项里的先验$P(w_i│t_i )$和似然$P(t_i |t _{i-1})$概率：\n$$\n{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})\n$$\n\n## HMM标注的维特比算法\n-\t在HMM模型中，已知转移概率和观察序列，求隐变量的任务叫做解码。解码的一种算法即维特比算法，实质上是一种动态规划算法，与之前求最小编辑距离的算法类似。\n-\t首先我们从语料中计算得到A和B两个矩阵，即模型的转移概率已知，对于给定的观察序列，按照以下步骤执行维特比算法：\n![FoZyvT.png](https://s2.ax1x.com/2019/01/03/FoZyvT.png)\n-\t算法维护一个$(N+2)\\*T$的概率矩阵viterbi，加了2代表初始状态和结束状态，viterbi[s,t]代表了在第t步状态为s时的最佳路径概率，而backpointer[s,t]对应着保存了该最佳路径的上一步是什么状态，用于回溯输出整个最佳路径。\n-\t关键的转移在于$viterbi[s,t] \\leftarrow max _{s^{\\*}=1}^N⁡ viterbi[s^{\\*},t-1] \\* a_{s^{\\*},s} \\* b_s (o_t)$即当前时间步最佳路径是由上一时间步各个状态的最佳路径转移过来的，选择上一步最佳路径概率与转移概率乘积最大的路径作为当前时间步的最佳路径。从动态规划的角度而言，即长度为t的最佳路径，必定是从长度为t-1的最佳路径里选择一条转移得到，否则肯定可以从另一条概率更大的路径转移获得更优解。这样就限制了最佳路径的生成可能，减少了计算量。\n\n## 将HMM算法扩展到三元语法\n-\t现代的HMM标注器一般在标注转移概率上考虑更长的上文历史：\n$$\nP(t_1^n ) \\approx \\prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})\n$$\n-\t这样的话需要在序列开头和结尾做一些边界处理。使用三元语法的一个问题是数据稀疏：例如我们从没有在训练集中见过标注序列PRP VB TO，则我们无法计算P(TO|PRP,VB)。一种解决办法是线性插值：\n$$\nP(t_i│t _{i-1} t _{i-2} ) = \\lambda _1 P ̂(t_i│t _{i-1} t _{i-2} )+\\lambda _2 P ̂(t_i│t _{i-1} )+\\lambda _3 P ̂(t_i)\n$$\n-\t使用删除插值的办法确定系数$\\lambda$：\n![FoZr80.png](https://s2.ax1x.com/2019/01/03/FoZr80.png)\n\n## 基于变换的标注\n-\t基于变换的方法结合了基于规则和基于概率方法的优点。基于变换的方法依然需要规则，但是从数据中总结出规则，是一种监督学习方法，称为基于变换的学习（Transformation Based Learning，TBL）。在TBL算法中，语料库首先用比较宽的规则来标注，然后再选择稍微特殊的规则来修改，接着再使用更窄的规则来修改数量更少的标记。\n\n## 如何应用TBL规则\n-\t首先应用最宽泛的规则，就是根据概率给每个词标注，选择概率最大的词类作为标注。之后应用变换规则，即如果满足某一条件，就将之前标注的某一词类变换（纠正）为正确的词类，之后不断应用更严格的变换，在上一次变换的基础上进行小部分的修改。\n-\t如何学习到TBL规则\n\t-\t首先给每个词打上最可能的标签\n\t-\t检查每一个可能的变换，选择效果提升最多的变换，此处需要直到每一个词正确的标签来衡量变换带来的提升效果，因此是监督学习。\n\t-\t根据这个被选择的变换给数据重新打标，重复步骤2，直到收敛（提升效果小于某一阈值）\n-\t以上过程输出的结果是一有序变换序列，用来组成一个标注过程，在新语料上应用。虽然可以穷举所有的规则，但是那样复杂度太高，因此我们需要限制变换集合的大小。解决方案是设计一个小的模板集合（抽象变换）,每一个允许的变换都是其中一个模板的实例化。\n\n## 评价和错误分析\n-\t一般分为训练集、验证集、测试集，在训练集内做十折交叉验证。\n-\t与人类标注的黄金标准比较计算准确率作为衡量指标。\n-\t一般用人类表现作为ceiling，用一元语法最大概率标注的结果作为baseline。\n-\t通过含混矩阵或者列联表来进行错误分析。在N分类任务中，一个N*N的含混矩阵的第i行第j列元素指示第i类被错分为第j类的次数在总分错次数中的占比。一些常见的容易分错的词性包括：\n\t-\t单数名词、专有名词、形容词\n\t-\t副词、小品词、介词\n\t-\t动词过去式、动词过去分词、形容词\n\n## 词性标注中的一些其他问题\n-\t标注不确定性：一个词在多个词性之间存在歧义，很难区分。这种情况下有些标注器允许一个词被打上多个词性标注。在训练和测试的时候，有三种方式解决这种多标注词：\n\t-\t通过某种方式从这些候选标注中选择一个标注\n\t-\t训练时指定一个词性，测试时只要打上了候选词性中任意一个就认为标注正确\n\t-\t将整个不确定的词性集看成一个新的复杂词性\n-\t多部分词：在标注之前需要先分词，一些多部分词是否应该被分为一部分，例如New York City应该分成三部分还是一个整体，也是各个标注系统需要考虑的。\n-\t未知词：不在词典中的词称为未知词。对于未知词，训练集无法给出它的似然P(w_i |t_i)，可以通过以下几种方式解决：\n\t-\t只依赖上下文的POS信息预测\n\t-\t用只出现一次的词来估计未知词的分布，类似于Good Turing打折法\n\t-\t使用未知词的单词拼写信息，正词法信息。例如连字符、ed结尾、首字母大写等特征。之后在训练集中计算每个特征的似然，并假设特征之间独立，然后累乘特征似然作为未知词的似然：$P(w_i│t_i )=p(unknown word│t_i ) \\* p(capital│t_i ) \\* p(endings/hyph|t_i)$\n\t-\t使用最大熵马尔可夫模型\n\t-\t使用对数线性模型\n\n## 噪声信道模型\n-\t贝叶斯推断用于标注可以认为是一种噪声信道模型的应用，本节介绍如何用噪声信道模型来完成拼写纠正任务。\n之前对于非单词错误，通过词典查找可以检测到错误，并根据最小编辑距离纠正错误，但这种方法对于真实单词错误无能为力。噪声信道模型可以纠正这两种类型的拼写错误。\n-\t噪声信道模型的动机在于将错误拼写的单词看成是一个正确拼写的单词经过一个噪声信道时受到干扰扭曲得到。我们尝试所有可能的正确的词，将其输入信道，最后得到的干扰之后的词与错误拼写的词比较，最相似的例子对应的输入词就认为是正确的词。这类噪声信道模型，比如之前的HMM标注模型，是贝叶斯推断的一种特例。我们看到一个观察两（错误拼写词）并希望找到生成这个观察量的隐变量（正确拼写词），也就是找最大后验。\n-\t将噪声信道模型应用于拼写纠正：首先假设各种拼写错误类型，错拼一个、错拼两个、漏拼一个等，然后产生所有可能的纠正，除去词典中不存在的，最后分别计算后验概率，选择后验概率最大的作为纠正。其中需要根据局部上下文特征来计算似然。\n-\t另一种纠正算法是通过迭代来改进的方法：先假设拼写纠正的含混矩阵是均匀分布的，之后根据含混矩阵运行纠正算法，根据纠正之后的数据集更新含混矩阵，反复迭代。这种迭代的算法是一种EM算法。\n\n## 根据上下文进行拼写纠正\n-\t即真实单词拼写错误的纠正。为了解决这类任务需要对噪声信道模型进行扩展：在产生候选纠正词时，需要包括该单词本身以及同音异形词。之后根据整个句子的最大似然来选择正确的纠正词。\n\n# 第六章：隐马尔科夫模型和最大熵模型\n-\t隐马尔科夫模型用来解决序列标注（序列分类问题）。\n-\t最大熵方法是一种分类思想，在满足给定条件下分类应满足限制最小（熵最大），满足奥卡姆剃刀原理。\n-\t最大熵马尔可夫模型是最大熵方法在序列标注任务上的扩展。\n\n## 马尔可夫链\n-\t加权有限自动状态机是对有限自动状态机的扩展，每条转移路径上加上了概率作为权重，说明从这条路径转移的可能性。马尔可夫链是加权有限状态自动机的一种特殊情况，其输入序列唯一确定了自动机会经过的状态序列。马尔可夫链只能对确定性序列分配概率。\n-\t我们将马尔可夫链看作一种概率图模型，一个马尔可夫链由下面的成分确定：\n$$\nQ=q_1 q_2…q_N \\\\\nA=a_{01} a_{02} … a_{n1} … a_{nn} \\\\\nq_0,q_F \\\\\n$$\n-\t分别是\n\t-\t状态集合\n\t-\t转移概率矩阵，其中a_ij代表了从状态i转移到状态j的概率$P(q_j |q_i)$\n\t-\t特殊的开始状态和结束状态\n-\t概率图表示将状态看成图中的点，将转移看成边。\n-\t一阶马尔可夫对转移做了很强的假设：某一状态的概率只与前一状态相关：\n$$\nP(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})\n$$\n-\t马尔可夫链的另一种表示不需要开始和结束状态：\n$$\n\\pi = \\pi _1,\\pi _2 , … , \\pi _N \\\\\nQA={q_x,q_y…} \\\\\n$$\n-\t分别是：\n\t-\t状态的初始概率分布，马尔可夫链以概率$\\pi _i$从状态i开始\n\t-\t集合QA是Q的子集，代表合法的接受状态\n-\t因此状态1作为初始状态的概率既可以写成$a_{01}$也可以写成$\\pi _1$。\n\n## 隐马尔科夫模型\n-\t当马尔可夫链已知时，我们可以用其计算一个观测序列出现的概率。但是观测序列可能依赖于一些不可观测的隐变量，我们可能感兴趣的是推断出这些隐变量。隐马尔科夫模型允许我们同时考虑观测变量和隐变量。\n-\t如之前一样定义隐马尔科夫模型：\n\t-\tQ：大小为N的状态集\n\t-\tA：大小为N*N的转移概率矩阵\n\t-\tO：大小为T的观察事件集\n\t-\tB：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察$o_t$的概率\n\t-\t$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量\n-\t同样的，隐马尔科夫也可以用另一种不依赖初始和结束状态的方式表示。隐马尔科夫模型也做了两个假设，分别是隐状态之间转移和隐状态到观察量转移的一阶马尔可夫性。\n-\t对于隐马尔科夫模型需要解决三类问题：\n\t-\t似然计算：已知参数和观测序列，求似然$P(O|\\lambda)$\n\t-\t解码：已知参数和观测序列，求隐状态序列\n\t-\t学习：已知观测序列和隐状态集合，求解模型参数\n\n## 计算似然：前向算法\n-\t对于马尔可夫链，其没有隐状态到观测量的转移概率矩阵，可以看成观察量与隐状态相同。在隐马尔科夫模型中不能直接计算似然，我们需要直到隐状态序列。\n-\t先假设隐状态序列已知，则似然计算为：\n$$\nP(O│Q) = \\prod _{i=1}^T P(o_i |q_i)\n$$\n-\t根据隐状态转移的一阶马尔可夫性，可以求得隐状态的先验，乘以似然得到观测序列和隐状态序列的联合概率：\n$$\nP(O,Q)=P(O│Q) \\* P(Q) = \\prod _{i=1}^n P(o_i│q_i )  \\prod _{i=1}^n P(q_i |q _{i-1})\n$$\n-\t对于联合概率积分掉隐状态序列，就可以得到观测概率的似然：\n$$\nP(O) = \\sum _Q P(O,Q) = \\sum _Q P(O|Q)P(Q) \n$$\n-\t这样计算相当于考虑了所有的隐状态可能，并对每一种可能从隐状态序列开始到结束计算一次似然，实际上可以保留每次计算的中间状态来减少重复计算，也就是动态规划。在前向计算HMM观测似然使用的动态规划算法称为前向算法：\n\t-\t令$\\alpha _t (j)$代表在得到前t个观测量之后当前时刻隐变量处于状态j的概率,\\lambda为模型参数：\n\t$$\n\t\\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\\lambda)\n\t$$\n\t-\t这个概率值可以根据前一时间步的\\alpha值计算出来，避免了每次从头开始计算：\n\t$$\n\t\\alpha _t (j) = \\sum _{i=1}^N \\alpha _{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t初始化$\\alpha _1 (j)$：\n\t$$\n\t\\alpha _1 (j)=a_{0s} b_s (o_1)\n\t$$\n\t-\t终止状态：\n\t$$\n\tP(O│\\lambda) = \\alpha _T (q_F) = \\sum _{i=1}^N \\alpha _T (i) \\alpha _{iF}\n\t$$\n\n## 解码：维特比算法\n-\t解码任务是根据观测序列和参数推断出最有可能隐状态序列。最朴素的做法：对于每种可能的隐状态序列，计算观测序列的似然，取似然最大时对应的隐状态序列。但是这样做就如同朴素的计算似然方法一样，时间复杂度过高，同样的，我们使用动态规划来缩小求解的规模。在解码时使用了一种维特比算法。\n\t-\t令$v_t (j)$代表已知前t个观测量（1~t）和已知前t个隐状态（0~t-1）的条件下，当前时刻隐状态为j的概率：\n\t$$\n\tv_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 … o_t,q_t=j|\\lambda)\n\t$$\n\t-\t其中我们已知了前t个时间步最大可能的隐状态序列，这些状态序列也是通过动态规划得到的：\n\t$$\n\tv_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t为了得到最佳的隐状态序列，还需要记录每一步的最佳选择，方便回溯得到路径：\n\t$$\n\t{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)\n\t$$\n\t-\t初始化：\n\t$$\n\tv_1 (j) = a_{0j} b_j (o_1) \\ \\  1 \\leq j \\leq N \\\\\n\t{bt}_1 (j) = 0 \\\\\n\t$$\n\t-\t终止，分别得到最佳隐状态序列（回溯开始值）及其似然值：\n\t$$\n\tP \\* = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) \\* a_{i,F} \\\\\n\tq_{T\\*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) \\* a_{i,F} \\\\\n\t$$\n-\t维特比算法减小时间复杂度的原因在于其并没有计算所有的隐状态路径，而是利用了每一时间步的最佳路径只能从上一时间步的最佳路径中延伸而来这一条件，减少了路径候选，避免了许多不必要的路径计算。并且每一步利用上一步的结果也是用了动态规划的思想减少了计算量。\n\n## 训练隐马尔科夫模型：前向后向算法\n-\t学习问题是指已知观测序列和隐状态集合，求解模型参数。\n-\t前向后向算法，又称Baum-Welch算法，是EM算法的一种特例，用来求解包含隐变量的概率生成模型的参数。该算法通过迭代的方式反复更新转移概率和生成概率，直到收敛。BW算法通过设计计数值之比作为隐变量，将转移概率矩阵和生成概率矩阵一起迭代更新。\n-\t先考虑马尔科夫链的学习问题。马尔科夫链可以看作是退化的隐马尔科夫模型，即每个隐变量只生成和自己一样的观测量，生成其他观测量的概率为0。因此只需学习转移概率。\n-\t对于马尔可夫链，可以通过古典概型统计出转移概率：\n$$\na_{ij} = \\frac {Count(i \\rightarrow j)} {\\sum _{q \\in Q} Count(i \\rightarrow q)}\n$$\n-\t我们可以这样直接计算概率是因为在马尔可夫链中我们知道当前所处的状态。对于隐马尔科夫模型我们无法这样直接计算是因为对于给定输入，隐状态序列无法确定。Badum-Welch算法使用了两种简洁的直觉来解决这一问题：\n\t-\t迭代估计，先假设一种转移概率和生成概率，再根据假设的概率推出更好的概率\n\t-\t计算某一观测量的前向概率，并将这个概率分摊到不同的路径上，通过这种方式估计概率\n-\t首先类似于前向概率，我们定义后向概率：\n\t-\t令$\\beta _t (i)$代表在得到后t个观测量之后当前时刻隐变量处于状态i的概率,$\\lambda$为模型参数：\n\t$$\n\t\\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\\lambda)\n\t$$\n\t-\t类似于后向概率的归纳计算：\n\t$$\n\t\\beta_t (i) = \\sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j),  \\ \\   1≤i≤N,1≤t<T\n\t$$\n\t-\t初始化$\\alpha _1 (j)$：\n\t$$\n\t\\beta _T (i)=\\alpha _(i,F)\n\t$$\n\t-\t终止状态：\n\t$$\n\tP(O│\\lambda)=\\alpha _t (q_F )=\\beta_1 (0)= \\sum _{i=1}^N a_{0j} b_j (o_1) \\beta _1 (j)\n\t$$\n-\t类似的，我们希望马尔可夫链中的古典概率能帮助我们估计转移概率：\n$$\na_{ij}^{\\*} = \\frac{从状态i转移到状态j的计数值期望}{从状态i转移出去的计数值期望}\n$$\n-\t如何估计计数值：我们将整个序列的转移路径计数值转化为时间步之间转移路径计数值之和，时间步之间某一条转移路径的概率为：\n$$\nP(q_t=i,q_{t+1}=j)\n$$\n-\t首先考虑所有的观测序列和这一转移路径的联合概率（省略了以参数$\\lambda$为条件）：\n$$\nP(q_t=i,q_{t+1}=j,O)\n$$\n-\t观察下面的概率图：\n![FoZWVJ.png](https://s2.ax1x.com/2019/01/03/FoZWVJ.png)\n-\t可以看到这一联合概率包含了三个部分：\n\t-\tT时刻隐状态为i的前向概率\n\t-\tT+1时刻隐状态为j的后向概率\n\t-\tT时刻与T+1时刻的状态转移概率以及生成对应观测量的生成概率\n-\t所以有：\n$$\nP(q_t=i,q_{t+1}=j,O)=\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j)\n$$\n-\t为了从联合分布中得到已知观测序列求转移路径的联合概率，需要计算观测序列的概率，可以通过前向概率或者后向概率求得：\n$$\nP(O)=\\alpha _t (N)=\\beta _T (1) = \\sum _{j=1}^N \\alpha _t (j) \\beta_t (j)\n$$\n-\t最终得到\n$$\nξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \\frac {(\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))}{(\\alpha _t (N))}\n$$\n-\t最后，对所有时间步求和就可以得到从状态i转移到状态j的期望计数值，从而进一步得到转移概率的估计：\n$$\na_{ij}^{\\*} = \\frac {\\sum _{t=1}^{T-1} ξ_t (i,j)}{\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1} ξ_t (i,j)}\n$$\n-\t同样的，我们还希望得到生成概率的估计：\n$$\nb_{j}^{\\*} (v_k) = \\frac {在状态j观测到符号v_k 的计数值期望}{状态j观测到所有符号的计数值期望}\n$$\n-\t类似的，通过先计算联合分布再计算条件分布的方式得到在t时刻处于隐状态j的概率：\n$$\nγ_t (j)=P(q_t=j│O) = \\frac {P(q_t=j,O)}{P(O)}\n$$\n-\t联合概率包含两个部分，即t时刻处于状态j的前向概率和后向概率，所以有：\n$$\nγ_t (j) = \\frac {\\alpha _t (j) \\beta_t (j)}{\\alpha _t (N)}\n$$\n-\t类似的，对所有时间步累加，进而得到生成概率的估计：\n$$\nb_{j}^{\\*} (v_k) = \\frac{\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) }{\\sum _{t=1}^T   γ_t (j) }\n$$\n-\t这两个式子是在已知前向概率和后向概率$(\\alpha,\\beta)$的情况下，计算出中间变量（隐变量）(ξ,γ),引入隐变量的动机是将a、b估计值的期望计数值之比转化为概率之比，且这两个隐变量可以用a,b表示。再由隐变量计算出转移概率和生成概率，因此形成了一个迭代的循环，可以用EM算法求解：\n$$\na,b→\\alpha,\\beta→ξ,γ→a,b\n$$\n-\tE-step:\n$$\nγ_t (j) = (\\alpha _t (j) \\beta_t (j))/(\\alpha _t (N)) ξ_t (i,j) \\\\\n= (\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))/(\\alpha _t (N)) \\\\\n$$\n-\tM-step（最大化的目标是什么）:\n$$\na _{ij} = (\\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1}   ξ_t (i,j)  ) \\\\\nb ̂_j(v_k) = (\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\\sum _{t=1}^T   γ_t (j) ) \\\\\n$$\n-\t迭代时需重新计算：\n$$\n\\alpha _t (j) = \\sum _{i=1}^N   \\alpha_{t-1} (i) a_ij b_j (o_t) \\\\\n\\beta_t (i) = \\sum _{j=1}^N   a_ij b_j (o_{t+1} ) \\beta_{t+1} (j)  \\\\\n$$\n-\t迭代的初始状态对于EM算法来说很重要，经常是通过引入一些外部信息来设计一个好的初始状态。\n\n## 最大熵模型：背景\n-\t最大熵模型另一种广为人知的形式是多项Logistic回归（Softmax?）。\n-\t最大熵模型解决分类问题，最大熵模型作为一种概率分类器，能够根据样本的特征求出样本属于每一个类别的概率，进而进行分类。\n-\t最大熵模型属于指数家族（对数线性）分类器，通过将特征线性组合，取指数得到分类概率：\n$$\np(c│x)=\\frac 1Z exp⁡(\\sum _i   weight_i feature_i) \n$$\n-\tZ是一个归一化系数，使得生成的概率之和为1。\n\n## 最大熵建模\n-\t将二分类Logistic回归推广到多分类问题就得到：\n$$\nP(c│x) = \\frac {exp⁡(\\sum _(i=0)^N   w_ci f_i) } {\\sum _{c^{\\*} in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i)  }\n$$\n-\t语音和语言处理中的特征通常是二值的（是否有该特征），因此使用指示函数表示特征\n$$\nP(c│x) = \\frac {exp⁡(\\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\\sum _{c^{\\*} \\in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i (c^{\\*},x))  }\n$$\n-\t注意到在该模型中每一个类都有其独立的线性权重w_c。相比于硬分布，最大熵模型能够给出分到每一类的概率，因此可以求出每一时刻的分类概率进而求出整体分类概率，得到全局最优分类结果。注意到不同于支持向量机等模型，最大熵模型无法利用特征之间的组合，必须手动构造组合作为新的特征。\n-\t一般使用加了正则化的最大似然作为优化的目标函数：\n$$\nw ̂={argmax} _w \\sum _i   \\log P(y^{(i)}│x^{(i) } ) - \\alpha \\sum _{j=1}^N w_j^2  \n$$\n-\t这种正则化相当于给权重的概率分布加了一个零均值高斯先验，权重越偏离均值，即权重越大，其概率越低。\n-\t为什么多分类Logistic回归是最大熵模型：最大熵模型保证在满足给定约束下，无约束的部分分类应该是等概率分配，例如在两个约束下：\n$$\nP(NN)+P(JJ)+P(NNS)+P(VB)=1 \\\\\nP(t_i=NN or t_i=NNS)=8/10 \\\\\n$$\n-\t则满足这两个约束，最大熵模型分配的概率结果为：\n$$\np(NN)=4/10  \\\\\np(JJ)=1/10  \\\\\np(NNS)=4/10  \\\\\np(VB)=1/10 \\\\\n$$\n-\t在The equivalence of logistic regression and maximum entropy models一文中证明了在广义线性回归模型的平衡条件约束下，满足最大熵分布的非线性激活函数就是sigmoid，即logistic回归。\n\n## 最大熵马尔可夫模型\n-\t最大熵模型只能对单一观测量分类，使用最大熵马尔可夫模型可以将其扩展到序列分类问题上。\n-\t最大熵马尔可夫比隐马尔科夫模型好在哪儿？隐马尔科夫模型对于每个观测量的分类依赖于转移概率和生成概率，假如我们想要在标注过程中引入外部知识，则需要将外部知识编码进这两类概率中，不方便。最大熵马尔可夫模型能够更简单的引入外部知识。\n-\t在隐马尔科夫模型中我们优化似然，并且乘以先验来估计后验：\n$$\nT ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )   \n$$\n-\t在最大熵隐马尔科夫模型中，我们直接计算后验。因为我们直接训练模型来分类，即最大熵马尔可夫模型是一类判别模型，而不是生成模型：\n$$\nT ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1}) \n$$\n-\t因此在最大熵隐马尔科夫模型中没有分别对似然和先验建模，而是通过一个单一的概率模型来估计后验。两者的区别如下图所示：\n![FoZgrF.png](https://s2.ax1x.com/2019/01/03/FoZgrF.png) \n-\t另外最大熵马尔可夫模型可以依赖的特征更多，依赖方式更灵活，如下图：\n![FoZcKU.png](https://s2.ax1x.com/2019/01/03/FoZcKU.png)\n-\t用公式表示这一差别：\n$$\nHMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\\\\nMEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\\\\n$$\n-\t当估计单一转移概率（从状态q^{\\*}转移到状态q，产生观测量o）时，我们使用以下的最大熵模型：\n$$\nP(q│q^{\\*},o)=\\frac{1}{Z(o,q^{\\*})} exp⁡(\\sum _i   w_i f_i (o,q)) \n$$\n\n## 最大熵马尔可夫模型的解码（推断）\n-\tMEMM同样使用维特比算法进行解码\n-\t使用维特比算法解码的通用框架是：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j) \n$$\n-\t在HMM模型中这一框架具体化为：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t) \n$$\n-\t在MEMM中直接将似然和先验替换为后验：\n$$\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t) \n$$\n\n## 最大熵马尔可夫模型的训练\n-\tMEMM作为最大熵模型的推广，训练过程使用和最大熵模型一样的监督算法。如果训练数据的标签序列存在缺失，也可以通过EM算法进行半监督学习。\n\n\n# 第十二章：英语的形式语法\n## 组成性\n-\t英语中的单词是如何组成一个词组的呢？\n-\t换句话说，我们如何判断一些单词组合成了一个部分？一种可能是这种组合都能在相似的句法环境中出现，例如名词词组都能在一个动词之前出现。另一种可能依据来自于前置和后置结构，例如前置短语on September seventeenth可以放在句子的前面，中间或者后面，但是组合成这个短语的各个部分不能拆出来放在句子的不同位置，因此我们判断on September seventeenth这三个词组成了一个短语。\n\n## 上下文无关法则\n-\t上下文无关语法，简称CFG，又称为短语结构语法，其形式化方法等价于Backus-Naur范式。一个上下文无关语法包含两个部分：规则或者产生式，词表。\n-\t例如，用上下文无关语法描述名词词组，一种描述方式是名词词组可以由一个专有名词构成，也可以由一个限定词加一个名词性成分构成，而名词性成分可以是一个或多个名词，此CFG的规则为：\n\t-\tNP→Det Nominal\n\t-\tNP→ProperNoun\n\t-\tNominal→Noun|Noun Nominal\n-\tCFG可以层级嵌套，因此上面的规则可以与下面表示词汇事实的规则（词表）结合起来：\n\t-\tDet→a\n\t-\tDet→the\n\t-\tNoun→flight\n-\t符号分为两类：\n\t-\t终极符号：与现实中单词对应的符号，词表是引入终极符号的规则的集合\n\t-\t非终极符号：表示终极符号的聚类或者概括性符号\n-\t在每个规则里箭头右边包含一个或多个终极符号和非终极符号，箭头左边为一个非终极符号，与每个单词相关联的是其词类范畴（词类）。\n-\tCFG既可以看成是生成句子的一种机制，也可以看成是给一个句子分配结构的机制。\n-\t以之前提到的CFG为例，对一个符号串NP，可以逐步生成：\n$$\nNP→Det Nominal→Det Noun→a flight\n$$\n-\t称 a flight是NP的一个推导，一般用一个剖析树表示一种推导：\n![FoZ5P1.png](https://s2.ax1x.com/2019/01/03/FoZ5P1.png)\n一个CFG定义了一个形式语言，形式语言是符号串的集合，如果有一个语法推导出的句子处于由该语法定义的形式语言中，这个句子就是合语法的。使用形式语言来模拟自然语言的语法成为生成式语法。\n-\t上下文无关语法的正式定义：\n\t-\tN：非终止符号（或者变量）的集合\n\t-\tSigma：终止符号的集合，与N不相交\n\t-\tR：规则或者产生式的集合\n\t-\tS：指定的开始符号\n-\t一些约定定义：\n\t-\t大写字母：代表非终止符号\n\t-\tS：开始符号\n\t-\t小写希腊字母：从非终止符号和终止符号的并集中抽取出来的符号串\n\t-\t小写罗马字母：终止符号串\n-\t直接导出的定义：\n**公式待补充**\n-\t导出是直接导出的泛化。之后我们可以正式定义由语法G生成的语言L是一个由终止符号组成的字符串集合，这些终止符号可以从指定的开始符号S通过语法G导出：\n**公式待补充**\n-\t将一个单词序列映射到其对应的剖析树成为句法剖析。\n\n## 英语的一些语法规则\n-\t英语中最常用最重要的四种句子结构：\n\t-\t陈述式结构：主语名词短语加一个动词短语\n\t-\t命令式结构：通常以一个动词短语开头，并且没有主语\n\t-\tYes-no疑问式结构：通常用于提问，并且以一个助动词开头，后面紧跟一个主语NP，再跟一个VP\n\t-\tWh疑问式结构：包含一个wh短语成分\n-\t在之前的描述中开始符号用于单独生成整个句子，但是S也可以出现在语法生成规则的右边，嵌入到更大的句子当中。这样的S称为从句，拥有完整的语义。拥有完整的语义是指这个S在整体句子的语法剖析树当中，其子树当中的主要动词拥有所需的所有论元。\n\n## 名词短语\n-\t限定词Det：名词短语可以以一些简单的词法限定词开始，例如a,the,this,those,any,some等等，限定词的位置也可以被更复杂的表示替代，例如所有格。这样的表示是可以递归定义的，例如所有格加名词短语可以构成更大的名词短语的限定词。在复数名词、物质名词之前不需要加限定词。\n-\t名词性词Nominal：包含一些名词前或者名词后修饰语\n-\t名词之前，限定词之后：一些特殊的词类可以出现在名词之前限定词之后，包括基数词Card、序数词Ord、数量修饰语Quant。\n-\t形容词短语AP：形容词短语之前可以出现副词\n-\t可以讲名词短语的前修饰语规则化如下（括号内代表可选）：\n-\tNP->(Det)(Card)(Ord)(Quant)(AP)Nominal\n-\t后修饰语主要包含三种：\n\t-\t介词短语PP：Nominal->Nominal PP(PP)(PP)\n\t-\t非限定从句：动名词后修饰语GerundVP,GerundVP->GerundV NP | GerundV PP | GerundV | GerundV NP PP\n\t-\t关系从句：以关系代词开头的从句 Nominal ->Nominal RelCaluse;RelCaluse -> (who|that) VP\n\n## 一致关系\n-\t每当动词有一个名词作为它的主语时，就会发生一致关系的现象，凡是主语和他的动词不一致的句子都是不合语法的句子，例如第三人称单数动词没有加-s。可以使用多个规则的集合来扩充原有的语法，使得语法可以处理一致关系。例如yes-no疑问句的规则是\n$$\nS \\rightarrow Aux \\ NP \\ VP\n$$\n-\t可以用如下形式的两个规则来替代：\n$$\nS \\rightarrow 3sgAux \\ 3sgNP \\ VP \\\\\nS \\rightarrow Non3sgAux \\ Non3sgNP \\ VP \\\\\n$$\n-\t再分别指定第三人称单数和非第三人称单数的助动词形态。这样的方法会导致语法规模增加。\n\n## 动词短语和次范畴化\n-\t动词短语包括动词和其他一些成分的组合，包括NP和PP以及两者的组合。整个的嵌入句子也可以跟随在动词之后，成为句子补语。\n-\t动词短语的另一个潜在成分是另一个动词短语。\n-\t动词后面也可以跟随一个小品词，小品词类似于借此，但与动词组合在一起是构成一个短语动词，与动词不可分割。\n-\t次范畴化即再分类。传统语法把动词次范畴化为及物动词和不及物动词，而现代语法已经把动词区分为100个次范畴。讨论动词和可能的成分之间的关系是将动词看成一个谓词，而成分想象成这个谓词的论元(argument)。\n-\t对于动词和它的补语之间的关系，我们可以用上下文无关语法表示一致关系特征，且需要区分动词的各个次类。\n\n## 助动词\n-\t助动词是动词的一个次类，具有特殊的句法约束。助动词包括情态动词、完成时助动词、进行时助动词、被动式助动词。每一个助动词都给他后面的动词形式一个约束，且需要按照一定的顺序进行结合。\n-\t四种助动词给VP次范畴化时，VP的中心动词分别是光杆动词、过去分词形式、现在分词形式、过去分词形式。\n-\t一个句子可以用多个助动词，但是要按照情态助动词、完成时助动词、进行式助动词、被动式助动词的顺序。\n\n## 树图资料库\n-\t上下文无关语法可以将一个句子剖析成一个句法剖析树，如果一个语料中所有句子都以句法剖析树的形式表示，这样的句法标注了的语料就称为树图资料库(treebank)。\n-\t树图资料库中的句子隐含的组成了一种语言的语法，我们可以对于每一棵句法剖析树提取其中的CFG规则。从宾州树库中提取出来的CFG规则非常扁平化，使得规则数量很多且规则很长。\n-\t在树库中搜索需要一种特殊的表达式，能够表示关于节点和连接的约束，用来搜索特定的模式。例如tgrep或者TGrep2。\n-\t在tgrep、TGrep2中的一个模式由一个关于节点的描述组成，一个节点描述可以用来返回一个以此节点为根的子树。\n-\t可以使用双斜线对某一类模式命名：\n$$\n/NNS?/\tNN|NNS\n$$\n-\tTgrep/Tgrep2模式的好处在于能够描述连接的信息。小于号代表直接支配，远小于符号代表支配，小数点代表线性次序。这种对于连接的描述反应在剖析树中的关系如下：\n![FoZ2b4.png](https://s2.ax1x.com/2019/01/03/FoZ2b4.png)\n \n## 中心词和中心词查找\n-\t句法成分能够与一个词法中心词相关联。在一个简单的词法中心词模型中，每一个上下文无关规则与一个中心词相关联，中心词传递给剖析树，因此剖析树中每一个非终止符号都被一个单一单词所标注，这个单一单词就是这个非终止符号的中心词。一个例子如下：\n![FoZfa9.png](https://s2.ax1x.com/2019/01/03/FoZfa9.png)\n-\t为了生成这样一棵树，每一个CFG规则都必须扩充来识别一个右手方向的组成成分来作为中心词子女节点。一个节点的中心词词被设置为其子女中心词的中心词。\n-\t另一种方式是通过一个计算系统来完成中心词查找。在这种方式下是依据树的上下文来寻找指定的句子，从而动态的识别中心词。一旦一个句子被解析出来，树将会被遍历一遍并使用合适的中心词来装饰每一个节点。\n\n## 语法等价与范式\n-\t语法等价包括两种：强等价，即两个语法生成相同的符号串集合，且他们对于每个句子都指派同样的短语结构；弱等价，即两个语法生成相同的符号串集合，但是不给每个句子指派相同的短语结构。\n-\t语法都使用一个范式，在范式中每个产生式都使用一个特定的形式。例如一个上下文五官与法是sigma自由的，并且如果他们的每个产生式的形式为A->BC或者是A->a，就说明这个上下文无关语法是符合Chomsky范式的，简称CNF。凡是Chomsky范式的语法都具有二叉树形式。任何上下文无关语法都可以转变成一个弱等价的Chomsky范式语法。\n-\t使用二叉树形式的剖析树能够产生更小的语法。形如A->A B的规则称为Chomsky并连。\n\n## 有限状态语法和上下文无关语法\n-\t复杂的语法模型必须表示组成性，因而不适合用有限状态模型来描述语法。\n-\t当一个非终止符号的展开式中也包含了这个非终止符号时，就会产生语法的递归问题。\n-\t例如，使用正则表达式来描述以Nominal为中心的名词短语：\n(Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*\n-\t为了完成这个正则表达式，只需要按顺序展开PP，展开结果为(P NP)*，这样就出现了地柜问题，因为此时出现了NP，在NP的正则表达式中出现了NP。\n-\t一个上下文无关语法能够被有限自动机生成，当且仅当存在一个生成语言L的没有任何中心自嵌入递归的上下文无关语法。\n\n## 依存语法\n-\t依存语法与上下文无关语法相对，其句法结构完全由词、词与词之间的语义或句法关系描述。一个例子如下：\n![FoZOVH.png](https://s2.ax1x.com/2019/01/03/FoZOVH.png)\n-\t其中没有非终止符号或者短语节点，树中的连接只将两个词语相连。连接即依存关系，代表着语法功能或者一般的语义联系，例如句法主语、直接对象、间接宾语、时间状语等等。\n-\t依存语法具有很强的预测剖析能力，且在处理具有相对自由词序的语言时表现更好。\n\n# 第十三章：基于上下文无关语法的剖析\n## 剖析即搜索\n\n-\t在句法剖析中，剖析可以看成对一个句子搜索一切可能的剖析树空间并发现正确的剖析树。\n-\t对于某一个句子（输入符号串），剖析搜索的目标是发现以初始符号S为根并且恰好覆盖整个输入符号串的一切剖析树。搜索算法的约束来自两方面：\n\t-\t来自数据的约束，即输入句子本身，搜索出来的剖析树的叶子应该是原句的所有单词。\n\t-\t来自语法的约束，搜索出来的剖析树应该有一个根，即初始符号S\n-\t根据这两种约束，产生了两种搜索策略：自顶向下，目标制导的搜索；自下而上，数据制导的搜索。\n-\t对于自顶向下的搜索，从根开始，我们通过生成式不断生成下一层的所有可能子节点，搜索每一层的每一种可能，如下图（对于句子book that flight）：\n![FoZh5R.png](https://s2.ax1x.com/2019/01/03/FoZh5R.png)\n-\t对于自底向上的搜索，剖析从输入的单词开始，每次都使用语法中的规则，试图从底部的单词向上构造剖析树，如果剖析树成功的构造了以初始符号S为根的树，而且这个树覆盖了整个输入，那么就剖析成功。首先通过词表将每个单词连接到对应的词类，如果一个单词有不止一个词类，就需要考虑所有可能。与自顶向下相反，每次进入下一层时，自底向上需要考虑被剖析的成分是否与某个规则的右手边相匹配，而自顶向下是与左手边相匹配。中途如果无法匹配到规则则将这个树枝从搜索空间中删除，如下图所示：\n![FoZI8x.png](https://s2.ax1x.com/2019/01/03/FoZI8x.png) \n-\t两者对比：\n\t-\t自顶向下是从S开始搜索的，因此不会搜索那些在以S为根的树中找不到位置的子树，而自底向上会产生许多不可能的搜索树\n\t-\t相对应的，自顶向下把搜索浪费在了不可能产生输入单词序列的树上\n\t-\t综上，我们需要将自顶向下和自底向上相结合\n\n## 歧义\n-\t在句法剖析中需要解决的一个问题是结构歧义，即语法会给一个句子多种剖析结果可能。\n-\t最常见的两种歧义：附着歧义和并列连接歧义。\n-\t如果一个特定的成分可以附着在剖析树的一个以上的位置，句子就会出现附着歧义。例如We saw the Eiffel Tower flying to Paris一句中,flying to Paris可以修饰Eiffel Tower也可以修饰We。\n-\t在并列连接歧义中，存在着不同的短语，这些短语之间用and这样的连接词相连。例如old men and women可以是老年男性和老年女性，或者老年男性和普通女性，即old是否同时分配到men和women上。\n-\t以上两种歧义还能相互组合嵌套形成更复杂的歧义。假如我们不消歧，仅仅返回所有的可能，留给用户或者人工判断，则随着剖析句子结构变复杂或者剖析规则的增加，得到的可能是成指数级增长的，具体而言，这种剖析句子可能的增长数和算术表达式插入括号问题相同，以Catalan数按指数增长：\n$$\nC(n)=\\frac{1}{1+n} C_{2n}^n\n$$\n-\t摆脱这种指数爆炸的方法有两个：\n\t-\t动态规划，研究搜索空间的规律性，使得常见的部分只推导一次，减少与歧义相关的开销\n\t-\t使用试探性的方法来改善剖析器的搜索策略\n-\t使用例如深度优先搜索或者宽度优先搜索之类的有计划与回溯的搜索算法是在复杂搜索空间中搜索常用的算法，然而在复杂语法空间中无处不在的歧义使得这一类搜索算法效率低下，因为有许多重复的搜索过程。\n\n## 动态规划剖析方法\n-\t在动态规划中，我们维护一个表，系统的将对于子问题的解填入表中，利用已经存储的子问题的解解决更大的子问题，而不用重复从头开始计算。\n-\t在剖析中，这样的表用来存储输入中各个部分的子树，当子树被发现时就存入表中，以便以后调用，就这样解决了重复剖析的问题（只需查找子树而不需要重新剖析）和歧义问题（剖析表隐含的存储着所有可能的剖析结果）。\n-\t主要的三种动态规划剖析方法有三种，CKY算法、Earley算法和表剖析算法。\n\n### CKY剖析\n-\tCKY剖析要求语法必须满足Chomsky范式，即生成式右边要么时两个非终止符号要么是一个终止符号。如果不是Chomsky范式，则需要把一个一般的CFG转换成CNF：\n\t-\t右边有终止符号也有非终止符号：给右边的终止符号单独建一个非终止符号，例如：INF-VP → to VP，改成INF-VP → TO VP和TO → to\n\t-\t右边只有一个非终止符号：这种非终止符号称为单元产物，它们最终会生成非单元产物，用最终生成的非单元产物规则来替换掉单元产物\n\t-\t右边不止2个符号：引入新的非终止符号将规则分解\n\t-\t词法规则保持不变，但是在转换的过程中可能会生成新的词法规则\n-\t当所有的规则都转换成CNF之后，表中的非终止符号在剖析中有两个子节点，且表中每一个入口代表了输入中的某个区间，对于某个入口例如[0,3]，其可以被拆分成两部分，假如一部分为[0,2]，则另一部分为[2,3]，前者在[0,3]的左边，后者在[0,3]的正下方，如下图：\n![FoZo26.png](https://s2.ax1x.com/2019/01/03/FoZo26.png)\n-\t接下来就是如何填表，我们通过自底向上的方法来剖析，对于每个入口[i,j]，包含了输入中i到j这一区间部分的表格单元都会对这个入口值做出贡献，即入口[i,j]左边的单元和下边的单元。下表中的CKY伪算法图描述了这一过程：\n![FoZjIA.png](https://s2.ax1x.com/2019/01/03/FoZjIA.png)\n-\t外层循环从左往右循环列，内层循环从下往上循环行，而最里面的循环式遍历串[i,j]的所有可能二分子串，表中存的是可以代表[i,j]区间符号串的非终止符号集合，因为是集合，所以不会出现重复的非终止符号。\n-\t现在我们完成了识别任务，接下来是剖析。剖析即在[0,N]入口，对应整个句子，找到一个非终止符号作为起始符号S。首先我们要对算法做两点更改：\n\t-\t存入表中的不仅仅是非终止符号，还有其对应的指针，指向生成这个非终止符号的表入口\n\t-\t允许一个入口中存在同一个非终止符号的不同版本\n-\t做了这些改动之后，这张表就包含了一个给定输入的所有可能剖析信息。我们可以选择[0,N]入口中任意一个非终止符号作为起始符号S，然后根据指针迭代提取出剖析信息。\n-\t当然，返回所有的可能剖析会遇到指数爆炸的问题，因此我们在完整的表上应用维特比算法，计算概率最大的剖析并返回这个剖析结果。\n\n### Early算法\n-\t相比CKY自底向上的剖析，Early算法采用了自顶向下的剖析，而且只用了一维的表保存状态，每个状态包含三类信息：\n\t-\t对应某一单一语法规则的子树\n\t-\t子树的完成状态\n\t-\t子树对应于输入中的位置\n-\t算法流程图如下：\n![FoZHKO.png](https://s2.ax1x.com/2019/01/03/FoZHKO.png)\n-\t算法对于状态的操作有三种：\n\t-\t预测：造出一个新的状态来表示在剖析过程中生成的自顶向下的预测。当待剖析的状态为非终极符号但又不是词类范畴时，对于这个非终极符号的不同展开，预测操作都造出一个新的状态。\n\t-\t扫描：当待剖析的状态是词类范畴时，就检查输入符号串，并把对应于所预测的词类范畴的状态加入线图中。\n\t-\t完成：当右边所有状态剖析完成时，完成操作查找输入中在这个位置的语法范畴，发现并推进前面造出的所有状态。\n\n### 表剖析\n-\t表剖析允许动态的决定表格处理的顺序，算法动态的依照计划依次删除图中的一条边，而计划中的元素排序是由规则决定的。\n![FoZTxK.png](https://s2.ax1x.com/2019/01/03/FoZTxK.png)\n\n## 部分剖析\n-\t有时我们只需要输入句子的部分剖析信息\n-\t可以用有限状态自动机级联的方式完成部分剖析，这样会产生比之前提到的方法更加“平”的剖析树。\n-\t另一种有效的部分剖析的方法是分块。使用最广泛覆盖的语法给句子做词类标注，将其分为有主要词类标注信息且不没有递归结构的子块，子块之间不重叠，就是分块。\n-\t我们用中括号将每一个分块框起来，有可能一些词并没有被框住，属于分块之外。\n-\t分块中最重要的是基本分块中不能递归包含相同类型的成分。\n\n### 基于规则的有限状态分块\n-\t利用有限状态方式分块，需要为了特定目的手动构造规则，之后从左到右，找到最长匹配分块，并接着依次分块下去。这是一个贪心的分块过程，不保证全局最优解。\n-\t这些分块规则的主要限制是不能包含递归。\n-\t使用有限状态分块的优点在于可以利用之前转录机的输出作为输入来组成级联，在部分剖析中，这种方法能够有效近似真正的上下文无关剖析器。\n\n### 基于机器学习的分块\n-\t分块可以看成序列分类任务，每个位置分类为1（分块）或者0（不分块）。用于训练序列分类器的机器学习方法都能应用于分块中。\n-\t一种卓有成效的方法是将分块看成类似于词类标注的序列标注任务，用一个小的标注符号集同时编码分块信息和每一个块的标注信息，这种方式称为IOB标注，用B表示分块开始，I表示块内，O表示块外。其中B和I接了后缀，代表该块的句法信息。\n-\t机器学习需要训练数据，而分块的已标数据很难获得，一种方法是使用已有的树图资料库，例如宾州树库。\n\n### 评价分块系统\n-\t准确率：模型给出的正确分块数/模型给出的总分块数\n-\t召回率：模型给出的正确分块数/文本中总的正确分块数\n-\tF1值：准确率和召回率的调和平均\n\n\n\n# 第十四章：统计剖析\n## 概率上下文无关语法\n-\t概率上下文无关语法PCFG是上下文无关语法的一种简单扩展，又称随机上下文无关语法。PCFG在定义上做出了一点改变：\n\t-\tN：非终止符号集合\n\t-\tΣ：终止符号集合\n\t-\tR：规则集合，与上下文无关语法相同，只不过多了一个概率p，代表某一项规则执行的条件概率$P(\\beta|A)$\n\t-\tS：一个指定的开始符号\n-\t当某个语言中所有句子的概率和为1时，我们称这个PCFG时一致的。一些递归规则可能导致PCFG不一致。\n\n## 用于消歧的PCFG\n-\t对于一个给定句子，其某一特定剖析的概率是所有规则概率的乘积，这个乘积既是一个剖析的概率，也是剖析和句子的联合概率。这样，对于出现剖析歧义的句子，其不同剖析的概率不同，通过选择概率大的剖析可以消歧。\n\n## 用于语言建模的PCFG\n-\tPCFG为一个句子分配了一个概率（即剖析的概率），因此可以用于语言建模。相比n元语法模型，PCFG在计算生成每一个词的条件概率时考虑了整个句子，效果更好。对于含歧义的句子，其概率是所有可能剖析的概率之和。\n\n## PCFG的概率CKY剖析\n-\tPCFG的概率剖析问题：为一个句子产生概率最大的剖析\n-\t概率CKY算法扩展了CKY算法，CKY剖析树中的每一个部分被编码进一个$(n+1)\\*(n+1)$的矩阵（只用上三角部分），矩阵中每一个元素包含一个非终止符号集合上的概率分布，可以看成每一个元素也是V维，因此整个存储空间为$(n+1)\\*(n+1)\\*V$，其中[i,j,A]代表非终止符号A可以用来表示句子的i位置到j位置这一段的概率。\n-\t算法伪代码：\n![FoZbrD.png](https://s2.ax1x.com/2019/01/03/FoZbrD.png)\n-\t可以看到也是用k对某一区间[i,j]做分割遍历，取最大的概率组合作为该区间的概率，并向右扩展区间进行动态规划。\n\n## 学习到PCFG的规则概率\n-\t上面的伪算法图用到了每一个规则的概率。如何获取这个概率？两种方法，第一种朴素的方法是在一个已知的树库数据集上用古典概型统计出概率：\n$$\nP(\\alpha \\rightarrow \\beta | \\alpha) = \\frac{Count(\\alpha \\rightarrow \\beta)}{\\sum _{\\gamma} Count(\\alpha \\rightarrow \\gamma)}\n$$\n-\t假如我们没有树库，则可以用非概率剖析算法来剖析一个数据集，再统计出概率。但是非概率剖析算法在剖析歧义句子时，需要对每一种可能剖析计算概率，但是计算概率需要概率剖析算法，这样就陷入了鸡生蛋蛋生鸡的死循环。一种解决方案是先用等概率的剖析算法，剖析句子，得出每一种剖析得概率，然后用概率加权统计量，然后重新估计剖析规则的概率，继续剖析，反复迭代直到收敛。这种算法称为inside-outside算法，是前向后向算法的扩展，同样也是EM算法的一种特例。\n\n## PCFG的问题\n-\t独立性假设导致不能很好的建模剖析树的结构性依存：每个PCFG规则被假定为与其他规则独立，例如，统计结果表明代词比名词更有可能称为主语，因此当NP被展开时，如果NP是主语，则展开为代词的可能性较高——这里需要考虑NP在句子种的位置，然而这种概率依存关系是PCFG所不允许的，\n-\t缺乏对特定单词的敏感，导致次范畴化歧义、介词附着、联合结构歧义的问题：例如在介词附着问题中，某一个介词短语into Afghanistan附着于哪一个部分，在PCFG中计算时被抽象化为介词短语应该附着一个哪一个部分，而抽象化的概率来自于对语料的统计，这种统计不会考虑特定的单词。又例如联合结构歧义，假如一个句子的两种可能剖析树使用了相同的规则，而规则在树中的位置不同，则PCFG对两种剖析计算出相同的概率：因为PCFG假定规则之间是独立的，联合概率是各个概率的乘积。\n\n## 通过拆分和合并非终止符号来改进PCFG\n-\t先解决结构性依存的问题。之前提到了我们希望NP作为主语和宾语时有不同概率的规则，一种想法就是将NP拆分成主语NP和宾语NP。实现这种拆分的方法是父节点标注，及每个节点标注了其父节点，对于主语NP其父节点是S，对于宾语NP，其父节点是VP，因此不同的NP就得到了区分。除此之外，还可以通过词性拆分的方式增强剖析树。\n-\t拆分会导致规则增多，用来训练每一条规则的数据变少，引起过拟合。因此要通过一个手写规则或者自动算法来根据每个训练集合并一些拆分。\n\n## 概率词汇化的CFG\n-\t概率CKY剖析更改了语法规则，而概率词汇化模型更改了概率模型本身。对于每一条规则，不仅要产生成分的规则变化，还要在每个成分上标注其中心词和词性，如下图：\n![FoeSRP.png](https://s2.ax1x.com/2019/01/03/FoeSRP.png)\n-\t为了产生这样的剖析树，每一条PCFG规则右侧需要选择一个成分作为中心词子节点，用子节点的中心词和词性作为该节点的中心词和词性。\n其中，规则被分成了两类，内部规则和词法规则，后者是确定的，前者是需要我们估计的：\n![FoZqqe.png](https://s2.ax1x.com/2019/01/03/FoZqqe.png)\n-\t我们可以用类似父节点标注的思想来拆分规则，拆分后每一部分都对应一种可能的中心词选择。假如我们将概率词汇话的CFG看成一个大的有很多规则CFG，则可以用之前的古典概型来估计概率。但是这样的效果不会很好，因为这样的规则划分太细了，没有足够的数据来估计概率。因此我们需要做出一些独立性假设，将概率分解为更小的概率乘积，这些更小的概率能容易从语料中估计出来。\n-\t不同的统计剖析器区别在于做出怎样的独立性假设。\n-\tCollins剖析如下图所示：\n![FoZzGt.png](https://s2.ax1x.com/2019/01/03/FoZzGt.png)\n-\t其概率拆解为：\n$$\nP(VP(dumped,VBD)→VBD(dumped,VBD)NP(sacks,NNS)PP(into,P))= \\\\\nP_H (VBD│VP,dumped)\\* \\\\\nP_L (STOP│VP,VBD,dumped)\\* \\\\\nP_R (NP(sacks,NNS)│VP,VBD,dumped)\\* \\\\\nP_R (PP(into,P)│VP,VBD,dumped)\\* \\\\\nP_R (STOP|VP,VBD,dumped) \\\\\n$$\n-\t给出生成式左边之后，首先生成规则的中心词，之后一个一个从里到外生成中心词的依赖。先从中心词左侧一直生成直到遇到STOP符号，之后生成右边。如上式做出概率拆分之后，每一个概率都很容易从较小的数据量中统计出来。完整的Collins剖析器更为复杂，还考虑了词的距离关系、平滑技术、未知词等等。\n\n## 评价剖析器\n-\t剖析器评价的标准方法叫做PARSEVAL测度，对于每一个句子s：\n\t-\t标记召回率=(Count(s的候选剖析中正确成分数）)/(Count(s的树库中正确成分数）)\n\t-\t标记准确率=(Count(s的候选剖析中正确成分数）)/(Count(s的候选剖析中全部成分数）)\n\n## 判别式重排序\n-\tPCFG剖析和Collins词法剖析都属于生成式剖析器。生成式模型的缺点在于很难引入任意信息，即很难加入对某一个PCFG规则局部不相关的特征。例如剖析树倾向于右生成这一特征就不方便加入生成式模型当中。\n-\t对于句法剖析，有两类判别式模型，基于动态规划的和基于判别式重排序的。\n-\t判别式重排包含两个阶段，第一个阶段我们用一般的统计剖析器产生前N个最可能的剖析及其对应的概率序列。第二个阶段我们引入一个分类器，将一系列句子以及每个句子的前N个剖析-概率对作为输入，抽取一些特征的大集合并针对每一个句子选择最好的剖析。特征包括：剖析概率、剖析树中的CFG规则、平行并列结构的数量、每个成分的大小、树右生成的程度、相邻非终止符号的二元语法、树的不同部分出现的次数等等。\n\n## 基于剖析的语言建模\n-\t使用统计剖析器来进行语言建模的最简单方式就是利用之前提到的二阶段算法。第一阶段我们运行一个普通的语音识别解码器或者机器翻译解码器（基于普通的N元语法），产生N个最好的候选；第二阶段，我们运行统计剖析器并为每一个候选句分配一个概率，选择概率最佳的。\n\n## 人类剖析\n-\t人类在识别句子时也用到了类似的概率剖析思想，两个例子：\n\t-\t对于出现频率高的二元语法，人们阅读这个二元语法所花的时间就更少\n\t-\t一些实验表明人类在消歧时倾向于选择统计概率大的剖析\n\n\n\n","slug":"coling","published":1,"updated":"2019-07-22T03:45:23.004Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyzl6vxt006iq8t50v9fu8mq","content":"<p>计算语言学课程笔记<br>参考教材：Speech and Language Processing：An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition<br>一些公式待修订<br><a id=\"more\"></a> </p>\n<h1 id=\"第二章：正则表达式与自动机\"><a href=\"#第二章：正则表达式与自动机\" class=\"headerlink\" title=\"第二章：正则表达式与自动机\"></a>第二章：正则表达式与自动机</h1><ul>\n<li>正则表达式：一种用于查找符合特定模式的子串或者用于以标准形式定义语言的工具，本章主要讨论其用于查找子串的功能。正则表达式用代数的形式来表示一些字符串集合。</li>\n<li>正则表达式接收一个模式，然后在整个语料中查找符合这个模式的子串，这个功能可以通过设计有限状态自动机实现。</li>\n<li>字符串看成符号的序列，所有的字符，数字，空格，制表符，标点和空格均看成符号。</li>\n</ul>\n<h2 id=\"基本正则表达式模式\"><a href=\"#基本正则表达式模式\" class=\"headerlink\" title=\"基本正则表达式模式\"></a>基本正则表达式模式</h2><ul>\n<li>用双斜线表示正则表达式开始和结束（perl中的形式）<ul>\n<li>查找子串，大小写敏感：/woodchuck/-&gt; woodchuck</li>\n<li>用方括号代表取其中一个，或：/[Ww]oodchuck/-&gt;woodchuck or Woodchuck</li>\n<li>方括号加减号，范围内取或：/[2-5]/-&gt;/[2345]</li>\n<li>插入符号放在左方括号后，代表模式中不出现后接的所有符号，取非: /^Ss/ -&gt;既不是大写S也不是小写s</li>\n<li>问号代表之前的符号出现一个或不出现：/colou?r/-&gt;color or colour</li>\n<li>星号代表之前的符号出现多个或不出现：/ba*/-&gt;b or ba or baa or baaa……</li>\n<li>加号代表之前的符号出现至少一次：/ba+/-&gt;ba or baa or baaa…….</li>\n<li>小数点代表通配符，与任何除了回车符之外的符号匹配：/beg.n/-&gt;begin or begun or beg’n or …….</li>\n<li>锚符号，用来表示特定位置的子串，插入符号代表行首，美元符号代表行尾，\\b代表单词分界线，\\B代表单词非分界线，perl将单词的定义为数字、下划线、字母的序列，不在其中的符号便可以作为单词的分界。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"析取、组合和优先\"><a href=\"#析取、组合和优先\" class=\"headerlink\" title=\"析取、组合和优先\"></a>析取、组合和优先</h2><ul>\n<li>用竖线代表析取，字符串之间的或：/cat|dog/-&gt;cat or dog</li>\n<li>用圆括号代表部分析取（组合），圆括号内也可以用基本算符：/gupp(y|ies)/-&gt;guppy or guppies</li>\n<li>优先级：圆括号&gt;计数符&gt;序列与锚&gt;析取符</li>\n</ul>\n<h2 id=\"高级算符\"><a href=\"#高级算符\" class=\"headerlink\" title=\"高级算符\"></a>高级算符</h2><ul>\n<li>\\d：任何数字</li>\n<li>\\D：任何非数字字符</li>\n<li>\\w：任何字母、数字、空格</li>\n<li>\\W：与\\w相反</li>\n<li>\\s：空白区域</li>\n<li>\\S：与\\s相反</li>\n<li>{n}：前面的模式出现n个</li>\n<li>{n,m}：前面的模式出现n到m个</li>\n<li>{n,}：前面的模式至少出现n个</li>\n<li>\\n：换行</li>\n<li>\\t：表格符</li>\n</ul>\n<h2 id=\"替换、寄存器\"><a href=\"#替换、寄存器\" class=\"headerlink\" title=\"替换、寄存器\"></a>替换、寄存器</h2><ul>\n<li>替换s/A/B/：A替换成B</li>\n<li>s/(A)/&lt;\\1&gt;/：用数字算符\\1指代A，在A的两边加上尖括号</li>\n<li>在查找中也可以用数字算符，指代圆括号内内容，可以多个算符指代多个圆括号内内容</li>\n<li>这里数字算符起到了寄存器的作用</li>\n</ul>\n<h2 id=\"有限状态自动机\"><a href=\"#有限状态自动机\" class=\"headerlink\" title=\"有限状态自动机\"></a>有限状态自动机</h2><ul>\n<li>有限状态自动机和正则表达式彼此对称，正则表达式是刻画正则语言的一种方法。正则表达式、正则语法和自动状态机都是表达正则语言的形式。FSA用有向图表示，圆圈或点代表状态，箭头或者弧代表状态转移，用双圈表示最终状态，如下图表示识别/baa+!/的状态机图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVj3V.png\" alt=\"FoVj3V.png\"></li>\n<li>状态机从初始状态出发，依次读入符号，若满足条件，则进行状态转移，若读入的符号序列满足模式，则状态机可以到达最终状态；若符号序列不满足模式，或者自动机在某个非最终状态卡住，则称自动机拒绝了此次输入。</li>\n<li>另一种表示方式是状态转移表：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVqNn.png\" alt=\"FoVqNn.png\"></li>\n<li>一个有限自动机可以用5个参数定义：<ul>\n<li>$Q$：状态{q_i}的有限集合</li>\n<li>\\sum ：有限的输入符号字母表</li>\n<li>$q_0$：初始状态</li>\n<li>$F$：终极状态集合</li>\n<li>$\\delta (q,i)$：状态之间的转移函数或者转移矩阵，是从$Q × \\Sigma$到$2^Q$的一个关系</li>\n</ul>\n</li>\n<li>以上描述的自动机是确定性的，即DFSA，在已知的记录在状态转移表上的状态时，根据查表自动机总能知道如何进行状态转移。算法如下，给定输入和自动机模型，算法确定输入是否被状态机接受：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZpB4.png\" alt=\"FoZpB4.png\"></li>\n<li>当出现了表中没有的状态时自动机就会出错，可以添加一个失败状态处理这些情况。</li>\n</ul>\n<h2 id=\"形式语言\"><a href=\"#形式语言\" class=\"headerlink\" title=\"形式语言\"></a>形式语言</h2><ul>\n<li>形式语言是一个模型，能且只能生成和识别一些满足形式语言定义的某一语言的符号串。形式语言是一种特殊的正则语言。通常使用形式语言来模拟自然语言的某些部分。以上例/baa+!/为例，设对应的自动机模型为m，输入符号表$\\Sigma = {a,b,!}$，$L(m)$代表由m刻画的形式语言，是一个无限集合${baa!,baaa!,baaaa!,…}$</li>\n</ul>\n<h2 id=\"非确定有限自动机\"><a href=\"#非确定有限自动机\" class=\"headerlink\" title=\"非确定有限自动机\"></a>非确定有限自动机</h2><ul>\n<li>非确定的有限自动机NFSA,把之前的例子稍微改动，自返圈移动到状态2，就形成了NFSA，因为此时在状态2，输入a，有两种转移可选，自动机无法确定转移路径：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVLhq.png\" alt=\"FoVLhq.png\"></li>\n<li>另一种NFSA的形式是引入$\\epsilon$转移，即不需要输入符号也可以通过此$\\epsilon$转移进行转移，如下图，在状态3时依然不确定如何进行转移：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVX90.png\" alt=\"FoVX90.png\"></li>\n<li>在NFSA时，面临转移选择时自动机可能做出错误的选择，此时存在三种解决方法：<ul>\n<li>回退：标记此时状态，当确定发生错误选择之后，回退到此状态</li>\n<li>前瞻：在输入中向前看，帮助判定进行选择</li>\n<li>并行：并行的进行所有可能的转移</li>\n</ul>\n</li>\n<li>在自动机中，采用回退算法时需要标记的状态称为搜索状态，包括两部分：状态节点和输入位置。对于NFSA，其状态转移表也有相应改变，如图，添加了代表$\\epsilon$转移的$\\epsilon$列，且转移可以转移到多个状态：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZE36.png\" alt=\"FoZE36.png\"></li>\n<li>采用回退策略的非确定自动机算法如下，是一种搜索算法：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZSuF.png\" alt=\"FoZSuF.png\"></li>\n<li>子函数GENERATE-NEW-STATES接受一个搜索状态，提取出状态节点和输入位置，查找这个状态节点上的所有状态转移可能，生成一个搜索状态列表作为返回值；</li>\n<li>子函数ACCEPT-STATE接受一个搜索状态，判断是否接受，接受时的搜索状态应该是最终状态和输入结束位置的二元组。</li>\n<li>算法使用进程表（agenda）记录所有的搜索状态，初始只包括初始的搜索状态，即自动机初始状态节点和输入起始。之后不断循环，从进程表中调出搜索状态，先调用ACCEPT-STATE判断是否搜索成功，之后再调用GENERATE-NEW-STATES生成新的搜索状态加入进程表。循环直到搜索成功或者进程表为空（所有可能转移均尝试且未成功）返回拒绝。</li>\n<li>可以注意到NFSA算法就是一种状态空间搜索，可以通过改变搜索状态的顺序提升搜索效率，例如用栈实现进程表，进行深度优先搜索DFS；或者使用队列实现进程表，进行宽度优先搜索BFS。</li>\n<li>对于任何NFSA，存在一个完全等价的DFSA。</li>\n</ul>\n<h2 id=\"正则语言和NFSA\"><a href=\"#正则语言和NFSA\" class=\"headerlink\" title=\"正则语言和NFSA\"></a>正则语言和NFSA</h2><ul>\n<li>定义字母表\\sum 为所有输入符号集合；空符号串$\\epsilon$，空符号串不包含再字母表中；空集∅。在\\sum 上的正则语言的类（或者正则集）可以形式的定义如下：<ul>\n<li>∅是正则语言</li>\n<li>∀a ∈ $\\sum$ ∪$\\epsilon$,{a}是形式语言</li>\n<li>如果$L_1$和$L_2$是正则语言，那么：</li>\n<li>$L_1$和$L_2$的拼接是正则语言</li>\n<li>$L_1$和$L_2$的合取、析取也是正则语言</li>\n<li>$L_1$^*，即$L_1$的Kleene闭包也是正则语言</li>\n</ul>\n</li>\n<li>可见正则语言的三种基本算符：拼接、合取及析取、Kleene闭包。任何正则表达式可以写成只使用这三种基本算符的形式。</li>\n<li>正则语言对以下运算也封闭（$L_1$和$L_2$均为正则语言）：<ul>\n<li>交：$L_1$和$L_2$的符号串集合的交构成的语言也是正则语言</li>\n<li>差：$L_1$和$L_2$的符号串集合的差构成的语言也是正则语言</li>\n<li>补：不在$L_1$的符号串集合中的集合构成的语言也是正则语言</li>\n<li>逆：$L_1$所有符号串的逆构成的集合构成的语言也是正则语言</li>\n</ul>\n</li>\n<li>可以证明正则表达式和自动机等价，一个证明任何正则表达式可以建立对应的自动机的方法是，根据正则语言的定义，构造基础自动机代表$\\epsilon$、∅以及$\\sum$中的单个符号a，然后将三种基本算符表示为自动机上的操作，归纳性的，在基础自动机上应用这些操作，得到新的基础自动机，这样就可以构造满足任何正则表达式的自动机，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVxjU.png\" alt=\"FoVxjU.png\"><br>基础自动机<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZPE9.png\" alt=\"FoZPE9.png\"><br>拼接算符<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ9HJ.png\" alt=\"FoZ9HJ.png\"><br>Kleene闭包算符<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZiNR.png\" alt=\"FoZiNR.png\"><br>合取析取算符</li>\n</ul>\n<h1 id=\"第三章：形态学与有限状态转录机\"><a href=\"#第三章：形态学与有限状态转录机\" class=\"headerlink\" title=\"第三章：形态学与有限状态转录机\"></a>第三章：形态学与有限状态转录机</h1><ul>\n<li>剖析：取一个输入并产生关于这个输入的各类结构</li>\n</ul>\n<h2 id=\"英语形态学概论\"><a href=\"#英语形态学概论\" class=\"headerlink\" title=\"英语形态学概论\"></a>英语形态学概论</h2><ul>\n<li>形态学研究词的构成，词可以进一步拆解为语素，语素可分为词干和词缀，词缀可分为前缀、中缀、后缀、位缀。</li>\n<li>屈折形态学：英语中，名词只包括两种屈折变化：一个词缀表示复数，一个词缀表示领属：<ul>\n<li>复数：-s，-es，不规则复数形式</li>\n<li>领属：-‘s，-s’</li>\n</ul>\n</li>\n<li>动词的屈折变化包括规则动词和非规则动词的变化：<ul>\n<li>规则动词：主要动词和基础动词，-s，-ing，-ed，</li>\n<li>非规则动词</li>\n</ul>\n</li>\n<li>派生形态学：派生将词干和一个语法语素结合起来，形成新的单词<ul>\n<li>名词化：-ation，-ee，-er，-ness</li>\n<li>派生出形容词：-al，-able，-less</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"形态剖析\"><a href=\"#形态剖析\" class=\"headerlink\" title=\"形态剖析\"></a>形态剖析</h2><ul>\n<li>例子：我们希望建立一个形态剖析器，输入单词，输出其词干和有关的形态特征，如下表，我们的目标是产生第二列和第四列：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZA9x.png\" alt=\"FoZA9x.png\"></li>\n<li>我们至少需要：<ul>\n<li>词表（lexicon）：词干和词缀表及其基本信息</li>\n<li>形态顺序规则（morphotactics）：什么样的语素跟在什么样的语素之后</li>\n<li>正词法规则（orthographic rule）：语素结合时拼写规则的变化</li>\n</ul>\n</li>\n<li>一般不直接构造词表，而是根据形态顺序规则，设计FSA对词干进行屈折变化生成词语。例如一个名词复数化的简单自动机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZmuD.png\" alt=\"FoZmuD.png\"></li>\n<li>其中reg-noun代表规则名词，可以通过加s形成复数形式，并且忽略了非规则单数名词(irreg-sg-noun)和非规则复数名词(irreg-pl-noun)。另外一个模拟动词屈折变化的自动机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZQUA.png\" alt=\"FoZQUA.png\"></li>\n<li>使用FSA解决形态识别问题（判断输入符号串是否合法）的一种方法是，将状态转移细分到字母层次，但是这样仍然会存在一些问题：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZZjO.png\" alt=\"FoZZjO.png\"></li>\n</ul>\n<h2 id=\"有限状态转录机\"><a href=\"#有限状态转录机\" class=\"headerlink\" title=\"有限状态转录机\"></a>有限状态转录机</h2><ul>\n<li>双层形态学：将一个词表示为词汇层和表层，词汇层表示该词语素之间的简单毗连（拼接，concatenation），表层表示单词实际最终的拼写，有限状态转录机是一种有限状态自动机，但其实现的是转录，实现词汇层和表层之间的对应，它有两个输入，产生和识别字符串对，每一个状态转移的弧上有两个标签，代表两个输入。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZVgK.png\" alt=\"FoZVgK.png\"></li>\n<li>从四个途径看待FST：<ul>\n<li>作为识别器：FST接受一对字符串，作为输入，如果这对字符串在语言的字符串对中则输出接受否则拒绝</li>\n<li>作为生成器：生成语言的字符串对</li>\n<li>作为翻译器：读入一个字符串，输出另一个</li>\n<li>作为关联器：计算两个集合之间的关系</li>\n</ul>\n</li>\n<li>定义有限状态转录机：<ul>\n<li>Q：状态{q_i}的有限集合</li>\n<li>\\sum ：有限的输入符号字母表</li>\n<li>∆：有限的输出符号字母表</li>\n<li>$q_0 \\in Q$：初始状态</li>\n<li>$F⊆Q$：终极状态集合</li>\n<li>$\\delta (q,w)$：状态之间的转移函数或者转移矩阵，是从Q×\\sum 到2^Q的一个关系，q是状态，w是字符串，返回新状态集合</li>\n<li>$\\sigma (q,w)$：输出函数，给定每一个状态和输入，返回可能输出字符串的集合，是从$Q × \\Sigma$到$2^∆$的一个关系</li>\n</ul>\n</li>\n<li>在FST中，字母表的元素不是单个符号，而是符号对，称为可行偶对。类比于FSA和正则语言，FST和正则关系同构，对于并运算封闭，一般对于差、补、交运算不封闭。</li>\n<li>此外，FST，<ul>\n<li>关于逆反（逆的逆）闭包，逆反用于方便的实现作为剖析器的FST到作为生成器的FST的转换</li>\n<li>关于组合（嵌套）闭包，用于将多个转录机用一个更复杂的转录机替换。</li>\n</ul>\n</li>\n<li>转录机一般是非确定性的，如果用FSA的搜索算法会很慢，如果用非确定性到确定性的转换算法，则有些FST本身是不可以被转换为为确定的。</li>\n<li>顺序转录机是一种输入确定的转录机，每个状态转移在给定状态和输入之后是确定的，不像上图中的FST，状态0在输入b时有两种状态转移（转移到相同的状态，但是输出不同）。顺序转录机可以使用$\\epsilon$符号，但是只能加在输出字符串上，不能加在输入字符串上，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZuHH.png\" alt=\"FoZuHH.png\"></li>\n<li>顺序转录机输出不一定是序列的，即从同一状态发出的不同转移可能产生相同输出，因此顺序转录机的逆不一定是顺序转录机，所以在定义顺序转录机时需要定义方向，且转移函数和输出函数需要稍微修改，输出空间缩小为Q和∆。</li>\n<li>顺序转录机的一种泛化形式是并发转录机，其在最终状态额外输出一个字符串，拼接到已经输出的字符串之后。顺序和并发转录机的效率高，且有有效的算法对其进行确定化和最小化，因此很重要。P并发转录机在此基础上可以解决歧义问题。</li>\n</ul>\n<h2 id=\"用有限状态转录机进行形态剖析\"><a href=\"#用有限状态转录机进行形态剖析\" class=\"headerlink\" title=\"用有限状态转录机进行形态剖析\"></a>用有限状态转录机进行形态剖析</h2><ul>\n<li>将单词看成词汇层和表层之间的关系，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZnDe.png\" alt=\"FoZnDe.png\"></li>\n<li>在之前双层形态学的基础定义上，定义自己到自己的映射为基本对，用一个字母表示；用^代表语素边界；用#代表单词边界，在任务中提到需要输出+SG之类的语素特征，这些特征在另一个输出上没有对应的输出符号，因此映射到空字符串或边界符号。我们把输入输出对用冒号连接，也可以写在弧的上下。一个抽象的表示英语名词复数屈折变化的转录机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZl4I.png\" alt=\"FoZl4I.png\"></li>\n<li>之后我们需要更新词表，使得非规则复数名词能够被剖析为正确的词干：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZMEd.png\" alt=\"FoZMEd.png\"></li>\n<li>之后将抽象的转录机写成具体的，由字母组成转移弧的转录机，如下图，只展示了具体化部分非规则复数和单数名词之后的转录机：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ3Ct.png\" alt=\"FoZ3Ct.png\"></li>\n</ul>\n<h2 id=\"转录机和正词法规则\"><a href=\"#转录机和正词法规则\" class=\"headerlink\" title=\"转录机和正词法规则\"></a>转录机和正词法规则</h2><ul>\n<li>用拼写规则，也就是正词法规则来处理英语中经常在语素边界发生拼写错误的问题。</li>\n<li>以下是一些拼写规则实例：<ul>\n<li>辅音重叠：beg/beggin</li>\n<li>E的删除：make/making</li>\n<li>E的插入：watch/watches</li>\n<li>Y的替换：try/tries</li>\n<li>K的插入：panic/panicked</li>\n</ul>\n</li>\n<li>为了实现拼写规则，我们在词汇层和表层之间加入中间层，以符合特定规则的语素毗连作为输入，以修改之后的正确的语素毗连作为输出，例如fox +N +PL输入到中间层即第一次转录，得到fox ^ s #，之后中间层到表层的第二次转录检测到特殊语素毗连：x^和s#，就在表层的x和s之间插入一个e，得到foxes。下面的转录机示意图展示了这个过程：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ88P.png\" alt=\"FoZ88P.png\"></li>\n<li>这个转录机只考虑x^和s#毗连需插入e这一正词法规则</li>\n<li>其他的词能正常通过</li>\n<li>$Q_0$代表无关词通过，是接受状态</li>\n<li>$Q_1$代表看见了zsx，作为中间状态保存，一直保存的是最后的与语素毗连的z,s,x，如果出现了其他字母则返回到q0，其本身也可以作为接受态</li>\n<li>$Q_2$代表看见了与z,s,x毗连的语素，这之后有四种转移<ul>\n<li>接了$x$,$z$，回到$q_1$，也就是认为重新接到了可能和语素毗连的x,z</li>\n<li>接了$s$，分为两种情况，一种是正常需要插入e，这时通过$\\epsilon$转移到$q_3$再到$q_4$；另一种是本来就需要插入$e$，这就到达$q_5$，之后视情况回退了$q_1$、$q_0$，或者$s$又毗连语素回到$q_2$。两种情况不确定，需要通过搜索解决</li>\n<li>接单词边界和其他符号，回到$q_0$</li>\n<li>$q_2$本身也可以作为接受态</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"结合\"><a href=\"#结合\" class=\"headerlink\" title=\"结合\"></a>结合</h2><ul>\n<li>现在可以通过三层结构，结合产生语素和进行正词法规则矫正的转录机。从词汇层到中间层用一个转录机产生语素，从中间层到表层可并行使用多个转录机进行正词法规则的矫正。</li>\n<li>两类转录机叠加时可以改写成一类转录机，这时需要对两类状态机状态集合计算笛卡尔积，对新集合内每一个元素建立状态。</li>\n<li>这种三层结构是可逆的，但是进行剖析时（从表层到词汇层）会出现歧义问题，即一个单词可能剖析出多种语素结合，这时单纯依靠转录机无法消歧，需要借助上下文。</li>\n</ul>\n<h2 id=\"其他应用（简单介绍）\"><a href=\"#其他应用（简单介绍）\" class=\"headerlink\" title=\"其他应用（简单介绍）\"></a>其他应用（简单介绍）</h2><ul>\n<li>不需要词表的FST，PORTER词干处理器：将层叠式重写规则用FST实现，提取出单词的词干。</li>\n<li>分词和分句：一个简单的英文分词可以基于正则表达式实现，一个简单的中文分词可以通过maxmatch（一种基于最大长度匹配的贪婪搜索算法）实现。</li>\n<li>拼写检查与矫正：使用了投影操作的FST可以完成非词错误的检测，然后基于最小编辑距离（使用动态规划算法实现）可以矫正。正常词错误检测和矫正需借助N元语法模型。</li>\n</ul>\n<h2 id=\"人如何进行形态处理\"><a href=\"#人如何进行形态处理\" class=\"headerlink\" title=\"人如何进行形态处理\"></a>人如何进行形态处理</h2><ul>\n<li>研究表明，人的心理词表存储了一部分形态机构，其他的结构不组合在心理词表中，而需要分别提取并组合。研究说明了两个问题：<ul>\n<li>形态尤其是屈折变化之类的能产性形态在人的心理词表中起作用，且人的语音词表和正词法词表可能具有相同结构。</li>\n<li>例如形态这种语言处理的很多性质，可以应用于语言的理解和生成。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第四章：N元语法\"><a href=\"#第四章：N元语法\" class=\"headerlink\" title=\"第四章：N元语法\"></a>第四章：N元语法</h1><ul>\n<li>语言模型是关于单词序列的统计模型，N元语法模型是其中的一种，它根据之前N-1个单词推测第N个单词，且这样的条件概率可以组成整个单词序列（句子）的联合概率。</li>\n</ul>\n<h2 id=\"在语料库中统计单词\"><a href=\"#在语料库中统计单词\" class=\"headerlink\" title=\"在语料库中统计单词\"></a>在语料库中统计单词</h2><ul>\n<li>区别：word type或者叫 vocabulary size V，代表语料中不同单词的个数，而tokens，不去重，代表语料的大小。有研究认为词典大小不低于tokens数目的平方根。<br>非平滑N元语法模型</li>\n<li>任务：根据以前的单词推断下一个单词的概率：$P(w|h)$，以及计算整个句子的概率$P(W)$。</li>\n<li>最朴素的做法是用古典概型，统计所有历史h和当前词w组成的片段在整个语料中出现的次数，并除以历史h片段在整个语料中出现的次数。句子的概率也用相似的方法产生。缺点：依赖大语料，且语言本身多变，这样的计算限制过于严格。</li>\n<li>接下来引入N元语法模型，首先通过概率的链式法则，可以得到条件概率$P(w|h)$和整个句子的联合概率$P(W)$之间的关系：<script type=\"math/tex; mode=display\">\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\\\\n= \\prod _{k=1}^n P(w_k|w_1^{k-1}) \\\\</script></li>\n<li>N元语法模型放松了条件概率的限制，做出一个马尔可夫假设：每个单词的概率只和它之前N-1个单词相关，例如二元语法模型，只和前一个单词相关，用这个条件概率去近似$P(w|h)$:<script type=\"math/tex; mode=display\">\nP(w_n|w_1^{n-1}) \\approx P(w_n|w_{n-1}) \\\\</script></li>\n<li>N元语法模型里的条件概率用最大似然估计来估算，统计语料中各种N元语法的个数，并归一化，其中可以简化的一点是：以二元语法为例，所有给定单词开头的二元语法总数必定等于该单词一元语法的计数：<script type=\"math/tex; mode=display\">\nP(w_n|w_{n-1}) = \\frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\\\</script></li>\n<li>使用N元语法之后，句子概率的链式分解变得容易计算，我们可以通过计算各种句子的概率来判断句子是否包含错字，或者计算某些句子在给定上下文中出现的可能，因为N元语法能捕捉一些语言学上的特征，或者一些用语习惯。在语料充足的时候，我们可以使用三元语法模型获得更好的效果。</li>\n</ul>\n<h2 id=\"训练集和测试集\"><a href=\"#训练集和测试集\" class=\"headerlink\" title=\"训练集和测试集\"></a>训练集和测试集</h2><ul>\n<li>N元语法模型对训练集非常敏感。N元语法的N越大，依赖的上下文信息越多，利用N元语法模型生成的句子就越流畅，但这些未必“过于流畅”，其原因在于N元语法概率矩阵非常大且非常稀疏，在N较大例如四元语法中，一旦生成了第一个单词，之后可供的选择非常少，接着生成第二个单词之后选择更少了，往往只有一个选择，这样生成的就和原文中某一个四元语法一模一样。过于依赖训练集会使得模型的泛化能力变差。因此我们选择的训练集和测试集应来自同一细分领域。</li>\n<li>有时候测试集中会出现训练集词典里没有的词，即出现未登录词（Out Of Vocabulty,OOV）。在开放词典系统中，我们先固定词典大小，并将所有未登录词用特殊符号<unk>代替，然后才进行训练。</unk></li>\n</ul>\n<h2 id=\"评价N元语法模型：困惑度\"><a href=\"#评价N元语法模型：困惑度\" class=\"headerlink\" title=\"评价N元语法模型：困惑度\"></a>评价N元语法模型：困惑度</h2><ul>\n<li>模型的评价分两种：外在评价和内在评价。外在评价是一种端到端的评价，看看某一模块的改进是否改进了整个模型的效果。内在评价的目的是快速衡量模块的潜在改进效果。内在评价的潜在改进效果不一定会使得端到端的外在评价提高，但是一般两者都存在某种正相关关系。</li>\n<li>困惑度（Perplexsity,PP）是一种关于概率模型的内在评价方法。语言模型的在测试集上的困惑度是语言模型给测试集分配的概率的函数。以二元语法为例，测试集上的困惑度为：<script type=\"math/tex; mode=display\">\nPP(W) = \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i|w_{i-1})}} \\\\</script></li>\n<li>概率越高，困惑度越低。困惑度的两种解释：<ul>\n<li>加权的平均分支因子：分支因子是指可能接在任何上文之后的单词的数目。显然，如果我们的模型啥也没学习到，那么测试集任何单词可以接在任何上文之后，分支因子很高，困惑度很高；相反，如果我们的模型学习到了具体的规则，那么单词被限制接在一些指定上文之后，困惑度变低。困惑度使用了概率加权分支因子，分支因子的大小在模型学习前后不变，”morning”仍然可以接到任何上文之后，但是它接到”good”之后的概率变大了，因此是加权的分支因子。</li>\n<li>熵：对于语言序列，我们定义一个序列的熵为：    <script type=\"math/tex\">H(w_1,w_2,…,w_n )=-\\sum _{W_1^n \\in L} p(W_1^n) \\log ⁡p(W_1^n)</script>也就是这个序列中所有前缀子序列的熵之和，其均值是序列的熵率。计算整个语言的熵，假设语言是一个产生单词序列的随机过程，单词序列无限长，则其熵率是：<script type=\"math/tex\">H(L)=\\lim _{n \\rightarrow \\infty}⁡ \\frac 1n H(w_1,w_2,…,w_n) =\\lim _{n \\rightarrow \\infty} -⁡\\frac 1n \\sum _{W \\in L} p(W_1^n)  \\log ⁡p(W_1^n)</script>根据Shannon-McMillan-Breiman理论，在n趋于无穷的情况下，如果语言既是平稳又是正则的，上面这些子串的和的熵，可以用最大串代替每一个子串得到，这里的代替是指log后面求的是最大串的概率，log之前的概率依然是各个子串的概率？假如是这样的话提出最大串的概率对数，对所有子串概率求和得到：<script type=\"math/tex\">H(L)=\\lim _{n \\rightarrow \\infty} -⁡ \\frac 1n \\log ⁡p(w_1,w_2,…,w_n)</script>交叉熵可以衡量我们的模型生成的概率分布到指定概率分布之间的距离，我们希望模型生成概率分布尽可能近似真实分布，即交叉熵小。具体衡量时是对相同的语言序列，计算训练得到的模型m和理想模型p在生成这个序列上的概率的交叉熵：<script type=\"math/tex\">H(p,m) = \\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\sum _{W \\in L} p(W_1^n) \\log⁡ m(W_1^n)</script>但是我们不知道理想的分布p，这时根据之前的Shannon-McMillan-Breiman定理，得到了只包含一个概率分布的序列交叉熵（？）：<script type=\"math/tex\">H(p,m)=\\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\log⁡ m(W_1^n)</script>在测试数据上我们没有无限长的序列，就用有限长的序列的交叉熵近似这个无限长序列的交叉熵。困惑度则是这个（近似的？只包含一个概率分布的？）交叉熵取指数运算：<script type=\"math/tex; mode=display\">\nPerplexity(W) = 2^{H(W)} \\\\\n= P(w_1 w_2 ... w_N)^{\\frac {-1}{N}} \\\\\n= \\sqrt[n]{\\frac {1}{P(w_1 w_2 ... w_N)}} \\\\\n= \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i | w_1 ... w_{i-1})}} \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"平滑\"><a href=\"#平滑\" class=\"headerlink\" title=\"平滑\"></a>平滑</h2><ul>\n<li>因为N元语法模型依赖语料，一般而言对于N越高的N元语法，语料提供的数据越稀疏。这种情况下N元语法对于那些计数很小的语法估计很差，且如果测试集中某一句包含了训练集中没有出现的N元语法时，我们无法使用困惑度进行评价。因此我们使用平滑作为一种改进方法，使得N元语法的最大似然估计能够适应这些存在0概率的情况。</li>\n<li>接下来介绍了两种平滑：<ul>\n<li>拉普拉斯平滑（加1平滑）</li>\n<li>Good-Turing 打折法</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"拉普拉斯平滑\"><a href=\"#拉普拉斯平滑\" class=\"headerlink\" title=\"拉普拉斯平滑\"></a>拉普拉斯平滑</h3><ul>\n<li>加1平滑就是在计算概率归一化之前，给每个计数加1，对应的，归一化时分母整体加了一个词典大小:<script type=\"math/tex; mode=display\">\nP_{Laplace}(w_i) = \\frac {c_i + 1}{N+V} \\\\</script></li>\n<li>为了表现平滑的作用，引入调整计数$c^{*}$，将平滑后的概率写成和平滑之前一样的形式：<script type=\"math/tex; mode=display\">\nP_{Laplace} (w_i) = \\frac {(C_i^{\\*})}{N} \\\\\nC_i^{\\*} = \\frac {(C_i+1)N}{(N+V)} \\\\</script></li>\n<li>一种看待平滑的角度是：对每个非0计数打折，分一些概率给0计数，定义相对打折$d_c$（定义在非0计数上），<script type=\"math/tex; mode=display\">\nd_c = \\frac {c^{\\*}} {c}</script></li>\n<li>$d_c$代表了打折前后单词计数的变化。平滑之后，对于非0计数，当$C_i &lt; \\frac NV$时，计数增加；否则计数减少。计数越大，打折越多，增加越少（减少越多）。当0计数很多时，N/V较小，这时大部分非0计数都会减少，且减少较多。</li>\n<li>而0计数则没有收到打折的影响。因此在一轮不同程度的增长之后，再归一化的结果就是非0计数分享了一些概率给0计数。写成调整计数的形式，就是非0计数减少数值，0计数变化（一般是减少）数值（但不是减少的完全等于增加的）。 书中给出了一个例子，下图是一部分语料的二元语法平滑之后的计数，蓝色代表平滑加1之后的0计数：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZNDg.png\" alt=\"FoZNDg.png\"><br>如果把表写成调整计数的形式：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZtKS.png\" alt=\"FoZtKS.png\"> </li>\n<li>可以看到，本来的0计数（蓝色）从0变大，而其他的计数减少，例如&lt; i want&gt;，从827减少到527，<want to>从608减少到238。</want></li>\n<li>当0计数很多时，非0计数减少的数值很多，可以使用一个小于1的小数$\\delta$代替1，即加$\\delta$平滑。通常这个$\\delta$是动态变化的。</li>\n</ul>\n<h3 id=\"GT打折法\"><a href=\"#GT打折法\" class=\"headerlink\" title=\"GT打折法\"></a>GT打折法</h3><ul>\n<li>类似于Good-Turing打折法, Witten-Bell打折法， Kneyser-Ney 平滑一类的方法，它们的基本动机是用只出现一次的事物的计数来估计从未出现的事物的计数。只出现一次的语法称为单件（singleton）或者罕见语（hapax legomena）。Good-Turing打折法用单件的频率来估计0计数二元语法。</li>\n<li>定义N_c为出现c次的N元语法的总个数（不是总个数乘以c），并称之为频度c的频度。对N_c中的c的最大似然估计是c。这样相当于将N元语法按其出现次数分成了多个桶，GT打折法用c+1号桶里语法概率的最大似然估计来重新估计c号桶内语法的概率。因此GT估计之后最大似然估计得到的c被替换成：<script type=\"math/tex; mode=display\">\nc^{\\*}=(c+1) \\frac {N_{c+1}}{N_c}</script></li>\n<li>之后计算某N元语法的概率：<ul>\n<li>从未出现：$P_{GT}^{*}=\\frac{N_1}{N}$。其中N是所有N元语法数$(\\sum _i N_i * i)$。这里假设了我们已知$N_0$，则此式表示某一具体未知计数N元语法概率时还应除以$N_0$。</li>\n<li>已出现（已知计数）：$P_{GT}^{*} = \\frac{c^{*}}{N}$</li>\n</ul>\n</li>\n<li>这样计算，$N_1$的一些概率转移到了$N_0$上。GT打折法假设所有的N元语法概率分布满足二项式分布，且假设我们已知$N_0$，以二元语法为例：<script type=\"math/tex; mode=display\">\nN_0 = V^2 - \\sum _{i>0} N_i \\\\</script></li>\n<li>其他注意事项：<ul>\n<li>有些$N_c$为0，这时我们无法用这些$N_c$来计算平滑后的c。这种情况下我们直接放弃平滑，令$c^{*} = c$，再根据正常的数据计算出一个对数线性映射，$log⁡(N_c) = a + b \\log(c)$，代入放弃平滑的c并用其倒推计算计数为0的$N_c$，使得这些$N_c$有值，不会影响更高阶的c的计算。</li>\n<li>只对较小c的$N_c$进行平滑，较大c的$N_c$认为足够可靠，设定一个阈值k，对$c &lt; k$的$N_c$计算：<script type=\"math/tex; mode=display\">\nc^{\\*} = \\frac {(c+1) \\frac {N_c+1}{N_c} - c \\frac {(k+1) N_{k+1} }{N_1} } {1- \\frac {(k+1)N_{k+1}} {N_1}} \\\\</script></li>\n<li>计算较小的c如c=1时，也看成c=0的情况进行平滑</li>\n</ul>\n</li>\n<li>一个例子：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZGgf.png\" alt=\"FoZGgf.png\"></li>\n</ul>\n<h2 id=\"插值与回退\"><a href=\"#插值与回退\" class=\"headerlink\" title=\"插值与回退\"></a>插值与回退</h2><ul>\n<li>上述的平滑只考虑了如何转移概率到计数为0的语法上去，对于条件概率$p(w|h)$，我们也可以采用类似的思想，假如不存在某个三元语法帮助计算$p(w_n |w_{n-1} w_{n-2})$，则可以用阶数较低的语法$p(w_n |w_{n-1})$帮助计算，有两种方案：<ul>\n<li>回退：用低阶数语法的替代0计数的高阶语法</li>\n<li>插值：用低阶数语法的加权估计高阶语法</li>\n</ul>\n</li>\n<li>在Katz回退中，我们使用GT打折作为方法的一部分：GT打折告诉我们有多少概率可以从已知语法中分出来，Katz回退告诉我们如何将这些分出来的概率分配给未知语法。在之前的GT打折法中，我们将分出的概率均匀分给每一个未知语法，而Katz回退则依靠低阶语法的信息来分配：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZJv8.png\" alt=\"FoZJv8.png\"></li>\n<li>其中$P^{*}$是打折之后得到的概率；\\alpha是归一化系数，保证分出去的概率等于未知语法分配得到的概率。</li>\n<li>插值则是用低阶语法概率加权求和得到未知高阶语法概率：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZUbQ.png\" alt=\"FoZUbQ.png\"></li>\n<li>加权的系数还可以通过上下文动态计算。具体系数的计算有两种方法：<ul>\n<li>尝试各种系数，用在验证集上表现最好的系数组合</li>\n<li>将系数看成是概率生成模型的隐变量，使用EM算法进行推断</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"实际问题：工具和数据格式\"><a href=\"#实际问题：工具和数据格式\" class=\"headerlink\" title=\"实际问题：工具和数据格式\"></a>实际问题：工具和数据格式</h2><ul>\n<li>在语言模型计算中，一般将概率取对数进行计算，原因有二：防止数值下溢；取对数能将累乘运算变成累加，加速计算。</li>\n<li>回退N元语法模型一般采用ARPA格式。ARPA格式文件由一些头部信息和各类N元语法的列表组成，列表中包含了该类N元语法下所有语法，概率，和回退的归一化系数。只有能够称为高阶语法前缀的低阶语法才能在回退中被利用，并拥有归一化系数。</li>\n<li>两种计算N元语法模型的工具包：SRILM toolkit 和Cambridge-CMU toolkit</li>\n</ul>\n<h2 id=\"语言建模中的高级问题\"><a href=\"#语言建模中的高级问题\" class=\"headerlink\" title=\"语言建模中的高级问题\"></a>语言建模中的高级问题</h2><h3 id=\"高级平滑方法：Kneser-Ney平滑\"><a href=\"#高级平滑方法：Kneser-Ney平滑\" class=\"headerlink\" title=\"高级平滑方法：Kneser-Ney平滑\"></a>高级平滑方法：Kneser-Ney平滑</h3><ul>\n<li>注意到在GT打折法当中，打折之后估计的c值比最大似然估计得到的c值近似多出一个定值d。绝对打折法便考虑了这一点，在每个计数中减去这个d：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZwUs.png\" alt=\"FoZwUs.png\"></li>\n<li>Kneser-Ney平滑吸收了这种观点，并且还考虑了连续性：在不同上文中出现的单词更有可能出现在新的上文之后，在回退时，我们应该优先考虑这种在多种上文环境里出现的词，而不是那些出现次数很多，但仅仅在特定上文中出现的词。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZdEj.png\" alt=\"FoZdEj.png\"></li>\n<li>在Kneser-Ney中，插值法能够比回退法取得更加好的效果：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ05n.png\" alt=\"FoZ05n.png\"></li>\n</ul>\n<h3 id=\"基于分类的N元语法\"><a href=\"#基于分类的N元语法\" class=\"headerlink\" title=\"基于分类的N元语法\"></a>基于分类的N元语法</h3><ul>\n<li>这种方法是为了解决训练数据的稀疏性。例如IBM聚类，每个单词只能属于一类，以二元语法为例，某个二元语法的条件概率的计算变为给定上文所在类，某个单词的条件概率，还可以进一步链式分解为两个类的条件概率乘以某个单词在给定其类条件下的条件概率：<script type=\"math/tex; mode=display\">\np(w_i│w_{i-1} ) \\approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \\cdot p(w_i |c_i)</script></li>\n</ul>\n<h3 id=\"语言模型适应和网络应用\"><a href=\"#语言模型适应和网络应用\" class=\"headerlink\" title=\"语言模型适应和网络应用\"></a>语言模型适应和网络应用</h3><ul>\n<li>适应是指在大型宽泛的语料库上训练语言模型，并在小的细分领域的语言模型上进一步改进。网络是大型语料库的一个重要来源。在实际应用时我们不可能搜索每一个语法并统计搜索得到所有页面上的所有语法，我们用搜索得到的页面数来近似计数。</li>\n</ul>\n<h3 id=\"利用更长距离的上文信息\"><a href=\"#利用更长距离的上文信息\" class=\"headerlink\" title=\"利用更长距离的上文信息\"></a>利用更长距离的上文信息</h3><ul>\n<li>通常我们使用二元和三元语法模型，但是更大的N能够带来更好的效果。为了捕捉更长距离的上文信息，有以下几种方法：<ul>\n<li>基于缓存机制的N元语法模型</li>\n<li>基于主题建模的N元语法模型，对不同主题建模语言模型，再加权求和</li>\n<li>不一定使用相邻的上文信息，例如skip N-grams或者不一定使用定长的上文信息，例如变长N-grams</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第十六章：语言的复杂性\"><a href=\"#第十六章：语言的复杂性\" class=\"headerlink\" title=\"第十六章：语言的复杂性\"></a>第十六章：语言的复杂性</h1><h2 id=\"Chomsky层级\"><a href=\"#Chomsky层级\" class=\"headerlink\" title=\"Chomsky层级\"></a>Chomsky层级</h2><ul>\n<li>Chomsky层级反映了不同形式化方法描述的语法之间的蕴含关系，较强生成能力或者说更复杂的语法在层级的外层。从外到内，加在可重写语法规则上的约束增加，语言的生成能力逐渐降低。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZXad.png\" alt=\"FoZXad.png\"></li>\n<li>五种语法对应的规则和应用实例：<br><img src=\"https://s2.ax1x.com/2019/01/03/Foepxf.png\" alt=\"Foepxf.png\"><ul>\n<li>0型语法：规则上只有一个限制，即规则左侧不能为空字符串。0型语法刻画了递归可枚举语言</li>\n<li>上下文相关语法：可以把上下文\\alpha，\\beta之间的非终极符号A重写成任意非空符号串</li>\n<li>温和的上下文相关语法</li>\n<li>上下文无关语法：可以把任何单独的非终极符号重写为由终极符号和非终极符号构成的字符串，也可以重写为空字符串</li>\n<li>正则语法：可以是右线性也可以是左线性，以右线性为例，非终极符号可以重写为左边加了若干终极符号的另一个非终极符号，右线性不断地在字符串左侧生成终极符号。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自然语言是否正则\"><a href=\"#自然语言是否正则\" class=\"headerlink\" title=\"自然语言是否正则\"></a>自然语言是否正则</h2><ul>\n<li>判断语言是否正则能够让我们了解应该用哪一层次的语法来描述一门语言，且这个问题能够帮助我们了解自然语言的不同方面的某些形式特性。</li>\n<li>抽吸引理：用来证明一门语言不是正则语言。<ul>\n<li>如果一门语言可以被有限状态自动机来描述，则与自动机对应有一个记忆约束量。这个约束量对于不同的符号串不会增长的很大，因为其状态数目是固定的，更长的符号串应该是通过状态之间转移产生而不是增加状态数目。因此这个记忆量不一定和输入的长度成比例。</li>\n<li>如果一个正则语言能够描述任意长的符号序列，比自动机的状态数目还多，则该语言的自动机中必然存在回路。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZxPI.png\" alt=\"FoZxPI.png\"></li>\n</ul>\n</li>\n<li>如图所示自动机，可以表述xyz,xyyz,xyyyz…..，当然也可以将中间无限长的y序列“抽吸掉”，表述xz。抽吸引理表述如下：</li>\n<li>设L是一个有限的正则语言，那么必然存在符号串x,y,z,使得对于任意n≥0，y≠$\\epsilon$，且xy^n z∈L</li>\n<li>即假如一门语言是正则语言，则存在某一个符号串y，可以被适当的“抽吸”。这个定理是一门语言是正则语言的必要非充分条件。</li>\n<li>有学者证明英语不是一门正则语言：<ul>\n<li>具有镜像性质的句子通过抽吸原理可以证明不是正则语言，而英语中一个特殊的子集合和这种镜像性质的句子是同态的。</li>\n<li>另一种证明基于某些带有中心-嵌套结构的句子。这种句子可以由英语和某一类简单的正则表达式相交得到，通过抽吸原理可以得到这种句子不是正则语言。英语和正则语言的交不是正则语言，则英语不是正则语言。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自然语言是否上下文无关\"><a href=\"#自然语言是否上下文无关\" class=\"headerlink\" title=\"自然语言是否上下文无关\"></a>自然语言是否上下文无关</h2><ul>\n<li>既然自然语言不是正则语言，我们接着考虑更宽松的限定，自然语言是否是上下文无关的？</li>\n<li>不是……</li>\n</ul>\n<h2 id=\"计算复杂性和人的语言处理\"><a href=\"#计算复杂性和人的语言处理\" class=\"headerlink\" title=\"计算复杂性和人的语言处理\"></a>计算复杂性和人的语言处理</h2><ul>\n<li>人对中心嵌套句子处理很困难，因为人们剖析时利用的栈记忆有限，且栈中不同层次记忆容易混淆。</li>\n</ul>\n<h1 id=\"第五章：词类标注\"><a href=\"#第五章：词类标注\" class=\"headerlink\" title=\"第五章：词类标注\"></a>第五章：词类标注</h1><ul>\n<li>各种表述：POS（Part Of Speech）、word classes（词类）、morphological classes（形态类）、lexical tags（词汇标记）。</li>\n<li>POS的意义在于：<ul>\n<li>能够提供关于单词及其上下文的大量信息。</li>\n<li>同一单词在不同词类下发音不同，因此POS还能为语音处理提供信息。</li>\n<li>进行词干分割（stemming），辅助信息检索</li>\n</ul>\n</li>\n<li>本章介绍三种词类标注算法：<ul>\n<li>基于规则的算法</li>\n<li>基于概率的算法，隐马尔科夫模型</li>\n<li>基于变换的算法</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"一般词类\"><a href=\"#一般词类\" class=\"headerlink\" title=\"一般词类\"></a>一般词类</h2><ul>\n<li>POS分为封闭集和开放集，封闭集集合相对稳定，例如介词，开放集的词语则不断动态扩充，例如名词和动词。特定某个说话人或者某个语料的开放集可能不同，但是所有说一种语言以及各种大规模语料库可能共享相同的封闭集。封闭集的单词称为虚词（功能词，function word），这些词是语法词，一般很短，出现频次很高。</li>\n<li>四大开放类：名词、动词、形容词、副词。</li>\n<li>名词是从功能上定义的而不是从语义上定义的，因此名词一般表示人、地点、事物，但既不充分也不必要。定义名词：<ul>\n<li>与限定词同时出现</li>\n<li>可以受主有代词修饰</li>\n<li>大多数可以以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词</li>\n</ul>\n</li>\n<li>动词，表示行为和过程的词，包括第三人称单数、非第三人称单数、进行时、过去分词几种形态</li>\n<li>形容词，描述性质和质量</li>\n<li>副词，用于修饰，副词可以修饰动词、动词短语、其它副词。</li>\n<li>英语中的一些封闭类：<ul>\n<li>介词 prepositions：出现在名词短语之前，表示关系</li>\n<li>限定词 determiners 冠词 articles：与有定性（definiteness）相关</li>\n<li>代词 pronouns：简短的援引某些名词短语、实体、或事件的一种形式</li>\n<li>连接词 conjunctions：用于连接和补足（complementation）</li>\n<li>助动词 auxiliary verbs：标志主要动词的某些语义特征，包括：时态、完成体、极性对立、情态</li>\n<li>小品词 particles：与动词结合形成短语动词</li>\n<li>数词 numerals</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"词类标注\"><a href=\"#词类标注\" class=\"headerlink\" title=\"词类标注\"></a>词类标注</h2><ul>\n<li>标注算法的输入是单词的符号串和标记集，输出要让每一个单词标注上一个单独且最佳的标记。如果每个单词只对应一种词性，那么根据已有的标记集，词类标注就是一个简单的查表打标的过程，但是很多词存在多种词性，例如book既可以是名词也可以是动词，因此要进行消歧，词类标注是歧义消解的一个重要方面。</li>\n</ul>\n<h2 id=\"基于规则的词类标注\"><a href=\"#基于规则的词类标注\" class=\"headerlink\" title=\"基于规则的词类标注\"></a>基于规则的词类标注</h2><ul>\n<li>介绍了ENGTWOL系统，根据双层形态学构建，对于每一个词的每一种词类分别立条，计算时不计屈折形式和派生形式.</li>\n<li>标注算法的第一阶段是将单词通过双层转录机，得到该单词的所有可能词类</li>\n<li>之后通过施加约束规则排除不正确的词类。这些规则通过上下文的类型来决定排除哪些词类。</li>\n</ul>\n<h2 id=\"基于隐马尔科夫模型的词类标注\"><a href=\"#基于隐马尔科夫模型的词类标注\" class=\"headerlink\" title=\"基于隐马尔科夫模型的词类标注\"></a>基于隐马尔科夫模型的词类标注</h2><ul>\n<li>使用隐马尔科夫模型做词类标注是一类贝叶斯推断，这种方法将词类标注看成是序列分类任务。观察量为一个词序列（比如句子），任务是给这个序列分配一个标注序列。</li>\n<li>给定一个句子，贝叶斯推断想要在所有标注序列可能中选择最好的一个序列，即<script type=\"math/tex; mode=display\">\n{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)</script></li>\n<li>使用贝叶斯法则将其转化为：<script type=\"math/tex; mode=display\">\n{t_1^n} _{best}={argmax} _{t_1^n}  \\frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n} P(w_1^n│t_1^n)P(t_1^n)</script></li>\n<li>隐马尔科夫模型在此基础上做了两点假设<ul>\n<li>一个词出现的概率只与该词的词类标注有关，与上下文其他词和其他标注无关，从而将序列的联合概率拆解为元素概率之积，即：P(w_1^n│t_1^n) \\approx \\prod _{i=1}^n P(w_i |t_i)</li>\n<li>一个标注出现的概率只与前一个标注相关，类似于二元语法的假设：P(t_1^n ) \\approx \\prod _{i=1}^n P(t_i |t_{i-1})</li>\n</ul>\n</li>\n<li>在两种假设下简化后的最好标注序列表达式为：<script type=\"math/tex; mode=display\">\n{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})</script></li>\n<li>上面这个概率表达式实际上将HMM模型的联合概率拆成了各个部分转移概率的乘积，具体而言分为标签转移概率（隐变量之间转移）和词似然（隐变量转移到可观察变量）。通过最大似然估计，我们可以通过古典概型的方法从已标注的语料中计算出这两类概率：<script type=\"math/tex; mode=display\">\nP(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\\\\nP(w_i│t_i ) = \\frac{C(t_i,w_i)}{C(t_i)} \\\\</script></li>\n<li>一个例子：HMM模型如何正确的将下句中的race识别为动词而不是名词：</li>\n<li>Secretariat is expected to race tomorrow.</li>\n<li>画出上句中race被识别为动词和名词两种情况下的HMM模型，可以看到两个模型对比只有三个转移概率不同，用加粗线标出：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZDCq.png\" alt=\"FoZDCq.png\"></li>\n<li>HMM词类标注器消歧的方式是全局的而不是局部的。我们在语料中统计得到这三种转移概率，再累乘，结果是(a)的概率是(b)概率的843倍。显然race应该被标注为动词。</li>\n</ul>\n<h2 id=\"形式化隐马尔科夫模型标注器\"><a href=\"#形式化隐马尔科夫模型标注器\" class=\"headerlink\" title=\"形式化隐马尔科夫模型标注器\"></a>形式化隐马尔科夫模型标注器</h2><ul>\n<li>HMM模型是有限自动机的扩展，具体而言是一种加权有限自动机，马尔可夫链的扩展，这种模型允许我们考虑观察量和隐变量，考虑包含隐变量的概率模型。HMM包含以下组件：<ul>\n<li>Q：大小为N的状态集</li>\n<li>A：大小为N*N的转移概率矩阵</li>\n<li>O：大小为T的观察事件集</li>\n<li>B：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察o_t的概率</li>\n<li>$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量</li>\n</ul>\n</li>\n<li>A中的概率和B中的概率对应着之前式子中每一个累乘项里的先验$P(w_i│t_i )$和似然$P(t_i |t _{i-1})$概率：<script type=\"math/tex; mode=display\">\n{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})</script></li>\n</ul>\n<h2 id=\"HMM标注的维特比算法\"><a href=\"#HMM标注的维特比算法\" class=\"headerlink\" title=\"HMM标注的维特比算法\"></a>HMM标注的维特比算法</h2><ul>\n<li>在HMM模型中，已知转移概率和观察序列，求隐变量的任务叫做解码。解码的一种算法即维特比算法，实质上是一种动态规划算法，与之前求最小编辑距离的算法类似。</li>\n<li>首先我们从语料中计算得到A和B两个矩阵，即模型的转移概率已知，对于给定的观察序列，按照以下步骤执行维特比算法：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZyvT.png\" alt=\"FoZyvT.png\"></li>\n<li>算法维护一个$(N+2)*T$的概率矩阵viterbi，加了2代表初始状态和结束状态，viterbi[s,t]代表了在第t步状态为s时的最佳路径概率，而backpointer[s,t]对应着保存了该最佳路径的上一步是什么状态，用于回溯输出整个最佳路径。</li>\n<li>关键的转移在于$viterbi[s,t] \\leftarrow max _{s^{*}=1}^N⁡ viterbi[s^{*},t-1] * a_{s^{*},s} * b_s (o_t)$即当前时间步最佳路径是由上一时间步各个状态的最佳路径转移过来的，选择上一步最佳路径概率与转移概率乘积最大的路径作为当前时间步的最佳路径。从动态规划的角度而言，即长度为t的最佳路径，必定是从长度为t-1的最佳路径里选择一条转移得到，否则肯定可以从另一条概率更大的路径转移获得更优解。这样就限制了最佳路径的生成可能，减少了计算量。</li>\n</ul>\n<h2 id=\"将HMM算法扩展到三元语法\"><a href=\"#将HMM算法扩展到三元语法\" class=\"headerlink\" title=\"将HMM算法扩展到三元语法\"></a>将HMM算法扩展到三元语法</h2><ul>\n<li>现代的HMM标注器一般在标注转移概率上考虑更长的上文历史：<script type=\"math/tex; mode=display\">\nP(t_1^n ) \\approx \\prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})</script></li>\n<li>这样的话需要在序列开头和结尾做一些边界处理。使用三元语法的一个问题是数据稀疏：例如我们从没有在训练集中见过标注序列PRP VB TO，则我们无法计算P(TO|PRP,VB)。一种解决办法是线性插值：<script type=\"math/tex; mode=display\">\nP(t_i│t _{i-1} t _{i-2} ) = \\lambda _1 P ̂(t_i│t _{i-1} t _{i-2} )+\\lambda _2 P ̂(t_i│t _{i-1} )+\\lambda _3 P ̂(t_i)</script></li>\n<li>使用删除插值的办法确定系数$\\lambda$：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZr80.png\" alt=\"FoZr80.png\"></li>\n</ul>\n<h2 id=\"基于变换的标注\"><a href=\"#基于变换的标注\" class=\"headerlink\" title=\"基于变换的标注\"></a>基于变换的标注</h2><ul>\n<li>基于变换的方法结合了基于规则和基于概率方法的优点。基于变换的方法依然需要规则，但是从数据中总结出规则，是一种监督学习方法，称为基于变换的学习（Transformation Based Learning，TBL）。在TBL算法中，语料库首先用比较宽的规则来标注，然后再选择稍微特殊的规则来修改，接着再使用更窄的规则来修改数量更少的标记。</li>\n</ul>\n<h2 id=\"如何应用TBL规则\"><a href=\"#如何应用TBL规则\" class=\"headerlink\" title=\"如何应用TBL规则\"></a>如何应用TBL规则</h2><ul>\n<li>首先应用最宽泛的规则，就是根据概率给每个词标注，选择概率最大的词类作为标注。之后应用变换规则，即如果满足某一条件，就将之前标注的某一词类变换（纠正）为正确的词类，之后不断应用更严格的变换，在上一次变换的基础上进行小部分的修改。</li>\n<li>如何学习到TBL规则<ul>\n<li>首先给每个词打上最可能的标签</li>\n<li>检查每一个可能的变换，选择效果提升最多的变换，此处需要直到每一个词正确的标签来衡量变换带来的提升效果，因此是监督学习。</li>\n<li>根据这个被选择的变换给数据重新打标，重复步骤2，直到收敛（提升效果小于某一阈值）</li>\n</ul>\n</li>\n<li>以上过程输出的结果是一有序变换序列，用来组成一个标注过程，在新语料上应用。虽然可以穷举所有的规则，但是那样复杂度太高，因此我们需要限制变换集合的大小。解决方案是设计一个小的模板集合（抽象变换）,每一个允许的变换都是其中一个模板的实例化。</li>\n</ul>\n<h2 id=\"评价和错误分析\"><a href=\"#评价和错误分析\" class=\"headerlink\" title=\"评价和错误分析\"></a>评价和错误分析</h2><ul>\n<li>一般分为训练集、验证集、测试集，在训练集内做十折交叉验证。</li>\n<li>与人类标注的黄金标准比较计算准确率作为衡量指标。</li>\n<li>一般用人类表现作为ceiling，用一元语法最大概率标注的结果作为baseline。</li>\n<li>通过含混矩阵或者列联表来进行错误分析。在N分类任务中，一个N*N的含混矩阵的第i行第j列元素指示第i类被错分为第j类的次数在总分错次数中的占比。一些常见的容易分错的词性包括：<ul>\n<li>单数名词、专有名词、形容词</li>\n<li>副词、小品词、介词</li>\n<li>动词过去式、动词过去分词、形容词</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"词性标注中的一些其他问题\"><a href=\"#词性标注中的一些其他问题\" class=\"headerlink\" title=\"词性标注中的一些其他问题\"></a>词性标注中的一些其他问题</h2><ul>\n<li>标注不确定性：一个词在多个词性之间存在歧义，很难区分。这种情况下有些标注器允许一个词被打上多个词性标注。在训练和测试的时候，有三种方式解决这种多标注词：<ul>\n<li>通过某种方式从这些候选标注中选择一个标注</li>\n<li>训练时指定一个词性，测试时只要打上了候选词性中任意一个就认为标注正确</li>\n<li>将整个不确定的词性集看成一个新的复杂词性</li>\n</ul>\n</li>\n<li>多部分词：在标注之前需要先分词，一些多部分词是否应该被分为一部分，例如New York City应该分成三部分还是一个整体，也是各个标注系统需要考虑的。</li>\n<li>未知词：不在词典中的词称为未知词。对于未知词，训练集无法给出它的似然P(w_i |t_i)，可以通过以下几种方式解决：<ul>\n<li>只依赖上下文的POS信息预测</li>\n<li>用只出现一次的词来估计未知词的分布，类似于Good Turing打折法</li>\n<li>使用未知词的单词拼写信息，正词法信息。例如连字符、ed结尾、首字母大写等特征。之后在训练集中计算每个特征的似然，并假设特征之间独立，然后累乘特征似然作为未知词的似然：$P(w_i│t_i )=p(unknown word│t_i ) * p(capital│t_i ) * p(endings/hyph|t_i)$</li>\n<li>使用最大熵马尔可夫模型</li>\n<li>使用对数线性模型</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"噪声信道模型\"><a href=\"#噪声信道模型\" class=\"headerlink\" title=\"噪声信道模型\"></a>噪声信道模型</h2><ul>\n<li>贝叶斯推断用于标注可以认为是一种噪声信道模型的应用，本节介绍如何用噪声信道模型来完成拼写纠正任务。<br>之前对于非单词错误，通过词典查找可以检测到错误，并根据最小编辑距离纠正错误，但这种方法对于真实单词错误无能为力。噪声信道模型可以纠正这两种类型的拼写错误。</li>\n<li>噪声信道模型的动机在于将错误拼写的单词看成是一个正确拼写的单词经过一个噪声信道时受到干扰扭曲得到。我们尝试所有可能的正确的词，将其输入信道，最后得到的干扰之后的词与错误拼写的词比较，最相似的例子对应的输入词就认为是正确的词。这类噪声信道模型，比如之前的HMM标注模型，是贝叶斯推断的一种特例。我们看到一个观察两（错误拼写词）并希望找到生成这个观察量的隐变量（正确拼写词），也就是找最大后验。</li>\n<li>将噪声信道模型应用于拼写纠正：首先假设各种拼写错误类型，错拼一个、错拼两个、漏拼一个等，然后产生所有可能的纠正，除去词典中不存在的，最后分别计算后验概率，选择后验概率最大的作为纠正。其中需要根据局部上下文特征来计算似然。</li>\n<li>另一种纠正算法是通过迭代来改进的方法：先假设拼写纠正的含混矩阵是均匀分布的，之后根据含混矩阵运行纠正算法，根据纠正之后的数据集更新含混矩阵，反复迭代。这种迭代的算法是一种EM算法。</li>\n</ul>\n<h2 id=\"根据上下文进行拼写纠正\"><a href=\"#根据上下文进行拼写纠正\" class=\"headerlink\" title=\"根据上下文进行拼写纠正\"></a>根据上下文进行拼写纠正</h2><ul>\n<li>即真实单词拼写错误的纠正。为了解决这类任务需要对噪声信道模型进行扩展：在产生候选纠正词时，需要包括该单词本身以及同音异形词。之后根据整个句子的最大似然来选择正确的纠正词。</li>\n</ul>\n<h1 id=\"第六章：隐马尔科夫模型和最大熵模型\"><a href=\"#第六章：隐马尔科夫模型和最大熵模型\" class=\"headerlink\" title=\"第六章：隐马尔科夫模型和最大熵模型\"></a>第六章：隐马尔科夫模型和最大熵模型</h1><ul>\n<li>隐马尔科夫模型用来解决序列标注（序列分类问题）。</li>\n<li>最大熵方法是一种分类思想，在满足给定条件下分类应满足限制最小（熵最大），满足奥卡姆剃刀原理。</li>\n<li>最大熵马尔可夫模型是最大熵方法在序列标注任务上的扩展。</li>\n</ul>\n<h2 id=\"马尔可夫链\"><a href=\"#马尔可夫链\" class=\"headerlink\" title=\"马尔可夫链\"></a>马尔可夫链</h2><ul>\n<li>加权有限自动状态机是对有限自动状态机的扩展，每条转移路径上加上了概率作为权重，说明从这条路径转移的可能性。马尔可夫链是加权有限状态自动机的一种特殊情况，其输入序列唯一确定了自动机会经过的状态序列。马尔可夫链只能对确定性序列分配概率。</li>\n<li>我们将马尔可夫链看作一种概率图模型，一个马尔可夫链由下面的成分确定：<script type=\"math/tex; mode=display\">\nQ=q_1 q_2…q_N \\\\\nA=a_{01} a_{02} … a_{n1} … a_{nn} \\\\\nq_0,q_F \\\\</script></li>\n<li>分别是<ul>\n<li>状态集合</li>\n<li>转移概率矩阵，其中a_ij代表了从状态i转移到状态j的概率$P(q_j |q_i)$</li>\n<li>特殊的开始状态和结束状态</li>\n</ul>\n</li>\n<li>概率图表示将状态看成图中的点，将转移看成边。</li>\n<li>一阶马尔可夫对转移做了很强的假设：某一状态的概率只与前一状态相关：<script type=\"math/tex; mode=display\">\nP(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})</script></li>\n<li>马尔可夫链的另一种表示不需要开始和结束状态：<script type=\"math/tex; mode=display\">\n\\pi = \\pi _1,\\pi _2 , … , \\pi _N \\\\\nQA={q_x,q_y…} \\\\</script></li>\n<li>分别是：<ul>\n<li>状态的初始概率分布，马尔可夫链以概率$\\pi _i$从状态i开始</li>\n<li>集合QA是Q的子集，代表合法的接受状态</li>\n</ul>\n</li>\n<li>因此状态1作为初始状态的概率既可以写成$a_{01}$也可以写成$\\pi _1$。</li>\n</ul>\n<h2 id=\"隐马尔科夫模型\"><a href=\"#隐马尔科夫模型\" class=\"headerlink\" title=\"隐马尔科夫模型\"></a>隐马尔科夫模型</h2><ul>\n<li>当马尔可夫链已知时，我们可以用其计算一个观测序列出现的概率。但是观测序列可能依赖于一些不可观测的隐变量，我们可能感兴趣的是推断出这些隐变量。隐马尔科夫模型允许我们同时考虑观测变量和隐变量。</li>\n<li>如之前一样定义隐马尔科夫模型：<ul>\n<li>Q：大小为N的状态集</li>\n<li>A：大小为N*N的转移概率矩阵</li>\n<li>O：大小为T的观察事件集</li>\n<li>B：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察$o_t$的概率</li>\n<li>$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量</li>\n</ul>\n</li>\n<li>同样的，隐马尔科夫也可以用另一种不依赖初始和结束状态的方式表示。隐马尔科夫模型也做了两个假设，分别是隐状态之间转移和隐状态到观察量转移的一阶马尔可夫性。</li>\n<li>对于隐马尔科夫模型需要解决三类问题：<ul>\n<li>似然计算：已知参数和观测序列，求似然$P(O|\\lambda)$</li>\n<li>解码：已知参数和观测序列，求隐状态序列</li>\n<li>学习：已知观测序列和隐状态集合，求解模型参数</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"计算似然：前向算法\"><a href=\"#计算似然：前向算法\" class=\"headerlink\" title=\"计算似然：前向算法\"></a>计算似然：前向算法</h2><ul>\n<li>对于马尔可夫链，其没有隐状态到观测量的转移概率矩阵，可以看成观察量与隐状态相同。在隐马尔科夫模型中不能直接计算似然，我们需要直到隐状态序列。</li>\n<li>先假设隐状态序列已知，则似然计算为：<script type=\"math/tex; mode=display\">\nP(O│Q) = \\prod _{i=1}^T P(o_i |q_i)</script></li>\n<li>根据隐状态转移的一阶马尔可夫性，可以求得隐状态的先验，乘以似然得到观测序列和隐状态序列的联合概率：<script type=\"math/tex; mode=display\">\nP(O,Q)=P(O│Q) \\* P(Q) = \\prod _{i=1}^n P(o_i│q_i )  \\prod _{i=1}^n P(q_i |q _{i-1})</script></li>\n<li>对于联合概率积分掉隐状态序列，就可以得到观测概率的似然：<script type=\"math/tex; mode=display\">\nP(O) = \\sum _Q P(O,Q) = \\sum _Q P(O|Q)P(Q)</script></li>\n<li>这样计算相当于考虑了所有的隐状态可能，并对每一种可能从隐状态序列开始到结束计算一次似然，实际上可以保留每次计算的中间状态来减少重复计算，也就是动态规划。在前向计算HMM观测似然使用的动态规划算法称为前向算法：<ul>\n<li>令$\\alpha _t (j)$代表在得到前t个观测量之后当前时刻隐变量处于状态j的概率,\\lambda为模型参数：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\\lambda)</script></li>\n<li>这个概率值可以根据前一时间步的\\alpha值计算出来，避免了每次从头开始计算：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = \\sum _{i=1}^N \\alpha _{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>初始化$\\alpha _1 (j)$：<script type=\"math/tex; mode=display\">\n\\alpha _1 (j)=a_{0s} b_s (o_1)</script></li>\n<li>终止状态：<script type=\"math/tex; mode=display\">\nP(O│\\lambda) = \\alpha _T (q_F) = \\sum _{i=1}^N \\alpha _T (i) \\alpha _{iF}</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"解码：维特比算法\"><a href=\"#解码：维特比算法\" class=\"headerlink\" title=\"解码：维特比算法\"></a>解码：维特比算法</h2><ul>\n<li>解码任务是根据观测序列和参数推断出最有可能隐状态序列。最朴素的做法：对于每种可能的隐状态序列，计算观测序列的似然，取似然最大时对应的隐状态序列。但是这样做就如同朴素的计算似然方法一样，时间复杂度过高，同样的，我们使用动态规划来缩小求解的规模。在解码时使用了一种维特比算法。<ul>\n<li>令$v_t (j)$代表已知前t个观测量（1~t）和已知前t个隐状态（0~t-1）的条件下，当前时刻隐状态为j的概率：<script type=\"math/tex; mode=display\">\nv_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 … o_t,q_t=j|\\lambda)</script></li>\n<li>其中我们已知了前t个时间步最大可能的隐状态序列，这些状态序列也是通过动态规划得到的：<script type=\"math/tex; mode=display\">\nv_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>为了得到最佳的隐状态序列，还需要记录每一步的最佳选择，方便回溯得到路径：<script type=\"math/tex; mode=display\">\n{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>初始化：<script type=\"math/tex; mode=display\">\nv_1 (j) = a_{0j} b_j (o_1) \\ \\  1 \\leq j \\leq N \\\\\n{bt}_1 (j) = 0 \\\\</script></li>\n<li>终止，分别得到最佳隐状态序列（回溯开始值）及其似然值：<script type=\"math/tex; mode=display\">\nP \\* = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) \\* a_{i,F} \\\\\nq_{T\\*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) \\* a_{i,F} \\\\</script></li>\n</ul>\n</li>\n<li>维特比算法减小时间复杂度的原因在于其并没有计算所有的隐状态路径，而是利用了每一时间步的最佳路径只能从上一时间步的最佳路径中延伸而来这一条件，减少了路径候选，避免了许多不必要的路径计算。并且每一步利用上一步的结果也是用了动态规划的思想减少了计算量。</li>\n</ul>\n<h2 id=\"训练隐马尔科夫模型：前向后向算法\"><a href=\"#训练隐马尔科夫模型：前向后向算法\" class=\"headerlink\" title=\"训练隐马尔科夫模型：前向后向算法\"></a>训练隐马尔科夫模型：前向后向算法</h2><ul>\n<li>学习问题是指已知观测序列和隐状态集合，求解模型参数。</li>\n<li>前向后向算法，又称Baum-Welch算法，是EM算法的一种特例，用来求解包含隐变量的概率生成模型的参数。该算法通过迭代的方式反复更新转移概率和生成概率，直到收敛。BW算法通过设计计数值之比作为隐变量，将转移概率矩阵和生成概率矩阵一起迭代更新。</li>\n<li>先考虑马尔科夫链的学习问题。马尔科夫链可以看作是退化的隐马尔科夫模型，即每个隐变量只生成和自己一样的观测量，生成其他观测量的概率为0。因此只需学习转移概率。</li>\n<li>对于马尔可夫链，可以通过古典概型统计出转移概率：<script type=\"math/tex; mode=display\">\na_{ij} = \\frac {Count(i \\rightarrow j)} {\\sum _{q \\in Q} Count(i \\rightarrow q)}</script></li>\n<li>我们可以这样直接计算概率是因为在马尔可夫链中我们知道当前所处的状态。对于隐马尔科夫模型我们无法这样直接计算是因为对于给定输入，隐状态序列无法确定。Badum-Welch算法使用了两种简洁的直觉来解决这一问题：<ul>\n<li>迭代估计，先假设一种转移概率和生成概率，再根据假设的概率推出更好的概率</li>\n<li>计算某一观测量的前向概率，并将这个概率分摊到不同的路径上，通过这种方式估计概率</li>\n</ul>\n</li>\n<li>首先类似于前向概率，我们定义后向概率：<ul>\n<li>令$\\beta _t (i)$代表在得到后t个观测量之后当前时刻隐变量处于状态i的概率,$\\lambda$为模型参数：<script type=\"math/tex; mode=display\">\n\\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\\lambda)</script></li>\n<li>类似于后向概率的归纳计算：<script type=\"math/tex; mode=display\">\n\\beta_t (i) = \\sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j),  \\ \\   1≤i≤N,1≤t<T</script></li>\n<li>初始化$\\alpha _1 (j)$：<script type=\"math/tex; mode=display\">\n\\beta _T (i)=\\alpha _(i,F)</script></li>\n<li>终止状态：<script type=\"math/tex; mode=display\">\nP(O│\\lambda)=\\alpha _t (q_F )=\\beta_1 (0)= \\sum _{i=1}^N a_{0j} b_j (o_1) \\beta _1 (j)</script></li>\n</ul>\n</li>\n<li>类似的，我们希望马尔可夫链中的古典概率能帮助我们估计转移概率：<script type=\"math/tex; mode=display\">\na_{ij}^{\\*} = \\frac{从状态i转移到状态j的计数值期望}{从状态i转移出去的计数值期望}</script></li>\n<li>如何估计计数值：我们将整个序列的转移路径计数值转化为时间步之间转移路径计数值之和，时间步之间某一条转移路径的概率为：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j)</script></li>\n<li>首先考虑所有的观测序列和这一转移路径的联合概率（省略了以参数$\\lambda$为条件）：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j,O)</script></li>\n<li>观察下面的概率图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZWVJ.png\" alt=\"FoZWVJ.png\"></li>\n<li>可以看到这一联合概率包含了三个部分：<ul>\n<li>T时刻隐状态为i的前向概率</li>\n<li>T+1时刻隐状态为j的后向概率</li>\n<li>T时刻与T+1时刻的状态转移概率以及生成对应观测量的生成概率</li>\n</ul>\n</li>\n<li>所以有：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j,O)=\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j)</script></li>\n<li>为了从联合分布中得到已知观测序列求转移路径的联合概率，需要计算观测序列的概率，可以通过前向概率或者后向概率求得：<script type=\"math/tex; mode=display\">\nP(O)=\\alpha _t (N)=\\beta _T (1) = \\sum _{j=1}^N \\alpha _t (j) \\beta_t (j)</script></li>\n<li>最终得到<script type=\"math/tex; mode=display\">\nξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \\frac {(\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))}{(\\alpha _t (N))}</script></li>\n<li>最后，对所有时间步求和就可以得到从状态i转移到状态j的期望计数值，从而进一步得到转移概率的估计：<script type=\"math/tex; mode=display\">\na_{ij}^{\\*} = \\frac {\\sum _{t=1}^{T-1} ξ_t (i,j)}{\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1} ξ_t (i,j)}</script></li>\n<li>同样的，我们还希望得到生成概率的估计：<script type=\"math/tex; mode=display\">\nb_{j}^{\\*} (v_k) = \\frac {在状态j观测到符号v_k 的计数值期望}{状态j观测到所有符号的计数值期望}</script></li>\n<li>类似的，通过先计算联合分布再计算条件分布的方式得到在t时刻处于隐状态j的概率：<script type=\"math/tex; mode=display\">\nγ_t (j)=P(q_t=j│O) = \\frac {P(q_t=j,O)}{P(O)}</script></li>\n<li>联合概率包含两个部分，即t时刻处于状态j的前向概率和后向概率，所以有：<script type=\"math/tex; mode=display\">\nγ_t (j) = \\frac {\\alpha _t (j) \\beta_t (j)}{\\alpha _t (N)}</script></li>\n<li>类似的，对所有时间步累加，进而得到生成概率的估计：<script type=\"math/tex; mode=display\">\nb_{j}^{\\*} (v_k) = \\frac{\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) }{\\sum _{t=1}^T   γ_t (j) }</script></li>\n<li>这两个式子是在已知前向概率和后向概率$(\\alpha,\\beta)$的情况下，计算出中间变量（隐变量）(ξ,γ),引入隐变量的动机是将a、b估计值的期望计数值之比转化为概率之比，且这两个隐变量可以用a,b表示。再由隐变量计算出转移概率和生成概率，因此形成了一个迭代的循环，可以用EM算法求解：<script type=\"math/tex; mode=display\">\na,b→\\alpha,\\beta→ξ,γ→a,b</script></li>\n<li>E-step:<script type=\"math/tex; mode=display\">\nγ_t (j) = (\\alpha _t (j) \\beta_t (j))/(\\alpha _t (N)) ξ_t (i,j) \\\\\n= (\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))/(\\alpha _t (N)) \\\\</script></li>\n<li>M-step（最大化的目标是什么）:<script type=\"math/tex; mode=display\">\na _{ij} = (\\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1}   ξ_t (i,j)  ) \\\\\nb ̂_j(v_k) = (\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\\sum _{t=1}^T   γ_t (j) ) \\\\</script></li>\n<li>迭代时需重新计算：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = \\sum _{i=1}^N   \\alpha_{t-1} (i) a_ij b_j (o_t) \\\\\n\\beta_t (i) = \\sum _{j=1}^N   a_ij b_j (o_{t+1} ) \\beta_{t+1} (j)  \\\\</script></li>\n<li>迭代的初始状态对于EM算法来说很重要，经常是通过引入一些外部信息来设计一个好的初始状态。</li>\n</ul>\n<h2 id=\"最大熵模型：背景\"><a href=\"#最大熵模型：背景\" class=\"headerlink\" title=\"最大熵模型：背景\"></a>最大熵模型：背景</h2><ul>\n<li>最大熵模型另一种广为人知的形式是多项Logistic回归（Softmax?）。</li>\n<li>最大熵模型解决分类问题，最大熵模型作为一种概率分类器，能够根据样本的特征求出样本属于每一个类别的概率，进而进行分类。</li>\n<li>最大熵模型属于指数家族（对数线性）分类器，通过将特征线性组合，取指数得到分类概率：<script type=\"math/tex; mode=display\">\np(c│x)=\\frac 1Z exp⁡(\\sum _i   weight_i feature_i)</script></li>\n<li>Z是一个归一化系数，使得生成的概率之和为1。</li>\n</ul>\n<h2 id=\"最大熵建模\"><a href=\"#最大熵建模\" class=\"headerlink\" title=\"最大熵建模\"></a>最大熵建模</h2><ul>\n<li>将二分类Logistic回归推广到多分类问题就得到：<script type=\"math/tex; mode=display\">\nP(c│x) = \\frac {exp⁡(\\sum _(i=0)^N   w_ci f_i) } {\\sum _{c^{\\*} in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i)  }</script></li>\n<li>语音和语言处理中的特征通常是二值的（是否有该特征），因此使用指示函数表示特征<script type=\"math/tex; mode=display\">\nP(c│x) = \\frac {exp⁡(\\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\\sum _{c^{\\*} \\in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i (c^{\\*},x))  }</script></li>\n<li>注意到在该模型中每一个类都有其独立的线性权重w_c。相比于硬分布，最大熵模型能够给出分到每一类的概率，因此可以求出每一时刻的分类概率进而求出整体分类概率，得到全局最优分类结果。注意到不同于支持向量机等模型，最大熵模型无法利用特征之间的组合，必须手动构造组合作为新的特征。</li>\n<li>一般使用加了正则化的最大似然作为优化的目标函数：<script type=\"math/tex; mode=display\">\nw ̂={argmax} _w \\sum _i   \\log P(y^{(i)}│x^{(i) } ) - \\alpha \\sum _{j=1}^N w_j^2</script></li>\n<li>这种正则化相当于给权重的概率分布加了一个零均值高斯先验，权重越偏离均值，即权重越大，其概率越低。</li>\n<li>为什么多分类Logistic回归是最大熵模型：最大熵模型保证在满足给定约束下，无约束的部分分类应该是等概率分配，例如在两个约束下：<script type=\"math/tex; mode=display\">\nP(NN)+P(JJ)+P(NNS)+P(VB)=1 \\\\\nP(t_i=NN or t_i=NNS)=8/10 \\\\</script></li>\n<li>则满足这两个约束，最大熵模型分配的概率结果为：<script type=\"math/tex; mode=display\">\np(NN)=4/10  \\\\\np(JJ)=1/10  \\\\\np(NNS)=4/10  \\\\\np(VB)=1/10 \\\\</script></li>\n<li>在The equivalence of logistic regression and maximum entropy models一文中证明了在广义线性回归模型的平衡条件约束下，满足最大熵分布的非线性激活函数就是sigmoid，即logistic回归。</li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型\"><a href=\"#最大熵马尔可夫模型\" class=\"headerlink\" title=\"最大熵马尔可夫模型\"></a>最大熵马尔可夫模型</h2><ul>\n<li>最大熵模型只能对单一观测量分类，使用最大熵马尔可夫模型可以将其扩展到序列分类问题上。</li>\n<li>最大熵马尔可夫比隐马尔科夫模型好在哪儿？隐马尔科夫模型对于每个观测量的分类依赖于转移概率和生成概率，假如我们想要在标注过程中引入外部知识，则需要将外部知识编码进这两类概率中，不方便。最大熵马尔可夫模型能够更简单的引入外部知识。</li>\n<li>在隐马尔科夫模型中我们优化似然，并且乘以先验来估计后验：<script type=\"math/tex; mode=display\">\nT ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )</script></li>\n<li>在最大熵隐马尔科夫模型中，我们直接计算后验。因为我们直接训练模型来分类，即最大熵马尔可夫模型是一类判别模型，而不是生成模型：<script type=\"math/tex; mode=display\">\nT ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1})</script></li>\n<li>因此在最大熵隐马尔科夫模型中没有分别对似然和先验建模，而是通过一个单一的概率模型来估计后验。两者的区别如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZgrF.png\" alt=\"FoZgrF.png\"> </li>\n<li>另外最大熵马尔可夫模型可以依赖的特征更多，依赖方式更灵活，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZcKU.png\" alt=\"FoZcKU.png\"></li>\n<li>用公式表示这一差别：<script type=\"math/tex; mode=display\">\nHMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\\\\nMEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\\\</script></li>\n<li>当估计单一转移概率（从状态q^{*}转移到状态q，产生观测量o）时，我们使用以下的最大熵模型：<script type=\"math/tex; mode=display\">\nP(q│q^{\\*},o)=\\frac{1}{Z(o,q^{\\*})} exp⁡(\\sum _i   w_i f_i (o,q))</script></li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型的解码（推断）\"><a href=\"#最大熵马尔可夫模型的解码（推断）\" class=\"headerlink\" title=\"最大熵马尔可夫模型的解码（推断）\"></a>最大熵马尔可夫模型的解码（推断）</h2><ul>\n<li>MEMM同样使用维特比算法进行解码</li>\n<li>使用维特比算法解码的通用框架是：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j)</script></li>\n<li>在HMM模型中这一框架具体化为：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t)</script></li>\n<li>在MEMM中直接将似然和先验替换为后验：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t)</script></li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型的训练\"><a href=\"#最大熵马尔可夫模型的训练\" class=\"headerlink\" title=\"最大熵马尔可夫模型的训练\"></a>最大熵马尔可夫模型的训练</h2><ul>\n<li>MEMM作为最大熵模型的推广，训练过程使用和最大熵模型一样的监督算法。如果训练数据的标签序列存在缺失，也可以通过EM算法进行半监督学习。</li>\n</ul>\n<h1 id=\"第十二章：英语的形式语法\"><a href=\"#第十二章：英语的形式语法\" class=\"headerlink\" title=\"第十二章：英语的形式语法\"></a>第十二章：英语的形式语法</h1><h2 id=\"组成性\"><a href=\"#组成性\" class=\"headerlink\" title=\"组成性\"></a>组成性</h2><ul>\n<li>英语中的单词是如何组成一个词组的呢？</li>\n<li>换句话说，我们如何判断一些单词组合成了一个部分？一种可能是这种组合都能在相似的句法环境中出现，例如名词词组都能在一个动词之前出现。另一种可能依据来自于前置和后置结构，例如前置短语on September seventeenth可以放在句子的前面，中间或者后面，但是组合成这个短语的各个部分不能拆出来放在句子的不同位置，因此我们判断on September seventeenth这三个词组成了一个短语。</li>\n</ul>\n<h2 id=\"上下文无关法则\"><a href=\"#上下文无关法则\" class=\"headerlink\" title=\"上下文无关法则\"></a>上下文无关法则</h2><ul>\n<li>上下文无关语法，简称CFG，又称为短语结构语法，其形式化方法等价于Backus-Naur范式。一个上下文无关语法包含两个部分：规则或者产生式，词表。</li>\n<li>例如，用上下文无关语法描述名词词组，一种描述方式是名词词组可以由一个专有名词构成，也可以由一个限定词加一个名词性成分构成，而名词性成分可以是一个或多个名词，此CFG的规则为：<ul>\n<li>NP→Det Nominal</li>\n<li>NP→ProperNoun</li>\n<li>Nominal→Noun|Noun Nominal</li>\n</ul>\n</li>\n<li>CFG可以层级嵌套，因此上面的规则可以与下面表示词汇事实的规则（词表）结合起来：<ul>\n<li>Det→a</li>\n<li>Det→the</li>\n<li>Noun→flight</li>\n</ul>\n</li>\n<li>符号分为两类：<ul>\n<li>终极符号：与现实中单词对应的符号，词表是引入终极符号的规则的集合</li>\n<li>非终极符号：表示终极符号的聚类或者概括性符号</li>\n</ul>\n</li>\n<li>在每个规则里箭头右边包含一个或多个终极符号和非终极符号，箭头左边为一个非终极符号，与每个单词相关联的是其词类范畴（词类）。</li>\n<li>CFG既可以看成是生成句子的一种机制，也可以看成是给一个句子分配结构的机制。</li>\n<li>以之前提到的CFG为例，对一个符号串NP，可以逐步生成：<script type=\"math/tex; mode=display\">\nNP→Det Nominal→Det Noun→a flight</script></li>\n<li>称 a flight是NP的一个推导，一般用一个剖析树表示一种推导：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ5P1.png\" alt=\"FoZ5P1.png\"><br>一个CFG定义了一个形式语言，形式语言是符号串的集合，如果有一个语法推导出的句子处于由该语法定义的形式语言中，这个句子就是合语法的。使用形式语言来模拟自然语言的语法成为生成式语法。</li>\n<li>上下文无关语法的正式定义：<ul>\n<li>N：非终止符号（或者变量）的集合</li>\n<li>Sigma：终止符号的集合，与N不相交</li>\n<li>R：规则或者产生式的集合</li>\n<li>S：指定的开始符号</li>\n</ul>\n</li>\n<li>一些约定定义：<ul>\n<li>大写字母：代表非终止符号</li>\n<li>S：开始符号</li>\n<li>小写希腊字母：从非终止符号和终止符号的并集中抽取出来的符号串</li>\n<li>小写罗马字母：终止符号串</li>\n</ul>\n</li>\n<li>直接导出的定义：<br><strong>公式待补充</strong></li>\n<li>导出是直接导出的泛化。之后我们可以正式定义由语法G生成的语言L是一个由终止符号组成的字符串集合，这些终止符号可以从指定的开始符号S通过语法G导出：<br><strong>公式待补充</strong></li>\n<li>将一个单词序列映射到其对应的剖析树成为句法剖析。</li>\n</ul>\n<h2 id=\"英语的一些语法规则\"><a href=\"#英语的一些语法规则\" class=\"headerlink\" title=\"英语的一些语法规则\"></a>英语的一些语法规则</h2><ul>\n<li>英语中最常用最重要的四种句子结构：<ul>\n<li>陈述式结构：主语名词短语加一个动词短语</li>\n<li>命令式结构：通常以一个动词短语开头，并且没有主语</li>\n<li>Yes-no疑问式结构：通常用于提问，并且以一个助动词开头，后面紧跟一个主语NP，再跟一个VP</li>\n<li>Wh疑问式结构：包含一个wh短语成分</li>\n</ul>\n</li>\n<li>在之前的描述中开始符号用于单独生成整个句子，但是S也可以出现在语法生成规则的右边，嵌入到更大的句子当中。这样的S称为从句，拥有完整的语义。拥有完整的语义是指这个S在整体句子的语法剖析树当中，其子树当中的主要动词拥有所需的所有论元。</li>\n</ul>\n<h2 id=\"名词短语\"><a href=\"#名词短语\" class=\"headerlink\" title=\"名词短语\"></a>名词短语</h2><ul>\n<li>限定词Det：名词短语可以以一些简单的词法限定词开始，例如a,the,this,those,any,some等等，限定词的位置也可以被更复杂的表示替代，例如所有格。这样的表示是可以递归定义的，例如所有格加名词短语可以构成更大的名词短语的限定词。在复数名词、物质名词之前不需要加限定词。</li>\n<li>名词性词Nominal：包含一些名词前或者名词后修饰语</li>\n<li>名词之前，限定词之后：一些特殊的词类可以出现在名词之前限定词之后，包括基数词Card、序数词Ord、数量修饰语Quant。</li>\n<li>形容词短语AP：形容词短语之前可以出现副词</li>\n<li>可以讲名词短语的前修饰语规则化如下（括号内代表可选）：</li>\n<li>NP-&gt;(Det)(Card)(Ord)(Quant)(AP)Nominal</li>\n<li>后修饰语主要包含三种：<ul>\n<li>介词短语PP：Nominal-&gt;Nominal PP(PP)(PP)</li>\n<li>非限定从句：动名词后修饰语GerundVP,GerundVP-&gt;GerundV NP | GerundV PP | GerundV | GerundV NP PP</li>\n<li>关系从句：以关系代词开头的从句 Nominal -&gt;Nominal RelCaluse;RelCaluse -&gt; (who|that) VP</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"一致关系\"><a href=\"#一致关系\" class=\"headerlink\" title=\"一致关系\"></a>一致关系</h2><ul>\n<li>每当动词有一个名词作为它的主语时，就会发生一致关系的现象，凡是主语和他的动词不一致的句子都是不合语法的句子，例如第三人称单数动词没有加-s。可以使用多个规则的集合来扩充原有的语法，使得语法可以处理一致关系。例如yes-no疑问句的规则是<script type=\"math/tex; mode=display\">\nS \\rightarrow Aux \\ NP \\ VP</script></li>\n<li>可以用如下形式的两个规则来替代：<script type=\"math/tex; mode=display\">\nS \\rightarrow 3sgAux \\ 3sgNP \\ VP \\\\\nS \\rightarrow Non3sgAux \\ Non3sgNP \\ VP \\\\</script></li>\n<li>再分别指定第三人称单数和非第三人称单数的助动词形态。这样的方法会导致语法规模增加。</li>\n</ul>\n<h2 id=\"动词短语和次范畴化\"><a href=\"#动词短语和次范畴化\" class=\"headerlink\" title=\"动词短语和次范畴化\"></a>动词短语和次范畴化</h2><ul>\n<li>动词短语包括动词和其他一些成分的组合，包括NP和PP以及两者的组合。整个的嵌入句子也可以跟随在动词之后，成为句子补语。</li>\n<li>动词短语的另一个潜在成分是另一个动词短语。</li>\n<li>动词后面也可以跟随一个小品词，小品词类似于借此，但与动词组合在一起是构成一个短语动词，与动词不可分割。</li>\n<li>次范畴化即再分类。传统语法把动词次范畴化为及物动词和不及物动词，而现代语法已经把动词区分为100个次范畴。讨论动词和可能的成分之间的关系是将动词看成一个谓词，而成分想象成这个谓词的论元(argument)。</li>\n<li>对于动词和它的补语之间的关系，我们可以用上下文无关语法表示一致关系特征，且需要区分动词的各个次类。</li>\n</ul>\n<h2 id=\"助动词\"><a href=\"#助动词\" class=\"headerlink\" title=\"助动词\"></a>助动词</h2><ul>\n<li>助动词是动词的一个次类，具有特殊的句法约束。助动词包括情态动词、完成时助动词、进行时助动词、被动式助动词。每一个助动词都给他后面的动词形式一个约束，且需要按照一定的顺序进行结合。</li>\n<li>四种助动词给VP次范畴化时，VP的中心动词分别是光杆动词、过去分词形式、现在分词形式、过去分词形式。</li>\n<li>一个句子可以用多个助动词，但是要按照情态助动词、完成时助动词、进行式助动词、被动式助动词的顺序。</li>\n</ul>\n<h2 id=\"树图资料库\"><a href=\"#树图资料库\" class=\"headerlink\" title=\"树图资料库\"></a>树图资料库</h2><ul>\n<li>上下文无关语法可以将一个句子剖析成一个句法剖析树，如果一个语料中所有句子都以句法剖析树的形式表示，这样的句法标注了的语料就称为树图资料库(treebank)。</li>\n<li>树图资料库中的句子隐含的组成了一种语言的语法，我们可以对于每一棵句法剖析树提取其中的CFG规则。从宾州树库中提取出来的CFG规则非常扁平化，使得规则数量很多且规则很长。</li>\n<li>在树库中搜索需要一种特殊的表达式，能够表示关于节点和连接的约束，用来搜索特定的模式。例如tgrep或者TGrep2。</li>\n<li>在tgrep、TGrep2中的一个模式由一个关于节点的描述组成，一个节点描述可以用来返回一个以此节点为根的子树。</li>\n<li>可以使用双斜线对某一类模式命名：<script type=\"math/tex; mode=display\">\n/NNS?/    NN|NNS</script></li>\n<li>Tgrep/Tgrep2模式的好处在于能够描述连接的信息。小于号代表直接支配，远小于符号代表支配，小数点代表线性次序。这种对于连接的描述反应在剖析树中的关系如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ2b4.png\" alt=\"FoZ2b4.png\"></li>\n</ul>\n<h2 id=\"中心词和中心词查找\"><a href=\"#中心词和中心词查找\" class=\"headerlink\" title=\"中心词和中心词查找\"></a>中心词和中心词查找</h2><ul>\n<li>句法成分能够与一个词法中心词相关联。在一个简单的词法中心词模型中，每一个上下文无关规则与一个中心词相关联，中心词传递给剖析树，因此剖析树中每一个非终止符号都被一个单一单词所标注，这个单一单词就是这个非终止符号的中心词。一个例子如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZfa9.png\" alt=\"FoZfa9.png\"></li>\n<li>为了生成这样一棵树，每一个CFG规则都必须扩充来识别一个右手方向的组成成分来作为中心词子女节点。一个节点的中心词词被设置为其子女中心词的中心词。</li>\n<li>另一种方式是通过一个计算系统来完成中心词查找。在这种方式下是依据树的上下文来寻找指定的句子，从而动态的识别中心词。一旦一个句子被解析出来，树将会被遍历一遍并使用合适的中心词来装饰每一个节点。</li>\n</ul>\n<h2 id=\"语法等价与范式\"><a href=\"#语法等价与范式\" class=\"headerlink\" title=\"语法等价与范式\"></a>语法等价与范式</h2><ul>\n<li>语法等价包括两种：强等价，即两个语法生成相同的符号串集合，且他们对于每个句子都指派同样的短语结构；弱等价，即两个语法生成相同的符号串集合，但是不给每个句子指派相同的短语结构。</li>\n<li>语法都使用一个范式，在范式中每个产生式都使用一个特定的形式。例如一个上下文五官与法是sigma自由的，并且如果他们的每个产生式的形式为A-&gt;BC或者是A-&gt;a，就说明这个上下文无关语法是符合Chomsky范式的，简称CNF。凡是Chomsky范式的语法都具有二叉树形式。任何上下文无关语法都可以转变成一个弱等价的Chomsky范式语法。</li>\n<li>使用二叉树形式的剖析树能够产生更小的语法。形如A-&gt;A B的规则称为Chomsky并连。</li>\n</ul>\n<h2 id=\"有限状态语法和上下文无关语法\"><a href=\"#有限状态语法和上下文无关语法\" class=\"headerlink\" title=\"有限状态语法和上下文无关语法\"></a>有限状态语法和上下文无关语法</h2><ul>\n<li>复杂的语法模型必须表示组成性，因而不适合用有限状态模型来描述语法。</li>\n<li>当一个非终止符号的展开式中也包含了这个非终止符号时，就会产生语法的递归问题。</li>\n<li>例如，使用正则表达式来描述以Nominal为中心的名词短语：<br>(Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*</li>\n<li>为了完成这个正则表达式，只需要按顺序展开PP，展开结果为(P NP)*，这样就出现了地柜问题，因为此时出现了NP，在NP的正则表达式中出现了NP。</li>\n<li>一个上下文无关语法能够被有限自动机生成，当且仅当存在一个生成语言L的没有任何中心自嵌入递归的上下文无关语法。</li>\n</ul>\n<h2 id=\"依存语法\"><a href=\"#依存语法\" class=\"headerlink\" title=\"依存语法\"></a>依存语法</h2><ul>\n<li>依存语法与上下文无关语法相对，其句法结构完全由词、词与词之间的语义或句法关系描述。一个例子如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZOVH.png\" alt=\"FoZOVH.png\"></li>\n<li>其中没有非终止符号或者短语节点，树中的连接只将两个词语相连。连接即依存关系，代表着语法功能或者一般的语义联系，例如句法主语、直接对象、间接宾语、时间状语等等。</li>\n<li>依存语法具有很强的预测剖析能力，且在处理具有相对自由词序的语言时表现更好。</li>\n</ul>\n<h1 id=\"第十三章：基于上下文无关语法的剖析\"><a href=\"#第十三章：基于上下文无关语法的剖析\" class=\"headerlink\" title=\"第十三章：基于上下文无关语法的剖析\"></a>第十三章：基于上下文无关语法的剖析</h1><h2 id=\"剖析即搜索\"><a href=\"#剖析即搜索\" class=\"headerlink\" title=\"剖析即搜索\"></a>剖析即搜索</h2><ul>\n<li>在句法剖析中，剖析可以看成对一个句子搜索一切可能的剖析树空间并发现正确的剖析树。</li>\n<li>对于某一个句子（输入符号串），剖析搜索的目标是发现以初始符号S为根并且恰好覆盖整个输入符号串的一切剖析树。搜索算法的约束来自两方面：<ul>\n<li>来自数据的约束，即输入句子本身，搜索出来的剖析树的叶子应该是原句的所有单词。</li>\n<li>来自语法的约束，搜索出来的剖析树应该有一个根，即初始符号S</li>\n</ul>\n</li>\n<li>根据这两种约束，产生了两种搜索策略：自顶向下，目标制导的搜索；自下而上，数据制导的搜索。</li>\n<li>对于自顶向下的搜索，从根开始，我们通过生成式不断生成下一层的所有可能子节点，搜索每一层的每一种可能，如下图（对于句子book that flight）：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZh5R.png\" alt=\"FoZh5R.png\"></li>\n<li>对于自底向上的搜索，剖析从输入的单词开始，每次都使用语法中的规则，试图从底部的单词向上构造剖析树，如果剖析树成功的构造了以初始符号S为根的树，而且这个树覆盖了整个输入，那么就剖析成功。首先通过词表将每个单词连接到对应的词类，如果一个单词有不止一个词类，就需要考虑所有可能。与自顶向下相反，每次进入下一层时，自底向上需要考虑被剖析的成分是否与某个规则的右手边相匹配，而自顶向下是与左手边相匹配。中途如果无法匹配到规则则将这个树枝从搜索空间中删除，如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZI8x.png\" alt=\"FoZI8x.png\"> </li>\n<li>两者对比：<ul>\n<li>自顶向下是从S开始搜索的，因此不会搜索那些在以S为根的树中找不到位置的子树，而自底向上会产生许多不可能的搜索树</li>\n<li>相对应的，自顶向下把搜索浪费在了不可能产生输入单词序列的树上</li>\n<li>综上，我们需要将自顶向下和自底向上相结合</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"歧义\"><a href=\"#歧义\" class=\"headerlink\" title=\"歧义\"></a>歧义</h2><ul>\n<li>在句法剖析中需要解决的一个问题是结构歧义，即语法会给一个句子多种剖析结果可能。</li>\n<li>最常见的两种歧义：附着歧义和并列连接歧义。</li>\n<li>如果一个特定的成分可以附着在剖析树的一个以上的位置，句子就会出现附着歧义。例如We saw the Eiffel Tower flying to Paris一句中,flying to Paris可以修饰Eiffel Tower也可以修饰We。</li>\n<li>在并列连接歧义中，存在着不同的短语，这些短语之间用and这样的连接词相连。例如old men and women可以是老年男性和老年女性，或者老年男性和普通女性，即old是否同时分配到men和women上。</li>\n<li>以上两种歧义还能相互组合嵌套形成更复杂的歧义。假如我们不消歧，仅仅返回所有的可能，留给用户或者人工判断，则随着剖析句子结构变复杂或者剖析规则的增加，得到的可能是成指数级增长的，具体而言，这种剖析句子可能的增长数和算术表达式插入括号问题相同，以Catalan数按指数增长：<script type=\"math/tex; mode=display\">\nC(n)=\\frac{1}{1+n} C_{2n}^n</script></li>\n<li>摆脱这种指数爆炸的方法有两个：<ul>\n<li>动态规划，研究搜索空间的规律性，使得常见的部分只推导一次，减少与歧义相关的开销</li>\n<li>使用试探性的方法来改善剖析器的搜索策略</li>\n</ul>\n</li>\n<li>使用例如深度优先搜索或者宽度优先搜索之类的有计划与回溯的搜索算法是在复杂搜索空间中搜索常用的算法，然而在复杂语法空间中无处不在的歧义使得这一类搜索算法效率低下，因为有许多重复的搜索过程。</li>\n</ul>\n<h2 id=\"动态规划剖析方法\"><a href=\"#动态规划剖析方法\" class=\"headerlink\" title=\"动态规划剖析方法\"></a>动态规划剖析方法</h2><ul>\n<li>在动态规划中，我们维护一个表，系统的将对于子问题的解填入表中，利用已经存储的子问题的解解决更大的子问题，而不用重复从头开始计算。</li>\n<li>在剖析中，这样的表用来存储输入中各个部分的子树，当子树被发现时就存入表中，以便以后调用，就这样解决了重复剖析的问题（只需查找子树而不需要重新剖析）和歧义问题（剖析表隐含的存储着所有可能的剖析结果）。</li>\n<li>主要的三种动态规划剖析方法有三种，CKY算法、Earley算法和表剖析算法。</li>\n</ul>\n<h3 id=\"CKY剖析\"><a href=\"#CKY剖析\" class=\"headerlink\" title=\"CKY剖析\"></a>CKY剖析</h3><ul>\n<li>CKY剖析要求语法必须满足Chomsky范式，即生成式右边要么时两个非终止符号要么是一个终止符号。如果不是Chomsky范式，则需要把一个一般的CFG转换成CNF：<ul>\n<li>右边有终止符号也有非终止符号：给右边的终止符号单独建一个非终止符号，例如：INF-VP → to VP，改成INF-VP → TO VP和TO → to</li>\n<li>右边只有一个非终止符号：这种非终止符号称为单元产物，它们最终会生成非单元产物，用最终生成的非单元产物规则来替换掉单元产物</li>\n<li>右边不止2个符号：引入新的非终止符号将规则分解</li>\n<li>词法规则保持不变，但是在转换的过程中可能会生成新的词法规则</li>\n</ul>\n</li>\n<li>当所有的规则都转换成CNF之后，表中的非终止符号在剖析中有两个子节点，且表中每一个入口代表了输入中的某个区间，对于某个入口例如[0,3]，其可以被拆分成两部分，假如一部分为[0,2]，则另一部分为[2,3]，前者在[0,3]的左边，后者在[0,3]的正下方，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZo26.png\" alt=\"FoZo26.png\"></li>\n<li>接下来就是如何填表，我们通过自底向上的方法来剖析，对于每个入口[i,j]，包含了输入中i到j这一区间部分的表格单元都会对这个入口值做出贡献，即入口[i,j]左边的单元和下边的单元。下表中的CKY伪算法图描述了这一过程：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZjIA.png\" alt=\"FoZjIA.png\"></li>\n<li>外层循环从左往右循环列，内层循环从下往上循环行，而最里面的循环式遍历串[i,j]的所有可能二分子串，表中存的是可以代表[i,j]区间符号串的非终止符号集合，因为是集合，所以不会出现重复的非终止符号。</li>\n<li>现在我们完成了识别任务，接下来是剖析。剖析即在[0,N]入口，对应整个句子，找到一个非终止符号作为起始符号S。首先我们要对算法做两点更改：<ul>\n<li>存入表中的不仅仅是非终止符号，还有其对应的指针，指向生成这个非终止符号的表入口</li>\n<li>允许一个入口中存在同一个非终止符号的不同版本</li>\n</ul>\n</li>\n<li>做了这些改动之后，这张表就包含了一个给定输入的所有可能剖析信息。我们可以选择[0,N]入口中任意一个非终止符号作为起始符号S，然后根据指针迭代提取出剖析信息。</li>\n<li>当然，返回所有的可能剖析会遇到指数爆炸的问题，因此我们在完整的表上应用维特比算法，计算概率最大的剖析并返回这个剖析结果。</li>\n</ul>\n<h3 id=\"Early算法\"><a href=\"#Early算法\" class=\"headerlink\" title=\"Early算法\"></a>Early算法</h3><ul>\n<li>相比CKY自底向上的剖析，Early算法采用了自顶向下的剖析，而且只用了一维的表保存状态，每个状态包含三类信息：<ul>\n<li>对应某一单一语法规则的子树</li>\n<li>子树的完成状态</li>\n<li>子树对应于输入中的位置</li>\n</ul>\n</li>\n<li>算法流程图如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZHKO.png\" alt=\"FoZHKO.png\"></li>\n<li>算法对于状态的操作有三种：<ul>\n<li>预测：造出一个新的状态来表示在剖析过程中生成的自顶向下的预测。当待剖析的状态为非终极符号但又不是词类范畴时，对于这个非终极符号的不同展开，预测操作都造出一个新的状态。</li>\n<li>扫描：当待剖析的状态是词类范畴时，就检查输入符号串，并把对应于所预测的词类范畴的状态加入线图中。</li>\n<li>完成：当右边所有状态剖析完成时，完成操作查找输入中在这个位置的语法范畴，发现并推进前面造出的所有状态。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"表剖析\"><a href=\"#表剖析\" class=\"headerlink\" title=\"表剖析\"></a>表剖析</h3><ul>\n<li>表剖析允许动态的决定表格处理的顺序，算法动态的依照计划依次删除图中的一条边，而计划中的元素排序是由规则决定的。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZTxK.png\" alt=\"FoZTxK.png\"></li>\n</ul>\n<h2 id=\"部分剖析\"><a href=\"#部分剖析\" class=\"headerlink\" title=\"部分剖析\"></a>部分剖析</h2><ul>\n<li>有时我们只需要输入句子的部分剖析信息</li>\n<li>可以用有限状态自动机级联的方式完成部分剖析，这样会产生比之前提到的方法更加“平”的剖析树。</li>\n<li>另一种有效的部分剖析的方法是分块。使用最广泛覆盖的语法给句子做词类标注，将其分为有主要词类标注信息且不没有递归结构的子块，子块之间不重叠，就是分块。</li>\n<li>我们用中括号将每一个分块框起来，有可能一些词并没有被框住，属于分块之外。</li>\n<li>分块中最重要的是基本分块中不能递归包含相同类型的成分。</li>\n</ul>\n<h3 id=\"基于规则的有限状态分块\"><a href=\"#基于规则的有限状态分块\" class=\"headerlink\" title=\"基于规则的有限状态分块\"></a>基于规则的有限状态分块</h3><ul>\n<li>利用有限状态方式分块，需要为了特定目的手动构造规则，之后从左到右，找到最长匹配分块，并接着依次分块下去。这是一个贪心的分块过程，不保证全局最优解。</li>\n<li>这些分块规则的主要限制是不能包含递归。</li>\n<li>使用有限状态分块的优点在于可以利用之前转录机的输出作为输入来组成级联，在部分剖析中，这种方法能够有效近似真正的上下文无关剖析器。</li>\n</ul>\n<h3 id=\"基于机器学习的分块\"><a href=\"#基于机器学习的分块\" class=\"headerlink\" title=\"基于机器学习的分块\"></a>基于机器学习的分块</h3><ul>\n<li>分块可以看成序列分类任务，每个位置分类为1（分块）或者0（不分块）。用于训练序列分类器的机器学习方法都能应用于分块中。</li>\n<li>一种卓有成效的方法是将分块看成类似于词类标注的序列标注任务，用一个小的标注符号集同时编码分块信息和每一个块的标注信息，这种方式称为IOB标注，用B表示分块开始，I表示块内，O表示块外。其中B和I接了后缀，代表该块的句法信息。</li>\n<li>机器学习需要训练数据，而分块的已标数据很难获得，一种方法是使用已有的树图资料库，例如宾州树库。</li>\n</ul>\n<h3 id=\"评价分块系统\"><a href=\"#评价分块系统\" class=\"headerlink\" title=\"评价分块系统\"></a>评价分块系统</h3><ul>\n<li>准确率：模型给出的正确分块数/模型给出的总分块数</li>\n<li>召回率：模型给出的正确分块数/文本中总的正确分块数</li>\n<li>F1值：准确率和召回率的调和平均</li>\n</ul>\n<h1 id=\"第十四章：统计剖析\"><a href=\"#第十四章：统计剖析\" class=\"headerlink\" title=\"第十四章：统计剖析\"></a>第十四章：统计剖析</h1><h2 id=\"概率上下文无关语法\"><a href=\"#概率上下文无关语法\" class=\"headerlink\" title=\"概率上下文无关语法\"></a>概率上下文无关语法</h2><ul>\n<li>概率上下文无关语法PCFG是上下文无关语法的一种简单扩展，又称随机上下文无关语法。PCFG在定义上做出了一点改变：<ul>\n<li>N：非终止符号集合</li>\n<li>Σ：终止符号集合</li>\n<li>R：规则集合，与上下文无关语法相同，只不过多了一个概率p，代表某一项规则执行的条件概率$P(\\beta|A)$</li>\n<li>S：一个指定的开始符号</li>\n</ul>\n</li>\n<li>当某个语言中所有句子的概率和为1时，我们称这个PCFG时一致的。一些递归规则可能导致PCFG不一致。</li>\n</ul>\n<h2 id=\"用于消歧的PCFG\"><a href=\"#用于消歧的PCFG\" class=\"headerlink\" title=\"用于消歧的PCFG\"></a>用于消歧的PCFG</h2><ul>\n<li>对于一个给定句子，其某一特定剖析的概率是所有规则概率的乘积，这个乘积既是一个剖析的概率，也是剖析和句子的联合概率。这样，对于出现剖析歧义的句子，其不同剖析的概率不同，通过选择概率大的剖析可以消歧。</li>\n</ul>\n<h2 id=\"用于语言建模的PCFG\"><a href=\"#用于语言建模的PCFG\" class=\"headerlink\" title=\"用于语言建模的PCFG\"></a>用于语言建模的PCFG</h2><ul>\n<li>PCFG为一个句子分配了一个概率（即剖析的概率），因此可以用于语言建模。相比n元语法模型，PCFG在计算生成每一个词的条件概率时考虑了整个句子，效果更好。对于含歧义的句子，其概率是所有可能剖析的概率之和。</li>\n</ul>\n<h2 id=\"PCFG的概率CKY剖析\"><a href=\"#PCFG的概率CKY剖析\" class=\"headerlink\" title=\"PCFG的概率CKY剖析\"></a>PCFG的概率CKY剖析</h2><ul>\n<li>PCFG的概率剖析问题：为一个句子产生概率最大的剖析</li>\n<li>概率CKY算法扩展了CKY算法，CKY剖析树中的每一个部分被编码进一个$(n+1)*(n+1)$的矩阵（只用上三角部分），矩阵中每一个元素包含一个非终止符号集合上的概率分布，可以看成每一个元素也是V维，因此整个存储空间为$(n+1)*(n+1)*V$，其中[i,j,A]代表非终止符号A可以用来表示句子的i位置到j位置这一段的概率。</li>\n<li>算法伪代码：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZbrD.png\" alt=\"FoZbrD.png\"></li>\n<li>可以看到也是用k对某一区间[i,j]做分割遍历，取最大的概率组合作为该区间的概率，并向右扩展区间进行动态规划。</li>\n</ul>\n<h2 id=\"学习到PCFG的规则概率\"><a href=\"#学习到PCFG的规则概率\" class=\"headerlink\" title=\"学习到PCFG的规则概率\"></a>学习到PCFG的规则概率</h2><ul>\n<li>上面的伪算法图用到了每一个规则的概率。如何获取这个概率？两种方法，第一种朴素的方法是在一个已知的树库数据集上用古典概型统计出概率：<script type=\"math/tex; mode=display\">\nP(\\alpha \\rightarrow \\beta | \\alpha) = \\frac{Count(\\alpha \\rightarrow \\beta)}{\\sum _{\\gamma} Count(\\alpha \\rightarrow \\gamma)}</script></li>\n<li>假如我们没有树库，则可以用非概率剖析算法来剖析一个数据集，再统计出概率。但是非概率剖析算法在剖析歧义句子时，需要对每一种可能剖析计算概率，但是计算概率需要概率剖析算法，这样就陷入了鸡生蛋蛋生鸡的死循环。一种解决方案是先用等概率的剖析算法，剖析句子，得出每一种剖析得概率，然后用概率加权统计量，然后重新估计剖析规则的概率，继续剖析，反复迭代直到收敛。这种算法称为inside-outside算法，是前向后向算法的扩展，同样也是EM算法的一种特例。</li>\n</ul>\n<h2 id=\"PCFG的问题\"><a href=\"#PCFG的问题\" class=\"headerlink\" title=\"PCFG的问题\"></a>PCFG的问题</h2><ul>\n<li>独立性假设导致不能很好的建模剖析树的结构性依存：每个PCFG规则被假定为与其他规则独立，例如，统计结果表明代词比名词更有可能称为主语，因此当NP被展开时，如果NP是主语，则展开为代词的可能性较高——这里需要考虑NP在句子种的位置，然而这种概率依存关系是PCFG所不允许的，</li>\n<li>缺乏对特定单词的敏感，导致次范畴化歧义、介词附着、联合结构歧义的问题：例如在介词附着问题中，某一个介词短语into Afghanistan附着于哪一个部分，在PCFG中计算时被抽象化为介词短语应该附着一个哪一个部分，而抽象化的概率来自于对语料的统计，这种统计不会考虑特定的单词。又例如联合结构歧义，假如一个句子的两种可能剖析树使用了相同的规则，而规则在树中的位置不同，则PCFG对两种剖析计算出相同的概率：因为PCFG假定规则之间是独立的，联合概率是各个概率的乘积。</li>\n</ul>\n<h2 id=\"通过拆分和合并非终止符号来改进PCFG\"><a href=\"#通过拆分和合并非终止符号来改进PCFG\" class=\"headerlink\" title=\"通过拆分和合并非终止符号来改进PCFG\"></a>通过拆分和合并非终止符号来改进PCFG</h2><ul>\n<li>先解决结构性依存的问题。之前提到了我们希望NP作为主语和宾语时有不同概率的规则，一种想法就是将NP拆分成主语NP和宾语NP。实现这种拆分的方法是父节点标注，及每个节点标注了其父节点，对于主语NP其父节点是S，对于宾语NP，其父节点是VP，因此不同的NP就得到了区分。除此之外，还可以通过词性拆分的方式增强剖析树。</li>\n<li>拆分会导致规则增多，用来训练每一条规则的数据变少，引起过拟合。因此要通过一个手写规则或者自动算法来根据每个训练集合并一些拆分。</li>\n</ul>\n<h2 id=\"概率词汇化的CFG\"><a href=\"#概率词汇化的CFG\" class=\"headerlink\" title=\"概率词汇化的CFG\"></a>概率词汇化的CFG</h2><ul>\n<li>概率CKY剖析更改了语法规则，而概率词汇化模型更改了概率模型本身。对于每一条规则，不仅要产生成分的规则变化，还要在每个成分上标注其中心词和词性，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoeSRP.png\" alt=\"FoeSRP.png\"></li>\n<li>为了产生这样的剖析树，每一条PCFG规则右侧需要选择一个成分作为中心词子节点，用子节点的中心词和词性作为该节点的中心词和词性。<br>其中，规则被分成了两类，内部规则和词法规则，后者是确定的，前者是需要我们估计的：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZqqe.png\" alt=\"FoZqqe.png\"></li>\n<li>我们可以用类似父节点标注的思想来拆分规则，拆分后每一部分都对应一种可能的中心词选择。假如我们将概率词汇话的CFG看成一个大的有很多规则CFG，则可以用之前的古典概型来估计概率。但是这样的效果不会很好，因为这样的规则划分太细了，没有足够的数据来估计概率。因此我们需要做出一些独立性假设，将概率分解为更小的概率乘积，这些更小的概率能容易从语料中估计出来。</li>\n<li>不同的统计剖析器区别在于做出怎样的独立性假设。</li>\n<li>Collins剖析如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZzGt.png\" alt=\"FoZzGt.png\"></li>\n<li>其概率拆解为：<script type=\"math/tex; mode=display\">\nP(VP(dumped,VBD)→VBD(dumped,VBD)NP(sacks,NNS)PP(into,P))= \\\\\nP_H (VBD│VP,dumped)\\* \\\\\nP_L (STOP│VP,VBD,dumped)\\* \\\\\nP_R (NP(sacks,NNS)│VP,VBD,dumped)\\* \\\\\nP_R (PP(into,P)│VP,VBD,dumped)\\* \\\\\nP_R (STOP|VP,VBD,dumped) \\\\</script></li>\n<li>给出生成式左边之后，首先生成规则的中心词，之后一个一个从里到外生成中心词的依赖。先从中心词左侧一直生成直到遇到STOP符号，之后生成右边。如上式做出概率拆分之后，每一个概率都很容易从较小的数据量中统计出来。完整的Collins剖析器更为复杂，还考虑了词的距离关系、平滑技术、未知词等等。</li>\n</ul>\n<h2 id=\"评价剖析器\"><a href=\"#评价剖析器\" class=\"headerlink\" title=\"评价剖析器\"></a>评价剖析器</h2><ul>\n<li>剖析器评价的标准方法叫做PARSEVAL测度，对于每一个句子s：<ul>\n<li>标记召回率=(Count(s的候选剖析中正确成分数）)/(Count(s的树库中正确成分数）)</li>\n<li>标记准确率=(Count(s的候选剖析中正确成分数）)/(Count(s的候选剖析中全部成分数）)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"判别式重排序\"><a href=\"#判别式重排序\" class=\"headerlink\" title=\"判别式重排序\"></a>判别式重排序</h2><ul>\n<li>PCFG剖析和Collins词法剖析都属于生成式剖析器。生成式模型的缺点在于很难引入任意信息，即很难加入对某一个PCFG规则局部不相关的特征。例如剖析树倾向于右生成这一特征就不方便加入生成式模型当中。</li>\n<li>对于句法剖析，有两类判别式模型，基于动态规划的和基于判别式重排序的。</li>\n<li>判别式重排包含两个阶段，第一个阶段我们用一般的统计剖析器产生前N个最可能的剖析及其对应的概率序列。第二个阶段我们引入一个分类器，将一系列句子以及每个句子的前N个剖析-概率对作为输入，抽取一些特征的大集合并针对每一个句子选择最好的剖析。特征包括：剖析概率、剖析树中的CFG规则、平行并列结构的数量、每个成分的大小、树右生成的程度、相邻非终止符号的二元语法、树的不同部分出现的次数等等。</li>\n</ul>\n<h2 id=\"基于剖析的语言建模\"><a href=\"#基于剖析的语言建模\" class=\"headerlink\" title=\"基于剖析的语言建模\"></a>基于剖析的语言建模</h2><ul>\n<li>使用统计剖析器来进行语言建模的最简单方式就是利用之前提到的二阶段算法。第一阶段我们运行一个普通的语音识别解码器或者机器翻译解码器（基于普通的N元语法），产生N个最好的候选；第二阶段，我们运行统计剖析器并为每一个候选句分配一个概率，选择概率最佳的。</li>\n</ul>\n<h2 id=\"人类剖析\"><a href=\"#人类剖析\" class=\"headerlink\" title=\"人类剖析\"></a>人类剖析</h2><ul>\n<li>人类在识别句子时也用到了类似的概率剖析思想，两个例子：<ul>\n<li>对于出现频率高的二元语法，人们阅读这个二元语法所花的时间就更少</li>\n<li>一些实验表明人类在消歧时倾向于选择统计概率大的剖析</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>计算语言学课程笔记<br>参考教材：Speech and Language Processing：An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition<br>一些公式待修订<br></p>","more":"<p></p>\n<h1 id=\"第二章：正则表达式与自动机\"><a href=\"#第二章：正则表达式与自动机\" class=\"headerlink\" title=\"第二章：正则表达式与自动机\"></a>第二章：正则表达式与自动机</h1><ul>\n<li>正则表达式：一种用于查找符合特定模式的子串或者用于以标准形式定义语言的工具，本章主要讨论其用于查找子串的功能。正则表达式用代数的形式来表示一些字符串集合。</li>\n<li>正则表达式接收一个模式，然后在整个语料中查找符合这个模式的子串，这个功能可以通过设计有限状态自动机实现。</li>\n<li>字符串看成符号的序列，所有的字符，数字，空格，制表符，标点和空格均看成符号。</li>\n</ul>\n<h2 id=\"基本正则表达式模式\"><a href=\"#基本正则表达式模式\" class=\"headerlink\" title=\"基本正则表达式模式\"></a>基本正则表达式模式</h2><ul>\n<li>用双斜线表示正则表达式开始和结束（perl中的形式）<ul>\n<li>查找子串，大小写敏感：/woodchuck/-&gt; woodchuck</li>\n<li>用方括号代表取其中一个，或：/[Ww]oodchuck/-&gt;woodchuck or Woodchuck</li>\n<li>方括号加减号，范围内取或：/[2-5]/-&gt;/[2345]</li>\n<li>插入符号放在左方括号后，代表模式中不出现后接的所有符号，取非: /^Ss/ -&gt;既不是大写S也不是小写s</li>\n<li>问号代表之前的符号出现一个或不出现：/colou?r/-&gt;color or colour</li>\n<li>星号代表之前的符号出现多个或不出现：/ba*/-&gt;b or ba or baa or baaa……</li>\n<li>加号代表之前的符号出现至少一次：/ba+/-&gt;ba or baa or baaa…….</li>\n<li>小数点代表通配符，与任何除了回车符之外的符号匹配：/beg.n/-&gt;begin or begun or beg’n or …….</li>\n<li>锚符号，用来表示特定位置的子串，插入符号代表行首，美元符号代表行尾，\\b代表单词分界线，\\B代表单词非分界线，perl将单词的定义为数字、下划线、字母的序列，不在其中的符号便可以作为单词的分界。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"析取、组合和优先\"><a href=\"#析取、组合和优先\" class=\"headerlink\" title=\"析取、组合和优先\"></a>析取、组合和优先</h2><ul>\n<li>用竖线代表析取，字符串之间的或：/cat|dog/-&gt;cat or dog</li>\n<li>用圆括号代表部分析取（组合），圆括号内也可以用基本算符：/gupp(y|ies)/-&gt;guppy or guppies</li>\n<li>优先级：圆括号&gt;计数符&gt;序列与锚&gt;析取符</li>\n</ul>\n<h2 id=\"高级算符\"><a href=\"#高级算符\" class=\"headerlink\" title=\"高级算符\"></a>高级算符</h2><ul>\n<li>\\d：任何数字</li>\n<li>\\D：任何非数字字符</li>\n<li>\\w：任何字母、数字、空格</li>\n<li>\\W：与\\w相反</li>\n<li>\\s：空白区域</li>\n<li>\\S：与\\s相反</li>\n<li>{n}：前面的模式出现n个</li>\n<li>{n,m}：前面的模式出现n到m个</li>\n<li>{n,}：前面的模式至少出现n个</li>\n<li>\\n：换行</li>\n<li>\\t：表格符</li>\n</ul>\n<h2 id=\"替换、寄存器\"><a href=\"#替换、寄存器\" class=\"headerlink\" title=\"替换、寄存器\"></a>替换、寄存器</h2><ul>\n<li>替换s/A/B/：A替换成B</li>\n<li>s/(A)/&lt;\\1&gt;/：用数字算符\\1指代A，在A的两边加上尖括号</li>\n<li>在查找中也可以用数字算符，指代圆括号内内容，可以多个算符指代多个圆括号内内容</li>\n<li>这里数字算符起到了寄存器的作用</li>\n</ul>\n<h2 id=\"有限状态自动机\"><a href=\"#有限状态自动机\" class=\"headerlink\" title=\"有限状态自动机\"></a>有限状态自动机</h2><ul>\n<li>有限状态自动机和正则表达式彼此对称，正则表达式是刻画正则语言的一种方法。正则表达式、正则语法和自动状态机都是表达正则语言的形式。FSA用有向图表示，圆圈或点代表状态，箭头或者弧代表状态转移，用双圈表示最终状态，如下图表示识别/baa+!/的状态机图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVj3V.png\" alt=\"FoVj3V.png\"></li>\n<li>状态机从初始状态出发，依次读入符号，若满足条件，则进行状态转移，若读入的符号序列满足模式，则状态机可以到达最终状态；若符号序列不满足模式，或者自动机在某个非最终状态卡住，则称自动机拒绝了此次输入。</li>\n<li>另一种表示方式是状态转移表：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVqNn.png\" alt=\"FoVqNn.png\"></li>\n<li>一个有限自动机可以用5个参数定义：<ul>\n<li>$Q$：状态{q_i}的有限集合</li>\n<li>\\sum ：有限的输入符号字母表</li>\n<li>$q_0$：初始状态</li>\n<li>$F$：终极状态集合</li>\n<li>$\\delta (q,i)$：状态之间的转移函数或者转移矩阵，是从$Q × \\Sigma$到$2^Q$的一个关系</li>\n</ul>\n</li>\n<li>以上描述的自动机是确定性的，即DFSA，在已知的记录在状态转移表上的状态时，根据查表自动机总能知道如何进行状态转移。算法如下，给定输入和自动机模型，算法确定输入是否被状态机接受：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZpB4.png\" alt=\"FoZpB4.png\"></li>\n<li>当出现了表中没有的状态时自动机就会出错，可以添加一个失败状态处理这些情况。</li>\n</ul>\n<h2 id=\"形式语言\"><a href=\"#形式语言\" class=\"headerlink\" title=\"形式语言\"></a>形式语言</h2><ul>\n<li>形式语言是一个模型，能且只能生成和识别一些满足形式语言定义的某一语言的符号串。形式语言是一种特殊的正则语言。通常使用形式语言来模拟自然语言的某些部分。以上例/baa+!/为例，设对应的自动机模型为m，输入符号表$\\Sigma = {a,b,!}$，$L(m)$代表由m刻画的形式语言，是一个无限集合${baa!,baaa!,baaaa!,…}$</li>\n</ul>\n<h2 id=\"非确定有限自动机\"><a href=\"#非确定有限自动机\" class=\"headerlink\" title=\"非确定有限自动机\"></a>非确定有限自动机</h2><ul>\n<li>非确定的有限自动机NFSA,把之前的例子稍微改动，自返圈移动到状态2，就形成了NFSA，因为此时在状态2，输入a，有两种转移可选，自动机无法确定转移路径：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVLhq.png\" alt=\"FoVLhq.png\"></li>\n<li>另一种NFSA的形式是引入$\\epsilon$转移，即不需要输入符号也可以通过此$\\epsilon$转移进行转移，如下图，在状态3时依然不确定如何进行转移：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVX90.png\" alt=\"FoVX90.png\"></li>\n<li>在NFSA时，面临转移选择时自动机可能做出错误的选择，此时存在三种解决方法：<ul>\n<li>回退：标记此时状态，当确定发生错误选择之后，回退到此状态</li>\n<li>前瞻：在输入中向前看，帮助判定进行选择</li>\n<li>并行：并行的进行所有可能的转移</li>\n</ul>\n</li>\n<li>在自动机中，采用回退算法时需要标记的状态称为搜索状态，包括两部分：状态节点和输入位置。对于NFSA，其状态转移表也有相应改变，如图，添加了代表$\\epsilon$转移的$\\epsilon$列，且转移可以转移到多个状态：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZE36.png\" alt=\"FoZE36.png\"></li>\n<li>采用回退策略的非确定自动机算法如下，是一种搜索算法：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZSuF.png\" alt=\"FoZSuF.png\"></li>\n<li>子函数GENERATE-NEW-STATES接受一个搜索状态，提取出状态节点和输入位置，查找这个状态节点上的所有状态转移可能，生成一个搜索状态列表作为返回值；</li>\n<li>子函数ACCEPT-STATE接受一个搜索状态，判断是否接受，接受时的搜索状态应该是最终状态和输入结束位置的二元组。</li>\n<li>算法使用进程表（agenda）记录所有的搜索状态，初始只包括初始的搜索状态，即自动机初始状态节点和输入起始。之后不断循环，从进程表中调出搜索状态，先调用ACCEPT-STATE判断是否搜索成功，之后再调用GENERATE-NEW-STATES生成新的搜索状态加入进程表。循环直到搜索成功或者进程表为空（所有可能转移均尝试且未成功）返回拒绝。</li>\n<li>可以注意到NFSA算法就是一种状态空间搜索，可以通过改变搜索状态的顺序提升搜索效率，例如用栈实现进程表，进行深度优先搜索DFS；或者使用队列实现进程表，进行宽度优先搜索BFS。</li>\n<li>对于任何NFSA，存在一个完全等价的DFSA。</li>\n</ul>\n<h2 id=\"正则语言和NFSA\"><a href=\"#正则语言和NFSA\" class=\"headerlink\" title=\"正则语言和NFSA\"></a>正则语言和NFSA</h2><ul>\n<li>定义字母表\\sum 为所有输入符号集合；空符号串$\\epsilon$，空符号串不包含再字母表中；空集∅。在\\sum 上的正则语言的类（或者正则集）可以形式的定义如下：<ul>\n<li>∅是正则语言</li>\n<li>∀a ∈ $\\sum$ ∪$\\epsilon$,{a}是形式语言</li>\n<li>如果$L_1$和$L_2$是正则语言，那么：</li>\n<li>$L_1$和$L_2$的拼接是正则语言</li>\n<li>$L_1$和$L_2$的合取、析取也是正则语言</li>\n<li>$L_1$^*，即$L_1$的Kleene闭包也是正则语言</li>\n</ul>\n</li>\n<li>可见正则语言的三种基本算符：拼接、合取及析取、Kleene闭包。任何正则表达式可以写成只使用这三种基本算符的形式。</li>\n<li>正则语言对以下运算也封闭（$L_1$和$L_2$均为正则语言）：<ul>\n<li>交：$L_1$和$L_2$的符号串集合的交构成的语言也是正则语言</li>\n<li>差：$L_1$和$L_2$的符号串集合的差构成的语言也是正则语言</li>\n<li>补：不在$L_1$的符号串集合中的集合构成的语言也是正则语言</li>\n<li>逆：$L_1$所有符号串的逆构成的集合构成的语言也是正则语言</li>\n</ul>\n</li>\n<li>可以证明正则表达式和自动机等价，一个证明任何正则表达式可以建立对应的自动机的方法是，根据正则语言的定义，构造基础自动机代表$\\epsilon$、∅以及$\\sum$中的单个符号a，然后将三种基本算符表示为自动机上的操作，归纳性的，在基础自动机上应用这些操作，得到新的基础自动机，这样就可以构造满足任何正则表达式的自动机，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoVxjU.png\" alt=\"FoVxjU.png\"><br>基础自动机<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZPE9.png\" alt=\"FoZPE9.png\"><br>拼接算符<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ9HJ.png\" alt=\"FoZ9HJ.png\"><br>Kleene闭包算符<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZiNR.png\" alt=\"FoZiNR.png\"><br>合取析取算符</li>\n</ul>\n<h1 id=\"第三章：形态学与有限状态转录机\"><a href=\"#第三章：形态学与有限状态转录机\" class=\"headerlink\" title=\"第三章：形态学与有限状态转录机\"></a>第三章：形态学与有限状态转录机</h1><ul>\n<li>剖析：取一个输入并产生关于这个输入的各类结构</li>\n</ul>\n<h2 id=\"英语形态学概论\"><a href=\"#英语形态学概论\" class=\"headerlink\" title=\"英语形态学概论\"></a>英语形态学概论</h2><ul>\n<li>形态学研究词的构成，词可以进一步拆解为语素，语素可分为词干和词缀，词缀可分为前缀、中缀、后缀、位缀。</li>\n<li>屈折形态学：英语中，名词只包括两种屈折变化：一个词缀表示复数，一个词缀表示领属：<ul>\n<li>复数：-s，-es，不规则复数形式</li>\n<li>领属：-‘s，-s’</li>\n</ul>\n</li>\n<li>动词的屈折变化包括规则动词和非规则动词的变化：<ul>\n<li>规则动词：主要动词和基础动词，-s，-ing，-ed，</li>\n<li>非规则动词</li>\n</ul>\n</li>\n<li>派生形态学：派生将词干和一个语法语素结合起来，形成新的单词<ul>\n<li>名词化：-ation，-ee，-er，-ness</li>\n<li>派生出形容词：-al，-able，-less</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"形态剖析\"><a href=\"#形态剖析\" class=\"headerlink\" title=\"形态剖析\"></a>形态剖析</h2><ul>\n<li>例子：我们希望建立一个形态剖析器，输入单词，输出其词干和有关的形态特征，如下表，我们的目标是产生第二列和第四列：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZA9x.png\" alt=\"FoZA9x.png\"></li>\n<li>我们至少需要：<ul>\n<li>词表（lexicon）：词干和词缀表及其基本信息</li>\n<li>形态顺序规则（morphotactics）：什么样的语素跟在什么样的语素之后</li>\n<li>正词法规则（orthographic rule）：语素结合时拼写规则的变化</li>\n</ul>\n</li>\n<li>一般不直接构造词表，而是根据形态顺序规则，设计FSA对词干进行屈折变化生成词语。例如一个名词复数化的简单自动机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZmuD.png\" alt=\"FoZmuD.png\"></li>\n<li>其中reg-noun代表规则名词，可以通过加s形成复数形式，并且忽略了非规则单数名词(irreg-sg-noun)和非规则复数名词(irreg-pl-noun)。另外一个模拟动词屈折变化的自动机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZQUA.png\" alt=\"FoZQUA.png\"></li>\n<li>使用FSA解决形态识别问题（判断输入符号串是否合法）的一种方法是，将状态转移细分到字母层次，但是这样仍然会存在一些问题：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZZjO.png\" alt=\"FoZZjO.png\"></li>\n</ul>\n<h2 id=\"有限状态转录机\"><a href=\"#有限状态转录机\" class=\"headerlink\" title=\"有限状态转录机\"></a>有限状态转录机</h2><ul>\n<li>双层形态学：将一个词表示为词汇层和表层，词汇层表示该词语素之间的简单毗连（拼接，concatenation），表层表示单词实际最终的拼写，有限状态转录机是一种有限状态自动机，但其实现的是转录，实现词汇层和表层之间的对应，它有两个输入，产生和识别字符串对，每一个状态转移的弧上有两个标签，代表两个输入。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZVgK.png\" alt=\"FoZVgK.png\"></li>\n<li>从四个途径看待FST：<ul>\n<li>作为识别器：FST接受一对字符串，作为输入，如果这对字符串在语言的字符串对中则输出接受否则拒绝</li>\n<li>作为生成器：生成语言的字符串对</li>\n<li>作为翻译器：读入一个字符串，输出另一个</li>\n<li>作为关联器：计算两个集合之间的关系</li>\n</ul>\n</li>\n<li>定义有限状态转录机：<ul>\n<li>Q：状态{q_i}的有限集合</li>\n<li>\\sum ：有限的输入符号字母表</li>\n<li>∆：有限的输出符号字母表</li>\n<li>$q_0 \\in Q$：初始状态</li>\n<li>$F⊆Q$：终极状态集合</li>\n<li>$\\delta (q,w)$：状态之间的转移函数或者转移矩阵，是从Q×\\sum 到2^Q的一个关系，q是状态，w是字符串，返回新状态集合</li>\n<li>$\\sigma (q,w)$：输出函数，给定每一个状态和输入，返回可能输出字符串的集合，是从$Q × \\Sigma$到$2^∆$的一个关系</li>\n</ul>\n</li>\n<li>在FST中，字母表的元素不是单个符号，而是符号对，称为可行偶对。类比于FSA和正则语言，FST和正则关系同构，对于并运算封闭，一般对于差、补、交运算不封闭。</li>\n<li>此外，FST，<ul>\n<li>关于逆反（逆的逆）闭包，逆反用于方便的实现作为剖析器的FST到作为生成器的FST的转换</li>\n<li>关于组合（嵌套）闭包，用于将多个转录机用一个更复杂的转录机替换。</li>\n</ul>\n</li>\n<li>转录机一般是非确定性的，如果用FSA的搜索算法会很慢，如果用非确定性到确定性的转换算法，则有些FST本身是不可以被转换为为确定的。</li>\n<li>顺序转录机是一种输入确定的转录机，每个状态转移在给定状态和输入之后是确定的，不像上图中的FST，状态0在输入b时有两种状态转移（转移到相同的状态，但是输出不同）。顺序转录机可以使用$\\epsilon$符号，但是只能加在输出字符串上，不能加在输入字符串上，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZuHH.png\" alt=\"FoZuHH.png\"></li>\n<li>顺序转录机输出不一定是序列的，即从同一状态发出的不同转移可能产生相同输出，因此顺序转录机的逆不一定是顺序转录机，所以在定义顺序转录机时需要定义方向，且转移函数和输出函数需要稍微修改，输出空间缩小为Q和∆。</li>\n<li>顺序转录机的一种泛化形式是并发转录机，其在最终状态额外输出一个字符串，拼接到已经输出的字符串之后。顺序和并发转录机的效率高，且有有效的算法对其进行确定化和最小化，因此很重要。P并发转录机在此基础上可以解决歧义问题。</li>\n</ul>\n<h2 id=\"用有限状态转录机进行形态剖析\"><a href=\"#用有限状态转录机进行形态剖析\" class=\"headerlink\" title=\"用有限状态转录机进行形态剖析\"></a>用有限状态转录机进行形态剖析</h2><ul>\n<li>将单词看成词汇层和表层之间的关系，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZnDe.png\" alt=\"FoZnDe.png\"></li>\n<li>在之前双层形态学的基础定义上，定义自己到自己的映射为基本对，用一个字母表示；用^代表语素边界；用#代表单词边界，在任务中提到需要输出+SG之类的语素特征，这些特征在另一个输出上没有对应的输出符号，因此映射到空字符串或边界符号。我们把输入输出对用冒号连接，也可以写在弧的上下。一个抽象的表示英语名词复数屈折变化的转录机如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZl4I.png\" alt=\"FoZl4I.png\"></li>\n<li>之后我们需要更新词表，使得非规则复数名词能够被剖析为正确的词干：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZMEd.png\" alt=\"FoZMEd.png\"></li>\n<li>之后将抽象的转录机写成具体的，由字母组成转移弧的转录机，如下图，只展示了具体化部分非规则复数和单数名词之后的转录机：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ3Ct.png\" alt=\"FoZ3Ct.png\"></li>\n</ul>\n<h2 id=\"转录机和正词法规则\"><a href=\"#转录机和正词法规则\" class=\"headerlink\" title=\"转录机和正词法规则\"></a>转录机和正词法规则</h2><ul>\n<li>用拼写规则，也就是正词法规则来处理英语中经常在语素边界发生拼写错误的问题。</li>\n<li>以下是一些拼写规则实例：<ul>\n<li>辅音重叠：beg/beggin</li>\n<li>E的删除：make/making</li>\n<li>E的插入：watch/watches</li>\n<li>Y的替换：try/tries</li>\n<li>K的插入：panic/panicked</li>\n</ul>\n</li>\n<li>为了实现拼写规则，我们在词汇层和表层之间加入中间层，以符合特定规则的语素毗连作为输入，以修改之后的正确的语素毗连作为输出，例如fox +N +PL输入到中间层即第一次转录，得到fox ^ s #，之后中间层到表层的第二次转录检测到特殊语素毗连：x^和s#，就在表层的x和s之间插入一个e，得到foxes。下面的转录机示意图展示了这个过程：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ88P.png\" alt=\"FoZ88P.png\"></li>\n<li>这个转录机只考虑x^和s#毗连需插入e这一正词法规则</li>\n<li>其他的词能正常通过</li>\n<li>$Q_0$代表无关词通过，是接受状态</li>\n<li>$Q_1$代表看见了zsx，作为中间状态保存，一直保存的是最后的与语素毗连的z,s,x，如果出现了其他字母则返回到q0，其本身也可以作为接受态</li>\n<li>$Q_2$代表看见了与z,s,x毗连的语素，这之后有四种转移<ul>\n<li>接了$x$,$z$，回到$q_1$，也就是认为重新接到了可能和语素毗连的x,z</li>\n<li>接了$s$，分为两种情况，一种是正常需要插入e，这时通过$\\epsilon$转移到$q_3$再到$q_4$；另一种是本来就需要插入$e$，这就到达$q_5$，之后视情况回退了$q_1$、$q_0$，或者$s$又毗连语素回到$q_2$。两种情况不确定，需要通过搜索解决</li>\n<li>接单词边界和其他符号，回到$q_0$</li>\n<li>$q_2$本身也可以作为接受态</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"结合\"><a href=\"#结合\" class=\"headerlink\" title=\"结合\"></a>结合</h2><ul>\n<li>现在可以通过三层结构，结合产生语素和进行正词法规则矫正的转录机。从词汇层到中间层用一个转录机产生语素，从中间层到表层可并行使用多个转录机进行正词法规则的矫正。</li>\n<li>两类转录机叠加时可以改写成一类转录机，这时需要对两类状态机状态集合计算笛卡尔积，对新集合内每一个元素建立状态。</li>\n<li>这种三层结构是可逆的，但是进行剖析时（从表层到词汇层）会出现歧义问题，即一个单词可能剖析出多种语素结合，这时单纯依靠转录机无法消歧，需要借助上下文。</li>\n</ul>\n<h2 id=\"其他应用（简单介绍）\"><a href=\"#其他应用（简单介绍）\" class=\"headerlink\" title=\"其他应用（简单介绍）\"></a>其他应用（简单介绍）</h2><ul>\n<li>不需要词表的FST，PORTER词干处理器：将层叠式重写规则用FST实现，提取出单词的词干。</li>\n<li>分词和分句：一个简单的英文分词可以基于正则表达式实现，一个简单的中文分词可以通过maxmatch（一种基于最大长度匹配的贪婪搜索算法）实现。</li>\n<li>拼写检查与矫正：使用了投影操作的FST可以完成非词错误的检测，然后基于最小编辑距离（使用动态规划算法实现）可以矫正。正常词错误检测和矫正需借助N元语法模型。</li>\n</ul>\n<h2 id=\"人如何进行形态处理\"><a href=\"#人如何进行形态处理\" class=\"headerlink\" title=\"人如何进行形态处理\"></a>人如何进行形态处理</h2><ul>\n<li>研究表明，人的心理词表存储了一部分形态机构，其他的结构不组合在心理词表中，而需要分别提取并组合。研究说明了两个问题：<ul>\n<li>形态尤其是屈折变化之类的能产性形态在人的心理词表中起作用，且人的语音词表和正词法词表可能具有相同结构。</li>\n<li>例如形态这种语言处理的很多性质，可以应用于语言的理解和生成。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第四章：N元语法\"><a href=\"#第四章：N元语法\" class=\"headerlink\" title=\"第四章：N元语法\"></a>第四章：N元语法</h1><ul>\n<li>语言模型是关于单词序列的统计模型，N元语法模型是其中的一种，它根据之前N-1个单词推测第N个单词，且这样的条件概率可以组成整个单词序列（句子）的联合概率。</li>\n</ul>\n<h2 id=\"在语料库中统计单词\"><a href=\"#在语料库中统计单词\" class=\"headerlink\" title=\"在语料库中统计单词\"></a>在语料库中统计单词</h2><ul>\n<li>区别：word type或者叫 vocabulary size V，代表语料中不同单词的个数，而tokens，不去重，代表语料的大小。有研究认为词典大小不低于tokens数目的平方根。<br>非平滑N元语法模型</li>\n<li>任务：根据以前的单词推断下一个单词的概率：$P(w|h)$，以及计算整个句子的概率$P(W)$。</li>\n<li>最朴素的做法是用古典概型，统计所有历史h和当前词w组成的片段在整个语料中出现的次数，并除以历史h片段在整个语料中出现的次数。句子的概率也用相似的方法产生。缺点：依赖大语料，且语言本身多变，这样的计算限制过于严格。</li>\n<li>接下来引入N元语法模型，首先通过概率的链式法则，可以得到条件概率$P(w|h)$和整个句子的联合概率$P(W)$之间的关系：<script type=\"math/tex; mode=display\">\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1}) \\\\\n= \\prod _{k=1}^n P(w_k|w_1^{k-1}) \\\\</script></li>\n<li>N元语法模型放松了条件概率的限制，做出一个马尔可夫假设：每个单词的概率只和它之前N-1个单词相关，例如二元语法模型，只和前一个单词相关，用这个条件概率去近似$P(w|h)$:<script type=\"math/tex; mode=display\">\nP(w_n|w_1^{n-1}) \\approx P(w_n|w_{n-1}) \\\\</script></li>\n<li>N元语法模型里的条件概率用最大似然估计来估算，统计语料中各种N元语法的个数，并归一化，其中可以简化的一点是：以二元语法为例，所有给定单词开头的二元语法总数必定等于该单词一元语法的计数：<script type=\"math/tex; mode=display\">\nP(w_n|w_{n-1}) = \\frac {C(w_{n-1}w_n)}{C(w_{n-1})} \\\\</script></li>\n<li>使用N元语法之后，句子概率的链式分解变得容易计算，我们可以通过计算各种句子的概率来判断句子是否包含错字，或者计算某些句子在给定上下文中出现的可能，因为N元语法能捕捉一些语言学上的特征，或者一些用语习惯。在语料充足的时候，我们可以使用三元语法模型获得更好的效果。</li>\n</ul>\n<h2 id=\"训练集和测试集\"><a href=\"#训练集和测试集\" class=\"headerlink\" title=\"训练集和测试集\"></a>训练集和测试集</h2><ul>\n<li>N元语法模型对训练集非常敏感。N元语法的N越大，依赖的上下文信息越多，利用N元语法模型生成的句子就越流畅，但这些未必“过于流畅”，其原因在于N元语法概率矩阵非常大且非常稀疏，在N较大例如四元语法中，一旦生成了第一个单词，之后可供的选择非常少，接着生成第二个单词之后选择更少了，往往只有一个选择，这样生成的就和原文中某一个四元语法一模一样。过于依赖训练集会使得模型的泛化能力变差。因此我们选择的训练集和测试集应来自同一细分领域。</li>\n<li>有时候测试集中会出现训练集词典里没有的词，即出现未登录词（Out Of Vocabulty,OOV）。在开放词典系统中，我们先固定词典大小，并将所有未登录词用特殊符号<unk>代替，然后才进行训练。</unk></li>\n</ul>\n<h2 id=\"评价N元语法模型：困惑度\"><a href=\"#评价N元语法模型：困惑度\" class=\"headerlink\" title=\"评价N元语法模型：困惑度\"></a>评价N元语法模型：困惑度</h2><ul>\n<li>模型的评价分两种：外在评价和内在评价。外在评价是一种端到端的评价，看看某一模块的改进是否改进了整个模型的效果。内在评价的目的是快速衡量模块的潜在改进效果。内在评价的潜在改进效果不一定会使得端到端的外在评价提高，但是一般两者都存在某种正相关关系。</li>\n<li>困惑度（Perplexsity,PP）是一种关于概率模型的内在评价方法。语言模型的在测试集上的困惑度是语言模型给测试集分配的概率的函数。以二元语法为例，测试集上的困惑度为：<script type=\"math/tex; mode=display\">\nPP(W) = \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i|w_{i-1})}} \\\\</script></li>\n<li>概率越高，困惑度越低。困惑度的两种解释：<ul>\n<li>加权的平均分支因子：分支因子是指可能接在任何上文之后的单词的数目。显然，如果我们的模型啥也没学习到，那么测试集任何单词可以接在任何上文之后，分支因子很高，困惑度很高；相反，如果我们的模型学习到了具体的规则，那么单词被限制接在一些指定上文之后，困惑度变低。困惑度使用了概率加权分支因子，分支因子的大小在模型学习前后不变，”morning”仍然可以接到任何上文之后，但是它接到”good”之后的概率变大了，因此是加权的分支因子。</li>\n<li>熵：对于语言序列，我们定义一个序列的熵为：    <script type=\"math/tex\">H(w_1,w_2,…,w_n )=-\\sum _{W_1^n \\in L} p(W_1^n) \\log ⁡p(W_1^n)</script>也就是这个序列中所有前缀子序列的熵之和，其均值是序列的熵率。计算整个语言的熵，假设语言是一个产生单词序列的随机过程，单词序列无限长，则其熵率是：<script type=\"math/tex\">H(L)=\\lim _{n \\rightarrow \\infty}⁡ \\frac 1n H(w_1,w_2,…,w_n) =\\lim _{n \\rightarrow \\infty} -⁡\\frac 1n \\sum _{W \\in L} p(W_1^n)  \\log ⁡p(W_1^n)</script>根据Shannon-McMillan-Breiman理论，在n趋于无穷的情况下，如果语言既是平稳又是正则的，上面这些子串的和的熵，可以用最大串代替每一个子串得到，这里的代替是指log后面求的是最大串的概率，log之前的概率依然是各个子串的概率？假如是这样的话提出最大串的概率对数，对所有子串概率求和得到：<script type=\"math/tex\">H(L)=\\lim _{n \\rightarrow \\infty} -⁡ \\frac 1n \\log ⁡p(w_1,w_2,…,w_n)</script>交叉熵可以衡量我们的模型生成的概率分布到指定概率分布之间的距离，我们希望模型生成概率分布尽可能近似真实分布，即交叉熵小。具体衡量时是对相同的语言序列，计算训练得到的模型m和理想模型p在生成这个序列上的概率的交叉熵：<script type=\"math/tex\">H(p,m) = \\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\sum _{W \\in L} p(W_1^n) \\log⁡ m(W_1^n)</script>但是我们不知道理想的分布p，这时根据之前的Shannon-McMillan-Breiman定理，得到了只包含一个概率分布的序列交叉熵（？）：<script type=\"math/tex\">H(p,m)=\\lim _{n \\rightarrow \\infty}⁡ - \\frac 1n \\log⁡ m(W_1^n)</script>在测试数据上我们没有无限长的序列，就用有限长的序列的交叉熵近似这个无限长序列的交叉熵。困惑度则是这个（近似的？只包含一个概率分布的？）交叉熵取指数运算：<script type=\"math/tex; mode=display\">\nPerplexity(W) = 2^{H(W)} \\\\\n= P(w_1 w_2 ... w_N)^{\\frac {-1}{N}} \\\\\n= \\sqrt[n]{\\frac {1}{P(w_1 w_2 ... w_N)}} \\\\\n= \\sqrt[n]{\\prod _{i=1}^N \\frac {1}{P(w_i | w_1 ... w_{i-1})}} \\\\</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"平滑\"><a href=\"#平滑\" class=\"headerlink\" title=\"平滑\"></a>平滑</h2><ul>\n<li>因为N元语法模型依赖语料，一般而言对于N越高的N元语法，语料提供的数据越稀疏。这种情况下N元语法对于那些计数很小的语法估计很差，且如果测试集中某一句包含了训练集中没有出现的N元语法时，我们无法使用困惑度进行评价。因此我们使用平滑作为一种改进方法，使得N元语法的最大似然估计能够适应这些存在0概率的情况。</li>\n<li>接下来介绍了两种平滑：<ul>\n<li>拉普拉斯平滑（加1平滑）</li>\n<li>Good-Turing 打折法</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"拉普拉斯平滑\"><a href=\"#拉普拉斯平滑\" class=\"headerlink\" title=\"拉普拉斯平滑\"></a>拉普拉斯平滑</h3><ul>\n<li>加1平滑就是在计算概率归一化之前，给每个计数加1，对应的，归一化时分母整体加了一个词典大小:<script type=\"math/tex; mode=display\">\nP_{Laplace}(w_i) = \\frac {c_i + 1}{N+V} \\\\</script></li>\n<li>为了表现平滑的作用，引入调整计数$c^{*}$，将平滑后的概率写成和平滑之前一样的形式：<script type=\"math/tex; mode=display\">\nP_{Laplace} (w_i) = \\frac {(C_i^{\\*})}{N} \\\\\nC_i^{\\*} = \\frac {(C_i+1)N}{(N+V)} \\\\</script></li>\n<li>一种看待平滑的角度是：对每个非0计数打折，分一些概率给0计数，定义相对打折$d_c$（定义在非0计数上），<script type=\"math/tex; mode=display\">\nd_c = \\frac {c^{\\*}} {c}</script></li>\n<li>$d_c$代表了打折前后单词计数的变化。平滑之后，对于非0计数，当$C_i &lt; \\frac NV$时，计数增加；否则计数减少。计数越大，打折越多，增加越少（减少越多）。当0计数很多时，N/V较小，这时大部分非0计数都会减少，且减少较多。</li>\n<li>而0计数则没有收到打折的影响。因此在一轮不同程度的增长之后，再归一化的结果就是非0计数分享了一些概率给0计数。写成调整计数的形式，就是非0计数减少数值，0计数变化（一般是减少）数值（但不是减少的完全等于增加的）。 书中给出了一个例子，下图是一部分语料的二元语法平滑之后的计数，蓝色代表平滑加1之后的0计数：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZNDg.png\" alt=\"FoZNDg.png\"><br>如果把表写成调整计数的形式：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZtKS.png\" alt=\"FoZtKS.png\"> </li>\n<li>可以看到，本来的0计数（蓝色）从0变大，而其他的计数减少，例如&lt; i want&gt;，从827减少到527，<want to>从608减少到238。</want></li>\n<li>当0计数很多时，非0计数减少的数值很多，可以使用一个小于1的小数$\\delta$代替1，即加$\\delta$平滑。通常这个$\\delta$是动态变化的。</li>\n</ul>\n<h3 id=\"GT打折法\"><a href=\"#GT打折法\" class=\"headerlink\" title=\"GT打折法\"></a>GT打折法</h3><ul>\n<li>类似于Good-Turing打折法, Witten-Bell打折法， Kneyser-Ney 平滑一类的方法，它们的基本动机是用只出现一次的事物的计数来估计从未出现的事物的计数。只出现一次的语法称为单件（singleton）或者罕见语（hapax legomena）。Good-Turing打折法用单件的频率来估计0计数二元语法。</li>\n<li>定义N_c为出现c次的N元语法的总个数（不是总个数乘以c），并称之为频度c的频度。对N_c中的c的最大似然估计是c。这样相当于将N元语法按其出现次数分成了多个桶，GT打折法用c+1号桶里语法概率的最大似然估计来重新估计c号桶内语法的概率。因此GT估计之后最大似然估计得到的c被替换成：<script type=\"math/tex; mode=display\">\nc^{\\*}=(c+1) \\frac {N_{c+1}}{N_c}</script></li>\n<li>之后计算某N元语法的概率：<ul>\n<li>从未出现：$P_{GT}^{*}=\\frac{N_1}{N}$。其中N是所有N元语法数$(\\sum _i N_i * i)$。这里假设了我们已知$N_0$，则此式表示某一具体未知计数N元语法概率时还应除以$N_0$。</li>\n<li>已出现（已知计数）：$P_{GT}^{*} = \\frac{c^{*}}{N}$</li>\n</ul>\n</li>\n<li>这样计算，$N_1$的一些概率转移到了$N_0$上。GT打折法假设所有的N元语法概率分布满足二项式分布，且假设我们已知$N_0$，以二元语法为例：<script type=\"math/tex; mode=display\">\nN_0 = V^2 - \\sum _{i>0} N_i \\\\</script></li>\n<li>其他注意事项：<ul>\n<li>有些$N_c$为0，这时我们无法用这些$N_c$来计算平滑后的c。这种情况下我们直接放弃平滑，令$c^{*} = c$，再根据正常的数据计算出一个对数线性映射，$log⁡(N_c) = a + b \\log(c)$，代入放弃平滑的c并用其倒推计算计数为0的$N_c$，使得这些$N_c$有值，不会影响更高阶的c的计算。</li>\n<li>只对较小c的$N_c$进行平滑，较大c的$N_c$认为足够可靠，设定一个阈值k，对$c &lt; k$的$N_c$计算：<script type=\"math/tex; mode=display\">\nc^{\\*} = \\frac {(c+1) \\frac {N_c+1}{N_c} - c \\frac {(k+1) N_{k+1} }{N_1} } {1- \\frac {(k+1)N_{k+1}} {N_1}} \\\\</script></li>\n<li>计算较小的c如c=1时，也看成c=0的情况进行平滑</li>\n</ul>\n</li>\n<li>一个例子：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZGgf.png\" alt=\"FoZGgf.png\"></li>\n</ul>\n<h2 id=\"插值与回退\"><a href=\"#插值与回退\" class=\"headerlink\" title=\"插值与回退\"></a>插值与回退</h2><ul>\n<li>上述的平滑只考虑了如何转移概率到计数为0的语法上去，对于条件概率$p(w|h)$，我们也可以采用类似的思想，假如不存在某个三元语法帮助计算$p(w_n |w_{n-1} w_{n-2})$，则可以用阶数较低的语法$p(w_n |w_{n-1})$帮助计算，有两种方案：<ul>\n<li>回退：用低阶数语法的替代0计数的高阶语法</li>\n<li>插值：用低阶数语法的加权估计高阶语法</li>\n</ul>\n</li>\n<li>在Katz回退中，我们使用GT打折作为方法的一部分：GT打折告诉我们有多少概率可以从已知语法中分出来，Katz回退告诉我们如何将这些分出来的概率分配给未知语法。在之前的GT打折法中，我们将分出的概率均匀分给每一个未知语法，而Katz回退则依靠低阶语法的信息来分配：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZJv8.png\" alt=\"FoZJv8.png\"></li>\n<li>其中$P^{*}$是打折之后得到的概率；\\alpha是归一化系数，保证分出去的概率等于未知语法分配得到的概率。</li>\n<li>插值则是用低阶语法概率加权求和得到未知高阶语法概率：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZUbQ.png\" alt=\"FoZUbQ.png\"></li>\n<li>加权的系数还可以通过上下文动态计算。具体系数的计算有两种方法：<ul>\n<li>尝试各种系数，用在验证集上表现最好的系数组合</li>\n<li>将系数看成是概率生成模型的隐变量，使用EM算法进行推断</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"实际问题：工具和数据格式\"><a href=\"#实际问题：工具和数据格式\" class=\"headerlink\" title=\"实际问题：工具和数据格式\"></a>实际问题：工具和数据格式</h2><ul>\n<li>在语言模型计算中，一般将概率取对数进行计算，原因有二：防止数值下溢；取对数能将累乘运算变成累加，加速计算。</li>\n<li>回退N元语法模型一般采用ARPA格式。ARPA格式文件由一些头部信息和各类N元语法的列表组成，列表中包含了该类N元语法下所有语法，概率，和回退的归一化系数。只有能够称为高阶语法前缀的低阶语法才能在回退中被利用，并拥有归一化系数。</li>\n<li>两种计算N元语法模型的工具包：SRILM toolkit 和Cambridge-CMU toolkit</li>\n</ul>\n<h2 id=\"语言建模中的高级问题\"><a href=\"#语言建模中的高级问题\" class=\"headerlink\" title=\"语言建模中的高级问题\"></a>语言建模中的高级问题</h2><h3 id=\"高级平滑方法：Kneser-Ney平滑\"><a href=\"#高级平滑方法：Kneser-Ney平滑\" class=\"headerlink\" title=\"高级平滑方法：Kneser-Ney平滑\"></a>高级平滑方法：Kneser-Ney平滑</h3><ul>\n<li>注意到在GT打折法当中，打折之后估计的c值比最大似然估计得到的c值近似多出一个定值d。绝对打折法便考虑了这一点，在每个计数中减去这个d：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZwUs.png\" alt=\"FoZwUs.png\"></li>\n<li>Kneser-Ney平滑吸收了这种观点，并且还考虑了连续性：在不同上文中出现的单词更有可能出现在新的上文之后，在回退时，我们应该优先考虑这种在多种上文环境里出现的词，而不是那些出现次数很多，但仅仅在特定上文中出现的词。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZdEj.png\" alt=\"FoZdEj.png\"></li>\n<li>在Kneser-Ney中，插值法能够比回退法取得更加好的效果：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ05n.png\" alt=\"FoZ05n.png\"></li>\n</ul>\n<h3 id=\"基于分类的N元语法\"><a href=\"#基于分类的N元语法\" class=\"headerlink\" title=\"基于分类的N元语法\"></a>基于分类的N元语法</h3><ul>\n<li>这种方法是为了解决训练数据的稀疏性。例如IBM聚类，每个单词只能属于一类，以二元语法为例，某个二元语法的条件概率的计算变为给定上文所在类，某个单词的条件概率，还可以进一步链式分解为两个类的条件概率乘以某个单词在给定其类条件下的条件概率：<script type=\"math/tex; mode=display\">\np(w_i│w_{i-1} ) \\approx p(w_i│c_{i-1} ) = p(c_i |c_{i-1}) \\cdot p(w_i |c_i)</script></li>\n</ul>\n<h3 id=\"语言模型适应和网络应用\"><a href=\"#语言模型适应和网络应用\" class=\"headerlink\" title=\"语言模型适应和网络应用\"></a>语言模型适应和网络应用</h3><ul>\n<li>适应是指在大型宽泛的语料库上训练语言模型，并在小的细分领域的语言模型上进一步改进。网络是大型语料库的一个重要来源。在实际应用时我们不可能搜索每一个语法并统计搜索得到所有页面上的所有语法，我们用搜索得到的页面数来近似计数。</li>\n</ul>\n<h3 id=\"利用更长距离的上文信息\"><a href=\"#利用更长距离的上文信息\" class=\"headerlink\" title=\"利用更长距离的上文信息\"></a>利用更长距离的上文信息</h3><ul>\n<li>通常我们使用二元和三元语法模型，但是更大的N能够带来更好的效果。为了捕捉更长距离的上文信息，有以下几种方法：<ul>\n<li>基于缓存机制的N元语法模型</li>\n<li>基于主题建模的N元语法模型，对不同主题建模语言模型，再加权求和</li>\n<li>不一定使用相邻的上文信息，例如skip N-grams或者不一定使用定长的上文信息，例如变长N-grams</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"第十六章：语言的复杂性\"><a href=\"#第十六章：语言的复杂性\" class=\"headerlink\" title=\"第十六章：语言的复杂性\"></a>第十六章：语言的复杂性</h1><h2 id=\"Chomsky层级\"><a href=\"#Chomsky层级\" class=\"headerlink\" title=\"Chomsky层级\"></a>Chomsky层级</h2><ul>\n<li>Chomsky层级反映了不同形式化方法描述的语法之间的蕴含关系，较强生成能力或者说更复杂的语法在层级的外层。从外到内，加在可重写语法规则上的约束增加，语言的生成能力逐渐降低。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZXad.png\" alt=\"FoZXad.png\"></li>\n<li>五种语法对应的规则和应用实例：<br><img src=\"https://s2.ax1x.com/2019/01/03/Foepxf.png\" alt=\"Foepxf.png\"><ul>\n<li>0型语法：规则上只有一个限制，即规则左侧不能为空字符串。0型语法刻画了递归可枚举语言</li>\n<li>上下文相关语法：可以把上下文\\alpha，\\beta之间的非终极符号A重写成任意非空符号串</li>\n<li>温和的上下文相关语法</li>\n<li>上下文无关语法：可以把任何单独的非终极符号重写为由终极符号和非终极符号构成的字符串，也可以重写为空字符串</li>\n<li>正则语法：可以是右线性也可以是左线性，以右线性为例，非终极符号可以重写为左边加了若干终极符号的另一个非终极符号，右线性不断地在字符串左侧生成终极符号。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自然语言是否正则\"><a href=\"#自然语言是否正则\" class=\"headerlink\" title=\"自然语言是否正则\"></a>自然语言是否正则</h2><ul>\n<li>判断语言是否正则能够让我们了解应该用哪一层次的语法来描述一门语言，且这个问题能够帮助我们了解自然语言的不同方面的某些形式特性。</li>\n<li>抽吸引理：用来证明一门语言不是正则语言。<ul>\n<li>如果一门语言可以被有限状态自动机来描述，则与自动机对应有一个记忆约束量。这个约束量对于不同的符号串不会增长的很大，因为其状态数目是固定的，更长的符号串应该是通过状态之间转移产生而不是增加状态数目。因此这个记忆量不一定和输入的长度成比例。</li>\n<li>如果一个正则语言能够描述任意长的符号序列，比自动机的状态数目还多，则该语言的自动机中必然存在回路。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZxPI.png\" alt=\"FoZxPI.png\"></li>\n</ul>\n</li>\n<li>如图所示自动机，可以表述xyz,xyyz,xyyyz…..，当然也可以将中间无限长的y序列“抽吸掉”，表述xz。抽吸引理表述如下：</li>\n<li>设L是一个有限的正则语言，那么必然存在符号串x,y,z,使得对于任意n≥0，y≠$\\epsilon$，且xy^n z∈L</li>\n<li>即假如一门语言是正则语言，则存在某一个符号串y，可以被适当的“抽吸”。这个定理是一门语言是正则语言的必要非充分条件。</li>\n<li>有学者证明英语不是一门正则语言：<ul>\n<li>具有镜像性质的句子通过抽吸原理可以证明不是正则语言，而英语中一个特殊的子集合和这种镜像性质的句子是同态的。</li>\n<li>另一种证明基于某些带有中心-嵌套结构的句子。这种句子可以由英语和某一类简单的正则表达式相交得到，通过抽吸原理可以得到这种句子不是正则语言。英语和正则语言的交不是正则语言，则英语不是正则语言。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"自然语言是否上下文无关\"><a href=\"#自然语言是否上下文无关\" class=\"headerlink\" title=\"自然语言是否上下文无关\"></a>自然语言是否上下文无关</h2><ul>\n<li>既然自然语言不是正则语言，我们接着考虑更宽松的限定，自然语言是否是上下文无关的？</li>\n<li>不是……</li>\n</ul>\n<h2 id=\"计算复杂性和人的语言处理\"><a href=\"#计算复杂性和人的语言处理\" class=\"headerlink\" title=\"计算复杂性和人的语言处理\"></a>计算复杂性和人的语言处理</h2><ul>\n<li>人对中心嵌套句子处理很困难，因为人们剖析时利用的栈记忆有限，且栈中不同层次记忆容易混淆。</li>\n</ul>\n<h1 id=\"第五章：词类标注\"><a href=\"#第五章：词类标注\" class=\"headerlink\" title=\"第五章：词类标注\"></a>第五章：词类标注</h1><ul>\n<li>各种表述：POS（Part Of Speech）、word classes（词类）、morphological classes（形态类）、lexical tags（词汇标记）。</li>\n<li>POS的意义在于：<ul>\n<li>能够提供关于单词及其上下文的大量信息。</li>\n<li>同一单词在不同词类下发音不同，因此POS还能为语音处理提供信息。</li>\n<li>进行词干分割（stemming），辅助信息检索</li>\n</ul>\n</li>\n<li>本章介绍三种词类标注算法：<ul>\n<li>基于规则的算法</li>\n<li>基于概率的算法，隐马尔科夫模型</li>\n<li>基于变换的算法</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"一般词类\"><a href=\"#一般词类\" class=\"headerlink\" title=\"一般词类\"></a>一般词类</h2><ul>\n<li>POS分为封闭集和开放集，封闭集集合相对稳定，例如介词，开放集的词语则不断动态扩充，例如名词和动词。特定某个说话人或者某个语料的开放集可能不同，但是所有说一种语言以及各种大规模语料库可能共享相同的封闭集。封闭集的单词称为虚词（功能词，function word），这些词是语法词，一般很短，出现频次很高。</li>\n<li>四大开放类：名词、动词、形容词、副词。</li>\n<li>名词是从功能上定义的而不是从语义上定义的，因此名词一般表示人、地点、事物，但既不充分也不必要。定义名词：<ul>\n<li>与限定词同时出现</li>\n<li>可以受主有代词修饰</li>\n<li>大多数可以以复数形式出现（即可数名词），物质名词不可数。单数可数名词出现时不能没有冠词</li>\n</ul>\n</li>\n<li>动词，表示行为和过程的词，包括第三人称单数、非第三人称单数、进行时、过去分词几种形态</li>\n<li>形容词，描述性质和质量</li>\n<li>副词，用于修饰，副词可以修饰动词、动词短语、其它副词。</li>\n<li>英语中的一些封闭类：<ul>\n<li>介词 prepositions：出现在名词短语之前，表示关系</li>\n<li>限定词 determiners 冠词 articles：与有定性（definiteness）相关</li>\n<li>代词 pronouns：简短的援引某些名词短语、实体、或事件的一种形式</li>\n<li>连接词 conjunctions：用于连接和补足（complementation）</li>\n<li>助动词 auxiliary verbs：标志主要动词的某些语义特征，包括：时态、完成体、极性对立、情态</li>\n<li>小品词 particles：与动词结合形成短语动词</li>\n<li>数词 numerals</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"词类标注\"><a href=\"#词类标注\" class=\"headerlink\" title=\"词类标注\"></a>词类标注</h2><ul>\n<li>标注算法的输入是单词的符号串和标记集，输出要让每一个单词标注上一个单独且最佳的标记。如果每个单词只对应一种词性，那么根据已有的标记集，词类标注就是一个简单的查表打标的过程，但是很多词存在多种词性，例如book既可以是名词也可以是动词，因此要进行消歧，词类标注是歧义消解的一个重要方面。</li>\n</ul>\n<h2 id=\"基于规则的词类标注\"><a href=\"#基于规则的词类标注\" class=\"headerlink\" title=\"基于规则的词类标注\"></a>基于规则的词类标注</h2><ul>\n<li>介绍了ENGTWOL系统，根据双层形态学构建，对于每一个词的每一种词类分别立条，计算时不计屈折形式和派生形式.</li>\n<li>标注算法的第一阶段是将单词通过双层转录机，得到该单词的所有可能词类</li>\n<li>之后通过施加约束规则排除不正确的词类。这些规则通过上下文的类型来决定排除哪些词类。</li>\n</ul>\n<h2 id=\"基于隐马尔科夫模型的词类标注\"><a href=\"#基于隐马尔科夫模型的词类标注\" class=\"headerlink\" title=\"基于隐马尔科夫模型的词类标注\"></a>基于隐马尔科夫模型的词类标注</h2><ul>\n<li>使用隐马尔科夫模型做词类标注是一类贝叶斯推断，这种方法将词类标注看成是序列分类任务。观察量为一个词序列（比如句子），任务是给这个序列分配一个标注序列。</li>\n<li>给定一个句子，贝叶斯推断想要在所有标注序列可能中选择最好的一个序列，即<script type=\"math/tex; mode=display\">\n{t_1^n} _{best} = {argmax} _{t_1^n}  P(t_1^n |w_1^n)</script></li>\n<li>使用贝叶斯法则将其转化为：<script type=\"math/tex; mode=display\">\n{t_1^n} _{best}={argmax} _{t_1^n}  \\frac{P(w_1^n│t_1^n)P(t_1^n)}{P(w_1^n)} = {argmax} _{t_1^n} P(w_1^n│t_1^n)P(t_1^n)</script></li>\n<li>隐马尔科夫模型在此基础上做了两点假设<ul>\n<li>一个词出现的概率只与该词的词类标注有关，与上下文其他词和其他标注无关，从而将序列的联合概率拆解为元素概率之积，即：P(w_1^n│t_1^n) \\approx \\prod _{i=1}^n P(w_i |t_i)</li>\n<li>一个标注出现的概率只与前一个标注相关，类似于二元语法的假设：P(t_1^n ) \\approx \\prod _{i=1}^n P(t_i |t_{i-1})</li>\n</ul>\n</li>\n<li>在两种假设下简化后的最好标注序列表达式为：<script type=\"math/tex; mode=display\">\n{t_1^n}_{best} = {argmax} _{t_1^n} P(t_1^n│w_1^n) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i) P(t_i |t_{i-1})</script></li>\n<li>上面这个概率表达式实际上将HMM模型的联合概率拆成了各个部分转移概率的乘积，具体而言分为标签转移概率（隐变量之间转移）和词似然（隐变量转移到可观察变量）。通过最大似然估计，我们可以通过古典概型的方法从已标注的语料中计算出这两类概率：<script type=\"math/tex; mode=display\">\nP(t_i│t _{i-1} ) = (C(t _{i-1},t_i))/C(t _{i-1} ) \\\\\nP(w_i│t_i ) = \\frac{C(t_i,w_i)}{C(t_i)} \\\\</script></li>\n<li>一个例子：HMM模型如何正确的将下句中的race识别为动词而不是名词：</li>\n<li>Secretariat is expected to race tomorrow.</li>\n<li>画出上句中race被识别为动词和名词两种情况下的HMM模型，可以看到两个模型对比只有三个转移概率不同，用加粗线标出：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZDCq.png\" alt=\"FoZDCq.png\"></li>\n<li>HMM词类标注器消歧的方式是全局的而不是局部的。我们在语料中统计得到这三种转移概率，再累乘，结果是(a)的概率是(b)概率的843倍。显然race应该被标注为动词。</li>\n</ul>\n<h2 id=\"形式化隐马尔科夫模型标注器\"><a href=\"#形式化隐马尔科夫模型标注器\" class=\"headerlink\" title=\"形式化隐马尔科夫模型标注器\"></a>形式化隐马尔科夫模型标注器</h2><ul>\n<li>HMM模型是有限自动机的扩展，具体而言是一种加权有限自动机，马尔可夫链的扩展，这种模型允许我们考虑观察量和隐变量，考虑包含隐变量的概率模型。HMM包含以下组件：<ul>\n<li>Q：大小为N的状态集</li>\n<li>A：大小为N*N的转移概率矩阵</li>\n<li>O：大小为T的观察事件集</li>\n<li>B：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察o_t的概率</li>\n<li>$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量</li>\n</ul>\n</li>\n<li>A中的概率和B中的概率对应着之前式子中每一个累乘项里的先验$P(w_i│t_i )$和似然$P(t_i |t _{i-1})$概率：<script type=\"math/tex; mode=display\">\n{t_1^n}_{best}={argmax} _{t_1^n} P(t_1^n│w_1^n ) \\approx {argmax} _{t_1^n} \\prod _{i=1}^n P(w_i│t_i)P(t_i |t _{i-1})</script></li>\n</ul>\n<h2 id=\"HMM标注的维特比算法\"><a href=\"#HMM标注的维特比算法\" class=\"headerlink\" title=\"HMM标注的维特比算法\"></a>HMM标注的维特比算法</h2><ul>\n<li>在HMM模型中，已知转移概率和观察序列，求隐变量的任务叫做解码。解码的一种算法即维特比算法，实质上是一种动态规划算法，与之前求最小编辑距离的算法类似。</li>\n<li>首先我们从语料中计算得到A和B两个矩阵，即模型的转移概率已知，对于给定的观察序列，按照以下步骤执行维特比算法：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZyvT.png\" alt=\"FoZyvT.png\"></li>\n<li>算法维护一个$(N+2)*T$的概率矩阵viterbi，加了2代表初始状态和结束状态，viterbi[s,t]代表了在第t步状态为s时的最佳路径概率，而backpointer[s,t]对应着保存了该最佳路径的上一步是什么状态，用于回溯输出整个最佳路径。</li>\n<li>关键的转移在于$viterbi[s,t] \\leftarrow max _{s^{*}=1}^N⁡ viterbi[s^{*},t-1] * a_{s^{*},s} * b_s (o_t)$即当前时间步最佳路径是由上一时间步各个状态的最佳路径转移过来的，选择上一步最佳路径概率与转移概率乘积最大的路径作为当前时间步的最佳路径。从动态规划的角度而言，即长度为t的最佳路径，必定是从长度为t-1的最佳路径里选择一条转移得到，否则肯定可以从另一条概率更大的路径转移获得更优解。这样就限制了最佳路径的生成可能，减少了计算量。</li>\n</ul>\n<h2 id=\"将HMM算法扩展到三元语法\"><a href=\"#将HMM算法扩展到三元语法\" class=\"headerlink\" title=\"将HMM算法扩展到三元语法\"></a>将HMM算法扩展到三元语法</h2><ul>\n<li>现代的HMM标注器一般在标注转移概率上考虑更长的上文历史：<script type=\"math/tex; mode=display\">\nP(t_1^n ) \\approx \\prod_{i=1}^n P(t_i |t _{i-1},t_{i-2})</script></li>\n<li>这样的话需要在序列开头和结尾做一些边界处理。使用三元语法的一个问题是数据稀疏：例如我们从没有在训练集中见过标注序列PRP VB TO，则我们无法计算P(TO|PRP,VB)。一种解决办法是线性插值：<script type=\"math/tex; mode=display\">\nP(t_i│t _{i-1} t _{i-2} ) = \\lambda _1 P ̂(t_i│t _{i-1} t _{i-2} )+\\lambda _2 P ̂(t_i│t _{i-1} )+\\lambda _3 P ̂(t_i)</script></li>\n<li>使用删除插值的办法确定系数$\\lambda$：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZr80.png\" alt=\"FoZr80.png\"></li>\n</ul>\n<h2 id=\"基于变换的标注\"><a href=\"#基于变换的标注\" class=\"headerlink\" title=\"基于变换的标注\"></a>基于变换的标注</h2><ul>\n<li>基于变换的方法结合了基于规则和基于概率方法的优点。基于变换的方法依然需要规则，但是从数据中总结出规则，是一种监督学习方法，称为基于变换的学习（Transformation Based Learning，TBL）。在TBL算法中，语料库首先用比较宽的规则来标注，然后再选择稍微特殊的规则来修改，接着再使用更窄的规则来修改数量更少的标记。</li>\n</ul>\n<h2 id=\"如何应用TBL规则\"><a href=\"#如何应用TBL规则\" class=\"headerlink\" title=\"如何应用TBL规则\"></a>如何应用TBL规则</h2><ul>\n<li>首先应用最宽泛的规则，就是根据概率给每个词标注，选择概率最大的词类作为标注。之后应用变换规则，即如果满足某一条件，就将之前标注的某一词类变换（纠正）为正确的词类，之后不断应用更严格的变换，在上一次变换的基础上进行小部分的修改。</li>\n<li>如何学习到TBL规则<ul>\n<li>首先给每个词打上最可能的标签</li>\n<li>检查每一个可能的变换，选择效果提升最多的变换，此处需要直到每一个词正确的标签来衡量变换带来的提升效果，因此是监督学习。</li>\n<li>根据这个被选择的变换给数据重新打标，重复步骤2，直到收敛（提升效果小于某一阈值）</li>\n</ul>\n</li>\n<li>以上过程输出的结果是一有序变换序列，用来组成一个标注过程，在新语料上应用。虽然可以穷举所有的规则，但是那样复杂度太高，因此我们需要限制变换集合的大小。解决方案是设计一个小的模板集合（抽象变换）,每一个允许的变换都是其中一个模板的实例化。</li>\n</ul>\n<h2 id=\"评价和错误分析\"><a href=\"#评价和错误分析\" class=\"headerlink\" title=\"评价和错误分析\"></a>评价和错误分析</h2><ul>\n<li>一般分为训练集、验证集、测试集，在训练集内做十折交叉验证。</li>\n<li>与人类标注的黄金标准比较计算准确率作为衡量指标。</li>\n<li>一般用人类表现作为ceiling，用一元语法最大概率标注的结果作为baseline。</li>\n<li>通过含混矩阵或者列联表来进行错误分析。在N分类任务中，一个N*N的含混矩阵的第i行第j列元素指示第i类被错分为第j类的次数在总分错次数中的占比。一些常见的容易分错的词性包括：<ul>\n<li>单数名词、专有名词、形容词</li>\n<li>副词、小品词、介词</li>\n<li>动词过去式、动词过去分词、形容词</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"词性标注中的一些其他问题\"><a href=\"#词性标注中的一些其他问题\" class=\"headerlink\" title=\"词性标注中的一些其他问题\"></a>词性标注中的一些其他问题</h2><ul>\n<li>标注不确定性：一个词在多个词性之间存在歧义，很难区分。这种情况下有些标注器允许一个词被打上多个词性标注。在训练和测试的时候，有三种方式解决这种多标注词：<ul>\n<li>通过某种方式从这些候选标注中选择一个标注</li>\n<li>训练时指定一个词性，测试时只要打上了候选词性中任意一个就认为标注正确</li>\n<li>将整个不确定的词性集看成一个新的复杂词性</li>\n</ul>\n</li>\n<li>多部分词：在标注之前需要先分词，一些多部分词是否应该被分为一部分，例如New York City应该分成三部分还是一个整体，也是各个标注系统需要考虑的。</li>\n<li>未知词：不在词典中的词称为未知词。对于未知词，训练集无法给出它的似然P(w_i |t_i)，可以通过以下几种方式解决：<ul>\n<li>只依赖上下文的POS信息预测</li>\n<li>用只出现一次的词来估计未知词的分布，类似于Good Turing打折法</li>\n<li>使用未知词的单词拼写信息，正词法信息。例如连字符、ed结尾、首字母大写等特征。之后在训练集中计算每个特征的似然，并假设特征之间独立，然后累乘特征似然作为未知词的似然：$P(w_i│t_i )=p(unknown word│t_i ) * p(capital│t_i ) * p(endings/hyph|t_i)$</li>\n<li>使用最大熵马尔可夫模型</li>\n<li>使用对数线性模型</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"噪声信道模型\"><a href=\"#噪声信道模型\" class=\"headerlink\" title=\"噪声信道模型\"></a>噪声信道模型</h2><ul>\n<li>贝叶斯推断用于标注可以认为是一种噪声信道模型的应用，本节介绍如何用噪声信道模型来完成拼写纠正任务。<br>之前对于非单词错误，通过词典查找可以检测到错误，并根据最小编辑距离纠正错误，但这种方法对于真实单词错误无能为力。噪声信道模型可以纠正这两种类型的拼写错误。</li>\n<li>噪声信道模型的动机在于将错误拼写的单词看成是一个正确拼写的单词经过一个噪声信道时受到干扰扭曲得到。我们尝试所有可能的正确的词，将其输入信道，最后得到的干扰之后的词与错误拼写的词比较，最相似的例子对应的输入词就认为是正确的词。这类噪声信道模型，比如之前的HMM标注模型，是贝叶斯推断的一种特例。我们看到一个观察两（错误拼写词）并希望找到生成这个观察量的隐变量（正确拼写词），也就是找最大后验。</li>\n<li>将噪声信道模型应用于拼写纠正：首先假设各种拼写错误类型，错拼一个、错拼两个、漏拼一个等，然后产生所有可能的纠正，除去词典中不存在的，最后分别计算后验概率，选择后验概率最大的作为纠正。其中需要根据局部上下文特征来计算似然。</li>\n<li>另一种纠正算法是通过迭代来改进的方法：先假设拼写纠正的含混矩阵是均匀分布的，之后根据含混矩阵运行纠正算法，根据纠正之后的数据集更新含混矩阵，反复迭代。这种迭代的算法是一种EM算法。</li>\n</ul>\n<h2 id=\"根据上下文进行拼写纠正\"><a href=\"#根据上下文进行拼写纠正\" class=\"headerlink\" title=\"根据上下文进行拼写纠正\"></a>根据上下文进行拼写纠正</h2><ul>\n<li>即真实单词拼写错误的纠正。为了解决这类任务需要对噪声信道模型进行扩展：在产生候选纠正词时，需要包括该单词本身以及同音异形词。之后根据整个句子的最大似然来选择正确的纠正词。</li>\n</ul>\n<h1 id=\"第六章：隐马尔科夫模型和最大熵模型\"><a href=\"#第六章：隐马尔科夫模型和最大熵模型\" class=\"headerlink\" title=\"第六章：隐马尔科夫模型和最大熵模型\"></a>第六章：隐马尔科夫模型和最大熵模型</h1><ul>\n<li>隐马尔科夫模型用来解决序列标注（序列分类问题）。</li>\n<li>最大熵方法是一种分类思想，在满足给定条件下分类应满足限制最小（熵最大），满足奥卡姆剃刀原理。</li>\n<li>最大熵马尔可夫模型是最大熵方法在序列标注任务上的扩展。</li>\n</ul>\n<h2 id=\"马尔可夫链\"><a href=\"#马尔可夫链\" class=\"headerlink\" title=\"马尔可夫链\"></a>马尔可夫链</h2><ul>\n<li>加权有限自动状态机是对有限自动状态机的扩展，每条转移路径上加上了概率作为权重，说明从这条路径转移的可能性。马尔可夫链是加权有限状态自动机的一种特殊情况，其输入序列唯一确定了自动机会经过的状态序列。马尔可夫链只能对确定性序列分配概率。</li>\n<li>我们将马尔可夫链看作一种概率图模型，一个马尔可夫链由下面的成分确定：<script type=\"math/tex; mode=display\">\nQ=q_1 q_2…q_N \\\\\nA=a_{01} a_{02} … a_{n1} … a_{nn} \\\\\nq_0,q_F \\\\</script></li>\n<li>分别是<ul>\n<li>状态集合</li>\n<li>转移概率矩阵，其中a_ij代表了从状态i转移到状态j的概率$P(q_j |q_i)$</li>\n<li>特殊的开始状态和结束状态</li>\n</ul>\n</li>\n<li>概率图表示将状态看成图中的点，将转移看成边。</li>\n<li>一阶马尔可夫对转移做了很强的假设：某一状态的概率只与前一状态相关：<script type=\"math/tex; mode=display\">\nP(q_i│q_1…q _{i-1} )=P(q_i |q _{i-1})</script></li>\n<li>马尔可夫链的另一种表示不需要开始和结束状态：<script type=\"math/tex; mode=display\">\n\\pi = \\pi _1,\\pi _2 , … , \\pi _N \\\\\nQA={q_x,q_y…} \\\\</script></li>\n<li>分别是：<ul>\n<li>状态的初始概率分布，马尔可夫链以概率$\\pi _i$从状态i开始</li>\n<li>集合QA是Q的子集，代表合法的接受状态</li>\n</ul>\n</li>\n<li>因此状态1作为初始状态的概率既可以写成$a_{01}$也可以写成$\\pi _1$。</li>\n</ul>\n<h2 id=\"隐马尔科夫模型\"><a href=\"#隐马尔科夫模型\" class=\"headerlink\" title=\"隐马尔科夫模型\"></a>隐马尔科夫模型</h2><ul>\n<li>当马尔可夫链已知时，我们可以用其计算一个观测序列出现的概率。但是观测序列可能依赖于一些不可观测的隐变量，我们可能感兴趣的是推断出这些隐变量。隐马尔科夫模型允许我们同时考虑观测变量和隐变量。</li>\n<li>如之前一样定义隐马尔科夫模型：<ul>\n<li>Q：大小为N的状态集</li>\n<li>A：大小为N*N的转移概率矩阵</li>\n<li>O：大小为T的观察事件集</li>\n<li>B：观察似然序列，又叫发射概率，$b_i (o_t)$描述了从状态i里生成观察$o_t$的概率</li>\n<li>$q_0，q_F$：特殊的起始状态和最终状态，没有相连接的观察量</li>\n</ul>\n</li>\n<li>同样的，隐马尔科夫也可以用另一种不依赖初始和结束状态的方式表示。隐马尔科夫模型也做了两个假设，分别是隐状态之间转移和隐状态到观察量转移的一阶马尔可夫性。</li>\n<li>对于隐马尔科夫模型需要解决三类问题：<ul>\n<li>似然计算：已知参数和观测序列，求似然$P(O|\\lambda)$</li>\n<li>解码：已知参数和观测序列，求隐状态序列</li>\n<li>学习：已知观测序列和隐状态集合，求解模型参数</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"计算似然：前向算法\"><a href=\"#计算似然：前向算法\" class=\"headerlink\" title=\"计算似然：前向算法\"></a>计算似然：前向算法</h2><ul>\n<li>对于马尔可夫链，其没有隐状态到观测量的转移概率矩阵，可以看成观察量与隐状态相同。在隐马尔科夫模型中不能直接计算似然，我们需要直到隐状态序列。</li>\n<li>先假设隐状态序列已知，则似然计算为：<script type=\"math/tex; mode=display\">\nP(O│Q) = \\prod _{i=1}^T P(o_i |q_i)</script></li>\n<li>根据隐状态转移的一阶马尔可夫性，可以求得隐状态的先验，乘以似然得到观测序列和隐状态序列的联合概率：<script type=\"math/tex; mode=display\">\nP(O,Q)=P(O│Q) \\* P(Q) = \\prod _{i=1}^n P(o_i│q_i )  \\prod _{i=1}^n P(q_i |q _{i-1})</script></li>\n<li>对于联合概率积分掉隐状态序列，就可以得到观测概率的似然：<script type=\"math/tex; mode=display\">\nP(O) = \\sum _Q P(O,Q) = \\sum _Q P(O|Q)P(Q)</script></li>\n<li>这样计算相当于考虑了所有的隐状态可能，并对每一种可能从隐状态序列开始到结束计算一次似然，实际上可以保留每次计算的中间状态来减少重复计算，也就是动态规划。在前向计算HMM观测似然使用的动态规划算法称为前向算法：<ul>\n<li>令$\\alpha _t (j)$代表在得到前t个观测量之后当前时刻隐变量处于状态j的概率,\\lambda为模型参数：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = P(o_1,o_2…o_t,q_t=j|\\lambda)</script></li>\n<li>这个概率值可以根据前一时间步的\\alpha值计算出来，避免了每次从头开始计算：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = \\sum _{i=1}^N \\alpha _{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>初始化$\\alpha _1 (j)$：<script type=\"math/tex; mode=display\">\n\\alpha _1 (j)=a_{0s} b_s (o_1)</script></li>\n<li>终止状态：<script type=\"math/tex; mode=display\">\nP(O│\\lambda) = \\alpha _T (q_F) = \\sum _{i=1}^N \\alpha _T (i) \\alpha _{iF}</script></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"解码：维特比算法\"><a href=\"#解码：维特比算法\" class=\"headerlink\" title=\"解码：维特比算法\"></a>解码：维特比算法</h2><ul>\n<li>解码任务是根据观测序列和参数推断出最有可能隐状态序列。最朴素的做法：对于每种可能的隐状态序列，计算观测序列的似然，取似然最大时对应的隐状态序列。但是这样做就如同朴素的计算似然方法一样，时间复杂度过高，同样的，我们使用动态规划来缩小求解的规模。在解码时使用了一种维特比算法。<ul>\n<li>令$v_t (j)$代表已知前t个观测量（1~t）和已知前t个隐状态（0~t-1）的条件下，当前时刻隐状态为j的概率：<script type=\"math/tex; mode=display\">\nv_t (j)=max _{q_0,q_1,…,q_{t-1}} P(q_0,q_1…q_{t-1},o_1,o_2 … o_t,q_t=j|\\lambda)</script></li>\n<li>其中我们已知了前t个时间步最大可能的隐状态序列，这些状态序列也是通过动态规划得到的：<script type=\"math/tex; mode=display\">\nv_t (j)=max _{i=1}^N⁡ v_{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>为了得到最佳的隐状态序列，还需要记录每一步的最佳选择，方便回溯得到路径：<script type=\"math/tex; mode=display\">\n{bt}_t (j) = argmax _{i=1}^N v_{t-1} (i) a_{ij} b_j (o_t)</script></li>\n<li>初始化：<script type=\"math/tex; mode=display\">\nv_1 (j) = a_{0j} b_j (o_1) \\ \\  1 \\leq j \\leq N \\\\\n{bt}_1 (j) = 0 \\\\</script></li>\n<li>终止，分别得到最佳隐状态序列（回溯开始值）及其似然值：<script type=\"math/tex; mode=display\">\nP \\* = v_t (q_F ) = max_{i=1}^N⁡ v_T (i) \\* a_{i,F} \\\\\nq_{T\\*} = {bt}_T (q_F ) = argmax _{i=1}^N v_T (i) \\* a_{i,F} \\\\</script></li>\n</ul>\n</li>\n<li>维特比算法减小时间复杂度的原因在于其并没有计算所有的隐状态路径，而是利用了每一时间步的最佳路径只能从上一时间步的最佳路径中延伸而来这一条件，减少了路径候选，避免了许多不必要的路径计算。并且每一步利用上一步的结果也是用了动态规划的思想减少了计算量。</li>\n</ul>\n<h2 id=\"训练隐马尔科夫模型：前向后向算法\"><a href=\"#训练隐马尔科夫模型：前向后向算法\" class=\"headerlink\" title=\"训练隐马尔科夫模型：前向后向算法\"></a>训练隐马尔科夫模型：前向后向算法</h2><ul>\n<li>学习问题是指已知观测序列和隐状态集合，求解模型参数。</li>\n<li>前向后向算法，又称Baum-Welch算法，是EM算法的一种特例，用来求解包含隐变量的概率生成模型的参数。该算法通过迭代的方式反复更新转移概率和生成概率，直到收敛。BW算法通过设计计数值之比作为隐变量，将转移概率矩阵和生成概率矩阵一起迭代更新。</li>\n<li>先考虑马尔科夫链的学习问题。马尔科夫链可以看作是退化的隐马尔科夫模型，即每个隐变量只生成和自己一样的观测量，生成其他观测量的概率为0。因此只需学习转移概率。</li>\n<li>对于马尔可夫链，可以通过古典概型统计出转移概率：<script type=\"math/tex; mode=display\">\na_{ij} = \\frac {Count(i \\rightarrow j)} {\\sum _{q \\in Q} Count(i \\rightarrow q)}</script></li>\n<li>我们可以这样直接计算概率是因为在马尔可夫链中我们知道当前所处的状态。对于隐马尔科夫模型我们无法这样直接计算是因为对于给定输入，隐状态序列无法确定。Badum-Welch算法使用了两种简洁的直觉来解决这一问题：<ul>\n<li>迭代估计，先假设一种转移概率和生成概率，再根据假设的概率推出更好的概率</li>\n<li>计算某一观测量的前向概率，并将这个概率分摊到不同的路径上，通过这种方式估计概率</li>\n</ul>\n</li>\n<li>首先类似于前向概率，我们定义后向概率：<ul>\n<li>令$\\beta _t (i)$代表在得到后t个观测量之后当前时刻隐变量处于状态i的概率,$\\lambda$为模型参数：<script type=\"math/tex; mode=display\">\n\\beta _t (i) = P(o_{t+1},o_{t+2}…o_T,q_t=i|\\lambda)</script></li>\n<li>类似于后向概率的归纳计算：<script type=\"math/tex; mode=display\">\n\\beta_t (i) = \\sum _{j=1}^N a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j),  \\ \\   1≤i≤N,1≤t<T</script></li>\n<li>初始化$\\alpha _1 (j)$：<script type=\"math/tex; mode=display\">\n\\beta _T (i)=\\alpha _(i,F)</script></li>\n<li>终止状态：<script type=\"math/tex; mode=display\">\nP(O│\\lambda)=\\alpha _t (q_F )=\\beta_1 (0)= \\sum _{i=1}^N a_{0j} b_j (o_1) \\beta _1 (j)</script></li>\n</ul>\n</li>\n<li>类似的，我们希望马尔可夫链中的古典概率能帮助我们估计转移概率：<script type=\"math/tex; mode=display\">\na_{ij}^{\\*} = \\frac{从状态i转移到状态j的计数值期望}{从状态i转移出去的计数值期望}</script></li>\n<li>如何估计计数值：我们将整个序列的转移路径计数值转化为时间步之间转移路径计数值之和，时间步之间某一条转移路径的概率为：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j)</script></li>\n<li>首先考虑所有的观测序列和这一转移路径的联合概率（省略了以参数$\\lambda$为条件）：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j,O)</script></li>\n<li>观察下面的概率图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZWVJ.png\" alt=\"FoZWVJ.png\"></li>\n<li>可以看到这一联合概率包含了三个部分：<ul>\n<li>T时刻隐状态为i的前向概率</li>\n<li>T+1时刻隐状态为j的后向概率</li>\n<li>T时刻与T+1时刻的状态转移概率以及生成对应观测量的生成概率</li>\n</ul>\n</li>\n<li>所以有：<script type=\"math/tex; mode=display\">\nP(q_t=i,q_{t+1}=j,O)=\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta _{t+1} (j)</script></li>\n<li>为了从联合分布中得到已知观测序列求转移路径的联合概率，需要计算观测序列的概率，可以通过前向概率或者后向概率求得：<script type=\"math/tex; mode=display\">\nP(O)=\\alpha _t (N)=\\beta _T (1) = \\sum _{j=1}^N \\alpha _t (j) \\beta_t (j)</script></li>\n<li>最终得到<script type=\"math/tex; mode=display\">\nξ_t (i,j)=P(q_t=i,q_{t+1}=j│O) = \\frac {(\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))}{(\\alpha _t (N))}</script></li>\n<li>最后，对所有时间步求和就可以得到从状态i转移到状态j的期望计数值，从而进一步得到转移概率的估计：<script type=\"math/tex; mode=display\">\na_{ij}^{\\*} = \\frac {\\sum _{t=1}^{T-1} ξ_t (i,j)}{\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1} ξ_t (i,j)}</script></li>\n<li>同样的，我们还希望得到生成概率的估计：<script type=\"math/tex; mode=display\">\nb_{j}^{\\*} (v_k) = \\frac {在状态j观测到符号v_k 的计数值期望}{状态j观测到所有符号的计数值期望}</script></li>\n<li>类似的，通过先计算联合分布再计算条件分布的方式得到在t时刻处于隐状态j的概率：<script type=\"math/tex; mode=display\">\nγ_t (j)=P(q_t=j│O) = \\frac {P(q_t=j,O)}{P(O)}</script></li>\n<li>联合概率包含两个部分，即t时刻处于状态j的前向概率和后向概率，所以有：<script type=\"math/tex; mode=display\">\nγ_t (j) = \\frac {\\alpha _t (j) \\beta_t (j)}{\\alpha _t (N)}</script></li>\n<li>类似的，对所有时间步累加，进而得到生成概率的估计：<script type=\"math/tex; mode=display\">\nb_{j}^{\\*} (v_k) = \\frac{\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) }{\\sum _{t=1}^T   γ_t (j) }</script></li>\n<li>这两个式子是在已知前向概率和后向概率$(\\alpha,\\beta)$的情况下，计算出中间变量（隐变量）(ξ,γ),引入隐变量的动机是将a、b估计值的期望计数值之比转化为概率之比，且这两个隐变量可以用a,b表示。再由隐变量计算出转移概率和生成概率，因此形成了一个迭代的循环，可以用EM算法求解：<script type=\"math/tex; mode=display\">\na,b→\\alpha,\\beta→ξ,γ→a,b</script></li>\n<li>E-step:<script type=\"math/tex; mode=display\">\nγ_t (j) = (\\alpha _t (j) \\beta_t (j))/(\\alpha _t (N)) ξ_t (i,j) \\\\\n= (\\alpha _t (i) a_{ij} b_j (o_{t+1} ) \\beta_{t+1} (j))/(\\alpha _t (N)) \\\\</script></li>\n<li>M-step（最大化的目标是什么）:<script type=\"math/tex; mode=display\">\na _{ij} = (\\sum _{t=1}^{T-1}   ξ_t (i,j)  )/(\\sum _{t=1}^{T-1} \\sum _{j=1}^{N-1}   ξ_t (i,j)  ) \\\\\nb ̂_j(v_k) = (\\sum _{t=1 s.t. O_t=v_k}^T   γ_t (j) )/(\\sum _{t=1}^T   γ_t (j) ) \\\\</script></li>\n<li>迭代时需重新计算：<script type=\"math/tex; mode=display\">\n\\alpha _t (j) = \\sum _{i=1}^N   \\alpha_{t-1} (i) a_ij b_j (o_t) \\\\\n\\beta_t (i) = \\sum _{j=1}^N   a_ij b_j (o_{t+1} ) \\beta_{t+1} (j)  \\\\</script></li>\n<li>迭代的初始状态对于EM算法来说很重要，经常是通过引入一些外部信息来设计一个好的初始状态。</li>\n</ul>\n<h2 id=\"最大熵模型：背景\"><a href=\"#最大熵模型：背景\" class=\"headerlink\" title=\"最大熵模型：背景\"></a>最大熵模型：背景</h2><ul>\n<li>最大熵模型另一种广为人知的形式是多项Logistic回归（Softmax?）。</li>\n<li>最大熵模型解决分类问题，最大熵模型作为一种概率分类器，能够根据样本的特征求出样本属于每一个类别的概率，进而进行分类。</li>\n<li>最大熵模型属于指数家族（对数线性）分类器，通过将特征线性组合，取指数得到分类概率：<script type=\"math/tex; mode=display\">\np(c│x)=\\frac 1Z exp⁡(\\sum _i   weight_i feature_i)</script></li>\n<li>Z是一个归一化系数，使得生成的概率之和为1。</li>\n</ul>\n<h2 id=\"最大熵建模\"><a href=\"#最大熵建模\" class=\"headerlink\" title=\"最大熵建模\"></a>最大熵建模</h2><ul>\n<li>将二分类Logistic回归推广到多分类问题就得到：<script type=\"math/tex; mode=display\">\nP(c│x) = \\frac {exp⁡(\\sum _(i=0)^N   w_ci f_i) } {\\sum _{c^{\\*} in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i)  }</script></li>\n<li>语音和语言处理中的特征通常是二值的（是否有该特征），因此使用指示函数表示特征<script type=\"math/tex; mode=display\">\nP(c│x) = \\frac {exp⁡(\\sum _{i=0}^N   w_{c_i} f_i (c,x)) }{\\sum _{c^{\\*} \\in C}   exp⁡(\\sum _{i=0}^N   w_{c^{\\*} i} f_i (c^{\\*},x))  }</script></li>\n<li>注意到在该模型中每一个类都有其独立的线性权重w_c。相比于硬分布，最大熵模型能够给出分到每一类的概率，因此可以求出每一时刻的分类概率进而求出整体分类概率，得到全局最优分类结果。注意到不同于支持向量机等模型，最大熵模型无法利用特征之间的组合，必须手动构造组合作为新的特征。</li>\n<li>一般使用加了正则化的最大似然作为优化的目标函数：<script type=\"math/tex; mode=display\">\nw ̂={argmax} _w \\sum _i   \\log P(y^{(i)}│x^{(i) } ) - \\alpha \\sum _{j=1}^N w_j^2</script></li>\n<li>这种正则化相当于给权重的概率分布加了一个零均值高斯先验，权重越偏离均值，即权重越大，其概率越低。</li>\n<li>为什么多分类Logistic回归是最大熵模型：最大熵模型保证在满足给定约束下，无约束的部分分类应该是等概率分配，例如在两个约束下：<script type=\"math/tex; mode=display\">\nP(NN)+P(JJ)+P(NNS)+P(VB)=1 \\\\\nP(t_i=NN or t_i=NNS)=8/10 \\\\</script></li>\n<li>则满足这两个约束，最大熵模型分配的概率结果为：<script type=\"math/tex; mode=display\">\np(NN)=4/10  \\\\\np(JJ)=1/10  \\\\\np(NNS)=4/10  \\\\\np(VB)=1/10 \\\\</script></li>\n<li>在The equivalence of logistic regression and maximum entropy models一文中证明了在广义线性回归模型的平衡条件约束下，满足最大熵分布的非线性激活函数就是sigmoid，即logistic回归。</li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型\"><a href=\"#最大熵马尔可夫模型\" class=\"headerlink\" title=\"最大熵马尔可夫模型\"></a>最大熵马尔可夫模型</h2><ul>\n<li>最大熵模型只能对单一观测量分类，使用最大熵马尔可夫模型可以将其扩展到序列分类问题上。</li>\n<li>最大熵马尔可夫比隐马尔科夫模型好在哪儿？隐马尔科夫模型对于每个观测量的分类依赖于转移概率和生成概率，假如我们想要在标注过程中引入外部知识，则需要将外部知识编码进这两类概率中，不方便。最大熵马尔可夫模型能够更简单的引入外部知识。</li>\n<li>在隐马尔科夫模型中我们优化似然，并且乘以先验来估计后验：<script type=\"math/tex; mode=display\">\nT ̂= {argmax}_T ∏_i   P(word_i│tag_i ) ∏_i   P(tag_i│tag _{i-1} )</script></li>\n<li>在最大熵隐马尔科夫模型中，我们直接计算后验。因为我们直接训练模型来分类，即最大熵马尔可夫模型是一类判别模型，而不是生成模型：<script type=\"math/tex; mode=display\">\nT ̂= {argmax}_T ∏_i   P(tag_i |word_i,tag _{i-1})</script></li>\n<li>因此在最大熵隐马尔科夫模型中没有分别对似然和先验建模，而是通过一个单一的概率模型来估计后验。两者的区别如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZgrF.png\" alt=\"FoZgrF.png\"> </li>\n<li>另外最大熵马尔可夫模型可以依赖的特征更多，依赖方式更灵活，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZcKU.png\" alt=\"FoZcKU.png\"></li>\n<li>用公式表示这一差别：<script type=\"math/tex; mode=display\">\nHMM:P(Q│O)=∏_{i=1}^n   P(o_i |q_i)×∏_{i=1}^n   P(q_i |q _{i-1})  \\\\\nMEMM:P(Q│O)=∏_{i=1}^n   P(q_i |q _{i-1},o_i) \\\\</script></li>\n<li>当估计单一转移概率（从状态q^{*}转移到状态q，产生观测量o）时，我们使用以下的最大熵模型：<script type=\"math/tex; mode=display\">\nP(q│q^{\\*},o)=\\frac{1}{Z(o,q^{\\*})} exp⁡(\\sum _i   w_i f_i (o,q))</script></li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型的解码（推断）\"><a href=\"#最大熵马尔可夫模型的解码（推断）\" class=\"headerlink\" title=\"最大熵马尔可夫模型的解码（推断）\"></a>最大熵马尔可夫模型的解码（推断）</h2><ul>\n<li>MEMM同样使用维特比算法进行解码</li>\n<li>使用维特比算法解码的通用框架是：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i)P(s_j│s_i )P(o_t |s_j)</script></li>\n<li>在HMM模型中这一框架具体化为：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (i) a_ij b_j (o_t)</script></li>\n<li>在MEMM中直接将似然和先验替换为后验：<script type=\"math/tex; mode=display\">\nv_t (j)=max_{i=1}^N⁡  v_{t-1} (j)P(s_j |s_i,o_t)</script></li>\n</ul>\n<h2 id=\"最大熵马尔可夫模型的训练\"><a href=\"#最大熵马尔可夫模型的训练\" class=\"headerlink\" title=\"最大熵马尔可夫模型的训练\"></a>最大熵马尔可夫模型的训练</h2><ul>\n<li>MEMM作为最大熵模型的推广，训练过程使用和最大熵模型一样的监督算法。如果训练数据的标签序列存在缺失，也可以通过EM算法进行半监督学习。</li>\n</ul>\n<h1 id=\"第十二章：英语的形式语法\"><a href=\"#第十二章：英语的形式语法\" class=\"headerlink\" title=\"第十二章：英语的形式语法\"></a>第十二章：英语的形式语法</h1><h2 id=\"组成性\"><a href=\"#组成性\" class=\"headerlink\" title=\"组成性\"></a>组成性</h2><ul>\n<li>英语中的单词是如何组成一个词组的呢？</li>\n<li>换句话说，我们如何判断一些单词组合成了一个部分？一种可能是这种组合都能在相似的句法环境中出现，例如名词词组都能在一个动词之前出现。另一种可能依据来自于前置和后置结构，例如前置短语on September seventeenth可以放在句子的前面，中间或者后面，但是组合成这个短语的各个部分不能拆出来放在句子的不同位置，因此我们判断on September seventeenth这三个词组成了一个短语。</li>\n</ul>\n<h2 id=\"上下文无关法则\"><a href=\"#上下文无关法则\" class=\"headerlink\" title=\"上下文无关法则\"></a>上下文无关法则</h2><ul>\n<li>上下文无关语法，简称CFG，又称为短语结构语法，其形式化方法等价于Backus-Naur范式。一个上下文无关语法包含两个部分：规则或者产生式，词表。</li>\n<li>例如，用上下文无关语法描述名词词组，一种描述方式是名词词组可以由一个专有名词构成，也可以由一个限定词加一个名词性成分构成，而名词性成分可以是一个或多个名词，此CFG的规则为：<ul>\n<li>NP→Det Nominal</li>\n<li>NP→ProperNoun</li>\n<li>Nominal→Noun|Noun Nominal</li>\n</ul>\n</li>\n<li>CFG可以层级嵌套，因此上面的规则可以与下面表示词汇事实的规则（词表）结合起来：<ul>\n<li>Det→a</li>\n<li>Det→the</li>\n<li>Noun→flight</li>\n</ul>\n</li>\n<li>符号分为两类：<ul>\n<li>终极符号：与现实中单词对应的符号，词表是引入终极符号的规则的集合</li>\n<li>非终极符号：表示终极符号的聚类或者概括性符号</li>\n</ul>\n</li>\n<li>在每个规则里箭头右边包含一个或多个终极符号和非终极符号，箭头左边为一个非终极符号，与每个单词相关联的是其词类范畴（词类）。</li>\n<li>CFG既可以看成是生成句子的一种机制，也可以看成是给一个句子分配结构的机制。</li>\n<li>以之前提到的CFG为例，对一个符号串NP，可以逐步生成：<script type=\"math/tex; mode=display\">\nNP→Det Nominal→Det Noun→a flight</script></li>\n<li>称 a flight是NP的一个推导，一般用一个剖析树表示一种推导：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ5P1.png\" alt=\"FoZ5P1.png\"><br>一个CFG定义了一个形式语言，形式语言是符号串的集合，如果有一个语法推导出的句子处于由该语法定义的形式语言中，这个句子就是合语法的。使用形式语言来模拟自然语言的语法成为生成式语法。</li>\n<li>上下文无关语法的正式定义：<ul>\n<li>N：非终止符号（或者变量）的集合</li>\n<li>Sigma：终止符号的集合，与N不相交</li>\n<li>R：规则或者产生式的集合</li>\n<li>S：指定的开始符号</li>\n</ul>\n</li>\n<li>一些约定定义：<ul>\n<li>大写字母：代表非终止符号</li>\n<li>S：开始符号</li>\n<li>小写希腊字母：从非终止符号和终止符号的并集中抽取出来的符号串</li>\n<li>小写罗马字母：终止符号串</li>\n</ul>\n</li>\n<li>直接导出的定义：<br><strong>公式待补充</strong></li>\n<li>导出是直接导出的泛化。之后我们可以正式定义由语法G生成的语言L是一个由终止符号组成的字符串集合，这些终止符号可以从指定的开始符号S通过语法G导出：<br><strong>公式待补充</strong></li>\n<li>将一个单词序列映射到其对应的剖析树成为句法剖析。</li>\n</ul>\n<h2 id=\"英语的一些语法规则\"><a href=\"#英语的一些语法规则\" class=\"headerlink\" title=\"英语的一些语法规则\"></a>英语的一些语法规则</h2><ul>\n<li>英语中最常用最重要的四种句子结构：<ul>\n<li>陈述式结构：主语名词短语加一个动词短语</li>\n<li>命令式结构：通常以一个动词短语开头，并且没有主语</li>\n<li>Yes-no疑问式结构：通常用于提问，并且以一个助动词开头，后面紧跟一个主语NP，再跟一个VP</li>\n<li>Wh疑问式结构：包含一个wh短语成分</li>\n</ul>\n</li>\n<li>在之前的描述中开始符号用于单独生成整个句子，但是S也可以出现在语法生成规则的右边，嵌入到更大的句子当中。这样的S称为从句，拥有完整的语义。拥有完整的语义是指这个S在整体句子的语法剖析树当中，其子树当中的主要动词拥有所需的所有论元。</li>\n</ul>\n<h2 id=\"名词短语\"><a href=\"#名词短语\" class=\"headerlink\" title=\"名词短语\"></a>名词短语</h2><ul>\n<li>限定词Det：名词短语可以以一些简单的词法限定词开始，例如a,the,this,those,any,some等等，限定词的位置也可以被更复杂的表示替代，例如所有格。这样的表示是可以递归定义的，例如所有格加名词短语可以构成更大的名词短语的限定词。在复数名词、物质名词之前不需要加限定词。</li>\n<li>名词性词Nominal：包含一些名词前或者名词后修饰语</li>\n<li>名词之前，限定词之后：一些特殊的词类可以出现在名词之前限定词之后，包括基数词Card、序数词Ord、数量修饰语Quant。</li>\n<li>形容词短语AP：形容词短语之前可以出现副词</li>\n<li>可以讲名词短语的前修饰语规则化如下（括号内代表可选）：</li>\n<li>NP-&gt;(Det)(Card)(Ord)(Quant)(AP)Nominal</li>\n<li>后修饰语主要包含三种：<ul>\n<li>介词短语PP：Nominal-&gt;Nominal PP(PP)(PP)</li>\n<li>非限定从句：动名词后修饰语GerundVP,GerundVP-&gt;GerundV NP | GerundV PP | GerundV | GerundV NP PP</li>\n<li>关系从句：以关系代词开头的从句 Nominal -&gt;Nominal RelCaluse;RelCaluse -&gt; (who|that) VP</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"一致关系\"><a href=\"#一致关系\" class=\"headerlink\" title=\"一致关系\"></a>一致关系</h2><ul>\n<li>每当动词有一个名词作为它的主语时，就会发生一致关系的现象，凡是主语和他的动词不一致的句子都是不合语法的句子，例如第三人称单数动词没有加-s。可以使用多个规则的集合来扩充原有的语法，使得语法可以处理一致关系。例如yes-no疑问句的规则是<script type=\"math/tex; mode=display\">\nS \\rightarrow Aux \\ NP \\ VP</script></li>\n<li>可以用如下形式的两个规则来替代：<script type=\"math/tex; mode=display\">\nS \\rightarrow 3sgAux \\ 3sgNP \\ VP \\\\\nS \\rightarrow Non3sgAux \\ Non3sgNP \\ VP \\\\</script></li>\n<li>再分别指定第三人称单数和非第三人称单数的助动词形态。这样的方法会导致语法规模增加。</li>\n</ul>\n<h2 id=\"动词短语和次范畴化\"><a href=\"#动词短语和次范畴化\" class=\"headerlink\" title=\"动词短语和次范畴化\"></a>动词短语和次范畴化</h2><ul>\n<li>动词短语包括动词和其他一些成分的组合，包括NP和PP以及两者的组合。整个的嵌入句子也可以跟随在动词之后，成为句子补语。</li>\n<li>动词短语的另一个潜在成分是另一个动词短语。</li>\n<li>动词后面也可以跟随一个小品词，小品词类似于借此，但与动词组合在一起是构成一个短语动词，与动词不可分割。</li>\n<li>次范畴化即再分类。传统语法把动词次范畴化为及物动词和不及物动词，而现代语法已经把动词区分为100个次范畴。讨论动词和可能的成分之间的关系是将动词看成一个谓词，而成分想象成这个谓词的论元(argument)。</li>\n<li>对于动词和它的补语之间的关系，我们可以用上下文无关语法表示一致关系特征，且需要区分动词的各个次类。</li>\n</ul>\n<h2 id=\"助动词\"><a href=\"#助动词\" class=\"headerlink\" title=\"助动词\"></a>助动词</h2><ul>\n<li>助动词是动词的一个次类，具有特殊的句法约束。助动词包括情态动词、完成时助动词、进行时助动词、被动式助动词。每一个助动词都给他后面的动词形式一个约束，且需要按照一定的顺序进行结合。</li>\n<li>四种助动词给VP次范畴化时，VP的中心动词分别是光杆动词、过去分词形式、现在分词形式、过去分词形式。</li>\n<li>一个句子可以用多个助动词，但是要按照情态助动词、完成时助动词、进行式助动词、被动式助动词的顺序。</li>\n</ul>\n<h2 id=\"树图资料库\"><a href=\"#树图资料库\" class=\"headerlink\" title=\"树图资料库\"></a>树图资料库</h2><ul>\n<li>上下文无关语法可以将一个句子剖析成一个句法剖析树，如果一个语料中所有句子都以句法剖析树的形式表示，这样的句法标注了的语料就称为树图资料库(treebank)。</li>\n<li>树图资料库中的句子隐含的组成了一种语言的语法，我们可以对于每一棵句法剖析树提取其中的CFG规则。从宾州树库中提取出来的CFG规则非常扁平化，使得规则数量很多且规则很长。</li>\n<li>在树库中搜索需要一种特殊的表达式，能够表示关于节点和连接的约束，用来搜索特定的模式。例如tgrep或者TGrep2。</li>\n<li>在tgrep、TGrep2中的一个模式由一个关于节点的描述组成，一个节点描述可以用来返回一个以此节点为根的子树。</li>\n<li>可以使用双斜线对某一类模式命名：<script type=\"math/tex; mode=display\">\n/NNS?/    NN|NNS</script></li>\n<li>Tgrep/Tgrep2模式的好处在于能够描述连接的信息。小于号代表直接支配，远小于符号代表支配，小数点代表线性次序。这种对于连接的描述反应在剖析树中的关系如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZ2b4.png\" alt=\"FoZ2b4.png\"></li>\n</ul>\n<h2 id=\"中心词和中心词查找\"><a href=\"#中心词和中心词查找\" class=\"headerlink\" title=\"中心词和中心词查找\"></a>中心词和中心词查找</h2><ul>\n<li>句法成分能够与一个词法中心词相关联。在一个简单的词法中心词模型中，每一个上下文无关规则与一个中心词相关联，中心词传递给剖析树，因此剖析树中每一个非终止符号都被一个单一单词所标注，这个单一单词就是这个非终止符号的中心词。一个例子如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZfa9.png\" alt=\"FoZfa9.png\"></li>\n<li>为了生成这样一棵树，每一个CFG规则都必须扩充来识别一个右手方向的组成成分来作为中心词子女节点。一个节点的中心词词被设置为其子女中心词的中心词。</li>\n<li>另一种方式是通过一个计算系统来完成中心词查找。在这种方式下是依据树的上下文来寻找指定的句子，从而动态的识别中心词。一旦一个句子被解析出来，树将会被遍历一遍并使用合适的中心词来装饰每一个节点。</li>\n</ul>\n<h2 id=\"语法等价与范式\"><a href=\"#语法等价与范式\" class=\"headerlink\" title=\"语法等价与范式\"></a>语法等价与范式</h2><ul>\n<li>语法等价包括两种：强等价，即两个语法生成相同的符号串集合，且他们对于每个句子都指派同样的短语结构；弱等价，即两个语法生成相同的符号串集合，但是不给每个句子指派相同的短语结构。</li>\n<li>语法都使用一个范式，在范式中每个产生式都使用一个特定的形式。例如一个上下文五官与法是sigma自由的，并且如果他们的每个产生式的形式为A-&gt;BC或者是A-&gt;a，就说明这个上下文无关语法是符合Chomsky范式的，简称CNF。凡是Chomsky范式的语法都具有二叉树形式。任何上下文无关语法都可以转变成一个弱等价的Chomsky范式语法。</li>\n<li>使用二叉树形式的剖析树能够产生更小的语法。形如A-&gt;A B的规则称为Chomsky并连。</li>\n</ul>\n<h2 id=\"有限状态语法和上下文无关语法\"><a href=\"#有限状态语法和上下文无关语法\" class=\"headerlink\" title=\"有限状态语法和上下文无关语法\"></a>有限状态语法和上下文无关语法</h2><ul>\n<li>复杂的语法模型必须表示组成性，因而不适合用有限状态模型来描述语法。</li>\n<li>当一个非终止符号的展开式中也包含了这个非终止符号时，就会产生语法的递归问题。</li>\n<li>例如，使用正则表达式来描述以Nominal为中心的名词短语：<br>(Det)(Card)(Ord)(Quant)(AP)Nominal(PP)*</li>\n<li>为了完成这个正则表达式，只需要按顺序展开PP，展开结果为(P NP)*，这样就出现了地柜问题，因为此时出现了NP，在NP的正则表达式中出现了NP。</li>\n<li>一个上下文无关语法能够被有限自动机生成，当且仅当存在一个生成语言L的没有任何中心自嵌入递归的上下文无关语法。</li>\n</ul>\n<h2 id=\"依存语法\"><a href=\"#依存语法\" class=\"headerlink\" title=\"依存语法\"></a>依存语法</h2><ul>\n<li>依存语法与上下文无关语法相对，其句法结构完全由词、词与词之间的语义或句法关系描述。一个例子如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZOVH.png\" alt=\"FoZOVH.png\"></li>\n<li>其中没有非终止符号或者短语节点，树中的连接只将两个词语相连。连接即依存关系，代表着语法功能或者一般的语义联系，例如句法主语、直接对象、间接宾语、时间状语等等。</li>\n<li>依存语法具有很强的预测剖析能力，且在处理具有相对自由词序的语言时表现更好。</li>\n</ul>\n<h1 id=\"第十三章：基于上下文无关语法的剖析\"><a href=\"#第十三章：基于上下文无关语法的剖析\" class=\"headerlink\" title=\"第十三章：基于上下文无关语法的剖析\"></a>第十三章：基于上下文无关语法的剖析</h1><h2 id=\"剖析即搜索\"><a href=\"#剖析即搜索\" class=\"headerlink\" title=\"剖析即搜索\"></a>剖析即搜索</h2><ul>\n<li>在句法剖析中，剖析可以看成对一个句子搜索一切可能的剖析树空间并发现正确的剖析树。</li>\n<li>对于某一个句子（输入符号串），剖析搜索的目标是发现以初始符号S为根并且恰好覆盖整个输入符号串的一切剖析树。搜索算法的约束来自两方面：<ul>\n<li>来自数据的约束，即输入句子本身，搜索出来的剖析树的叶子应该是原句的所有单词。</li>\n<li>来自语法的约束，搜索出来的剖析树应该有一个根，即初始符号S</li>\n</ul>\n</li>\n<li>根据这两种约束，产生了两种搜索策略：自顶向下，目标制导的搜索；自下而上，数据制导的搜索。</li>\n<li>对于自顶向下的搜索，从根开始，我们通过生成式不断生成下一层的所有可能子节点，搜索每一层的每一种可能，如下图（对于句子book that flight）：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZh5R.png\" alt=\"FoZh5R.png\"></li>\n<li>对于自底向上的搜索，剖析从输入的单词开始，每次都使用语法中的规则，试图从底部的单词向上构造剖析树，如果剖析树成功的构造了以初始符号S为根的树，而且这个树覆盖了整个输入，那么就剖析成功。首先通过词表将每个单词连接到对应的词类，如果一个单词有不止一个词类，就需要考虑所有可能。与自顶向下相反，每次进入下一层时，自底向上需要考虑被剖析的成分是否与某个规则的右手边相匹配，而自顶向下是与左手边相匹配。中途如果无法匹配到规则则将这个树枝从搜索空间中删除，如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZI8x.png\" alt=\"FoZI8x.png\"> </li>\n<li>两者对比：<ul>\n<li>自顶向下是从S开始搜索的，因此不会搜索那些在以S为根的树中找不到位置的子树，而自底向上会产生许多不可能的搜索树</li>\n<li>相对应的，自顶向下把搜索浪费在了不可能产生输入单词序列的树上</li>\n<li>综上，我们需要将自顶向下和自底向上相结合</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"歧义\"><a href=\"#歧义\" class=\"headerlink\" title=\"歧义\"></a>歧义</h2><ul>\n<li>在句法剖析中需要解决的一个问题是结构歧义，即语法会给一个句子多种剖析结果可能。</li>\n<li>最常见的两种歧义：附着歧义和并列连接歧义。</li>\n<li>如果一个特定的成分可以附着在剖析树的一个以上的位置，句子就会出现附着歧义。例如We saw the Eiffel Tower flying to Paris一句中,flying to Paris可以修饰Eiffel Tower也可以修饰We。</li>\n<li>在并列连接歧义中，存在着不同的短语，这些短语之间用and这样的连接词相连。例如old men and women可以是老年男性和老年女性，或者老年男性和普通女性，即old是否同时分配到men和women上。</li>\n<li>以上两种歧义还能相互组合嵌套形成更复杂的歧义。假如我们不消歧，仅仅返回所有的可能，留给用户或者人工判断，则随着剖析句子结构变复杂或者剖析规则的增加，得到的可能是成指数级增长的，具体而言，这种剖析句子可能的增长数和算术表达式插入括号问题相同，以Catalan数按指数增长：<script type=\"math/tex; mode=display\">\nC(n)=\\frac{1}{1+n} C_{2n}^n</script></li>\n<li>摆脱这种指数爆炸的方法有两个：<ul>\n<li>动态规划，研究搜索空间的规律性，使得常见的部分只推导一次，减少与歧义相关的开销</li>\n<li>使用试探性的方法来改善剖析器的搜索策略</li>\n</ul>\n</li>\n<li>使用例如深度优先搜索或者宽度优先搜索之类的有计划与回溯的搜索算法是在复杂搜索空间中搜索常用的算法，然而在复杂语法空间中无处不在的歧义使得这一类搜索算法效率低下，因为有许多重复的搜索过程。</li>\n</ul>\n<h2 id=\"动态规划剖析方法\"><a href=\"#动态规划剖析方法\" class=\"headerlink\" title=\"动态规划剖析方法\"></a>动态规划剖析方法</h2><ul>\n<li>在动态规划中，我们维护一个表，系统的将对于子问题的解填入表中，利用已经存储的子问题的解解决更大的子问题，而不用重复从头开始计算。</li>\n<li>在剖析中，这样的表用来存储输入中各个部分的子树，当子树被发现时就存入表中，以便以后调用，就这样解决了重复剖析的问题（只需查找子树而不需要重新剖析）和歧义问题（剖析表隐含的存储着所有可能的剖析结果）。</li>\n<li>主要的三种动态规划剖析方法有三种，CKY算法、Earley算法和表剖析算法。</li>\n</ul>\n<h3 id=\"CKY剖析\"><a href=\"#CKY剖析\" class=\"headerlink\" title=\"CKY剖析\"></a>CKY剖析</h3><ul>\n<li>CKY剖析要求语法必须满足Chomsky范式，即生成式右边要么时两个非终止符号要么是一个终止符号。如果不是Chomsky范式，则需要把一个一般的CFG转换成CNF：<ul>\n<li>右边有终止符号也有非终止符号：给右边的终止符号单独建一个非终止符号，例如：INF-VP → to VP，改成INF-VP → TO VP和TO → to</li>\n<li>右边只有一个非终止符号：这种非终止符号称为单元产物，它们最终会生成非单元产物，用最终生成的非单元产物规则来替换掉单元产物</li>\n<li>右边不止2个符号：引入新的非终止符号将规则分解</li>\n<li>词法规则保持不变，但是在转换的过程中可能会生成新的词法规则</li>\n</ul>\n</li>\n<li>当所有的规则都转换成CNF之后，表中的非终止符号在剖析中有两个子节点，且表中每一个入口代表了输入中的某个区间，对于某个入口例如[0,3]，其可以被拆分成两部分，假如一部分为[0,2]，则另一部分为[2,3]，前者在[0,3]的左边，后者在[0,3]的正下方，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZo26.png\" alt=\"FoZo26.png\"></li>\n<li>接下来就是如何填表，我们通过自底向上的方法来剖析，对于每个入口[i,j]，包含了输入中i到j这一区间部分的表格单元都会对这个入口值做出贡献，即入口[i,j]左边的单元和下边的单元。下表中的CKY伪算法图描述了这一过程：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZjIA.png\" alt=\"FoZjIA.png\"></li>\n<li>外层循环从左往右循环列，内层循环从下往上循环行，而最里面的循环式遍历串[i,j]的所有可能二分子串，表中存的是可以代表[i,j]区间符号串的非终止符号集合，因为是集合，所以不会出现重复的非终止符号。</li>\n<li>现在我们完成了识别任务，接下来是剖析。剖析即在[0,N]入口，对应整个句子，找到一个非终止符号作为起始符号S。首先我们要对算法做两点更改：<ul>\n<li>存入表中的不仅仅是非终止符号，还有其对应的指针，指向生成这个非终止符号的表入口</li>\n<li>允许一个入口中存在同一个非终止符号的不同版本</li>\n</ul>\n</li>\n<li>做了这些改动之后，这张表就包含了一个给定输入的所有可能剖析信息。我们可以选择[0,N]入口中任意一个非终止符号作为起始符号S，然后根据指针迭代提取出剖析信息。</li>\n<li>当然，返回所有的可能剖析会遇到指数爆炸的问题，因此我们在完整的表上应用维特比算法，计算概率最大的剖析并返回这个剖析结果。</li>\n</ul>\n<h3 id=\"Early算法\"><a href=\"#Early算法\" class=\"headerlink\" title=\"Early算法\"></a>Early算法</h3><ul>\n<li>相比CKY自底向上的剖析，Early算法采用了自顶向下的剖析，而且只用了一维的表保存状态，每个状态包含三类信息：<ul>\n<li>对应某一单一语法规则的子树</li>\n<li>子树的完成状态</li>\n<li>子树对应于输入中的位置</li>\n</ul>\n</li>\n<li>算法流程图如下：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZHKO.png\" alt=\"FoZHKO.png\"></li>\n<li>算法对于状态的操作有三种：<ul>\n<li>预测：造出一个新的状态来表示在剖析过程中生成的自顶向下的预测。当待剖析的状态为非终极符号但又不是词类范畴时，对于这个非终极符号的不同展开，预测操作都造出一个新的状态。</li>\n<li>扫描：当待剖析的状态是词类范畴时，就检查输入符号串，并把对应于所预测的词类范畴的状态加入线图中。</li>\n<li>完成：当右边所有状态剖析完成时，完成操作查找输入中在这个位置的语法范畴，发现并推进前面造出的所有状态。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"表剖析\"><a href=\"#表剖析\" class=\"headerlink\" title=\"表剖析\"></a>表剖析</h3><ul>\n<li>表剖析允许动态的决定表格处理的顺序，算法动态的依照计划依次删除图中的一条边，而计划中的元素排序是由规则决定的。<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZTxK.png\" alt=\"FoZTxK.png\"></li>\n</ul>\n<h2 id=\"部分剖析\"><a href=\"#部分剖析\" class=\"headerlink\" title=\"部分剖析\"></a>部分剖析</h2><ul>\n<li>有时我们只需要输入句子的部分剖析信息</li>\n<li>可以用有限状态自动机级联的方式完成部分剖析，这样会产生比之前提到的方法更加“平”的剖析树。</li>\n<li>另一种有效的部分剖析的方法是分块。使用最广泛覆盖的语法给句子做词类标注，将其分为有主要词类标注信息且不没有递归结构的子块，子块之间不重叠，就是分块。</li>\n<li>我们用中括号将每一个分块框起来，有可能一些词并没有被框住，属于分块之外。</li>\n<li>分块中最重要的是基本分块中不能递归包含相同类型的成分。</li>\n</ul>\n<h3 id=\"基于规则的有限状态分块\"><a href=\"#基于规则的有限状态分块\" class=\"headerlink\" title=\"基于规则的有限状态分块\"></a>基于规则的有限状态分块</h3><ul>\n<li>利用有限状态方式分块，需要为了特定目的手动构造规则，之后从左到右，找到最长匹配分块，并接着依次分块下去。这是一个贪心的分块过程，不保证全局最优解。</li>\n<li>这些分块规则的主要限制是不能包含递归。</li>\n<li>使用有限状态分块的优点在于可以利用之前转录机的输出作为输入来组成级联，在部分剖析中，这种方法能够有效近似真正的上下文无关剖析器。</li>\n</ul>\n<h3 id=\"基于机器学习的分块\"><a href=\"#基于机器学习的分块\" class=\"headerlink\" title=\"基于机器学习的分块\"></a>基于机器学习的分块</h3><ul>\n<li>分块可以看成序列分类任务，每个位置分类为1（分块）或者0（不分块）。用于训练序列分类器的机器学习方法都能应用于分块中。</li>\n<li>一种卓有成效的方法是将分块看成类似于词类标注的序列标注任务，用一个小的标注符号集同时编码分块信息和每一个块的标注信息，这种方式称为IOB标注，用B表示分块开始，I表示块内，O表示块外。其中B和I接了后缀，代表该块的句法信息。</li>\n<li>机器学习需要训练数据，而分块的已标数据很难获得，一种方法是使用已有的树图资料库，例如宾州树库。</li>\n</ul>\n<h3 id=\"评价分块系统\"><a href=\"#评价分块系统\" class=\"headerlink\" title=\"评价分块系统\"></a>评价分块系统</h3><ul>\n<li>准确率：模型给出的正确分块数/模型给出的总分块数</li>\n<li>召回率：模型给出的正确分块数/文本中总的正确分块数</li>\n<li>F1值：准确率和召回率的调和平均</li>\n</ul>\n<h1 id=\"第十四章：统计剖析\"><a href=\"#第十四章：统计剖析\" class=\"headerlink\" title=\"第十四章：统计剖析\"></a>第十四章：统计剖析</h1><h2 id=\"概率上下文无关语法\"><a href=\"#概率上下文无关语法\" class=\"headerlink\" title=\"概率上下文无关语法\"></a>概率上下文无关语法</h2><ul>\n<li>概率上下文无关语法PCFG是上下文无关语法的一种简单扩展，又称随机上下文无关语法。PCFG在定义上做出了一点改变：<ul>\n<li>N：非终止符号集合</li>\n<li>Σ：终止符号集合</li>\n<li>R：规则集合，与上下文无关语法相同，只不过多了一个概率p，代表某一项规则执行的条件概率$P(\\beta|A)$</li>\n<li>S：一个指定的开始符号</li>\n</ul>\n</li>\n<li>当某个语言中所有句子的概率和为1时，我们称这个PCFG时一致的。一些递归规则可能导致PCFG不一致。</li>\n</ul>\n<h2 id=\"用于消歧的PCFG\"><a href=\"#用于消歧的PCFG\" class=\"headerlink\" title=\"用于消歧的PCFG\"></a>用于消歧的PCFG</h2><ul>\n<li>对于一个给定句子，其某一特定剖析的概率是所有规则概率的乘积，这个乘积既是一个剖析的概率，也是剖析和句子的联合概率。这样，对于出现剖析歧义的句子，其不同剖析的概率不同，通过选择概率大的剖析可以消歧。</li>\n</ul>\n<h2 id=\"用于语言建模的PCFG\"><a href=\"#用于语言建模的PCFG\" class=\"headerlink\" title=\"用于语言建模的PCFG\"></a>用于语言建模的PCFG</h2><ul>\n<li>PCFG为一个句子分配了一个概率（即剖析的概率），因此可以用于语言建模。相比n元语法模型，PCFG在计算生成每一个词的条件概率时考虑了整个句子，效果更好。对于含歧义的句子，其概率是所有可能剖析的概率之和。</li>\n</ul>\n<h2 id=\"PCFG的概率CKY剖析\"><a href=\"#PCFG的概率CKY剖析\" class=\"headerlink\" title=\"PCFG的概率CKY剖析\"></a>PCFG的概率CKY剖析</h2><ul>\n<li>PCFG的概率剖析问题：为一个句子产生概率最大的剖析</li>\n<li>概率CKY算法扩展了CKY算法，CKY剖析树中的每一个部分被编码进一个$(n+1)*(n+1)$的矩阵（只用上三角部分），矩阵中每一个元素包含一个非终止符号集合上的概率分布，可以看成每一个元素也是V维，因此整个存储空间为$(n+1)*(n+1)*V$，其中[i,j,A]代表非终止符号A可以用来表示句子的i位置到j位置这一段的概率。</li>\n<li>算法伪代码：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZbrD.png\" alt=\"FoZbrD.png\"></li>\n<li>可以看到也是用k对某一区间[i,j]做分割遍历，取最大的概率组合作为该区间的概率，并向右扩展区间进行动态规划。</li>\n</ul>\n<h2 id=\"学习到PCFG的规则概率\"><a href=\"#学习到PCFG的规则概率\" class=\"headerlink\" title=\"学习到PCFG的规则概率\"></a>学习到PCFG的规则概率</h2><ul>\n<li>上面的伪算法图用到了每一个规则的概率。如何获取这个概率？两种方法，第一种朴素的方法是在一个已知的树库数据集上用古典概型统计出概率：<script type=\"math/tex; mode=display\">\nP(\\alpha \\rightarrow \\beta | \\alpha) = \\frac{Count(\\alpha \\rightarrow \\beta)}{\\sum _{\\gamma} Count(\\alpha \\rightarrow \\gamma)}</script></li>\n<li>假如我们没有树库，则可以用非概率剖析算法来剖析一个数据集，再统计出概率。但是非概率剖析算法在剖析歧义句子时，需要对每一种可能剖析计算概率，但是计算概率需要概率剖析算法，这样就陷入了鸡生蛋蛋生鸡的死循环。一种解决方案是先用等概率的剖析算法，剖析句子，得出每一种剖析得概率，然后用概率加权统计量，然后重新估计剖析规则的概率，继续剖析，反复迭代直到收敛。这种算法称为inside-outside算法，是前向后向算法的扩展，同样也是EM算法的一种特例。</li>\n</ul>\n<h2 id=\"PCFG的问题\"><a href=\"#PCFG的问题\" class=\"headerlink\" title=\"PCFG的问题\"></a>PCFG的问题</h2><ul>\n<li>独立性假设导致不能很好的建模剖析树的结构性依存：每个PCFG规则被假定为与其他规则独立，例如，统计结果表明代词比名词更有可能称为主语，因此当NP被展开时，如果NP是主语，则展开为代词的可能性较高——这里需要考虑NP在句子种的位置，然而这种概率依存关系是PCFG所不允许的，</li>\n<li>缺乏对特定单词的敏感，导致次范畴化歧义、介词附着、联合结构歧义的问题：例如在介词附着问题中，某一个介词短语into Afghanistan附着于哪一个部分，在PCFG中计算时被抽象化为介词短语应该附着一个哪一个部分，而抽象化的概率来自于对语料的统计，这种统计不会考虑特定的单词。又例如联合结构歧义，假如一个句子的两种可能剖析树使用了相同的规则，而规则在树中的位置不同，则PCFG对两种剖析计算出相同的概率：因为PCFG假定规则之间是独立的，联合概率是各个概率的乘积。</li>\n</ul>\n<h2 id=\"通过拆分和合并非终止符号来改进PCFG\"><a href=\"#通过拆分和合并非终止符号来改进PCFG\" class=\"headerlink\" title=\"通过拆分和合并非终止符号来改进PCFG\"></a>通过拆分和合并非终止符号来改进PCFG</h2><ul>\n<li>先解决结构性依存的问题。之前提到了我们希望NP作为主语和宾语时有不同概率的规则，一种想法就是将NP拆分成主语NP和宾语NP。实现这种拆分的方法是父节点标注，及每个节点标注了其父节点，对于主语NP其父节点是S，对于宾语NP，其父节点是VP，因此不同的NP就得到了区分。除此之外，还可以通过词性拆分的方式增强剖析树。</li>\n<li>拆分会导致规则增多，用来训练每一条规则的数据变少，引起过拟合。因此要通过一个手写规则或者自动算法来根据每个训练集合并一些拆分。</li>\n</ul>\n<h2 id=\"概率词汇化的CFG\"><a href=\"#概率词汇化的CFG\" class=\"headerlink\" title=\"概率词汇化的CFG\"></a>概率词汇化的CFG</h2><ul>\n<li>概率CKY剖析更改了语法规则，而概率词汇化模型更改了概率模型本身。对于每一条规则，不仅要产生成分的规则变化，还要在每个成分上标注其中心词和词性，如下图：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoeSRP.png\" alt=\"FoeSRP.png\"></li>\n<li>为了产生这样的剖析树，每一条PCFG规则右侧需要选择一个成分作为中心词子节点，用子节点的中心词和词性作为该节点的中心词和词性。<br>其中，规则被分成了两类，内部规则和词法规则，后者是确定的，前者是需要我们估计的：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZqqe.png\" alt=\"FoZqqe.png\"></li>\n<li>我们可以用类似父节点标注的思想来拆分规则，拆分后每一部分都对应一种可能的中心词选择。假如我们将概率词汇话的CFG看成一个大的有很多规则CFG，则可以用之前的古典概型来估计概率。但是这样的效果不会很好，因为这样的规则划分太细了，没有足够的数据来估计概率。因此我们需要做出一些独立性假设，将概率分解为更小的概率乘积，这些更小的概率能容易从语料中估计出来。</li>\n<li>不同的统计剖析器区别在于做出怎样的独立性假设。</li>\n<li>Collins剖析如下图所示：<br><img src=\"https://s2.ax1x.com/2019/01/03/FoZzGt.png\" alt=\"FoZzGt.png\"></li>\n<li>其概率拆解为：<script type=\"math/tex; mode=display\">\nP(VP(dumped,VBD)→VBD(dumped,VBD)NP(sacks,NNS)PP(into,P))= \\\\\nP_H (VBD│VP,dumped)\\* \\\\\nP_L (STOP│VP,VBD,dumped)\\* \\\\\nP_R (NP(sacks,NNS)│VP,VBD,dumped)\\* \\\\\nP_R (PP(into,P)│VP,VBD,dumped)\\* \\\\\nP_R (STOP|VP,VBD,dumped) \\\\</script></li>\n<li>给出生成式左边之后，首先生成规则的中心词，之后一个一个从里到外生成中心词的依赖。先从中心词左侧一直生成直到遇到STOP符号，之后生成右边。如上式做出概率拆分之后，每一个概率都很容易从较小的数据量中统计出来。完整的Collins剖析器更为复杂，还考虑了词的距离关系、平滑技术、未知词等等。</li>\n</ul>\n<h2 id=\"评价剖析器\"><a href=\"#评价剖析器\" class=\"headerlink\" title=\"评价剖析器\"></a>评价剖析器</h2><ul>\n<li>剖析器评价的标准方法叫做PARSEVAL测度，对于每一个句子s：<ul>\n<li>标记召回率=(Count(s的候选剖析中正确成分数）)/(Count(s的树库中正确成分数）)</li>\n<li>标记准确率=(Count(s的候选剖析中正确成分数）)/(Count(s的候选剖析中全部成分数）)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"判别式重排序\"><a href=\"#判别式重排序\" class=\"headerlink\" title=\"判别式重排序\"></a>判别式重排序</h2><ul>\n<li>PCFG剖析和Collins词法剖析都属于生成式剖析器。生成式模型的缺点在于很难引入任意信息，即很难加入对某一个PCFG规则局部不相关的特征。例如剖析树倾向于右生成这一特征就不方便加入生成式模型当中。</li>\n<li>对于句法剖析，有两类判别式模型，基于动态规划的和基于判别式重排序的。</li>\n<li>判别式重排包含两个阶段，第一个阶段我们用一般的统计剖析器产生前N个最可能的剖析及其对应的概率序列。第二个阶段我们引入一个分类器，将一系列句子以及每个句子的前N个剖析-概率对作为输入，抽取一些特征的大集合并针对每一个句子选择最好的剖析。特征包括：剖析概率、剖析树中的CFG规则、平行并列结构的数量、每个成分的大小、树右生成的程度、相邻非终止符号的二元语法、树的不同部分出现的次数等等。</li>\n</ul>\n<h2 id=\"基于剖析的语言建模\"><a href=\"#基于剖析的语言建模\" class=\"headerlink\" title=\"基于剖析的语言建模\"></a>基于剖析的语言建模</h2><ul>\n<li>使用统计剖析器来进行语言建模的最简单方式就是利用之前提到的二阶段算法。第一阶段我们运行一个普通的语音识别解码器或者机器翻译解码器（基于普通的N元语法），产生N个最好的候选；第二阶段，我们运行统计剖析器并为每一个候选句分配一个概率，选择概率最佳的。</li>\n</ul>\n<h2 id=\"人类剖析\"><a href=\"#人类剖析\" class=\"headerlink\" title=\"人类剖析\"></a>人类剖析</h2><ul>\n<li>人类在识别句子时也用到了类似的概率剖析思想，两个例子：<ul>\n<li>对于出现频率高的二元语法，人们阅读这个二元语法所花的时间就更少</li>\n<li>一些实验表明人类在消歧时倾向于选择统计概率大的剖析</li>\n</ul>\n</li>\n</ul>","popularPost_tmp_postPath":true,"eyeCatchImage":"https://s2.ax1x.com/2019/01/03/FoVj3V.png","popularPost_tmp_gaData":{"updated":"Mon Jul 22 2019 11:45:23 GMT+0800 (GMT+08:00)","title":"计算语言学笔记","path":"2018/11/16/coling/","eyeCatchImage":"https://s2.ax1x.com/2019/01/03/FoVj3V.png","excerpt":"<p>计算语言学课程笔记<br>参考教材：Speech and Language Processing：An Introduction to Natural Language Processing,Computational Linguistics, and Speech Recognition<br>一些公式待修订<br></p>","date":"2018-11-16T02:15:34.000Z","pv":0,"totalPV":0,"categories":"机器学习","tags":["math","machinelearning","theory","nlp"],"internalLinks":[],"keywords":[],"keywordsLength":0}}],"PostAsset":[],"PostCategory":[{"post_id":"cjyzl6ugq0006q8t5gjjkm0lm","category_id":"cjyzl6uga0003q8t5akntb696","_id":"cjyzl6uh0000aq8t51qw8xhui"},{"post_id":"cjyzl6uft0001q8t5nsfblf79","category_id":"cjyzl6uga0003q8t5akntb696","_id":"cjyzl6uh2000dq8t5g34f17yo"},{"post_id":"cjyzl6ug50002q8t5ork5a4u8","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6uh3000eq8t5u65brky9"},{"post_id":"cjyzl6ugf0005q8t5szv2405p","category_id":"cjyzl6uh1000bq8t59olju5sr","_id":"cjyzl6uh4000hq8t57d5ymbjy"},{"post_id":"cjyzl6ugu0007q8t5782i4jt5","category_id":"cjyzl6uh3000fq8t59rsiamjf","_id":"cjyzl6uh5000jq8t5tevrc6jk"},{"post_id":"cjyzl6v1m0015q8t5rh4j20af","category_id":"cjyzl6uh1000bq8t59olju5sr","_id":"cjyzl6v2x001dq8t57w5d55bg"},{"post_id":"cjyzl6v250017q8t5w6ijjtw9","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v34001gq8t5s4qs0hta"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","category_id":"cjyzl6uga0003q8t5akntb696","_id":"cjyzl6v3f001kq8t5k0f16t8x"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v3m001nq8t527eu68gh"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v3v001rq8t5umbrq67d"},{"post_id":"cjyzl6v39001jq8t5uchbmgpj","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v41001uq8t5tgiad4kt"},{"post_id":"cjyzl6v3j001mq8t5c22youiv","category_id":"cjyzl6uga0003q8t5akntb696","_id":"cjyzl6v4b001zq8t5g183ru20"},{"post_id":"cjyzl6v3y001tq8t5arlkkzb8","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v4p0025q8t5otk9vhcg"},{"post_id":"cjyzl6v46001yq8t5evnnqfb6","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v4w0028q8t5lqyt1ppt"},{"post_id":"cjyzl6v4g0021q8t5ctgyarzn","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v50002bq8t5i6mw035e"},{"post_id":"cjyzl6v3s001qq8t53esarwaf","category_id":"cjyzl6v44001wq8t5uq0xgo8o","_id":"cjyzl6v58002fq8t5iqees96p"},{"post_id":"cjyzl6v4m0024q8t5lfl52et2","category_id":"cjyzl6uh1000bq8t59olju5sr","_id":"cjyzl6v5a002hq8t54m0qybdx"},{"post_id":"cjyzl6v4t0027q8t55vh92gj8","category_id":"cjyzl6uh1000bq8t59olju5sr","_id":"cjyzl6v5c002kq8t53nm6o3fq"},{"post_id":"cjyzl6v4z002aq8t582f7wtuo","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6v5d002mq8t5aeihz75n"},{"post_id":"cjyzl6v54002eq8t5cgiqlg7x","category_id":"cjyzl6uh1000bq8t59olju5sr","_id":"cjyzl6v5f002pq8t5gosf47gt"},{"post_id":"cjyzl6v98003xq8t5d07qr5ez","category_id":"cjyzl6uh3000fq8t59rsiamjf","_id":"cjyzl6va20042q8t5pqyzpvyf"},{"post_id":"cjyzl6v9g003yq8t5kffn38zm","category_id":"cjyzl6uh3000fq8t59rsiamjf","_id":"cjyzl6vae0045q8t5ijky03lc"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vam0049q8t52mmhqd0w"},{"post_id":"cjyzl6v9y0041q8t52qdwmwqx","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vav004cq8t5cziy9qu4"},{"post_id":"cjyzl6va50044q8t5goc0bs6z","category_id":"cjyzl6uh3000fq8t59rsiamjf","_id":"cjyzl6vb0004fq8t5fuco180d"},{"post_id":"cjyzl6vaj0048q8t5fl1fqbyu","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vb9004kq8t5e36jomb1"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","category_id":"cjyzl6uga0003q8t5akntb696","_id":"cjyzl6vbl004oq8t5ete600yk"},{"post_id":"cjyzl6vb4004jq8t5vzuld000","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vbr004sq8t50f5azjno"},{"post_id":"cjyzl6vaq004bq8t5v507mb95","category_id":"cjyzl6vb2004gq8t5etx39xux","_id":"cjyzl6vbt004uq8t5lu31many"},{"post_id":"cjyzl6vbe004mq8t5pszv4ess","category_id":"cjyzl6vbp004qq8t5247nvv59","_id":"cjyzl6vc10050q8t53gqpk7vl"},{"post_id":"cjyzl6vig005sq8t53xxmwcmf","category_id":"cjyzl6v44001wq8t5uq0xgo8o","_id":"cjyzl6vis005xq8t5wivuon47"},{"post_id":"cjyzl6vik005tq8t51c4pn1iw","category_id":"cjyzl6uh3000fq8t59rsiamjf","_id":"cjyzl6vit005yq8t5gxmuvn3y"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vm10064q8t58bsj1sk3"},{"post_id":"cjyzl6vwm006cq8t5vxwvq4in","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vwr006eq8t5gvdbfyfu"},{"post_id":"cjyzl6vxt006iq8t50v9fu8mq","category_id":"cjyzl6ugx0008q8t5bkzv7txk","_id":"cjyzl6vxz006lq8t5a5b1or45"}],"PostTag":[{"post_id":"cjyzl6uft0001q8t5nsfblf79","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6uh6000lq8t5ng76hxc0"},{"post_id":"cjyzl6uft0001q8t5nsfblf79","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6uh6000mq8t5b2yb6ps2"},{"post_id":"cjyzl6uft0001q8t5nsfblf79","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6uh9000oq8t5m2o0m1x9"},{"post_id":"cjyzl6uft0001q8t5nsfblf79","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6uha000pq8t5ugw4vo62"},{"post_id":"cjyzl6uft0001q8t5nsfblf79","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6uhb000rq8t537tt7bnl"},{"post_id":"cjyzl6ug50002q8t5ork5a4u8","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6uhb000sq8t5v97ijuxd"},{"post_id":"cjyzl6ug50002q8t5ork5a4u8","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6uhc000uq8t58c12vmrw"},{"post_id":"cjyzl6ugf0005q8t5szv2405p","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6uhd000wq8t5jyga8g52"},{"post_id":"cjyzl6ugf0005q8t5szv2405p","tag_id":"cjyzl6uhc000tq8t582hvnrwr","_id":"cjyzl6uhe000xq8t5ww4ruelb"},{"post_id":"cjyzl6ugq0006q8t5gjjkm0lm","tag_id":"cjyzl6uhd000vq8t5qeukq45q","_id":"cjyzl6uhf0010q8t5bcuf7gvg"},{"post_id":"cjyzl6ugq0006q8t5gjjkm0lm","tag_id":"cjyzl6uhe000yq8t5steb0o1s","_id":"cjyzl6uhf0011q8t52xq9ro6t"},{"post_id":"cjyzl6ugu0007q8t5782i4jt5","tag_id":"cjyzl6uhf000zq8t5napwkyg9","_id":"cjyzl6uhh0013q8t5ge56mvh6"},{"post_id":"cjyzl6ugu0007q8t5782i4jt5","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6uhh0014q8t5e2cp0739"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6v30001eq8t5r9qgsbs0"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v38001iq8t5qs7roj4z"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v3h001lq8t5540gun1m"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6v3q001pq8t55rpsk2sd"},{"post_id":"cjyzl6v2k001aq8t50s4cpdg2","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6v3x001sq8t5y9u1uu30"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6v45001xq8t597mqp9qk"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v4e0020q8t5msroxdpb"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v4k0022q8t5cdlbtz90"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6v4r0026q8t55ebolay8"},{"post_id":"cjyzl6v2s001cq8t5zvi5w6k2","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6v4y0029q8t5jq5r1kli"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6v52002dq8t5w29mx4x2"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v59002gq8t5tijfrroq"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v5c002jq8t57kfx297v"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6v5d002lq8t58dv0jjp9"},{"post_id":"cjyzl6v31001fq8t57pn8lhho","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6v5e002oq8t5b57dsea2"},{"post_id":"cjyzl6v39001jq8t5uchbmgpj","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v5f002qq8t5l5y8qkrp"},{"post_id":"cjyzl6v39001jq8t5uchbmgpj","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v5h002sq8t53a7xj5oc"},{"post_id":"cjyzl6v1m0015q8t5rh4j20af","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v5h002tq8t5rhev3f6l"},{"post_id":"cjyzl6v1m0015q8t5rh4j20af","tag_id":"cjyzl6v2i0019q8t542dt74q0","_id":"cjyzl6v5i002uq8t5e6k9a6ko"},{"post_id":"cjyzl6v1m0015q8t5rh4j20af","tag_id":"cjyzl6v35001hq8t5w03lxp8s","_id":"cjyzl6v5j002wq8t5bxf1iti5"},{"post_id":"cjyzl6v4m0024q8t5lfl52et2","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v5k002xq8t5eepl7ek9"},{"post_id":"cjyzl6v4m0024q8t5lfl52et2","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v5p002zq8t5bz43q1j9"},{"post_id":"cjyzl6v4m0024q8t5lfl52et2","tag_id":"cjyzl6uhc000tq8t582hvnrwr","_id":"cjyzl6v5q0030q8t5lqmjv4xz"},{"post_id":"cjyzl6v4m0024q8t5lfl52et2","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v5r0032q8t53lqu8mde"},{"post_id":"cjyzl6v4t0027q8t55vh92gj8","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v5s0033q8t5n5r9seoi"},{"post_id":"cjyzl6v4t0027q8t55vh92gj8","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v5t0035q8t5jr0wdqfd"},{"post_id":"cjyzl6v4t0027q8t55vh92gj8","tag_id":"cjyzl6uhc000tq8t582hvnrwr","_id":"cjyzl6v5u0036q8t5smwqrufw"},{"post_id":"cjyzl6v4t0027q8t55vh92gj8","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v5v0038q8t58n509krn"},{"post_id":"cjyzl6v250017q8t5w6ijjtw9","tag_id":"cjyzl6v3o001oq8t54i08fdog","_id":"cjyzl6v5w0039q8t5a4eaeuek"},{"post_id":"cjyzl6v250017q8t5w6ijjtw9","tag_id":"cjyzl6v43001vq8t5rcu1lhgl","_id":"cjyzl6v5x003aq8t5us9uod4q"},{"post_id":"cjyzl6v250017q8t5w6ijjtw9","tag_id":"cjyzl6v4l0023q8t5pdt1mjce","_id":"cjyzl6v62003cq8t5rtk2hwk2"},{"post_id":"cjyzl6v54002eq8t5cgiqlg7x","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v63003dq8t5geezgcav"},{"post_id":"cjyzl6v54002eq8t5cgiqlg7x","tag_id":"cjyzl6uhc000tq8t582hvnrwr","_id":"cjyzl6v66003eq8t5gig21ev9"},{"post_id":"cjyzl6v3j001mq8t5c22youiv","tag_id":"cjyzl6v51002cq8t5t6c0nw8u","_id":"cjyzl6v67003fq8t54xd9a7r0"},{"post_id":"cjyzl6v3j001mq8t5c22youiv","tag_id":"cjyzl6v43001vq8t5rcu1lhgl","_id":"cjyzl6v68003gq8t5izecr3if"},{"post_id":"cjyzl6v3j001mq8t5c22youiv","tag_id":"cjyzl6v5e002nq8t52jzcy6v8","_id":"cjyzl6v6a003hq8t56byvht7a"},{"post_id":"cjyzl6v3s001qq8t53esarwaf","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v6b003iq8t5qtqz9abm"},{"post_id":"cjyzl6v3s001qq8t53esarwaf","tag_id":"cjyzl6v5g002rq8t51ldxlhzo","_id":"cjyzl6v6c003jq8t5bxkx7f1n"},{"post_id":"cjyzl6v3y001tq8t5arlkkzb8","tag_id":"cjyzl6v5i002vq8t5rnce1b6h","_id":"cjyzl6v6d003kq8t5qm93z3ow"},{"post_id":"cjyzl6v3y001tq8t5arlkkzb8","tag_id":"cjyzl6v43001vq8t5rcu1lhgl","_id":"cjyzl6v6f003lq8t5j6yb47vs"},{"post_id":"cjyzl6v3y001tq8t5arlkkzb8","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v6j003mq8t525jjtvqa"},{"post_id":"cjyzl6v46001yq8t5evnnqfb6","tag_id":"cjyzl6v5q0031q8t5n1fwjy2a","_id":"cjyzl6v6l003nq8t5ufjpp2yn"},{"post_id":"cjyzl6v46001yq8t5evnnqfb6","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v6m003oq8t52x5s8n7q"},{"post_id":"cjyzl6v46001yq8t5evnnqfb6","tag_id":"cjyzl6v5t0034q8t5d34u0smt","_id":"cjyzl6v6o003pq8t527250vbt"},{"post_id":"cjyzl6v4g0021q8t5ctgyarzn","tag_id":"cjyzl6v5v0037q8t5y566fw04","_id":"cjyzl6v6q003qq8t5diral36u"},{"post_id":"cjyzl6v4g0021q8t5ctgyarzn","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v6r003rq8t5rvk117w1"},{"post_id":"cjyzl6v4g0021q8t5ctgyarzn","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6v6t003sq8t59557czrq"},{"post_id":"cjyzl6v4z002aq8t582f7wtuo","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6v6t003tq8t5xj42kem3"},{"post_id":"cjyzl6v4z002aq8t582f7wtuo","tag_id":"cjyzl6v43001vq8t5rcu1lhgl","_id":"cjyzl6v6u003uq8t583f2j0j4"},{"post_id":"cjyzl6v4z002aq8t582f7wtuo","tag_id":"cjyzl6v5x003bq8t5r2mip1xf","_id":"cjyzl6v6v003vq8t5bufjr5pz"},{"post_id":"cjyzl6v4z002aq8t582f7wtuo","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6v6w003wq8t5r0srkgki"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6va50043q8t59gvgy2zx"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vai0047q8t5btvgoc9u"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6vap004aq8t5it2jrdtx"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6vax004dq8t517e1gyqc"},{"post_id":"cjyzl6v9o0040q8t5bgxnearj","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6vb3004iq8t59hr5wpyn"},{"post_id":"cjyzl6v9y0041q8t52qdwmwqx","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6vbb004lq8t5pqfvyecy"},{"post_id":"cjyzl6v9y0041q8t52qdwmwqx","tag_id":"cjyzl6v43001vq8t5rcu1lhgl","_id":"cjyzl6vbk004nq8t502r65sym"},{"post_id":"cjyzl6v98003xq8t5d07qr5ez","tag_id":"cjyzl6v9m003zq8t5fvdlpfvj","_id":"cjyzl6vbq004rq8t5t7ilmrmn"},{"post_id":"cjyzl6v98003xq8t5d07qr5ez","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vbs004tq8t5lprbnehx"},{"post_id":"cjyzl6va50044q8t5goc0bs6z","tag_id":"cjyzl6v9m003zq8t5fvdlpfvj","_id":"cjyzl6vbw004wq8t53pmwlukq"},{"post_id":"cjyzl6va50044q8t5goc0bs6z","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vbx004xq8t550xlkozo"},{"post_id":"cjyzl6vaj0048q8t5fl1fqbyu","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6vc0004zq8t55bplvvsq"},{"post_id":"cjyzl6vaj0048q8t5fl1fqbyu","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6vc10051q8t5iirgrow4"},{"post_id":"cjyzl6v9g003yq8t5kffn38zm","tag_id":"cjyzl6v9m003zq8t5fvdlpfvj","_id":"cjyzl6vc30053q8t55v79it96"},{"post_id":"cjyzl6v9g003yq8t5kffn38zm","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vc70054q8t531e1uhbg"},{"post_id":"cjyzl6vaq004bq8t5v507mb95","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6vc80055q8t5t7xkgok9"},{"post_id":"cjyzl6vaq004bq8t5v507mb95","tag_id":"cjyzl6uhc000tq8t582hvnrwr","_id":"cjyzl6vcc0057q8t5iijc845u"},{"post_id":"cjyzl6vaq004bq8t5v507mb95","tag_id":"cjyzl6vb2004hq8t5suvud5nz","_id":"cjyzl6vcd0058q8t5ecsfjf6t"},{"post_id":"cjyzl6vaq004bq8t5v507mb95","tag_id":"cjyzl6vbm004pq8t5j1tynoe8","_id":"cjyzl6vch005aq8t5frs5aaw7"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6uge0004q8t5bee6bv8t","_id":"cjyzl6vch005bq8t5t9whnpt7"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6vbt004vq8t5qwf6212b","_id":"cjyzl6vcl005dq8t5dke2oooe"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6vcm005eq8t5rx8ezwoo"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6vbx004yq8t5h0w3vx07","_id":"cjyzl6vcp005gq8t55yy0izkt"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6vcv005hq8t5g6mre76h"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6vc20052q8t5vf1se4yw","_id":"cjyzl6vcw005iq8t5pjr818w3"},{"post_id":"cjyzl6vay004eq8t51ugdr1g9","tag_id":"cjyzl6vc80056q8t5o48d6x7g","_id":"cjyzl6vcy005kq8t5evf0xfu3"},{"post_id":"cjyzl6vb4004jq8t5vzuld000","tag_id":"cjyzl6vcd0059q8t5dte7pbyx","_id":"cjyzl6vcz005lq8t5nfv92biz"},{"post_id":"cjyzl6vb4004jq8t5vzuld000","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vd3005nq8t5fg9qll43"},{"post_id":"cjyzl6vb4004jq8t5vzuld000","tag_id":"cjyzl6vci005cq8t5k085the7","_id":"cjyzl6vd4005oq8t5jw4o74m6"},{"post_id":"cjyzl6vbe004mq8t5pszv4ess","tag_id":"cjyzl6vco005fq8t5ihm3p7wa","_id":"cjyzl6vd5005pq8t5cwyg43a2"},{"post_id":"cjyzl6vbe004mq8t5pszv4ess","tag_id":"cjyzl6vcw005jq8t59u6gl9px","_id":"cjyzl6vd6005qq8t5mflwo73w"},{"post_id":"cjyzl6vbe004mq8t5pszv4ess","tag_id":"cjyzl6vd0005mq8t5tri2hq9u","_id":"cjyzl6vd7005rq8t5o9wmgs50"},{"post_id":"cjyzl6vig005sq8t53xxmwcmf","tag_id":"cjyzl6uh5000kq8t5w0q0wi93","_id":"cjyzl6vio005uq8t5v106l337"},{"post_id":"cjyzl6vig005sq8t53xxmwcmf","tag_id":"cjyzl6v5g002rq8t51ldxlhzo","_id":"cjyzl6vir005wq8t5ai0o29mt"},{"post_id":"cjyzl6vik005tq8t51c4pn1iw","tag_id":"cjyzl6viq005vq8t5tg2mfle0","_id":"cjyzl6viu005zq8t5e146py5l"},{"post_id":"cjyzl6vik005tq8t51c4pn1iw","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6viv0060q8t51h96dofi"},{"post_id":"cjyzl6vik005tq8t51c4pn1iw","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6viw0061q8t5g7igsxgr"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","tag_id":"cjyzl6vm00063q8t5p5rzllwt","_id":"cjyzl6vm40067q8t5ip5p9gie"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vm50068q8t54l0rcoo4"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","tag_id":"cjyzl6vci005cq8t5k085the7","_id":"cjyzl6vm60069q8t54wc5dhk8"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","tag_id":"cjyzl6vm20065q8t5dryoqai8","_id":"cjyzl6vm6006aq8t5xxbxzutn"},{"post_id":"cjyzl6vlw0062q8t5uwn5behq","tag_id":"cjyzl6vm30066q8t5ewg3suwz","_id":"cjyzl6vm7006bq8t5qou5ax5b"},{"post_id":"cjyzl6vwm006cq8t5vxwvq4in","tag_id":"cjyzl6vwp006dq8t564c1pa82","_id":"cjyzl6vws006fq8t5seodr0vu"},{"post_id":"cjyzl6vwm006cq8t5vxwvq4in","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vws006gq8t5j02xquky"},{"post_id":"cjyzl6vwm006cq8t5vxwvq4in","tag_id":"cjyzl6vci005cq8t5k085the7","_id":"cjyzl6vwt006hq8t5jfro9zlk"},{"post_id":"cjyzl6vxt006iq8t50v9fu8mq","tag_id":"cjyzl6ugx0009q8t509asmzge","_id":"cjyzl6vxx006jq8t5lxcbdnpx"},{"post_id":"cjyzl6vxt006iq8t50v9fu8mq","tag_id":"cjyzl6uh1000cq8t5ykm95ceh","_id":"cjyzl6vxy006kq8t5f3cqbe7v"},{"post_id":"cjyzl6vxt006iq8t50v9fu8mq","tag_id":"cjyzl6uh3000gq8t504pinlvz","_id":"cjyzl6vxz006mq8t5xijr6kb9"},{"post_id":"cjyzl6vxt006iq8t50v9fu8mq","tag_id":"cjyzl6uh4000iq8t54v9zllz7","_id":"cjyzl6vy0006nq8t5b841pa9g"}],"Tag":[{"name":"abstractive summarization","_id":"cjyzl6uge0004q8t5bee6bv8t"},{"name":"math","_id":"cjyzl6ugx0009q8t509asmzge"},{"name":"machinelearning","_id":"cjyzl6uh1000cq8t5ykm95ceh"},{"name":"theory","_id":"cjyzl6uh3000gq8t504pinlvz"},{"name":"nlp","_id":"cjyzl6uh4000iq8t54v9zllz7"},{"name":"code","_id":"cjyzl6uh5000kq8t5w0q0wi93"},{"name":"python","_id":"cjyzl6uhc000tq8t582hvnrwr"},{"name":"comprehension","_id":"cjyzl6uhd000vq8t5qeukq45q"},{"name":"NLI","_id":"cjyzl6uhe000yq8t5steb0o1s"},{"name":"convex optimization","_id":"cjyzl6uhf000zq8t5napwkyg9"},{"name":"server","_id":"cjyzl6v2i0019q8t542dt74q0"},{"name":"linux","_id":"cjyzl6v35001hq8t5w03lxp8s"},{"name":"corex","_id":"cjyzl6v3o001oq8t54i08fdog"},{"name":"machine learning","_id":"cjyzl6v43001vq8t5rcu1lhgl"},{"name":"topic model","_id":"cjyzl6v4l0023q8t5pdt1mjce"},{"name":"acl","_id":"cjyzl6v51002cq8t5t6c0nw8u"},{"name":"natural language processing","_id":"cjyzl6v5e002nq8t52jzcy6v8"},{"name":"android","_id":"cjyzl6v5g002rq8t51ldxlhzo"},{"name":"gcn","_id":"cjyzl6v5i002vq8t5rnce1b6h"},{"name":"glove","_id":"cjyzl6v5q0031q8t5n1fwjy2a"},{"name":"word embedding","_id":"cjyzl6v5t0034q8t5d34u0smt"},{"name":"logistic regression","_id":"cjyzl6v5v0037q8t5y566fw04"},{"name":"statistical learning","_id":"cjyzl6v5x003bq8t5r2mip1xf"},{"name":"linearalgebra","_id":"cjyzl6v9m003zq8t5fvdlpfvj"},{"name":"c++","_id":"cjyzl6vb2004hq8t5suvud5nz"},{"name":"algorithm","_id":"cjyzl6vbm004pq8t5j1tynoe8"},{"name":"seq2seq","_id":"cjyzl6vbt004vq8t5qwf6212b"},{"name":"rnn","_id":"cjyzl6vbx004yq8t5h0w3vx07"},{"name":"lstm","_id":"cjyzl6vc20052q8t5vf1se4yw"},{"name":"gru","_id":"cjyzl6vc80056q8t5o48d6x7g"},{"name":"vae","_id":"cjyzl6vcd0059q8t5dte7pbyx"},{"name":"mcmc","_id":"cjyzl6vci005cq8t5k085the7"},{"name":"web","_id":"cjyzl6vco005fq8t5ihm3p7wa"},{"name":"hexo","_id":"cjyzl6vcw005jq8t59u6gl9px"},{"name":"github","_id":"cjyzl6vd0005mq8t5tri2hq9u"},{"name":"bayes","_id":"cjyzl6viq005vq8t5tg2mfle0"},{"name":"inference","_id":"cjyzl6vm00063q8t5p5rzllwt"},{"name":"variational inference","_id":"cjyzl6vm20065q8t5dryoqai8"},{"name":"em","_id":"cjyzl6vm30066q8t5ewg3suwz"},{"name":"lda","_id":"cjyzl6vwp006dq8t564c1pa82"}]}}